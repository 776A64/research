transformer,bbslzy001,The TFLite get different results on Python and Android," 1. System information  OS Platform and Distribution: `MacOS 14.6`, `Android 15`  TensorFlow version: `TensorFlow 2.13.1 (Python)`, `TensorFlowLite 2.12.0 (Android)`  Others: `Python 3.10.16`, `JDK17`, `Android Gradle Plugin 8.7.3`, `Gradle 8.9`  2. Code  Step 1: Convert the model to tflite > The model is derived from the hanlp project, here is the url for model: https://file.hankcs.com/hanlp/ner/msra_ner_albert_base_20211228_173323.zip    Step 2: Run on Python (Get the right result)    Step 3: Run on Android (Get the wrong result) > The BertTokenizer is derived from the github project: https://github.com/ankiteciitkgp/bertTokenizer    3. Failure after conversion  The model produces right results on python.  The model produces wrong result on android, it does not recognize any named entities. All values in `predictedClasses` are 1. I checked the shape, data type, and each value of the input tensor, and checked the shape and data type of the output tensor. They were all fine. And no error logs were generated when the model was called. I don't know how to solve it. :(",2024-12-31T07:00:21Z,comp:lite type:others TFLiteConverter TF 2.13,closed,0,2,https://github.com/tensorflow/tensorflow/issues/83936,"I solved it!!! I ignored the order of the input tensors. In Android, I mistakenly reversed the order of input_ids and attention_mask. I modify as follows: `Object[] inputs = {paddedAttentionMask, paddedInputIds, paddedTokenTypeIds};`",Are you satisfied with the resolution of your issue? Yes No
yi,cemery123,@llvm_toolchain error," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf2.14  Custom code Yes  OS platform and distribution Ubuntu 22.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi there, Apologies for any inconvenience, and I truly appreciate your time and help. I’m currently trying to crosscompile TensorFlow Lite for my RISC system. Unfortunately, I’ve encountered issues that I can’t seem to resolve. On my x86_64 machine, I attempted the following Bazel build command: bazel build //tensorflow/tools/pip_package:build_pip_package crosstool_top=//:toolchain cpu=riscv64 host_cpu=x86_64 copt=march=rv64gc copt=mabi=lp64d linkopt=march=rv64gc linkopt=mabi=lp64d However, I keep receiving the error: no such package '//': The repository '' could not be resolved: Repository '' is not defined I’ve tried modifying the WORKSPACE file and pointing to a local LLVM installation, but that hasn’t resolved the issue either. If you have any suggestions or guidance on how to address this problem, I’d greatly appr",2024-12-30T18:44:18Z,stat:awaiting response type:support stale comp:lite TF2.14,closed,0,4,https://github.com/tensorflow/tensorflow/issues/83913,"Hi **** , Apologies for the delay, and thank you for raising your concern here. Could you please check the compatible version of Bazel? This might be causing the issue. I have attached the documentation for your reference. Additionally, consider upgrading to the latest version of TensorFlow, as it may help resolve the issue and ensure smoother execution. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[xla:cpu] Use vector::data() to access underlying values on a hot path,[xla:cpu] Use vector::data() to access underlying values on a hot path ,2024-12-28T05:31:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83814
yi,copybara-service[bot],[xla:cpu] Use sorted inputs + offsets to optimize SortIterator,"[xla:cpu] Use sorted inputs + offsets to optimize SortIterator Instead of carrying pointers + primitive sizes together with every iterator, reference and value, keep them separate inside Inputs/DInputs struct and only keep an offset into the inputs arrays.  Will work on fixing regressions for small number of inputs in followup PRs.",2024-12-27T17:24:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83809
yi,carlosg-m,Cannot Fine-Tune Hugging Face TF model on GPU (it works on CPU)," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.1 LTS  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5.1 / 9  GPU model and memory Nvidia Tesla T4 GPU (16GB)  Current behavior? I'm trying to fine tune HuggingFace's `TFResNetModel` using `tf.keras`. The provided example works on CPU, however when I enable the GPU I get the following error.  The GPU is running and working fine for a simple Keras model example.  The problem seems to be related to the integration of tf.Keras and HuggingFace.  Standalone code to reproduce the issue   Relevant log output ",2024-12-27T15:31:24Z,stat:awaiting response type:bug stale TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/83802,"m, Hi, By default the colab notebook is using tensorflow v2.17 and v2.18 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.  Also I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it here. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
gemma,copybara-service[bot],[xla:cpu] Add e2e benchmark for gemma2 flax,[xla:cpu] Add e2e benchmark for gemma2 flax,2024-12-23T20:50:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83629
yi,Hemavarna12,Failed to load the native TensorFlow runtime.," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? It seems that the error is encountered is related to a failure in loading the TensorFlow native runtime. This type of issue is usually related to missing or incompatible dependencies, specifically dynamic link libraries (DLLs), which are required for TensorFlow to function prop  Standalone code to reproduce the issue   Relevant log output ",2024-12-23T19:11:57Z,stat:awaiting response type:build/install stale TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/83626,"Hi **** , Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. https://github.com/tensorflow/tensorflow/issues/61887 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],[XLA:GPU] Use NCCL thunk for degenerate RaggedAllToAll.,"[XLA:GPU] Use NCCL thunk for degenerate RaggedAllToAll. All collectives default to a copy is there is no communication between replicas needed. Using a copy doesn't work for RaggedAllToAll, because it because a generic DynamicUpdateSlice that we can not express in HLO. The best option we have right now is to use the same NCCL thunk.",2024-12-23T17:12:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83623
yi,Karma5s,failed to build tensorflow 2.10 GPU support," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.10  Custom code Yes  OS platform and distribution Windows 11. MSYS_NT10.022631   Mobile device _No response_  Python version 3.9  Bazel version 5.1.1  GCC/compiler version MSVC 2019  CUDA/cuDNN version CUDA 11.2/ cuDNN 8.1  GPU model and memory NVIDIA RTX A3000 12GB  Current behavior? Missing header files from cudnn_frontend, have tried modifying the bazel files to download other versions, same issue missing header from cudnn_frontend_EngineFallbackList.h  Standalone code to reproduce the issue   Relevant log output ",2024-12-23T02:46:26Z,stat:awaiting response type:build/install stale subtype:windows TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/83566,", Tensorflow v2.10 is a pretty older version which does not actively supported. Also GPU support on nativeWindows is only available for 2.10 or earlier versions, starting in TF 2.11, CUDA build is not supported for Windows. For using TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2 or use tensorflowcpu with TensorFlowDirectMLPlugin. https://github.com/tensorflow/tensorflow/issues/61226 https://www.tensorflow.org/install/source_windowsinstall_gpu_support_optional Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,Davischoice1,DLL load failure, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code Yes  OS platform and distribution windows 10  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? i want a successful run  Standalone code to reproduce the issue   Relevant log output ,2024-12-21T05:15:18Z,stat:awaiting response type:bug TF 2.18,closed,0,8,https://github.com/tensorflow/tensorflow/issues/83508,same error here,Same error here,"Hi **** , Apologies for the delay. Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. https://github.com/tensorflow/tensorflow/issues/61887 Thank you!",tensorflow version: 2.18.Orc2,actually is version 2.18.0 ,"Hi **** , Could you please open another issue with all the relevant details? This will make it easier for us to track and assist you effectively. Thank you!",Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[XLA] Fix ShapeError crashes when element_type is not in the enum,[XLA] Fix ShapeError crashes when element_type is not in the enum We tried to prettyprint the name of the type but this is not possible if the element_type is not in the enum. Print the underlying integer instead.,2024-12-20T20:18:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83489
yi,copybara-service[bot],[XLA:Python] Add locking around lazily-initialized fields in PyDeviceList.,"[XLA:Python] Add locking around lazilyinitialized fields in PyDeviceList. * We protect is_fully_addressable_, addressable_device_list_, memory_kind_info_ and hash_ with the PyDeviceList object's associated lock. * DefaultMemoryKind and MemoryKinds are update to be static methods that take a Python object reference, so we have easy access to that lock. * We change a number of other methods to be private. * We move the module registration function into a static method so it can access private methods more easily. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19096 from openxla:skozub/e2m1 d4de0a369d9dc853f34f3cf3bf7dcc5a47502106",2024-12-20T20:03:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83488
yi,tbjerke04,TF - Problems when trying to use GPU on M3 Max," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.2  Custom code Yes  OS platform and distribution Sequoia 15.2  Mobile device _No response_  Python version 3.9.15  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory 30core (M3 Max)  Current behavior? I have tried to troubleshoot the problem together with ChatGPT o1, and after a lot of testing, it came to the conclusion that it might be a problem with tensorflow running om my M3 Max. It works fine with the CPU, but as soon as I try to use the GPU, no matter the complexity of the program, it crashes immediately.  Standalone code to reproduce the issue   Relevant log output ",2024-12-20T00:55:58Z,stat:awaiting response type:bug TF 2.16,closed,0,9,https://github.com/tensorflow/tensorflow/issues/83371,"The issue you're encountering with TensorFlow on the Apple M3 Max GPU seems to be related to the Metal backend for TensorFlow. From the logs, we can see the following key information: 1. **TensorFlow Metal Plugin**: The logs indicate that TensorFlow is trying to use the Metal API (`metal_plugin/src/device/metal_device.cc`) to run on the GPU, which is Apple's framework for GPU acceleration. 2. **Error Message**: The error message `malloc: *** error for object 0x315008234: pointer being freed was not allocated` suggests a memory allocation issue, possibly related to TensorFlow interacting with the Metal API. Here's a stepbystep approach to troubleshoot and resolve the issue:  1. **Verify Metal Backend Installation**    TensorFlow uses Metal (Apple's GPU API) for macOS devices with M1 and M2 chips, and now M3 chips as well. Make sure that you have the correct version of TensorFlow that supports the Metal backend. As of TensorFlow 2.16.2, Metal support should be in place, but compatibility with the M3 Max specifically may have issues that are not present in older M1 or M2 chips.    You can verify if TensorFlow is correctly using the Metal backend by running this code:        This will give you information about which devices TensorFlow is recognizing, including your Metalcompatible GPU. If the GPU is listed, it indicates that TensorFlow is correctly identifying and attempting to use the Metal GPU backend.  2. **Use CPU Instead of GPU**    If the issue is exclusive to the Metal GPU backend, you might want to disable the GPU usage temporarily to verify that the CPU setup works fine. You can do this by setting the environment variable `CUDA_VISIBLE_DEVICES` to an empty string before running your script:        This will force TensorFlow to use the CPU instead of the GPU, which should allow the program to run without crashing.   3. **Ensure Latest macOS & TensorFlow Version**    Since you're using the M3 Max, which is relatively new, it's possible that TensorFlow's Metal plugin hasn't been fully optimized for the new architecture. Here’s what you can do:     **Update macOS**: Make sure your macOS is up to date. Sometimes, macOS updates contain critical updates for the Metal API, which can affect TensorFlow’s ability to utilize the GPU correctly.     **Update TensorFlow**: Ensure you're using the latest nightly build or stable version of TensorFlow. Some Metal backend optimizations might not have been included in TensorFlow 2.16.2.      You can upgrade TensorFlow with pip:            Alternatively, you can try the latest nightly build, which may have better support for the M3 Max:            And if you want to ensure the latest support for Metal, you can install the TensorFlow nightly release for macOS:            After updating, try running the same code again to check if the issue is resolved.  4. **Metal Device Compatibility**    TensorFlow’s Metal support is relatively new and evolving, so there could be issues related to certain hardware configurations like the M3 Max. While the GPU is being recognized in the logs, there might be subtle bugs or configuration issues specific to this model.    You could try running a simple GPUaccelerated operation (such as matrix multiplication or convolution) to see if TensorFlow crashes in a minimal setup. This would help confirm if the issue is with TensorFlow’s Metal implementation or something specific in your code.    Here's a minimal test you can try:        This test will let you know if TensorFlow can successfully execute basic GPU operations on the M3 Max.  5. **Inspect TensorFlow Error Logs**    The error message you've provided indicates that TensorFlow is crashing due to a memory issue (pointer being freed that wasn't allocated). To gain more insight into what's causing this crash, you can inspect more detailed logs using a debugger like `lldb` or `gdb` on macOS:        When the crash happens, you can type `bt` (backtrace) in the `lldb` prompt to get more detailed debugging information. This can help pinpoint the exact location in TensorFlow where the error occurs.  6. **Try Disabling GPU Memory Growth**    If the issue is related to GPU memory allocation, you can try limiting GPU memory growth, which might help with memory fragmentation issues:        This ensures that TensorFlow doesn’t try to allocate all available memory upfront, and it may help avoid memory allocation crashes. If the problem persists, it might be worth reporting it to the [TensorFlow GitHub repository](https://github.com/tensorflow/tensorflow/issues) to see if it's a known issue or if any fixes are planned for the M3 Max.","Hi **** , Apologies for the delay, and thank you for raising your concern here. I tried running your code on an M3 Max with the GPU, and it is working fine for me. I have attached screenshots for your reference—please check them once, and let me know if I made any mistakes. !image (7) !image (6) !image (8) Thank you!","Hmm, okey thats weird. Many it is because I’m using Python 3.9.15 and not 3.9.6, as you do? I’ll have to troubleshoot a little more. I am using an Anaconda environment, so maybe that’s a problem too. Anyways, thanks for the update! Have a nice Christmas:) Fra: Venkat6871 ***@***.***> Dato: mandag, 23. desember 2024 kl. 11:08 Til: tensorflow/tensorflow ***@***.***> Kopi: Tobias Nøklegård Bjerke ***@***.***>, Mention ***@***.***> Emne: Re: [tensorflow/tensorflow] TF  Problems when trying to use GPU on M3 Max (Issue CC(TF  Problems when trying to use GPU on M3 Max)) Hi  , Apologies for the delay, and thank you for raising your concern here. I tried running your code on an M3 Max with the GPU, and it is working fine for me. I have attached screenshots for your reference—please check them once, and let me know if I made any mistakes. image.7.png (view on web) image.6.png (view on web) image.8.png (view on web) Thank you! — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you were mentioned.Message ID: ***@***.***>","I have the same exact issue but on M1 air, runs well on cpu but not on metal GPU","Also , the minimal code you provided works, the crash only seems to occur when you run an operation from keras","I tried to switch from an Anaconda environment to using Brew, and it actually solved the problem! Sent from Outlook for iOS ________________________________ From: Vincent Precious ***@***.***> Sent: Wednesday, December 25, 2024 10:29:13 PM To: tensorflow/tensorflow ***@***.***> Cc: Tobias Nøklegård Bjerke ***@***.***>; Mention ***@***.***> Subject: Re: [tensorflow/tensorflow] TF  Problems when trying to use GPU on M3 Max (Issue CC(TF  Problems when trying to use GPU on M3 Max)) Also , the minimal code you provided works, the crash only seems to occur when you run an operation from keras — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you were mentioned.Message ID: ***@***.***>","It appears that the crash occurs specifically when running operations from Keras on the M3 Max GPU, while basic GPU operations such as matrix multiplication work fine. Based on the error message `malloc: *** error for object 0x315008234: pointer being freed was not allocated`, this seems to be a memory allocation or management issue related to TensorFlow's Metal backend. Since the minimal test works but Keras operations cause the crash, here are a few additional steps you can take to troubleshoot and resolve the issue:   1. **Try a Simplified Keras Model** Sometimes, certain layers or configurations in Keras might trigger memory management issues. To narrow down the cause, try running a simple Keras model with the same Metal backend setup and see if the crash still occurs. This can help isolate whether it's a specific layer or operation causing the issue. Here's an example of a minimal Keras model:  This minimal Keras model can help determine if the crash is related to specific layers or operations in your full model.   2. **Disable Keras GPU Acceleration (For Debugging)** You can try forcing Keras to run on the CPU instead of the GPU temporarily to isolate whether the problem is specific to the Metal backend with Keras. Set the environment variable to disable GPU usage:  Alternatively, you can configure TensorFlow to explicitly use only the CPU within your script:  If the crash stops occurring when using the CPU, it strongly suggests the issue lies with GPU acceleration in Keras or the Metal backend.   3. **Disable Keras Layer Caching (Memory Fragmentation)** If the issue is related to memory fragmentation, disabling Keras's layer caching might help. You can do this by clearing the session before running your Keras operations:  Clearing the session may help resolve issues related to GPU memory fragmentation or improper memory allocation during model creation and training.   4. **Check TensorFlow's Memory Management Settings** The memory issue might be related to TensorFlow's GPU memory allocation strategy. You can adjust TensorFlow's memory growth settings to prevent it from allocating all available memory at once, which can help avoid crashes due to memory fragmentation.  This ensures that TensorFlow only allocates GPU memory as needed, which might reduce memoryrelated issues.   5. **Investigate the Crash Using Debugging Tools** If the problem persists and you're getting a crash related to memory allocation, use **lldb** or **gdb** to get more information about the exact point of failure. You can use the following command in the terminal to debug your script:  Once the crash happens, you can type `bt` in the `lldb` prompt to get a backtrace and pinpoint where the crash is occurring in TensorFlow's code. This may provide more insight into whether the issue lies with the TensorFlow Metal implementation, Keras, or the way resources are managed.   6. **Update TensorFlow and macOS** As the M3 Max is a relatively new chip, TensorFlow may not yet be fully optimized for it. Ensure that you're running the latest versions of both macOS and TensorFlow to benefit from the latest fixes and improvements, especially related to Metal support. To upgrade TensorFlow:  Or install the latest nightly version that may include better Metal support for newer hardware:  Additionally, make sure your macOS is up to date, as Apple frequently releases updates that improve compatibility with the Metal framework, which could help with TensorFlow's ability to utilize the GPU correctly.   7. **Report the Issue** If none of these steps resolve the issue, it might be worth reporting the bug to TensorFlow's GitHub repository. The issue could be a known problem with the Metal backend on the M3 Max or an issue that has not been fixed yet. You can report the issue on TensorFlow's GitHub [issue tracker](https://github.com/tensorflow/tensorflow/issues). Provide details about your hardware (M3 Max), macOS version, TensorFlow version, the error message, and any debugging information you've gathered (such as crash logs or backtraces).  By following these steps, you should be able to either resolve the issue or gather enough information to report the bug and potentially receive a fix.","Hi **** , Glad to see your issue is resolved. Please feel free to close this issue if everything is working as expected. Thank you!",Are you satisfied with the resolution of your issue? Yes No
yi,nathant-tommys,Tflite support for XNN Pack for x86 CPU/Intel GPU in Python package, Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tflite_runtime 2.11.0  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.4  Mobile device NA  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I can't find a way to use the XNN Pack to leverage a GPU in my python repo with tflite. Trying to find ways to help my models perform better on a ubuntu system.   Standalone code to reproduce the issue   Relevant log output _No response_,2024-12-19T22:54:55Z,stat:awaiting response type:support stale comp:lite comp:lite-xnnpack TF 2.11,closed,0,4,https://github.com/tensorflow/tensorflow/issues/83369,"Hi, tommys I apologize for the delayed response, I believe you've referred this official documentation and blog, If I'm not wrong It seems like TFLite does not support XNNPACK for x86 CPU/Intel GPU in Python and we've updated LiteRT (Formerly known as TFLite) delegates support so please refer these official documentations LiteRT Delegates and GPU delegates for LiteRT  To confirm, did you try to enable `XNNPACK` via Bazel build flags (recommended on desktop) when building TensorFlow Lite with Bazel, add `define tflite_with_xnnpack=true`, and the TensorFlow Lite interpreter will use `XNNPACK` engine by default ? The exact command depends on the target platform, e.g. for Android AAR you'd use  Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,4570235,How to run TFLite benchmark with QNN delegate in Android," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15.0  Custom code No  OS platform and distribution macOS 15.2  Mobile device One Plus 7 Pro, Android 11  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I have built/installed/run TFLite benchmark following this instruction for Android, and used TensorFlow 2.15.0 according to issue CC(benchmark_model no longer crosscompiles for Android from macOS). I test the benchmark via the following commands and the output result seems correct.   benchmark result.txt Now I want to run the benchmark with QNN delegate. I setup the on device environment and run a QNN delegate using an external delegate. The model being tested comes from tflite example image_classification.  I tested the benchmark using the following commands, but the result was a failure.     The full output is attached. benchmarkQNN result.txt I have also tested it on Android phones equipped with Snapdragon 855 and Gen 3 chips, and the results were the same. Could anyone tell me how to deal with thi",2024-12-19T12:40:25Z,stat:awaiting tensorflower type:bug comp:lite,open,0,4,https://github.com/tensorflow/tensorflow/issues/83344,"Hi,  Please take a look into this issue. Thank you.","Hi , can you give me a very detailed reproducible script for how you built the benchmark_model executable. I.e. I'm looking to answer these questions: 1. Did you use the dockerfile? 2. Which NDK/SDK versions did you use? 3. Which commit or branch of the source code did you use? 4. What is the OS you built it on (if not the dockerfile) 5. What was your actual build command? 6. Did you run the configure script? If so, what were your answers? Thanks for your help.","  1. Did you use the dockerfile? No. I followed this guide ""Set up build environment without Docker"". 2. Which NDK/SDK versions did you use? in the `.tf_configure.bazelrc` file:  3. Which commit or branch of the source code did you use? Release 2.15.1 4. What is the OS you built it on (if not the dockerfile) macOS Sequoia 15.2 5. What was your actual build command?  6. Did you run the configure script? If so, what were your answers? Yes. Here is the resulting config file:  tf_configure.bazelrc.txt 7. Detailed reproducible steps for how I built the benchmark_model executable. (1) Install the latest version of Bazel by Homebrew. `➜  tensorflow git:(v2.15.1) brew install bazel` (2) Run the `./configure` script. The resulting config file is attached. tf_configure.bazelrc.txt (3) Change Bazel version as required.  (4) Build. ","So I ended up building on linux instead and with tfnightly rather than 2.15 to see if it would make a difference, I mostly get the same result:  dziuba, can you please take a look? Thanks."
yi,copybara-service[bot],PR #19451: Setting xla_gpu_multi_streamed_windowed_einsum to true by default,PR CC(Feature Request: evaluate both train_data and test_data using tf.contrib.learn.Experiment?): Setting xla_gpu_multi_streamed_windowed_einsum to true by default Imported from GitHub PR https://github.com/openxla/xla/pull/19451 We are trying to deprecate xla_gpu_multi_streamed_windowed_einsum  since we always have better perf with it enabled. This is the first pr to enable it by default to test for stability. Copybara import of the project:  808a9cc0af8901d36a3c219bdf19f38323d01bf3 by Tj Xu : Turn xla_gpu_multi_streamed_windowed_einsum on by default  8221fc4481773f457f5e0235625be22f255fe75b by TJ Xu : Add an option to StreamAttributeAnnotator to skip annotating copystart and async DUS Don't annotate copystart and async DUS when the pass is run before remat  352c1c593b9dcd895f123dea4f7c38e44a787ae6 by TJ Xu : Remove the option to skip annotating copy start and inpect if the module has schedule  257ff6768b59fc7c47c04fa5faa524399f74c80e by TJ Xu : Address rollback by disabling a2a rewrite by default  d3bafebdc0961d61384a49616c29cb9bb6c59db9 by TJ Xu : reverted new flag changes Merging this change closes CC(Feature Request: evaluate both train_data and test_data using tf.contrib.learn.Experiment?) FUTURE,2024-12-18T22:57:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83303
yi,copybara-service[bot],[XLA:CPU] Consistently initialize the LLVM native target.,[XLA:CPU] Consistently initialize the LLVM native target. Fixes the following TSAN race: ,2024-12-18T22:31:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83302
rag,copybara-service[bot],[XLA:CPU] Acquire the LLVM options lock before calling RunHloPasses or RunBackend.,[XLA:CPU] Acquire the LLVM options lock before calling RunHloPasses or RunBackend. Both of these call into LLVM code that reads the compiler options. Fixes the following race: ,2024-12-18T22:29:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83301
rag,copybara-service[bot],[XLA:GPU] Fix output_offsets usage in RaggedAllToAll implementation.,"[XLA:GPU] Fix output_offsets usage in RaggedAllToAll implementation. The expected behaviour of `output_offsets` was not fully documented and the initial implementation and tests assumed that offsets are relative to the local output buffer. In reality the offsets are ""transposed"" and refer to the buffer on the target peer's memory. To use NCCL send and recv, we need to performance an additional alltoall and the `output_offsets` buffer to get the needed offset values.",2024-12-18T15:52:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83278
rag,copybara-service[bot],Clarify documentation for output_offsets operand of ragged_all_to_all.,Clarify documentation for output_offsets operand of ragged_all_to_all.,2024-12-18T13:11:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83270
rag,copybara-service[bot],"Public interface for legalizations and supporting types. Generic function for ""partitioning"" graph via a set of legalizations and backend hook (similar to executorch flow).","Public interface for legalizations and supporting types. Generic function for ""partitioning"" graph via a set of legalizations and backend hook (similar to executorch flow). Also expand the example plugins to include an implementation of these types and a plugin that leverages them.",2024-12-18T02:41:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83211
yi,Ivan21213232,Problen with tensorflow," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.10  Custom code Yes  OS platform and distribution Windows 11  x64  Mobile device _No response_  Python version Python 3.9.21  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version  cudatoolkit=11.2 cudnn=8.1.0  GPU model and memory 3070 ti  Current behavior? I was trying to follow this guide: https://www.tensorflow.org/install/pipwindowsnative, but it doesn't work at stage 7. I think that it's a problem with TensorFlow, but I can't understand what exactly is wrong.  Standalone code to reproduce the issue   Relevant log output ```shell ImportError                               Traceback (most recent call last) TypeError                                 Traceback (most recent call last) Cell In[7], line 1 > 1 import tensorflow as tf       2 from tensorflow.python.client import device_lib       4  Check if GPU is available File c:\Users\NEMIFIST\miniconda3\envs\tf\lib\sitepackages\tensorflow\__init__.py:37      34 import sys as _sys      35 import typing as _typing > 37 from tensorflow.python.tools import module_util as _module_util      38 from tensorflow",2024-12-17T13:59:56Z,type:build/install subtype:windows TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/83170, This is the error. What version of Python are you using?,>  >  > This is the error. What version of Python are you using? I am using Python 3.9.21,Are you satisfied with the resolution of your issue? Yes No,The errors message yesterday was different from the one after the edit today
few shot,Xephier102,"I'm NGL, I've been using Linux for 4 months now, the first month and a half of that was on a laptop not compatible with Linux(it was a nightmare). And still, this has to be the most convoluted, asinine, ass-backwards process for a simple configuration of a optimization backend, that I have ever seen in my life, much less the past 4 months.. I've spent the last 3-4 days just trying to make this thing function.. That's literally all I've done.","              I'm NGL, I've been using Linux for 4 months now, the first month and a half of that was on a laptop not compatible with Linux(it was a nightmare). And still, this has to be the most convoluted, asinine, assbackwards process for a simple configuration of a optimization backend, that I have ever seen in my life, much less the past 4 months.. I've spent the last 34 days just trying to make this thing function.. That's literally all I've done.  All I see everywhere I've gone 'bazelbin' But this damned thing doesn't create a bazelbin symlink or folder.. It creates bazelout and bazeltensorflow, but no bazelbin. I just has the epiphany just now to rename the bazelout to bazelbin, and adjusted this line to  `` bazelbin/tensorflow/tools/pip_package/build_pip_package.py /tmp/tensorflow_pkg`` added .py and also went into the folder with that .py file and made the damn thing executable with chmod +x build_pip_package.py cuz it still wouldn't run unless I did. Upon running it, I got the strangest damn thing I ever seen.  It ran just fine, sorta.. It froze my display and turned my cursor into a crosshair, and when I clicked, my screen unfroze. I had to click several times just to get to the end of that ",2024-12-16T22:57:47Z,,closed,0,4,https://github.com/tensorflow/tensorflow/issues/83100,Duplicate of CC(TensorFlow source code compilation error)  Please don't open an issue from a comment in another open issue as that could be seen as spamming.,"> Duplicate of CC(TensorFlow source code compilation error) >  > Please don't open an issue from a comment in another open issue as that could be seen as spamming. Apparently the bot removed the 'awaiting response' flag before anyone even responded, hence I made another. I don't type shit just to send information into the void for no reason. I got better things to do with my time.","Thank you for the context. The ""awaiting response"" label is for PRs where someone from triage team pinged to see if the issue is still relevant and gets removed as soon as there is some reply there (in the hope that that means that the issue is still relevant, so it should not be autoclosed). It is poorly named but it's also extremely hard to change it. Apologies if the closing message was a little bit too harsh and thank you for explaining the context","> Thank you for the context. The ""awaiting response"" label is for PRs where someone from triage team pinged to see if the issue is still relevant and gets removed as soon as there is some reply there (in the hope that that means that the issue is still relevant, so it should not be autoclosed). It is poorly named but it's also extremely hard to change it. >  > Apologies if the closing message was a little bit too harsh and thank you for explaining the context Thank you for your politeness and tact in this matter. It's refreshing, in scenarios like this, not to just get dumped on and ghosted/cancelled/banned. I know I don't always come across very kind in my own communications, but that's because I try my hardest to do things on my own. So, by the time I get around to asking for help, I've generally reached an agitated state, so that's what I put off. "
yi,shaoyuyoung,[XLA] `tf.keras.layers.LSTM` behaves differently on GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version nightly  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When executing LSTM on **XLA**, it fails. However, when executing it without XLA, it passes. The above failure is on GPU. If I use CPU as backend, with or without XLA both pass the check.  Standalone code to reproduce the issue   Relevant log output ",2024-12-16T13:57:22Z,stat:awaiting tensorflower type:bug comp:gpu comp:xla TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/83063,here is the gist,I was able to reproduce the issue on Colab using TensorFlow versions 2.18.0 and TFnightly. Please find the gist here for your reference. Thank you!
yi,copybara-service[bot],[XLA:TPU] Reuse calls to AliasAnalysis in CopyInsertion,[XLA:TPU] Reuse calls to AliasAnalysis in CopyInsertion,2024-12-16T13:15:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83060
yi,copybara-service[bot],Internal change only,Internal change only,2024-12-16T01:59:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83032
yi,braahkrayem,TensorFlow 2.10.0 cannot be installed on Python 2.7.5., Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.10.0  Custom code Yes  OS platform and distribution windows  Mobile device _No response_  Python version 2.7.5  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? i expected the code to run  Standalone code to reproduce the issue   Relevant log output _No response_,2024-12-15T11:23:18Z,stat:awaiting response type:build/install type:support stale TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/83013,", Currently Tensorflow v2.10 is not compatible with the python 2.7.5. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations. https://www.tensorflow.org/install/source_windowscpu Also Tensorflow v2.10 is a pretty older version, please upgrade to the latest Tensorflow version. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Don't delegate reshape if the output rank is unsupported by XNNPACK,Don't delegate reshape if the output rank is unsupported by XNNPACK Check the rank of the output tensor before trying to fit its shape into a XNN_MAX_TENSOR_DIMSlength array as doing so will crash on an otherwise completely valid model.,2024-12-14T00:37:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82958
yi,copybara-service[bot],[IFRT] Add option to compile IFRT IR atom programs using Sdy,[IFRT] Add option to compile IFRT IR atom programs using Sdy,2024-12-13T18:40:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82934
yi,YigitKaanBingol,Lite Rt Build Cross Compile Toolchain version incompatible," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version 8.3.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to build with cross compile for armv6 raspberry pi 0. In the instruction of the cross compile lite rt, given toolchain link is not compatible with armv6, its not even running simple  cross compiled hello_world.cpp, When we check the attributes of file, the arch of toolchain is armv7, but raspberry pi zero's arch is armv6.   Standalone code to reproduce the issue   Relevant log output ",2024-12-13T11:45:50Z,type:build/install comp:lite TF 2.18,closed,0,7,https://github.com/tensorflow/tensorflow/issues/82920,"Hi,   Thank you for bringing this issue to our attention. It appears that you are encountering compatibility problems with the crosscompilation toolchain for ARMv6 on the Raspberry Pi Zero. From your description it seems that the toolchain you are using `gccarm8.32019.03` is targeting ARMv7 architecture, which is indeed incompatible with the ARMv6 architecture of the Raspberry Pi Zero. This mismatch can lead to the `Illegal instruction` error you are experiencing when trying to run your compiled binary. I believe you're referring this official documentation so I'll have a look once from my end and will update you. Thank you for understanding and patience.","Hi , thank you for your time and response.  I followed every step from the official documentation as it explains. Unfortunately, I couldn't succeed.","Hi,  Please take a look into this issue. Thank you.","Hi , I was able to build successfully on a Debian linux system with the ARMv6 target:  Should work similarly on Ubuntu 22.04, can you tell me what exact commands you do and at which command you fail at? Thanks.","Thank you for your reply. The issue is not building, it does build. The problem is, the provided toolchain in official documentation in Lite Rt Cross Compile for Armv6 cannot run the basic hello_world example. All the commands and relative outputs given above. Thanks, Have a nice one.","Hi , we are transferring this issue to LiteRT. Please use that repo for LiteRT issues moving forward. Thanks.",Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],Add method to transfer ir storage from one to another.,Add method to transfer ir storage from one to another.,2024-12-13T04:10:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82880
yi,copybara-service[bot],Make max value in Range optional to allow for Unbounded Range calculations.,"Make max value in Range optional to allow for Unbounded Range calculations. Also, cache the intermediate calculated ranges when calling RecrusivelyIdentifyRange.",2024-12-13T01:59:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/82875
yi,copybara-service[bot],Make max value in Range optional to allow for Unbounded Range calculations.,"Make max value in Range optional to allow for Unbounded Range calculations. Also, cache the intermediate calculated ranges when calling RecrusivelyIdentifyRange.",2024-12-13T01:55:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82874
yi,copybara-service[bot],[XLA:GPU] Rely on LLVM parser rather than objcopy to load fatbin in tests ,[XLA:GPU] Rely on LLVM parser rather than objcopy to load fatbin in tests  To avoid relying on `objcopy` from toolchains,2024-12-12T16:41:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82843
yi,RicardoRobledo,Inconsistency with 'height_shift_range' and 'width_shift_range' parameters in ImageDataGenerator," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Windows  Mobile device _No response_  Python version 3.11.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The parameters are exchanged, 'height_shift_range' moves left and right and 'width_shift_range' moves up and bottom.   !image    !image  Standalone code to reproduce the issue   Relevant log output _No response_",2024-12-12T13:32:27Z,stat:awaiting response type:support stale comp:keras TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/82838,can you elaborate this issue more?,", In the Keras3.0(default for the tensorflow2.17/2.18) the ImageDataGenerator has been deprecated. **image_dataset_from_directory** is the replacement of the **ImageDataGenerator**, for augmentation you need to use preprocessing layer separately. For more details on API usage, visit https://keras.io/api/data_loading/image/image_dataset_from_directoryfunction. If you want to use the keras2.0, try to use the below code for the import and then try to test the code.  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,RabJon,"Annoying ""Ignoring Assert operator"" warning in console output"," Issue type Others  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.17.0 and 2.18.0  Custom code Yes  OS platform and distribution WSL2 Ubuntu 22.04.3 LTS and Native Ubuntu 24.04.1 LTS  Mobile device _No response_  Python version 3.9.20 and 3.11.5  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.3/8 and 12.5.1/9  GPU model and memory _No response_  Current behavior? Ever since I switched to Keras 3.x with Tensorflow 2.17+ as backend I am observing the following annoying warning which is printed several times during the `model.fit()` operation:  `W0000 00:00:1734000173.082846   18720 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert`. This warning is even printed when I set the environment variable `os.environ['TF_CPP_MIN_LOG_LEVEL'] = ""2""` to disable info and warning logs. The warning is annoying because it interrupts the progress bar which shows the progress during a training epoch and therefore makes the output less readable.  Standalone code to reproduce the issue   Relevant log output ",2024-12-12T11:06:40Z,stat:awaiting response stale type:others comp:keras,closed,0,7,https://github.com/tensorflow/tensorflow/issues/82832,", Could you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Also could you please try using tfnighty/kerasnightly and test the code.  **pip install tfnightly or pip install kerasnightly** Thank you!","> , Could you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster.  I created a simple gist using a mock model with which I was able to reproduce the problem. > Also could you please try using tfnighty/kerasnightly and test the code. > **pip install tfnightly or pip install kerasnightly** First I tried to install only kerasnightly but this was not enough, I immediately got the ""module not found"" error for Tensorflow. Then I also installed tfnightly and then all the imports worked.  Furthermore, also the problem mentioned in this issue did not appear anymore. 👍 Nevertheless, it would be nice to know where this warning comes from, why it can't be silenced and if it indicates some kind of mistake from my side? Thanks and kind regards,  Jonas",", Glad to know that the warnings disappear with tfnightly. Could you please try using **os.environ['TF_CPP_MIN_LOG_LEVEL'] = ""3""** which might help to disable the warnings.  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"> , Glad to know that the warnings disappear with tfnightly. Could you please try using **os.environ['TF_CPP_MIN_LOG_LEVEL'] = ""3""** which might help to disable the warnings. >  >  >  > Thank you! Sorry for my late reply. I now tried setting the log level to ""3"" but the warning still occurs!"
yi,copybara-service[bot],Internal change only,Internal change only,2024-12-12T00:18:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82791
yi,nazimisik,remove non-maintained tensorflow-io-gcs-filesystem dependency from pip_package,"this dependency has been causing a problem for more than a year without a solution, wheels are not built and it seems not very well maintained, if at all. tensorflow only depends on it for some optional purpose  as seen by it's addition PR it's only enabled with a certain variable: https://github.com/tensorflow/tensorflow/pull/51460 yet community has been trying to find a solution for the problems it has been causing without a direct fix, it might be beneficial to get rid of this dependency for extended belief on tensorflow's multiplatform support and reliability https://github.com/tensorflow/io/issues/2087 https://github.com/tensorflow/tensorflow/issues/56636 https://github.com/tensorflow/io/issues/1789 https://github.com/tensorflow/io/issues/2093 https://github.com/tensorflow/io/issues/2087 https://discuss.ai.google.dev/t/tensorflowiogcsfilesystemwithwindows/32191/3",2024-12-11T20:38:12Z,size:XS,closed,0,9,https://github.com/tensorflow/tensorflow/issues/82771,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",This would be okay if the depended library was correctly maintained but right now it brings lots of problems to real environments and we would at least like to see some plan to remediate this.,same since 2023 https://github.com/tensorflow/io/issues/1789issuecomment1491355793,"Would a less extreme option be considered acceptable . I would see the following decent options: * This dependency could be made an extra (as it is optional and nonexistence is handled),  * get os dependent version requirements, as the `<=0.31.0` for windows, this way the normal install from the docs with pip would not break, tutorials with gcs would continue to work, and UV / poetry would not choke on this and could resolve to the fallback version. If you deem either of these solutions worthwhile i would be happy to create a pr.",I think making it an extra would work better. The team is currently preparing for a new release of TF and they might reach a different decision too.,This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,rather keep it open,Not sure if toplay has seen this,"This should now be fixed from TF side, with tensorflowiogcsfilesystem an optional dependency."
llm,copybara-service[bot],Fix a bug in TFXlaCallModuleOpToStablehloPass regarding PlatformIndexArg handling,"Fix a bug in TFXlaCallModuleOpToStablehloPass regarding PlatformIndexArg handling The issue is that the ""PlatformIndexArg"" of a StableHLO module is not always a noop argument as was originally expected. When a StableHLO module contains function calls inside, this ""PlatformIndexArg"" will be propagated along the call graph. Therefore, unconditional removing this arg will remove a stillbeingused SSA value and trigger an assertion. The fix is that instead of removing the arg on the callee function side, we can add a dummy I32 operand on the caller side. After the inlining, this dummy operand will be dead code eliminated and produce the same result as before.",2024-12-11T19:34:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82767
yi,copybara-service[bot],Fix infinite loop in TopKSplitter,"Fix infinite loop in TopKSplitter TopK Splitter was not correctly handling the case where the split dimension (n) is equal to the split threshold. The splitted (new) dimension n is calculated as floor(n / split_threshold) which is equal to n, therefore no split is happening and since the pass is implemented as an HLO graph traversal we end up in an infinite loop that is trying to split the very same TopK instruction over and over again. The fix skips the rewrite for the cases where n == split_threshold. I also added a unit test which fails without the fix: http://sponge2/fc872deb7ecb4164b5282e7f6a4596b9 (fail)",2024-12-11T12:27:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82743
yi,copybara-service[bot],"timespan, xplane_visitor: Support operator<=.","timespan, xplane_visitor: Support operator<=. Timespan exposes operator< and operator==, while xplane_visitor only supports operator<. Annoyingly, C++20 is required to synthesize the relational operators from each other, so we simply implement operator<= on both (and operator== on XPlaneVisitor for completeness using the logical definition.",2024-12-11T00:09:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82705
rag,copybara-service[bot],Add a default error spec field to HloRunnerAgnosticTestBase.,Add a default error spec field to HloRunnerAgnosticTestBase. This error spec field is the same as the default used in HloTestBase. We provide this explicit default so that test writers avoid choosing an arbitrary spec that is too low.,2024-12-10T20:47:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82694
yi,JuanVargas,TF 2.18 fails to use GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version TF 2.18  Custom code No  OS platform and distribution Linux Ubuntu 22.04.4 LTS  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 12.6; CUDNN 9.0    GPU model and memory NVIDIA GeForce RTX 3060  12 GBs  Current behavior? TF fails to connect to the GPU card on some operations. Specifically, it fails with the code from the TF Basics tutorials, located at   https://www.tensorflow.org/guide/basics :  ""history = new_model.fit(x, y,                         epochs=100,                         batch_size=32,                         verbose=0) "" The error is  "" DNN library initialization failed. Look at the errors above for more details. \t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_24212]"" "" 2. The code below returns : print(""Python version: "", sys.version) print(""TensorFlow version: "", tf.__version__) print(tf.config.list_physical_devices('GPU')) print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) The result returned is: Python Version : 3.12.3 [GCC 13.2.0] Te",2024-12-10T14:47:43Z,type:bug,closed,0,3,https://github.com/tensorflow/tensorflow/issues/82669,I was able to run the code after I updated the path to LD_LIBRARY_PATH. Thereore I am closing this issue. ,Are you satisfied with the resolution of your issue? Yes No,I was able to run the code after I updated the path to LD_LIBRARY_PATH. Thereore I am closing this issue.
rag,copybara-service[bot],Add MHLO `mhlo.custom_call @ragged_all_to_all` -> HLO RaggedAllToAll pass,Add MHLO `mhlo.custom_call ` > HLO RaggedAllToAll pass The following mlir module  translates to ,2024-12-10T07:52:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82630
rag,NEIL-smtg,[MSVC] Compile error in external/llvm-project/llvm/lib/Support/TrieRawHashMap.cpp," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version latest commit  Custom code Yes  OS platform and distribution Windows 11  Mobile device Windows 11  Python version python 3.12  Bazel version bazel 6.5  GCC/compiler version MSVC 19.43.34708.97 (inner build)  CUDA/cuDNN version 12.4  GPU model and memory _No response_  Current behavior? Recently, as I build Tensorflow from source with internal build of MSVC, I encountered the following errors:  Repro steps: 1. git clone https://github.com/tensorflow/tensorflow.git 2. cd /d C:\gitP\tensorflow\tensorflow 3. pip3 install r tensorflow/tools/ci_build/release/requirements_common.txt upgrade 4. yes """" 2>nul | python ./configure.py 2>&1 5. mkdir build_amd64 & cd build_amd64 6. set BAZEL_VC=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC 7. set BAZEL_VC_FULL_VERSION=14.40.33807 8. bazel output_user_root C:\bazelTemp build jobs 16 config=opt local_ram_resources=16384  subcommands //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tf_nightly repo_env=TF_PYTHON_VERSION=3.12 Build log: tf.txt (Search for `Command ` in the log to view the complete list of commands executed.)  Standalone code t",2024-12-10T02:47:41Z,stat:awaiting response type:build/install stale subtype:windows,closed,0,4,https://github.com/tensorflow/tensorflow/issues/82574,"smtg, Could you please provide the exact sequence of commands / steps that you executed before running into the error? Also, please take a look at the tested build configurations and check if you have the correct dependencies installed. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal: add missing dependency on numpy,Internal: add missing dependency on numpy,2024-12-09T10:16:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82512
rag,copybara-service[bot],[XLA:GPU] Implement NcclRaggedAllToAllThunk.,"[XLA:GPU] Implement NcclRaggedAllToAllThunk. This change add proper implementation of RaggedAllToAll with Nccl. `RaggedAllToAllDecomposer` is now disabled, since it's not needed for integration. Test coverage is in `collective_ops_e2e_test.cc`. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20214 from shraiysh:ds_fusion 92e120354e04c8b754b5853c79662b96342b44f1",2024-12-06T19:02:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82416
yi,copybara-service[bot],Internal changes only,Internal changes only,2024-12-05T23:21:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82359
rag,copybara-service[bot],Correct the order of arguments in comment for RaggedAllToAll.,Correct the order of arguments in comment for RaggedAllToAll.,2024-12-05T23:08:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82358
yi,copybara-service[bot],Add method for HloRunnerAgnosticTestBase implementations to preprocess modules.,"Add method for HloRunnerAgnosticTestBase implementations to preprocess modules. This change introduces a new `PreprocessModuleForTestRunner` virtual method with an empty default implementation. The idea is to invoke this on on a module intended to be run with the designated test runner, applying a set of default module transforms for compatibility. The transforms are specific to the runner being used, so the test base class implementation is best suited to specifying these transforms (or not specifying them if not required).",2024-12-05T22:27:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82351
yi,koranten2,GPU delegate error for Flex models in Android C++ level," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16  Custom code Yes  OS platform and distribution Ubuntu 22.04  Mobile device Android  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hello. I am trying to use GPU delegate on Android C++ level for model that need Flex support. On runtime an error is returned on ModifyGraphWithDelegate. Please, tell if Flex + GPU is possible for Android C++? What corrections should I do to proceed?   Standalone code to reproduce the issue   Relevant log output ",2024-12-05T22:05:10Z,stat:awaiting response type:bug stale comp:lite TF 2.16,closed,0,7,https://github.com/tensorflow/tensorflow/issues/82349,"Thank you for reply! Do you mean that I need to initialize flex delegate directly, like auto* flex_delegate = tflite::FlexDelegate::Create();? As I could see from documentations only linking is needed for Android. And, please, tell your code is for android or running on android is impossible due to flex unsupporting?",I used similar code as yours previously on android but not successfully. So my question is it possible to use Flex + GPU on android at all/,"Hi,  I apologize for the delayed response, As far I know Flex operations (TensorFlow operations that are not natively supported by TensorFlow Lite) can't be run on the GPU delegate. Specifically, when you try to apply the GPU delegate and Flex delegate together the operations required by Flex are not supported by the GPU delegate leading to the error. The GPU delegate in TensorFlow Lite requires the operations to be offloaded to the GPU and it has specific kernels implemented for operations like Conv2D, DepthwiseConv2D, Add, etc. However, Flex operations may involve more complex or custom operations that either don’t have GPU kernels or require CPUbased execution. This results in the error you're facing when you attempt to apply both delegates. Please refer official documentation for supported operations of GPU delegates for LiteRT You'll need to work around this limitation either by splitting the model using CPU for Flex ops or preprocessing the model to remove Flex operations.The best course of action for your use case would be to separate operations that require Flex from those that can run on the GPU. Use the GPU delegate only for supported ops and run Flex operations on the CPU which can be slower.  If I have missed something here please let me know.  Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Added `GetAliveTasks` RPC to coordination service.,"Added `GetAliveTasks` RPC to coordination service. This CL introduces a new `GetAliveTasks` RPC to the multicontroller JAX coordination service. For a set of tasks `T`, `GetAliveTasks(T)` returns the subset `A` of healthy tasks in `T`. To avoid hosts from disagreeing on which tasks are healthy, `GetAliveTasks` has barrierlike semantics. In particular, `GetAliveTasks` returns `A` only after every task in `A` has called `GetAliveTasks(T)`. This API is intended to enable fault tolerant training using multicontroller JAX. Note that this CL introduces the `GetAliveTasks` API but it is not yet used. In future CLs, I will pipe the API through to JAX and expose a `jax.alive_devices` API. I'm separating the CLs to make the code easier to review.  Implementation Details `GetAliveTasks` has barrierlike semantics, and the coordination service already implements barriers. However, the implementation of `GetAliveTasks` differs from the barrier implementation in a couple of ways. First, the barrier API expects a unique barrier name. For example, you might call `barrier(""foo"")` to block on the barrier named ""foo"". For ergonomics, `GetAliveTasks` does not require barrier names. Instead, the set of tasks passed to `GetAl",2024-12-05T19:36:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82339
yi,copybara-service[bot],Internal change only,Internal change only,2024-12-05T19:17:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/82337
yi,atharva-kelkar,`TimeDistributed.call()` does not work correctly with `Masking`, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Mac OS Sequoia 15.1.1  Mobile device Macbook Pro  Python version 3.11.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to use a `Masking` layer to deal with variablesized sequences being fed to an LSTM layer. Using `TimeDistributed` with `Masking` gives the following error:  Tensorflow version: 2.16.1 Tensorflowmetal version: 1.1.0 Keras version: 3.0.0  Standalone code to reproduce the issue   Relevant log output ,2024-12-05T13:18:48Z,type:bug,closed,0,8,https://github.com/tensorflow/tensorflow/issues/82316,"The Masking layer in Keras is designed to work with 3D input tensors that have the shape (batch_size, timesteps, features). This is because the Masking layer is typically used in the context of sequence models like RNNs, LSTMs, and GRUs, which process data sequentially over time. As your input is missing at least one dimension this is expected.","My input to the masking layer is 3D, see the model summary below:  `None` is the placeholder for the batchsize dimension, 8 is the number of time steps, and 1 is the feature dimension.","I see, Let me check.",Try this :  ,!image,Thanks! Explicitly switching on eager execution in this line did the trick: `tf.config.run_functions_eagerly(True)`,Are you satisfied with the resolution of your issue? Yes No,Really felt so nice helping you out :) 
yi,copybara-service[bot],PR #19913: [ROCm] Do not use fast approximation for exp and log,PR CC(ValueError: Could not interpret optimizer identifier (tf.keras)): [ROCm] Do not use fast approximation for exp and log Imported from GitHub PR https://github.com/openxla/xla/pull/19913 Error started occurring from this commit for exp https://github.com/openxla/xla/commit/6e9eefeec077f49c2b22bfeee8da537ed8517b22 (originally introduced here https://github.com/openxla/xla/commit/9b19353a30821fb990afa456a2a5e7fae71e9afcdiff61ab646c9c3b8b0fc5ed1e9a62f535e9df5843adddd071250343f3bec48eacb6) and from this one https://github.com/openxla/xla/commit/53d533845f3c97d08a49e3d8589ec98c745ac09e for log. Trying to compile following MLIR code:  would result in:  Copybara import of the project:  616c10b5308cb827c593a89455fea4b772d6e870 by Milica Makevic : Do not use fast approximation for exp and log for ROCm  3fa4914f90458a0285deb8801c5689421f945fe4 by Milica Makevic : Add unit test for log and exp lowering on ROCm Merging this change closes CC(ValueError: Could not interpret optimizer identifier (tf.keras)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19913 from ROCm:fix_exp_log_lowering 3fa4914f90458a0285deb8801c5689421f945fe4,2024-12-05T09:04:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82285
yi,copybara-service[bot],"Add a helper function for ""applying"" plugin end 2 end.","Add a helper function for ""applying"" plugin end 2 end.",2024-12-05T03:39:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82269
llm,copybara-service[bot],Avoid using getCurrentVersion when re-serializing XlaCallModule ops,Avoid using getCurrentVersion when reserializing XlaCallModule ops,2024-12-05T01:04:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82263
llm,copybara-service[bot],"Change XlaCallModuleLoader to take module string by reference, avoid string copies when possible.","Change XlaCallModuleLoader to take module string by reference, avoid string copies when possible.",2024-12-05T00:59:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82261
yi,copybara-service[bot],Delete HloUnaryInstruction used in CreateUnary. Instead make result_accuracy a Rare field in HloInstruction and optionally set the field in CreateUnary.,Delete HloUnaryInstruction used in CreateUnary. Instead make result_accuracy a Rare field in HloInstruction and optionally set the field in CreateUnary.,2024-12-04T23:53:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82251
rag,copybara-service[bot],[XLA:GPU] Add a test for RaggedAllToAll that runs on 8 GPUs.,"[XLA:GPU] Add a test for RaggedAllToAll that runs on 8 GPUs. Writing correct input and expected data by hand is hard. We can't rely on default XLA random generator for input data, because all parameters (ragged data, sizes and slices) are dependent. Writing a test manually would require a lot of constant that are hard to read, change and debug. I decided to write a helper `CreateRandomTestData` that takes `input_sizes` as a parameter and computes all necessary sizes and strides, fills input with random data and calculates expected output. This way we can write concise tests that are also testing correctness.",2024-12-04T18:22:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82217
rag,samr301,Potential Remote Code Execution (RCE) Vulnerability in Custom Layers Handling," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version  2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device Kali Linux 2024.1  Python version Python 3.11.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA: 11.2   GPU model and memory NVIDIA GTX 1080  Current behavior? Currently, TensorFlow (primarily through Keras) allows users to create custom layers that can contain malicious code. When a model containing these custom layers is loaded or run, the malicious code in the layer can be executed without any restrictions or filtering. This opens up potential exploits because the executed code could potentially damage the system or steal sensitive data. For example, in this report, the saved model contains a MaliciousLayer layer that calls the os.system() system command to execute malicious code when the model is loaded, which could result in corruption or unauthorized data acquisition. I hope TensorFlow can provide protection that limits or filters the execution of malicious code in registered custom layers. For example, by blocking malicious functions such as os.system() or prohibiti",2024-12-04T16:08:09Z,type:others TF 2.18,open,0,4,https://github.com/tensorflow/tensorflow/issues/82214,", I tried to execute the mentioned code on both tensorflow v2.17, v2.18 and observed that the code was executed and provided the expected output. Kindly find the gist of it here. Thank you!",hi thanks for your response it seems i gave the wrong script i apologize this is the correct script  after running this script we can see the malicious code executed  This script more clearly shows the potential risk because it runs a malicious system command (os.system) that can be executed when the model is loaded and run. This can be dangerous if someone loads the model without understanding the inserted code.,I tried to execute the above mentioned code on both tensorflow v2.18 and observed that the code was executed and provided the expected output. Kindly find the gist of it here.  Also this issue is more related to Keras. For further information please raise the request in keras repo. Thank you!,will there be any improvement in this issue?
yi,chaudhariraj,Unable to Install TensorFlow on Raspberry Pi Zero W (ARMv6)," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.10.1  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm trying to install TensorFlow on a Raspberry Pi Zero W with ARMv6 architecture with 32bit bookworm. However, the installation fails, and I cannot find a version of TensorFlow compatible with ARMv6 architecture. I would appreciate guidance or an official release for ARMv6 support. raspberrypiwh:~ $ uname m armv6l raspberrypiwh:~ $ cat /etc/osrelease PRETTY_NAME=""Raspbian GNU/Linux 12 (bookworm)"" NAME=""Raspbian GNU/Linux"" VERSION_ID=""12"" VERSION=""12 (bookworm)"" VERSION_CODENAME=bookworm ID=raspbian ID_LIKE=debian HOME_URL=""http://www.raspbian.org/"" SUPPORT_URL=""http://www.raspbian.org/RaspbianForums"" BUG_REPORT_URL=""http://www.raspbian.org/RaspbianBugs""  Standalone code to reproduce the issue   Relevant log output _No response_",2024-12-04T07:18:44Z,stat:awaiting response type:bug type:build/install stale TF 2.10,closed,0,7,https://github.com/tensorflow/tensorflow/issues/82146,"Hi,  I apologize for the delayed response, I see communitybuilt TensorFlow binaries which provides an official wheel for `Python3.5` on `Raspberry Pi 13 and Pi Zero`. if you're looking for an official wheel for Python3.5 on Raspberry Pi Zero please download it from here and install it on your Raspberry Pi Zero W (ARMv6) You can also build from source for the Raspberry Pi Zero (ARMv6) so we recommend crosscompiling the TensorFlow Raspbian package. Crosscompilation is using a different platform to build the package than deploy to. Instead of using the Raspberry Pi's limited RAM and comparatively slow processor, it's easier to build TensorFlow on a more powerful host machine running Linux, macOS, or Windows. Please refer this documentation. If I have missed something here please let me know. Thank you for your cooperation and patience.","Hi,   I recently purchased a Raspberry Pi 4 Model B (2018), installed a 64bit OS (Bookworm, aarch64 architecture), and the default Python version is 3.11. I installed TensorFlow version 2.17 in a virtual environment (venv). However, I'm encountering issues accessing the Pi Camera using the picamera2 library. The error states that the libcamera module cannot be found, even though I have installed picamera2 and its dependencies, including libcameraapps. When I attempt to install TensorFlow directly in the root environment (outside of the virtual environment), I encounter errors. How can I resolve these issues to successfully access the Pi Camera while using TensorFlow and also how i install tensor flow specific version like 2.10 in root or is possible to install tensor flow with apt command? Error: raspberrypi4b:~ $ pip install tensorflow2.18.0cp311cp311manylinux_2_17_aarch64.manylinux2014_aarch64.whl error: externallymanagedenvironment × This environment is externally managed ╰─> To install Python packages systemwide, try apt install     python3xyz, where xyz is the package you are trying to     install.     If you wish to install a nonDebianpackaged Python package,     create a virtual environment using python3 m venv path/to/venv.     Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make     sure you have python3full installed.     For more information visit http://rptl.io/venv note: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing breaksystempackages. hint: See PEP 668 for the detailed specification. OS Details: raspberrypi4b:~ $ uname m aarch64 raspberrypi4b:~ $ uname r 6.6.62+rptrpiv8 raspberrypi4b:~ $  cat /etc/osrelease PRETTY_NAME=""Debian GNU/Linux 12 (bookworm)"" NAME=""Debian GNU/Linux"" VERSION_ID=""12"" VERSION=""12 (bookworm)"" VERSION_CODENAME=bookworm ID=debian HOME_URL=""https://www.debian.org/"" SUPPORT_URL=""https://www.debian.org/support"" BUG_REPORT_URL=""https://bugs.debian.org/"" raspberrypi4b:~ $ Wheel: link  https://www.tensorflow.org/install/pip https://storage.googleapis.com/tensorflow/versions/2.18.0/tensorflow2.18.0cp311cp311manylinux_2_17_aarch64.manylinux2014_aarch64.whl","Hi,   I apologize for the delayed response, could you please follow the below instructions before installing the TensorFlow and see is it resolving your issue or not ?  After trying above instructions if issue still persists please let us know with updated error log to investigate this issue further from our end.  Thank you for your cooperation and patience.","Hi,   To confirm, have you got chance to try above provided instructions ? If so please let us know is it working as expected or not ?  Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
llm,copybara-service[bot],Set the XlaCallModule's StableHLO payload version during deserialization so that it can be used in re-serialization.,Set the XlaCallModule's StableHLO payload version during deserialization so that it can be used in reserialization.,2024-12-04T01:10:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82132
yi,copybara-service[bot],Internal change only,Internal change only,2024-12-04T00:25:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82127
rag,copybara-service[bot],Rename `NewHloTestBase` to `HloRunnerAgnosticTestBase`.,Rename `NewHloTestBase` to `HloRunnerAgnosticTestBase`. The name `NewHloTestBase` is potentially confusing beacuse it does not reflect the use & purpose of this class. `HloRunnerAgnosticTestBase` is a lot clearer.,2024-12-03T23:56:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/82125
yi,copybara-service[bot],Update `py_import` macros for the ability to unpack additional wheels in the same folder as the main wheel.,"Update `py_import` macros for the ability to unpack additional wheels in the same folder as the main wheel. Usage example: provide NVIDIA wheel dependencies for ML wheels that have rpaths pointing to NVIDIA folders. When a user executes `pip install tensorflow[and_cuda]`, NVIDIA wheels are installed together with Tensorflow wheel. To reproduce this behavior in hermetic Python approach, we need to define `py_import` as follows (provided NVIDIA dependencies are defined in `requirements.in` and requirements lock files):         py_import(             name = ""tf_py_import"",             wheel = "":wheel"",             deps = [                 ""//:pkg"",                 ""//:pkg"",                 ""//:pkg"",                 ""//:pkg"",                 ""//:pkg"",                 ""//:pkg"",                 ""//:pkg"",                 ""//:pkg"",                 ""//:pkg"",                 ""//:pkg"",                 ""//:pkg"",                 ""//:pkg"",                 ""//:pkg"",             ],             wheel_deps = [                 ""//:whl"",                 ""//:whl"",                 ""//:whl"",                 ""//:whl"",                 ""//:whl"",                 ""//:whl"",                 ""//:whl"",                 ""//:whl"",       ",2024-12-03T18:53:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82104
rag,copybara-service[bot],[XLA:GPU] Add RaggedAllToAllDecomposer to the GPU compilation pipeline.,[XLA:GPU] Add RaggedAllToAllDecomposer to the GPU compilation pipeline. Also add a small e2e test. More e2e tests are coming later.,2024-12-03T09:32:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/82047
yi,kayladoann,Add GPU configuration options to TensorFlow Go bindings," Introduced a new GPUOptions struct in TensorFlow's Go bindings to allow users to configure GPUspecific settings, such as enabling memory growth (AllowGrowth) and specifying a GPU allocator type (AllocatorType).  Added unit tests to ensure the GPUOptions behave as expected and integrate seamlessly with existing TensorFlow functionality.  Updated the session creation API to accept and apply GPUOptions during initialization, aligning the Go API with TensorFlow’s Python API.",2024-12-03T00:03:40Z,comp:gpu size:M,open,0,7,https://github.com/tensorflow/tensorflow/issues/81989,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",Please sign the CLA and then tag me again to review.,I've signed the CLA,"It still shows as unsigned,  I cannot debug more for the next ~2 weeks due to not being home, sorry for the delay.",", Could you please sign CLA, as it is still unsigned?  Thank you !","So the issue with the CLA is that you signed with your Github account but the email associated to the commit (`...​24945207.wireless.oregonstate.edu`) is not associated to your GitHub account, . If you can attach them, I think that would work. Alternatively, you can amend your current commit to change the email address (`git commit amend ...` followed by `git push f`) I might need to click one button on our side if the process is not automated, so please ping me once you fix this.","Hi , Please refer to the comment  to resolve any issue with the CLA signing process."
yi,yuchen001,[MacOS] TensorFlow Metal Plugin Symbol Not Found Error with TensorFlow 2.18.0," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code No  OS platform and distribution macOS 15.1.1  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When trying to import TensorFlow with Metal plugin support on macOS using a virtual environment, an error occurs while loading the Metal plugin. Specifically, TensorFlow fails to locate a required symbol in the shared library libmetal_plugin.dylib. The error indicates a mismatch between the TensorFlow core library and the Metal plugin.  Standalone code to reproduce the issue   Relevant log output ",2024-12-02T23:43:46Z,stat:awaiting response type:build/install stale subtype:macOS TF 2.18,closed,5,6,https://github.com/tensorflow/tensorflow/issues/81987,"Hi **** , Apologies for the delay, and thank you for raising your concern. Instead of using the command `pip install tensorflow tensorflowmetal` , please use the specific compatible version as documented: `pip install tensorflow==2.14.0 tensorflowmetal`. Additionally, the default NumPy version being installed is 2.02, but the compatible version is 1.24.3. You can install it with:`pip install numpy==1.24.3`. I have tested this setup, and it is detecting everything as we expected.  Thank you!",I have the same issue. Have to use the previous version of tensorflow. Fixed by installing tensorflow==2.17.0 and tensorflowmetal==1.1.0,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"This finally fixed my set up thankyou! Here's the versions which now can run on a Mac M1 Silicon, and successfully reproduce the Image Classification Notebook here: https://www.tensorflow.org/tutorials/images/classification  > (myenv) maayaramacbookpro:myenv maayara$ python c ""import tensorflow as tf; print('TensorFlow version:', tf.__version__); print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))"" > TensorFlow version: 2.14.0 > Num GPUs Available: 1 > (myenv) maayaramacbookpro:myenv maayara$  > ``` >  > Thank you!"
rag,copybara-service[bot],[XLA] Support splitting ragged all-to-all into async start and done.,[XLA] Support splitting ragged alltoall into async start and done.,2024-12-02T21:53:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/81977
rag,copybara-service[bot],Update clone with new operands to handle ragged all-to-all.,Update clone with new operands to handle ragged alltoall.,2024-12-02T21:48:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/81976
rag,copybara-service[bot],Rename `NewHloTestBase` to `HloRunnerAgnosticTestBase`.,Rename `NewHloTestBase` to `HloRunnerAgnosticTestBase`. The name `NewHloTestBase` is potentially confusing beacuse it does not reflect the use & purpose of this class. `HloRunnerAgnosticTestBase` is a lot clearer.,2024-12-02T18:42:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/81963
yi,copybara-service[bot],Un-deprecate `HloTestBase`.,"Undeprecate `HloTestBase`. In CC(Introduce NewHloTestBase.) we previously deprecated HloTestBase in favor of `NewHloTestBase`. `NewHloTestBase` is not a dropin replacement for `HloTestBase` and requires testwriters to adapt their tests to not relying on a specific runner implementation. There are still a few problems with switching everything to `HloRunnerPjRt` right away, so we cannot yet offer an equivalent shim that can serve as a convenient dropin replacement. For the time being, we have decided to therefore undeprecate the class to avoid further confusion. We will migrate all users of `HloTestBase` in due course.",2024-12-02T17:30:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/81948
yi,liamaltarac,Strange results for gradient tape : Getting positive gradients for negative response," Issue type Others  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.11.0  Custom code Yes  OS platform and distribution Windows  Mobile device _No response_  Python version 3.7.16  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hello, I'm working with some gradient based interpretability method (based on the GradCam code from Keras  ) , and I'm running into a result that seems inconsistent with what would expect from backpropagation.   I am working with a pertrained VGG16 on imagenet, and I am interested in find the most relevent filters for a given class. I start by forward propagating an image through the network, and then from the relevant bin, I find the gradients to the layer in question (just like they do in the Keras tutorial). Then, from the pooled gradients (`pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))`), I find the topK highest/most pertinent filters.  From this experiment, I run into 2 strange results.  1.  For almost any image I pass through (even completely different classes), the network almost always seems to be placing the most i",2024-12-02T16:44:57Z,stat:awaiting response stale type:others comp:keras TF 2.11,closed,0,6,https://github.com/tensorflow/tensorflow/issues/81944,"Hi **** , Please post this issue on kerasteam/keras repo. as this issue is more related to keras Thank you!","Hi  , Thanks for your reply. I reposted it. .","Hi **** , Could you please close this issue, as it is already being tracked in another repository? Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],[XLA:GPU] Add RaggedAllToAllDecomposer pass.,"[XLA:GPU] Add RaggedAllToAllDecomposer pass. The pass rewrites `raggedalltoall` as a regular `alltoall`. This rewrite is not intended to be the production implementation of `raggedalltoall`, because it uses much more memory than necessary. Adding this pass had the following goals:   * Unblock endtoend integration of `raggedalltoall` in XLA:GPU.   * Serve as a reference implementation.   * Help write endtoend tests Once we have a proper implementation with NCCL, this pass should be removed. Integration into the GPU compilation pipeline will follow.",2024-12-02T16:13:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/81943
rag,copybara-service[bot],CHLO defns for a ragged dot that permits ragged batch and contraction.,CHLO defns for a ragged dot that permits ragged batch and contraction.,2024-12-02T14:53:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/81934
rag,copybara-service[bot],MHLO defns for a ragged dot that permits ragged batch and contraction.,MHLO defns for a ragged dot that permits ragged batch and contraction.,2024-12-02T14:51:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/81933
rag,copybara-service[bot],PR #19655: [ROCm] Make MLIR Math dialect lowering more deterministic,PR CC(Fix setuptools version to avoid a bad release.): [ROCm] Make MLIR Math dialect lowering more deterministic Imported from GitHub PR https://github.com/openxla/xla/pull/19655 First apply patterns from GpuToROCDLConversionPatterns then do the cleanup with MathToLLVMConversionPatterns Copybara import of the project:  377d5a1f1a624196eef3a241c65c388ba886e5ef by Dragan Mladjenovic : [ROCm] Make MLIR Math dialect lowering more deterministic First apply patterns from GpuToROCDLConversionPatterns then do the cleanup with MathToLLVMConversionPatterns Merging this change closes CC(Fix setuptools version to avoid a bad release.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19655 from ROCm:ci_mlir_math_fix 377d5a1f1a624196eef3a241c65c388ba886e5ef,2024-12-02T13:41:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/81925
yi,copybara-service[bot],PR #19451: Setting xla_gpu_multi_streamed_windowed_einsum to true by default,PR CC(Feature Request: evaluate both train_data and test_data using tf.contrib.learn.Experiment?): Setting xla_gpu_multi_streamed_windowed_einsum to true by default Imported from GitHub PR https://github.com/openxla/xla/pull/19451 We are trying to deprecate xla_gpu_multi_streamed_windowed_einsum  since we always have better perf with it enabled. This is the first pr to enable it by default to test for stability. Copybara import of the project:  f8e88e6b22ec4702081a4949b60fd669bead8c33 by Tj Xu : Turn xla_gpu_multi_streamed_windowed_einsum on by default  fbd2f328954a2a66be7a8cbd1e4570908b3424b7 by TJ Xu : Add an option to StreamAttributeAnnotator to skip annotating copystart and async DUS Don't annotate copystart and async DUS when the pass is run before remat  43f9dbbc55dd1eb84c3b4452e5274b2571cf78cd by TJ Xu : Remove the option to skip annotating copy start and inpect if the module has schedule Merging this change closes CC(Feature Request: evaluate both train_data and test_data using tf.contrib.learn.Experiment?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19451 from Tixxx:tixxx/remove_multi_stream_flag 43f9dbbc55dd1eb84c3b4452e5274b2571cf78cd,2024-12-02T13:14:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/81922
yi,d0tb0t71,"error: defining a type within 'offsetof' is a Clang extension [-Werror,-Wgnu-offsetof-extensions]","I'm new to this.   Trying to build the Selective Framework for iOS using the below command   bash tensorflow/lite/ios/build_frameworks.sh \   input_models=model1.tflite,model2.tflite \   target_archs=x86_64,armv7,arm64   But getting the error     ERROR: /private/var/tmp/_bazel_tonmoy/d1033cf820cfe9e8569d67cf059cb6df/external/upb/BUILD:57:11: Compiling upb/upb.c [for tool] failed: (Exit 1): wrapped_clang failed: error executing command (from target //:upb) external/local_config_cc/wrapped_clang 'D_FORTIFY_SOURCE=1' fstackprotector fcolordiagnostics Wall Wthreadsafety Wselfassign fnoomitframepointer g0 O2 DNDEBUG 'DNS_BLOCK_ASSERTIONS=1' ... (remaining 32 arguments skipped) external/upb/upb/upb.c:192:10: error: defining a type within 'offsetof' is a Clang extension [Werror,Wgnuoffsetofextensions]   192                                            ^ 1 error generated. Error in child process '/usr/bin/xcrun'. 1 Target //tensorflow/lite/ios/tmp:TensorFlowLiteSelectTfOps_framework failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 218.346s, Critical Path: 17.92s INFO: 4162 processes: 747 internal, 3415 local. FAILED: Build did NOT complete successfully",2024-12-01T09:28:05Z,stat:awaiting response type:build/install stale comp:lite TF 2.18,closed,0,12,https://github.com/tensorflow/tensorflow/issues/81724,No response and the issue still happening. ,", Apologies. Could you please provide the exact sequence of commands / steps that you executed before running into the problem. Also provide the tensorflow version and environment details which helps to debug the issue. Thank you!",1. bazel clean async 2. export HERMETIC_PYTHON_VERSION=3.12 3. bash tensorflow/lite/ios/build_frameworks.sh \ input_models=model.tflite \ target_archs=arm64 I'm on the **master** branch,Any update regarding this issue?,"Hi,   I apologize for the delayed response, if you're using bazel then could you please try by adding this flag in your bazel command `copt=Wnognuoffsetofextensions` and see is it resolving your issue or not ? If the issue persists please help us with complete steps including package versions or if you're following any official documentation to replicate the same behavior on our end. Thank you for your cooperation and patience.","Thank you  for your response.  I am new to Tensorflow and all this stuff. Actually, my purpose is to reduce the framework size for iOS. When I use this SpecialTF framework using cocoa pod the app size becomes 200MB+. So now I want to build a reducedsize framework with only the ops used in my model. So can you please give a hint where to use the provided command? I am using **Bazel 6.5.0**.","Hi,  Alright, please use the below command with `copt=Wnognuoffsetofextensions` flag if it does not work with below command then please modify the `build_frameworks.sh` script manually and You can do this by editing the script and finding the `bazel build` command within the script then adding `copt=Wnognuoffsetofextensions ` flag like this :`bazel build //tensorflow/lite/ios:TensorFlowLiteSelectTfOps_framework copt=Wnognuoffsetofextensions` may solve your issue so please give it try and let us know is it working as expected or not ?  Thank you for your cooperation and patience.",Which branch should be used to build the framework?  ,"Hi,   I apologize for the delayed response, It is recommended to build from master branch after cloning the TensorFlow Github repo by default it will use `master` branch. Thank you",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[XLA:CPU] Use `absl::call_once` to lazily initialize kernel and comparator functions,"[XLA:CPU] Use `absl::call_once` to lazily initialize kernel and comparator functions This simplifies the code a little and should otherwise be performance neutral. It is also a bit easier to understand whether the code is safe or not. Previously, sort_thunk does:  However, two racing threads might both observe that `less_than` is nullptr which results in both of them trying to acquire the mutex and populate both `less_than_` and `less_than_ptr_`. The problem is that another thread may witness that `less_than_` is nonnull without acquiring the mutex and thus may have its hands on objects in bad states. While it is simple enough to recheck `less_than_ptr_` after the mutex is acquired, it is even simpler to just use `call_once`. This has the added benefit of only using an acquire atomic operation internal to the `call_once` implementation vs the `seq_cst` load on `less_than_ptr_`.",2024-11-30T03:12:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/81358
gpt,armanschwarz,"Unspecific error message ""None values not supported"" when no labels are provided in dataset"," Issue type Documentation Feature Request  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? If I write some tf/keras code that fails to include a label in the dataset, the resulting error message is:  `ValueError: None values not supported.` The backtrace leads me through `optree/ops.py`. It's not clear from the message what the `None` values are referring to, nor does the backtrace give me any useful information (as far as I can tell) about the source of the error. I posted this issue here: https://stackoverflow.com/q/79236884/1613983 If the error message could instead make reference to a missing labels set, null `y_true`, etc. that would make it much easier to figure out what's going on. Somewhat ironically, I ended up solving it after asking GPT o1 about the problem, and it got the answer in one go.  Standalone code to reproduce the issue   Relevant log output ",2024-11-29T22:36:45Z,stat:awaiting tensorflower type:feature comp:data,open,0,3,https://github.com/tensorflow/tensorflow/issues/81356,"I'm not sure if error messages fall under ""documentation"", so I might have labelled this incorrectly.",", As the error mentioned, you are trying to feed the None values which were not supported. Could you try to provide the labels and try to execute the mentioned code. THank you!",Just commenting to remove the awaiting response label.  I don't think you've read/understood the question.
yi,sicong-li-arm,Request: Add int8 support to Unsorted_Segment_X ops,"(I also raised this request in the new LiteRT repo:  duplicate of https://github.com/googleaiedge/LiteRT/issues/190) **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 24.04.1, but targeting Android  TensorFlow installed from (source or binary): Binary  TensorFlow version (or github SHA if from source): 2.18.0 **Provide the text output from tflite_convert**: Used a custom converter that performs post training full quantization using representative datasets.  Hi! As part of my investigation of deploying a graph neural network (GNN), built from the TFGNN  library on mobile (Android), I found that the operator Unsorted_Segment_Sum doesn't support int8. This prevented us from taking advantage of a full quantization of the GNN which relies on the unsorted_segment_x operators for core message passing steps. This resulted in quantized models less performant than the nonquantized ones because of the extra dequantizationquantization layers. I'd like to request the addition of this data type, since it is a very important operator used by the TFGNN library itself, and we may see more demand for quantization of GNNs in the future. Thanks in advance!",2024-11-29T16:21:33Z,stat:awaiting tensorflower type:feature comp:lite TFLiteConverter TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/81348,"Hi, liarm  I apologize for the delayed response, I see at the moment we do support `int32` and `float32` data types but not `int8` data type so this issue will be considered as feature request and will update you  https://github.com/tensorflow/tensorflow/blob/38f4882ba0a24bcdc2f1ce52be62e1b19242dbee/tensorflow/lite/kernels/unsorted_segment.ccL205 Thank you for your cooperation and patience.","Hi,   Please take a look into this issue. Thank you.","Please follow progress in LiteRT, thanks."
rag,Tina1L,T," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. L",2024-11-28T14:48:54Z,stat:awaiting response stale comp:lite TFLiteConverter,closed,0,3,https://github.com/tensorflow/tensorflow/issues/81213,"Hi,  I apologize for the delayed response, if possible could you please explain and elaborate more about your issue it seems like you just created empty issue template with **TensorFlow Lite Converter Issue** option ? Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Please don't spam
yi,Kiritor729,"在 Windows 环境中从源代码构建时，Action failed to execute: java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(""C:\Users\mi\scoop\apps\msys2\current\usr\bin\bash.exe""……绯荤粺鎵句笉鍒版寚瀹氱殑鏂囦欢銆?", Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version TF 2.6  Custom code No  OS platform and distribution Windows  Mobile device _No response_  Python version 3.9  Bazel version 3.7.2  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? 应该正常编译，但是error且fail了，具体见日志输出  Standalone code to reproduce the issue   Relevant log output ,2024-11-28T01:38:57Z,stat:awaiting response type:build/install stale subtype:windows 2.6.0,closed,0,5,https://github.com/tensorflow/tensorflow/issues/80989,自问自答下，问题不在“绯荤粺鎵句笉鍒版寚瀹氱殑鏂囦欢銆?”（也就是找不到指定文件附近），而是出在第一句“C:/Users/mi/scoop/apps/msys2/current/usr/bin/bash.exe”，这个bash找不到，是msys的问题，编译文件里默认msys是用scoop安装，但是从官方文档跳转的msys是安装包安装，导致编译时调用msys找不到bash，即使按步骤设置了bash的path路径。,", Tensorflow version 2.6 is not actively supported. Hence, kindly update to the latest stable version 2.18.0 and let us know if you are facing the same issue. Also could you try **bazel clean expunge** followed by bazel sync.   https://www.tensorflow.org/install/source_windowscpu Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Replace dependency on pre-built wheels with `py_import` dependency.,Replace dependency on prebuilt wheels with `py_import` dependency.,2024-11-27T20:33:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/80980
yi,phpYj,"Very serious! Using this method will definitely result in memory leaks, I hope you can provide support"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code Yes  OS platform and distribution ubuntu 2.2 or mac m1  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I have tried various methods, but the memory is definitely leaking, it seems that the release of memory cannot keep up. Through the logs, it can be found that there is periodic memory recycling, but with the increase of time, there is still a clear upward trend  Standalone code to reproduce the issue   Relevant log output ",2024-11-27T06:27:34Z,stat:awaiting tensorflower comp:runtime type:performance TF 2.18,open,0,9,https://github.com/tensorflow/tensorflow/issues/80895,"Hi, I see this issue is already assigned, but I would love to contribute and learn from this task. Could you please assign it to me as well or let me know if I can assist in any way.",", Please raise the PR for contributing to the Tensorflow. Also refer to the official doc for the reference. https://www.tensorflow.org/community/contribute https://github.com/tensorflow/tensorflow/pulls","   Feedback from Keras, this issue has existed for a long time and has been confirmed to originate from TF.  If it is resolved, please let me know. Thank you very much https://github.com/kerasteam/keras/issues/20245issuecomment2342594184 https://github.com/kerasteam/tfkeras/issues/286 https://github.com/kerasteam/keras/issues/20552issuecomment2503944388","Hello , so it's ok for me to jump on this issue",. Can we work together on this?,> . Can we work together on this? Yes. Sure. ," Could u try adding clear memory and garbage collector and let me know what happens? not quite sure if it will work, but something to try.",">  >  > Could u try adding clear memory and garbage collector and let me know what happens? not quite sure if it will work, but something to try. This was suggested in one of the links shared above. I did try it and it didn't work. ","   tf.keras.backend.clear_session()     gc.collect() This method cannot completely solve the problem. As time goes on, the memory still increases, but it only slightly alleviates the issue"
agent,LongZE666,Heap-buffer-overflow in `SparseMatrixSparseCholesky`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.17.0 tf2.16.1  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When batch_index is scalar, `SparseMatrixSparseCholesky` triggered a crash.  Standalone code to reproduce the issue   Relevant log output ",2024-11-26T12:42:50Z,stat:awaiting tensorflower type:bug comp:ops 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/80847,", I was able to reproduce the issue on the both tensorflow v2.17 and tfnightly. Kindly find the gist of it here. Also could you please try to contribute with the PR for the changes required for the mentioned issue? Thank you!"
yi,NEIL-smtg,[MSVC] Compiler error in external/nsync/platform/c++11/platform.h," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version latest commit  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.12  Bazel version 6.5  GCC/compiler version MSVC 19.43.34617.95  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I work on Microsoft Visual C++ testing, where we regularly build popular opensource projects, including yours, with development builds of our compiler and libraries to detect and prevent shipping regressions that would affect you. This also allows us to provide advance notice of breaking changes, which is the case here. Recently this commit https://github.com/microsoft/STL/pull/5105 is revealing an issue in nysnc. Compiler error with this STL change:  I have created an upstream PR (https://github.com/google/nsync/pull/25), which is currently awaiting their reponse. In the meantime, would it possible for you to add this patch (include_chrono.patch) to https://github.com/tensorflow/tensorflow/tree/master/third_party  and update the following line in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace2.bzlL400:",2024-11-26T09:15:36Z,stat:awaiting tensorflower type:bug comp:core,closed,0,2,https://github.com/tensorflow/tensorflow/issues/80836,"this issue is no longer appears in the latest commit, closing this issue.",Are you satisfied with the resolution of your issue? Yes No
rag,gaikwadrahul8,Testing this Issue for LiteRT Migration Task," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. L",2024-11-26T05:52:43Z,comp:lite TFLiteConverter,closed,0,1,https://github.com/tensorflow/tensorflow/issues/80822,"Hi, Team I am closing this issue now because I created this issue to test transfer option for issue migration task of TFLite issue from TesorFlow core repo to respective LiteRT and aiedgetorch but unfortunately it did not work because **""You can only transfer issues between repositories owned by the same user or organization account. A private repository issue cannot be transferred to a public repository.""** here is reference Thank you."
rag,copybara-service[bot],Limit the number of stragglers we log to avoid `RESOURCE_EXHAUSTED` errors in the RPC layer from sending overly verbose errors.,Limit the number of stragglers we log to avoid `RESOURCE_EXHAUSTED` errors in the RPC layer from sending overly verbose errors.,2024-11-26T05:40:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80820
yi,copybara-service[bot],Internal change only.,Internal change only.,2024-11-26T04:08:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80814
yi,LongZE666,Aborted (core dumped) in `RaggedBincount`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.17.0 tf2.16.1  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When the value of size is close to the maximum value for the dtype, an integer overflow error will occur in following multiplication calculations.  Standalone code to reproduce the issue   Relevant log output ",2024-11-26T03:18:31Z,stat:awaiting tensorflower type:bug comp:ops 2.17,open,0,3,https://github.com/tensorflow/tensorflow/issues/80812,"Hi **** , Thank you for raising your concern here. Is there a specific reason for using such a long value like 6522107765268123892? This value is extremely large, and allocating memory for a tensor of this size would require at least 52 exabytes, which far exceeds current hardware capabilities. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly versions and faced the same issue. However, as an alternative, I used smaller values, and it worked fine for me. I hope this helps you. Please find the gist here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"  Thanks for your reply, I use this long value to test if there is an error in the operator. And this error seems to be a removal error caused by calculation. The specific location is in RaggedBincountOp::Compute "
rag,copybara-service[bot],[XLA] Alias ragged all-to-all output with operand 1.,[XLA] Alias ragged alltoall output with operand 1.,2024-11-26T00:08:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80801
chat,copybara-service[bot],[XLA:GPU] Make SortRewriter VLOG level 2 less chatty.,"[XLA:GPU] Make SortRewriter VLOG level 2 less chatty. Stop printing the full HLO module before and after at VLOG level 2. Do this at level 3 instead, which is in line with what other HLO passes do.",2024-11-25T12:30:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80754
yi,phpYj,This method creates a model with a 100% memory leak loop using model. fit()," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code Yes  OS platform and distribution ubuntu 2.2 or mac m1  Mobile device ubuntu 2.2 or mac m1  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I have tried various methods, but the memory is definitely leaking, it seems that the release of memory cannot keep up. Through the logs, it can be found that there is periodic memory recycling, but with the increase of time, there is still a clear upward trend  Standalone code to reproduce the issue   Relevant log output ",2024-11-25T12:12:55Z,stat:awaiting tensorflower type:bug comp:keras TF 2.18,open,1,7,https://github.com/tensorflow/tensorflow/issues/80753,"The main issue is the creation method of the model and the code related to the fit loop, which leads to memory leakage. Other codes have little significance。 I am running on the CPU",Are you satisfied with the resolution of your issue? Yes No,"I would like to correct that the version number is 2.18, but I accidentally closed the issue","Hi **** , Thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions and encountered the same issue. Please find the gist here for your reference. Please post this issue on kerasteam/keras repo. as this issue is more related to keras Thank you!","Hi   Someone has raised a similar issue before, as if it was TF's mistake I've identified the source of the leak in this case, and it's a TF bug. Basically when we create TF functions their resources never get garbage collected. Even if we keep a reference to the functions created and we manually delete them, the memory isn't freed. JAX and PyTorch do not appear to have any such problems. I can run the script with no memory accumulation with JAX + XLA compilation for instance. https://github.com/kerasteam/keras/issues/20245issuecomment2342594184 https://github.com/kerasteam/tfkeras/issues/286 https://github.com/kerasteam/keras/issues/20552issuecomment2503944388 Answer from the Keras team, this error belongs to TF","same issue, and only occur on GPU (not TPU)","Dear Tensorflow users and devs.  I experienced CPU memory leaks during the development of my ML ops framework. I spent over 50 hours searching for a solution So i will share my research and final solution.  How I Identified the RAM Leak :  Initialy i worked under a Tensorflow NVIDIA at https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow release 24.10  In order to find the limit of my hyperparameter search space, i did some stress test to find the limit of my hardware (RTX 4500 ada generation). And i found a RAM leak during keras.fit() loop.  Sorry i don't have screenshoot but the RAM curve is the following model.fit() (with jit_compile=True)  Phase 1 > compiling training loop > Sharp drop of RAM (this is the XLA compiling effect) Phase 2 > Inverse Log curve > the first fit steps use more ram than last step and i see a stabilizing effect where the RAM leak become linear) During my research i expected the following RAM leaks suspect list > (Nvidia Driver / Cuda, XLA compiler, tf data pipeline, Tensorflow version, keras version, Grappler optimiser config) i spend most of the time suspecting XLA, thus i tried many config flags > thus XLA suspect removed from list i spend a lot of time trying different container config with (Nvidia hub and tensorflow official docker hub),  turn out any NVIDIA driver from 535 to 565 and tf 2.16 ,2.17 ,2.8 still have the issue.  > NVIDIA driver / cuda suspect removed from list i converted the model back to keras 2 API  > still have the issue i run the tf data pipeline without keras.fit > no memory leak after reading many (maybe all) TF RAM leaks issues, i feel agree with   so i imagined why internal tensorflow dev still having memory leak issue tickers on github since 2.10 and no fix for many users. and i remembered that tcmalloc is a internal google tools. So i decided to try. Thus under tensorflow/tensorflow/2.17.0gpu container i installed TCmalloc  BASH : aptget install googleperftools find /usr name ""libtcmalloc_minimal.so* (for know the path)  and run my .py script with this env variable LD_PRELOAD:/usr/lib/x86_64linuxgnu/libtcmalloc_minimal.so.4 (maybe it will be another path for you) CPU RAM dataleak removed !!! model.fit() (with jit_compile=True)  Phase 1 > Compiling > Sharp drop of RAM (this is the XLA compiling effect) Phase 2 > Training :constant RAM usage with no fluctuation I tried again without TCmalloc, and i had RAM leaks again I tried again with TCmalloc and i had NO RAM leaks Hope my finding will help some of you my nvidia smi under the container > Driver Version: 535.183.01   CUDA Version: 12.3  "
rag,therohanchoudhary,Android Tensorflow tflite error version 2.17. Didn't find op for builtin opcode 'FULLY_CONNECTED' version '12'. An older version of this builtin might be supported,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installed from (source or binary):  TensorFlow version (or github SHA if from source): 2.17 **Provide the text output from tflite_convert** **Standalone code to reproduce the issue**  ``(IOException::class)     fun loadModelFile(         context: Context,         modelFileName: String = ""constant_output_model.tflite""     ): MappedByteBuffer {         val fileDescriptor = context.assets.openFd(modelFileName)         val inputStream = FileInputStream(fileDescriptor.fileDescriptor)         val fileChannel = inputStream.channel         val startOffset = fileDescriptor.startOffset         val declareLength = fileDescriptor.declaredLength         return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declareLength)     }`` This is the interpreter API initialization ` InterpreterApi.create(             TfLiteUtils.loadModelFile(this, ""voice_detection_2_17_0_new_ops.tflite""),             InterpreterApi.Options().apply {                 numThreads = 1             }         ) ` This is the dummy model architecture with same shape and output. `input_shape = (None, 39, 63, 1)   Input shape (excluding batch dimen",2024-11-25T09:18:18Z,stat:awaiting response stale comp:lite 2.17,closed,0,3,https://github.com/tensorflow/tensorflow/issues/80736,"This looks like it might be due to a mismatch between the version of TensorFlow used to build your model, and the version of TensorFlow Lite used to execute it.  In particular, are you sure that you are using version 2.17 of TF Lite? That version of TF Lite supports versions 1 to 12 (inclusive) of the 'FULLY_CONNECTED'  op https://github.com/tensorflow/tensorflow/blob/ad6d8cc177d0c868982e39e0823d0efbfb95f04c/tensorflow/lite/core/kernels/register.ccL85 so I would not expect to see that message unless either (a) you are using an older version of TF Lite or (b) you are using an op resolver that doesn't support the 'FULLY_CONNECTED' op at all.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,LongZE666,heap-buffer-overflow in `ConjugateTranspose`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.17.0 tf2.16.1  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Under specific inputs,`ConjugateTranspose` triggered a crash.  Standalone code to reproduce the issue   Relevant log output ",2024-11-24T09:39:44Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/80682,", I request you to take a look at this https://github.com/tensorflow/tensorflow/issues/69213 and https://github.com/tensorflow/tensorflow/issues/63033 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!","Okay, I will pay attention to these issues, thank you",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],Increase test coverage in `CollectiveParamResolverDistributed`,Increase test coverage in `CollectiveParamResolverDistributed`,2024-11-23T04:27:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/80615
yi,copybara-service[bot],PR #18407: Fix xla-mlir failures on Windows,"PR CC(Updating the sed command for docker parameterized build.): Fix xlamlir failures on Windows Imported from GitHub PR https://github.com/openxla/xla/pull/18407 This PR aims to enable the XLA/mlir/tool test cases on the Windows Platform. Error: //xla/mlir/tools/mlir_bisect/... tests were failing on the Windows platform with the errors shown below: Errors Error 1.Error with llvm::seq no matching function for call to 'seq' for (auto i : llvm::seq(0ul, sizeof...(T))) { Solution: change to llvm::seq(0, sizeof...(T)) By explicitly specifying the type (unsigned long) in llvm::seq, the compiler now clearly understands the type of the sequence. Error 2. Missing dlfcn.h: Location: xla/mlir/tools/mlir_interpreter/dialects/func.: 'dlfcn.h' file not found Solution: include 'windows.h' for Windows platform Error 3. Use of Undeclared Identifiers sym and RTLD_DEFAULT: Location: xla/mlir/tools/mlir_interpreter/dialects/func. 'sym' sym = dlsym(RTLD_DEFAULT, callee.getSymName().str().c_str()); ^ use of undeclared identifier 'RTLD_DEFAULT' Solution: On Windows, the approach to obtaining a symbol's address differs from Unixbased systems. GetModuleHandle function retrieves a handle to the specified module (DLL) that is lo",2024-11-21T12:47:23Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/80489,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
yi,copybara-service[bot],Add all the quantized models to test_models constants and try to unifying the naming.,Add all the quantized models to test_models constants and try to unifying the naming.,2024-11-21T08:36:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80474
rag,copybara-service[bot],[PjRt-IFRT] Add optional global device mapping support to PjRt-IFRT,"[PjRtIFRT] Add optional global device mapping support to PjRtIFRT This change adds a capability to PjRtIFRT to optionally take a userspecified global device mapping. PjRtIFRT will materialize global (multihost) view for IFRT devices even if the wrapped PjRt client may only provide a local (singlehost) view with addressable devices only. This capability is achieved by forging `xla::GlobalTopologyProto` from the device information in the provided `pjrt_client` and the global device mapping information in `xla::ifrt::PjRtClient::CreateOptions::global_device_mapping`. The current iteration of the global view (when created from `global_device_mapping` and not obtained from `pjrt_client`) enables replicated execution of XLA computations. SPMD executions are not yet supported. This global device mapping feature is available when not using topology exchange via a keyvalue store. The global topology information from local topology exchange via the keyvalue store is used asis by PjRtIFRT as before at the moment. A side effect is that PjRtIFRT devices have a slightly different format for `ToString()` and `DebugString()` if device ID is remapped. The new device ID will be noted as `""[PjRtIFRTDeviceId=XX]""` in the e",2024-11-21T02:14:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80467
rag,copybara-service[bot],[xla:cpu] Add a test for nanort executable with temp storage,[xla:cpu] Add a test for nanort executable with temp storage,2024-11-21T00:26:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80407
yi,copybara-service[bot],[IfOp] Call `std::vector::reserve()` on the `args` vector before copying input tensors to it.,[IfOp] Call `std::vector::reserve()` on the `args` vector before copying input tensors to it.,2024-11-21T00:21:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80404
llm,copybara-service[bot],Move `jax` visibility inside `internal_visibility` call,Move `jax` visibility inside `internal_visibility` call,2024-11-20T22:00:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80390
rag,copybara-service[bot],PR #19484: [ROCm] Fix //xla/tests:complex_unary_op_test and //xla/service/gpu/te…,PR CC(Question about using of TFRecordDataset): [ROCm] Fix //xla/tests:complex_unary_op_test and //xla/service/gpu/te… Imported from GitHub PR https://github.com/openxla/xla/pull/19484 …sts:gpu_input_fusible_slice_test Copybara import of the project:  0d307384bff386d5182f89ae5a5422f8ca1a1290 by Dragan Mladjenovic : [ROCm] Fix //xla/tests:complex_unary_op_test and //xla/service/gpu/tests:gpu_input_fusible_slice_test Merging this change closes CC(Question about using of TFRecordDataset) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19484 from ROCm:mlir_tests_new 0d307384bff386d5182f89ae5a5422f8ca1a1290,2024-11-20T13:20:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80350
rag,x0w3n,Aborted (core dumped) in `tf.raw_ops.RaggedTensorToVariantGradient`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.3 LTS (x86_64)  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? With a specific input and the gpu available, tf.raw_ops.RaggedTensorToVariantGradient triggers a crash. It can be reproduced on tfnightly when the gpu is available.  Standalone code to reproduce the issue   Relevant log output ",2024-11-20T07:05:56Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/80332,"Hi **** , Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions and faced the same issue. However, I tried an alternative approach, and it worked fine for me. I hope this will be helpful to you. Please find the gist here for your reference. Thank you!",Thank you for the suggestion.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,ssdv1,tflite int8 export is twice as large as saved_model.pb," 1. System information Windows 11  2. Code import tensorflow as tf saved_model_dir = ""C:/Users/s/Downloads/best (1)_saved_model"" converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) converter.optimizations = [tf.lite.Optimize.DEFAULT] def representative_dataset_gen():     for _ in range(100):          yield [tf.random.normal([1, 640, 640, 3])] converter.representative_dataset = representative_dataset_gen converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] converter.inference_input_type = tf.int8 converter.inference_output_type = tf.int8 tflite_model = converter.convert() tflite_model_path = ""model_int8.tflite"" with open(tflite_model_path, ""wb"") as f:     f.write(tflite_model) !Screenshot 20241119 231208",2024-11-20T04:12:29Z,stat:awaiting response stale comp:lite TFLiteConverter,closed,0,9,https://github.com/tensorflow/tensorflow/issues/80319,"Hi,   Thank you for bringing this issue to our attention, if possible could you please help us with your saved_model along with Google colab notebook which you used to create saved_model also to replicate the similar behavior from our end ?  Thank you for your cooperation and patience.","> Hi,  Thank you for bringing this issue to our attention, if possible could you please help us with your saved_model along with Google colab notebook which you used to create saved_model also to replicate the similar behavior from our end ? >  > Thank you for your cooperation and patience. saved_model pytorch file used to create saved_model `pip install ultralytics  from ultralytics import yolo model=YOLO('best (1).pt')  model.export(format='tflite') ` this code will export .pt file to saved_model","Hi,   I do not have access to your shared files please provide me access. Thank you","> Hi,  I do not have access to your shared files please provide me access. Thank you sorry for that. Access set for anyone with link","Hi,   I apologize for the delayed response, I was trying to replicate the similar behavior from my end while converting **saved_model** to **TFLite** I'm getting this error message` ValueError: Only support at least one signature key.` w.r.t signature so I tried most common default signature but still it's not converting **saved_model** to **TFLite** so please refer this gistfile If I'm doing something wrong please let me know that will be greatly appreciated to replicate same behavior which you mentioned in issue template. Thank you for your cooperation and patience.","> Hi,  I apologize for the delayed response, I was trying to replicate the similar behavior from my end while converting **saved_model** to **TFLite** I'm getting this error message` ValueError: Only support at least one signature key.` w.r.t signature so I tried most common default signature but still it's not converting **saved_model** to **TFLite** so please refer this gistfile If I'm doing something wrong please let me know that will be greatly appreciated to replicate same behavior which you mentioned in issue template. >  > Thank you for your cooperation and patience. I dont know about that error. I ran locally not on colab and it worked. Maybe some issue with env. Try doing it locally on your computer. My python version is 3.8 so maybe try with that. Did you try using my saved model directly. Also check this check last few comments","Hi,  I apologize for the delayed response, I tried downgrading the TensorFlow version in Google colab but still I'm getting the same error message `ValueError: Only support at least one signature key.` here is gistfile for reference if possible could you please help me with your complete Jupyter notebook along with ultralytics and TensorFlow versions so I'll try on Windows system ?  If you're using older version of TensorFlow please give it try with latest TensorFlow version 2.18.0 and see is it resolving your issue or not ? Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,copybara-service[bot],[tsl:concurrency] Keep AsyncValueRef a part of CountDownAsyncValueRef State,[tsl:concurrency] Keep AsyncValueRef a part of CountDownAsyncValueRef State By keeping AsyncValueRef as a part of the State we avoid one extra reference counting operation when copying CountDownAsyncValue (and we expect to copy it `cnt` times). name                     old cpu/op   new cpu/op   delta BM_CountDownSuccess/8    95.8ns ± 4%  81.7ns ± 1%  14.64%  (p=0.000 n=40+35) BM_CountDownSuccess/16    142ns ± 1%   127ns ± 1%  10.05%  (p=0.000 n=37+38) BM_CountDownSuccess/32    229ns ± 2%   216ns ± 1%   5.56%  (p=0.000 n=40+38) BM_CountDownError/4       165ns ± 1%   152ns ± 2%   7.65%  (p=0.000 n=39+40) BM_CountDownError/8       238ns ± 2%   225ns ± 1%   5.65%  (p=0.000 n=40+38) BM_CountDownError/16      388ns ± 2%   369ns ± 2%   4.77%  (p=0.000 n=40+36) BM_CountDownError/32      684ns ± 1%   666ns ± 1%   2.50%  (p=0.000 n=38+38),2024-11-20T03:22:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80317
yi,x0w3n,Aborted (core dumped) in `tf.raw_ops.Cholesky`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.3 LTS (x86_64)  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When the shape of the input argument is empty and the gpu is available, tf.raw_ops.Cholesky triggers a crash. It can be reproduced on tfnightly when the gpu is available.  Standalone code to reproduce the issue   Relevant log output ",2024-11-20T01:59:39Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/80312,"Hi **** , Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions and faced the same issue. As an alternative, I used a Hermitian matrix instead of a scalar matrix, and it worked fine. I hope this helps you. Please find the gist here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only,Internal change only,2024-11-20T00:56:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80309
yi,copybara-service[bot],Remove `py_import.bzl` and `verify_manylinux_compliance` from export after https://github.com/openxla/xla/commit/c6f26d4efa1ac071a1b39c9a81dae6214da22be3#diff-58867c8fba0bb0782bc1a4ef7e3f3c2727706be99debc019c256640ff720b3d7,Remove `py_import.bzl` and `verify_manylinux_compliance` from export after https://github.com/openxla/xla/commit/c6f26d4efa1ac071a1b39c9a81dae6214da22be3diff58867c8fba0bb0782bc1a4ef7e3f3c2727706be99debc019c256640ff720b3d7,2024-11-20T00:29:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/80307
yi,similo01,can not import tensorflow, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.8  Custom code Yes  OS platform and distribution windows  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? jupyter lab does not import tensorflow i brought python down from version 3.13 to 3.11  Standalone code to reproduce the issue   Relevant log output ,2024-11-19T22:50:37Z,type:support TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/80293,", Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:  https://github.com/tensorflow/tensorflow/issues/61887 Thank you!",I started with Python 3.13 and what i assumed is the latest version of Tensorflow. I realised when it was not getting installed that there is not Tensorflow for Python 3.13. I un installed and went down to Pythorn 3.11 and even python 3.9 and 3.8 and it would not installed completely though the commands seem to work in the command prompt but they would not work on the Jupyterlab. I went to the tensorrflow website and followed instructions there with no success,Duplicate of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.). Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.,Are you satisfied with the resolution of your issue? Yes No
yi,stefan-it,tf.gather and workarouds are very slow on TPU," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Ubuntu 22.04.2 LTS  Mobile device TPU VM  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi all, I am trying to solve a performance bug that occurs during pretraining/finetuning a DeBERTa model on TPUs. In a nutshell, the `tf.gather` implemention is very slow on TPUs (but very fast on GPUs). I am now looking for a way, that boosts up the `tf.one_hot` + `tf.einsum` trick, which would have massive impact in pretraining DeBERTa models to widespread it's usage. There are some issues that have also reported this issue: * https://github.com/huggingface/transformers/issues/18239 * https://github.com/kerasteam/kerashub/issues/606 But with no solution yet. Any help is highly appreciated!  Standalone code to reproduce the issue Here's a code snippet with an example:  Taken from https://github.com/WissamAntoun/CamemBERTa/blob/1a1fb4a658729dfac2bb93842d88261132803ec3/modeling_tf_deberta_v2.pyL734L750  Relevant log output",2024-11-19T16:43:31Z,stat:awaiting tensorflower comp:tpus type:performance TF 2.16,open,0,6,https://github.com/tensorflow/tensorflow/issues/80253,"it, Could you please share a reproducible code that supports your statement so that it helps to debug the issue in an effective way and also please try to use the latest tensorflow v2.17 or v2.18 and provide the update. Thank you!","Hi  , here's some basic code to reproduce the bahaviour:  On my RTX 3090 with TF 2.16.1 (on GPU) this code takes ~5.16 seconds. Without GPUsupport 9.74 seconds. On a **v432** TPU VM Pod (TF 2.16.1) I ran this example:  And it took 13.86 seconds.","it How long does it take for `gathered = tf.gather(x, indices, batch_dims=2)` or for the `one_hot+einsum` variant?","Thanks  , I forgot to run experiments with version 2:  On my GPU it runs in 1.71 seconds. Same code on the TPU VM Pod using:  takes 522 seconds!",Version 3 of the benchmark just uses `tf.gather`: GPU:  Takes 1.67 seconds. TPU code:  Takes 3.41 seconds.,Overview: 
yi,kayladoann,Improve Documentation for TensorFlow Setup on Windows," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16  Custom code Yes  OS platform and distribution Windows 10/11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The TensorFlow installation guide for Windows does not include detailed troubleshooting guidance for:  Resolving PATH environment variable conflicts.  Managing mismatches in CUDA and cuDNN versions during GPU setup.  Debugging pip installation errors in virtual environments. This creates challenges for new users trying to install TensorFlow on Windows, especially when dealing with GPU setup.  Standalone code to reproduce the issue   Relevant log output _No response_",2024-11-19T07:19:44Z,type:docs-bug stat:awaiting response type:bug stale,closed,0,4,https://github.com/tensorflow/tensorflow/issues/80237,", If you are specifically asking for the Windows, TensorFlow 2.10 was the last TensorFlow release that supported GPU on nativeWindows. Starting with TensorFlow 2.11, you will need to install TensorFlow in WSL2, or install tensorflowcpu and, optionally, try the TensorFlowDirectMLPlugin. Instead you can use WSL2  https://www.tensorflow.org/install/pipwindowswsl2_1 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],Update clone with new operands to handle ragged all-to-all.,Update clone with new operands to handle ragged alltoall.,2024-11-19T01:12:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80228
yi,copybara-service[bot],PR #19451: Setting xla_gpu_multi_streamed_windowed_einsum to true by default,PR CC(Feature Request: evaluate both train_data and test_data using tf.contrib.learn.Experiment?): Setting xla_gpu_multi_streamed_windowed_einsum to true by default Imported from GitHub PR https://github.com/openxla/xla/pull/19451 We are trying to deprecate xla_gpu_multi_streamed_windowed_einsum  since we always have better perf with it enabled. This is the first pr to enable it by default to test for stability. Copybara import of the project:  7facf3c8a1401a1dcd578f3d192cc4ff631052ce by Tj Xu : Turn xla_gpu_multi_streamed_windowed_einsum on by default Merging this change closes CC(Feature Request: evaluate both train_data and test_data using tf.contrib.learn.Experiment?) Reverts c3fd63e779156e3d9eb8fbeac1aebea59c16c564 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19451 from Tixxx:tixxx/remove_multi_stream_flag 7facf3c8a1401a1dcd578f3d192cc4ff631052ce,2024-11-18T23:45:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/80224
yi,Abhay-2412,Build Failure with nvcc: TensorFlow 2.15.0 with CUDA 12.2 on Jetson Orin Nano ," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15.0  Custom code No  OS platform and distribution Ubantu 22.04 for jetson orin nano  Mobile device Jetson Linux 36.3  Python version 3.10  Bazel version 6.1.0  GCC/compiler version 11.4.0  CUDA/cuDNN version cuda 12.2.140, cuDNN 8.9.4.25  GPU model and memory NVIDIA Jetson orin nano devkit  Current behavior? I'm attempting to build TensorFlow 2.15.0 with CUDA support using Bazel for . However, the build process fails with multiple errors.  Key observations: Warnings about the cuda config being expanded multiple times. The issue persists even after trying alternative compilers like Clang 16 and integrating NVCC. Below are logs with nvcc compiler  ERRORLOGS.txt  Standalone code to reproduce the issue   Relevant log output _No response_",2024-11-18T23:27:19Z,stat:awaiting response type:build/install type:support stale subtype: ubuntu/linux TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/80223,"2412, Thanks for reporting the issue. Looks like this is a known issue while building the TensorFlow v2.15 with aarch64. Could you please try to refer to the issue for the updates on the same. Also please check the version compatibility w.r.t CUDA and cuDNN https://github.com/tensorflow/tensorflow/issues/62490 https://github.com/tensorflow/tensorflow/issues/59924 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only,Internal change only,2024-11-18T17:23:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/80195
yi,krishnarajk,tflite-model-maker TFlite model accurracy ,"Hello, I was trying to install tflitemodelmaker, and it fails in the installation. But I have converted my BertQa model to tflite using TFlite.Converter. How do I calculate, the model accuracy (F1) score in this case. In the examples gives, it uses evaluate from the tflitemodelmaker, which I  cannot install. Thank you",2024-11-17T22:53:22Z,stat:awaiting response type:support stale comp:lite TFLiteConverter TFLiteModelMaker,closed,0,4,https://github.com/tensorflow/tensorflow/issues/80179,"Hi,   Thank you for bringing this issue to our attention, I was trying to run TensorFlow Lite Model Maker with a custom dataset and it got installed successfully please refer this gistfile  in that I used `Python version 3.9.x and numpy==1.23.4 version ` and it's known issue to us w.r.t `TensorFlow Lite Model Maker` and our relevant team is working on it meanwhile you can give it try with previously mentioned version by creating fresh virtual environment to just avoid package/libraries compatibilities issues If issue still persists after trying above workaround with TensorFlow Lite Model Maker then I would suggest you to please use MediaPipe Model Maker and please refer this tutorial   Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],IFRT proxy: hacky flag for faster `is_deleted()`.,IFRT proxy: hacky flag for faster `is_deleted()`. Setting the environment variable `IFRT_PROXY_ARRAY_IS_DELETED_HACK=enabled` in the OSScompiled version of the IFRT proxy will make all `Array::IsDeleted()` calls to immediately return false. This is not a generally safe optimization and trades off API correctness. This acts as an interim solution for users while proper solutions are being debated or implemented.,2024-11-17T19:55:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80178
transformer,noteandcode,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: ," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11,8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I cant run my streamlit file that contains pipline element.  Standalone code to reproduce the issue   Relevant log output ",2024-11-17T14:43:50Z,stat:awaiting response type:bug stale TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/80177,"Hi **** , The error you are facing, `DLL load failed`, is likely due to incompatible versions. Could you please check all the compatible versions and let us know which OS platform you are using? Additionally, please try to fill out all the required templates as it will help us troubleshoot more effectively. And here i am providing documentation for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only,Internal change only,2024-11-16T01:01:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80158
rag,mattbahr,Fix integer overflow in range,"Fixes CC(slient overflow occurs in tf.range leading to incorrect result, here is a possible fix) I modified the fixes from the rolled back PR's CC(fix integer overflow in range) and CC(Fix integer overflow in tf.range) to resolve an overflow that was identified by one of the new test cases. The issue was that in `ragged_range_op.cc`, `value` is incremented at the end of the loop logic, which results in a needless addition on the final iteration and creates the possibility for an overflow. This had no impact on the output of the operation and did not cause any of the tests to fail, but it was picked up by ASan. https://github.com/tensorflow/tensorflow/blob/ec9e1531f0114662baef0f3985de5620b43d0d29/tensorflow/core/kernels/ragged_range_op.ccL123L126 I added a condition that only increments the value if there are more iterations to be completed.",2024-11-15T19:10:24Z,awaiting review ready to pull size:M prtype:bugfix comp:core,closed,0,4,https://github.com/tensorflow/tensorflow/issues/80133," When you get the chance, would you mind taking a look at this review? Thanks!"," Leaving the size unsigned can cause a signed integer overflow when it is passed to the AddDimWithStatus function. https://github.com/tensorflow/tensorflow/blob/137b1e0e6890e8bad125911a325846db4ab05516/tensorflow/core/kernels/sequence_ops.ccL109 This function expects type int64_t, and for a very large size will interpret it as negative. https://github.com/tensorflow/tensorflow/blob/137b1e0e6890e8bad125911a325846db4ab05516/tensorflow/core/framework/tensor_shape.ccL427L433",  Looks like the tests at //tensorflow/python/kernel_tests:collective_ops_multi_worker_test are timing out.,>   Looks like the tests at //tensorflow/python/kernel_tests:collective_ops_multi_worker_test are timing out. Merged.  Thanks!
rag,copybara-service[bot],Handle ragged dot in precision config methods.,Handle ragged dot in precision config methods.,2024-11-15T01:59:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80079
yi,koranten2,Issue on inference of converted to tflight Super Resolution model," 1. System information  OS Linux Ubuntu 22.04  TensorFlow installation from sources  TensorFlow library version 2.16  2. Code I converted model from tensorflow to tflight. I should use Flex tf ops as not all layers were converted initially but finally model was converted successfully without errors with **tf ops**. On inference I have an issue  **_RuntimeError: tensorflow/lite/kernels/reshape.cc:92 num_input_elements != num_output_elements (0 != 8)Node number 0 (RESHAPE) failed to prepare.Node number 360 (IF) failed to prepare._**  and can not use this model. Please, help with this issue! Initial BasicVSR based model: ESWT1212_LSR_x4.pth.zip Tf model: sr.tf.zip Tf light model: sr_1212.tflight.zip This model initially appeared from FriedRiceLab Super Resolution model based on BaseSVR FriedRiceLab. I downloded ESWT1212_LSR_x4.pth from their page Google Drive This model was converted by scheme pth > onnx > tf > tflight Conversion script     _import numpy as np     import torch     from basicsr.models import build_model     from .utils import get_config     import onnx     import torchvision     import onnx_tf     import tensorflow as tf     from onnx import helper     def __init__(self, model_config_path, ",2024-11-14T23:40:23Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.16,closed,0,6,https://github.com/tensorflow/tensorflow/issues/80069,"Hi,   I apologize for the delayed response, I am able to replicate similar behavior from my end with your provided code snippet and converted TFLite model for reference I've added gistfile file where I'm getting `"" RuntimeError: tensorflow/lite/kernels/reshape.cc:92 num_input_elements != num_output_elements (0 != 8)Node number 0 (RESHAPE) failed to prepare.Node number 360 (IF) failed to prepare ""` so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention. Thank you for your cooperation and patience.","We resolved ""num_input_elements != num_output_elements (0 != 8)"" issue by converting tf>tflight slightly another way: converter.target_spec.supported_ops = [           tf.lite.OpsSet.TFLITE_BUILTINS, // it is commented now           tf.lite.OpsSet.SELECT_TF_OPS         ] Also, please, answer one additional question to avoid creating new issue. If we have Android Studio project that uses flex and tflite in native via shared libraries (not on java level) then is it possible to use GPU + flex? From internet searching I suspect that flex + GPU is not supported but not sure in it. Is flex support only CPU on android now?","Hi,  I apologize for the delayed response and Good to hear that original issue got resolved by including `tf.lite.OpsSet.SELECT_TF_OPS` which convert models that utilize operations not covered by the standard TFLite builtin operations during conversion of TensorFlow model to TFLite format. As far I know TensorFlow Lite supports both Flex and GPU acceleration individually, there is currently no support for using them together. The Flex delegate allows the use of TensorFlow operations that are not natively supported by TFLite. However, its implementation does not support GPU acceleration on Android devices.  TFLite provides a separate GPU delegate that accelerates inference for models optimized for GPU execution. However, this delegate does not work with Flex operations. The GPU delegate is designed to work with a specific set of operations that are optimized for performance on the GPU. Please refer this official documentation of GPU delegates for LiteRT for GPU ML operations support Thank you for your understanding and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],"[XLA:MSA] Adding two debugging functions for memory space assignment to facilitate reproduction of production bugs in small tests through steering decisions at two key points in the MSA pass flow, before and after AllocateSegment() call:","[XLA:MSA] Adding two debugging functions for memory space assignment to facilitate reproduction of production bugs in small tests through steering decisions at two key points in the MSA pass flow, before and after AllocateSegment() call: 1) debugging_allocation_request_modifier_fn(): allows modification of AllocationRequest before AllocateSegment(AllocationRequest) calls. 2) debugging_allocation_result_modifier_fn(): enables enforcing arbitrary failures on allocation requests, modifying the output of AllocateSegment(AllocationRequest) calls.",2024-11-14T21:21:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80058
yi,copybara-service[bot],Create HloUnaryInstruction to support result_accuracy for certain unary functions.,Create HloUnaryInstruction to support result_accuracy for certain unary functions.,2024-11-14T18:43:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80043
rag,copybara-service[bot],Update clone with new operands to handle ragged all-to-all.,Update clone with new operands to handle ragged alltoall.,2024-11-14T05:56:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/80011
yi,copybara-service[bot],[IFRT] Add ifrt-translate mlir tool for verifying dialect conversions.,"[IFRT] Add ifrttranslate mlir tool for verifying dialect conversions. This tool will run MLIR lit IFRT IR serialization and deserialization tests. In order to add support for this tool (and for some other possible cases), this change adds an optional `DeserializeIfrtIRProgramOptions`, which contains a pointer to an existing MLIRContext. If option is not null then the program is deserialized in this context, and the returned `IfrtIRProgram` doesn't own the context. Otherwise, the program is deserialized in a new context that is owned by the returned `IfrtIRProgram`.",2024-11-14T05:44:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/80010
yi,copybara-service[bot],[XLA:Python] Modify DLPack behavior with unit dimensions.,"[XLA:Python] Modify DLPack behavior with unit dimensions. As discovered in https://github.com/jaxml/jax/issues/24680, when a PyTorch tensor has a dimension with size `1`, it seems to report the DLPack stride for that dimension as `1`. This means that even when the torch Tensor is formally rowmajor, the imported array isn't. This shouldn't really matter (the placement of unit dimensions can be arbitrary!), but in practice (since XLA:CPU ignores layouts  that's another issue that is being worked on!) it can be annoying. This change updates the behavior to always produce rowmajor layouts for unit dimensions wrt to their neighbors.",2024-11-14T03:59:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/80006
yi,copybara-service[bot],Add Duplicate() method to TensorBuffer,"Add Duplicate() method to TensorBuffer Introduce reference counting to underlying TensorBufferT. When a TensorBuffer is duplicated, the created TensorBuffer with increasing reference to the underlying LiteRtTensorBuffer.",2024-11-14T02:17:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79998
rag,copybara-service[bot],gemm_rewriter_test: Split and optimize to allow passing in coverage mode.,"gemm_rewriter_test: Split and optimize to allow passing in coverage mode. Collecting coverage data from gemm_rewriter_test results in a significant slowdown, particularly in the already comparatively slow fp8 test cases. Splitting up the very large file file into separate tests with subsets of the test cases helps, as does shrinking the buffers in the largest test case.",2024-11-13T23:01:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79988
yi,copybara-service[bot],"PR #19272: Revert ""PR #15291: [NVIDIA GPU] Add Bitcast to collective pipeliner a…","PR CC(Gradient Penalty won't execute): Revert ""PR CC(Dockerfile.develgpu: infinite prompt loop): [NVIDIA GPU] Add Bitcast to collective pipeliner a… Imported from GitHub PR https://github.com/openxla/xla/pull/19272 This reverts commit 6c65d7a3e3358efef0d6fed4505236b41e5c68e7. Accepting Bitcast in collective pipeliner was a temporary solution for some workload relying on postlayout collective pipeliner. Recently we saw cases where including Bitcast can break the pattern matcher. Revert this PR since Bitcast will not show up in prelayout collective pipeliner, which is the default behavior moving forward. Copybara import of the project:  ad05557e7a46dcd44afa16c7a0cdb82e63651c4d by Terry Sun : Revert ""PR CC(Dockerfile.develgpu: infinite prompt loop): [NVIDIA GPU] Add Bitcast to collective pipeliner acceptable users"" This reverts commit 6c65d7a3e3358efef0d6fed4505236b41e5c68e7. Merging this change closes CC(Gradient Penalty won't execute) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19272 from terryysun:terryysun/revert_bitcast_in_cp ad05557e7a46dcd44afa16c7a0cdb82e63651c4d",2024-11-13T19:02:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79976
yi,copybara-service[bot],PR #16901: [XLA:GPU] Fix default device mesh for auto sharding,"PR CC(Fix the missing Windows cuDNN reference.): [XLA:GPU] Fix default device mesh for auto sharding Imported from GitHub PR https://github.com/openxla/xla/pull/16901 When the user does not specify the number of GPUs for auto sharding, XLA defaults to using all available GPUs. The current implementation uses the number of cores (SMs) on the GPU as the default shard count. For example, on an A100, the sharding algorithm will try to shard into 108 devices, which can be confusing for users. This patch changes the shard count to the number of cards, which has been tested to work correctly on an 8card A100 machine. Copybara import of the project:  232a62ae2599e6fe76e2e235ea18452195bce799 by Tianyi Liu : [XLA:GPU] Fix default device mesh for auto sharding Merging this change closes CC(Fix the missing Windows cuDNN reference.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16901 from iPear:try_fix_gpu_cards 232a62ae2599e6fe76e2e235ea18452195bce799",2024-11-13T11:10:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79948
yi,copybara-service[bot],Introducing a connection timeout in a ifrt proxy.,Introducing a connection timeout in a ifrt proxy.,2024-11-12T21:48:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79918
yi,copybara-service[bot],Use `cc_binary` (with `android_filegroup`) rather than `android_jni_library`,"Use `cc_binary` (with `android_filegroup`) rather than `android_jni_library` for `cc_3p_api_build_test`. Using `android_jni_library` was doing a link, but wasn't actually linking in the object files that define the C++ API, because they get put in a static library and don't get pulled into the `.so` file because they don't satisfy any of the symbol glob patterns (`Java_*`, etc.) that the linker is trying to find.  Better to use `cc_binary` so that the object files that define the C++ API do actually get linked in; that way, the linker detects if there are any undefined symbols in those object files. Also, mark the targets generated by the `cc_3p_api_build_test` build macro with `testonly = True`.",2024-11-12T21:41:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/79917
yi,copybara-service[bot],"PR #19272: Revert ""PR #15291: [NVIDIA GPU] Add Bitcast to collective pipeliner a…","PR CC(Gradient Penalty won't execute): Revert ""PR CC(Dockerfile.develgpu: infinite prompt loop): [NVIDIA GPU] Add Bitcast to collective pipeliner a… Imported from GitHub PR https://github.com/openxla/xla/pull/19272 This reverts commit 6c65d7a3e3358efef0d6fed4505236b41e5c68e7. Accepting Bitcast in collective pipeliner was a temporary solution for some workload relying on postlayout collective pipeliner. Recently we saw cases where including Bitcast can break the pattern matcher. Revert this PR since Bitcast will not show up in prelayout collective pipeliner, which is the default behavior moving forward. Copybara import of the project:  ad05557e7a46dcd44afa16c7a0cdb82e63651c4d by Terry Sun : Revert ""PR CC(Dockerfile.develgpu: infinite prompt loop): [NVIDIA GPU] Add Bitcast to collective pipeliner acceptable users"" This reverts commit 6c65d7a3e3358efef0d6fed4505236b41e5c68e7. Merging this change closes CC(Gradient Penalty won't execute) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19272 from terryysun:terryysun/revert_bitcast_in_cp ad05557e7a46dcd44afa16c7a0cdb82e63651c4d",2024-11-12T21:24:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/79914
yi,piskamenagabygaga,Could not create task ':app:processDebugResources'. Cannot use @TaskAction annotation on method IncrementalTask.taskAction$gradle_core() because interface org.gradle.api.tasks.incremental.IncrementalTaskInputs is not a valid parameter to an action method.,Build/Install Have you reproduced the bug with TensorFlow Nightly? No Source source TensorFlow version donot know Custom code No OS platform and distribution windows 11 Mobile device No response Python version No response Bazel version No response GCC/compiler version No response CUDA/cuDNN version No response GPU model and memory No response Standalone code to reproduce the issue Could not create task ':app:processDebugResources'. Cannot use  annotation on method IncrementalTask.taskAction$gradle_core() because interface org.gradle.api.tasks.incremental.IncrementalTaskInputs is not a valid parameter to an action method. * Try: > Run with info or debug option to get more log output. > Run with scan to get full insights. > Get more help at https://help.gradle.org. * Exception is: com.intellij.openapi.externalSystem.model.ExternalSystemException: Could not create task ':app:processDebugResources'. Cannot use  annotation on method IncrementalTask.taskAction$gradle_core() because interface org.gradle.api.tasks.incremental.IncrementalTaskInputs is not a valid parameter to an action method. 	at com.intellij.gradle.toolingExtension.impl.modelAction.GradleModelFetchAction.executeAction(GradleModelFetchAction.ja,2024-11-12T14:40:36Z,stat:awaiting response type:build/install stale subtype:windows type:performance,closed,0,6,https://github.com/tensorflow/tensorflow/issues/79891,"Hi **** , Apologies for the delay, and thank you for raising your concern here. Could you please confirm if you are experiencing any issues with TensorFlow? If so, kindly fill out all the required templates. Additionally, please check the compatibility of all versions involved. I am providing the documentation link1, link2 here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"I'm also facing this issue, please help. // build.gradle  // gradlewrapper.properties  ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Fix missing version case bug in context_binary_info.cc,Fix missing version case bug in context_binary_info.cc,2024-11-12T09:15:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79875
rag,copybara-service[bot],PR #19112: [GPU] GEMM fusion: support more broadcasts.,"PR CC(Feeding to a list of placeholders leads to the same values for the whole list placeholders): [GPU] GEMM fusion: support more broadcasts. Imported from GitHub PR https://github.com/openxla/xla/pull/19112 Support broadcasts involving 1sized fragments of dimensions like [1,n] > broadcast > [1,m,n]. Copybara import of the project:  f02445356ed623895ce0c2ac4cf06594f07facf8 by Ilia Sergachev : [GPU] GEMM fusion analysis: support broadcasts of triviallysized dimensions.  86ba22e4c635717b74ca39e8d1bcc0414f92acdd by Ilia Sergachev : [GPU] Triton GEMM emitter: support broadcasts of triviallysized dimensions.  2ca1165b9d72e29f241ea29df49fe71879c21fec by Ilia Sergachev : address feedback  308cf80a053812425fbff6bbdc17e07edac160f6 by Ilia Sergachev : add another test  8f38c5cd0b50a1d9abc77a6856027fc89de2ec43 by Ilia Sergachev : fix tensor pointer advancement Merging this change closes CC(Feeding to a list of placeholders leads to the same values for the whole list placeholders) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19112 from openxla:gemm_fusion_support_more_broadcasts 8f38c5cd0b50a1d9abc77a6856027fc89de2ec43",2024-11-12T01:06:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79820
yi,copybara-service[bot],Propagate shutdown errors before destroying agent.,Propagate shutdown errors before destroying agent. Also add a short buffer to the RPC timeouts so that servicerelated errors may get propagated before the RPC layer times it out. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19162 from ROCm:ci_fix_gridy_20241107 dd86f21a20d1391cefaf6f74f89abce787bb236a,2024-11-12T00:36:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79815
yi,copybara-service[bot],"Simplify MSA's BaseCosts api by removing the BytesPerSecond() method, and instead relying on CostAnalysisOptions that specify the bandwidth.","Simplify MSA's BaseCosts api by removing the BytesPerSecond() method, and instead relying on CostAnalysisOptions that specify the bandwidth. Also, rename async_copy_bandwidth_bytes_per_second to  default_mem_copy_bandwidth_bytes_per_second.",2024-11-11T23:06:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79809
yi,MrYoavon,JIT compliation failed," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory Radeon 7900XT  Current behavior? I tried to run some tensorflow code to process a few videos in a model I have. When running this code on the CPU, everything works fine. It doesn't work when running it on the GPU.  Standalone code to reproduce the issue   Relevant log output ",2024-11-11T19:13:11Z,stat:awaiting response type:bug TF 2.16,closed,0,13,https://github.com/tensorflow/tensorflow/issues/79798,"By the way, I'm using ROCm version 6.2.2","Hi **** , Apologies for the delay, and thank you for raising your concern here. I am trying to replicate your code on Colab but am encountering a different issue. Could you please provide your Colab gist? It would make troubleshooting your issue easier. Thank you!","I could but I encounter the same issue when running a simple code from ROCm tensorflow configuration guide. import tensorflow as tf print(""TensorFlow version:"", tf.__version__) mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 model = tf.keras.models.Sequential([   tf.keras.layers.Flatten(input_shape=(28, 28)),   tf.keras.layers.Dense(128, activation='relu'),   tf.keras.layers.Dropout(0.2),   tf.keras.layers.Dense(10) ]) predictions = model(x_train[:1]).numpy() tf.nn.softmax(predictions).numpy() loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) loss_fn(y_train[:1], predictions).numpy() model.compile(optimizer='adam',               loss=loss_fn,               metrics=['accuracy']) model.fit(x_train, y_train, epochs=5) model.evaluate(x_test,  y_test, verbose=2) This code is meant to help you check if you can run Tensorflow on your AMD GPU, and this too doesn't work with the same error message (the line that crashes is a different one but the error code is still the same)","Okay... I have made some odd progress. I can run the code perfectly fine if I use the terminal to run the main.py file. The ""Run File"" button in Pycharm Professional 2024.2.3 somehow causes it to not work properly. I would like to be able to use the button but if it's an issue with Pycharm then I guess I'll just use the terminal. Maybe it's something with the run configuration of Pycharm?","Hi **** , Apologies for the delay. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly versions, and it worked fine for me. Please find the gist here for your reference. Let me know if I missed anything. Thank you!","This wouldn't cause an issue on Colab since the problem is with the use of Tensorflow with ROCm. The code itself isn't problematic. According to ROCm documentation, my ROCm version is only compatible with Tensorflow versions 2.14.1, 2.15.1, 2.16.1. I'm using Tensorflow 2.16.1 and ROCm 6.2.4.","Hi,  I follow the topic, I have the same problem with 7900XTX ubuntu 24.04 rocm 6.2.4 and tensorflowrocm 2.16.2 and python 3.10 in a venv.","Hi **** , Could you please provide the ROCm documentation you are following? Also, try using the latest TensorFlow versions (2.17.0 or 2.18.0), as they may help you run more smoothly. Thank you!","Hi, This is the ROCm documentation I followed: https://rocm.docs.amd.com/projects/installonlinux/en/latest/install/3rdparty/tensorflowinstall.html Using Tensorflow 2.17 or 2.18 isn't possible, according to the documentation.","Hi, i had the same issue and fixed it. The hint came from: https://github.com/ROCm/ROCm/issues/3835 I just set the following environment variable: `ROCM_PATH=/opt/rocm` before executing the script","Hi, , 0 I apologize for the delayed response, Hi, praher thank you for your pointers, it is known issue please refer this original issue https://github.com/ROCm/ROCm/issues/1796issuecomment1447710413 and setting `ROCM_PATH` to `/opt/rocm` should fix this issue and also make sure that you're using correct supported versions of `ROCm` and `TensorFlow` mentioned here **Note :** As of **ROCm 6.1**, tensorflowrocm packages are found at https://repo.radeon.com/rocm/manylinux. Prior to ROCm 6.1, packages were found at https://pypi.org/project/tensorflowrocm. Please let us know after setting `ROCM_PATH` to `/opt/rocm` resolving your issue or not ? if issue still persists please let us know.  Thank you for your cooperation and patience.",This worked. Thanks to everyone who has taken their time to help :),Are you satisfied with the resolution of your issue? Yes No
rag,gaikwadrahul8,Update 02 broken links in gpu.md,"Hi, Team I found 02 broken documentation links for **GPU support for Android** and **GPU support for iOS** the in the following paragraph: **""This document provides an overview of GPUs support in TensorFlow Lite, and some advanced uses for GPU processors. For more specific information about implementing GPU support on specific platforms, see the following guides: GPU support for Android GPU support for iOS ""** I have updated those links to functional links. Please review and merge this change as appropriate. Thank you for your consideration.",2024-11-11T05:50:33Z,awaiting review comp:lite ready to pull size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79777
rag,gaikwadrahul8,Update broken link for create the TFLite op hyperlink in convert_mode…,"Hi, Team I found a broken documentation link for create the TFLite op hyperlink in the following paragraph: **""Solution: The error occurs as your model has TF ops that don't have a corresponding TFLite implementation. You can resolve this by using the TF op in the TFLite model (recommended). If you want to generate a model with TFLite ops only, you can either add a request for the missing TFLite op in Github issue CC(Address missing TensorFlow operations to TFLite:) (leave a comment if your request hasn’t already been mentioned) or create the TFLite op yourself.""** so I have updated this to a functional link. Please review and merge this change as appropriate. Thank you for your consideration.",2024-11-09T20:41:49Z,ready to pull size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79759
yi,trieu1162000,Error when inferencing on a tflite model, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.13.1  Custom code Yes  OS platform and distribution Linux Ubuntu  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am facing this error when trying to test inference of a tflite model on an image. Could you please help take a look and give your appropriate support? Error invoking model: output size must be nonnegativeNode number 14 (TfLiteFlexDelegate) failed to invoke.Node number 7 (WHILE) failed to invoke.Node number 590 (WHILE) failed to invoke.  Standalone code to reproduce the issue   Relevant log output _No response_,2024-11-09T08:04:07Z,stat:awaiting response type:bug stale comp:lite TF 2.13,closed,0,4,https://github.com/tensorflow/tensorflow/issues/79736,"Hi,   Thank you for bringing this issue to our attention, if possible could you please help us with your model and image to replicate the similar behavior from our end ? If you're okay could you please give a try with AI Edge Torch package to convert a PyTorch model to the LiteRT format because there is no need to go with this conversion process **Pytorch model > ONNX > TF > TF lite** please refer this official documentation  Thank you for your cooperation and understanding",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
llm,copybara-service[bot],[MPMD-GPU] Make Pathways IFRT client get GPU topology as well.,[MPMDGPU] Make Pathways IFRT client get GPU topology as well.,2024-11-08T23:51:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/79708
yi,mcl-uk,error while converting to tflite,"Hello, I'm trying to convert a tensorflow ""saved model"" to tflite format on a W11 PC with Python 3.12 and  tf v2.18.0. The model I'm working with is this one: https://github.com/tonytw1/squirreldetector/tree/main/models/squirrelnet_ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu8/saved_model I just get a barrage of incomprehensible error messages (after about 30s), see below. Thanks for any help. `**cvt = tf.lite.TFLiteConverter.from_saved_model('model_directory') lm = cvt.convert()** Traceback (most recent call last):   File """", line 1, in      lm = cvt.convert()   File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py"", line 1238, in wrapper     return self._convert_and_export_metrics(convert_func, *args, **kwargs)   File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py"", line 1190, in _convert_and_export_metrics     result = convert_func(self, *args, **kwargs)   File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py"", line 1572, in convert     return self._convert_from_saved_model(graph_def)   File ""C:\Users\Steve\AppData\Local\Programs\Python\",2024-11-08T15:05:03Z,type:support comp:lite TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/79695,"Hi, uk  Thank you for bringing this issue to our attention, I am able to replicate the same behavior from my end, Here is gistfile for reference **Here is output error log for reference :**  To fix this you need to tell the converter to include TensorFlow ops or Select TF Ops. This can be achieved by setting the target_spec.supported_ops attribute of the converter to include tf.lite.OpsSet.SELECT_TF_OPS along with tf.lite.OpsSet.TFLITE_BUILTINS. 1. `tf.lite.OpsSet.TFLITE_BUILTINS`: This includes the standard set of TensorFlow Lite operations. 2. `tf.lite.OpsSet.SELECT_TF_OPS`: This allows the converter to include TensorFlow operations that are not directly supported in TensorFlow Lite and in this case `tf.StridedSlice` Op does not have direct support in TensorFlow Lite ops so to enable the use of `tf.StridedSlice` Op so we need to add `tf.lite.OpsSet.SELECT_TF_OPS` during conversion I have tried from my end and it seems like working as expected please refer second section of this gistfile where I converted model to TFLite successfully. Please give it try from your end and let me know is it working as expected or not ? if not please help me with error log to investigate this issue further from our end.  Thank you for you cooperation and patience.","Thanks for responding. Taking on board your suggestions, my convert script now looks like: ` import tensorflow as tf cvt = tf.lite.TFLiteConverter.from_saved_model('model_dir') cvt.target_spec.supported_ops.add(tf.lite.OpsSet.TFLITE_BUILTINS) cvt.target_spec.supported_ops.add(tf.lite.OpsSet.SELECT_TF_OPS) lm = cvt.convert() ` However this again fails, now reporting: ` Traceback (most recent call last):   File """", line 1, in      lm = cvt.convert()   File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py"", line 1238, in wrapper     return self._convert_and_export_metrics(convert_func, *args, **kwargs)   File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py"", line 1190, in _convert_and_export_metrics     result = convert_func(self, *args, **kwargs)   File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py"", line 1572, in convert     return self._convert_from_saved_model(graph_def)   File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py"", line 1430, in _convert_from_saved_model     result = _convert_saved_model(**converter_kwargs)   File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\convert_phase.py"", line 215, in wrapper     raise error from None   Rethrows the exception.   File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\convert_phase.py"", line 205, in wrapper     return func(*args, **kwargs)   File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\convert.py"", line 1044, in convert_saved_model     conversion_flags = build_conversion_flags(**kwargs)   File ""C:\Users\Steve\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\convert.py"", line 785, in build_conversion_flags     conversion_flags.select_user_tf_ops.extend(select_user_tf_ops) TypeError: bad argument type for builtin operation ` I'd be most gratefull for any further assistance.","Appologies, I have retried the script afresh and can confirm that it _does_ now work. Thanks again for your help.",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Disable a test to work around a numpy bug.,Disable a test to work around a numpy bug. See https://github.com/numpy/numpy/issues/27709,2024-11-08T13:17:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79693
yi,minncode,"Gradle project sync failed. Basic functionality (e.g. editing, debugging) will not work properly."," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tensorflow:tensorflowlite:2.9.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I just downloaded current examplemaster zip file. and extract it. And then I open the file path (tensorflowlite/examples/style_transfer/android) with android studio. When I try to click sync button, it keeps saying that as the title. ""Gradle project sync failed. Basic functionality (e.g. editing, debugging) will not work properly.""   Standalone code to reproduce the issue   Relevant log output _No response_",2024-11-08T08:14:17Z,stat:awaiting response type:build/install stale comp:lite,closed,1,5,https://github.com/tensorflow/tensorflow/issues/79657,"Hi,   I apologize for the delayed response, I'm able to replicate the same behavior from my end for reference I've added output log below so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention **Here is output log for reference :** !image Thank you for your cooperation and patience.","Hi,   I tried with gradle version `7.6.2` instead of `8.4` so I changed this line `distributionUrl=https\://services.gradle.org/distributions/gradle8.4bin.zip` to this `distributionUrl=https\://services.gradle.org/distributions/gradle7.6.2bin.zip` in `gradlewrapper.properties` and things are working as expected for reference I've added output screenshot below so at the moment please use gradle `version 7.6.2` and our relevant team will fix this issue with `gradle version 8.4` soon **Here is output screenshot for reference :** !image Thank you for your cooperation and understanding.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only,Internal change only,2024-11-07T22:12:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/79633
yi,copybara-service[bot],"Test out stuff #3 (windows,copy,image2)","Test out stuff CC(JVM, .NET Language Support) (windows,copy,image2)",2024-11-07T19:42:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/79625
yi,litn2018,tf.keras.Sequential model can't use tf.data.Dataset.from_generator," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution WSL Ubuntu  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? after creating dataset from tf.data.Dataset.from_generator, model of tf.keras.sequential always failed due to shape not match for the input. Even just the simple github copilot generated sample can't run. not sure if anyone succeeded. thanks.  Standalone code to reproduce the issue   Relevant log output ",2024-11-07T19:35:34Z,stat:awaiting response type:support stale 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/79624,"Hi **** , Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly versions, and I faced the same issue. However, after making some changes, it is now asking for the CSV file, which you did not provide. Could you please share the file? It will help us troubleshoot more easily. For your reference, I have provided the gist here. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Add support for querying memory space descriptions to Pjrt C API.,Add support for querying memory space descriptions to Pjrt C API.,2024-11-07T16:55:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/79612
yi,copybara-service[bot],PR #18773: [ReduceScatterCombiner] Provide option to not combine within while loop bodies. ,"PR CC(Fix numerical warning of np.float with explicitly specifying np.float32): [ReduceScatterCombiner] Provide option to not combine within while loop bodies.  Imported from GitHub PR https://github.com/openxla/xla/pull/18773 Same as CC(Android：No OpKernel was registered to support Op 'Cumsum' with these attrs) but for reducescatters. Copying from CC(Android：No OpKernel was registered to support Op 'Cumsum' with these attrs)  This PR provides an option to disable combining reducescatters inside while loop bodies. It is set to true, so existing behavior is maintained. This option is provided as some strategies for FSDP may only want to coalesce collectives that are outside of a while loop. Collectives inside while loop are not coalesced, as we assume there is sufficient compute to overlap. Copybara import of the project:  9a7d247969db708170095177e7227c62e22e0eb5 by ptoulmeaws : [ReduceScatterCombiner] Provide option to not combine within while loop bodies. Merging this change closes CC(Fix numerical warning of np.float with explicitly specifying np.float32) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18773 from ptoulmeaws:reduce_scatter_combine_while 9a7d247969db708170095177e722",2024-11-07T08:07:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79578
llm,copybara-service[bot],[StableHLO] Refactor XlaCallModule to use more upstream StableHLO machinery.,[StableHLO] Refactor XlaCallModule to use more upstream StableHLO machinery.,2024-11-07T02:21:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79558
rag,copybara-service[bot],[XLA] Modify comments in ragged all-to-all HLO.,[XLA] Modify comments in ragged alltoall HLO.,2024-11-06T21:38:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79551
rag,copybara-service[bot],[XLA] Don't calculate fragmentation unnecessarily.,[XLA] Don't calculate fragmentation unnecessarily.,2024-11-06T20:20:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79546
agent,copybara-service[bot],Remove barrier test in pjrt/distributed with equivalent test coverage in coordination service's fork of client_server_test.,"Remove barrier test in pjrt/distributed with equivalent test coverage in coordination service's fork of client_server_test. 1. This is in preparation for a change in barrier semantics (don't require users to specify unique ids). 2. Moving forward, we want to shift new business logic tests to be in coord service's test suite.  This allows us to cover more edge cases with intrusive hooks (e.g. agent dtor tests) as well as nonJax (i.e. TF) scenarios. 3. Only use pjrt/distributed tests + Jax multiprocess Python tests for Jaxspecific requirements (e.g. topology exchange contract), or to exercise the xla_client (nanobind) and pjrt/distributed codepaths to validate that args are plumbed correctly. This reduces review burden on the Jax team.",2024-11-06T17:33:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79539
yi,copybara-service[bot],Update patch file to avoid error when applying patch.,"Update patch file to avoid error when applying patch. Due to a recent change in the llvm/BUILD.bazel file, the previous patch doesn't apply anymore. Update the patch file accordingly.",2024-11-06T11:40:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79521
yi,copybara-service[bot],Add `AutotuneCacheMode` to `xla_client.pyi`.,"Add `AutotuneCacheMode` to `xla_client.pyi`. An `AutotuneCacheMode` enum was added to `xla_extension` and `xla_client` in https://github.com/openxla/xla/pull/18450, but it looks like it was missed in `xla_client.pyi`. This is one of the issues blocking the merge of https://github.com/jaxml/jax/pull/22899, but I think this should do the trick!",2024-11-06T10:53:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79518
rag,copybara-service[bot],[PJRT-IFRT] Improve IFRT SE GPU client test coverage,[PJRTIFRT] Improve IFRT SE GPU client test coverage,2024-11-05T21:57:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79464
llm,copybara-service[bot],[StableHLO] Remove XlaCallModule's MHLO dependency,"[StableHLO] Remove XlaCallModule's MHLO dependency Remove references to MHLO from XlaCallModule handling, the only reason MHLO dep exists in this file is because when lowering to HLO, the MLIR module is converted to MHLO, so all queries on `module_` must be inspecting MHL. This can be avoided by inspecting the module to pick out the required fields before lowering to HLO.",2024-11-05T21:54:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79463
yi,kodek16,[TFLite/LiteRT] Conv3D does not support acceleration,"**System information**  TensorFlow Python version: 2.17.  TensorFlow Android versions:  **Summary** I am trying to speed up a model which is mostly made up of `Conv3D` operators on Android (for video classification). As far as I understand, neither the GPU delegate nor the quantization mechanism support `Conv3D`, so I am stuck with just running the fullprecision model on CPU. Am I missing some other ways I could use to accelerate this? If not, could this be a feature request for supporting a way? Some performance numbers from my tests (one model pass): * LiteRT, CPUonly, Android (Galaxy S10+): ~1400ms * LiteRT, CPUonly, iOS (iPhone 13 Pro): ~700ms * CoreML, accelerated, iOS, `float32` (same phone): ~160ms * CoreML, accelerated, iOS, `float16`) (same phone): ~15ms",2024-11-05T16:43:25Z,stat:awaiting tensorflower type:feature comp:lite 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/79439,"Hi,   I apologize for the delayed response, I see at the moment we do not support Conv3D Op with GPU delegates for LiteRT and There are some limitations to what TensorFlow ML operations, or ops, can be accelerated by the LiteRT GPU delegate. The delegate supports the mentioned ops in 16bit and 32bit float precision in this official documentation so this issue will be considered as feature request, thank you for showing your interest to support Conv3D Op Thank you for you cooperation and patience."
llm,feff2,Problems with EfficientNet v2 b0 inference in tf lite format," 1. System information  OS Platform and Distribution: Windows 11  TensorFlow installation : pip   TensorFlow library: 2.18.0  2. Code import tensorflow as tf input_shape = (224, 224, 3) inputs = tf.keras.Input(shape=input_shape) model = tf.keras.applications.EfficientNetv2b0(     include_top=True,     weights='imagenet',     input_tensor=inputs ) full_model = tf.keras.Model(inputs=inputs, outputs=model.outputs) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) model.summary() converter = tf.lite.TFLiteConverter.from_keras_model(full_model) converter.optimizations = [tf.lite.Optimize.DEFAULT] tflite_model = converter.convert() tf.lite.experimental.Analyzer.analyze(model_content=tflite_model, gpu_compatibility=True) with open('EfficientNet_float32.tflite', 'wb') as f:     f.write(tflite_model) print(""TFLite model saved successfully."")  When I run inference in python, the model outputs the correct solutions. But model is much slower than in the onnx format . Results for TF lite: 0.145 for onnx: 0.0108   CPU: 6core AMD Risen 5 7500F  When I run inference in Kotlin, the model outputs are incorrect. Model always gives the same wrong classes with less than 0.05 probs like n",2024-11-05T11:36:03Z,stat:awaiting response stale comp:lite TFLiteConverter TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/79422,"Hi,   Thank you for bringing this issue to our attention, I see this PR https://github.com/tensorflow/tensorflow/pull/74830 was merged on September 12, 2024 for Deprecating GPU compatibility experimental feature `from both tf/compiler/mlir/lite:flatbuffer_export and tf/lite/python/analyzer_wrapper:model_analyzer` but still we are getting below messages   You mentioned that the inference time for the TFLite model is significantly slower (0.145 seconds) compared to the ONNX model (0.0108 seconds). This could be due to several factors like model optimization so ensure that you are using optimizations effectively. You can try using quantization techniques to reduce the model size and improve inference speed. Please refer this official documentation and also check if your CPU is being fully utilized. Sometimes the model may not be optimized for the specific hardware you are using. The model outputs incorrect classes with low probabilities when run in Kotlin. This could be due to input preprocessing so ensure that the input images are preprocessed in the same way as during training. This includes resizing, normalization and any other transformations. If possible could you please help us with your Github repo replicate similar behavior along with complete steps which will be great to investigate this issue further ? Thank you for your cooperation and patience.","Hi   I use this example: https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android I changed ImageClassifierHelper ImageClassifierHelper.zip I use the model that was created by the following code: import tensorflow as tf input_shape = (224, 224, 3) inputs = tf.keras.Input(shape=input_shape) model = tf.keras.applications.EfficientNetv2b0( include_top=True, weights='imagenet', input_tensor=inputs ) full_model = tf.keras.Model(inputs=inputs, outputs=model.outputs) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) model.summary() converter = tf.lite.TFLiteConverter.from_keras_model(full_model) converter.optimizations = [tf.lite.Optimize.DEFAULT] tflite_model = converter.convert() tf.lite.experimental.Analyzer.analyze(model_content=tflite_model, gpu_compatibility=True) with open('EfficientNet_float32.tflite', 'wb') as f: f.write(tflite_model) print(""TFLite model saved successfully."") and  added metadata according to this documentation: https://ai.google.dev/edge/litert/models/metadata?hl=en","Hi,  I'm able to create model successfully with above provided code snippet for reference here is gistfile, if possible could you please help us with your Github repo (along with complete steps) to replicate similar behavior from our end which will help us to investigate this issue further ? Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
rag,vykintazo,Current LiteRT Android dependencies in documentation look broken,"I think after the recent TensorflowLite rename to LiteRT some pages in documentation where renamed incorrectly and are currently very confusing. For a particular example, see this: https://ai.google.dev/edge/litert/android/gpu  The docs say to add `com.google.ai.edge.litert:litertgpu` and `com.google.ai.edge.litert:litertgpuapi` with versions `2.X.Y`, which do not exist (current latest version is `1.0.1`), to a toml version catalog.   In the next paragraph it also switches into using gradle files instead of toml to declare other dependencies, which I found somewhat confusing.  Later, in standalone setup, it says to include `com.google.ai.edge.litert:litertgpudelegateplugin` dependency, which does not exist and also follows with a code snipped supposedly showing how to include it, but it shows other dependencies.  Other places too include dependencies with incorrect versions, like `com.google.ai.edge.litert:litertgpu:2.3.0` I just happend to start working with LiteRT for Android right now and found it very difficult to distinguish which parts of documentation are outdated and which aren't.",2024-11-05T11:29:22Z,stat:awaiting tensorflower type:support comp:lite TFLiteConverter TFLiteGpuDelegate,open,0,2,https://github.com/tensorflow/tensorflow/issues/79421,"Hi,   I apologize for the delayed response, thank you for bringing this issue to our attention I'll have look into this issue and will update you.  Thank you for your cooperation and patience.",Was an update ever published for this issue? I'm attemping to ue liteRT and this missing library issue is cropping up
yi,AdwaithAnand,Create a trainable tensorflow or LiteRT (with signatures) graph from a frozen tensorflow model," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.17  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am working with a frozen TensorFlow model (saved as a .pb file) and am exploring ways to make it trainable again, in its original TensorFlow format and eventually a LiteRT model with signatures to train. The goal is to restore training capabilities (such as finetuning or continued training) from a model that has already been frozen. I would appreciate guidance on how to approach this scenario.  Key Points:  Frozen TensorFlow Model: I have a TensorFlow model that was previously trained and saved in its frozen state (as a .pb file). This model no longer contains trainable variables, as they have been converted into constants. However, I would like to continue training this model on new data or finetune it for a different task. **Question 1**: What is the recommended approach to restore a frozen TensorFlow model to a ",2024-11-05T04:20:28Z,stat:awaiting tensorflower type:support comp:lite 2.17,open,0,5,https://github.com/tensorflow/tensorflow/issues/79377,"Hi,   I apologize for the delayed response, To confirm did you try something like mentioned in this gistfile for restoring frozen TensorFlow model for training and selectively unfreezing and finetuning parts of the model ?  Just to confirm, TensorFlow model that was previously trained and saved in its frozen state using TensorFlow 1.x or 2.x version please ? Thank you for your cooperation and patience.","Hi , I tried what you've mentioned in the gistfile, but am facing an issue in the step to create the keras model from the frozen model in the line `model = tf.keras.Model(inputs=input_tensor, outputs=output_tensor)`. The following is the error I'm facing  Can you please help with this? Sorry for the delayed response, I was trying out a few things to confirm that the issue was not due to the frozengraph I'm working with. I'm working with a resnet18 frozen model.","Hi,   I apologize for the delayed response, if possible could you please check the the input tensor shape matches the model's expected input and also verify the tensor names are correct and also check if any preprocessing is needed ? you can see model architecture by loading your model here https://netron.app/  Could you please confirm, TensorFlow model that was previously trained and saved in its frozen state using TensorFlow 1.x or 2.x version ?  Thank you for your cooperation and patience.","Hi , The input tensor shape does match the model's expected input. The tensor names also seems to be correct. The model was previously trained and saved using Tensorflow 2.x. As a sample for testing, the following is the code I used to create the frozen model for mobilenetV2.  Following is the code I used to make it trainable, based on the reference in gistfile provided  Error being faced: ","Hi,   I apologize for the delayed response, I am able to replicate the same behavior from my end for reference here is gistfile so we'll have to dig more into this issue and wil update you Thank you for your cooperation and patience."
agent,copybara-service[bot],Add unused barrier counter field.,"Add unused barrier counter field. This is in preparation for a change to allow reuse of barrierids on the user/agentside, by letting the agent and service keep track of the  of times a barrier has been invoked.",2024-11-05T01:19:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79373
yi,sri-cherukuri,TFLite Interpreter `experimental_preserve_all_tensors` yields different output," 1. System information  OS Platform and Distribution: Pop!_OS 22.04 LTS  TensorFlow installation: pip package  TensorFlow library: 2.17.0  2. Code   3. Failure after conversion Conversion is successful, but interpreter and experimental interpreter yield a different result:  The difference here is: `out_interp[0][3][1][3] = 33` `out_exper[0][3][1][3] = 34` This ""offbyone"" may occur with other sized input tensors and larger nonzero inputs/weights as well.",2024-11-04T21:14:51Z,stat:awaiting tensorflower comp:lite TFLiteConverter 2.17,open,0,2,https://github.com/tensorflow/tensorflow/issues/79355," the issue was able to reproduce the issue on Colab using TF v2.18.0, Please find the gist here for reference  Thank you!","Hi, cherukuri  I apologize for the delayed response, I'm also able to reproduce the same behavior from my end here is gistfile for reference with TensorFlow version **2.17.x** and **2.18.x** so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention. Thank you for your cooperation and patience."
yi,bajramienes,Unable to install TensorFlow: No matching distribution found for TensorFlow!,"When trying to install TensorFlow via pip, I encounter an error stating that no matching distribution can be found. The command I used and the error message are as follows:  Operating System: Windows 10 Python Version: (Python 3.13.0) I am currently using Python 3.13. Could this be related to compatibility with this specific Python version? Could you provide guidance on how to resolve this issue, or suggest any compatible alternatives?",2024-11-04T17:54:37Z,stat:awaiting tensorflower type:feature type:build/install subtype:windows,open,1,10,https://github.com/tensorflow/tensorflow/issues/79349,", The error which you are facing is due to the mismatch of tensorflow compatible versions. I request the latest tensorflow v2.18 is compatible with the python 3.9312. Kindly take a look at the official document for the compatible versions. https://www.tensorflow.org/install/source_windowscpu Thank you!",", the latest version, TensorFlow 2.18.0, supports Python versions 3.9–3.12, while mine is 3.13.0 with pip 24.3.1. I need to wait for the next release of TensorFlow that supports Python 3.13.0.",All brand new Fedora 41 installations will face this. Please update TF to work with 3.13 default installs where we have no easy choice. Why is this even a problem?,"the latest version, TensorFlow 2.18.0, supports Python versions 3.9–3.12, We have to wait to supports Python 3.13.0.","Hello , any news about this issue?",Has anyone solved the problem?,"I'm afraid we'll have to wait next year for a new tensorflow release supporting Python 3.13... See this comment I think the only alternative for the time being is to use Python 3.12. Python 3.13.0 release is less than 2 months old, we can't ask every package to support it in such a short time...  By the way, several tensorflow dependencies are not supporting Python 3.13 either. As far as I'm concerned, after a new Python major release is out, I wait about 6 months before starting to use it, except for testing purpose.","Hello everyone, any updates on this issue?","> I'm afraid we'll have to wait next year for a new tensorflow release supporting Python 3.13... See this comment > I think the only alternative for the time being is to use Python 3.12. > Python 3.13.0 release is less than 2 months old, we can't ask every package to support it in such a short time...  > By the way, several tensorflow dependencies are not supporting Python 3.13 either. > As far as I'm concerned, after a new Python major release is out, I wait about 6 months before starting to use it, except for testing purpose. For a package as important as this one, it would make more sense if it was released as soon as possible to accommodate a breaking point release. So, unless a major code refactoring was required to move forward with the industry, then the work should have already been completed by now. If not, then the door gets opened pretty wide to have someone else shore up that deficiency and render the package obsolete.  OSes releasing newer versions of major dependencies are not really just choosing to do so without real reason and without fully testing it out. That means the vendor libraries had just as much time to do the same. Those OS vendors certainly don't want to be held back from pushing the evolution of software development forward because one library vendor, even one as important as this, didn't upgrade their offering. If they did that, nothing would ever move forward.  It's far too easy to stagnate, but by doing so, that action creates the risk of losing market share to those who can't wait an additional 6 months or longer. They might decide that it's in everyone's best interest to just make a new solution and not be forced to use older, less secure OSes. Depending on the actual difficulty to make it compatible , even if no features are added, It's really in everyone's best interest, especially their own, just to do that at minimum and open the blocked gate to continued innovation. If the vendor were to keep up in a reasonable time frame, or at least explain why it isn't possible while providing a release map and set a reasonable time frame for opening this massive gate, then everyone wins. 6 months in software development, ESPECIALLY these days, is an eternity. The current climate in which everyone feels compelled to jump in to stay viable demands that kind of company lead us into the future. So, as I said, if one company drops the ball too long, they will likely lose their edge and be replaced. Then regaining the throne will be impossible without some massive breakthrough discovery or featureset to regain the trust they have lost. One only has to look at Intel as a prime example for proof of what I am saying, and because of this, I'm guessing NVidia before too long, too. Mr. Huang is pretty smart, though, so I would also guess that the competition will really need to up their game, and they will, even if that includes rewriting a sovereign dependency should it be required. The lack of movement here since I first brought it up is starting to seem like it is.",Found out today 3.13 cannot run TF.  Numpy not in 3.13 as of a few weeks ago.  venv anyone?
yi,copybara-service[bot],Add cuda_driver dependency to cuda_executor,Add cuda_driver dependency to cuda_executor We are missing this dependency and so far have been relying a transitive dependency which recently broke due to the removal of GpuKernel. So let's add the dependency.,2024-11-04T17:10:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79347
yi,copybara-service[bot],[XLA:GPU] Handle only dimensions in FoldApplyIndexingOperands,[XLA:GPU] Handle only dimensions in FoldApplyIndexingOperands `FoldApplyIndexingOperands` runs along with `MoveSymbolsToDims`. Thus symbol handling is not tested or used properly as they got moved to dimensions anyway.,2024-11-04T14:34:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79333
yi,els8482,Tensorflow Build for Alpine on Multiple Architectures," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.5.0  Custom code No  OS platform and distribution Linux Alpine 3.18  Mobile device _No response_  Python version 3.11.10r1  Bazel version 6.5.0  GCC/compiler version 12.2.1_git20220924r10  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Expected behavior: Successful build. Actual behavior: A python related error is printed to the console and the build fails. I'm trying to build Tensorflow 2.15.0 on Alpine Linux 3.18. My objective is to build Alpine compatible wheels for x86 and Arm64. To do this I plan to dow a build inside docker: 1/ Install Python 3.11.10r1, along with any dependencies, 2/ Download Bazel 6.5.0 , do a bootstrap build, using scripts/bootstrap/compile.sh, 3/ Trigger the Tensorflow build using Bazel, 4/ Export the wheel that has been built. The first two steps are working fine, however I'm getting an error that I don't understand from Bazel when building Tensorflow. From what I understand Bazel bunks down to system processes for the python components, so I have verified that the packages that it appears to be failing on are availab",2024-11-04T12:20:52Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/79327,", I request you to take a look at this https://github.com/tensorflow/tensorflow/issues/62899 where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Do not use fast approximation for exp for BF16 types yet.,Do not use fast approximation for exp for BF16 types yet. Some users are still relying on higher precision than the BF16 type actually guarantees.,2024-11-04T10:52:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79322
yi,misterBart,`unresolved external symbol TfLiteGpuDelegateV2Create` linker error with Visual Studio," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version TfLite nightly, 2.16, 2.10  Custom code No  OS platform and distribution Windows 10 Pro & Home  Mobile device _No response_  Python version Irrelevant, C++ API is used  Bazel version _No response_  GCC/compiler version Microsoft Visual Studio 2022 C++ compiler  CUDA/cuDNN version _No response_  GPU model and memory Geforce RTX 2080 (8 GB), Geforce GT 1030 (2 GB GDDR5), Intel HD Graphics 520  Current behavior? The function `TfLiteGpuDelegateV2Create` in the minimalworking example below yields the linker error `unresolved external symbol __imp_TfLiteGpuDelegateV2Create` with Visual Studio. Why? Short answer: Function `TfLiteGpuDelegateV2Create` is prefixed with preprocessor macro `TFL_CAPI_EXPORT` as `TFL_CAPI_EXPORT TfLiteDelegate* TfLiteGpuDelegateV2Create(const TfLiteGpuDelegateOptionsV2* options);` (see `tensorflow/lite/delegates/gpu/delegate.h`) When *building* TfLite, `TFL_CAPI_EXPORT` is empty. When *using* `delegate.h` in my application, `TFL_CAPI_EXPORT` contains `__declspec(dllimport)`. This `__declspec(dllimport)` causes the linker error. Long answer: Function `TfLiteGpuDelegateV2Create`",2024-11-04T10:20:25Z,awaiting review stat:awaiting tensorflower type:bug comp:lite TF 2.16,closed,0,5,https://github.com/tensorflow/tensorflow/issues/79317,"Hi,   Please take a look into this issue. Thank you","Hi , I noticed you are using GeForce GPU's ... CUDA isn't supported for Windows past TF 2.11, please review this note: https://www.tensorflow.org/install/source_windowsgpu to ensure your setup is supported. That being said I see you are trying in nightly, 2.16, and 2.10 ... which version do you wish to try? (If you want to try with WSL2 or tensorflowcpu with TensorFlowDirectMLPlugin). Alternatively, you can make a PR with your suggestions and we can see how the review goes.","I'm not using CUDA. This issue is completely unrelated to CUDA and Nvidia GPUs. And aside from testing with Geforce GPUs I also tested with an Intel HD 520 GPU, as I wrote down under ""GPU model and memory"" in my opening post. This issue is about a linker error with the Visual Studio compiler, and I already posted a PR to solve this issue (see above). This issue is present in TfLite 2.10, 2.16 and in Nightly.","My PR to solve this issue has been accepted, therefore I close this issue.",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Handle only dimensions in GetNewIndexingMapAfterFoldingSequence,Handle only dimensions in GetNewIndexingMapAfterFoldingSequence GetNewIndexingMapAfterFoldingSequence is used in  `FoldApplyIndexingSequence` that runs along `MoveSymbolsToDims`  `SimplifyLoopOfApplyIndexing` that only considers dimensions so we can consider only dimension variables.,2024-11-04T07:37:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79312
yi,Alphxt,Can't build tensorflow," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution Linux Gentoo   Mobile device _No response_  Python version 3.11  Bazel version Bazelisk 1.22.1 (bazel 6.5.0)  GCC/compiler version llvm17 clang17  CUDA/cuDNN version 12..6.1 / 9.5.0  GPU model and memory RTX 3050 laptop 8GB, but compiling for RTX 6000 ADA  Current behavior? Hello everyone.  I am new to tensorflow building and it seems that I can't manage to do it correctly. Whatever version of CUDA or CUDNN I choose the builds always fail. Currently, I am trying to compile from source using the command   However, it returns the error ""clang17: error: cannot find libdevice for sm_89; provide path to different CUDA installation via 'cudapath', or pass 'nocudalib' to build without linking with libdevice clang17: error: cannot find CUDA installation; provide its path via 'cudapath', or pass 'nocudainc' to build without CUDA includes clang17: error: cannot find CUDA installation; provide its path via 'cudapath', or pass 'nocudainc' to build without CUDA includes Target //tensorflow/tools/pip_package:wheel failed to build INFO: Elapsed time: ",2024-11-03T16:45:48Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/79299,", Could you please try to use config=cuda_clang after configuring to build using clang. (I also think that using bazel build without config flags should work too) The build currently also fails because of an error in `configure` script. The fix should be upstream soon, the workaround before it lands is to change the following lines of `.tf_configure.bazelrc` from:  to:  This should be done after running `configure`. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Gracefully cancel any pending barriers when a task goes away.,"Gracefully cancel any pending barriers when a task goes away. Whenever possible, the sender (agent) should cancel these RPCs to decrease the likelihood of the receiver (service) holding on to invalid callbacks and attempting to respond to invalid RPC objects. While the RPC layer should be resilient to such issues, additional defense in depth is good. The service code can still invoke the invalid callbacks later, but the underlying RPC layer knows that the other party cancelled the call, and so wouldn't attempt to respond over a broken pipe. The other RPCs are responded almost immediately and are unlikely to hit such edge cases (we can opt to harden this but I've not observed any actual bugs yet).",2024-11-02T02:54:34Z,,closed,1,0,https://github.com/tensorflow/tensorflow/issues/79245
yi,copybara-service[bot],internal BUILD rule visibility,internal BUILD rule visibility,2024-11-01T13:24:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79213
yi,dependabot[bot],Bump the github-actions group with 6 updates,"Bumps the githubactions group with 6 updates:  Updates `actions/checkout` from 4.2.0 to 4.2.2  Release notes Sourced from actions/checkout's releases.  v4.2.2 What's Changed  urlhelper.ts now leverages wellknown environment variables by @​jww3 in actions/checkout CC(Idea: support dictionary fetches with tf.Session.run()) Expand unit test coverage for isGhes by @​jww3 in actions/checkout CC(Fix TensorBoard lib/css dependency (1926))  Full Changelog: https://github.com/actions/checkout/compare/v4.2.1...v4.2.2 v4.2.1 What's Changed  Check out other refs/* by commit if provided, fall back to ref by @​orhantoy in actions/checkout CC(opencv imported after tensorflow can't read jpeg)  New Contributors  @​Jcambass made their first contribution in actions/checkout CC(Support for ""Prod"" on GPU)  Full Changelog: https://github.com/actions/checkout/compare/v4.2.0...v4.2.1    Changelog Sourced from actions/checkout's changelog.  Changelog v4.2.2  urlhelper.ts now leverages wellknown environment variables by @​jww3 in actions/checkout CC(Idea: support dictionary fetches with tf.Session.run()) Expand unit test coverage for isGhes by @​jww3 in actions/checkout CC(Fix TensorBoard lib/css dependency (1926))  v4.2.1  Chec",2024-11-01T08:40:12Z,ready to pull size:S dependencies github_actions,closed,1,0,https://github.com/tensorflow/tensorflow/issues/79199
yi,copybara-service[bot],Internal change only,Internal change only,2024-11-01T02:05:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79165
yi,copybara-service[bot],Interprets negative values for `solver_timeout_in_seconds` as disabling `solve_nd_sharding_iteratively`.,Interprets negative values for `solver_timeout_in_seconds` as disabling `solve_nd_sharding_iteratively`.,2024-10-31T22:50:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79156
yi,copybara-service[bot],[xla:ffi] Add reinterpret_data method to AnyBuffer.,"[xla:ffi] Add reinterpret_data method to AnyBuffer. The existing `typed_data` method is requires that the requested type equal the underlying native buffer type, but I have a few cases where this is too strict (e.g. `std::complex != cuComplex`), so it would be useful to be able to cast to typed pointers of other types.",2024-10-31T10:57:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79119
rag,Cirno-2000,Discrepancy in Error Handling for `tf.gather` on CPU vs GPU Leading to Undefined Behavior," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version v2.18.0rc24g6550e4bd802 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 22.04 jammy  Mobile device _No response_  Python version 3.10.0  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When running a custom implementation of `tf_as_strided` on TensorFlow using `tf.gather` to simulate the behavior of creating a strided tensor, a significant discrepancy in error handling between CPU and GPU is observed. The CPU raises an `INVALID_ARGUMENT` error when indices are out of range, whereas the GPU produces unexpected outputs without raising an error. This lack of consistent error handling on the GPU could lead to silent data corruption or undefined behavior.  Reproduction Steps:    Create a tensor with some data and define a size and stride pattern that leads to outofbounds index access.    Implement the `tf_as_strided` function using tf.gather to simulate a custom striding mechanism.    Run the function on both CPU and GPU using TensorFlow's `tf.device` context.  Standalone code to reprod",2024-10-30T21:42:38Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,6,https://github.com/tensorflow/tensorflow/issues/79063,"Hi **2000** , Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab with TensorFlow version 2.17.0 and faced a different issue. Please find the gist here for reference. Let me know if I made any mistakes. Thank you!","Hi , sorry for the delay. I tried your gist and it returned results successfully. Have you run the code with a GPU runtime, based on the error message?","Hi **2000** , Apologies for the delay. I tried using the GPU runtime also, but I am facing the same issue as with the CPU. Please refer to the gist here for more details. Let me know if I made any mistakes. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,Cirno-2000,Inconsistent Results Between CPU and GPU for `tf.math.pow`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version v2.18.0rc24g6550e4bd802 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 22.04 jammy  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.6  GPU model and memory _No response_  Current behavior? When running the `tf.math.pow` function with specific large exponent values on the CPU and GPU, the results are significantly different. Specifically, for the same input, the CPU returns zero, while the GPU produces an extremely large result.  Standalone code to reproduce the issue  Code:   Colab:  The safetensors can be found here at: Google Drive  Relevant log output  ```",2024-10-30T21:06:26Z,stat:awaiting response type:bug comp:ops TF 2.18,closed,0,2,https://github.com/tensorflow/tensorflow/issues/79060,"2000, The reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors. The same behavior is not observed on Apple M1, using numpy or in CPU. https://github.com/tensorflow/tensorflow/issues/58479 Thank you!",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[XLA:GPU] Use default layout as a default layout for a computation.,"[XLA:GPU] Use default layout as a default layout for a computation. Instead of copying the layout from computation instructions, use default layout. Instructions may not have instructions defined, causing layout assignment to fail.",2024-10-30T16:03:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79042
yi,copybara-service[bot],PR #14897: [Nvidia GPU] Add mechanism to detect nccl timeout and return error status,"PR CC(A bug in tensorflow r1.4 when applying  MultiRNNCell): [Nvidia GPU] Add mechanism to detect nccl timeout and return error status Imported from GitHub PR https://github.com/openxla/xla/pull/14897 The current behavior crashes the program whenever a nccl async error has occured, timeout errors are also not detected for async events. This pr adds a mechanism to do: 1. poll statuses of async events and return timeout if status is pending for too long 2. return nccl async event status as xla status so a proper python exception can be thrown. Copybara import of the project:  c12c0c6062106071e1cf62a7884d5d64bc9c198e by TJ Xu : Add mechanism to detect nccl timeout and return error status  a96d4d1f6430195c2919c923dd38fc49e2616a53 by TJ Xu : move async status and queue management to gpu executable  9ee2c05fbf11cc7478293e83621111b2256f9d6b by TJ : Added e2e test for testing nccl timeout and error propagation  b4fe8e3b784d7024461d7464b0e6461fc5122480 by TJ : address pr comments  8fb78e114d62642a41e23424b5d83cd71802cd6b by TJ : changed back the formatting for xla/python/pjit. : Fix ci failure  81ecf430e53a2fc813f16641d5a366151bf8b341 by TJ Xu : Add a grace period when async error happens so nccl has time to shu",2024-10-30T13:56:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/79038
yi,copybara-service[bot],[XLA:GPU] Add EstimateRunTimesForMultiOutputFusion.,[XLA:GPU] Add EstimateRunTimesForMultiOutputFusion. The current way we calculate fused time estimate for multioutput fusion is incorrect. We do `time_fused += producer_runtime.write_time` after applying latency hiding heuristic. As a result the formula for fused time look something like this:  For a compute bound kernel the formula will likely say that the fusion is not beneficial even if it saves a lot of consumer memory read.  In many cases current Cost Model in MultiOutputFusion gives identical costs for fused and unfused case and it's mostly luck that we proceed with beneficial fusions. The correct formula is  I've added a separate `EstimateRunTimesForMultiOutputFusion` to make it easier to migrate MultiOutputFusion to the same Cost Model calculation as in PriorityFusion.  Eventually `EstimateRunTimesForMultiOutputFusion` and  `EstimateRunTimesForPriorityFusion` will be unified in one function.,2024-10-30T12:44:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/79037
yi,arianmaghsoudnia,TensorFlow Ignores Logging Configurations and Outputs Unwanted Warnings," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.17.0  Custom code No  OS platform and distribution (Colab runtime) Ubuntu 22.04.4 LTS  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm encountering an issue where TensorFlow outputs runtime warnings messages despite configuring logging to suppress them. Messages are logged even though `TF_CPP_MIN_LOG_LEVEL` is set to ""3"" and `absl.logging` is configured to show only errors. Using a custom context manager to suppress the output doesn't work either, which suggests that certain TensorFlow modules might override these logging settings.  Standalone code to reproduce the issue You can try this snippet also on this colab notebook. Please check the runtime warnings.   Relevant log output ",2024-10-30T09:58:53Z,stat:awaiting response type:bug stale comp:apis 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/79029,", Apologies for the delay. Could you please check whether you are facing with the latest tensorflow v2.18 and also please create a virtual environment and test your code again. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"> [](https://github.com/arianmaghsoudnia), Apologies for the delay. Could you please check whether you are facing with the latest tensorflow v2.18 and also please create a virtual environment and test your code again. Thank you! I am using Windows 11 with TensorFlow 2.18 on WSL."
rag,copybara-service[bot],Create a new ML Build container for ARM64. The new container will leverage the hermetic Python and hermetic cuda requirements to reduce its size by removing unneeded softwares.,Create a new ML Build container for ARM64. The new container will leverage the hermetic Python and hermetic cuda requirements to reduce its size by removing unneeded softwares.,2024-10-30T00:35:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78989
yi,dan-kazbek,Grouped Conv3D convolutions don't work with some of the number of input filters," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.9.1  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA version 11.2, cuDNN version 8.1.0  GPU model and memory NVIDIA A100 80GB  Current behavior? As the title says, grouped 3D convolutions don't work if we have some specific combinations of input / output filters and batch size.  If I try to execute the standalone code I provided, the script crashes with the log output I provided. If I reduce the number of groups  to 1, the calculations proceed as expected. If I reduce the `batch_size` to 8, the forward pass of the layers is performed successfully, but the following warning is shown:   Standalone code to reproduce the issue   Relevant log output ",2024-10-29T21:28:29Z,stat:awaiting response type:bug stale comp:keras TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/78984,"kazbek, Looks like this is an issue which is related to Keras. So, could you please raise the issue in the Kerasteam/Keras repo for the quick resolution. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,kevqcpp,GPU support tensorflow NVIDIA JETSON ORIN NX 8GB UBUNTU 22.04,https://github.com/tensorflow/tensorflow/issues/62711  came across this and we can install but trying to run on python3.12 Jetpack 6.0 and having trouble getting the gpu recognized. nvcc version 12.2 and LB_LIBRARY_PATH is fine. Still can't get this running. No andcuda apparently. Anything special to get it to recognize the gpu? Tried the link in the post as well as 12.16.1 python 3.10/2 without success Wheel file not supporting the format although uname a agrees with arch64 in releases Please help,2024-10-29T20:16:33Z,type:build/install,closed,0,5,https://github.com/tensorflow/tensorflow/issues/78983,"Hi **** , Please check all compatibility versions for that specific TensorFlow version. Here, I am providing documentation for your reference. Also, please let us know where you are facing the issue and which versions you are using. This will make it easier for us to troubleshoot. Thank you!",Advertised as compatible 12.16.1  python 3.10 My understanding was that this installation uses a different version from those listed in the tensorflow datasheet https://docs.nvidia.com/deeplearning/frameworks/installtfjetsonplatform/index.html The user who mentioned that they could get gpu working on a jetson used the dp distro and idk what that is or why I should use that but it doesn't appear that I can install it for gpu use although I can install the library. How to enable gpu on jetson? Jetpack 6.0 nvcc version 12.2,Updated to jetpack 6.1 comes with cuda 12.6 changed cudnn path links LD_LIBRARY and PATH to include 12.6.1 Python 3.10/12 (both advertise support) Tensorflow 12.16.1 Still no GPU,Reflashed the device with Jetpack version and only one python and it worked fine,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Install python3.12 last as the default version. Add a TODO so we can switch to pyenv eventually.,Install python3.12 last as the default version. Add a TODO so we can switch to pyenv eventually.,2024-10-29T19:03:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78980
yi,pboutinaud,tf.keras.Concatenate fails with 5 dimensional tensors," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5.1 / 9  GPU model and memory A100 80G  Current behavior? When trying to use a keras Concatenate layer with two 5 dimensional tensors with all but the last dimension to 1, there is an ""IndexError"". In the provided example the result should be a tensor of shape (1, 1, 1, 1, 10). It works ok with  4 tf.keras.layers.Concatenate()([       5     np.arange(5).reshape(1, 1, 1, 1, 5),       6     np.arange(5).reshape(1, 1, 1, 1, 5)       7 ]) File ~/.conda/pa311tf/lib/python3.11/sitepackages/keras/src/utils/traceback_utils.py:122, in filter_traceback..error_handler(*args, **kwargs)     119     filtered_tb = _process_traceback_frames(e.__traceback__)     120      To get the full stack trace, call:     121      `keras.config.disable_traceback_filtering()` > 122     raise e.with_traceback(filtered_tb) from None     123 finally:     124     del filtered_tb File ~/.conda/pa311tf/lib/python3.11/sitepackages/",2024-10-29T13:48:27Z,stat:awaiting response type:support comp:keras TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/78957,", I guess the problem might be due to Keras3.0 which contains by default with the tensorflow v2.17 and v2.18. When I tried to execute the same code with the Keras2.0(tfkeras), it was executed without fail/error. Kindly find the gist of it here.  For Keras3.0, please raise the request in the Kerasteam/Keras repo for the quick resolution. Thank you!","Ok, thank you",Are you satisfied with the resolution of your issue? Yes No
rag,LeeABarron,Build from source v2.18.0 - countless warnings e.g. Do not pass an `input_shape`/`input_dim` argument to a layer.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version v2.18.0  Custom code No  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 13.2  Bazel version bazilisk v1.22.1  GCC/compiler version gcc (Ubuntu 13.2.023ubuntu4) 13.2.0  CUDA/cuDNN version 12.6.1 / 9.4.0  GPU model and memory  NVIDIA GeForce RTX 4060 Ti  Current behavior? I'm building from source, tag v2.18.0. Compilation succeeds with the **configuration below.** When used I get countless warnings. GPU seems to be used. I'm running the basic example from the startpage of the web site and get the following output: 20241029 13:56:32.470690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1730206592.497706   98446 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1730206592.505972   98446 cuda_blas.cc:1418] Unable to register cuBL",2024-10-29T13:18:25Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.18,closed,0,8,https://github.com/tensorflow/tensorflow/issues/78955,"Hi **** , Thank you for raising your issue here. I noticed there may be a version compatibility problem. Although everything compiled successfully, any compatibility issues could lead to performance problems. It is best to install compatible versions. I am providing the documentation here for your reference, please review it and check all compatibility versions. Let us know if the issue persists. Thank you!","Hi   Thank you for your response and your time. I did 2.17 before and dealt with another issue that's why I went to 2.18. At time of build this table was not updated yet for 2.18  so I went with the max versions of third_party\xla\third_party\tsl\third_party\gpus\cuda\hermetic\cuda_redist_versions.bzl at the time of the tag v2.18.0 I guess this mean these max versions (12.6.1 and 9.4) are ""experimental"" or for development only. Did I miss another way to find the max recommended versions from the code, when the table is not updated yet? I will happily give it a whirl with 2.5.1 and 9.3 and post my results. Thank you very much.","Unfortunately exactly the same  build successful (should I post configuration?) but warnings remain: 20241101 19:16:54.296179: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1730485014.331975   26232 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1730485014.342833   26232 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20241101 19:16:54.382102: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: SSE3 SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags. Downloading data from https://storage.googleapis.com/tensorflow/tfkerasdatasets/mnist.npz 11490434/11490434 ━━━━━━━━━━━━━━━━━━━━ 2s 0us/step  /home/user/source/tfeval/venv/lib/python3.12/sitepackages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.   super().__init__(**kwargs) I0000 00:00:1730485023.031375   26232 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14242 MB memory:  > device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9 Epoch 1/5 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1730485027.010202   26313 service.cc:148] XLA service 0x7804e00083c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: I0000 00:00:1730485027.010277   26313 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9 20241101 19:17:07.141875: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. I0000 00:00:1730485027.408144   26313 cuda_dnn.cc:529] Loaded cuDNN version 90300 I0000 00:00:1730485029.842869   26313 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process. 1875/1875 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step  accuracy: 0.8568  loss: 0.4844     Epoch 2/5 1875/1875 ━━━━━━━━━━━━━━━━━━━━ 4s 2ms/step  accuracy: 0.9567  loss: 0.1485    Epoch 3/5 1875/1875 ━━━━━━━━━━━━━━━━━━━━ 4s 2ms/step  accuracy: 0.9688  loss: 0.1038    Epoch 4/5 1875/1875 ━━━━━━━━━━━━━━━━━━━━ 4s 2ms/step  accuracy: 0.9734  loss: 0.0850    Epoch 5/5 1875/1875 ━━━━━━━━━━━━━━━━━━━━ 4s 2ms/step  accuracy: 0.9755  loss: 0.0749    20241101 19:17:46.978835: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_34', 4 bytes spill stores, 4 bytes spill loads 313/313 ━━━━━━━━━━━━━━━━━━━━ 3s 5ms/step  accuracy: 0.9720  loss: 0.0944 What could I try? Thank you very much.","The warning ""To enable the following instructions: SSE3 SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags"" is CPUrelated and has nothing to do with the CUDA and CUDNN versions. Using march=native as a CPU flag when you use the configure.py would do the job. For the recommended CUDA and CUDNN versions, there are in the file .bazelrc of the repo, search for the line ""build:cuda_clang_official repo_env=HERMETIC_CUDA_VERSION"" and ""build:cuda_clang_official repo_env=HERMETIC_CUDNN_VERSION"" for your rc. The other warnings are just there and don't seem to impact training","Hi **** , Apologies for the delay. For the similar issue one more issue was raised and still it is in open state.  please follow them for further updates. I am providing the link to that issue here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[GpuCommandBuffer] Move GraphGetNodeCount calls into subclasses,[GpuCommandBuffer] Move GraphGetNodeCount calls into subclasses  Introduce pure virtual function `GpuCommandBuffer::GetNodeCount() const` which returns the number of nodes in the underlying graph  Replace direct `GpuDriver::GraphGetNodeCount` calls by calls to `GpuCommandBuffer::GetNodeCount`  Inline `GpuDriver::GraphGetNodeCount` into the implementations of `GpuCommandBuffer::GetNodeCount`,2024-10-29T07:31:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/78922
yi,Alliswell1234,tensorflow importation error after installation, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.10  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior?  ImportError                               Traceback (most recent call last) File ~\Anaconda3\envs\tensorflow\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:62      61 try: > 62   from tensorflow.python._pywrap_tensorflow_internal import *      63  This try catch logic is because there is no bazel equivalent for py_extension.      64  Externally in opensource we must enable exceptions to load the shared object      65  by exposing the PyInit symbols with pybind. This error will only be      66  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      67       68  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling ,2024-10-29T03:00:44Z,stat:awaiting response type:build/install type:support stale TF 2.10,closed,0,5,https://github.com/tensorflow/tensorflow/issues/78903,", Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:  https://github.com/tensorflow/tensorflow/issues/61887 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.
yi,fuhailin,[v2.18] Compiling llvm/lib/Target/RISCV/GISel/RISCVInstructionSelector.cpp failed: undeclared inclusion(s) in rule '@llvm-project//llvm:RISCVCodeGen'," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version v2.18.0  Custom code No  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version 6.5.0  GCC/compiler version Ubuntu clang version 18.1.8  CUDA/cuDNN version CUDA 12.2/cuDNN 8  GPU model and memory NVIDIA A30 24GiB * 4  Current behavior? I am trying to build Tensorflow v2.18.0 from source, before compiling I choose the CUDA and clang option enabled and other options is the default setting, but it shows an error:   Standalone code to reproduce the issue   Relevant log output ",2024-10-29T02:26:54Z,type:build/install subtype: ubuntu/linux TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/78902,"Hi **** , Thank you for raising your issue here. I noticed that there is a version compatibility problem. Before building, please verify that all versions are compatible as specified in the documentation. This will help ensure a smooth build process without issues. I am providing the documentation here for your reference, please review it and check all compatibility versions. Let us know if the issue still persists. Thank you!","> Hi **** , Thank you for raising your issue here. I noticed that there is a version compatibility problem. Before building, please verify that all versions are compatible as specified in the documentation. This will help ensure a smooth build process without issues. I am providing the documentation here for your reference, please review it and check all compatibility versions. Let us know if the issue still persists. >  > Thank you! Thanks for your reply, I followed the document you provided here, and change my cuda version to the same with the document, the compiling error still existed.  But I try the method here https://github.com/tensorflow/tensorflow/issues/55563issuecomment1312393701 , add `spawn_strategy=sandboxed` solved this problem for me too.",Are you satisfied with the resolution of your issue? Yes No
transformer,copybara-service[bot],PR #18766: [NVIDIA] Fix the segfault in scatter_determinism_expander when running Transformer Engine,"PR CC(update $ source spacing): [NVIDIA] Fix the segfault in scatter_determinism_expander when running Transformer Engine Imported from GitHub PR https://github.com/openxla/xla/pull/18766  Issue: When running TE using XLA with xla_gpu_determinisitc_ops=true, the prefix scan inside the scatter determinism expander caused segfault when then there is only one update in the scatter operation.   Steps to reproduce:  The scatter determinism expander assumed the original scatter operation is not deterministic, meaning the case where there is only one indexupdate pair is not possible. However, if the creator of the scatter operation did not mark the scatter operation to have unique_indices, the scatter determinism expander pass will still match and be applied. In this case, the prefix scan implementation will have a segfault because the log2ceiling of num_indices=1 is 0, leading to direct return in the prefix scan without getting into the loop body, and hence a nullptr update will be returned.  Changes: 1. Modified the implementation of pattern matching of scatter_determinism_expander, so that even if the unique_indices flag of the scatter operation is not set properly, we will still check the number of indices",2024-10-28T12:02:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78878
yi,adamjstewart,Unable to build TensorFlow 2.18 with GCC," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0  Custom code No  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.11.9  Bazel version 6.5.0  GCC/compiler version GCC 13.2.0  CUDA/cuDNN version CUDA 12.6.2, cuDNN 8.9.7.29  GPU model and memory _No response_  Current behavior? When compiling TF 2.18 with GCC, the build passes many CUDA flags to the compiler:  I would expect these flags to be passed to nvcc instead of GCC. P.S. Yes, I know that Clang 17.0.6 is the only supported compiler, but I am nevertheless trying to build with GCC. I'm hoping another community member can help me figure out how to build with GCC. I experienced a similar issue when building JAX with GCC: https://github.com/jaxml/jax/issues/23689  Standalone code to reproduce the issue I'm using the following build script: https://github.com/spack/spack/pull/47211 Then I run:   Relevant log output * build log * build env",2024-10-27T10:25:18Z,stat:awaiting tensorflower type:build/install subtype: ubuntu/linux TF 2.18,closed,0,8,https://github.com/tensorflow/tensorflow/issues/78846, ,TensorFlow supports building with Clang.,Are you satisfied with the resolution of your issue? Yes No,"Strictly speaking, the docs say:  That does not mean that GCC+NVCC is unsupported or categorically forbidden.  It just says that it isn't default.  I understand that Google may not want to shoulder the burden of maintaining multiple compilers, especially given the complexities of GPU programming toolchains, but they should just say so if that is the case. Responding that  Is unresponsive to the bug being reported, and it certainly isn't `completed` in that there is a fix or work around that exists for this issue.  A resolution like `closed wontfix` would be more appropriate.",Are you satisfied with the resolution of your issue? Yes No,"Changed to closed won't fix. Please let me know if you prefer me to reopen this ticket, as other users may have comments about the issues for GCC build.","I think you're going to find that it is difficult to simply ""not support"" the world's most common family of compilers. It would be more realistic to support a good default compiler family on each OS: * Linux: GCC * macOS: Xcode/Apple Clang * Windows: Visual Studio Even if you close this issue, other people will open new issues to report the same problem. If Google is not willing or able to support GCC, the rest of the community will. All of this is to say that Google doesn't have to support anything they don't want to, but I would greatly appreciate it if standard compilers like GCC were still supported by someone (either Google or the greater TF community).","FYI, we managed to solve the GCC build issue. In addition to:  we also needed to set:  The Spack TF recipe now correctly builds on Linux x86_64 and aarch64 for GCC and Clang."
yi,copybara-service[bot],Internal change only,Internal change only,2024-10-25T19:09:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78782
yi,PhyllisJi,tf.keras.Input layer performs implicit data conversion when dtype is not specified," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.14.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Since tf.keras.Input layer performs implicit data conversion when dtype is not specified, the model can still operate normally even if the input tensor is of bool type, whereas other frameworks will explicitly indicate that the user input data type does not match the data type of the weights.  This feature is also not described in the documentation. https://www.tensorflow.org/api_docs/python/tf/keras/Input As shown in the code, in TensorFlow, convolutionrelated operators (such as tf.nn.conv2d or tf.keras.layers.Conv2D) do not natively support bool type inputs. However, when a user provides a tensor of bool type, the framework performs an implicit type conversion, converting the boolean values to a supported numerical type (e.g., float32) without notifying the user. This implicit conversion can lead to several issue",2024-10-25T09:53:29Z,type:docs-bug stat:awaiting response type:bug stale comp:keras type:performance TF2.14,closed,0,7,https://github.com/tensorflow/tensorflow/issues/78754,"Hi **** , Apologies for the delay. I tried to running your code on colab using Tensorflow 2.17.0 and nightly version and faced the same issue. Please find the gist here for reference. Please post this issue on kerasteam/keras repo. as this issue is more related to keras. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,can i work on fixing this issue? im new to the community and this could be a good starting point for me.,> can i work on fixing this issue? im new to the community and this could be a good starting point for me. It would be really great if you could!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,cybersupersoap,`tf.io.encode_png` can cause a crash, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.19.0dev20241023  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04.3 LTS  Mobile device Linux Ubuntu 20.04.3 LTS  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I have confirmed that above code would crash on `tfnightly 22.19.0dev20241023` (nightlybuild) Please find the gist to reproduce the issue.  Standalone code to reproduce the issue   Relevant log output ,2024-10-25T05:49:19Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/78735,"Hi **** , Apologies for the delay. Thank you for reporting this issue. I suggest taking a look at issue  CC(Aborted (core dumped) in `tf.io.encode_png`/`tf.compat.v1.image.encode_png`), where a similar issue has been proposed and is still open. Following this related issue may also help you stay updated on potential solutions. I tried running your code on Colab using the TensorFlow nightly version and encountered the same issue. I have provided an alternative solution that I hope will be helpful. Please find the gist here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],Adds RaggedAllToAll HLO Instruction.,Adds RaggedAllToAll HLO Instruction.,2024-10-25T05:36:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78734
yi,copybara-service[bot],Internal change only,Internal change only,2024-10-25T03:05:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78719
yi,copybara-service[bot],Implements the `CopyToHostBuffer` method for the `BasicStringArray`.,Implements the `CopyToHostBuffer` method for the `BasicStringArray`.,2024-10-25T02:21:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78718
yi,copybara-service[bot],Pass also non-string values as Dispatch API and Dispatch Delegate options,"Pass also nonstring values as Dispatch API and Dispatch Delegate options As an example, in order to specify ""/data/local/tmp/"" as the directory from where to load shared library=ies, the user can set up dispatch delegate options as shown below: auto dispatch_delegate_options = litert::CreateDispatchDelegateOptionsPtr(); LiteRtAddDispatchDelegateOption(dispatch_delegate_options.get(),     LiteRtDispatchOption{       .name = kDispatchOptionSharedLibraryDir,       .value = LiteRtAny {          .type = kLiteRtAnyTypeString,          .str_value = ""/data/local/tmp/"",       }, }); ASSERT_EQ(     LiteRtAddDispatchDelegateExecInfoOption( dispatch_delegate_options.get(), ""npu_bytecode"", npu_model>data(), npu_model>size(), /*function_name=*/""simple""), kTfLiteOk);     auto dispatch_delegate = litert::CreateDispatchDelegatePtr(std::move(dispatch_delegate_options));",2024-10-24T21:44:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78710
yi,sailesh2710,AttributeError: 'ModelCheckpoint' object has no attribute '_implements_train_batch_hooks' in MoViNet Streaming Model Training," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.17.0  Custom code No  OS platform and distribution Google Colab  Mobile device _No response_  Python version 3.1  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an AttributeError while trying to fit the model using the MoViNet Streaming Model Training and Inference notebook provided in the TensorFlow Model Garden repository. The specific error message is as follows: !image in addition, there is a minor adjustment needed to be done in this code: !image  Standalone code to reproduce the issue   Relevant log output ",2024-10-24T17:51:15Z,stat:awaiting response type:bug stale comp:model 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/78694,"Hi **** , Thanks for raising your concern. The issue you are facing may be due to TensorFlow 2.17.0 automatically using Keras 3. This can cause compatibility errors. To resolve this, you can import Keras separately, which will avoid this issue. Please make these changes: Replace:  with:  Update all instances of `tf.keras` in your code to `tf_keras`. I have provided a gist here for reference. Let me know if this resolves the issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],[xla:ffi] Add decoding for PlatformStream in internal FFI.,"[xla:ffi] Add decoding for PlatformStream in internal FFI. The test coverage is limited without a GPU test configuration, but I've added a basic decoding test like the one used for the external API.",2024-10-24T13:47:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78683
yi,copybara-service[bot],[XLA] Fix case in which conditional_canonicalizer mishandled conditional branch computations shared between multiple conditionals.,[XLA] Fix case in which conditional_canonicalizer mishandled conditional branch computations shared between multiple conditionals. Clone computations rather than modifying them in place.,2024-10-24T01:11:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/78637
rag,copybara-service[bot],Add a pattern matcher for ragged dot HLO.,Add a pattern matcher for ragged dot HLO.,2024-10-23T23:30:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78628
yi,AlexandruIordan99,Calling .batch() on a Dataset alters the input shape and leads to EfficientNet aborting training," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17  Custom code Yes  OS platform and distribution ArchLinux  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory GeForce RTX 4080  Current behavior? Calling .batch() on a dataset as presented in the Prepare Inputs section of https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/transferlearningfrompretrainedweights, leads to the input dataset changing shape from (None, 224, 224, 3) to (32, None, 224, 224, 3), where 32 is the chose batch size. This then gives a value error: ValueError: Input 0 of layer ""efficientnetb0"" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(32, None, 224, 224, 3). If relevant, I am attempting to train a model on the IP102 dataset, which can be downloaded from https://www.kaggle.com/datasets/rtlmhjbn/ip02dataset.   Standalone code to reproduce the issue   Relevant log output ",2024-10-23T20:13:20Z,type:bug,closed,0,2,https://github.com/tensorflow/tensorflow/issues/78621,Calling .batch() is unnecessary if you already declare batch size when first declaring the model,Are you satisfied with the resolution of your issue? Yes No
rag,NeilPandya,Build Failure: Compiling upb/upb.c failed; defining a type within 'offsetof' is a Clang extension., Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17  Custom code No  OS platform and distribution Ubuntu 22.04.5 LTS  Mobile device _No response_  Python version 3.12  Bazel version 6.5.0  GCC/compiler version gcc 11.4.0 / Clang 18.1.8  CUDA/cuDNN version CUDA 12.5.1 / cuDNN 9.3.0  GPU model and memory NVIDIA RTX 3090 24GB VRAM; 32GB SDRAM   Current behavior? Expected to compile tensorflowgpu successfully and export a `.whl` for installation into `conda` environments.  Standalone code to reproduce the issue   Relevant log output ,2024-10-23T17:30:15Z,type:build/install subtype: ubuntu/linux 2.17,closed,0,7,https://github.com/tensorflow/tensorflow/issues/78613,", Tensorflow v2.17 is compatible with Clang 17.0.6, Bazel 6.5.0, CUDA  8.9, cudNN12.3. Could you please try with the mentioned versions and also please take a look at the official document for the tested build configurations.  https://www.tensorflow.org/install/source Thank you!","> , > Tensorflow v2.17 is compatible with Clang 17.0.6, Bazel 6.5.0, CUDA  8.9, cudNN12.3. Could you please try with the mentioned versions and also please take a look at the official document for the tested build configurations.  > https://www.tensorflow.org/install/source >  > Thank you!  Just to confirm, I think you have the CUDA and cuDNN versions reversed?",", Apologies for that. Yes it is .For tensorflow v2.17,  cudNN  8.9, CUDA12.3 is compatible. Thank you!","> , > Apologies for that. Yes it is .For tensorflow v2.17,  cudNN  8.9, CUDA12.3 is compatible. Thank you!  I will run a container pulling from `tensorflow/build:2.17python3.12`; I think this would be the cleanest way to ensure compatibility across `clang`, `CUDA`, and `cuDNN`. Hopefully, this does the trick."," I successfully completed the compilation, but what's strange is that it's outputting the `.whl` tagged as `tf2.19`.  Docker Command   Configure   Build   Inspect the Output  Did I clone the wrong `tensorflow` branch? Thanks for your help.",I checked out the wrong branch. Closing this issue.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only,Internal change only,2024-10-23T15:12:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78607
rag,copybara-service[bot],Set up a ragged dot HLO instruction.,Set up a ragged dot HLO instruction.,2024-10-23T02:48:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78551
yi,copybara-service[bot],[GpuCommandBuffer] Move GraphGetNodeCount calls into subclasses,[GpuCommandBuffer] Move GraphGetNodeCount calls into subclasses  Introduce pure virtual function `GpuCommandBuffer::GetNodeCount() const` which returns the number of nodes in the underlying graph  Replace direct `GpuDriver::GraphGetNodeCount` calls by calls to `GpuCommandBuffer::GetNodeCount`  Inline `GpuDriver::GraphGetNodeCount` into the implementations of `GpuCommandBuffer::GetNodeCount`,2024-10-22T12:59:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78516
rag,copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18185 from PragmaTwice:patch1 71fcc0ebb2373cae0b713eba535cc52bd504ac68,2024-10-22T09:40:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/78490
rag,copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18185 from PragmaTwice:patch1 71fcc0ebb2373cae0b713eba535cc52bd504ac68,2024-10-22T09:35:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/78489
rag,copybara-service[bot],PR #18535: [ROCm] Fix //xla/stream_executor/rocm:rocm_timer_test_gpu_amd_any,"PR CC(Current Bazel version is 0.12.0, expected at least 0.4.2): [ROCm] Fix //xla/stream_executor/rocm:rocm_timer_test_gpu_amd_any Imported from GitHub PR https://github.com/openxla/xla/pull/18535 Copybara import of the project:  34045acf1fcd50717a4f2e6f5c5908620f0131b9 by Dragan Mladjenovic : [ROCm] Fix //xla/stream_executor/rocm:rocm_timer_test_gpu_amd_any Merging this change closes CC(Current Bazel version is 0.12.0, expected at least 0.4.2) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18535 from ROCm:rocm_timer 34045acf1fcd50717a4f2e6f5c5908620f0131b9",2024-10-22T07:59:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78481
transformer,i3abghany,"What does the tensor ""transformer_layer_0/BroadcastTo"" do, and why is it quantized to INT32?"," 1. System information  Linux Ubuntu 22.04  TensorFlow 2.12 installed via pip  2. Code  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: The KerasNLP workshop from IO2023 The link to the model I produced using the notebook is here: https://drive.google.com/file/d/1nSABgkHysrwkAn8K3H45Eqq646iNZOBp/view?usp=sharing  Other info I follow the IO 2023 workshop notebook above to use KerasNLP to produce a quantized GPT2 in the tflite format. I then visualize the tflite model using `tensorflow/lite/tools/visualize.py`. Most of the parameters are quantized to INT8, which is expected. I am confused about two things regarding the following tensors: `transformer_layer_0/BroadcastTo`, `transformer_layer_0/BroadcastTo1`, `transformer_layer_0/cached_multi_head_attention/ExpandDims`, (... for each ""transformer_layer""). My first question is: What does a broadcast/ExpandDims ""tensor"" store? It seems to me that those are not trainable operations (e.g. doing broadcasting), so why would the tensors have a huge shape [1, 100, 100] (i.e. 10,000 elements)? My second question is: Why are those tensors quantized to a higher precision (INT32 vs INT8 like other params)? I really appreciate any help you can ",2024-10-22T03:18:29Z,stat:awaiting response type:bug comp:lite TFLiteConverter TF 2.12,closed,0,4,https://github.com/tensorflow/tensorflow/issues/78475,"For documentation on the semantics of the BroadcastTo and ExpandDims operations, see the following:  https://www.tensorflow.org/api_docs/python/tf/broadcast_to  https://www.tensorflow.org/api_docs/python/tf/expand_dims"," Thank you. I already know the operations, but I am asking why they have trainable parameters and why they are quantized to a higher bit width (int32 instead of int8).","Hi,  As far I know `BroadcastTo` and `ExpandDims` tensors operations are used in the attention mechanism of transformers specifically to create attention masks and handle batched inputs, Those are not trainable parameters but necessary runtime tensors saves memory compared to storing separate masks for each batch and attention head As per my understanding the **INT32** quantization is necessary for numerical stability in attention computations (Q * K multiplications) which is needed for accumulating products across sequence dimension which preserves precision for mask values and attention weights, query and key are typically **INT8** after quantization ( INT8 values range (127 to 128) when multiplied together which exceeds **INT8** range (128 to 127) to avoid overflow in cumulative operations may be using **INT32** if I'm not wrong This behavior will depend upon which quantization technique you apply on the model for more details please refer this official documentation If I have missed something here please let me know. Thank you for your cooperation and patience.",Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],[HLO Componentization] Add deprecation warning to aliased build targets.,[HLO Componentization] Add deprecation warning to aliased build targets. This step towards encouraging extrenal projects to migrate to the already migrated hlo subcomponents.,2024-10-22T01:44:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78474
yi,copybara-service[bot],"#sdy add shardy CPU config for all JAX tests, disabling any known failing test cases.","sdy add shardy CPU config for all JAX tests, disabling any known failing test cases. Only test cases breaking on CPU are related to:  pure callbacks  export  shard alike Note that `layout_test` is broken on TPU, leaving a comment saying to enable it. Also fixed `shard_map_test` test that was broken when running Shardy on one TPU, and `aot_test` which was breaking due to calling a different C++ StableHLO compilation function.",2024-10-21T22:42:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78467
yi,copybara-service[bot],Internal change only.,Internal change only.,2024-10-21T17:52:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78449
yi,copybara-service[bot],[XLA] Small speedups to the latency hiding scheduler.,"[XLA] Small speedups to the latency hiding scheduler. * change GetResourcesFromInstruction to return an absl::Span rather than copying the resources vector. This appears to be safe: no callers appear to mutate the resource map while the absl::Span is alive. * fix GetResourcesFromInstruction to only do a single hash lookup, rather than two. * make GetResourcesFromInstruction nonvirtual, change GPU implementation to override GetResourcesFromInstructionImpl instead. * move recursively_compute_resource_map into an outofline helper function. Simplify its invariants so it performs the async_in_computation_cache_ lookup and insertion, which both simplifies the code and avoids a double hash lookup.",2024-10-21T17:49:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78448
yi,copybara-service[bot],Fix a bug in the calibrator when trying to log null calibration values. Copy of https://github.com/tensorflow/tensorflow/pull/76955.,Fix a bug in the calibrator when trying to log null calibration values. Copy of https://github.com/tensorflow/tensorflow/pull/76955.,2024-10-21T12:18:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/78429
rag,copybara-service[bot],PR #18185: Fix dead source code path in XLAFramework dialect,PR CC(Building Graphs documentation): Fix dead source code path in XLAFramework dialect Imported from GitHub PR https://github.com/openxla/xla/pull/18185 `tensorflow/compiler/xla/` has already been moved. Seems now we can just use `xla/` since XLA became a standalone project. Copybara import of the project:  71fcc0ebb2373cae0b713eba535cc52bd504ac68 by Twice : Fix dead source code path in XLAFramework dialect Merging this change closes CC(Building Graphs documentation) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18185 from PragmaTwice:patch1 71fcc0ebb2373cae0b713eba535cc52bd504ac68,2024-10-21T07:48:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78420
rag,copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18185 from PragmaTwice:patch1 71fcc0ebb2373cae0b713eba535cc52bd504ac68,2024-10-21T05:35:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/78416
yi,copybara-service[bot],Include all headers explicitly.,Include all headers explicitly.,2024-10-19T17:38:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78345
yi,copybara-service[bot],Internal change only,Internal change only,2024-10-18T23:56:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78225
rag,copybara-service[bot],Adds opcode for RaggedAllToAll HLO (NFC),Adds opcode for RaggedAllToAll HLO (NFC),2024-10-18T17:34:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78205
yi,saradha04,Import TensorFlow Error: Conflict with Mesop and rules_python Causes AttributeError," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17.0, tfnightly==2.18.0.dev20240906  Custom code No  OS platform and distribution Ubuntu 24.04  Mobile device _No response_  Python version 3.11  Bazel version Not specified  GCC/compiler version Not specified  CUDA/cuDNN version Not installed (running on CPU)  GPU model and memory Not applicable (running on CPU)  Current behavior? When I installed both TensorFlow and Mesop simultaneously, I encountered an error while importing TensorFlow. It seems that installing Mesop also installs rules_python, which causes TensorFlow to malfunction. Specifically, at the following code:     [tensorflow/tensorflow/python/platform/resource_loader.py] Lines 117 to 119 in [bc90265]  r = runfiles.Create()   new_fpath = r.Rlocation(       _os.path.abspath(_os.path.join('tensorflow', path))) r becomes None, leading to an error. According to rules_python: https://github.com/bazelbuild/rules_python/blob/0.26.0/python/runfiles/runfiles.pyL40 It seems possible that r = runfiles.Create() can indeed return None. While it seems the issue could be linked to Mesop installing rules_python, there might also be an underlyin",2024-10-18T16:39:14Z,stat:awaiting response type:build/install stale subtype:bazel 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/78201,", As a Temporary fix, Could you please try to add python definitions in the Project's {project}/WORKSPACE file, and it might work and the Project can be build successfully. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,kubarybpl,Cannot include headers loader.h session.h," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17  Custom code No  OS platform and distribution Windows 10  Mobile device _No response_  Python version _No response_  Bazel version 6.5.0  GCC/compiler version Clang 17.0.6  CUDA/cuDNN version _No response_  GPU model and memory only CPU  Current behavior? During adding headers tensorflow/core/public/session.h and tensorflow/cc/saved_model/loader.h there are issues with EventCount.h from eigen libary. Using MSVC 2022, but same error occurs while using LLVM/MSVC2019 compiler with my C++ application (in Qt 6.5.0) and same thing happend when i was trying to use TF 2.9.3 GPU compiled with tested version of tools from TF official page .  Standalone code to reproduce the issue   Relevant log output ",2024-10-17T20:30:32Z,stat:awaiting tensorflower type:bug comp:core 2.17,closed,0,2,https://github.com/tensorflow/tensorflow/issues/78125,"The issue is related to fact that word ""singals"" occurs in Qt as signal mechanism. Both libraries use signals as a identifier which leads to a namespace collision. The solution is: undef signals include ""tensorflow/cc/saved_model/loader.h"" include ""tensorflow/core/public/session.h"" define signals",Are you satisfied with the resolution of your issue? Yes No
yi,Lorsu,ResNet model has wrong output shape,"  Issue I applied the tutorial for posttraining integer quantization to a ResNet model, but the conversion leads to a model with a wrong output shape of [   1    7    7 2048], instead of a 1000element vector  1. System information Google Colab, with the default tensorlow 2.17  2. Code the image_array is a numpy array of shape (n, 224, 224, 3) and dtype float32 ",2024-10-17T14:40:06Z,stat:awaiting response type:bug comp:lite TFLiteConverter 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/78111,/assign,"Hi,   I apologize for the delayed response, thank you for bringing this issue to our attention if possible could you please help us with exact Google colab notebook which you created with  **ResNet** model to replicate the same behavior from our end ?  Thank you for your cooperation and patience.","Hi,   It seems like the issue you're facing is because you're using `include_top=False` in the `ResNet50` model which removes the final classification layers. To get the **1000** element output vector we need to include the top layers by setting `include_top=True` so model includes the final classification layers with output 1000 element vector corresponding to the **1000** ImageNet classes.This is useful when you want to use the model for ImageNet classification tasks. Please refer this gistfile  If I have missed something here please let me know. Thank you for your cooperation and patience.","> Hi,  >  > It seems like the issue you're facing is because you're using `include_top=False` in the `ResNet50` model which removes the final classification layers. To get the **1000** element output vector we need to include the top layers by setting `include_top=True` so model includes the final classification layers with output 1000 element vector corresponding to the **1000** ImageNet classes.This is useful when you want to use the model for ImageNet classification tasks. Please refer this gistfile >  > If I have missed something here please let me know. >  > Thank you for your cooperation and patience. yes, the issue is in the include_top parameter, thank you really much ",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only,Internal change only,2024-10-17T07:11:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78080
yi,copybara-service[bot],PR #18139: Enable gpu latency hiding for copy-start/copy-done,"PR CC(Error in restoring the tensorflow model and metagraph while using seq2seq and attention model.): Enable gpu latency hiding for copystart/copydone Imported from GitHub PR https://github.com/openxla/xla/pull/18139 This CL adds copystart and copydone instructions to the missing pairs in GPUProfileStatisticsAggregator, enabling correct handling of host memory offloading segments in profiling data. The scheduler optimizes instruction placement by moving copystart instructions earlier and adjusting copydone instructions to maximize overlap with computation while ensuring program correctness. This helps MaxText latency hiding when enabling offloading and scan_layers=false. Copybara import of the project:  e4f72dd5afe25fb1d537b22b3a35099118ebae1e by Jane Liu : Enable gpu latency hiding for copystart/copydone  f5bc4e4de00419dd5d285a6a0f3254e3269dc432 by Jane Liu : simplify the tests  9b38e137738539dcf9255e32b48a800120a2f892 by Jane Liu : add comments Merging this change closes CC(Error in restoring the tensorflow model and metagraph while using seq2seq and attention model.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18139 from zhenyingliu:latency_hiding 9b38e137738539dcf9255e32b48",2024-10-16T20:10:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78054
yi,copybara-service[bot],Internal change only,Internal change only,2024-10-16T18:42:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/78051
rag,copybara-service[bot],Add platform constraint to tflite_gpu_extra_gles_deps config_setting(),Add platform constraint to tflite_gpu_extra_gles_deps config_setting(),2024-10-16T15:44:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/78036
yi,copybara-service[bot],Fix tsan bug by avoiding string references after done callbacks have been invoked.,"Fix tsan bug by avoiding string references after done callbacks have been invoked. Tsan bug: Done callback completes the RPC and destroys the request, releasing the underlying string (barrier_id).  Fun fact: We already had a comment in coordination_service. `barrier_id`, but I did it anyway. Reverts 7a976f8c802ede5aaaeba6db33a99c1ef352d30f",2024-10-16T00:10:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77997
yi,gaikwadrahul8,Fix typo in code logic of Conv3DTranspose(),"Hi, Team It seems like there is typo error in the code logic of **Conv3DTranspose()** in the below line which is trying to combine padding values for **height** and **depth** dimensions so as far I know padding values should correspond to their respective dimensions in this case **depth** dimension not **height** so the padding calculation should only use the padding value relevant to the depth related padding values I think it should be like below :  https://github.com/tensorflow/tensorflow/blob/d540fd62d027dbb86e9434f8c43802997c8ab29c/tensorflow/lite/kernels/internal/optimized/optimized_ops.hL8220 If you've any suggestion or feedback or Am I missing something here please let me know ? Thank you",2024-10-15T14:38:50Z,stale comp:lite size:XS,closed,0,8,https://github.com/tensorflow/tensorflow/issues/77968,"Hi , Can you please review this PR? Thank you !",Can you please also add a regression test that will exercise this case?,"Hi,   I apologize for the delayed response and thank you for your feedback, now I've added test cases If you've any suggestion or feedback or Am I missing something here please let me know ? Thank you","It looks like with your second commit, the following code snippet from the test is now duplicated. I think that would mean that those tests would unnecessarily get run twice? ","Hi,   Thank you for bringing my oversight to my attention. I inadvertently placed the new test cases within an existing test case. I have now made the necessary modifications as per your suggestion. I appreciate your understanding and support. Thank you for your cooperation.","It looks like some of the newly added tests are failing, e.g. `tensorflow/lite/kernels:conv3d_transpose_test`:  and `tensorflow/lite/delegates/utils/experimental/sample_stable_delegate:sample_stable_delegate_kernel_tests`: ",This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,This PR was closed because it has been inactive for 14 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,copybara-service[bot],PR #14897: [Nvidia GPU] Add mechanism to detect nccl timeout and return error status,"PR CC(A bug in tensorflow r1.4 when applying  MultiRNNCell): [Nvidia GPU] Add mechanism to detect nccl timeout and return error status Imported from GitHub PR https://github.com/openxla/xla/pull/14897 The current behavior crashes the program whenever a nccl async error has occured, timeout errors are also not detected for async events. This pr adds a mechanism to do: 1. poll statuses of async events and return timeout if status is pending for too long 2. return nccl async event status as xla status so a proper python exception can be thrown. Copybara import of the project:  79f2058634fe24cda5ded45ef138d831eb9ea1f9 by TJ Xu : Add mechanism to detect nccl timeout and return error status  bdeb35da41d4c6417989cd2cb40a5b2c63ee10c3 by TJ Xu : move async status and queue management to gpu executable  4d936f4d11a01cc6a5d64e98c3577b0cc79fd0d5 by TJ : Added e2e test for testing nccl timeout and error propagation  3d0e9792ed219dd2aee02d4454364487d5661ae1 by TJ : address pr comments  26882703a6475b78d05121be96cc192ff5b7e93d by TJ : changed back the formatting for xla/python/pjit. : Added back the IsIdle api in gpu stream interface  ba7f02b4aeac46b1ed00ca8e487f5359b9d3384a by TJ Xu : Fix ci failure  5ce2139bd184bc7a",2024-10-15T13:41:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/77967
yi,henghamao,"TypeError: true_fn and false_fn arguments to tf.cond must have the same number, type, and overall structure of return values"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution Linux CentOS 7.9  Mobile device _No response_  Python version 3.12.4  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? We used tensorflow for 3 class classifications. The code worked well on existed environment: Python 3.8.3 + tensorflow 2.13.1. But when we tried on new enviroment: Python 3.12.4 + tensorflow 2.17.0, several errors blocked.  Standalone code to reproduce the issue Here are the simple code to reproduce the issue.  The first error we met:  To solve the issue, we used tf dataset to wrap the generator.   And another error happened:  It looks like class weight might not work well.  Relevant log output _No response_",2024-10-15T11:16:10Z,type:bug 2.17,closed,0,19,https://github.com/tensorflow/tensorflow/issues/77958,"Another try, we removed class weight in fit(), another error happened. All these code worked well on old enviroment Python 3.84. + tf 2.15, but failed on Python 3.12.4 + tf 2.17.0.  Error log: ","   I modified the data_gen function to output not only the data (x_batch, y_batch) but also sample_weights, which are calculated based on the class_weights. Instead of class_weight, the model now uses sample_weights per batch in the generator, which handles class imbalance. can you check this code out:  this shows no error output for tis code !image what do you say about this??","Thanks for providing work around solutions! By using sample weight, we could do the model training on tf 2.17.","  Does sample_weight and class_weight behave the same in model training? We observed different training results from tf 2.15 used class weight and tf 2.17 used sample weight. tf 2.17 stopped at epoch 1 with best val_loss, while tf 2.15 using class weight stopped at epoch 3 with best val loss. And thus, tf 2.15 got better training results. BTW, the training speed is about 25% faster on new enviroment python 3.12 + tf 2.17.","Hi **** , Apologies for the delay. I tried running your code on Colab using TensorFlow 2.13.0, and it worked fine. I also tested it with versions 2.17.0 and the nightly release, and I encountered the same issue. However, it works fine with an alternative approach. Please find the gist here for reference. You mentioned that the training speed is faster in this alternative way — this is because Keras 3 is working on JAX, which enhances performance compared to older versions. Thank you!","  Thanks for the reply. I read the gist, the approch is almost the same as  provided. The code used 'sample weight' instead of 'class weight' to deal with imbalance data classifications. However, our question is whether sample weight behave exactly the same as class weight in model training. We observed different training results from tf 2.13 used class weight and tf 2.17 used sample weight. Not sure it is caused by run to run variance. Comparing early stop epoch, tf 2.17 using sample weight stopped at epoch 1 with best val_loss, while tf 2.13 using class weight stopped at epoch 3 with best val loss. tf 2.13 using class weight got better training results with smaller val_loss tested by our data.","Any updates on this issue? Will ""class weight' be fixed in latest version of TF?","I've been struggling for 2 days with this bug, I changed many things in my code, and the problem was the (class_weight) have a bug with tensorflow > 2.15 just revert it back to 2.13 and the code is working great !",Are you satisfied with the resolution of your issue? Yes No,"We reported the issue 3 months ago, and seems the issue still not resolved. We had to stay at old python version and old tf 2.13.",Are you satisfied with the resolution of your issue? Yes No,"Wrong operation to close the issue. As the issue had not resolved, reopen it and warit for the response.","Transforming `if` from a control block to an expression requires that both branches are present and have the same type. Please fix your code to pass the additional checks that have been added, this is working as intended.","""Transforming if from a control block to an expression requires that both branches are present and have the same type. "" Sorry, I did not catch the problem of the code. Could you please help clarify the issue? The code worked correctly with tf 2.13, but failed with tf 2.15.","It was an error before and it got fixed. In imperative constructs you can have  and  But in functional constructs, you have an expression `cond(condition, true_fn, false_fn)`. Already need to pass both branches. If you want the code to do type inference correctly, then the type of the return value of `cond` needs to be consistent and to reach that you need to have the types of `true_fn` and `false_fn` be equivalent. Until 2.13 this was not completely correctly done, but it got fixed.","From your description, it looks like a bug fix inside tf code caused the issue. Our concern is how to use class weight for new version of tf. The previouse code did not work now.","  Looking into the keras/src/backend/tensorflow/trainer.py and keras/src/trainers/data_adapters code, `class_weight` and `sample_weight` should behave in a similar way for most common data types (I checked the code for ArrayDataAdapter and tf.dataset types, but haven't looked into other types). In fact, `class_weights` are converted into `sample_weights` internally in both implementations. For example: https://github.com/kerasteam/keras/blob/master/keras/src/trainers/data_adapters/array_data_adapter.pyL66L74.  To test this, I initialized two versions of the model in your code and set the initial weights for the models to be the same. Then I ran model.fit() on the first model with `sample_weights`, and on the second model with the `class_weights` (using the dictionary). Note that I set `shuffle=False` explicitly during the model.fit() call, so that each training step for both models uses the same data.  The metrics (accuracy, auc and loss) are identical for each epoch for both models. Here is the gist.  A few caveats. I tested the code on TF 2.18.0. I'm not sure if an earlier version had a different implementation. Both the models in the gist pass the inputs and outputs to the model directly i.e. they do not use a generator. `class_weights` do not work with a generator and there was never support for it AFAIK (source).  I hope this clears up your question on whether sample_weight and class_weight work the same. Let me know if you have any other doubts.",  Thanks! We will compare model training results running with sample weight and class weight.,Are you satisfied with the resolution of your issue? Yes No
yi,LilyDong0127,TensorFlow argmin function returns incorrect index when dealing with subnormal float values.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When using the argmin function on an input array containing subnormal float values, TensorFlow incorrectly returns the index of 0.0 as the minimum value, even though a smaller subnormal value (1.401298464324817e45) exists in the array. Other deep learning frameworks (e.g., PyTorch, Chainer) correctly return the index of the subnormal value, but TensorFlow (and Keras) consistently return the index of 0.  Standalone code to reproduce the issue  shell Summary of results: PyTorch argmin: 2 TensorFlow argmin: 0 Keras argmin: 0 Chainer argmin: 2 JAX argmin: 0 ",2024-10-15T08:15:27Z,stat:awaiting response type:bug stale comp:ops TF 2.16,closed,0,4,https://github.com/tensorflow/tensorflow/issues/77946,", Looks like this is similar to the another issue which was raised for the Argmax. https://github.com/tensorflow/tensorflow/issues/77853 And also calculations performed on the CPU using floatingpoint numbers smaller than a certain threshold (like 1e45) are rounded down to zero and subnormal numbers (like 1e45) are flushed to zeros Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,Ame3yakatole,Update pywrap_mlir.py,"Input Validation: Added validation for inputs such as graphdef, pass_pipeline, and input_names. Improved Docstrings: Added detailed docstrings to explain the arguments, return values, and potential exceptions. Pathlib Support: Replaced stringbased file paths with Pathlib.Path for crossplatform compatibility. Default Argument Handling: Updated default arguments for mutable lists (output_names). Logging: Added logging functionality when show_debug_info is set to True. Batch Processing: Added support for batch processing of multiple GraphDef objects using batch_import_graphdef.",2024-10-15T07:12:04Z,size:L,closed,0,5,https://github.com/tensorflow/tensorflow/issues/77940,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi  , Can you please sign CLA , thank you",Someone from MLIR should review this,"Hi , Can you please review this PR? Thank you !","Hi  , Can you please sign CLA , thank you"
yi,copybara-service[bot],Generate configuration_generated.h properly,Generate configuration_generated.h properly This change is a bug fix in the automatically generated code that was introduced by the new version of the flatbuffer generator that TensorFlow updated to in https://github.com/tensorflow/tensorflow/commit/c17d64df85a83c1bd0fd7dcc0b1230812b0d3d48 which includes the following change https://github.com/google/flatbuffers/pull/7813 which fixed the underlying flatbuffer code generator bug.,2024-10-15T04:38:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77926
yi,copybara-service[bot],Internal change only,Internal change only,2024-10-15T03:43:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77906
yi,copybara-service[bot],Finalize refactors for CompilerPlugin C++ wrapper,"Finalize refactors for CompilerPlugin C++ wrapper * Move internal compiler plugin stuff to core/compiler_plugin * don't put the algo logic to , rename the public functions * Rename PluginManager > CompilerPlugin * Merge CompilerPluginApi impl with CompilerPlugin * Incorporate feedback from earlier CLs to have CompilerPlugin manager the lifetimes of underlying LrtCompilerPlugin and dynamic lib. * Update interface to be more concise and use C++ better, use LrtResult * add dumping for CompilerPlugin * Move dynamic loading dumping to tools/ * Move qnn dumping into its own tools/ folder",2024-10-15T01:08:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77898
rag,copybara-service[bot],Rollback of PR #77018 because it made the unit test `python/ops/ragged:ragged_range_op_test` to cause signed integer overflow.,Rollback of PR CC(fix integer overflow in range) because it made the unit test `python/ops/ragged:ragged_range_op_test` to cause signed integer overflow. The error call stack was: tensorflow/core/kernels/ragged_range_op.cc:144:15: runtime error: signed integer overflow: 713794229 + 1849827689 cannot be represented in type 'int' CC(未找到相关数据) 0x55f036bf067a in  tensorflow::RaggedRangeOp::Compute(tensorflow::OpKernelContext*)  tensorflow/core/kernels/ragged_range_op.cc:144:15 Reverts fb673632e8b7f4f525ac63378a42096053540683,2024-10-15T00:23:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77895
yi,maybeLee,"tf.math.is_strictly_increasing's behavior is not clear on a (2,2) matrix"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When receiving this input:  `tf.math.is_strictly_increasing` outputs `False` instead of `True`. Also, for a tensor with shape (1,3,3):  It's output is still `False` instead of `True` even when x's first dimension only has one element. Based on the description ""Elements of x are compared in rowmajor order."", it seems that elements in x are compared along row (i.e., the first dimension). Therefore, to my understanding, if the first dimension contains only one element (such as 1x3x3 shape tensor), the output should be True. If the input is [[1,2],[3,4]], the output should also be `True` since the value in the first dimension is increasing (from `[1,2]` to `[3,4]`)  Standalone code to reproduce the issue  ```  Relevant log output _No response_",2024-10-14T15:28:21Z,stat:awaiting tensorflower type:bug comp:ops 2.17,open,0,2,https://github.com/tensorflow/tensorflow/issues/77863,I was able to reproduce the issue on tensorflow v2.17 and tfnightly. Kindly find the gist of it here. **Output in 2.17 and nightly:**  Thank you!,"The description says: _""Elements of `x` are compared in rowmajor order.  The tensor `[x[0],...]`   is strictly increasing if for every adjacent pair we have `x[i] < x[i+1]`.   If `x` has less than two elements, it is trivially strictly increasing""_ It means that the multidimensional array is effectively flattened into a onedimensional array in rowmajor order before comparison. There is also elementwise comparison that probably reflect your scenario. It can be calculated with:  "
yi,Mammancy, Failed to load the native TensorFlow runtime.,"ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[414], line 1 > 1 import tensorflow File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:38      35 import sys as _sys      37  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 38 from tensorflow.python import pywrap_tensorflow a",2024-10-14T10:12:01Z,stat:awaiting response stale,closed,0,3,https://github.com/tensorflow/tensorflow/issues/77854,"Hi  , Could you please provide the complete steps you followed to install the tensorflow and also please fill issue template from here which helps to debug the issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,Kim-William,AttributeError: 'NoneType' object has no attribute 'shape'," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17  Custom code Yes  OS platform and distribution ubuntu 22.04  Mobile device _No response_  Python version 3.9.20  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? TensorFlow version: 2.17.0 Transformers version: 4.46.0.dev0 Keras version: 3.6.0 This problem can be solved by using TensorFlow version 2.11, but it is not solved because i'm trying to use Tensorflow version 2.17. The existing BERT model is implemented in version 2.17, but this also does not work in version 2.11 of Tensorflow...  Therefore, I would like to solve this problem and make the entire code work in version 2.17.  Standalone code to reproduce the issue   Relevant log output ",2024-10-13T15:07:19Z,stat:awaiting response type:support stale 2.17,closed,0,7,https://github.com/tensorflow/tensorflow/issues/77826,"Hi **William** , Apologies for the delay. I tried to run your code on Colab using TensorFlow 2.17.0, and it is throwing the following error:. Could you please provide the CSV file associated with your code? This will make it easier for us to replicate and troubleshoot the issue. Please find the gist here for reference. Thank you!","> Hi **William** , Apologies for the delay. I tried to run your code on Colab using TensorFlow 2.17.0, and it is throwing the following error:` No such file or directory: '../Output/proto_models_rev2/train_cleaned.csv'`. Could you please provide the CSV file associated with your code? This will make it easier for us to replicate and troubleshoot the issue. Please find the gist here for reference. >  > Thank you! test_cleaned.csv train_cleaned.csv   Here i attached the data set. please check this :)",Are you satisfied with the resolution of your issue? Yes No,"Hi **William** , Apologies for the delay, and thank you for providing the necessary files. I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version, and I encountered the same issue. Please find the gist here for reference. Please post this issue on kerasteam/keras repo. as this issue is more related to keras Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Create StreamExecutor::Activate method as a factory method for creating ActivateContexts.,Create StreamExecutor::Activate method as a factory method for creating ActivateContexts. This allows simplifying the creation of ScopedActivateContexts.,2024-10-11T21:51:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77731
yi,copybara-service[bot],Retry `go install` commands to prevent network flakes,Retry `go install` commands to prevent network flakes Also `bazel build nobuild` ahead of querying in `bazel_tags.yml`,2024-10-11T20:17:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77725
yi,copybara-service[bot],Fix segfault in ExportXlaOp for DotGeneralOp when an algorithm is specified.,"Fix segfault in ExportXlaOp for DotGeneralOp when an algorithm is specified. This fixes the issue reported in https://github.com/jaxml/jax/issues/24236. The problem was that when the precision is not explicitly specified in the HLO, but the algorithm is, we were trying to call `set_algorithm` on a `nullptr`. I'm not 100% sure where the default precision config is being stripped between JAX lowering and here because JAX includes `precision = [DEFAULT, DEFAULT]` in the lowering, but it seems like a good idea to protect against this segfault regardless!",2024-10-11T19:26:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77719
yi,nassimus26,How to install the TfLite after succeed the cmake build ?," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version Latest 2.17  Custom code Yes  OS platform and distribution Ubuntu 24.04.1 LTS  Mobile device _No response_  Python version 3.9.20  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? After build the Lite project succesffuly with cmake :  I didn't find any doc about how to install it, so I tried  `>>>sudo cmake install .` I got this error :  And before trying to compile the project I also tried to install directly without success with different Python version from 3.9.x to 3.12.x :   Relevant log output _No response_",2024-10-11T19:06:29Z,stat:awaiting response type:bug stale comp:lite 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/77718,"Hi,   Thank you for bringing this issue to our attention, as per official documentation To build an installable package that can be used as a dependency by another CMake project with `find_package(tensorflowlite CONFIG)`, in your projects `CMakeLists.txt` file and I do not believe these instructions are meant to be used with `cmake install .` Refer to CMake documentation for find_package to learn more about handling and locating packages. If I've missed something here please let me know. Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #17890: [ROCM] Add nanoo fp8 support in type traits,"PR CC(Does tf.slim out of maintenance ?): [ROCM] Add nanoo fp8 support in type traits Imported from GitHub PR https://github.com/openxla/xla/pull/17890 We are trying to add NANOO FP8 support in TensorFlow. To achieve this, a small modification in the `type_traits.h` file on the XLA side is required, where the NANOO FP8 should also be classified as a simple type. This change will be utilized in tensor.. You can find the relevant code here: TensorFlow tensor.cc. Copybara import of the project:  1c27aebbc206559861ee8d5e5f44522a2a44858b by scxfjiang : add nanoo fp8 support in type traits Merging this change closes CC(Does tf.slim out of maintenance ?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17890 from ROCm:ci_nanoo_fp8_in_type_traits 1c27aebbc206559861ee8d5e5f44522a2a44858b",2024-10-11T18:41:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77715
rag,nassimus26,"Ho to deal with this Message : TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s): Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack"," 1. System information  Linux Ubuntu 16.04  TensorFlow installation 2.17 I want to convert this model and check if it could run on a Coral Edge TPU  2. Code  I am getting this message (I have no idea how to deal with this, I didn't find any relevant documentation about this message): ",2024-10-11T14:59:51Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter 2.17,closed,0,7,https://github.com/tensorflow/tensorflow/issues/77704,"Hi,   I apologize for the delayed response, if possible could you please help us with Google colab to reproduce the same bahavior from our end to investigate this issue further from our end ?  Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,">   tf.lite.OpsSet.SELECT_TF_OPS  enable TensorFlow ops. Try removing that line? In general that is needed for running on CPU, if your model contains TF ops that can't be converted into TF Lite ops. But I don't think such ops will run on EdgeTPU.  If you remove that line, either it will now just work, or if not you should get some information from the converter about which ops can't be converted.","Hi,  Hi,  Thank you for your pointers As far I know that  message indicates that your model contains TensorFlow operations that are not natively supported in TFLite and requires the Flex delegate. Try using a simpler architecture that is known to work with Edge TPU. Use the Edge TPU compatibility checker tool and quantize your model to **INT8**, you'll have to replace `TimeDistributed` and `LSTM` with supported operations mentioned here You can use Flex Delegate to avoid this message `TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack` but as you mentioned in the issue template after converting the model to TFLite you want to run on Coral Edge TPU so which is not recommended for Edge TPU because the Edge TPU does not currently support the Flex Delegate Please refer this supported operations on the Edge TPU in that `TimeDistributed` is not supported operations and unidirectional `LSTM` only supported If I have missed something here please let me know.  Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
gpt,copybara-service[bot],PR #16975: Add a few related optimization passes for fp8 gemm custom-calls.,"PR CC(libcublas.so 9.0 cannot open shared object file { CUDA 8.0 + GeForce 940MX}): Add a few related optimization passes for fp8 gemm customcalls. Imported from GitHub PR https://github.com/openxla/xla/pull/16975 This caused convergence issue for fp8 training, tested on GPT3 models: Before:  After:  Copybara import of the project:  81af29c8667792fe9ed189ab55308ca6e83859d4 by Elfie Guo : Add a few related optimization pass for fp8 gemm rerwriter. Merging this change closes CC(libcublas.so 9.0 cannot open shared object file { CUDA 8.0 + GeForce 940MX}) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16975 from elfiegg:pass 81af29c8667792fe9ed189ab55308ca6e83859d4",2024-10-11T14:44:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77702
yi,copybara-service[bot],PR #18152: [XLA:GPU] Avoid fusion-wrapping copies,PR CC(Missing //third_party/address_sorting): [XLA:GPU] Avoid fusionwrapping copies Imported from GitHub PR https://github.com/openxla/xla/pull/18152 Fusion wrapping copies breaks the logic for detecting copies from copyinsertion in rematerialization pass. This patch avoids wrapping copy instructions and instead emits them directly in IrEmitterUnnested. This should fix https://github.com/openxla/xla/issues/17922 Copybara import of the project:  49daad1836186fd7abe2ad089aa8783f1125f605 by Jaroslav Sevcik : Avoid fusionwrapping copies Merging this change closes CC(Missing //third_party/address_sorting) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18152 from jarosevcik:avoidfusionwrappingcopies 49daad1836186fd7abe2ad089aa8783f1125f605,2024-10-11T08:21:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77665
yi,copybara-service[bot],PR #18152: [XLA:GPU] Avoid fusion-wrapping copies,PR CC(Missing //third_party/address_sorting): [XLA:GPU] Avoid fusionwrapping copies Imported from GitHub PR https://github.com/openxla/xla/pull/18152 Fusion wrapping copies breaks the logic for detecting copies from copyinsertion in rematerialization pass. This patch avoids wrapping copy instructions and instead emits them directly in IrEmitterUnnested. This should fix https://github.com/openxla/xla/issues/17922 Copybara import of the project:  49daad1836186fd7abe2ad089aa8783f1125f605 by Jaroslav Sevcik : Avoid fusionwrapping copies Merging this change closes CC(Missing //third_party/address_sorting) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18152 from jarosevcik:avoidfusionwrappingcopies 49daad1836186fd7abe2ad089aa8783f1125f605,2024-10-11T06:04:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/77654
yi,copybara-service[bot],"Migrate dtensor_graph_to_mlir_pass to ConvertGraphToTfExecutor. This is no-op, the underlying api is the same but ConvertGraphToMlir is deprecated and will be removed shortly.","Migrate dtensor_graph_to_mlir_pass to ConvertGraphToTfExecutor. This is noop, the underlying api is the same but ConvertGraphToMlir is deprecated and will be removed shortly.",2024-10-10T23:45:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77606
gpt,copybara-service[bot],PR #16975: Add a few related optimization passes for fp8 gemm custom-calls.,"PR CC(libcublas.so 9.0 cannot open shared object file { CUDA 8.0 + GeForce 940MX}): Add a few related optimization passes for fp8 gemm customcalls. Imported from GitHub PR https://github.com/openxla/xla/pull/16975 This caused convergence issue for fp8 training, tested on GPT3 models: Before:  After:  Copybara import of the project:  81af29c8667792fe9ed189ab55308ca6e83859d4 by Elfie Guo : Add a few related optimization pass for fp8 gemm rerwriter. Merging this change closes CC(libcublas.so 9.0 cannot open shared object file { CUDA 8.0 + GeForce 940MX}) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16975 from elfiegg:pass 81af29c8667792fe9ed189ab55308ca6e83859d4",2024-10-10T19:33:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/77579
yi,copybara-service[bot],Move __HIP_DISABLE_CPP_FUNCTIONS__ to BUILD file,Move __HIP_DISABLE_CPP_FUNCTIONS__ to BUILD file This unbreaks the ROCm build which got broken by uncluding hip_runtime.h before rocm_driver_wrapper.h Applying the preprocessor definition on the build system level should avoid the issue in the future.,2024-10-10T14:49:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77569
yi,jhoydis,tf.custom_gradient for function with kwarg shows unexpected behavior," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.17.0  Custom code No  OS platform and distribution Ubuntu 22.04.5 LTS  Mobile device _No response_  Python version 3.12.7  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.3/8  GPU model and memory _No response_  Current behavior? I have a function that takes two tensors as inputs, one as argument and one as keyword argument. The function has a custom gradient. When ``tape.gradient`` for both input tensors with respect to the output of the function is called, TensorFlow throws an error, saying that only one gradient is expect and not two. When the function is called with both inputs as arguments (and not one of them as kwarg), no error is thrown.  Standalone code to reproduce the issue   Relevant log output ",2024-10-10T10:18:54Z,stat:awaiting tensorflower type:bug comp:ops 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/77559,I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version. I faced the same issue. Please find gist here for reference. Thank you!
yi,copybara-service[bot],PR #18065: Pass Ordering Test for GPU Compiler,PR CC(Feature request : warning for feeding unused values): Pass Ordering Test for GPU Compiler Imported from GitHub PR https://github.com/openxla/xla/pull/18065 Adds a class for testing the order of passes in the GPU compiler. The names of the passes expected to run first and last can be described by regular expressions. Also adds a test for verifying the order of the collective quantizer and collective pipeliner passes. Copybara import of the project:  2e3624d50ba100b9402a48aab4f3dca2e19208e4 by Philipp Hack : Adds a class for testing the order of passes in the GPU compiler.  3243bf2834052f9ec20b9b4ea0a8b078202a7c2e by Philipp Hack : Adds a class for testing the order of passes in the GPU compiler.  1b13ef048168072d771185f5e6b81b124c31ea2b by Philipp Hack : Adds a class for testing the order of passes in the GPU compiler. Merging this change closes CC(Feature request : warning for feeding unused values) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18065 from philipphack:u_ordering_test_xla 1b13ef048168072d771185f5e6b81b124c31ea2b,2024-10-10T07:52:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77457
yi,copybara-service[bot],Internal change only,Internal change only,2024-10-10T05:10:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77445
yi,copybara-service[bot],Trying to see what would fail.,Trying to see what would fail.,2024-10-09T20:54:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/77404
yi,copybara-service[bot],Move GpuDriver::GetDeviceMemoryInfo into CudaExecutor.,Move GpuDriver::GetDeviceMemoryInfo into CudaExecutor.,2024-10-09T18:15:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77394
yi,copybara-service[bot],Internal change only,Internal change only,2024-10-09T15:35:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77385
yi,copybara-service[bot],[XLA:GPU] Fix `dlpack_managed_tensor_to_buffer` function,"[XLA:GPU] Fix `dlpack_managed_tensor_to_buffer` function This CL passes the `stream` parameter to the underlying `CreateViewOfDeviceBuffer` function. Also turns on DLPack tests for the new API, because so far only legacy `dlpack_managed_tensor_to_buffer` was tested by some tests.",2024-10-09T13:12:30Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/77368,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
yi,therooler,Tensorflow Distributed AlltoAll," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version Tf 2.9  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi, I am trying to find a workaround, or ideally an implementation of the MPI AlltoAll primitive. As far as I can tell, Tensorflow has an AlltoAll op, but only for TPUs: https://www.tensorflow.org/api_docs/python/tf/raw_ops/AllToAll. In a distributed setup with 4 devices, this would allow me to go from   to   with a single communication call. I have managed to get this working by sharding with DTensor, but I'm trying to stay within the `tf.distribute` setting. Is there an easy workaround?  Standalone code to reproduce the issue   Relevant log output ",2024-10-09T13:09:12Z,type:feature comp:ops,open,0,0,https://github.com/tensorflow/tensorflow/issues/77367
yi,PowerToThePeople111,OOM when trying to save several concatenated datasets as one sharded dataset," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15.1  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version   GPU model and memory _No response_  Current behavior? Hi, I am trying to create a single, sharded (100 shards) tf.data.dataset from several tf.data.dataset(s) of overall 282GB size. Those separate datasets have been created by converting a list of parquet files. Each of those separate datasets should in the end be one of the shards of the overall dataset. I got a field in them (which has the same value for each example in a given dataset) that actually will be used in the savemethod as an argument which will determine the shard number. In order to generate the sharded dataset, I concat all separate ones and try to save them. What happens is that for the first 9 minutes several shards are being created without using more than 1% of the overall 128GB of memory on the node I got. There is absolutly no increase in memory usage visible. After that tho, memory usage suddenly starts to incr",2024-10-09T11:42:38Z,type:performance,closed,0,2,https://github.com/tensorflow/tensorflow/issues/77359,"Ok, I found a solution to the problem. It is to just skip the step of trying to save a sharded dataset and just directly feed the concatenated separate datasets to the model. At least 1 epoch of training on the complete set only brought me to 9% of memory usage. I hope it will stay that way. :) I think it is still quite surprising tho that saving a merged dataset seems to be harder than to train on it.",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Fix inaccurate RsqrtInt16 test,"Fix inaccurate RsqrtInt16 test ElementWise.RsqrtInt16 expects rsqrt(0.1) to be ~3.19407 from the reference kernel, while the correct answer is ~= 3.16228. This breaks some correctlyimplemented delegates. Fix the accuracy issue by using the right error range and the output produced from CPU. Note that this CL also renames some variable names for clarity. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17704 from gspschmid:gschmid/distcompression 99f4fa02cf9d0b6268f791a297fabb43a592799d",2024-10-09T07:28:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77335
yi,copybara-service[bot],Internal change only,Internal change only,2024-10-09T03:13:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77310
yi,isuchy,RuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab with Python 3.10.12  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): v2.17.0  2. Code   3. Failure after conversion After converting the model from ONNX using onnx2tf, I got saved_model, which I tried to convert to int8 quantized model using the code above. When trying to inference the model, after reading the model by the interpreter and calling the function `allocate_tensors()`   I get the following error:  Could someone give me some advice, suggestions on how to solve this error? I couldn't even find that anyone has solved the same problem.  The closest to this error is this issue, but the workaround is to convert the ONNX model to Keras and for my complex model it is not possible to fix.",2024-10-08T22:02:42Z,stat:awaiting tensorflower type:bug comp:lite TFLiteConverter 2.17,open,0,12,https://github.com/tensorflow/tensorflow/issues/77293,"Hi,  Thank you for bringing this issue to our attention, if possible could you please help us with your **saved_model** or Google colab notebook which you used to get `saved_model` so I'll try to replicate the same behavior from my end and will help you further ?  Thank you for your cooperation and patience.","Hello , Thank you for your fast response. I've uploaded the saved_model, please try in your environment to see if you can reproduce the error. ","Hi,   Thank you for providing the **saved_model** access, I'm able to reproduce the same error `RuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.` from my end and will need to dig more into this issue for reference here is gistfile Thank you for your cooperation and patience.","Hi,   Please take look into this issue. Thank you.","Hi, ,  have you had a chance to check it out? Does it look like a more serious problem with the XNNPACK runtime implementation?","Hi , can you convert to a pytorch model and use aiedgetorch? Let us know if that works better for you.","I'm running into something very similar with a simple MobileNetV3 based model:  Going to give aiedgetorch a shot in the meantime.. > UPDATE: Nope, aiedgetorch doesn't support int8 with a calibration dataset. So we're a bit stuck..","Hi , I ran your script and I noticed this model needs flexops... in what runtime environment are you running inference? Can you ensure your have followed direction here? https://ai.google.dev/edge/litert/models/ops_select","Hi , > I ran your script and I noticed this model needs flexops... in what runtime environment are you running inference? Can you ensure your have followed direction here? https://ai.google.dev/edge/litert/models/ops_select I ran the model inference in Google Colab and as it says here https://ai.google.dev/edge/litert/models/ops_selectpython, for LiteRT runtime it is enough to have the TensorFlow package installed. But later I would like to deploy the model on a Raspberry Pi 5, running on Python as well. > Hi , can you convert to a pytorch model and use aiedgetorch? Let us know if that works better for you. I also tried ai_edge_torch as you recommended, it worked well for converting from the Pytorch model, but its output is directly the TFLite format (.tflite), with no way to set parameters to perform posttraining int8 quantization. If I wanted to use tf.lite.TFLiteConverter, it does not support creating a TFLiteConverter object from a TFLite format model, only the following methods are supported   experimental_from_jax  from_concrete_functions  from_keras_model  from_saved_model So I haven't actually figured out how to quantize a float32 .tflite model to be able to perform inference on that model.","I was also able to replicate on tfnightly here. , can you please take a look? Thanks.  For the aiedgetorch workflow, you would need to do TFL quantization here.","Hi , I tried the quantization as you advised in your comment and I was also inspired by this article. >  For the aiedgetorch workflow, you would need to do TFL quantization here.  The model is saved as a tflite file after conversion and quantization, but several warnings are raised during the run.   I don't understand the warning message that the model is converted in training mode. I think the model is set in evaluation mode by calling `resnet50.eval()`.  Even calling `torch.ao.quantization.move_exported_model_to_eval(model)` should ensure that the GrapModule is exported to the eval mode. I know this is out of the question, but could you please give me some hints, if you have any experience with that, why is it like it happens? Thank you.","Hi , as far as I can tell you did it correctly, I think this is just a bug for now."
yi,faressc,fatal error: 'NEON_2_SSE.h' file not found - macOS x86_64 build tensorflowlite_c library," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution macOS  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version AppleClang 15.0.0.15000309  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When trying to build the tensorflowlite_c lib according to this guide, the build fails on macOS x86_64 plattform with the following error code:   Standalone code to reproduce the issue The error happens when I build using github workflows. This is the action run.  Relevant log output ",2024-10-08T15:02:52Z,stat:awaiting tensorflower type:build/install comp:lite subtype:macOS 2.17,open,0,6,https://github.com/tensorflow/tensorflow/issues/77264,"Hi,   Thank you for bringing this issue to our attention, I'm using below versions and steps but I'm getting different error if possible could you please help me with your versions and exact steps which you're following from official documentation which will be helpful to replicate same behavior from our end ? **Below are the versions I'm using:**  **I am following below steps :**  If I have missed something here please let me know. Thank you for your cooperation and patience.","Hi there, Thank you for your quick response! I am building tflite with the github runners. You can inspect all steps here: https://github.com/faressc/tfliteclib in the workflow file. Here is the versions the runner is using:  Bazel is not installed I guess, but I am using only cmake anyway (or does it need bazel under the hood?). Here is what steps I am following on the runner: ","Hi,  I apologize for the delayed response, I am able to replicate the same behavior from my end for reference I've added error log below so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention I really appreciate your valuable time and efforts **Here is error log for reference :**  Thank you for your cooperation and patience.","Hi,   Please take look into this issue. Thank you","I'm not able to replicate on M1, most likely specific to x86_64. , can you please take a look? Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.
yi,copybara-service[bot],Support specifying target SoC device architecture in Compiler Plugin API and in Qualcomm implementation,Support specifying target SoC device architecture in Compiler Plugin API and in Qualcomm implementation,2024-10-08T14:09:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77260
yi,Reyadeyat,"Bazel installer not run on recent Bazel version, kindly provide widly accepted CMake installer"," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.0  Custom code No  OS platform and distribution Linux linux 6.11.2arch11 CC(Add support for Python 3.x) SMP PREEMPT_DYNAMIC Fri, 04 Oct 2024 21:51:11 +0000 x86_64 GNU/Linux  Mobile device _No response_  Python version _No response_  Bazel version bazel 7.3.2  GCC/compiler version gcc (GCC) 14.2.1 20240910  CUDA/cuDNN version CUDA Version: 12.6   GPU model and memory NVIDIA GeForce RTX 3050  583MiB /   8192MiB  Current behavior? bazel is configured to version 6.5 on tensorflow git files that is not cabable to run because current version is 7.3.2 please provide CMake installer since this is a C based lib.  Standalone code to reproduce the issue   Relevant log output _No response_",2024-10-07T21:16:47Z,stat:awaiting response type:feature stale subtype:bazel 2.17,closed,0,3,https://github.com/tensorflow/tensorflow/issues/77176,", As per the official documentation, the latest tensorflow v2.17 supports the Bazel 6.5.0. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations https://www.tensorflow.org/install/sourcegpu Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
rag,copybara-service[bot],Skip `test_ragged_copy_on_host` if `xla_extension_version` < 290,Skip `test_ragged_copy_on_host` if `xla_extension_version` < 290,2024-10-07T20:50:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77174
chat,cdesouza-chromium,`SimpleDynamicBuffer::AddString` is calling `memcpy` with null data,I've noticed this hitting on our ubsan builds recently: ,2024-10-07T18:57:35Z,type:bug comp:lite TFLiteConverter awaiting PR merge,open,0,2,https://github.com/tensorflow/tensorflow/issues/77168, do you think you could help me getting this PR reviewed?,"Hi **chromium** , Apologies for the delay. Your PR is currently under review. Once it is merged, your issue should be resolved. Thank you!"
yi,copybara-service[bot],[xla:ffi] Add support for DenseElementAttr attributes.,"[xla:ffi] Add support for DenseElementAttr attributes. Because of https://github.com/openxla/stablehlo/issues/2121, array attributes are always serialized to DenseElementAttr types when passed via VHLO, which includes all users of JAX's plugin interface (i.e. all external GPU users). With this in mind, it's probably good to support the use of these types for specifying array attributes. This is one approach for doing that.",2024-10-07T16:46:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77157
yi,copybara-service[bot],Internal change only,Internal change only,2024-10-07T16:13:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77154
yi,dambeebu,DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed,"{ 	""name"": ""ImportError"", 	""message"": ""Traceback (most recent call last):   File \""c:\\Users\\OWNER\\anaconda3\\envs\\tf_env\\lib\\sitepackages\\tensorflow\\python\\pywrap_tensorflow.py\"", line 62, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."", 	""stack"": "" ImportError                               Traceback (most recent call last) File c:\\Users\\OWNER\\anaconda3\\envs\\tf_env\\lib\\sitepackages\\tensorflow\\python\\pywrap_tensorflow.py:62, in       61 try: > 62   from tensorflow.python._pywrap_tensorflow_internal import *      63  This try catch logic is because there is no bazel equivalent for py_extension.      64  Externally in opensource we must enable exceptions to load the shared object      65  by exposing the PyInit symbols with pybind. This error will only be      66  ",2024-10-07T14:46:02Z,stat:awaiting response stale,closed,0,3,https://github.com/tensorflow/tensorflow/issues/77150,"Hi @**dambeebu** , Could you please try using the latest version of TensorFlow 2.18.0rc1? I am providing the documentation for the latest version here. Please let us know if the issue still persists. We see that the issue template has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced.  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
gemma,copybara-service[bot],PR #17814: [ROCM] buffer_comparator init bugfix,"PR CC(Add resize_image_aspect_with_pad method): [ROCM] buffer_comparator init bugfix Imported from GitHub PR https://github.com/openxla/xla/pull/17814 This PR https://github.com/openxla/xla/pull/11880 created a latent bug on ROCM side which was really hard to track.  Due to gemm_algorithm_picker, the problem occurs only for nonzero beta when the output matrix is large enough (so it cannot be filled with two first runs). This results in buffer comparator errors like:  but in fact it was just because of uninitialized buffers. rotation could you please take a look ? Copybara import of the project:  58cd0e78dc19075e7c935d7cdb31676ce868e64c by Pavel Emeliyanenko : buffer_comparator init fix Merging this change closes CC(Add resize_image_aspect_with_pad method) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17814 from ROCm:ci_buffer_initialization_fix 58cd0e78dc19075e7c935d7cdb31676ce868e64c",2024-10-07T10:33:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/77140
gemma,copybara-service[bot],PR #17814: [ROCM] buffer_comparator init bugfix,"PR CC(Add resize_image_aspect_with_pad method): [ROCM] buffer_comparator init bugfix Imported from GitHub PR https://github.com/openxla/xla/pull/17814 This PR https://github.com/openxla/xla/pull/11880 created a latent bug on ROCM side which was really hard to track.  Due to gemm_algorithm_picker, the problem occurs only for nonzero beta when the output matrix is large enough (so it cannot be filled with two first runs). This results in buffer comparator errors like:  but in fact it was just because of uninitialized buffers. rotation could you please take a look ? Copybara import of the project:  58cd0e78dc19075e7c935d7cdb31676ce868e64c by Pavel Emeliyanenko : buffer_comparator init fix Merging this change closes CC(Add resize_image_aspect_with_pad method) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17814 from ROCm:ci_buffer_initialization_fix 58cd0e78dc19075e7c935d7cdb31676ce868e64c",2024-10-07T07:04:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/77126
yi,copybara-service[bot],Dynamic load api wrapper for compiler plugins. This is needed to access compiler plugin `.so` in driver code. Copying approach QNN uses.,Dynamic load api wrapper for compiler plugins. This is needed to access compiler plugin `.so` in driver code. Copying approach QNN uses.,2024-10-07T00:51:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/77119
yi,Adedeji-hub,A dynamic link library (DLL) initialization routine failed.,"ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred:",2024-10-05T14:35:44Z,stat:awaiting response stale,closed,0,3,https://github.com/tensorflow/tensorflow/issues/77089,"hub, Could you please provide the complete steps you followed to install the tensorflow and also please fill issue template from here which helps to debug the issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,Brandonn-Etheve,Can't compile Tensorflow 2.17 from source for cpu on fedora 40 : undefined reference, I'm trying to compile Tensorflow 2.17 on a new fresh install of Fedora40 lxqt desktop (official spin).  what i've donne (all command as root):   Fresh Fedora install   dnf update   reboot   dnf install python3devel g++ gcc cmake python3pip git    eigen3devel   pip install U user pip   pip install U  pip six numpy wheel setuptools mock   wget O bazel    https://github.com/bazelbuild/bazelisk/releases/download/v1.22.0/bazelisklinuxamd64   mv bazel /bin/   chmod 555 /bin/bazel   git clone https://github.com/tensorflow/tensorflow.git   cd tensorflow   git checkout r2.17   export TF_PYTHON_VERSION=3.12   export LD_LIBRARY_PATH=/usr/local/lib   ./configure  ::CSRMatMulOp(tensorflow::OpKernelConstruction*)' collect2: error: ld returned 1 exit status Target //tensorflow/tools/pip_package:wheel failed to build ``` **how do i solve : undefined reference to 'tensorflow::CSRMatMulOp::CSRMatMulOp(tensorflow::OpKernelConstruction\*)'?**   Version: Fedora 40 lxqt desktop kernel 6.10.11200.fc40.x86_64 (runing in virtualbox with: 8 cores and 23Gb of ram) gcc (GCC) 14.2.1 20240912 (Red Hat 14.2.13) g++ (GCC) 14.2.1 20240912 (Red Hat 14.2.13) Python 3.12.6 GNU Make 4.4.1 cmake version 3.28.2 Bazelisk version: v1.22.0 Bui,2024-10-04T22:01:41Z,stat:awaiting tensorflower type:build/install comp:core 2.17,open,0,2,https://github.com/tensorflow/tensorflow/issues/77072,Using the same configuration and procedure building 2.16 is working with pip_package:build_pip_package instead of pip_package:wheel. but it's like there no pip_package:build_pip_package in 2.17.,"Etheve, Could you please try whether this issue is happening with the latest tensorflow version 2.18.0rc1 and update if you are facing the same. Thank you!"
yi,copybara-service[bot],Internal change only,Internal change only FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/76831 from snadampal:compute_library_def_runtime_fix b3c62ba5777fb192ca612bca8c20fff9797f99b9,2024-10-04T18:06:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/77049
yi,richardwhitehead,NotImplementedError from tf.constant in trivial case," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Trying to make a tensor that has the same value for all items in the batch, see the following bare minimum code.  I get `NotImplementedError: cannot convert a symbolic tf.Tensor (custom_model_5_1/strided_slice:0) to a numpy array.` I am not trying to use numpy, this is an internal error.  Standalone code to reproduce the issue  shell { 	""name"": ""NotImplementedError"", 	""message"": ""Exception encountered when calling CustomModel.call(). Cannot convert a symbolic tf.Tensor (custom_model_5_1/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported. Arguments received by CustomModel.call():   • inputs=tf.Tensor(shape=(None, 1), dtype=float32)"", 	""stack"": "" NotImplementedError                       Traceback (most recent call last) Cell In[6], line 13  ",2024-10-04T13:38:25Z,type:bug TF 2.16,open,0,3,https://github.com/tensorflow/tensorflow/issues/77045,"I tried running your code on Colab using TensorFlow v2.16.1, 2.17.0 and the nightly version. I faced the same issue. Please find the gist here for reference. Thank you!","Hi **** , Apologies for the delay, and thank you for your patience. I am providing an alternative solution to your issue. In graph execution mode, TensorFlow expects operations to be symbolic and differentiable. The `tf.constant` function is not fully symbolic and cannot handle shapes that depend on tensors dynamically in graph mode, which might be causing the issue. To resolve this, you can use `tf.fill` or `tf.ones` multiplied by the desired value instead of `tf.constant`. These functions are compatible with dynamic shapes in graph execution. I tested these functions, and they worked for me. Please find the gist attached here for your reference. Thank you!","Thanks. Maybe a note in the documentation could be added?On 3 Jan 2025 06:35, Venkat6871 ***@***.***> wrote: Hi  , Apologies for the delay, and thank you for your patience. I am providing an alternative solution to your issue. In graph execution mode, TensorFlow expects operations to be symbolic and differentiable. The tf.constant function is not fully symbolic and cannot handle shapes that depend on tensors dynamically in graph mode, which might be causing the issue. To resolve this, you can use tf.fill or tf.ones multiplied by the desired value instead of tf.constant. These functions are compatible with dynamic shapes in graph execution. I tested these functions, and they worked for me. Please find the gist attached here for your reference. Thank you! —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you were mentioned.Message ID: ***@***.***>"
rag,copybara-service[bot],PR #74090: Removing distutils leftover,"PR CC(Removing distutils leftover): Removing distutils leftover Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/74090 This aims to finish CC(Remove all uses of distutils). The patch is untested but follow https://setuptools.pypa.io/en/latest/deprecated/distutilslegacy.html. I did not touch the install scripts. Thanks, Copybara import of the project:  fe3c0659eb168dc155f7d579255953dfc89b1387 by Alexis Praga : Removing distutils leftover  57c24de0b367dd3ba78211f08ef4397014cff48f by Alexis Praga : Fix linter error  9476971caa2e5805cecdfcad9b0f5a7f0e331d9e by Alexis Praga : Correcting import order for linter Shutil mainly  d74adacac6e544d75b6446d5565ec85bb4f25b6f by Alexis Praga : Fix conflict between sysconfig __init__py has :   from tensorflow._api.v2 import sysconfig which conflict with python sysconfig Merging this change closes CC(Removing distutils leftover) Reverts changelist a CL rolling back a previous version of this PR FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/74090 from apraga:nodistutil 52909d48fece0dec3c6b2e4b3673443bd83d46e3",2024-10-02T19:55:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76987
yi,copybara-service[bot],[XLA:GPU] Remove the now obsolete `--xla_gpu_enable_triton_hopper` flag.,"[XLA:GPU] Remove the now obsolete `xla_gpu_enable_triton_hopper` flag. MMA_V3 has been enabled by default, and this only gated varying the number of CTAs when autotuning matrix multiplications at this point. This also fixes a bug where the number of CTAs was not being autotuned when using exhaustive tiling.",2024-10-02T18:59:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76980
yi,copybara-service[bot],[XLA:CPU] Return error when trying to create a view of an unaligned buffer.,[XLA:CPU] Return error when trying to create a view of an unaligned buffer.,2024-10-02T14:05:31Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/76964,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
yi,p1x31,TensorFlow 2.17.0 fails to install with grpcio," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.17  Custom code No  OS platform and distribution Windows 11/ wsl  2.1.5.0  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 11.8  GPU model and memory _No response_  Current behavior? I'm trying to install tesnsorflow 2.17 under the same environment with protobuf==5.27.2 grpciotools==1.64 and getting this error: `tensorflowintel 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,=3.20.3, but you have protobuf 5.27.2 which is incompatible.` Downgrading protobuf is not a option for me, are there any workarounds this issue  Standalone code to reproduce the issue   Relevant log output ",2024-10-02T12:18:41Z,stat:awaiting response type:support stale 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/76957,"Hi **** , Thank you for raising the issue here. It is crucial to check all compatibility versions for everything to work smoothly. In your case, while downgrading the `protobuf` version is a common solution. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,tagunil,Add null pointer check for an output tensor calibration,"Otherwise it segfaults while trying to log the WHILE op. It's the simplest possible fix for CC(Calibrator segfaults trying to log the ""while"" operation). I believe that it makes sense to fix the crash now, and look for the proper way to fix the calibration logic later.",2024-10-02T11:33:43Z,awaiting review comp:lite size:XS prtype:bugfix,open,0,3,https://github.com/tensorflow/tensorflow/issues/76955," Hi , Can you please review this PR? Thank you !","Hi , Can you please review this PR? Thank you !","Hi , Can you please review this PR? Thank you !"
yi,chrism0dwk,Jit-compiling `tf.while_loop` inside `tf.vectorized_map` raises `InvalidArgumentError`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version v2.17.0  Custom code Yes  OS platform and distribution Colab  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? MRE  The following mockup of `cumsum` attempts to JIT compile a `tf.vectorized_map`ped function containing a `tf.scan`:  __Expected behaviour__: `vec_cumsum(xs_batched)` returns a batch of cumulative sums. __Actual behaviour__: Even though all data structures are known statically at JIT compile time, an InvalidArgumentError is raised with ""No registered 'TensorListReserve'"".  The fault is clearly related to `tf.scan`'s use of `tf.while_loop`, as a (longer) example using naked `tf.while_loop(..., max_iterations=n)` will confirm. In JAX, it is possible to jitcompile a `vmap`ped function containing a `lax.while_loop` indicating that this is possible in HLO.  It seems the `tf.function(jit_compile=True)` machinery may be mistranspiling to HLO somehow. May be related to CC(Gradient computation for `vectorized_map` nested inside `whil",2024-10-01T17:01:36Z,stat:awaiting tensorflower type:bug comp:apis comp:ops 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/76891,I tried running your code on Colab using the TensorFlow nightly version and encountered the same issue. Please find the gist here for reference. Thank you
yi,dependabot[bot],Bump the github-actions group with 5 updates,"Bumps the githubactions group with 5 updates:  Updates `actions/checkout` from 4.1.7 to 4.2.0  Release notes Sourced from actions/checkout's releases.  v4.2.0 What's Changed  Add Ref and Commit outputs by @​lucacome in actions/checkout CC(Installation Issue 0.7) Dependabot updates in actions/checkout CC(Add footnote about dropout in MNIST tutorial for expert) &amp; actions/checkout CC(How to get top N results for seq2seq?)  New Contributors  @​yasonk made their first contribution in actions/checkout CC(Add QOL improvements for tf.one_hot()) @​lucacome made their first contribution in actions/checkout CC(Installation Issue 0.7)  Full Changelog: https://github.com/actions/checkout/compare/v4.1.7...v4.2.0    Changelog Sourced from actions/checkout's changelog.  Changelog v4.2.0  Add Ref and Commit outputs by @​lucacome in actions/checkout CC(Installation Issue 0.7) Dependency updates by @​dependabot actions/checkout CC(Add footnote about dropout in MNIST tutorial for expert), actions/checkout CC(How to get top N results for seq2seq?)  v4.1.7  Bump the minornpmdependencies group across 1 directory with 4 updates by @​dependabot in actions/checkout CC(Fix links to nightly gpu build for python 3) Bump actions",2024-10-01T08:13:42Z,awaiting review ready to pull size:S dependencies github_actions,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76867
yi,copybara-service[bot],[XLA:SPMD] Fix ReshapeSharding for dimensions of size 1 and >1 partitions.,"[XLA:SPMD] Fix ReshapeSharding for dimensions of size 1 and >1 partitions. If there is a source dimension satisfying the following conditions, we replicate the source sharding along this dimension since the source sharding cannot be propagated along this dimension. 1. its size is 1 2. its partitions is greater than 1 3. there is no corresponding target dimension An example is shown below. Please refer to the added examples in hlo_sharding_util_test.cc. ",2024-10-01T00:22:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76839
rag,copybara-service[bot],[XLA:GPU] Improve error message for unsupported dot algorithms.,"[XLA:GPU] Improve error message for unsupported dot algorithms. The current error message can be a bit confusing because it will report that an algorithm is ""unsupported on the current device"" when the real problem is the input or output storage type. This change updates the error message to include a comment about the storage types.",2024-09-30T20:25:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/76827
yi,copybara-service[bot],PR #16520: [ROCM] ResetStream function for GemmAlgorithmPicker (BlasSupport interface),"PR CC(Periodic resample operation gradients and optimization): [ROCM] ResetStream function for GemmAlgorithmPicker (BlasSupport interface) Imported from GitHub PR https://github.com/openxla/xla/pull/16520 Here I added **ResetStream** function which sets the underlying stream for cublas/rocblas libraries to default stream 0. This is useful for GemmAlgorithmPicker which uses a temporary stream object for autotuning. In rocblas, **rocblas_set_stream** function is **persistent**, meaning that once the stream value is set, it will be used in all subsequent computations until new stream value is set.  In case of GemmAlgorithmPicker, we leave a **destroyed** stream object set into the math library. This does not produce any error behaviour but merely just a warning on ROCM side: ""Stream Capture Check Failed"". With this new ResetStream function, one can reset the stream value in GemmAlgorithmPicker destructor. Potentially, it can also be useful in other places where temporary stream value is used. Besides, I have also made some small code restructure for GemmAlgorithmPicker rotation: could you have a look please?  Copybara import of the project:  2bd0cf22c50b6724a7facd2471800dc31d1eb39d by Pavel Emeliyanenko : ",2024-09-30T19:29:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76820
rag,copybara-service[bot],copy the legalize tf passes under lite/stablehlo,copy the legalize tf passes under lite/stablehlo FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17737 from PragmaTwice:fixtypo 17575b22ef2fa43d9409c2b29ea408e3fc958155,2024-09-30T18:59:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/76817
yi,copybara-service[bot],PR #17704: [jax.distributed] Allow enabling grpc channel compression,"PR CC(atrous_conv2d doc fix newline): [jax.distributed] Allow enabling grpc channel compression Imported from GitHub PR https://github.com/openxla/xla/pull/17704 Allows passing an additional boolean argument `use_compression` via `xla_extension.get_distributed_runtime_client(...)` that controls whether compression is enabled on the gRPC channels created for each distributed runtime client. Motivation: XLA sends O(mesh) device topologies through its centralized coordination service and we have reason to believe that this becomes a bottleneck at large scale. Compression of the underlying gRPC communication is currently implicitly disabled, and might give us a cheap avenue to scale a bit further with the centralized KV store design. One small note: I refrained from adding `use_compression` to `DistributedRuntimeClient::Options` because the new flag is only relevant during channel creation in `distributed.cc`, but not within `DistributedRuntimeClient`. If we added `use_compression` to Options then the `GetDistributedRuntimeClient(channel, options)` defined in `client.cc` would seem to allow controlling compression, but it's really ignored. Let me know if you'd rather go that way. Corresponding JAX PR: https",2024-09-30T18:54:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/76816
rag,copybara-service[bot],PR #17737: [XLA:GPU] Rename `uint32_count` to `uint8_count` in `GPUDriver::AsynchronousMemsetUint8`,"PR CC(README.cmd typo.): [XLA:GPU] Rename `uint32_count` to `uint8_count` in `GPUDriver::AsynchronousMemsetUint8` Imported from GitHub PR https://github.com/openxla/xla/pull/17737 In `GpuDriver::AsynchronousMemsetUint8`, the count should be in bytes, instead of in uint32. Copybara import of the project:  17575b22ef2fa43d9409c2b29ea408e3fc958155 by PragmaTwice : [XLA:GPU] Rename uint32_count to uint8_count in GPUDriver::AsynchronousMemsetUint8 Merging this change closes CC(README.cmd typo.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17737 from PragmaTwice:fixtypo 17575b22ef2fa43d9409c2b29ea408e3fc958155",2024-09-30T17:02:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76809
transformer,KunduruJayasimhareddy,Multithreading is not working with teansorflow," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tensorflow==2.15.0.post1  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version python:3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am using bert model for classification and serving the model with gunicorn worker_class=gthreads, tf.config.threading.set_intra_op_parallelism_threads(1) tf.config.threading.set_inter_op_parallelism_threads(1) when I using above two line of code the code is working fine as expected and if increase the number to more than 1, the code getting blocked at the below line of code  Make predictions outputs = model_obj(inputs) and also inorder to reduce the docker image size I am using  RUN pip3 install torch==2.0.0+cpu f https://download.pytorch.org/whl/torch_stable.html before installing all the dependencies Flask==2.2.5 g2pen==2.1.0 gunicorn==21.2.0 jellyfish==1.0.3 kenlm==0.2.0 nltk==3.8.1 numpy==1.26.3 pandas==2.2.0 pythondotenv==1.0.1 requests==2.31.0 scikitlearn==1.4.0 semanticrouter==0.0.17 semanticr",2024-09-30T10:37:06Z,stat:awaiting tensorflower type:bug TF 2.15,open,0,3,https://github.com/tensorflow/tensorflow/issues/76794,also if I use the latest tensorflow I am getting below error terminate called after throwing an instance of 'std::runtime_error' askaipuncservice.1.iydr14592ivdTravelMateP21453  Aborted  (core dumped),"Hi **** , I tried running your code on Colab using TensorFlow 2.17.0 and the nightly version, and I did not encounter any issues. Please find the gist here for your reference. Thank you!",I am trying to serve using uvicorn and gunicorn and when I use more than 1 workers I am encountering that issue.
rag,Grizzzlyy,ExponentialMovingAverage doesn't work with KerasVariable," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.17.0  Custom code No  OS platform and distribution Windows 10  Mobile device _No response_  Python version 3.11.4  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior?  Description tf.train.ExponentialMovingAverage apply() method takes list of variables. It expects list of tf.Variable and doesn't work with list of KerasVariable because of dtypes tf.keras.layers.Dense trainable_variables property returns list of `KerasVariable`, KerasVariable dtype is `str` tf.Variable dtype is `tf.Dtype`  Expected behavior I suppose tf.keras layer or model should return list of KerasVariable, so tf.train.ExponentialMovingAverage.apply() should support list of KerasVariables as argument  Standalone code to reproduce the issue   Relevant log output ",2024-09-29T18:20:52Z,stat:awaiting response type:bug comp:apis 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/76771,", Tensorflow 2.17 contains Keras3.0 which might be the reason for the error. Could you please try to install tfkeras i.e., keras2.0 which the code was executed without any issues/error. Kindly find the gist of it here.  Thank you!","  Yep, that works. Thank you",", Glad the issue got resolved. Could you please feel free to move this issue to closed status? Thank you!",Are you satisfied with the resolution of your issue? Yes No
yi,x0w3n,Aborted (core dumped) in `tf.linalg.det/slogdet/logdet/cholesky/inv`, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0dev20240925  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.3 LTS (x86_64)  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? tf.linalg.det/slogdet/logdet/cholesky/inv triggered a crash when the input is empty. Note that this will only be triggered if the gpu is available.  Standalone code to reproduce the issue   Relevant log output ,2024-09-28T13:15:48Z,stat:awaiting tensorflower type:bug comp:ops 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/76730,", I tried to execute the mentioned code on both CPU and GPU, in CPU the code was executed with the error output and on GPU the same code was aborted. Kindly find the gist of it here !Screenshot 20241001 4 54 44 PM"
rag,x0w3n,Aborted (core dumped) in `tf.nn.max_pool/tf.nn.max_pool1d`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.3 LTS (x86_64)  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Under specific inputs, tf.nn.max_pool triggered a crash.  Standalone code to reproduce the issue    Relevant log output ",2024-09-28T12:28:21Z,stat:awaiting tensorflower type:bug comp:ops 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/76722,"I tried running your code on Colab using TensorFlow v2.17.0 and the nightly version with both CPU and GPU, With the CPU, it is not crashing, but with the GPU, I faced the same issue. Please find the gist1, gist2 here for reference.  Thank you!"
yi,x0w3n,A crash is triggered when unknown flags are set., Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0dev20240925  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.3 LTS (x86_64)  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? A crash is triggered when unknown flags are set.  Standalone code to reproduce the issue   Relevant log output ,2024-09-28T12:03:04Z,stat:awaiting response type:bug stale comp:xla 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/76717,", As mentioned when the unknown flags are set for **TF_XLA_FLAGS**  it was as providing the error **""Unknown flags in TF_XLA_FLAGS: tf_xla_force_host_platform_device_count=8""** which was expected. Could you please provide the opinions on the same to debug the issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Increase device count to support 2x2x2 topology,Increase device count to support 2x2x2 topology,2024-09-28T01:19:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/76695
yi,copybara-service[bot],hlo_runner_pjrt: Have PjRtWrappedExecutable own the underlying executable.,hlo_runner_pjrt: Have PjRtWrappedExecutable own the underlying executable. Avoids a memory leak in various unit tests currently masked by not actually looking for leaks.,2024-09-27T21:16:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76681
yi,copybara-service[bot],"In a previous change, we throw an error if we encounter an unknown sharding when saving shardings for instructions. However, this ingored the fact that we deliberately replace some module parameter/root shardings with unknown sharding objects. This CL makes the condition tigher so we only throw an error when we encounter unknown sharding objects intended for shard_as or shard_like annotations, which was the original intention anyway.","In a previous change, we throw an error if we encounter an unknown sharding when saving shardings for instructions. However, this ingored the fact that we deliberately replace some module parameter/root shardings with unknown sharding objects. This CL makes the condition tigher so we only throw an error when we encounter unknown sharding objects intended for shard_as or shard_like annotations, which was the original intention anyway.",2024-09-27T20:58:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76679
yi,feranick,Crash when calling TFSMLayer object during TF_lite conversion," 1. System information  Linux Ubuntu 24.04  TF 2.17.0 (pip) and Keras 3.5.0  2. Code A TF model is first exported using Keras 3.5.0:  Then when the following method is called below, the conversion crashes in TF 2.17.0 (log below). It works fine with TF 2.16.2.  The error with TF 2.17.0: ",2024-09-27T19:44:16Z,comp:lite TFLiteConverter 2.17,open,0,13,https://github.com/tensorflow/tensorflow/issues/76675,"Hi,  Thank you for bringing this issue to our attention, I was trying to replicate the same behavior from my end with `TensorFlow version 2.17.0 ` and I did not encounter error which is mentioned in the issue template for your reference I have attached gistfile Please let me know if I have missed something here. Thank you for your cooperation and patience.","Thanks for the quick reply. Please find the gistfile with the code to convert a TF model to a tflite. It works on TF2.16.2, not on TF 2.17.0. (The same script in github is available here).  Attached here are the zipped models created with tf2.16.2 and tf2.17.0. After unzipping, you can try to convert them using the script above. Again, conversion of either TF model works with this script (which suggests that how you made the model doesn't seem to be the issue) when using TF on 2.16.2 but not with TF 2.17.0 (with error in this report).  k3_tf_2.16.2_model_regressor.zip k3_tf_2.17.0_model_regressor.zip Please let me know if you need anything else.",The issue is still present in TF nightly (2024/10/01) provisional v. 2.19.0 and 2.18.0rc0,"Tagging: https://github.com/tensorflow/tensorflow/tree/v2.18.0rc0 Tagging ""v2.18.0rc0""","Hi,   I apologize for the delayed response, I was trying to replicate the same behavior from my end with your this script but I'm getting different error so if possible could you please give me access to your gistfile to replicate same behavior from my end ?  Thank you for your cooperation and patience.","Thanks and no worries. You should be able to access the gistfile. Just run it, and the error should appear.  I used the colab AI feature that suggested a completely different way to do the conversion, that does not run on Keras but it is built in TF. Anyway, I added both version of the method *BROKEN"" and ""SUGGESTED"" and you can comment/uncomment the last line to run either one. The Broken one is the one that triggers the issue. Please note that there is nothing special about the code in the broken call, I am using code that used to work fine in TF 2.16.2 and it is the one suggested.","Hi,   Thank you for providing the access to your gistfile and I'm able to replicate the similar behavior from my end `AttributeError: 'TFSMLayer' object has no attribute '_get_save_spec'`so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention.  Thank you for your cooperation and patience.","Hi,  Please take look into this issue. Thank you","I was able to replicate as above, this issue did happen before in more general cases (partially caused by Keras 2 to 3). Hi , can you please take a look?","Hi   👋 , do you have a chance to look into this issue? I also have the `'_get_save_spec'.`  problem.  Thanks!","Hi , , what's the whole workflow expected here? Reason I ask is the preferred workflow now would be to save in PyTorch (or perhaps pytorchbacked Keras), reload the model in PyTorch, then convert using aiedgetorch. Can you guys switch to that workflow or is there a reason you guys can't?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"> Hi [](https://github.com/feranick), [](https://github.com/jmarrietar), what's the whole workflow expected here? Reason I ask is the preferred workflow now would be to save in PyTorch (or perhaps pytorchbacked Keras), reload the model in PyTorch, then convert using aiedgetorch. Can you guys switch to that workflow or is there a reason you guys can't? I have my whole coding infrastructure based on a tensorflowbackend with keras frontend. You are suggesting to move to PyTorch completely, which isn't feasible on our end at the moment as our nonquantized models are all TF based. Furthermore, as we still rely on edgeTPUs it is also not clear whether aiedgetorch would be any helpful, and as far as I know it still requires TFlite.  As an aside: there is a lot of uncertainty on the fate of EdgeTPU future. Google seems to implying that support is dead. And yet ASUS is happy to still sell it for business customers, yet with no support whatsoever (I can provide interesting email exchanges with Asus business customer support if needed). Grassroot attempts to keep it alive (including myself) are compensating for this, but clearly it's becoming increasingly difficult to understand the status of the device support, or in general, how much one should rely in tensorflow for TFlitelike quantized modeling. Confusing documentation, stale repos, conflicting information ever changing directives certainly don't help planning. Definite clarity would be much appreciated where Google wants to go."
rag,copybara-service[bot],#sdy Merge XLA `CallInliner` and `ShardyCallInliner`.,"sdy Merge XLA `CallInliner` and `ShardyCallInliner`. Now that Shardy will now be fully integrated, we should delete the `ShardyCallInliner` and update `CallInliner` to look for what `ShardyCallInliner` checks for. We've had two bugs because of this thus far. Reverts changelist a CL rolling back a previous version of this PR FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/74090 from apraga:nodistutil 52909d48fece0dec3c6b2e4b3673443bd83d46e3",2024-09-27T18:18:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76674
yi,nishchal-v,TensorFlowLiteSwift iOS 18 specific issues of bad memory access,"Pod versions:  TensorFlowLiteSwift: 2.17.0  TensorFlowLiteSelectTfOps: 0.0.1nightly.20240927 or 2.17.0 Reproducible on: iOS 18 (Sim and Physical device) Followed this guide for TensorFlow operators. Load the model using below code  This works just fine  When trying to load the model from local disk storage using below code,  Getting memory address error while invoking `interpreter.allocateTensors()` > Thread 14: EXC_BAD_ACCESS (code=1, address=0x7fc453e9d338). Stack trace of 'TensorFlowLite.Interpreter.allocateTensors()' ",2024-09-27T16:24:46Z,stat:awaiting response type:support stale comp:lite iOS 2.17,closed,0,6,https://github.com/tensorflow/tensorflow/issues/76669,"The solution for now is to use  Still, the initialisation using model data is an issue.","Hi,   Please take look into this issue. Thank you","Hi v, can you please share the model which produces this error? If you wish a reduced/smaller model which reproduces the issue is sufficient. Thanks for your help.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #17319: Fixes XLA build with numpy>=2.1.0,PR CC(Model trained with `tf.data.Dataset` can not be converted to `dlc` file of snpe): Fixes XLA build with numpy>=2.1.0 Imported from GitHub PR https://github.com/openxla/xla/pull/17319 When building XLA using the command from dev guide: docs/developer_guide.md  Using numpy==2.1.0 we can have the following linking error:  Which should be related to https://github.com/numpy/numpy/blob/main/doc/source/release/2.1.0notes.rstapisymbolsnowhiddenbutcustomizable This PR fixes the build issue Copybara import of the project:  2f6e1b3e7e1bb189a1b9b5a9e4a94e60bd116a9d by vfdev5 : Fixes XLA build with numpy>=2.1.0 When building XLA using the command from dev guide: docs/developer_guide.md  Using numpy==2.1.0 we can have the following linking error:  Which should be related to https://github.com/numpy/numpy/blob/main/doc/source/release/2.1.0notes.rstapisymbolsnowhiddenbutcustomizable This PR fixes the build issue Merging this change closes CC(Model trained with `tf.data.Dataset` can not be converted to `dlc` file of snpe) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/17319 from vfdev5:fixbuildnumpy2.1.0andlater 2f6e1b3e7e1bb189a1b9b5a9e4a94e60bd116a9d,2024-09-27T14:47:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76665
yi,copybara-service[bot],PR #16520: [ROCM] ResetStream function for GemmAlgorithmPicker (BlasSupport interface),"PR CC(Periodic resample operation gradients and optimization): [ROCM] ResetStream function for GemmAlgorithmPicker (BlasSupport interface) Imported from GitHub PR https://github.com/openxla/xla/pull/16520 Here I added **ResetStream** function which sets the underlying stream for cublas/rocblas libraries to default stream 0. This is useful for GemmAlgorithmPicker which uses a temporary stream object for autotuning. In rocblas, **rocblas_set_stream** function is **persistent**, meaning that once the stream value is set, it will be used in all subsequent computations until new stream value is set.  In case of GemmAlgorithmPicker, we leave a **destroyed** stream object set into the math library. This does not produce any error behaviour but merely just a warning on ROCM side: ""Stream Capture Check Failed"". With this new ResetStream function, one can reset the stream value in GemmAlgorithmPicker destructor. Potentially, it can also be useful in other places where temporary stream value is used. Besides, I have also made some small code restructure for GemmAlgorithmPicker rotation: could you have a look please?  Copybara import of the project:  2bd0cf22c50b6724a7facd2471800dc31d1eb39d by Pavel Emeliyanenko : ",2024-09-27T14:40:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/76664
yi,copybara-service[bot],PR #17704: [jax.distributed] Allow enabling grpc channel compression,"PR CC(atrous_conv2d doc fix newline): [jax.distributed] Allow enabling grpc channel compression Imported from GitHub PR https://github.com/openxla/xla/pull/17704 Allows passing an additional boolean argument `use_compression` via `xla_extension.get_distributed_runtime_client(...)` that controls whether compression is enabled on the gRPC channels created for each distributed runtime client. Motivation: XLA sends O(mesh) device topologies through its centralized coordination service and we have reason to believe that this becomes a bottleneck at large scale. Compression of the underlying gRPC communication is currently implicitly disabled, and might give us a cheap avenue to scale a bit further with the centralized KV store design. One small note: I refrained from adding `use_compression` to `DistributedRuntimeClient::Options` because the new flag is only relevant during channel creation in `distributed.cc`, but not within `DistributedRuntimeClient`. If we added `use_compression` to Options then the `GetDistributedRuntimeClient(channel, options)` defined in `client.cc` would seem to allow controlling compression, but it's really ignored. Let me know if you'd rather go that way. Corresponding JAX PR: https",2024-09-27T14:13:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76662
rag,copybara-service[bot],[XLA:TPU] Only restore the original heap state for early forced prefetches that were not allocated. This was a bug in the original change to enable fragmentation aware loop optimizer. It was also a bug in the earlier loop optimizer which was causing some prefetches to be pushed later instead of earlier.,[XLA:TPU] Only restore the original heap state for early forced prefetches that were not allocated. This was a bug in the original change to enable fragmentation aware loop optimizer. It was also a bug in the earlier loop optimizer which was causing some prefetches to be pushed later instead of earlier.,2024-09-27T08:01:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/76634
rag,copybara-service[bot],[XLA:TPU] Reenable fragmentation aware loop optimizer. Only restore the original heap state for early forced prefetches that were not allocated. This was a bug in the original change to enable fragmentation aware loop optimizer. It was also a bug in the earlier loop optimizer which was causing some prefetches to be pushed later instead of earlier.,[XLA:TPU] Reenable fragmentation aware loop optimizer. Only restore the original heap state for early forced prefetches that were not allocated. This was a bug in the original change to enable fragmentation aware loop optimizer. It was also a bug in the earlier loop optimizer which was causing some prefetches to be pushed later instead of earlier. Reverts 343058c782ebe85b30c2b46e70d471febcbca8cf,2024-09-27T08:01:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76633
yi,Ashraful-Dowla,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed., Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17  Custom code Yes  OS platform and distribution Windows 10  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior?  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the a,2024-09-27T05:13:34Z,stat:awaiting response type:build/install subtype:cpu-intel 2.17,closed,0,7,https://github.com/tensorflow/tensorflow/issues/76627,I am also facing exactly the same error.,> I am also facing exactly the same error. I also couldn't resolve the issue. I need a dataset from that library. I found another way to import the dataset.,"Hi **Dowla** , Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. https://github.com/tensorflow/tensorflow/issues/61887 Thank you!","Hello Venkat, My PC does not support AVX2 instructions. Is there alternative to use tensorflow library in my PC?","Hi **Dowla** , Apologies for the delay. According to the documentation, there is no alternative; you must follow AVX2 instructions for optimal results. Thank you!",Thanks for your reply !,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #23853: Enable the activation offloading test,PR CC(Graph optimized using tf.contrib.tensorrt is not loadable with TF_GraphImportGraphDef): Enable the activation offloading test Imported from GitHub PR https://github.com/jaxml/jax/pull/23853 This test ActivationOffloadingTest.test_remat_scan_layout_change_offloadable can be enabled after XLA PR 17500 is in. This test is also a small reproducer for MaxText activation offloading rematpolicy=qkv_proj_offloaded. Copybara import of the project:  adaf54a4bbe10ce05edcfeb29039c6948444c641 by Jane Liu : enable the activation offloading test Merging this change closes CC(Graph optimized using tf.contrib.tensorrt is not loadable with TF_GraphImportGraphDef) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/jaxml/jax/pull/23853 from zhenyingliu:rematscan adaf54a4bbe10ce05edcfeb29039c6948444c641,2024-09-27T00:14:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76604
yi,copybara-service[bot],Internal change only,Internal change only,2024-09-26T12:33:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/76570
rag,saagar-pat,Updated README.md,"Updated tensorflow/tensorflow README.md file on 9/25/2024 with the following:  Included additional details in the introduction to inform newcomers of the power and possible applications of TensorFlow.  Added in a Table of Contents and linked to each heading and the subsequent subsections within Install.  Added detail to the Install section, included enabling GPU support, Docker installation commands, and building from source details and link to documentation.  Included other applications of TensorFlow (LiteRT, TensorFlow.js, Serving) and linked proper documentation sites/installation guides with description for greater clarity.  Added message in contributing section and link to good first issue label for clarity and to encourage first time contributors.",2024-09-26T03:33:01Z,size:M,closed,0,2,https://github.com/tensorflow/tensorflow/issues/76531,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi pat, Can you please sign CLA , thank you."
yi,copybara-service[bot],[XLA:Python] Avoid copying an nb::detail::dict_iterator.,"[XLA:Python] Avoid copying an nb::detail::dict_iterator. Nanobind 2.2.0 makes dict iterators uncopyable. In addition, avoid a possible exceptionsafety problem where Python .equals() was called from an equality test used by an ABSL hash table.",2024-09-26T00:14:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76520
yi,copybara-service[bot],[XLA] Introduce outfeed sanity,"[XLA] Introduce outfeed sanity Today mismatches between outfeed enqueue and dequeue shapes either cause device hangs or silent data corruption. Users have no way to triage these issues themselves, as they would need to look into the internal state of the outfeed queue to discover that something went wrong. The consumer can validate at runtime that its trying to dequeue the same type of literal that the producer enqueues. This can be done by having both sides communicate on the shape of the literal in the queue. To do this, we need upper levels to pass down more information about the literal they are trying to dequeue.",2024-09-25T23:58:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/76519
yi,copybara-service[bot],Add a pass to nest gemm fusions.,Add a pass to nest gemm fusions. This pass takes a fusion with a single dot and creates nested fusions for the two dot operands. The nested fusions are annotated with block_level_fusion_config specifying the tile sizes as propagated from the output through the dot to the operands. This pass is not hooked up yet other than for one initial test. The next step is to add more complex test cases and extend the implementation to handle those.,2024-09-25T18:08:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76493
yi,copybara-service[bot],hlo_runner_pjrt: Have PjRtWrappedExecutable own the underlying executable.,hlo_runner_pjrt: Have PjRtWrappedExecutable own the underlying executable. Avoids a memory leak in various unit tests.,2024-09-25T17:21:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/76488
yi,copybara-service[bot],XLA:GPU Fix data type for normalization of FFTs for F64 and C128 types.,"XLA:GPU Fix data type for normalization of FFTs for F64 and C128 types. The current normalization ""scale_factor"" is stored as a float, but it should be a double for kZ2ZInverse and kZ2D. This change caches the denominator as a uint64 in the plan, and then takes the inverse in the appropriate type only when applying the normalization. Reported in https://github.com/jaxml/jax/issues/23827, when compared to numpy FFTs. I've tested that this fixes the issue reported there, but I'm not sure where would be best to add a test in XLA.",2024-09-25T15:34:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76481
llm,copybara-service[bot],[IFRT] Add pass for populating atom program metadata.,[IFRT] Add pass for populating atom program metadata. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15144 from terryysun:terryysun/all2all_memcpyp2p f9b75b0e088286ded770b27fff9d020f8e85a648,2024-09-25T13:30:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76472
yi,arun-kumark,Failed to synchronize the stop event: CUDA_ERROR_UNKNOWN: unknown error,"Hello together, I am trying to run the newbenchmark AI for my new platform: https://pypi.org/project/newaibenchmark/ My System configuration is as follows: *  TF Version: 2.17.0 *  Platform: Linux6.8.045genericx86_64withglibc2.35 *  CPU: 13th Gen Intel(R) Core(TM) i913900E *  CPU RAM: 62 GB *  GPU/0: NVIDIA L4 *  GPU RAM: 20.3 GB *  CUDA Version: 12.4 *  CUDA Build: V12.4.131 arun:~$ nvidiasmi Wed Sep 25 12:52:43 2024        ++  ++ While executing the program, at any point of time while running, it fails to complete.  In another instance, there is another reason for failure:  Could you please help in resolving the issue?  Thank you Arun",2024-09-25T10:01:45Z,type:build/install subtype: ubuntu/linux 2.17,open,0,2,https://github.com/tensorflow/tensorflow/issues/76456,"Hi **kumark** , Apologies for the delay. There seems to be a version mismatch, please check the version compatibility. I am providing the documentation for your reference. Could you also please explain the exact steps you followed? Thank you!","Hi Venkat,  I am setting up the environment on my freshly installed Ubuntu 22.04LTS using the Lambda: https://lambdalabs.com/blog/nvidiangctutorialrunpytorchdockercontainerusingnvidiacontainertoolkitonubuntu This container set up the CUDA, TensorFlow and CuDNN in reliable way. I prepared two setups, one with the L4 Nvidia GPU card and another on my ZBook with RTX3000 GPU. On RTX3000 GPU notebook, the things go smooth, and benchmarking test passes. While running exactly the same scripts, they never completes and break away in the mid. I am using the following repository to test the Ai benchmarking score: https://pypi.org/project/newaibenchmark/ Do you have further comments to test/share. Thank you for the document, yes my setup meets the matching installation with Lambda: __ My Python version is 3.10.12, which comes by default with Ubuntu 22.04LTS. But the same version works well on other notebook. thanks  Arun"
rag,copybara-service[bot],PR #15144: [NVIDIA GPU] Use memcpy for intra-node all-to-all,"PR CC(Error_Converting_TliteFormat): [NVIDIA GPU] Use memcpy for intranode alltoall Imported from GitHub PR https://github.com/openxla/xla/pull/15144 The communications of alltoall rely on NCCL even when it is intranode. By leveraging memcpy for intranode communications, alltoall can have better performance while reducing SM consumption (right now consumed by NCCL).  Copybara import of the project:  38720c73f5817dbbf5b6d98751140bb53f572690 by Terry Sun : memcpyp2p for local a2a  90018f4a3fe0ed3018767db810518faf9435bc93 by Terry Sun : use nccl to pass recv ptrs  f9b75b0e088286ded770b27fff9d020f8e85a648 by Terry Sun : refactor and cleanup Merging this change closes CC(Error_Converting_TliteFormat) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15144 from terryysun:terryysun/all2all_memcpyp2p f9b75b0e088286ded770b27fff9d020f8e85a648",2024-09-25T09:47:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76454
chat,copybara-service[bot],PR #16882: Symlink hermetic cuda headers to permit clang cuda version detection,PR CC(Compilation error when building tfcompile on Windows / Visual Studio 2017): Symlink hermetic cuda headers to permit clang cuda version detection Imported from GitHub PR https://github.com/openxla/xla/pull/16882 Fixes CC(未找到相关数据) Copybara import of the project:  1ff356ac0870002b369c3ec09547aae2a62c70e2 by tchatow : Symlink hermetic cuda headers to permit clang cuda version detection Fixes CC(未找到相关数据) Merging this change closes CC(Compilation error when building tfcompile on Windows / Visual Studio 2017) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16882 from tchatow:patch1 1ff356ac0870002b369c3ec09547aae2a62c70e2,2024-09-25T08:17:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76446
rag,copybara-service[bot],PR #15144: [NVIDIA GPU] Use memcpy for intra-node all-to-all,"PR CC(Error_Converting_TliteFormat): [NVIDIA GPU] Use memcpy for intranode alltoall Imported from GitHub PR https://github.com/openxla/xla/pull/15144 The communications of alltoall rely on NCCL even when it is intranode. By leveraging memcpy for intranode communications, alltoall can have better performance while reducing SM consumption (right now consumed by NCCL).  Copybara import of the project:  38720c73f5817dbbf5b6d98751140bb53f572690 by Terry Sun : memcpyp2p for local a2a  90018f4a3fe0ed3018767db810518faf9435bc93 by Terry Sun : use nccl to pass recv ptrs  f9b75b0e088286ded770b27fff9d020f8e85a648 by Terry Sun : refactor and cleanup Merging this change closes CC(Error_Converting_TliteFormat) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15144 from terryysun:terryysun/all2all_memcpyp2p f9b75b0e088286ded770b27fff9d020f8e85a648",2024-09-24T20:51:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/76409
yi,copybara-service[bot],[JAX] Temporarily release GIL while destroying ifrt::LoadedExecutable inside PyLoadedExecutable,[JAX] Temporarily release GIL while destroying ifrt::LoadedExecutable inside PyLoadedExecutable,2024-09-24T20:31:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/76408
yi,copybara-service[bot],[XLA] Introduce unrolling as a way to eliminate loop aliasing copies,"[XLA] Introduce unrolling as a way to eliminate loop aliasing copies Pipelined loops have inherent aliasing interference in them, due to loop inputs shift positions across iterations. This results in copy insertion adding copies for each pipelined input. In some cases extra copies on top of this are needed to properly sequence all the mandatory aliasing copies. Overall, this type of overhead makes pipelined loops a nonviable choice in HLO. It is not necessary to insert copies to resolve interference in this case. The loop inputs, despite directly carried out as loop outputs, still have finite lifetimes across a certain amount of loop iterations. If the loop was unrolled just enough times to have the lifetimes of its inputs end before the outputs would be materialized, this would implicitly remove any sort of interference. The drawback of this approach is that it can in some cases drastically increase compile times due to linearly increasing graph size. Add an option to CopyInsertion to perform this type of loop preprocessing before any copies would be inserted. Users can opt in if they want to trade compile time for execution time.",2024-09-24T18:23:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76402
yi,copybara-service[bot],PR #17500: Move HostOffloadLegalize before LayoutNormalization for GPUs,"PR CC('colocate_gradients_with_ops' colocate with unused ops): Move HostOffloadLegalize before LayoutNormalization for GPUs Imported from GitHub PR https://github.com/openxla/xla/pull/17500 Fix ActivationOffloadingTest.test_remat_scan_layout_change_offloadable in JAX. The test in memories_test.py failed with an INVALID_ARGUMENT error:  A tensor moved to host (from ""dynamicupdateslice.13"") was used by an    instruction (""transpose.32"") not acceptable during pure memory offload. Root cause:  LayoutNormalization inserts a transpose  AlgebraicSimplifier replaces certain transposes with bitcast transposes  These transposes/bitcasts are invalid in host memory offloading segments Solution: Move HostOffloadLegalize before LayoutNormalization to prevent this issue. Copybara import of the project:  107d6b462084331f7366e5ae60c150dce090bf14 by Jane Liu : Move HostOffloadLegalize before LayoutNormalization for GPUs  f0fb7347a1bb61370a29e19230d87a2161c29ef7 by Jane Liu : Add comments to explain the pass order  30d2b4450be58ec32ca29f98dc7b822e80ec09df by Jane Liu : Add the test to validate the pass order Merging this change closes CC('colocate_gradients_with_ops' colocate with unused ops) FUTURE_COPYBARA_INTEGRATE_REV",2024-09-24T10:22:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76380
yi,copybara-service[bot],Remove unused gpu_types dependency from topk_kernel_gpu target,Remove unused gpu_types dependency from topk_kernel_gpu target This change also uncovered that `:topk_kernel_gpu` was relying on `:gpu_types` for its dependency on CUDA headers. `:topk_kernel_gpu` depends on CUDA headers because clang automatically includes `cuda_runtime.h` in all CUDA compilation units. To fix that properly I'm adding the dependency to `:cuda_headers` inside the `cuda_library` macro.,2024-09-23T11:25:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76312
yi,deepeshfujitsu,Build Failure on AWS Graviton3 with Custom oneDNN (oneDNN-3.6-rc): Invalid Preprocessing Directives in dnnl_config.h," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf v2.17.0  Custom code No  OS platform and distribution Ubuntu 22.04.2 LTS  Mobile device _No response_  Python version 3.10.12  Bazel version 6.5.0  GCC/compiler version gcc version 11.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am unable to build TensorFlow with the latest oneDNN or custom oneDNN (oneDNN3.6rc) on AWS Graviton3 (aarch64) CPU. The build process fails with several compilation errors related to invalid preprocessing directives in the dnnl_config.h file. I expected the build to complete successfully with the custom oneDNN settings, allowing TensorFlow to run efficiently on the AWS Graviton3 (aarch64) architecture.  Standalone code to reproduce the issue   Relevant log output ",2024-09-23T11:04:35Z,stat:awaiting tensorflower type:build/install comp:mkl subtype: ubuntu/linux 2.17,open,0,2,https://github.com/tensorflow/tensorflow/issues/76311,**toplay** ,  The latest supported version of oneDNN is 3.5 ( CC(PR 70765: [oneDNN] Update oneDNN library to v3.5))  We have not looked into building TF with oneDNN 3.6 yet. Will provide a solution if/when we find one.
yi,copybara-service[bot],[XLA:GPU] Avoid copying Shape in HloRematerialization,[XLA:GPU] Avoid copying Shape in HloRematerialization This CL avoids copying Shape in HloRematerialization by returning a pointer to the Shape in the internal cache. This change is expected to improve 30% the performance of HloRematerialization.,2024-09-23T07:53:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76304
yi,copybara-service[bot],Internal changes only,Internal changes only,2024-09-23T07:30:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/76303
yi,obinna-Muonanu,Importing tensorflow,"ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[1], line 9       7 from sklearn.metrics import r2_score,mean_squared_error       8 from sklearn.pipeline import Pipeline > 9 import tensorflow as tf File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:38      35 import sys as _sys      37  Do not remove this line",2024-09-22T07:36:43Z,stat:awaiting response type:build/install stale,closed,0,6,https://github.com/tensorflow/tensorflow/issues/76274,I got this error when trying to import tensorflow,Please search for similar issues in the repo.,"Hi **Muonanu** , We see that the issue template has not been filled, could you please do so as it helps us analyze the issue tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced. Some similar issues are being discussed, and I am providing those [links, link2 here for your reference. Please go through them. I hope they will be helpful for you. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
text generation,Jisencc,TensorFlow keeps creating threads when multi-GPU training （thread leak）," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.11.0  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 11.4  GPU model and memory Nvidia A800   Current behavior? I was using this machine to train a GPT2 example from (the data I used can also be found in this link, also here https://github.com/chinesepoetry/chinesepoetry.git) https://keras.io/examples/generative/gpt2_text_generation_with_kerasnlp/ Before we start, I would give a baseline amount of this machine's threads : !10 When starting with the multiGPU training of tensorflow by calling the tf.distribute.MirroredStrategy, the training process worked fine as usual.  But with the time went by, I found the amount of threads increased with the training process going, here is the evidence of thread increasing when processing 3354th batch (I used cat /proc/""this programs' pid""/status to check the number of threads): !3 !tf !2 Then, the evidence of thread increasing when processing 3791st batch (the amount of threads reached 22178): !6 !4 !5 When",2024-09-20T17:14:03Z,stat:awaiting tensorflower type:bug comp:dist-strat TF 2.11,open,0,2,https://github.com/tensorflow/tensorflow/issues/76157,", I can see that you are using tensorflow v2.11 which is pretty old. Could you please try using latest tensorflow v2.17 and update whether you are facing a similar issue? Thank you!","Hi , Very glad to hear your response. I would really like to try the tensorflow v2.17, and actually I have installed the latest tensorflow v2.17 in the initial round of using this machine, which cannot work on there.  When I checked the Nvidia Kernel Module in this machine, I found: **NVRM version: NVIDIA UNIX x86_64 Kernel Module  470.256.02** So, for meeting the requirements, I had to downgrade the Nvidia driver to **NVIDIASMI 470.256.02   Driver Version: 470.256.02   CUDA Version: 11.4** (I had tried a lot of the latest Nvidia driver on this machine such as 550, and these were not worked, just 470 was fine to this machine). Based on this, I have to install the tensorflow v2.11 to meet the CUDA tookit Version: 11.4, which eventually worked fine. Other programs run on the same tensorflow env was fine and no problem, but when run the GPT2,  this situation happend. I am so sorry for that. I compared the GPT2 code of calling tensorflow multiGPU training module with my previous projects run successfully on this machine, which was no any difference.  I have checked the same issue at https://github.com/tensorflow/tensorflow/issues/62466, but they overcome this problem by revising ., I cannot find this file in my machine.  Therefore, if possible, could you please give me a way to overcome this issue by just revising .py files or adding some codes to close the corresponding code that may inducing the issue (an accessible way I can process).  Sorry to disturb again."
yi,copybara-service[bot],Internal changes only,Internal changes only,2024-09-19T17:07:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76064
rag,copybara-service[bot],Pure cleanup,Pure cleanup Move a few inlined fragments to the separate Run....Passes functions.,2024-09-19T12:28:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/76046
yi,abivelu-prem,Warning! ***HDF5 library version mismatched error***," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tensorflowgpu 2.9.0  Custom code Yes  OS platform and distribution Linux, ubuntu 22.04  Mobile device Linux, ubuntu 22.04  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuda 11.5  GPU model and memory _No response_  Current behavior? Hello, I am trying to run my application and check GPU utilization, When trying to import h5py I get the following error. I have seen a lot of ""conda"" commands solving this issue,  but I am not supposed to use conda,  I don't use anaconda python on Ubuntu 22.04. HELP ME SOLVE THIS ISSUE, I HAVE TRIED TO UPDATE THE HDF5 TO LATEST VERSION i.e., 1.14.2 AS INDICATED IN THE WARNING MESSAGE , BUT STILL GET THE SAME WARNING. My LD_LIBRARY_PATH was set to a different location I changed it to ""/usr/local/hdf5/lib:"" but I get the same error. Any help is greatly appreciated.  Standalone code to reproduce the issue   Relevant log output ",2024-09-18T11:54:53Z,stat:awaiting response type:bug stale TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/75983,"prem, HDF5 library version mismatched error will be resolved with pip install h5py upgrade nodependencies force. You can ignore the above CUDA warnings if you do not have a GPU set up on your machine. Please can you install h5py as below  Also tensorflow v2.9 is a pretty old version, could you please upgrade to the latest tensorflow v2.17. Thank you!","  I have tried to install tensorflow with CUDA dependency using ""pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 indexurl https://download.pytorch.org/whl/cu118"" and have uninstalled h5py package by ""pip uninstall h5py"" which prevented the HDF5 warning to pop up on my application, I do have a GPU set up on my machine and I am trying to utilize GPU. But its not utilizing as of now, due to the tensorflow issue I guess. your suggestion would be highly appreciated.","prem, From the above comment I can see that you are using to install tensorflow using a pip install torch which might be the reason for the error. Could you please try to install from the tensorflow official document. https://www.tensorflow.org/install/pipstepbystep_instructions And also if you are facing issue while using the torch, please raise the issue in the respective forum. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
gemma,copybara-service[bot],Avoid duplicate work regarding Gemm autotuning.,"Avoid duplicate work regarding Gemm autotuning. We already run GemmAlgorithmPicker in the recursive autotuning compilation during GemmFusionAutotuner. On Ampere or later, autotuning of algorithms is done implicitly when running Gemms the first time, and we used to still run the GemmAlgorithmPicker pass as a warmup so that the first real run will be faster. But this warmup will have already happened with GemmFusionAutotuner.",2024-09-18T11:31:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75981
yi,copybara-service[bot],[lrt-compiler-plugin] Update Plugin interface and example Plugin,"[lrtcompilerplugin] Update Plugin interface and example Plugin * Compile all partitions at once * Update interface of compiled result * Add functions for querying backend (doesn't require loaded plugin) and support ""config"" (does require loaded plugin) * Add ""Lrt"" prefix to plugin functions * Incorporate these changes in tool",2024-09-18T09:40:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75977
yi,copybara-service[bot],"[lrt-compiler-plugin] Complete skeleton of e2e flow. Add binary tool for applying plugin to model. Do another pass over graph splitting, cleanup/debug. Add end2end testing ability.","[lrtcompilerplugin] Complete skeleton of e2e flow. Add binary tool for applying plugin to model. Do another pass over graph splitting, cleanup/debug. Add end2end testing ability.",2024-09-18T09:40:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75976
yi,TheAsherbot,Build Error on Windows 11 for TensorFlow r2.17.0 C++," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.0  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.12.5  Bazel version 6.5.0  GCC/compiler version MSVC 2022/CLANG 17.0.6  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to build a .dll, and dll_import_lib for TensorFlow for C++, but everything has been giving me errors, besides when building with Linux, but the files are not compatible with Windows, which has my preferred IDE, Visual Studios. I have tried both Clang 17.0.6, and MSVC 2022. It when it fails it says something about a package called snappy. I have attached the who console log so y'all can see the whole error.  Standalone code to reproduce the issue   Relevant log output ",2024-09-18T01:43:43Z,stat:awaiting tensorflower type:build/install subtype:windows type:performance comp:core 2.17,closed,0,8,https://github.com/tensorflow/tensorflow/issues/75964,"**toplay** ,","toplay or , Sorry to bother you, but has there been any updates on ways of resolving this error?", ,"Hi , Could you please confirm if you've successfully built the above packages using the latest master commit? This will help determine whether the issue lies with the path setup or the TensorFlow 2.17.0 wheels."," I was able to reproduce this error on both v2.18.0rc1, and the Master commit. I updated my version of Clang to 19.1.1, and clang 18.1.8, and I am still getting the same error.","Hi , I am not able to reproduce the above error. However, I am getting a linking error with the above target(still under investigation). Please try the following commands to set up the environment and delete the bazel cache before running the bazel command. Hopefully this should fix the above error. set BAZEL_SH=C:\msys64\usr\bin\bash.exe set BAZEL_VS=C:\Program Files\Microsoft Visual Studio\2022\Community set BAZEL_VC=C:\Program Files\Microsoft Visual Studio\2022\Community\VC set BAZEL_VC_FULL_VERSION=14.39.33519 set BAZEL_LLVM=C:\Program Files\LLVM set PATH=c:\Tools\Bazel;C:\Python312\Scripts;C:\msys64;C:\Program Files\Git\cmd;C:\Program Files\Git\usr\bin;C:\msys64\usr\bin;C:\Windows\SysWOW64;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files\Java\jre1.8.0_251\bin;C:\Program Files\Git\mingw64\bin;C:\Program Files (x86)\PowerShell\6\;C:\Windows\system32\config\systemprofile\AppData\Local\Microsoft\WindowsApps;C:\Program Files\LLVM\bin;%PATH% set PYTHON_BIN_PATH=C:\Python312\python.exe set PYTHON_LIB_PATH=C:\Python312\Lib set PYTHON_DIRECTORY=C:\Python312\Scripts", Sadly following those steps did not work. : (   On a 2nd PC I was able to get a linking error mentioned https://github.com/tensorflow/tensorflow/issues/77156. I think these errors are do to something one my PC. I will see if I can resolve the issue because it is only on my machine. Thank you for your help!,Are you satisfied with the resolution of your issue? Yes No
gpt,copybara-service[bot],PR #16975: Add a few related optimization passes for fp8 gemm custom-calls.,"PR CC(libcublas.so 9.0 cannot open shared object file { CUDA 8.0 + GeForce 940MX}): Add a few related optimization passes for fp8 gemm customcalls. Imported from GitHub PR https://github.com/openxla/xla/pull/16975 This caused convergence issue for fp8 training, tested on GPT3 models: Before:  After:  Copybara import of the project:  8bf6d19e5c10024d34a59b889893d203eee6691a by Elfie Guo : Add a few related optimization pass for fp8 gemm rerwriter. Merging this change closes CC(libcublas.so 9.0 cannot open shared object file { CUDA 8.0 + GeForce 940MX}) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16975 from elfiegg:pass 8bf6d19e5c10024d34a59b889893d203eee6691a",2024-09-17T23:52:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/75959
yi,copybara-service[bot],Implementation of simplify_ici_dummy_variables_pass,Implementation of simplify_ici_dummy_variables_pass,2024-09-17T21:35:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75948
text generation,abisdbest,"Tensorflow ""Text generation with an RNN"" tutorial out of date","I was taking a look at the tutorial on generating text with an RNN from the tensorflow website (https://www.tensorflow.org/text/tutorials/text_generation), and clicked the option to run on Google Colab. But when I tried running the full code I got overwhelmed with a multitude of errors. I thought I might have been doing something wrong, so I searched online, but found out that you are supposed to be able to just run the code without modification. I figured it could just be that the way the new versions of tensorflow worked may have changed. Would be very helpful to me if it could be fixed many thanks",2024-09-17T20:38:20Z,stat:awaiting response type:support stale comp:model,closed,0,7,https://github.com/tensorflow/tensorflow/issues/75945,", Thank you for reporting the issue. I observed the same that the code is failing in tensorflow v2.17. Please allow some time to deep dive into the issue. Thank you!",", Hi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.  Also I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it here. Thank you!",thank you very much!,", I have created the PR for the changes required. https://github.com/tensorflow/text/pull/1313 Could you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only.,Internal change only.,2024-09-17T14:24:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/75917
yi,copybara-service[bot],[XLA] Introduce infeed token propagation,"[XLA] Introduce infeed token propagation During computation inlining, specifically loop unrolling, it is posibble for infeeds (and outfeeds) to get reordered in a way that breaks the original scheduling constraints set by the computation boundaries. This is a result of Tensorflow not exposing tokens for these ops to the user, so the input and output tokens end up dangling. Loop unrolling in XLA can be thought of applying the same function repeatedly to itself, e.g. transforming f(x) into f(f(x)). By pushing the tokens outside the loop body, we can guarantee that the output token of the first infeed will become the input token of the next infeed, thus creating a data dependency chain and preserving the original ordering.",2024-09-16T21:13:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75869
yi,Ashish2000L,tensorflow lite windows c++ build error from source ," 1. System information  OS Platform and Distribution : windows 11  TensorFlow installation (pip package or built from source): built from source  TensorFlow library (version, if pip package or github SHA, if built from source): tag2.16.2  2. Code  I am trying to build tf lite dynamic or static build for windows but getting this error using bazel 7.3.1 I have also tried with the cmake but it is have too much work for now. I want to use the lib or dll in my msvc c++ project.  I am not familiar with bazel and would like to have some assistance on the same.  I have look for github, documentions, and stackoverflow for the same but wasn't able to resolve this issue.",2024-09-16T11:06:41Z,stat:awaiting response type:build/install comp:lite subtype:windows TF 2.16,closed,0,17,https://github.com/tensorflow/tensorflow/issues/75840,"Hi,  Thank you for bringing this issue to our attention, To conform are you following the below mentioned steps if you're using the GPU ? if you're using only CPU then please make this flag `DTFLITE_ENABLE_GPU=OFF `or please don't add that flag  I see you're using the `Bazel version 7.3.1` could you please try with `Bazel version 6.5.0` and see are you able to build successfully or not ?  If possible please go with `WSL` option because you will face less errors  If I have missed something here please let me know.  Thank you for your cooperation and patience.","> Hi,  >  > Thank you for bringing this issue to our attention, To conform are you following the below mentioned steps if you're using the GPU ? if you're using only CPU then please make this flag `DTFLITE_ENABLE_GPU=OFF `or please don't add that flag >  > I see you're using the `Bazel version 7.3.1` could you please try with `Bazel version 6.5.0` and see are you able to build successfully or not ? >  > If possible please go with `WSL` option because you will face less errors >  >  >  > If I have missed something here please let me know. >  > Thank you for your cooperation and patience. If I try building this on wsl won't it give me .so file instead of .dll since I want to make it work in my msvc 2017. Also, if possible can we make the static libs instead of dynamic one?","I have tried using bazel 6.5.0, but after some progress it is showing just loading with no verbose for a long time: > D:\tf\tensorflow>bazel build //tensorflow/lite:tensorflowlite INFO: Reading 'startup' options from d:\tf\tensorflow\.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=172 INFO: Reading rc options for 'build' from d:\tf\tensorflow\.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Options provided by the client:   'build' options: python_path=C:/Users/51010384/AppData/Local/Microsoft/WindowsApps/python.exe INFO: Reading rc options for 'build' from d:\tf\tensorflow\.bazelrc:   'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Reading rc options for 'build' from d:\tf\tensorflow\.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=C:/Users/51010384/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0/python.exe action_env PYTHON_LIB_PATH=C:/Program Files/WindowsApps/PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0/Lib/sitepackages python_path=C:/Users/51010384/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0/python.exe copt=/d2ReducedOptimizeHugeFunctions host_copt=/d2ReducedOptimizeHugeFunctions define=override_eigen_strong_inline=true INFO: Found applicable config definition build:short_logs in file d:\tf\tensorflow\.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file d:\tf\tensorflow\.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:windows in file d:\tf\tensorflow\.bazelrc: copt=/W0 host_copt=/W0 copt=/Zc:__cplusplus host_copt=/Zc:__cplusplus copt=/D_USE_MATH_DEFINES host_copt=/D_USE_MATH_DEFINES features=compiler_param_file features=archive_param_file copt=/d2ReducedOptimizeHugeFunctions host_copt=/d2ReducedOptimizeHugeFunctions enable_runfiles cxxopt=/std:c++17 host_cxxopt=/std:c++17 config=monolithic copt=DWIN32_LEAN_AND_MEAN host_copt=DWIN32_LEAN_AND_MEAN copt=DNOGDI host_copt=DNOGDI copt=/Zc:preprocessor host_copt=/Zc:preprocessor linkopt=/DEBUG host_linkopt=/DEBUG linkopt=/OPT:REF host_linkopt=/OPT:REF linkopt=/OPT:ICF host_linkopt=/OPT:ICF verbose_failures features=compiler_param_file config=no_tfrt INFO: Found applicable config definition build:monolithic in file d:\tf\tensorflow\.bazelrc: define framework_shared_object=false define tsl_protobuf_header_only=false experimental_link_static_libraries_once=false INFO: Found applicable config definition build:no_tfrt in file d:\tf\tensorflow\.bazelrc: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils Loading:"," I have downloaded cmake for the windows and tried building the static file using the cmake: 3.30.0rc3.  >cmake DTFLITE_ENABLE_GPU=OFF DBUILD_SHARED_LIBS=OFF DCMAKE_BUILD_TYPE=Debug DTFLITE_ENABLE_XNNPACK=ON A Win32 ../tensorflow/tensorflow/lite I have tried adding the flattbuffers.lib , tendorflowlite.lib and XNNPack.lib in my mscv external dependency folder but getting too many linker errors, can you help me why these linker error are coming. I have built the tensorflow:2.16.2 > 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,43): warning C4244: 'argument': conversion from 'SizeT' to 'size_t', possible loss of data 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,43): warning C4244:         with 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,43): warning C4244:         [ 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,43): warning C4244:             SizeT=unsigned __int64 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,43): warning C4244:         ] 1>(compiling source file 'main.cpp') 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,43): 1>the template instantiation context (the oldest one first) is 1>	D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1421,58): 1>	see reference to class template instantiation 'flatbuffers::FlatBufferBuilderImpl' being compiled 1>	D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1275,26): 1>	see reference to class template instantiation 'flatbuffers::vector_downward' being compiled 1>	D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(269,8): 1>	while compiling class template member function 'void flatbuffers::vector_downward::reallocate(size_t)' 1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(145,49): 1>		see the first reference to 'flatbuffers::vector_downward::reallocate' in 'flatbuffers::vector_downward::ensure_space' 1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(152,19): 1>		see the first reference to 'flatbuffers::vector_downward::ensure_space' in 'flatbuffers::vector_downward::make_space' 1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(214,15): 1>		see the first reference to 'flatbuffers::vector_downward::make_space' in 'flatbuffers::vector_downward::fill' 1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1363,14): 1>		see the first reference to 'flatbuffers::vector_downward::fill' in 'flatbuffers::FlatBufferBuilderImpl::CreateStringImpl' 1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1424,19): 1>		see the first reference to 'flatbuffers::FlatBufferBuilderImpl::CreateStringImpl' in 'flatbuffers::FlatBufferBuilderImpl::CreateString' 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,33): warning C4244: 'argument': conversion from 'SizeT' to 'size_t', possible loss of data 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,33): warning C4244:         with 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,33): warning C4244:         [ 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,33): warning C4244:             SizeT=unsigned __int64 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\vector_downward.h(278,33): warning C4244:         ] 1>(compiling source file 'main.cpp') 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(265,23): warning C4244: 'argument': conversion from 'T' to 'const size_t', possible loss of data 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(265,23): warning C4244:         with 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(265,23): warning C4244:         [ 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(265,23): warning C4244:             T=flatbuffers::uoffset64_t 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(265,23): warning C4244:         ] 1>(compiling source file 'main.cpp') 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(265,23): 1>the template instantiation context (the oldest one first) is 1>	D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(327,10): 1>	see reference to function template instantiation 'size_t flatbuffers::Verifier::VerifyOffset(const size_t) const' being compiled 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(266,12): warning C4244: 'return': conversion from 'const T' to 'size_t', possible loss of data 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(266,12): warning C4244:         with 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(266,12): warning C4244:         [ 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(266,12): warning C4244:             T=flatbuffers::uoffset64_t 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\verifier.h(266,12): warning C4244:         ] 1>(compiling source file 'main.cpp') 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1405,39): warning C4244: '=': conversion from 'unsigned __int64' to 'size_t', possible loss of data 1>(compiling source file 'main.cpp') 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1405,39): 1>the template instantiation context (the oldest one first) is 1>	D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1426,7): 1>	see reference to function template instantiation 'unsigned __int64 flatbuffers::FlatBufferBuilderImpl::CalculateOffset::offset_type>(void)' being compiled 1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1425,3): 1>		see the first reference to 'flatbuffers::FlatBufferBuilderImpl::CalculateOffset' in 'flatbuffers::FlatBufferBuilderImpl::CreateString' 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(523,38): warning C4244: 'argument': conversion from 'unsigned __int64' to 'size_t', possible loss of data 1>(compiling source file 'main.cpp') 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(523,38): 1>the template instantiation context (the oldest one first) is 1>	D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(520,8): 1>	while compiling class template member function 'void flatbuffers::FlatBufferBuilderImpl::PreAlign(size_t,size_t)' 1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(687,13): 1>		see the first reference to 'flatbuffers::FlatBufferBuilderImpl::PreAlign' in 'flatbuffers::FlatBufferBuilderImpl::StartVector' 1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1438,37): 1>		see the first reference to 'flatbuffers::FlatBufferBuilderImpl::StartVector' in 'flatbuffers::FlatBufferBuilderImpl::StartVector' 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(295,37): warning C4244: 'argument': conversion from 'SizeT' to 'size_t', possible loss of data 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(295,37): warning C4244:         with 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(295,37): warning C4244:         [ 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(295,37): warning C4244:             SizeT=unsigned __int64 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(295,37): warning C4244:         ] 1>(compiling source file 'main.cpp') 1>D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(295,37): 1>the template instantiation context (the oldest one first) is 1>	D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(293,8): 1>	while compiling class template member function 'void flatbuffers::FlatBufferBuilderImpl::Align(size_t)' 1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(316,10): 1>		see the first reference to 'flatbuffers::FlatBufferBuilderImpl::Align' in 'flatbuffers::FlatBufferBuilderImpl::PushElement' 1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1365,16): 1>		see the first reference to 'flatbuffers::FlatBufferBuilderImpl::PushElement' in 'flatbuffers::FlatBufferBuilderImpl::CreateStringImpl' 1>		D:\testWork\visualCodes\aadhartensor\dependencies\include\flatbuffers\flatbuffer_builder.h(1424,19): 1>		see the first reference to 'flatbuffers::FlatBufferBuilderImpl::CreateStringImpl' in 'flatbuffers::FlatBufferBuilderImpl::CreateString' 1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2005: ""public: void __thiscall std::basic_ostream >::_Osfx(void)"" (?_Osfx@?$basic_ostream?$char_traits@@@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2005: ""public: class std::basic_ostream > & __thiscall std::basic_ostream >::flush(void)"" (?flush@?$basic_ostream?$char_traits@@@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2005: ""public: void __thiscall std::basic_ios >::setstate(int,bool)"" (?setstate@?$basic_ios?$char_traits@@@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>IlmImf.lib(IlmThreadSemaphoreWin32.obj) : error LNK2005: ""public: int __thiscall std::basic_streambuf >::sputc(char)"" (?sputc@?$basic_streambuf?$char_traits@@@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>IlmImf.lib(IexBaseExc.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>IlmImf.lib(IexBaseExc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""public: virtual __thiscall std::basic_streambuf >::~basic_streambuf >(void)"" (??1?$basic_streambuf?$char_traits@@@) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj) 1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: __int64 __thiscall std::basic_streambuf >::_Gnavail(void)const "" (?_Gnavail@?$basic_streambuf?$char_traits@@@) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj) 1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: __int64 __thiscall std::basic_streambuf >::_Pnavail(void)const "" (?_Pnavail@?$basic_streambuf?$char_traits@@@) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj) 1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: void __thiscall std::basic_streambuf >::_Init(void)"" (?_Init@?$basic_streambuf?$char_traits@@@) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj) 1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: virtual __int64 __thiscall std::basic_streambuf >::xsgetn(char *,__int64)"" (?xsgetn@?$basic_streambuf?$char_traits@@@) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj) 1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: virtual __int64 __thiscall std::basic_streambuf >::xsputn(char const *,__int64)"" (?xsputn@?$basic_streambuf?$char_traits@@@) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj) 1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""public: virtual __thiscall std::basic_ios >::~basic_ios >(void)"" (??1?$basic_ios?$char_traits@@@) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj) 1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""protected: __thiscall std::basic_ios >::basic_ios >(void)"" (??0?$basic_ios?$char_traits@@@) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj) 1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""public: virtual __thiscall std::basic_ostream >::~basic_ostream >(void)"" (??1?$basic_ostream?$char_traits@@@) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj) 1>msvcprtd.lib(MSVCP140D.dll) : error LNK2005: ""public: virtual __thiscall std::basic_istream >::~basic_istream >(void)"" (??1?$basic_istream?$char_traits@@@) already defined in IlmImf.lib(IlmThreadSemaphoreWin32.obj) 1>libcpmt.lib(locale0.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(locale0.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(locale0.obj) : error LNK2005: ""void __cdecl std::_Facet_Register(class std::_Facet_base *)"" (?_Facet_Register@@) already defined in msvcprtd.lib(locale0_implib.obj) 1>libcpmt.lib(locale0.obj) : error LNK2005: ""private: static class std::locale::_Locimp * __cdecl std::locale::_Getgloballocale(void)"" (?_Getgloballocale@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(locale0.obj) : error LNK2005: ""private: static class std::locale::_Locimp * __cdecl std::locale::_Init(bool)"" (?_Init@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(locale0.obj) : error LNK2005: ""public: static void __cdecl std::_Locinfo::_Locinfo_ctor(class std::_Locinfo *,char const *)"" (?_Locinfo_ctor@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(locale0.obj) : error LNK2005: ""public: static void __cdecl std::_Locinfo::_Locinfo_dtor(class std::_Locinfo *)"" (?_Locinfo_dtor@@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(locale.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(locale.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(locale.obj) : error LNK2005: ""public: __thiscall std::locale::id::operator unsigned int(void)"" (??Bid@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(locale.obj) : error LNK2005: ""public: static unsigned int __cdecl std::codecvt::_Getcat(class std::locale::facet const * *,class std::locale const *)"" (?_Getcat@?$codecvt@@@@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(locale.obj) : error LNK2005: ""public: struct _Cvtvec __thiscall std::_Locinfo::_Getcvt(void)const "" (?_Getcvt@?AU_Cvtvec@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(locale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf >::_Gninc(void)"" (?_Gninc@?$basic_streambuf?$char_traits@@@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(locale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf >::_Pninc(void)"" (?_Pninc@?$basic_streambuf?$char_traits@@@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(locale.obj) : error LNK2005: ""public: int __thiscall std::ios_base::flags(void)const "" (?flags@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(locale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf >::gptr(void)const "" (?gptr@?$basic_streambuf?$char_traits@@@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(locale.obj) : error LNK2005: ""public: int __thiscall std::basic_streambuf >::sputc(char)"" (?sputc@?$basic_streambuf?$char_traits@@@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(locale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(__int64)"" (?width@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(locale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(void)const "" (?width@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(iosptrs.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(iosptrs.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xwctomb.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xwctomb.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(wlocale.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(wlocale.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: __thiscall std::locale::id::operator unsigned int(void)"" (??Bid@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: struct _Cvtvec __thiscall std::_Locinfo::_Getcvt(void)const "" (?_Getcvt@?AU_Cvtvec@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: char const * __thiscall std::_Locinfo::_Getdays(void)const "" (?_Getdays@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: char const * __thiscall std::_Locinfo::_Getmonths(void)const "" (?_Getmonths@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: unsigned short const * __thiscall std::_Locinfo::_W_Getdays(void)const "" (?_W_Getdays@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: unsigned short const * __thiscall std::_Locinfo::_W_Getmonths(void)const "" (?_W_Getmonths@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: int __thiscall std::ios_base::flags(void)const "" (?flags@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(__int64)"" (?width@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(wlocale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(void)const "" (?width@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xlocale.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xlocale.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: __thiscall std::locale::id::operator unsigned int(void)"" (??Bid@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: struct _Cvtvec __thiscall std::_Locinfo::_Getcvt(void)const "" (?_Getcvt@?AU_Cvtvec@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: char const * __thiscall std::_Locinfo::_Getdays(void)const "" (?_Getdays@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: char const * __thiscall std::_Locinfo::_Getmonths(void)const "" (?_Getmonths@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xlocale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf >::_Gninc(void)"" (?_Gninc@?$basic_streambuf?$char_traits@@@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xlocale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf >::_Pninc(void)"" (?_Pninc@?$basic_streambuf?$char_traits@@@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: unsigned short const * __thiscall std::_Locinfo::_W_Getdays(void)const "" (?_W_Getdays@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: unsigned short const * __thiscall std::_Locinfo::_W_Getmonths(void)const "" (?_W_Getmonths@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: int __thiscall std::ios_base::flags(void)const "" (?flags@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xlocale.obj) : error LNK2005: ""protected: char * __thiscall std::basic_streambuf >::gptr(void)const "" (?gptr@?$basic_streambuf?$char_traits@@@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: int __thiscall std::basic_streambuf >::sputc(char)"" (?sputc@?$basic_streambuf?$char_traits@@@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(__int64)"" (?width@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xlocale.obj) : error LNK2005: ""public: __int64 __thiscall std::ios_base::width(void)const "" (?width@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xstol.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xstol.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xstoul.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xstoul.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xstoll.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xstoll.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xstoull.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xstoull.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xlock.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xlock.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xlock.obj) : error LNK2005: ""public: __thiscall std::_Lockit::_Lockit(int)"" (??0_Lockit@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xlock.obj) : error LNK2005: ""public: __thiscall std::_Lockit::~_Lockit(void)"" (??1_Lockit@) already defined in msvcprtd.lib(MSVCP140D.dll) 1>libcpmt.lib(xstrcoll.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xstrcoll.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xdateord.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xdateord.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xwcscoll.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xwcscoll.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xwcsxfrm.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xwcsxfrm.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xgetwctype.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xgetwctype.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xtowlower.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xtowlower.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xtowupper.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xtowupper.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xstrxfrm.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xstrxfrm.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(xmtx.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(xmtx.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(StlCompareStringA.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(StlCompareStringA.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(StlCompareStringW.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(StlCompareStringW.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(StlLCMapStringW.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(StlLCMapStringW.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>libcpmt.lib(StlLCMapStringA.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in main.obj 1>libcpmt.lib(StlLCMapStringA.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MDd_DynamicDebug' in main.obj 1>LINK : warning LNK4098: defaultlib 'MSVCRTD' conflicts with use of other libs; use /NODEFAULTLIB:library 1>MSVCRT.lib(initializers.obj) : warning LNK4098: defaultlib 'libcmt.lib' conflicts with use of other libs; use /NODEFAULTLIB:library 1>tensorflowlite.lib(mfcc_mel_filterbank.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>flatbuffers.lib(util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(simple_memory_arena.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(portable_tensor_utils.cc.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(quantization_util.cc.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(mfcc_dct.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(tensor_slice_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(xnnpack_delegate.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(spectrogram.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(string_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(fully_connected_reference.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(static_hashtable.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(rng_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(initialization_status.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(eigen_support.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(sparsity_format_converter.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(tensor_ctypes.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(kernel_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(cpu_backend_context.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(resource_variable.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(detection_postprocess.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(graph_info.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(arena_planner.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(stablehlo_gather.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(dilate.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(numeric_verify.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(audio_spectrogram.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(stablehlo_min_max.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(stablehlo_pad.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(stablehlo_reduce_window.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(stablehlo_scatter.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(while.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(rng_bit_generator.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(stablehlo_add.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(stablehlo_multiply.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(unique.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(unpack.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(var_handle.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(where.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(topk_v2.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(transpose.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(unidirectional_sequence_lstm.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(strided_slice.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(sub.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(svdf.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(tile.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(split.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(split_v.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(squared_difference.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(squeeze.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(scatter_nd.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(skip_gram.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(slice.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(sparse_to_dense.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(pad.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(quantize.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(random_ops.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(reverse.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(reduce.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(mirror_pad.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(non_max_suppression.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(pack.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(gather_nd.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(hashtable_lookup.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(if.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(activations.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(fill.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(gather.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(densify.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(depthwise_conv.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(dequantize.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(dynamic_update_slice.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(cumsum.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(bucketize.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(call_once.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(cast.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(concatenation.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(add.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(add_n.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(arg_min_max.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(metadata_util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(root_profiler.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(mutable_op_resolver.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(elementwise.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(register.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(util.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(subgraph.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(signature_runner.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>main.obj : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(interpreter.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(model_builder.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(interpreter_builder.obj) : error LNK2001: unresolved external symbol __imp___invalid_parameter 1>tensorflowlite.lib(mfcc_mel_filterbank.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>flatbuffers.lib(util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(simple_memory_arena.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(portable_tensor_utils.cc.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(quantization_util.cc.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(mfcc_dct.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(tensor_slice_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(xnnpack_delegate.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(spectrogram.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(string_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(fully_connected_reference.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(static_hashtable.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(rng_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(initialization_status.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(eigen_support.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(sparsity_format_converter.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(tensor_ctypes.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(kernel_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(cpu_backend_context.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(resource_variable.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(detection_postprocess.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(graph_info.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(arena_planner.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(stablehlo_gather.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(dilate.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(numeric_verify.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(audio_spectrogram.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(stablehlo_min_max.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(stablehlo_pad.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(stablehlo_reduce_window.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(stablehlo_scatter.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(while.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(rng_bit_generator.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(stablehlo_add.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(stablehlo_multiply.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(unique.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(unpack.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(var_handle.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(where.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(topk_v2.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(transpose.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(unidirectional_sequence_lstm.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(strided_slice.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(sub.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(svdf.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(tile.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(split.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(split_v.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(squared_difference.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(squeeze.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(scatter_nd.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(skip_gram.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(slice.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(sparse_to_dense.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(pad.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(quantize.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(random_ops.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(reverse.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(reduce.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(mirror_pad.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(non_max_suppression.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(pack.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(gather_nd.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(hashtable_lookup.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(if.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(activations.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(fill.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(gather.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(densify.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(depthwise_conv.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(dequantize.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(dynamic_update_slice.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(cumsum.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(bucketize.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(call_once.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(cast.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(concatenation.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(add.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(add_n.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(arg_min_max.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(metadata_util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(root_profiler.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(mutable_op_resolver.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(elementwise.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(register.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(util.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(subgraph.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(signature_runner.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>main.obj : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(interpreter.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(model_builder.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(interpreter_builder.obj) : error LNK2001: unresolved external symbol __imp___CrtDbgReport 1>tensorflowlite.lib(interpreter.obj) : error LNK2019: unresolved external symbol ""public: __thiscall ruy::ScopedSuppressDenormals::ScopedSuppressDenormals(void)"" (??0ScopedSuppressDenormals@) referenced in function ""public: enum TfLiteStatus __thiscall tflite::impl::Interpreter::Invoke(void)"" (?Invoke@?AW4TfLiteStatus@) 1>tensorflowlite.lib(interpreter.obj) : error LNK2019: unresolved external symbol ""public: __thiscall ruy::ScopedSuppressDenormals::~ScopedSuppressDenormals(void)"" (??1ScopedSuppressDenormals@) referenced in function ""public: enum TfLiteStatus __thiscall tflite::impl::Interpreter::Invoke(void)"" (?Invoke@?AW4TfLiteStatus@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""class ruy::Ctx * __cdecl ruy::get_ctx(class ruy::Context *)"" (?get_ctx@@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::clear_performance_advisories(void)"" (?clear_performance_advisories@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: void __thiscall ruy::Ctx::set_performance_advisory(enum ruy::PerformanceAdvisory)"" (?set_performance_advisory@@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@?AW4Path@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@?AW4Path@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@?AW4Path@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@?AW4Path@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@?AW4Path@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@?AW4Path@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@?AW4Path@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: enum ruy::Path __thiscall ruy::Ctx::SelectPath(enum ruy::Path)"" (?SelectPath@?AW4Path@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: class ruy::Allocator * __thiscall ruy::Ctx::GetMainAllocator(void)"" (?GetMainAllocator@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""public: void * __thiscall ruy::Allocator::AllocateBytes(int)"" (?AllocateBytes@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2019: unresolved external symbol ""int __cdecl ruy::detail::MultiplyByQuantizedMultiplier(int,int,int)"" (?MultiplyByQuantizedMultiplier@) referenced in function ""public: static void __cdecl ruy::detail::ApplyMultiplierImpl::Run(class ruy::MulParams const &,int,int *)"" (?Run@?$ApplyMultiplierImpl$00@?$MulParams) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""int __cdecl ruy::detail::MultiplyByQuantizedMultiplier(int,int,int)"" (?MultiplyByQuantizedMultiplier@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""int __cdecl ruy::detail::MultiplyByQuantizedMultiplier(int,int,int)"" (?MultiplyByQuantizedMultiplier@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""int __cdecl ruy::detail::MultiplyByQuantizedMultiplier(int,int,int)"" (?MultiplyByQuantizedMultiplier@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx512@?$KernelParams8bit@$0BA@$0BA@@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx512@?$KernelParams8bit@$0BA@$0BA@@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx512@?$KernelParams8bit@$0BA@$0BA@@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx512@?$KernelParams8bit@$0BA@$0BA@@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx512@?$KernelParams8bit@$0BA@$0BA@@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx512SingleCol@?$KernelParams8bit@$0BA@$0BA@@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx512SingleCol@?$KernelParams8bit@$0BA@$0BA@@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx512SingleCol@?$KernelParams8bit@$0BA@$0BA@@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx512SingleCol@?$KernelParams8bit@$0BA@$0BA@@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx512SingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx512SingleCol@?$KernelParams8bit@$0BA@$0BA@@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512SingleCol@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512SingleCol@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512SingleCol@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512SingleCol@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512SingleCol@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512SingleCol@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512SingleCol@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx512SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx512SingleCol@?$KernelParamsFloat@$0BA@$0BA@@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx2@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx2@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx2@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx2@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx2@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx2SingleCol@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx2SingleCol@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx2SingleCol@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx2SingleCol@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx2SingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx2SingleCol@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2SingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2SingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2SingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2SingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2SingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2SingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2SingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx2SingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx2SingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvx@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvxSingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvxSingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvxSingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvxSingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvxSingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvxSingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvxSingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::KernelFloatAvxSingleCol(struct ruy::KernelParamsFloat const &)"" (?KernelFloatAvxSingleCol@?$KernelParamsFloat@$07$07@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvx(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvx@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvxSingleCol@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvxSingleCol@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvxSingleCol@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvxSingleCol@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Kernel8bitAvxSingleCol(struct ruy::KernelParams8bit const &)"" (?Kernel8bitAvxSingleCol@?$KernelParams8bit@$07$07@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx2(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx2@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx2(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx2@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitColMajorForAvx512(signed char const *,signed char,signed char const *,int,int,int,signed char *,int *)"" (?Pack8bitColMajorForAvx512@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::PackFloatColMajorForAvx512(float const *,float const *,int,int,int,float *)"" (?PackFloatColMajorForAvx512@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx2(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx2@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack8bitRowMajorForAvx512(unsigned char const *,int,int,signed char *,int,int,int,int,int,int,int,int *)"" (?Pack8bitRowMajorForAvx512@) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@@) 1>tensorflowlite.lib(lstm.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@@) 1>tensorflowlite.lib(lstm_eval.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@@) 1>tensorflowlite.lib(batch_matmul.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@@) 1>tensorflowlite.lib(conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@@) 1>tensorflowlite.lib(conv3d.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@@) 1>tensorflowlite.lib(conv3d_transpose.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::MulFrontEndFromTrMulParams(class ruy::Ctx *,struct ruy::TrMulParams *)"" (?MulFrontEndFromTrMulParams@@) 1>tensorflowlite.lib(conv.obj) : error LNK2019: unresolved external symbol ""void __cdecl ruy::Pack16bitColMajorForAvx512(short const *,short const *,int,int,int,short *,int *)"" (?Pack16bitColMajorForAvx512@) referenced in function ""public: static void __cdecl ruy::PackImpl,short,short,int,0>::Run(enum ruy::Tuning,struct ruy::Mat const &,struct ruy::PMat *,int,int)"" (?Run@?$PackImpl@$0EA?$FixedKernelLayout@$0A@$03$0BA@@$0A@@?$Mat?$PMat) 1>tensorflowlite.lib(fully_connected.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack16bitColMajorForAvx512(short const *,short const *,int,int,int,short *,int *)"" (?Pack16bitColMajorForAvx512@) 1>tensorflowlite.lib(transpose_conv.obj) : error LNK2001: unresolved external symbol ""void __cdecl ruy::Pack16bitColMajorForAvx512(short const *,short const *,int,int,int,short *,int *)"" (?Pack16bitColMajorForAvx512@) 1>tensorflowlite.lib(lsh_projection.obj) : error LNK2019: unresolved external symbol ""unsigned __int64 __cdecl util::Fingerprint64(char const *,unsigned int)"" (?Fingerprint64@) referenced in function ""int __cdecl tflite::ops::builtin::lsh_projection::RunningSignBit(struct TfLiteTensor const *,struct TfLiteTensor const *,float)"" (?RunningSignBit@@) 1>tensorflowlite.lib(rfft2d.obj) : error LNK2019: unresolved external symbol _rdft2d referenced in function ""void __cdecl tflite::ops::builtin::rfft2d::Rfft2dImpl(int,int,double * *,int *,double *)"" (?Rfft2dImpl@) 1>flatbuffers.lib(util.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg 1>tensorflowlite.lib(numeric_verify.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg 1>tensorflowlite.lib(audio_spectrogram.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg 1>tensorflowlite.lib(mfcc.cc.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg 1>tensorflowlite.lib(detection_postprocess.obj) : error LNK2001: unresolved external symbol __imp___calloc_dbg 1>tensorflowlite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol _pthreadpool_create referenced in function ""public: struct pthreadpool * __thiscall tflite::CpuBackendContext::get_xnnpack_threadpool(void)"" (?get_xnnpack_threadpool@@) 1>tensorflowlite.lib(xnnpack_delegate.obj) : error LNK2001: unresolved external symbol _pthreadpool_create 1>tensorflowlite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol _pthreadpool_destroy referenced in function ""public: __thiscall tflite::CpuBackendContext::CpuBackendContext(void)"" (??0CpuBackendContext@) 1>tensorflowlite.lib(xnnpack_delegate.obj) : error LNK2001: unresolved external symbol _pthreadpool_destroy 1>tensorflowlite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol ""public: __thiscall ruy::Context::Context(void)"" (??0Context@) referenced in function ""public: __thiscall tflite::CpuBackendContext::CpuBackendContext(void)"" (??0CpuBackendContext@) 1>tensorflowlite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol ""public: __thiscall ruy::Context::~Context(void)"" (??1Context@) referenced in function ""public: void * __thiscall ruy::Context::`scalar deleting destructor'(unsigned int)"" (??_GContext@) 1>tensorflowlite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol ""public: void __thiscall ruy::Context::set_max_num_threads(int)"" (?set_max_num_threads@) referenced in function ""public: virtual void __thiscall tflite::CpuBackendContext::SetMaxNumThreads(int)"" (?SetMaxNumThreads@) 1>tensorflowlite.lib(cpu_backend_context.obj) : error LNK2019: unresolved external symbol ""public: void __thiscall ruy::Context::ClearPrepackedCache(void)"" (?ClearPrepackedCache@) referenced in function ""public: virtual void __thiscall tflite::CpuBackendContext::ClearCaches(void)"" (?ClearCaches@) 1>tensorflowlite.lib(spectrogram.obj) : error LNK2019: unresolved external symbol _rdft referenced in function ""private: void __thiscall tflite::internal::Spectrogram::ProcessCoreFFT(void)"" (?ProcessCoreFFT@) 1>XNNPACK.lib(hardwareconfig.obj) : error LNK2019: unresolved external symbol _cpuinfo_initialize referenced in function _xnn_init_hardware_config 1>XNNPACK.lib(hardwareconfig.obj) : error LNK2001: unresolved external symbol _cpuinfo_isa 1>XNNPACK.lib(resizebilinearnhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(prelunc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(batchmatrixmultiplync.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(lutelementwisenc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(fullyconnectednc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(argmaxpoolingnhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(binaryelementwisend.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(resizebilinearnchw.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(globalaveragepoolingncw.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(globalaveragepoolingnwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(averagepoolingnhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(dynamicfullyconnectednc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(unaryelementwisenc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(convolutionnchw.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(convolutionnhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(deconvolutionnhwc.obj) : error LNK2001: unresolved external symbol _pthreadpool_get_threads_count 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_1d referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_1d_with_thread referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_1d_tile_1d referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_2d referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_2d_with_thread referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_2d_tile_1d referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_2d_tile_2d referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_3d referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_3d_tile_1d referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_3d_tile_1d_with_thread referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_3d_tile_2d referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_4d referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_4d_tile_2d referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_5d referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_5d_tile_2d referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(operatorrun.obj) : error LNK2019: unresolved external symbol _pthreadpool_parallelize_6d_tile_2d referenced in function _xnn_run_operator_with_index 1>XNNPACK.lib(gemmconfig.obj) : error LNK2019: unresolved external symbol _cpuinfo_get_core referenced in function _init_f32_gemm_config 1>msvcprtd.lib(locale0_implib.obj) : error LNK2019: unresolved external symbol __imp___free_dbg referenced in function ""public: static void __cdecl std::_Fac_node::operator delete(void *)"" (??3_Fac_node@) 1>msvcprtd.lib(locale0_implib.obj) : error LNK2019: unresolved external symbol __imp___malloc_dbg referenced in function ""public: static void * __cdecl std::_Fac_node::operator new(unsigned int)"" (??2_Fac_node@) below is my main code that I am using to predict something from my preexisting model: `include  include  include  include  int main() { 	tflite::StderrReporter error_reporter; 	std::unique_ptr interpreter; 	std::unique_ptr model = tflite::impl::FlatBufferModel::BuildFromFile(""model.tflite"", &error_reporter); 	tflite::ops::builtin::BuiltinOpResolver resolver; 	tflite::InterpreterBuilder builder(*model, resolver); 	builder(&interpreter); 	if (!interpreter) { 		std::cerr AllocateTensors(); 	if (status != kTfLiteOk) { 		std::cerr << ""Failed to allocate tensors.""<<status << std::endl; 		return 1; 	} return 0; }`",please wrap all your error messages in  to avoid notifying users whose usernames appear in your error message,"> please wrap all your error messages in >  >  >  > to avoid notifying users whose usernames appear in your error message apologies, i'll do the same next time thanks","Hi,  Thank you for the detailed error log, could you please refer this stackoverflow answer which is for C++ build and please follow the instructions mentioned in this video carefully which will help you to solve your issue If issue still persists after following the video instructions and stackoverflow answer steps please help us with error log to investigate this issue further Thank you for your cooperation and patience.",   I am also seeing these runtimelibrary missmatch errors as well after including all the libs also some other issues., after watching video steps I am getting error working with bazel and working on latest tensorflow r2.17 branch.  ,"Hi,   Thank you for providing the error log, May I know which bazel version are you using ? if you're not using bazel version `6.5.0 `then please try with it and also downgrade the python version to `3.11` or `3.10` and see is it working as expected or not ? Thank you.","> Hi,  >  > Thank you for providing the error log, May I know which bazel version are you using ? if you're not using bazel version `6.5.0 `then please try with it and also downgrade the python version to `3.11` or `3.10` and see is it working as expected or not ? Thank you.   I have used python 3.11 and bazel 6.50 to build the name and then getting the same error log as shared in last conversation.","its really typical to build tf in windows directly so i have built tflite .so file in wsl using    can you specify how we can get the include files as the documention says we have to extract the same from ourself from the source, but it will be best if you can specify what folders I need to extract so that I can directly link them without any issue of linking, or you can share some docs that specify paths that need to be extracted according to r2.17 branch.","Hi,  I believe you're able to see built library `libtensorflowlite.so` at this location :`/bazelbin/tensorflow/lite/libtensorflowlite.so` so as far I know you'll have to copy the `tensorflow` folder from this path `/bazelbin/tensorflow/` and you'll have to add `flatbuffers` folder either by cloning this repo :https://github.com/google/flatbuffers or download zip file from here and unzip it keep `flatbuffers` folder under the `include` folder in the C++ project  Could you please give it try and see is it working as expected or not ?  Thank you for your cooperation and patience.","> Hi,  >  > I believe you're able to see built library `libtensorflowlite.so` at this location :`/bazelbin/tensorflow/lite/libtensorflowlite.so` so as far I know you'll have to copy the `tensorflow` folder from this path `/bazelbin/tensorflow/` and you'll have to add `flatbuffers` folder either by cloning this repo :https://github.com/google/flatbuffers or download zip file from here and unzip it keep `flatbuffers` folder under the `include` folder in the C++ project >  > Could you please give it try and see is it working as expected or not ? >  > Thank you for your cooperation and patience.   the path you specified is not having main header files like interpreter.h, model.h etc. so I am thinking of extracting all the header files that are available in /tensorflow/lite folder. Is it the right practice for the same, will it give any linking error? Also I have cloned the flatbuffers and in the same I found the include folder and I have added the same include folder only in my include folder.","Hi,   Could you please refer this TensorFlow Lite C++ minimal example with tensorflow/lite/examples/minimal/CMakeLists.txt there is similar issue which got resolved by using the `CMakeLists.txt` from minimal example refer that issue https://github.com/tensorflow/tensorflow/issues/60779 which may help you to solve your issue. Thank you.","thanks, it resolved the issue.",Are you satisfied with the resolution of your issue? Yes No
yi,Zlisch,tensorflow lite `AllocateTensors()` breaks on raspberry pi pico w," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.15.0  Custom code Yes  OS platform and distribution Raspberry Pi Pico W  Mobile device _No response_  Python version 3.11.9  Bazel version _No response_  GCC/compiler version Apple clang version 14.0.3 (clang1403.0.22.14.1)  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm trying to build a Keras mdnrnn model and get a very basic prediction program running on my Raspberry Pi Pico W, which has 264KB of SRAM, and 2MB of onboard flash memory. The Keras model is converted to a tflite model by `tflite_file = model_to_tflite(inference_mdrnn.model, model_keras_file)`. It's then converted to a `model.cc` by `xxd i model.tflite > model.cc`. I followed the tinyML book example to build my basic prediction program, but adding the line `TfLiteStatus allocate_status = interpreter>AllocateTensors();` breaks the program and I cannot see any serial output from the connected pico. I have included my program source. I'm wondering if `kTensorArenaSize` is not large enough or is too large. But I do not know whether this is the case and if it is how to set up a correct `k",2024-09-16T04:47:52Z,stat:awaiting response type:bug comp:lite TF 2.15,closed,0,6,https://github.com/tensorflow/tensorflow/issues/75831,"Hi,   Thank you for bringing this issue to our attention, as far I know there might be the MDNRNN model you're using too complex for the limited memory of the raspberry pi pico w so please consider simplifying the model by reducing the number of layers, neurons or data types. The `kTensorArenaSize` value might be insufficient. Experiment with increasing it gradually until allocation succeeds but be mindful of memory limitations and also check if there are any other processes or libraries consuming significant memory on raspberry pi pico w. Please make sure that the input data you're providing to the model is valid and compatible with its expected format and dimensions. verify that the input tensor shape matches the model's expected input shape. If possible optimize your model using techniques like quantization or pruning to reduce its memory footprint for that please follow our official documentation which will help help you to optimize your model.  If issue still persists could you please post this issue in the tflitemicro repo for further help ?  Thank you for your cooperation and patience.","Thanks for the reply  I will try the optimization you mentioned to make the model as small as possible. I also noticed though `model.tflite` can be quite small, e.g. 41KB, the `model.cc` created by hex dump can be 251KB large. Is there anything that can be done?","Hey  , Regarding your query on model size blowup on conversion from Tflite to C data array. This is happening because HEX dump naturally takes more space than the Tflite optimised storing format, but once you compile the model. .tflite file. Here is the original thread regarding this issue... https://github.com/tensorflow/tensorflow/issues/43749issuecomment703227741","I have tried with a much reduced model (model size 13092B,  parameters = 369) but it still doesn't work. I've raised a new issue to the tflitemicro repo.","Hi,  Thank you for the update and trying things, I see you've posted this issue here https://github.com/tensorflow/tflitemicro/issues/2686 for further help so please feel free to close this issue from your end If you need any further help with TensorFlow core or TensorFlow Lite (now renamed as LiteRT)  please feel free to post your issue Thank you for your cooperation and patience.",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],#tf-data Allows `tf.data.experiemental.get_model_proto` to accept `NumpyIterator` as input,tfdata Allows `tf.data.experiemental.get_model_proto` to accept `NumpyIterator` as input,2024-09-13T18:29:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75725
yi,copybara-service[bot],#tf-data Fixes `data_service_ops.py` broken doc test in NumPy 2.0 upgrade by adding `np_array.item()`,tfdata Fixes `data_service_ops.py` broken doc test in NumPy 2.0 upgrade by adding `np_array.item()`,2024-09-13T17:11:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75717
yi,copybara-service[bot],[XLA:GPU] Fix InsertOp's lowering when applying indices,[XLA:GPU] Fix InsertOp's lowering when applying indices InsertOp is supposed to take the map results of the vector as inputs into its map and have no symbols as it doesn't create a loop for this.  Add a verifier to check that InsertOp is lowerable given these constraints.,2024-09-13T13:22:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75706
yi,copybara-service[bot],Internal breakage.,Internal breakage. Reverts 5adafde02706dfe24e3a56744d0baecfdc3df90b FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16921 from zhenyingliu:ncclbufferoutput b5e43d6455adc49f5ac99a9a9e95cf495eb46170,2024-09-13T00:53:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/75676
yi,copybara-service[bot],[XLA:GPU] Remove the forced cast to `f32` when generating Triton reductions.,"[XLA:GPU] Remove the forced cast to `f32` when generating Triton reductions. Triton can now handle reductions of types other than `f32`. Removing the cast makes a lot of the code simpler and also yields more ""correct"" numerics  in some cases this means less precise. I had to relax the error tolerance in a couple of `f16` tests because the calculations are now actually done in `f16` unlike the previous `f32`. Simplifications enabled by this:  No more casts in the generated code.  Removed the need for the `is_within_reduction_computation` parameter in `triton_support.cc`.  Removed a lot of cases that needed `skip_failure_branch_to_avoid_crash`.",2024-09-12T20:08:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75653
rag,copybara-service[bot],Move CUDA configuration paragraph up in the document.,Move CUDA configuration paragraph up in the document.,2024-09-12T16:24:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75640
yi,copybara-service[bot],PR #16913: [PJRT:GPU] Enable creating topology without a GPU device,"PR CC(Eager: tf.linalg.inv(tf.transpose(mat)) has undefined shape in function with tfe.defun): [PJRT:GPU] Enable creating topology without a GPU device Imported from GitHub PR https://github.com/openxla/xla/pull/16913 Currently PJRT_TopologyDescription_Create always creates topology from the local client. This requires having a local GPU device. This patch allows explicitly specifying topology shape and device config in PJRT_TopologyDescription_Create call, without querying local client. This enables deviceless compilation. Copybara import of the project:  bc85038cbdfbed4b43b5859037515ef9049ecfec by Jaroslav Sevcik : [PJRT:GPU] Enable creating topology without a GPU device  e4eb44ecf356f0c7d859c969ff38b69e52d569c5 by Jaroslav Sevcik : Enable overlaying topology on local device  ff25a5790bfba35262f9649cb6df234fc2af5a3d by Jaroslav Sevcik : Address reviewer comments  8bfb7a251f1f687b65e6db3744dc7c76d60a321e by Jaroslav Sevcik : Cleanup Merging this change closes CC(Eager: tf.linalg.inv(tf.transpose(mat)) has undefined shape in function with tfe.defun) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16913 from jarosevcik:devicelesstopologycreation 8bfb7a251f1f687b65e6db3744dc7c76d60a32",2024-09-12T13:39:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75632
yi,copybara-service[bot],internal BUILD rule visibility,internal BUILD rule visibility,2024-09-12T02:54:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75617
rag,copybara-service[bot],mlir_hlo_to_hlo: Propagate diagnostics from PrepareForExport.,mlir_hlo_to_hlo: Propagate diagnostics from PrepareForExport. Propagating diagnostics back to the resulting Status helps make it clear what has gone wrong. A new unit test for interfaces above the MLIR level (like this) is added to allow broader test coverage (and avoid linking Shape and StableHLO dialects to xlatranslate).,2024-09-12T00:08:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75613
yi,copybara-service[bot],Add explicit includes to rocm_executor.h,Add explicit includes to rocm_executor.h A recent refactoring broke the ROCm build as it was relying on transitive includes. This change adds explicit includes for all the symbols used in the header.,2024-09-11T12:14:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75564
yi,hansu2022,Bazel build fails on RISC-V (riscv64) architecture for TensorFlow 2.17.0, Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution Linux openeulerriscv42 6.6.0  Mobile device no  Python version 3.11.6  Bazel version 6.5.0  GCC/compiler version 12.3.1  CUDA/cuDNN version no  GPU model and memory no  Current behavior? I am trying to build TensorFlow 2.17.0 on a RISCV (riscv64) architecture using Bazel 6.5.0. The Bazel version is installed from an RPM package that has successfully passed C++ and Java tests in examples. When I run the following command to build the TensorFlow pip package:The build fails with the following error:RROR: An error occurred during the fetch of repository 'python': No platform declared for host OS linux on arch riscv64 It seems that the `riscv64` architecture is not supported in the current Bazel rules for Python toolchains.  Full error log:ERROR: An error occurred during the fetch of repository 'python': No platform declared for host OS linux on arch riscv64 ERROR: Error computing the main repository mapping: no such package '//': No platform declared for host OS linux on arch riscv64  Standalone code to reproduce the issue   Relevant log output,2024-09-11T09:46:32Z,stat:awaiting response type:support stale subtype:bazel 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/75555,", Looks like a similar type of issue is more related to Bazel and raised in the Bazel repo which the issue is open state https://github.com/bazelbuild/bazel/issues/23018 Please take a look and try to follow the same issue for the updates. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Fix various tag assignments in XLA,Fix various tag assignments in XLA I'm trying to make the `gpu` and `no_rocm` tags consistent in the code base in the sence that a not`gpu`tagged target doesn't depend on a `gpu` tagged target unconditionally. This change fixes the last remaining inconsistencies.,2024-09-11T08:48:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75553
yi,copybara-service[bot],"PR #17021: [NV] Use FP8 conversion intrinsics, when available","PR CC(sigmoid_cross_entropy Docstring bug): [NV] Use FP8 conversion intrinsics, when available Imported from GitHub PR https://github.com/openxla/xla/pull/17021 The previous CC(Fix incorrect reference DOI number/link for GDR) was rolled back. This PR addresses some comments from it. PTX ""cvt"" instruction supports converting to/from FP8 types. The NV hardware supports E4M3FN and E5M2 types. This PR updates the MLIR emitter to use this instruction instead of emitting a long sequence of operations (this matters in computebound FP8 kernels). The NVVM intrinsic allows converting two FP8 values with a single instruction, but as the emitter is elementwise, only one of the inputs is used. This is wasteful, but still much faster than emitting the sequence of instructions. Before ptx 7.8 (cuda 11.8), the instruction is not supported. Starting with ptx 8.1 (cuda 12.1), the instruction is supported for sm89+. Between those versions, the instruction is supported for sm90+, thas is, if trying to compile on Ada (sm89) with cuda version : [NV] Use FP8 conversion intrinsics, when available The previous https://github.com/openxla/xla/pull/16734 was rolled back. This PR addresses some comments from CC(Fix incorrect refere",2024-09-11T06:35:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75549
rag,copybara-service[bot],[tf.data] Fix broken sparse/ragged iterators on TPU devices.,"[tf.data] Fix broken sparse/ragged iterators on TPU devices. In a recent change, we added a colocation constraint between iterator ops and the sparse/ragged decoding ops that transform their outputs into structured tensors. This broke some workloads that attempted to prefetch SparseTensor and RaggedTensor objects to TPU memory, because the relevant decoding kernels were not registered for `DEVICE_TPU`. This change adds the missing kernel registrations, using the same hostmemory annotations that are present for `DEVICE_GPU` and preserving the previous behavior (when the decoding ops would fall back to running on some `DEVICE_CPU`... although not necessarily in the same process).",2024-09-10T21:11:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75534
rag,copybara-service[bot],PR #16696: Various macOS QOL enchancements,"PR CC(Add stream selection support for `tf.contrib.ffmpeg.decode_video`): Various macOS QOL enchancements Imported from GitHub PR https://github.com/openxla/xla/pull/16696 This PR adds various small quality of life improvements to macOS builds:  drop the `.so` suffix for PjRt plugin targets (`.dylib` on macOS)  add compatibility with Apple Command Line Tools (no need for Xcode anymore)  only export the `GetPjrtApi` symbol on macOS  leverage bazel's `cc_binary.additional_linker_inputs` instead of using `deps` It is probable the `.so` change my break some other builds, but I couldn't find any use in the XLA repo to patch ? Copybara import of the project:  2d392b016730e8811ea72f90d9b5ee67126e61e6 by Steeve Morin : [PjRt] Only export GetPjrtApi symbol on macOS Also add missing macOS linkops, remove the .so suffix to the plugin targets and add additional_linker_inputs for the linker script instead of deps.  9dbba3433bd88d3db95f1647782ba3bf9d3462cf by Steeve Morin : Do not force DEVELOPER_DIR on macOS It's is autodetected by Bazel and supports building using only the Apple Command Line Tools.  336122e2fb0e3d7dd93b5a4a30f7551d8e1a21b6 by Steeve Morin : Set the macosx deployment target via the bazel command lin",2024-09-10T20:58:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/75530
yi,copybara-service[bot],"PR #17021: [NV] Use FP8 conversion intrinsics, when available","PR CC(sigmoid_cross_entropy Docstring bug): [NV] Use FP8 conversion intrinsics, when available Imported from GitHub PR https://github.com/openxla/xla/pull/17021 The previous CC(Fix incorrect reference DOI number/link for GDR) was rolled back. This PR addresses some comments from it. PTX ""cvt"" instruction supports converting to/from FP8 types. The NV hardware supports E4M3FN and E5M2 types. This PR updates the MLIR emitter to use this instruction instead of emitting a long sequence of operations (this matters in computebound FP8 kernels). The NVVM intrinsic allows converting two FP8 values with a single instruction, but as the emitter is elementwise, only one of the inputs is used. This is wasteful, but still much faster than emitting the sequence of instructions. Before ptx 7.8 (cuda 11.8), the instruction is not supported. Starting with ptx 8.1 (cuda 12.1), the instruction is supported for sm89+. Between those versions, the instruction is supported for sm90+, thas is, if trying to compile on Ada (sm89) with cuda version : [NV] Use FP8 conversion intrinsics, when available The previous https://github.com/openxla/xla/pull/16734 was rolled back. This PR addresses some comments from CC(Fix incorrect refere",2024-09-10T17:59:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/75519
yi,copybara-service[bot],PR #16236: Align the scheduling name with the instruction name after HLO rematerialization,PR CC(Branch 182474037): Align the scheduling name with the instruction name after HLO rematerialization Imported from GitHub PR https://github.com/openxla/xla/pull/16236 This CL is to fix the assertion failures in HLO verifier when checking scheduling names after HLO rematerialization. Copybara import of the project:  274920bc7e7de159224bc8fa2cd7781eecbf1bb6 by Jane Liu : Align the scheduling name with the instruction name after HLO rematerialization.  d40260eb1846df37ce21faacc4c6dcb66c840b78 by Jane Liu : Simplify the code by changing hlo_rematerialization only.  1c1c76d42b05839563e4a44cb367ce091f44d00d by Jane Liu : Only update scheduling_name for mismatched instructions Merging this change closes CC(Branch 182474037) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16236 from zhenyingliu:rematschedulingname 1c1c76d42b05839563e4a44cb367ce091f44d00d,2024-09-10T10:36:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/75467
yi,jchwenger,Custom `tf.keras.metrics.Metric` example fails on GPU in TF 2.17 (but not on nightly): is it possible to get it to work on 2.17?," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.17  Custom code Yes  OS platform and distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory T4  Current behavior? When running code (from *Deep Learning with Python*, for teaching) in Colab with the standard TF 2.17, defining a custom metric seems to fail to move variables to the GPU. This goes away with TF nightly, but I wondered if there was something that could be specified in the class in 2.17 to solve the issue? Notebook here.  Standalone code to reproduce the issue   Relevant log output ",2024-09-10T09:55:00Z,stat:awaiting response type:bug comp:keras 2.17,closed,0,3,https://github.com/tensorflow/tensorflow/issues/75465,"Hi **** , I tried to run your code on Colab using TF v2.16.1, 2.17.0 and faced the same issue. Please find the gist here for reference. Please post this issue on kerasteam/keras repo. as this issue is more related to keras. Thank you!","Sounds good , I will, thanks! ",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],"PR #16734: [NV] Use FP8 conversion intrinsics, when available","PR CC(Fix incorrect reference DOI number/link for GDR): [NV] Use FP8 conversion intrinsics, when available Imported from GitHub PR https://github.com/openxla/xla/pull/16734 PTX ""cvt"" instruction supports converting to/from FP8 types. The NV hardware supports E4M3FN and E5M2 types. This PR updates the MLIR emitter to use this instruction instead of emitting a long sequence of operations (this matters in computebound FP8 kernels). The NVVM intrinsic allows converting two FP8 values with a single instruction, but as the emitter is elementwise, only one of the inputs is used. This is wasteful, but still much faster than emitting the sequence of instructions. Before ptx 7.8 (cuda 11.8), the instruction is not supported. Starting with ptx 8.1 (cuda 12.1), the instruction is supported for sm89+. Between those versions, the instruction is supported for sm90+, thas is, if trying to compile on Ada (sm89) with cuda version < 12.1, the ptxas will complain.. Reference: https://docs.nvidia.com/cuda/parallelthreadexecution/index.htmldatamovementandconversioninstructionscvt (see ""PTX ISA Notes"" and ""Target ISA Notes""). Reverts 4356505d79f726c2671767ade9b926d9fdce50ea",2024-09-10T06:09:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/75454
yi,copybara-service[bot],PR #16921: [PJRT:GPU] Treat GPU collective memory space as device memory space,PR CC(Improve formatting of shapes in tf.losses documentation): [PJRT:GPU] Treat GPU collective memory space as device memory space Imported from GitHub PR https://github.com/openxla/xla/pull/16921 This is a regression fix when using xla_gpu_enable_nccl_user_buffers=true. Return device memory space when collective memory space is used as an output on GPU. Copybara import of the project:  8113e6fbe23d5902ecdd406793555c602c1b7f81 by Jane Liu : Treat collective memory space as device memory space when using as an output  b5e43d6455adc49f5ac99a9a9e95cf495eb46170 by Jane Liu : fix the test Merging this change closes CC(Improve formatting of shapes in tf.losses documentation) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16921 from zhenyingliu:ncclbufferoutput b5e43d6455adc49f5ac99a9a9e95cf495eb46170,2024-09-09T19:00:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75430
yi,copybara-service[bot],"PR #16734: [NV] Use FP8 conversion intrinsics, when available","PR CC(Fix incorrect reference DOI number/link for GDR): [NV] Use FP8 conversion intrinsics, when available Imported from GitHub PR https://github.com/openxla/xla/pull/16734 PTX ""cvt"" instruction supports converting to/from FP8 types. The NV hardware supports E4M3FN and E5M2 types. This PR updates the MLIR emitter to use this instruction instead of emitting a long sequence of operations (this matters in computebound FP8 kernels). The NVVM intrinsic allows converting two FP8 values with a single instruction, but as the emitter is elementwise, only one of the inputs is used. This is wasteful, but still much faster than emitting the sequence of instructions. Before ptx 7.8 (cuda 11.8), the instruction is not supported. Starting with ptx 8.1 (cuda 12.1), the instruction is supported for sm89+. Between those versions, the instruction is supported for sm90+, thas is, if trying to compile on Ada (sm89) with cuda version : [NV] Use FP8 conversion intrinsics, when available PTX ""cvt"" instruction supports converting to/from FP8 types. The NV hardware supports E4M3FN and E5M2 types. This PR updates the MLIR emitter to use this instruction instead of emitting a long sequence of operations (this matters in computebou",2024-09-09T11:13:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75399
yi,n-kats,Fail import tensorflow if rules_python is installed.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17.0  Custom code No  OS platform and distribution Ubuntu 24.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When I installed both TensorFlow and Mesop simultaneously, I encountered an error while importing TensorFlow. It seems that installing Mesop also installs `rules_python`, which causes TensorFlow to malfunction. Specifically, at the following code: https://github.com/tensorflow/tensorflow/blob/bc90265931a0ca90ee2e7c2ef988ad6634733961/tensorflow/python/platform/resource_loader.pyL117L119 `r` becomes `None`, leading to an error. According to `rules_python`: https://github.com/bazelbuild/rules_python/blob/0.26.0/python/runfiles/runfiles.pyL40 It seems possible that `r = runfiles.Create()` can indeed return `None`. While it seems the issue could be linked to Mesop installing rules_python, there might also be an underlying problem with TensorFlow. I would appreciate your help in investigating further.  Standalone code to reproduce the",2024-09-06T22:08:00Z,stat:awaiting tensorflower type:build/install subtype: ubuntu/linux 2.17,open,2,2,https://github.com/tensorflow/tensorflow/issues/75288,"kats, As a Temporary fix, Could you please try to add python definitions in the Project's {project}/WORKSPACE file, and it might  work and the Project can be build succesfully. Thank you!","In my project, I'm not using Bazel or a WORKSPACE file. Should I still create a WORKSPACE file in this case?  In my project, I use TensorFlow and Mesop (installed via pip), and there is no build process involved.  However, in the Mesop project, the developers use Bazel and rules_python, and the Mesop wheel file includes rules_python (I believe this is causing the issue)."
yi,kevo200,traceback issue, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.0  Custom code No  OS platform and distribution windows 10  Mobile device _No response_  Python version 3.11.7  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? when trying to run the code shows  Standalone code to reproduce the issue   Relevant log output ,2024-09-06T20:00:42Z,stat:awaiting response type:bug stale 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/75280,", Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:  https://github.com/tensorflow/tensorflow/issues/61887 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,richardwhitehead,AutoGraph error: OP_REQUIRES failed at strided_slice_op.cc:266," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The following code works and produces the expected output when run in eager mode, but when using AutoGraph it produces bizarre error messages. The line numbers reported are not the actual location of the errors. Replacing the contents of _safe_lookup_pixel so it always returns zeros makes the error go away, though of course the output is not then correct.  My installation is using docker image based on the NVIDIA image, details below.  Standalone code to reproduce the issue ",2024-09-06T17:38:01Z,stat:awaiting response type:bug comp:keras comp:xla TF 2.16,closed,0,6,https://github.com/tensorflow/tensorflow/issues/75269,"Here is a different (and simpler) version of the code. This version works in AutoGraph mode in TensorFlow 2.13.1 on Python 3.8.10, but produces the above error when run in TensorFlow 2.16.1 on Python 3.10.12. ",", I tried to execute the code on the latest tensorflow v2.17 and it was executed without any issues. Kindly find the gist of it here and try to update to 2.17. Thank you! ","Thank you very much for investigating this.  I tried updating my docker container to the latest version provided by NVIDIA (24.09tf2py3) but that still seems to be TensorFlow v2.16.1. Having lost many days fighting version incompatibilities in the past, and given that Nvidia has chosen not to upgrade yet, I am unwilling to try updating TensorFlow and will wait until Nvidia release a container image with a newer version. I have no doubt that what you say is true and so please feel free to close the issue. Many thanks, Richard From: tilakrayal ***@***.***>  Sent: 25 September 2024 15:45 To: tensorflow/tensorflow ***@***.***> Cc: richardwhitehead ***@***.***>; Mention ***@***.***> Subject: Re: [tensorflow/tensorflow] AutoGraph error: OP_REQUIRES failed at strided_slice_op.cc:266 (Issue CC(AutoGraph error: OP_REQUIRES failed at strided_slice_op.cc:266))   , I tried to execute the code on the latest tensorflow v2.17 and it was executed without any issues. Kindly find the gist of it here   and try to update to 2.17. Thank you! — Reply to this email directly, view it on GitHub  , or unsubscribe  . You are receiving this because you were mentioned.   Message ID: ***@***.*** ***@***.***> >",", Glad the issue was resolved. Could you please feel free to move this issue to closed status. Thank you!",Fixed in v2.17,Are you satisfied with the resolution of your issue? Yes No
yi,RahulVadisetty91,Enhanced version of Benchmarking Script incorporating Artificial Intelligence and Optimization.," 1. **Summary**:   This pull request brings in major changes to the benchmarking script and incorporates the use of AI elements and also makes the code more readable and easy to maintain. The major changes that have been integrated are the utilization of the `AIDrivenMetricSelection` component that automates the identification of performance metrics thus providing more specific and relevant summary of benchmarking. The code in the script has been revamped in order to decrease the cognitive load, especially in the `summarize` function so that it is easier to modify. Moreover, the enhancement has been made in reporting with the help of the new function of `combine_summaries` which integrates the details of benchmarking results in a better manner for the purpose of comparison. The benchmarking process has also been improved by the use of context managers and temporary folders as a means of ensuring that the process is accurate. Last but not the least, the script has been updated in such a way that it can upload the benchmark summaries to the Hugging Face Hub for easy sharing and collaboration.    2. **Related Issues**:    To this, the problem of intricate benchmarking processes was solved by the AIbased me",2024-09-06T10:42:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/75246
yi,adrejohan,Segmentation Fault memcpy in tim::vx::TensorImpl::CopyDataToTensor TFLite with VX Delegate run," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.14.0  Custom code Yes  OS platform and distribution Linux Yocto nanbield  Mobile device iMX8M Plus EVK  Python version 3.11.5  Bazel version _No response_  GCC/compiler version 13.2.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Pardon me I tried to run this model in C++ using tensorflow for NPU run. but it seems I always had Segmentation Fault when Invoke() is triggered I also test this model using the python code, and it works. so I think the model itself is not the problem I dont have many experience with C++, but from the gdb backtrace it seems the fault triggered when tim::vx::TensorImpl::CopyDataToTensor(void const*, unsigned int) () triggered in Invoke if you had any idea regarding this, it would be helpful, thank you  Standalone code to reproduce the issue ",2024-09-06T09:27:33Z,stat:awaiting response type:support stale comp:lite TF2.14,closed,0,7,https://github.com/tensorflow/tensorflow/issues/75241,"Hi,  Could you please take look into this issue ? Thank you.","Hi , Can you explain the context in which you are running this code a little more? Like... I'm not sure what you mean by > in C++ using tensorflow for NPU run Are you running an executable? What NPU are you using (on a phone? iOS? Android?)? Are you running it on a server somewhere (If so, what command did you use to run it?)? How are you running it? What other information do you think I would need to reproduce this?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"Hey there, I am experiencing the same issue. I believe in a similar usecase to the OP. I am running a tflite model through a C++ application on an imx8mp. We are trying to run our model on the imx8's NPU using the vx_delegate, and we too are seeing the same segfault in the CopyDataToTensor() function. Our application is running on Yocto linux build, version 5.15.71. Here's a copy of the stacktrace: ",Hi  can you create a new issue for this with your data? Additionally are you able to share your model so that we may reproduce this? (If you are unable to share the full model  maybe you see if you can reproduce with a smaller model which has one of all the same ops you used in the full model  adjust as needed to get this through). Thanks for your help.
yi,copybara-service[bot],Fix KV Cache for GPU,Fix KV Cache for GPU  Set KV Cache `buffer_handle` before applying delegate  Fix typo on setting output `buffer_handle`,2024-09-06T00:34:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/75227
rag,dnoliver,Save and Load Notebook use a bad checkpoint_path file name, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version v2.17.0rc12gad6d8cc177d 2.17.0  Custom code No  OS platform and distribution Windows 11  WSL  Docker Container  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The cell:  Throws the following value error:   Standalone code to reproduce the issue   Relevant log output ,2024-09-05T18:42:39Z,stat:awaiting response type:bug stale comp:keras 2.17,closed,0,8,https://github.com/tensorflow/tensorflow/issues/75196,"Then it works after you change the line to `checkpoint_path = ""training_1/cp.ckpt.weights.h5""`","The sample runs into other problems as well. Like `trainning_2` folder needs to be created manually, `model.load_weights(latest)` complains about `ValueError: File format not supported: filepath=None. Keras 3 only supports V3 `.keras` and `.weights.h5` files, or legacy V1/V2 `.h5` files.`, similar problem in `model.save_weights('./checkpoints/my_checkpoint')`, the line `model.save('saved_model/my_model') ` complains with `ValueError: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=saved_model/my_model.`","Hey  ,  I looked into the issue and found out there was several errors in the specified notebook. I have created a pull request regarding same  https://github.com/tensorflow/docs/pull/2324",", The pr has been assigned for reviewing and once it is merged this issue will move to closed status. Thank you!",", The PR which was raised for the similar issue has been merged and also I tried to execute the official doc code and it was executed without any issues/errors. Kindly find the gist of it here. https://github.com/tensorflow/docs/pull/2324 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,dnoliver,Overfit and Underfit Notebook fails on compile_and_fit, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version v2.17.0rc12gad6d8cc177d 2.17.0  Custom code No  OS platform and distribution Windows 11  WSL  Docker Container  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Trying to run the `overfit_and_underfit.ipynb` sample fails. Error log (and the full Traceback is below as well):   Standalone code to reproduce the issue   Relevant log output ,2024-09-05T18:21:06Z,stat:awaiting response type:bug stale 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/75194,"Hi , I looked into this issue and have opened a pull request regarding the same  https://github.com/tensorflow/docs/pull/2325","Hi **** , The pr has been assigned for reviewing and once it is merged this issue will move to closed status. Thank You!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,tagunil,"Calibrator segfaults trying to log the ""while"" operation"," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): 2.17.0  2. Code reproducer.zip  3. Failure after conversion Segmentation fault (signal 11) during conversion  5. (optional) Any other info / logs The ""while"" operation does a check if an output tensor of the body subgraph is the same as the corresponding input tensor. If it's the case, it deallocates its own output tensor. The check is done at the prepare stage, so the affected tensor is already included in the ""loggable_outputs"" list by the calibrator. Then the calibrator tries to read the data from the deallocated tensor and segfaults. I've debugged it up to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/calibration/calibrator.ccL267 and found that the `tensor.data.f == nullptr`. The check in question was introduced between 2.13 and 2.14, so it might be considered a regression: https://github.com/tensorflow/tensorflow/commit/7d49fd431ee5cebbb76eda88bc17e48921e10c85",2024-09-05T01:23:29Z,awaiting review stat:awaiting tensorflower type:bug comp:lite TFLiteConverter 2.17,open,0,19,https://github.com/tensorflow/tensorflow/issues/75140,"Hi,   Thank you for bringing this issue to our attention and if possible could you please help us with Google colab notebook along with model to replicate the same behavior from our end to investigate this issue further from our end ?  Thank you for your cooperation and patience.","Hi , sure. I just don't want to clutter the issue with irrelevant details, so I'll make a simple reproducer and share the link with you. ",", I've attached a simple Jupyter notebook reproducing the crash to the issue. https://github.com/userattachments/files/16894947/reproducer.zip","BTW, I know that the ""while"" op is not quantizable as for now, but that's a separate issue. We might consider selective quantization, for example, but the segfault in the calibrator won't allow us doing that.","Hi,   I apologize for the delayed response, I tried to run your jupyter notebook in Google colab with `CPU` and `GPU` runtime after this line `converter.convert()` the colab session is crashing for an unknown reason for reference I've added runtime log here so did you face that issue while running your code in Google colab ? Thank you for your cooperation and patience.","Hi , I'm running it locally, but it crashes all the same, and it's the crash itself that I'm reporting here, because I've traced it to the root cause and it's a null pointer dereference inside the calibrator.",", If you have a workaround to address this issue we welcome a pull request. Our team will review it carefully and merge it if it aligns with our coding standards. Hi,  Please take look into this issue. Thank you","Hi , forgive me if I'm incorrect but isn't:  equivalent to  If so, can you use that to get past this issue?","Hi , It's just a simplest reproducer for the crash I could think of. Of course, you are right in that particular case, but, unfortunately, our real model is much more complicated and cannot be implemented without a while op. We want to quantize it partially, but we still need to run the calibrator over the whole model for that.","I was able to replicate with your original code... colab gist here for convenience. I attempted to do a an AIEdgeTorch conversion instead to see if it could solve this:  This runs into a dynamic slicing issue with PyTorch Export, I'm guessing because most people would do the reduce sum operation instead. , I would take a second look to see if perhaps you can vectorize your code and avoid a while loop/range. , can you please take a look? Thanks.",", unfortunately, our use case is pretty specific and it would take a lot of effort to rework it in a vectorized way, even if possible. By the way, we've worked around the issue by adding a null pointer check to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/calibration/calibrator.ccL267. I would make a pull request already if the open source contribution policy in our company were not so uncertain, although we're trying to clarify it.","And yes, dynamic slicing is another pain point, but for now we work it around by using tf.slice directly instead of relying on Python slicing.","Hmm... a check is probably correct ... being optional doesn't mean in all cases it's not ""loggable"", but it does mean that that tensor sometimes is not allocated. Edit: Actually just below, optional tensors are not loggable: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/calibration/calibrator.ccL283 ... so root solution will be different ... , do you have a stack trace? Feel free to censor/change PII.","Hi , The problem with the optional tensor logic in this particular case is that the list of loggable tensors is created before the while op decides to deallocate one of its tensors (the deallocation happens at the prepare stage). So either we need to create the loggable list later, just between prepare and eval, or we need to add an additional check before trying to actually log the tensor. The latter option works at least in our case. Of course, I can make a stack trace for you later today or tomorrow.",", so here's a trace from a debug build of TF v2.17.0, up to the Python wrapper. Let me know if you want me to capture anything else. ","Thanks , my current thinking is to remove the tensor from the loggable collection when we deallocate to keep the state consistent, that or reorder things (but that may cause further butterflies).","Well, , you obviously know better. I'm just saying that a simple null pointer check before logging the tensor also seem to work.","Hi , I propose to fix the segfault now ( CC(Add null pointer check for an output tensor calibration)) and deal with the calibration logic afterwards. As far as I can see, it's better to have no calibration values for an unused tensor than to crash the whole process.","Sounds good... , perhaps you can add a TODO so that we don't forget. (That's my worry with ""covering"" this up) at any rate, let's wait for review."
yi,copybara-service[bot],Internal change only,Internal change only,2024-09-04T23:08:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75136
rag,copybara-service[bot],PR #16696: Various macOS QOL enchancements,"PR CC(Add stream selection support for `tf.contrib.ffmpeg.decode_video`): Various macOS QOL enchancements Imported from GitHub PR https://github.com/openxla/xla/pull/16696 This PR adds various small quality of life improvements to macOS builds:  drop the `.so` suffix for PjRt plugin targets (`.dylib` on macOS)  add compatibility with Apple Command Line Tools (no need for Xcode anymore)  only export the `GetPjrtApi` symbol on macOS  leverage bazel's `cc_binary.additional_linker_inputs` instead of using `deps` It is probable the `.so` change my break some other builds, but I couldn't find any use in the XLA repo to patch ? Copybara import of the project:  2d392b016730e8811ea72f90d9b5ee67126e61e6 by Steeve Morin : [PjRt] Only export GetPjrtApi symbol on macOS Also add missing macOS linkops, remove the .so suffix to the plugin targets and add additional_linker_inputs for the linker script instead of deps.  9dbba3433bd88d3db95f1647782ba3bf9d3462cf by Steeve Morin : Do not force DEVELOPER_DIR on macOS It's is autodetected by Bazel and supports building using only the Apple Command Line Tools.  336122e2fb0e3d7dd93b5a4a30f7551d8e1a21b6 by Steeve Morin : Set the macosx deployment target via the bazel command lin",2024-09-04T19:34:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/75120
yi,copybara-service[bot],[IFRT] Change `CustomCallProgram::serialized_program_text` to use `absl::Cord`,"[IFRT] Change `CustomCallProgram::serialized_program_text` to use `absl::Cord` `CustomCallProgram::serialized_program_text` may contain a serialized string of a complex struct, where some fields of the struct are small metadata that need to be read by a runtime layer, ideally without decoding the rest of the string. There are a few ways to enable this access, while using `absl::Cord` as a serialized form is broadly compatible with various user representations of serialized program text. For example, the user may define a proto with large fields defined to use `CORD`; then, serialization to `absl::Cord` will avoid copying the large fields, and can avoid memory copies when the lower runtime layer deserializes it to access small metadata fields. The proto for `CustomCallProgram` SerDes is not yet using `CTYPE=CORD` because protobuf on arm64 builds for OpenXLA generates a field access code using `std::string`, which is not consistent with `absl::Cord` on other platforms.",2024-09-04T19:32:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75119
yi,Adedeji-hub,Issues with DLL,"ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred:",2024-09-04T18:18:00Z,stat:awaiting response type:support stale,closed,0,4,https://github.com/tensorflow/tensorflow/issues/75111,"hub, Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:  https://github.com/tensorflow/tensorflow/issues/61887 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #16754: Disable VerifyInstructionNameUnchanged() for GPUs due to crashes,"PR CC(Fix issue for JNI library loading): Disable VerifyInstructionNameUnchanged() for GPUs due to crashes Imported from GitHub PR https://github.com/openxla/xla/pull/16754 After hlo verifier was enabled to check instruction name changes, we saw applications failed. We disable this option for GPUs for now until all the fixes are in place. Copybara import of the project:  b18ea845288349b0950a9fe23a99dac341091edb by Jane Liu : Disable VerifyInstructionNameUnchanged() for GPUs as it crashed applications Merging this change closes CC(Fix issue for JNI library loading) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16754 from zhenyingliu:removeschedulingname b18ea845288349b0950a9fe23a99dac341091edb",2024-09-04T07:34:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75090
yi,AnandPolamarasetti,Improve the Existing Lazy Loading and Error Handling of TensorFlow-Keras Integration In Tools Compat Template V1,"This Pull Request refines the inter operation between TensorFlow and Keras by paying special attention to the enhancement of lazy initialization and the enhancement of error handling. The variable TF_USE_LEGACY_KERAS is now handled in a more permissive way to handle case sensitivity issues properly. Also, the script adaptively changes module paths based on the usage of legacy or standard version of Keras, thus making it compatible with multiple TensorFlow configurations. This way, use of tryexcept blocks around module imports guarantees that errors are captured and their messages displayed hence avoiding silent failures. The changes outlined above make the script much better for use and more stable. With improvements in the lazy loading feature and the incorporation of more explicit and detailed exception handling the script has the ability to handle a wider range of TensorFlow and Keras configurations thereby reducing the chances of encountering runtime exceptions. The changes also make the script more compatible across the versions of TensorFlow and help in making it more suitable for different environments.",2024-09-03T16:53:12Z,awaiting review size:M,open,1,5,https://github.com/tensorflow/tensorflow/issues/75038,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi  , Can you please review this PR? Thank you !","Hi  , Can you please review this PR? Thank you !","Hi  , Can you please review this PR? Thank you !","Hi  , Can you please review this PR? Thank you !"
yi,copybara-service[bot],Add canonicalization for xla_gpu.loop of xla_gpu.apply_indexing. Also refactor common logic between this canonicalization and ApplyIndexing fold sequences logic.,Add canonicalization for xla_gpu.loop of xla_gpu.apply_indexing. Also refactor common logic between this canonicalization and ApplyIndexing fold sequences logic.,2024-09-03T12:57:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/75030
yi,copybara-service[bot],"PR #16734: [NV] Use FP8 conversion intrinsics, when available","PR CC(Fix incorrect reference DOI number/link for GDR): [NV] Use FP8 conversion intrinsics, when available Imported from GitHub PR https://github.com/openxla/xla/pull/16734 PTX ""cvt"" instruction supports converting to/from FP8 types. The NV hardware supports E4M3FN and E5M2 types. This PR updates the MLIR emitter to use this instruction instead of emitting a long sequence of operations (this matters in computebound FP8 kernels). The NVVM intrinsic allows converting two FP8 values with a single instruction, but as the emitter is elementwise, only one of the inputs is used. This is wasteful, but still much faster than emitting the sequence of instructions. Before ptx 7.8 (cuda 11.8), the instruction is not supported. Starting with ptx 8.1 (cuda 12.1), the instruction is supported for sm89+. Between those versions, the instruction is supported for sm90+, thas is, if trying to compile on Ada (sm89) with cuda version : [NV] Use FP8 conversion intrinsics, when available PTX ""cvt"" instruction supports converting to/from FP8 types. The NV hardware supports E4M3FN and E5M2 types. This PR updates the MLIR emitter to use this instruction instead of emitting a long sequence of operations (this matters in computebou",2024-09-03T08:54:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/75013
yi,copybara-service[bot],PR #16236: Align the scheduling name with the instruction name after HLO rematerialization,PR CC(Branch 182474037): Align the scheduling name with the instruction name after HLO rematerialization Imported from GitHub PR https://github.com/openxla/xla/pull/16236 This CL is to fix the assertion failures in HLO verifier when checking scheduling names after HLO rematerialization. Copybara import of the project:  52d427fe5ff411491d692737f0d89b422f76e78d by Jane Liu : Align the scheduling name with the instruction name after HLO rematerialization.  31a88a27a5e38f2c3c8d9e8bfd7a6ad0ee0afe8b by Jane Liu : Simplify the code by changing hlo_rematerialization only. Merging this change closes CC(Branch 182474037) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16236 from zhenyingliu:rematschedulingname 31a88a27a5e38f2c3c8d9e8bfd7a6ad0ee0afe8b,2024-09-03T06:41:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74998
yi,hholokai,TFLite Hexagon Delegate not working for QCOM SM6375 SOC.,"I'm trying to get a quantized model running on the Qualcomm SM6375 DSP on a Zebra ET40 tablet, without success.  SOC info:  The FP32 model works fine on CPU and GPU.  However, when I try to run the INT8 model on the DSP, it falls back to running the XNNPack Delegate on CPU, which is slow:  I've tried several different versions of the Hexagon libraries referred to below, but all fall back to the CPU: https://www.tensorflow.org/lite/android/delegates/hexagon This page above has the following warning: > Caution: The currently released versions of the Hexagon delegate, up to version 1.20.0.1, are no longer supported. An updated version of this delegate is expected soon. I've also tried the NNAPI (use_nnapi=true), but that also falls back to the CPU. Any help would be appreciated.",2024-09-02T21:21:47Z,stat:awaiting response type:bug stale comp:lite,closed,0,4,https://github.com/tensorflow/tensorflow/issues/74993,"Hi,   I apologize for the delayed response, The NNAPI and Hexagon delegates are deprecated and no longer supported by TensorFlow Lite. For more information, see the NNAPI Migration Guide and TF Lite delegates documentation. Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Store runtime and driver version as SemanticVersion in DeviceDescription,Store runtime and driver version as SemanticVersion in DeviceDescription This allows replacing a whole bunch of usages of `CUDA_VERSION` and `TF_ROCM_VERSION` in tests by runtime conditionals. `DeviceDescription::runtime_version` and `DeviceDescription::driver_version` exist as a vendor dependent string which is only useful for displaying it to the user. So this change: 1. Renames `DeviceDescription::runtime_version` to `DeviceDescription::runtime_version_string` (the same for the driver version) 2. Introduces new fields `DeviceDescription::runtime_version` and `::driver_version` of type `SemanticVersion` 3. Adds version parsers for CUDA and ROCm and makes the respective executors set the correct runtime and driver version. 4. Fixes up usagesof the old `runtime_version` and `driver_version` fields. Replacing the macros in tests will follow in a subsequent change.,2024-09-02T18:36:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74992
yi,copybara-service[bot],"[XLA:GPU] Add Triton support tests for `parameter`, `constant`, `iota`, and `rng`.","[XLA:GPU] Add Triton support tests for `parameter`, `constant`, `iota`, and `rng`. The change in `triton_fusion_emitter.cc` is to:  Return an error if not `IsScalar`, because the underlying implementation of `EmitConstant` requires a scalar, not just `EffectiveScalar`.  Actually return an error in case the constant is unsupported, not just continue with the follow up cases. They include handling for Elementwise ops (which includes constnats) and that was failing.",2024-09-02T13:54:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74981
yi,Humbulani1234,Tensorflow/Keras_nlp bug," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.17.0  Custom code No  OS platform and distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I have experienced this error while running KerasNLP with Tensorflow as the backend. I tried to thoroughly investigate the issue, and I think the error is essentially with Tensorflow code and not KerasNLP. The following is a detailed description of the error: **Describe the bug** I encountred an error/bug while trying to execute a docstring code example from the file `keras_nlp.src.models.gpt2.causal_lm.py` and I have reproduced the example code below:  The following is a comprehensive description of the error, reproduced below and debugging using `pdb`:  Th error is clear:  `the 1 value`. I've traced the error to the following function from the file `keras.src.backend.tensorflow.trainer`:  The line `data=next(iterator)` computes the labels and therefore the 1 value is created here. The `iterator` argument is a tensorflow `Owned",2024-09-02T11:08:08Z,stat:awaiting response type:bug stale comp:keras 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/74972,", Though the backend is the tensorflow, it contains the Keras3.0 by default and also Unable to register cuBLAS factory error is the known issue in the tensorflow.  Could you please check with the kerasnlp or kerasteam/keras for the quick resolution. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,Adedeji-hub,resolve code compatibility with my system,"ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[1], line 1 > 1 import tensorflow as tf       2 from tensorflow import keras       3 from keras.layers import Input, Dense, Flatten File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:38      35 import sys as _sys      37  Do not remove this line; See https://gith",2024-09-01T09:20:13Z,stat:awaiting response type:support stale,closed,0,7,https://github.com/tensorflow/tensorflow/issues/74951,"import tensorflow as tf from tensorflow import keras from keras.layers import Input, Dense, Flatten from keras.models import Model from keras.applications.vgg16 import VGG16 from keras.preprocessing import image from keras.preprocessing.image import ImageDataGenerator from keras.models import Sequential import numpy as np from glob import glob import matplotlib.pyplot as plt In an attempt to run the code above  lines of import, it came up with an import error. Can you ","hub, Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:  https://github.com/tensorflow/tensorflow/issues/61887 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[3], line 2       1  import required libraries > 2 import tensorflow as tf       3 from tensorflow import keras       4 from keras.models import Sequential File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:85      83     sys.setdlopenflags(_default_dlopen_flags)      84 except ImportError: > 85   raise ImportError(      86       f'{traceback.format_exc()}'      87       f'\n\nFailed to load the native TensorFlow runtime.\n'      88       f'See https://www.tensorflow.org/install/errors '      89       f'for some common causes and solutions.\n'      90       f'If you need help, create an issue '      91       f'at https://github.com/tensorflow/tensorflow/issues '      92       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\user\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",i am having issues with tensorflow
yi,dependabot[bot],Bump the github-actions group with 6 updates,"Bumps the githubactions group with 6 updates:  Updates `google/osvscanneraction` from 1.8.2 to 1.8.4  Release notes Sourced from google/osvscanneraction's releases.  v1.8.4 Bump OSVScanner version https://github.com/google/osvscanner/releases/tag/v1.8.4 v1.8.3 What's Changed  Now uses OSVScanner v1.8.3, see https://github.com/google/osvscanner/blob/main/CHANGELOG.md for full changelog.  New Contributors  @​hogo6002 made their first contribution in google/osvscanneraction CC(Node.js (JavaScript) Wrapper API)  Full Changelog: https://github.com/google/osvscanneraction/compare/v1.8.2...v1.8.3    Commits  678a866 Merge pull request  CC(is a Python 3 support coming soon ?) from google/updatetov1.8.4 6a315db Update unified workflow example to point to v1.8.4 reusable workflows 712a57b Update reusable workflows to point to v1.8.4 actions fa6b699 Update actions to use v1.8.4 osvscanner image b756d11 Merge pull request  CC(Go API) from google/updatescript c63eeb7 Big multiline string dd8ff8f Fix string format 336764a Merge pull request  CC(Updated links in documentation.) from google/renovate_ignore ff89c57 Update package name c615bb5 Merge pull request  CC(Node.js (JavaScript) Wrapper API) from google/updatetov",2024-09-01T08:46:57Z,awaiting review ready to pull size:S dependencies github_actions,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74949
yi,copybara-service[bot],[XLA:GPU] Avoid applying a mask for the full tiles.,[XLA:GPU] Avoid applying a mask for the full tiles. This is a performance optimization that can be applied to all Triton fusions. When the contracting_dimension % block_k is non zero we need to apply the mask. We do that unconditionally but it make sense to do that only for the tiles that are on the border of dim_k. Let's check that the tile does not need masking and skip these ops.,2024-08-31T20:21:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74933
yi,KnightGOKU, `Aborted` issue raised in TensorFlow when using data_flow_ops.SparseConditionalAccumulator and apply_indexed_slices_grad with mismatched data types," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tfnightly 2.18.0.dev20240817  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `Aborted` issue in TensorFlow when using the `data_flow_ops.SparseConditionalAccumulator` API and `apply_indexed_slices_grad` with mismatched data types. The code was confirmed to crash on `tfnightly 2.18.0.dev20240817` (nightlybuild) > 20240831 13:45:53.349441: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (23 vs. 1) float expected, got uint64 Aborted (core dumped)  Standalone code to reproduce the issue   Relevant log output ",2024-08-31T05:41:07Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/74917,", I tried to execute the mentioned code on tfnightly and observed that the check fail is happening due to mismatch of the data type provided as input. I tried with the similar input data type and the code was executed without abort/fail. Kindly find the gist of it here. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[IFRT] Change DeviceList into an interface,"[IFRT] Change DeviceList into an interface This change makes `xla::ifrt::DeviceList` an interface of a device list object defined by an IFRT implementation. This is a preparation for enabling IFRT clients to have visibility to device list objects and use them in an optimized way (e.g., caching the content in a distributed way, identifying device lists by its unique id, etc.). Previous simple device list is moved to `xla::ifrt::BasicDeviceList`. Type change: * `xla::ifrt::DeviceList` => `tsl::RCReference` * `xla::ifrt::DeviceList::Devices` => `xla::ifrt::BasicDeviceList::Devices` Constructor change: * `xla::ifrt::DeviceList(...)` => `xla::ifrt::BasicDeviceList::Create(...)` Method change: * `device_list.size()` => `device_list>size()` * `device_list.devices()` => `device_list>devices()` * `device_list.begin()` (or `.end()`, `.front()`, `[i]`, ...) => `device_list>devices().begin()` (or `>devices().end()`, `>devices().front()`, `>devices()[i]`, ...) * `device_list.DebugString()` => `absl::StrCat(*device_list)`. * `device_list_a == device_list_b` => `*device_list_a == *device_list_b` (`device_list_a == device_list_b` will fail to build). Other: * Some default initialization of `xla::ifrt::DeviceList` uses ",2024-08-31T05:35:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74916
rag,copybara-service[bot],[RaggedTensor] Component tensors in a `RaggedTensorVariant` should always remain on host.,"[RaggedTensor] Component tensors in a `RaggedTensorVariant` should always remain on host. When an iterator is prefetched to GPU and contains a `RaggedTensor`, it was previously possible to trigger a case where the runtime became confused about whether stringtyped values had been copied to device memory. Since we only implement wrapping and unwrapping kernels for CPU, this change fixes that issue by maintaining `RaggedTensorVariant` component tensors in host memory always. We can extend this in future to supporting a ""GPU kernel"" for `RaggedTensorFromVariant` that produces its outputs in host memory.",2024-08-30T18:43:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74896
gemma,copybara-service[bot],Remove xla_gpu_override_gemm_autotuner flag. The flag does not support CuBLAS and CuDNN configs and is not in use.,Remove xla_gpu_override_gemm_autotuner flag. The flag does not support CuBLAS and CuDNN configs and is not in use.,2024-08-30T11:17:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74872
yi,copybara-service[bot],Revert gpu-backend to gpu_any-backend change,Revert gpubackend to gpu_anybackend change I recently claimed that `gpu_any` means any NVIDIA GPU and that it can replace the `gpu` backend when the test is tagged `no_rocm`. That's not entirely correct. The `gpu` backend expands to a number of `gpu_[pvah]100` test which `gpu_any` does not. Therefore the behavior is different. Hence this change reverts the change in a few places. I will look into unifying the behaviour of `xla_test` backends to make it easier to configure in the near future.,2024-08-30T10:08:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74870
yi,copybara-service[bot],Add boilerplate code to simplify_ici_dummy_variables pass,Add boilerplate code to simplify_ici_dummy_variables pass,2024-08-29T18:22:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74818
yi,copybara-service[bot],[XLA:GPU][MLIR-based emitters] Reuse chunks of mlir fusion pipeline for tests.,[XLA:GPU][MLIRbased emitters] Reuse chunks of mlir fusion pipeline for tests. That way we can test the actual pipeline without worrying that the test pipeline and the real one don't match.,2024-08-29T08:57:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74774
yi,copybara-service[bot],internal changes only,internal changes only,2024-08-29T05:37:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74746
yi,copybara-service[bot],[xla:gpu] Add a canonicalization pattern to move symbols to dims in ApplyIndexingOp.,[xla:gpu] Add a canonicalization pattern to move symbols to dims in ApplyIndexingOp.,2024-08-28T11:03:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74688
yi,copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16486 from zhenyingliu:cleanupannotator 34e3949477975d4ae5848977eb9b2e2284043404,2024-08-28T10:28:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74687
yi,copybara-service[bot],PR #16511: [NFC] Bump VLOG levels in latency_hiding_scheduler.,PR CC(Update LICENSE): [NFC] Bump VLOG levels in latency_hiding_scheduler. Imported from GitHub PR https://github.com/openxla/xla/pull/16511 This allows printing just the compact statistics by specifying level 1. Copybara import of the project:  2591cb52c626dcd7f8fa7dc99727311efacc6648 by Ilia Sergachev : [NFC] Bump VLOG levels in latency_hiding_scheduler. This allows printing just the compact statistics by specifying level 1. Merging this change closes CC(Update LICENSE) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16511 from openxla:lhs_vlog 2591cb52c626dcd7f8fa7dc99727311efacc6648,2024-08-28T09:30:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74684
yi,copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16486 from zhenyingliu:cleanupannotator 34e3949477975d4ae5848977eb9b2e2284043404,2024-08-28T07:36:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74679
yi,copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16486 from zhenyingliu:cleanupannotator 34e3949477975d4ae5848977eb9b2e2284043404,2024-08-28T07:30:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74675
yi,copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16486 from zhenyingliu:cleanupannotator 34e3949477975d4ae5848977eb9b2e2284043404,2024-08-28T07:18:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74671
yi,tilakrayal,Fixed the error message in nn_ops.cc,Modifyin the postive to positive in nn_ops.,2024-08-28T06:39:02Z,awaiting review ready to pull size:XS comp:core,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74666
yi,HemanthSai7,While op issue in TFLM,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 22.04):  TensorFlow installed from (source or binary): pip  TensorFlow version (or github SHA if from source): 2.15.0.post1 **Background of the issue** I created a conformer transducer model following the Conformer paper. The model consists of encoder, decoder and the joiner network. The Prediction network contains a single LSTM Layer. I converted the model to INT8 using the `get_concrete_function()`.  The model converted successfully and after visualising the model in netron, i verified that none of the ops were missing. I then flashed the model in **ESP32S3** and kept on getting this error infinitely in `while.cc` op. I have pasted the screenshot of the same below. !image I then created a simple model consisting of **only while loop** and then flashed it. I faced the same error.  NOTE: `AllocateTensors()` was passed **Provide the text output from tflite_convert**  **Standalone code to reproduce the issue**  This is how I converted the sample model consisting only while loop  Also, please include a link to a GraphDef or the model if possible. !image **Any other info / logs** This is what I meant by the loop getting stuck ",2024-08-28T06:34:34Z,stat:awaiting response type:bug comp:lite TFLiteConverter TF 2.15,closed,0,2,https://github.com/tensorflow/tensorflow/issues/74665,"Hi,  I apologize for the delayed response, I see you've posted this issue in this repo :https://github.com/tensorflow/tflitemicro/issues/2674 so that team will help you further on this While OP Issue in TFLM so please feel free to close this issue from your end. Thank you for your cooperation and patience.",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16486 from zhenyingliu:cleanupannotator 34e3949477975d4ae5848977eb9b2e2284043404,2024-08-28T05:26:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74662
yi,copybara-service[bot],Internal change only,Internal change only,2024-08-27T20:40:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74639
rag,copybara-service[bot],PR #15671: [ROCM] Enable basic functionality to run MLIR fussion tests,PR CC(Turn check_futures_test into a sanity check): [ROCM] Enable basic functionality to run MLIR fussion tests Imported from GitHub PR https://github.com/openxla/xla/pull/15671 Specific tests will be fixed as we go Copybara import of the project:  b955a9219d3602b0ac82c762f1fb93d6e2cd511d by Dragan Mladjenovic : [ROCM] Add basic scaffolding and enable MLIR fusion Merging this change closes CC(Turn check_futures_test into a sanity check) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15671 from ROCm:rocm_mlir b955a9219d3602b0ac82c762f1fb93d6e2cd511d,2024-08-27T19:33:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74636
rag,copybara-service[bot],Add `libclang-18-rt-dev` to Docker image,Add `libclang18rtdev` to Docker image Useful for running `bazel coverage`,2024-08-27T17:40:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74629
rag,copybara-service[bot],PR #15671: [ROCM] Enable basic functionality to run MLIR fussion tests,PR CC(Turn check_futures_test into a sanity check): [ROCM] Enable basic functionality to run MLIR fussion tests Imported from GitHub PR https://github.com/openxla/xla/pull/15671 Specific tests will be fixed as we go Copybara import of the project:  b955a9219d3602b0ac82c762f1fb93d6e2cd511d by Dragan Mladjenovic : [ROCM] Add basic scaffolding and enable MLIR fusion Merging this change closes CC(Turn check_futures_test into a sanity check) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15671 from ROCm:rocm_mlir b955a9219d3602b0ac82c762f1fb93d6e2cd511d,2024-08-27T17:27:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74626
yi,copybara-service[bot],PR #16486: Remove copy-start from stream attribute annotator,"PR CC(Change RELEASE.md to specify CUDA 9.0): Remove copystart from stream attribute annotator Imported from GitHub PR https://github.com/openxla/xla/pull/16486 This change removes the copystart instruction handling from StreamAttributeAnnotator, which was originally implemented in PR 10636. This removal is a direct consequence of PR 13597, which moved the responsibility of stream assignment for copystart instructions to ExecutionStreamAssignment. Copybara import of the project:  34e3949477975d4ae5848977eb9b2e2284043404 by Jane Liu : Remove copystart from stream attribute annotator Merging this change closes CC(Change RELEASE.md to specify CUDA 9.0) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16486 from zhenyingliu:cleanupannotator 34e3949477975d4ae5848977eb9b2e2284043404",2024-08-27T17:06:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74621
yi,copybara-service[bot],[XLA:GPU] Implement Fuse method in Priority Fusion.,"[XLA:GPU] Implement Fuse method in Priority Fusion. Instead of relying on `InstructionFusion::Fuse`. There is not much logic in the function, but now it requires going back and forth between two classes since we need to override `FuseInstruction` and `ChooseKind`. In the following changes I want to drop `InstructionFusion` from `PriorityFusion`, because it's not giving any benefits anymore. This is the last blocker to do that.",2024-08-27T16:45:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74619
yi,copybara-service[bot],Remove cc_api_version stage 4: deletion where cc_api_version = 2,Remove cc_api_version stage 4: deletion where cc_api_version = 2 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16256 from Cjkkkk:priority_incremental_update fd212fd77e41a5d2cae928ca94321f96d1367347,2024-08-27T15:34:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74618
yi,copybara-service[bot],PR #16256: [XLA:GPU] Speed up priority fusion with incremental update,PR CC([tflite] make calling NNAPI work again): [XLA:GPU] Speed up priority fusion with incremental update Imported from GitHub PR https://github.com/openxla/xla/pull/16256 * Use incremental updates for producers that already calculated priorities. This avoid looking at unchanged consumers. * Add `operands_to_new_consumers_` to record mapping from operand to new consumers and add `operands_to_removed_consumers_runtimes` to record mapping from operand to the runtimes of removed consumers. * Also deferred the cache invalidation a bit cause some cache entries are still needed in `ComputeRuntimesOfRemovedConsumers`. Copybara import of the project:  ba5ceb830a3cfcdf76b7c214d76f8fbe557f6c36 by cjkkkk : rebased and squashed  270c6f8fd27647d500e0277a787db44a687c7e22 by cjkkkk : address comments  5a5bc7505957dbd9e2136ffdbe6f1126fec97438 by cjkkkk : fix clang  1aea158147bc9fdccef6fb604b4d0bf3feddca54 by cjkkkk : use const span  fd212fd77e41a5d2cae928ca94321f96d1367347 by cjkkkk : address comments Merging this change closes CC([tflite] make calling NNAPI work again) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16256 from Cjkkkk:priority_incremental_update fd212fd77e41a5d2cae928ca94321f96d136,2024-08-27T14:08:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74603
yi,copybara-service[bot],Internal change only,Internal change only,2024-08-27T13:29:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74598
rag,rehmanaziz3,import tensorflow_federated as tff  # List all attributes and methods in tff.learning print(dir(tff.learning.algorithms.build_weighted_fed_avg))," AttributeError                            Traceback (most recent call last) [](https://localhost:8080/) in ()      54       55  Create a federated averaging process > 56 iterative_process = tff.learning.build_federated_averaging_process(      57     model_fn=model_fn,      58     client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.01), AttributeError: module 'tensorflow_federated.python.learning' has no attribute 'build_federated_averaging_process'",2024-08-27T10:04:33Z,stat:awaiting response type:support stale,closed,0,4,https://github.com/tensorflow/tensorflow/issues/74593,", Looks like this issue is more related to TensorFlow_Federated, not with the tensorflow. Could you please check with the concerned repo. Also there is an open PR raised in that repo for removing the support for **tf.keras.optimizers.Optimizer** in **tff.learning.algorithms** which might be the reason for respective error.  https://github.com/googleparfait/tensorflowfederated/pull/4866  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Clean up includes and inline deprecated functions.,Clean up includes and inline deprecated functions. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16256 from Cjkkkk:priority_incremental_update fd212fd77e41a5d2cae928ca94321f96d1367347,2024-08-27T02:17:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74563
yi,copybara-service[bot],Internal change only,Internal change only,2024-08-26T20:44:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74550
yi,copybara-service[bot],Internal change only,Internal change only,2024-08-26T17:31:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74535
yi,Humbulani1234,"Error while loading Tensorflow plugins - cuFFT, cuDNN, cuBLAS"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.17  Custom code No  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior?  does not produce the following log output errors. I tried to read the source for  to at least try find out what might be the issue. The following is what I found out: The executed piece of code for registering the plugin  is located at the file:  (you can change the tensorflow version  appropriately) and reproduced below:  I am not entirely sure as to when and where the very first object of  is created for tensorflow to display this error. I believe there has to be a point from running  and calling the above function  where the  object is created and since it must be a , hence the error. I hope someone can elaborate further on this, or provide better clarity.  Standalone code to reproduce the issue   Relevant log output ",2024-08-26T14:33:27Z,stat:awaiting tensorflower type:build/install subtype: ubuntu/linux 2.17,open,0,3,https://github.com/tensorflow/tensorflow/issues/74523,"The post from itbearshu is a scam, I received a similar comment in an issue I opened today.",Thank you for reporting the issue. This is a known issue where other issues are still open and developers are working on the same. I request you to take a look at those issues where a similar issue has been proposed. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. https://github.com/tensorflow/tensorflow/issues/71791issuecomment2237115569 https://github.com/tensorflow/tensorflow/issues/70947 https://github.com/tensorflow/tensorflow/issues/62075 Thank you!,"Closing as duplicate of CC(cuDNN, cuFFT, and cuBLAS Errors) "
yi,copybara-service[bot],PR #16451: [typo] fix a typo of latency_hiding_scheduler.h,PR CC(Parameter parsing error messages): [typo] fix a typo of latency_hiding_scheduler.h Imported from GitHub PR https://github.com/openxla/xla/pull/16451 Copybara import of the project:  9068743a7908349f89fb2ab4ab6b653dc2231a45 by flyingcat : [typo] fix a typo of latency_hiding_scheduler.h Merging this change closes CC(Parameter parsing error messages) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16451 from knightXun:patch1 9068743a7908349f89fb2ab4ab6b653dc2231a45,2024-08-26T09:16:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74511
gemma,copybara-service[bot],PR #16358: [XLA:GPU] Add cuDNN sliding window attention support,"PR CC(Request for updating keras/datasets files to r1.5): [XLA:GPU] Add cuDNN sliding window attention support Imported from GitHub PR https://github.com/openxla/xla/pull/16358 * Add support for cuDNN sliding window attention in XLA runner, not supported in pattern matcher. * required by Gemma 2 workload. Copybara import of the project:  c33778f893fcb2389aeff20b1f08a34dcfa2b6f3 by cjkkkk : add sliding window length  4c19e805fd2abed0b2404378abf2ebfe4684162d by cjkkkk : add unit test  d10644b824d6dadb138966cc206a4f2e9d3ec183 by cjkkkk : fix unit test  a3bb98a23c8809bbbbd15e14e67ec05deb0ea3e6 by cjkkkk : add comments for sliding window length  a46ed09d12bf0a0acdef7357738f0cf6467a86ef by cjkkkk : lift cudnn version for sliding window attn to 9.2 Merging this change closes CC(Request for updating keras/datasets files to r1.5) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16358 from Cjkkkk:sliding_window_attention a46ed09d12bf0a0acdef7357738f0cf6467a86ef",2024-08-26T06:26:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74503
yi,Jonii,GRU behaves very differently with GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution Google colab  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? GRU layer seems broken beyond repair with GPU execution. This is not the only problem, but the returned values are totally malformed from call. Another problem I don't have minimal code to reproduce for, is that calling fit() on model with gru, it seems to call len() on some tensor if you use GPU, but not on CPU. I do not have access to GPU of my own, so I'm relying on Google Colab to give me GPU access. It's possible Google Colab is to blame, or perhaps this bug has been fixed already since Google Colab uses Keras 3.4.1.   Standalone code to reproduce the issue   Relevant log output ",2024-08-25T07:57:20Z,type:bug comp:keras comp:gpu 2.17,closed,0,6,https://github.com/tensorflow/tensorflow/issues/74475,"I tried to run your code on Colab using TF v2.17.0 with CPU & GPU and faced the same issue. Please find the gist1, gist2 here for reference. Thank you!","Hi @**Jonii** , Please post this issue on kerasteam/keras repo. as this issue is more related to keras Thank you!","Done, I opened an issue on keras repo.","Hi **** , Please feel free to close this issue since it is already being tracked on the Keras repository. It will be easier to track there. Thank you!","Closed the issue as ""not planned"", since wrong repo for it.",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #13597: Activation offloading dependency fix: insert a wait,"PR CC( module 'tensorflow.contrib.tfprof' has no attribute 'ProfileContext' ): Activation offloading dependency fix: insert a wait Imported from GitHub PR https://github.com/openxla/xla/pull/13597 To ensure proper synchronization for the asynchronous copy, this CL makes the other stream wait for the completion of the operation (res_3) in the main stream.   %param_1 = f32[1024]{0} parameter(1)   %param_0 = f32[1024]{0} parameter(0)   %res_3 = f32[1024]{0} fusion(%param_1, %param_0), kind=kInput, calls=mul   %copystart = (f32[1024]{0:S(5)}, f32[1024]{0}, u32[]) copystart(f32[1024]{0} %res_3) Copybara import of the project:  5e41cc490b4ea7daf403ee3902103db2f5b15184 by Jane Liu : Activation offloading dependency fix: insert a wait  c6c8b3eddd1cd26ac313ff8c5405d303881682f8 by Jane Liu : Assign streams to copystart instructions and add a Waitfor thunk  9ffa12a2456d74e23963c23cb302af4556c66092 by Jane Liu : copystart always uses a new stream; add Waitfor for both D2H and H2D Merging this change closes CC( module 'tensorflow.contrib.tfprof' has no attribute 'ProfileContext' ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13597 from zhenyingliu:dependencyfix 9ffa12a2456d74e23963c23cb302af4",2024-08-23T17:06:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74419
yi,copybara-service[bot],Clean up includes in XLA's hlo_test_base.{cc|h},"Clean up includes in XLA's hlo_test_base.{cc|h} This fixes all includes in hlo_test_base.h (adds missing ones, removes superfluous ones) and fixes up all tests that were relying on transitive includes from hlo_test_base.h. (Spoiler alert: There were a lot!) Reverts ab4a5415fa8e50fd434d691877449e3e9f6a950a",2024-08-23T09:23:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74401
yi,copybara-service[bot],Improve tf.io.parse_single_example error message when features are empty,Improve tf.io.parse_single_example error message when features are empty,2024-08-22T17:41:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74335
yi,NEGU93,Compiling from source Error compiling Cython file," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version r2.10  Custom code No  OS platform and distribution Debian GNU/Linux 11 (bullseye)  Mobile device _No response_  Python version 3.9.2  Bazel version 5.1.1  GCC/compiler version gcc (Debian 10.2.16) 10.2.1 20210110  CUDA/cuDNN version NaN  GPU model and memory NaN  Current behavior? Cython version: '3.0.11' I followed the instructions in here. I had to do some changes, compile JAXlib, etc. I am now stuck after running `bazel build //tensorflow/tools/pip_package:build_pip_package repo_env=WHEEL_NAME=tensorflow_cpu` I get:  I tried changing cython version to '0.29.32' which is around the date of the of Tensorflow 2.10 but did not succeed.  Standalone code to reproduce the issue   Relevant log output _No response_",2024-08-22T14:14:09Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.10,closed,0,6,https://github.com/tensorflow/tensorflow/issues/74321,"Hi **** , Apologies for the delay. The error might due to compatibility of Cython version. For tensorflow 2.10, versions around 0.29.30 to 0.29.32 are usually compatible. However, since 0.29.32 did not resolve the issue, try downgrading Cython further  And clean the Bazel cache and rebuild  It looks like you are using an older Version of Tensorflow (2.10). Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version (2.17) and let us know if the issue still persists?  Thank you!","So I managed to continue downgrading the numpy version to 1.23.3. I also had to downgrade `mldtypes==0.2.0`.  I now have the issue that the next command is wrong. There is no `bazelbin/tensorflow/tools/pip_package/wheel_house/tensorflowversiontags.whl` I could get until `pip_package` and that is all. I just run `pip install` with said folder and apparently it worked.  I have more issues but it's for another topic. However, it would be useful that the official build website has the instructions for other versions. Like the previous command or the fact that you should replace `//tensorflow/tools/pip_package:wheel` with `//tensorflow/tools/pip_package:build_pip_package` for building version 2.10. I really could not find the build documentation for version 2.10.","Hi **** , Apologies for the delay, and thank you for your patience. The latest version of TensorFlow now supports NumPy 2.0 by default. Could you please check the following documentation and let us know if the issue still have any issues?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Improve tf.io.parse_single_example error message when features are empty,Improve tf.io.parse_single_example error message when features are empty,2024-08-22T11:28:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74313
rag,TingjiaInFuture,Proposal for Enhancing Neural Network Training: Unified Gradient Direction for Faster Convergence and Improved Accuracy," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version ALL  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In the current implementation of neural network training within TensorFlow, gradient descent typically relies on the average of gradients during the optimization process. While this approach is effective, there may be potential to enhance training efficiency and accuracy by exploring alternative methods for determining the direction of gradient updates. I have developed a method that identifies a unified direction rather than merely averaging the gradients, and preliminary experiments suggest that this approach leads to faster convergence and improved model accuracy. I believe this could be a valuable addition to TensorFlow's optimization techniques. I would appreciate the TensorFlow Development Team's feedback on this idea and am happy to provide further details or collaborate on its implementation.  Standalone cod",2024-08-22T03:29:05Z,stat:awaiting tensorflower type:feature type:performance,open,0,2,https://github.com/tensorflow/tensorflow/issues/74287,"Hi  , Could you please provide specific use case where we need to implement this particular feature which helps the issue more effectively. Thank You!",I have already shown one in https://github.com/TingjiaInFuture/UnionGradientDescent. Any question?
yi,copybara-service[bot],Add pattern to reorder gather and cast ops.,Add pattern to reorder gather and cast ops. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15460 from zhenyingliu:memfix e0443385eb7bacc852ab800aad9707c48315f480,2024-08-21T18:27:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74243
rag,copybara-service[bot],[XLA:TPU] Add LoopOptimizerBestFitHeap class that models alternate memory for memory bound loops and accounts for fragmentation.,[XLA:TPU] Add LoopOptimizerBestFitHeap class that models alternate memory for memory bound loops and accounts for fragmentation.,2024-08-21T17:49:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74241
rag,copybara-service[bot],[XLA:GPU] Fix `pred` loads and stores in the generic Triton emitter.,"[XLA:GPU] Fix `pred` loads and stores in the generic Triton emitter. The `pred` type is not packed, and each `pred` is stored in a byte. This makes the storage type `i8`, but we want to use `i1` within the Triton IR we emit. We now make sure to appropriately truncate loads and extend results before stores.",2024-08-21T15:42:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74232
yi,copybara-service[bot],PR #14897: [Nvidia GPU] Add mechanism to detect nccl timeout and return error status,"PR CC(A bug in tensorflow r1.4 when applying  MultiRNNCell): [Nvidia GPU] Add mechanism to detect nccl timeout and return error status Imported from GitHub PR https://github.com/openxla/xla/pull/14897 The current behavior crashes the program whenever a nccl async error has occured, timeout errors are also not detected for async events. This pr adds a mechanism to do: 1. poll statuses of async events and return timeout if status is pending for too long 2. return nccl async event status as xla status so a proper python exception can be thrown. Copybara import of the project:  4a6e3b1cee3af3bc0c83e31ee1ce5ecfffd524ac by TJ Xu : Add mechanism to detect nccl timeout and return error status  c7bdda819ade875f03d6fd8ed4a6d07c00aba5e7 by TJ Xu : move async status and queue management to gpu executable  d4b44f1afde05a78a4e233b05503015218c29ec7 by TJ : Added e2e test for testing nccl timeout and error propagation  c64997afee2d83a857b61cb0d91d8ca2a641097c by TJ : address pr comments  d3318e7f8703519b060f7efb1c77d3f409fc5ed8 by TJ : changed back the formatting for xla/python/pjit. : Added back the IsIdle api in gpu stream interface Merging this change closes CC(A bug in tensorflow r1.4 when applying  MultiRNNCell) F",2024-08-21T11:16:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74220
llm,SkBlaz,Einsum optimization setting guidelines," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.16  Custom code No  OS platform and distribution Linux Ubuntu 20  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Existing tensorflow supports einops, which enable seamless tensor reduction ops. Parameter to this functionality (kwarg) is the `optimize` parameter, i.e.  Is it documented somewhere what the use cases that benefit from these options are in practice?  Further, are effects of these expected to this library perhaps? https://optimizedeinsum.readthedocs.io/en/stable/ Simple benchmarking doesn't really reveal any (significant?) differences. Thanks!  Standalone code to reproduce the issue   Relevant log output ",2024-08-21T10:53:25Z,stat:awaiting tensorflower comp:ops type:performance TF 2.16,open,0,2,https://github.com/tensorflow/tensorflow/issues/74216,", I tried to execute the mentioned code and observed that the Greedy and optimal time taken is almost similar. Kindly find the gist of it here and let me know if the understanding the right in this case.   Thank you!"," thanks! Yes, you replicated the case. I'm asking, basically, when do these params actually yield something different in practice? I.e. how does one know when to use what (if at all possible to know his upfront)?"
yi,copybara-service[bot],PR #16279: [NFC]: remove dce redundant log,"PR CC(While using tf.while_loop , the _Slicehelper chooses strided_slice op(req 4 args) instead of slice op(req 3 args)): [NFC]: remove dce redundant log Imported from GitHub PR https://github.com/openxla/xla/pull/16279 Copybara import of the project:  9befa8231c42219b8a1d1aaeb342346d48513b0d by flyingcat : [NFC]: remove dce redundant log Merging this change closes CC(While using tf.while_loop , the _Slicehelper chooses strided_slice op(req 4 args) instead of slice op(req 3 args)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16279 from knightXun:DECNFC 9befa8231c42219b8a1d1aaeb342346d48513b0d",2024-08-21T08:59:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74209
yi,nkinnaird,EffienceNet .keras model with base model trainable has poor performance in TensorFlow 2.17," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution Linux Debian   Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am training an EfficientNet model with a custom head using TensorFlow and Keras, saving the model to a `.keras` format. If the base model `trainable` flag is set to False, such that I only train the head, then when I later load the `.keras` model and evaluate it on a dataset, I get the expected good performance. When I set the trainable flag to True and train a model (which converges well), then when I later load the model and evaluate it on the same dataset the performance has degraded significantly. (I am evaluating the model on the same dataset using the code both at the end of training, and later on in a separate notebook. It is in this separate notebook where the performance is bad, where again the same dataset is being used and the same code is being used in both evaluation places.) Saving to a `.h5` model does not have t",2024-08-21T00:45:32Z,stat:awaiting response type:bug comp:keras 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/74170,", Tensorflow v2.17 contains the keras3.0 by default and the tensorflow v2.15 contains keras2.0. Also the efficientnet model is related to keras application, could you please try to raise the issue in kerasteam/keras repo for the quick resolution. Thank you!",I have posted in the Keras issues section.,", Could you please feel free to move this issue to closed status, as this has been tracking in the keras repo. Thank you!",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #15460: Add the host memory deallocation in GpuExecutor::Deallocate,PR CC(Branch 179464468): Add the host memory deallocation in GpuExecutor::Deallocate Imported from GitHub PR https://github.com/openxla/xla/pull/15460 This CL adds the missing host memory deallocation according to the pointer's host memory space allocated by GpuExecutor::Allocate() . Copybara import of the project:  ceb25a951e758b92ef317b680945693dc935f5cd by Jane Liu : Add the host memory deallocation in GpuDriver  dc7d76d1244f12494ea9385ec63f39f274fe1b20 by Jane Liu : Add the unit test  e0443385eb7bacc852ab800aad9707c48315f480 by Jane Liu : function Deallocate does not return a value Merging this change closes CC(Branch 179464468) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15460 from zhenyingliu:memfix e0443385eb7bacc852ab800aad9707c48315f480,2024-08-20T17:33:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74141
yi,edbosne,tf.sparse.reduce_sum error in JIT," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version v2.17.0rc12gad6d8cc177d  Custom code Yes  OS platform and distribution Ubuntu Mate 22.04  Mobile device _No response_  Python version Python 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 12.3  GPU model and memory _No response_  Current behavior? Error when using tf.sparse.reduce_sum and JIT compilation. I have written a layer that passes all my unit tests except when I use in a model with predict or train.  Would that make sense with the JIT compilation on? I am not fully certain how and when this works. Anyway, I am not exactly sure why my layer code fails, but I think that this minimum reproducible example captures the issue.  Standalone code to reproduce the issue   Relevant log output ",2024-08-20T14:39:42Z,stat:awaiting tensorflower type:bug comp:ops comp:xla 2.17,open,0,2,https://github.com/tensorflow/tensorflow/issues/74131,"I tried to run your code on Colab using TF v2.15.0, nightly and faced the same issue. Please find the gist here for reference. Thank you!",This also happens for other sparse ops: `tf.sparse.reorder` and `tf.sparse.softmax`. Reproducable with code very closely related to above.
yi,copybara-service[bot],[xla:cpu] Add test for verifying xla_cpu_use_thunk_runtime is disabled in xla_jit_compiled_cpu_function,[xla:cpu] Add test for verifying xla_cpu_use_thunk_runtime is disabled in xla_jit_compiled_cpu_function,2024-08-20T11:56:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74123
yi,copybara-service[bot],Internal change only,Internal change only,2024-08-20T11:44:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74121
rag,copybara-service[bot],Add IWYU pragma export for `model_builder_base.h` to `model_builder.h`.,Add IWYU pragma export for `model_builder_base.h` to `model_builder.h`. This includewhatyouuse pragma allow clients of TF Lite to not have to depend directly on headers from `tensorflow/compiler/mlir/...` In particular this is needed to avoid lint warnings for clients using the `tflite::GetAllocationFromFile` function and including `model_builder.h` but not `tensorflow/compiler/mlir/core/model_builder_base.h`.,2024-08-20T10:34:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74117
yi,copybara-service[bot],PR #16241: Align the scheduling name with the instruction name for constants,"PR CC(Fix a buildscript error which prevents macro being used by other workspaces): Align the scheduling name with the instruction name for constants Imported from GitHub PR https://github.com/openxla/xla/pull/16241 This commit corrects the scheduling name after it is altered by SanitizeConstantNames, preventing assertion failures in the HLO verifier. Copybara import of the project:  8c784ea28d8a1a0fc7e3a4d1df78e1f4cfe8aac9 by Jane Liu : Align the scheduling name with the instruction name for constants Merging this change closes CC(Fix a buildscript error which prevents macro being used by other workspaces) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/16241 from zhenyingliu:constant_name 8c784ea28d8a1a0fc7e3a4d1df78e1f4cfe8aac9",2024-08-20T08:42:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/74109
rag,copybara-service[bot],Fixes L2Pool implementation to not average pooling region squares,Fixes L2Pool implementation to not average pooling region squares See discussion here: https://github.com/webmachinelearning/webnn/issues/278,2024-08-19T19:07:58Z,,open,1,0,https://github.com/tensorflow/tensorflow/issues/74079
yi,NazaRik555,Unexpected failure when preparing tensor allocations,"Hi! I've trained torch EfficientNetB0 model, then converted it to tflite and faced this failure: `Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/conv.cc:332 input>dims>data[3] != filter>dims>data[3] (4 != 3)` I've loaded model as _NewOtherTensorflowLite Model_ and then tried to use:  but get this error. Here is part of my code for conversion:  Am I doing something wrong?",2024-08-19T12:06:41Z,stat:awaiting response stale comp:lite TFLiteConverter,closed,0,3,https://github.com/tensorflow/tensorflow/issues/74049,"Hi,   I apologize for the delayed response and If possible could you please help us with Google colab notebook with your model along with complete steps to reproduce the same behavior from our end to investigate this issue further ?  Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,ronitmitra,ImportError: Traceback (most recent call last):, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.10  Custom code No  OS platform and distribution windows 11  Mobile device windows 11  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In jupyter notebook applying import tensorflow as tf gives the following error  Standalone code to reproduce the issue   Relevant log output ,2024-08-19T11:55:06Z,stat:awaiting response type:build/install stale subtype:windows TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/74048,", Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:  https://github.com/tensorflow/tensorflow/issues/61887 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],[XLA:GPU] Add tests to ensure that all HLO opcodes are either tested or known to be untested in Triton support tests.,[XLA:GPU] Add tests to ensure that all HLO opcodes are either tested or known to be untested in Triton support tests. This ensures that we know exactly what's covered and that coverage will be tracked correctly in case HLO Ops are added.,2024-08-19T10:38:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74046
yi,copybara-service[bot],Internal change only,Internal change only,2024-08-19T08:33:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/74042
few shot,chihuahua,HLO for TopK oddly casts uint8 input to uint32 before passing to radix sort. ," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version TF 2 (HEAD of internal repo)  Custom code Yes  OS platform and distribution GoogleInternal Environment  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory V100 (also reproducible on other GPUs)  Current behavior? An Alphabet model invokes `tf.math.top_k` with a tensor of dtype uint8 and shape (1, 1,32768).  !strange_hlo_text_with_uint32_radix_sort For this call, XLA ends up calling radix sort. However, the radix sort is suboptimal because TensorFlow casts the inputs to uint32 (instead of using original dtype uint8). Of course, radix sort is faster across smaller dtypes (with fewer bytes). > @@(u32[1,32768]{1,0}, s32[1,32768]{1,0}, u8[271871]{0}) customcall(u32[1,32768]{1,0}, s32[1,32768]{1,0}), custom_call_target=""__cub$DeviceRadixSort I would expect HLO text more like this, where the uint8 inputs are passed directly: > (u8[1,32768]{1,0}, s32[1,32768]{1,0}, u8[310527]{0}) customcall(u8[1,32768]{1,0}, s32[1,32768]{1,0}), custom_call_target=""__cub$DeviceRadixSort"" We actua",2024-08-18T21:52:59Z,stat:awaiting tensorflower type:bug comp:ops 2.17,open,0,2,https://github.com/tensorflow/tensorflow/issues/74035,"I tried to execute the mentioned code on tensorflow v2.17, tfnightly on the google colab which contains the T4 Gpu. Kindly find the gist of it here. Thank you!","> I tried to execute the mentioned code on tensorflow v2.17, tfnightly on the google colab which contains the T4 Gpu. Kindly find the gist of it here. Thank you! This issue files a performance issue. Per that Colab, the code runs correctly, but the HLO text produced to be passed to XLA is suboptimal because the uint8 input is upcasted to uint32 before radix sort happens. Thank you!"
yi,copybara-service[bot],Internal change only.,Internal change only.,2024-08-16T17:17:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73947
yi,ADarkDividedGem,TFLite Model used in official documentation doesn't compile on Edge TPU Compiler," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11  TensorFlow installation (pip package or built from source): pip  TensorFlow library (version, if pip package or github SHA, if built from source):     pip show tensorflow  = 2.17.0    tf.__version__ = 2.18.0dev20240815  2. Code Using the code from Posttraining integer quantization  official tutorial to create and convert a TensorFlow model to TFlite:  Which displays the following output:   3. Failure after conversion When I then run the newly created TFLite file through the `edgetpu_compiler` (via Docker) it fails saying it still has dynamicsized tensors:  Any idea how I can fully convert the model to staticsized tensors. I tried the suggestion of using `converter._experimental_new_quantizer` but that didn't help.",2024-08-16T17:17:22Z,stat:awaiting tensorflower comp:lite TFLiteConverter 2.17,open,0,8,https://github.com/tensorflow/tensorflow/issues/73946,It compiles if I remove the `Conv2D` and `MaxPooling2D` layers:   This is despite both operations being supported:  But I am not seeing any of those limitations being described in the documentation for the MaxPooling2D or Conv2D layer. I also read that even if there are no dynamic size tensors in the graph > graphs with control flow ops are regarded as dynamic graphs If this concept is indeed true does anyone know how I can detect which layers have control flows and possible alternatives for the  `Conv2D` and `MaxPooling2D` layers?,It looks like this exact code worked back in 2020 by turning off `experimental_new_quantizer` but doing that gives me an error:  ,"Hi , let's try to solve one issue at a time as solving one will probably affect everything else. To convert a static tensor model I think we can use AIEdgeTorch to do this. Can you replicate your model in PyTorch, then convert to tflite and see how that goes and use that with the edgeTPU? (Providing a static example input that the converter will trace  should staticize the graph)","> Can you replicate your model in PyTorch, then convert to tflite and see how that goes and use that with the edgeTPU? Sorry, I only know how to run models, not create them. I simply used the official documentation from https://www.tensorflow.org/lite (Posttraining integer quantization) that the `edgetpu_compiler` successfully compiled back in 2020 to create a simple working example of the `edgetpu_compiler` now failing to compile a TFLite model.  The best I could do was make use of a TensorFlow to PyTorch Converter which suggested the following code:  But I have no idea if that is even similar to the original model and it fails because `ai_edge_torch.convert()` expects a tuple of torch tensors:  I will need to ask for help converting the model used in the TensorFlow documentation to PyTorch.","Hi , no worries  let me give you a little more context with what we're trying to do here. I want to see if the model runs w/o training first (In most cases the exact model weights don't matter  it's more the structure  though this isn't always 100% true) so I'm going to try that first:  Can you try running this first to see if it works? (The output values will be gibberish, and also any accuracy) but if this works then I believe the trained model should work fine as well. Thanks.","Yes, the now model compiles but the operations won't run on the Edge TPU due to the use of unsupported data types. I tried all three quantization recipes but the compiler still reported unsupported data types.  First the PyTorch model is converted to `mnist_conv.tflite`:  Then the `mnist_conv.tflite` model successfully compiles but none of the operations will run on the Edge TPU:  The log file reports that the operations are using ""an unsupported data type"":  I can ask AI Edge Torch directly why this model is not working fully on the Edge TPU. Part of the model requirements for Edge TPU is that ""Tensor sizes are constant at compiletime (no dynamic sizes)"" EDIT: Looks like you are already aware that static quantization is not currently supported: https://github.com/googleaiedge/aiedgetorch/issues/150issuecomment2299777420","I see, i see, can you do me a favor and write a comment on that thread that it is blocking this issue? That way we can properly track things. blocked by https://github.com/googleaiedge/aiedgetorch/issues/150","Hi,   Thanks for raising this issue. Are you aware of AIEdgeTorch? As we believe this issue is better supported by and more relevant to AIEdgeTorch we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/aiedgetorch/issues/384 Let us know if you have any questions. Thanks."
yi,copybara-service[bot],PR #14897: [Nvidia GPU] Add mechanism to detect nccl timeout and return error status,"PR CC(A bug in tensorflow r1.4 when applying  MultiRNNCell): [Nvidia GPU] Add mechanism to detect nccl timeout and return error status Imported from GitHub PR https://github.com/openxla/xla/pull/14897 The current behavior crashes the program whenever a nccl async error has occured, timeout errors are also not detected for async events. This pr adds a mechanism to do: 1. poll statuses of async events and return timeout if status is pending for too long 2. return nccl async event status as xla status so a proper python exception can be thrown. Copybara import of the project:  4a6e3b1cee3af3bc0c83e31ee1ce5ecfffd524ac by TJ Xu : Add mechanism to detect nccl timeout and return error status  c7bdda819ade875f03d6fd8ed4a6d07c00aba5e7 by TJ Xu : move async status and queue management to gpu executable  d4b44f1afde05a78a4e233b05503015218c29ec7 by TJ : Added e2e test for testing nccl timeout and error propagation  c64997afee2d83a857b61cb0d91d8ca2a641097c by TJ : address pr comments  d3318e7f8703519b060f7efb1c77d3f409fc5ed8 by TJ : changed back the formatting for xla/python/pjit. : Added back the IsIdle api in gpu stream interface  77d86076ffa3d5688f03ba29bfe49db441b06cc5 by TJ Xu : Fix ci failure  dd3f1520e7f02252",2024-08-16T15:28:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/73935
yi,Carl0smvs,Unexpected behaviour training custom Siamese Network," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.15.1  Custom code Yes  OS platform and distribution Linux Mint 21.2 Cinnamon  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to train a custom built Siamese Network, following the Keras documentation closely, only modifying the architecture and other things as needed. What I'm trying to do differently is load the data in a different manner. Due to the dimensions of the dataset I will be working with, I can't store it all at once in arrays in memory as is done in the example. I tried loading it iteratively to the model to be trained in two ways:  Using a tf.data.Dataset that will load the data as needed  Building a custom training loop that would only fetch the data batch by batch To test these, I put together an example script (supplied below) to test these approaches and how they perform in comparison to supplying the whole dataset to the model from an array in memory  Surprisingly, both of my methods produce a weird behaviour",2024-08-15T16:27:32Z,stat:awaiting response type:bug stale comp:keras TF 2.15,closed,0,6,https://github.com/tensorflow/tensorflow/issues/73873,"Hi **** , Sorry for the dealy, Please post this issue on kerasteam/keras repo. as this issue is more related to keras Thank you!","I will, thank you!","Hi **** , Could you please close this issue if it has already been raised in the Keras repository? Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,AD-lite24,Tensorflow lite build causing multiple undefined reference errors after running make, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.2  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.12.4  Bazel version 6.5.0  GCC/compiler version 11.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Upon running make in the build directory after running cmake I get multiple undefined reference errors which I cannot understand why they occur. I am using all standard code from the minimal example except my CMakeLists.txt which is  cmake runs perfectly fine and the tensorflow.so file was also created within the bazelbin directory without any issue. I have spent days trying to solve it so it is sad to see it doesn't work even after coming so close. My main.cpp is also the exact same as the minimal example  Standalone code to reproduce the issue   Relevant log output ,2024-08-15T14:24:05Z,type:support comp:lite TF 2.16,closed,0,7,https://github.com/tensorflow/tensorflow/issues/73867,I am facing exact same issue OS : Ubuntu 22.04 built the libtensorflowlite.so from source with tf (v2.12.0) bazel (5.3.0) (tested configuration) and then I followed the following project structure  CMakeLists.txt  And I am facing exact same errors as lite24 ...,"Hi 2005 my issue was resolved, use the official CMakeLists.txt provided  Add all the other parts as required",Are you satisfied with the resolution of your issue? Yes No,"Hi lite24, Thanks for the heads up. :rocket:  I also followed, Build with CMake which make a STATIC library within the /build folder and uses it, and now its working :smile: . Turns out its hard to build the library from source once and use it in every project >_source_  Note: This generates a static library libtensorflowlite.a in the current directory but the library isn't selfcontained since all the transitive dependencies are not included. To use the library properly, you need to create a CMake project. Please refer the ""Create a CMake project which uses TensorFlow Lite"" section. I think for every new project we have to make a CMake project with subdirectories of source library. And build/make that freshly.",You can just run sudo make install and install it your system libraries ,Yup i read about that while searching for a solution here to build and install directly in the system...Are you aware of the pros and cons of this approach ? :thinking: ,"I am not very familiar with the dependency isolation systems in c++ tbh so I am not sure. But it shouldn't be a problem as such, you can always adjust your cmake if you wanted to install a different version of tensorflow for a certain project"
yi,AD-lite24,Tensorflow lite build causing multiple undefined reference errors after running make, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.2  Custom code No  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.12.4  Bazel version 6.5.0  GCC/compiler version 11.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Upon running make in the build directory after running cmake I get multiple undefined reference errors which I cannot understand why they occur. I am using all standard code from the minimal example except my CMakeLists.txt which is   cmake runs perfectly fine and the tensorflow.so file was also created within the bazelbin directory without any issue. I have spent days trying to solve it so it is sad to see it doesn't work even after coming so close.  My main.cpp is also the exact same as the minimal example  Standalone code to reproduce the issue   Relevant log output ,2024-08-15T14:19:00Z,type:build/install subtype: ubuntu/linux 2.17,closed,0,2,https://github.com/tensorflow/tensorflow/issues/73866,Please don't assign someone whose only contribution to the source is a typo fix,Are you satisfied with the resolution of your issue? Yes No
rag,leoull,App Storage Size Increases with CoreML or Metal Usage on iOS," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 0.0.1nightly  Custom code No  OS platform and distribution iPhone 17.5.1  Mobile device iPhone 13 mini  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm encountering an issue where the app's storage size increases each time an AI model is loaded, and the storage doesn't decrease afterward. Specifically, I'm using the PoseNet TensorFlow Lite model on iPhone to demonstrate the problem. for more detail checkout  this StackOverflow Post  Standalone code to reproduce the issue   Relevant log output _No response_",2024-08-14T17:10:29Z,stat:awaiting tensorflower type:bug comp:lite iOS,closed,0,4,https://github.com/tensorflow/tensorflow/issues/73800,Need a real device to test this.  can you please take a look? Thanks.,"I got reply on Apple's Developer Forum: CoreML  doUnloadModel:options:qos:error looks like the issue is with tflite's coreML delegate implementation. I don't know much about this stuff, but the issue is probably here: coreml_executor.mm","Hi,   Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/40 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
yi,hanssssssssssssss,[Building from source with bazel] Failure when linking libm-libraries during compilation of flatbuffers, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17  Custom code No  OS platform and distribution Linux Debian 5.10.2161 x86_64  Mobile device _No response_  Python version 3.12  Bazel version 7.3.0  GCC/compiler version clang 17.0.2  CUDA/cuDNN version CUDA 12 /cuDNN 8  GPU model and memory NVIDIA A2  Current behavior? I'm trying to build tensorflow from source with bazel for use in a C++ project. I'm working in a conda environment wit the specifics mentioned above and try to compile the libraries with the following command: `bazel build //tensorflow:libtensorflow_cc.so //tensorflow:libtensorflow_framework.so` Bazel fails with the follwing error message:   I tried to add the following lines to `.bazelrc` to explicitly link the locations of the libraries that were not found. That did not change the error.  Any help on this issue is much appreciated.  Standalone code to reproduce the issue   Relevant log output _No response_,2024-08-14T16:26:46Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/73798,"Hi **** ,  Apologies for the delay. This might be due to a version incompatibility. Please verify version compatibility using the attached documentation. Let me know if the issue persists.  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],Fix XLA build for windows when built in JAX context,"Fix XLA build for windows when built in JAX context Contains two fixes:   Defines (globally for all dependents of thunk_executor) _ENABLE_EXTENDED_ALIGNED_STORAGE, which corresponds to bug fix in msvc 2017 as described in https://devblogs.microsoft.com/cppblog/stlfeaturesandfixesinvs2017158/. It is preferred over _DISABLE_EXTENDED_ALIGNED_STORAGE because I don't think we need to keep any ABI backward compatibility here since it is very new code (like 2 weeks old)  Removes _MSC_VER conditional preprocessing block around `kEvalErrorDetailUrl` because it leads to symbol duplication on Windows. I assume the block was a bug (but very suspicious one, as that preprocessin condition was added on purpose), as it contained ""extern with initializer"" for `kEvalErrorDetailUrl` in header file, which made `kEvalErrorDetailUrl` not only declared in hlo_evaluator.h but also defined there. The same constant is defined in hlo_evaluator.. This basically made kEvalErrorDetailUrl defined in hlo_evaluator..h header, which naturally lead to symbol duplicaition while linking xla_extension for JAX.",2024-08-14T04:44:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73761
rag,fujunwei,The average should not be computed in L2Pool2d,"The ONNX L2Pool2d, DirectML L2 Pooling Desc and CoreML's l2_pool2d calculate the l2 pooling by the expression `Y = (X1^2 + X2^2 + ... + Xn^2) ^ (1/2)`,  but TFLite L2_PooL2d kernel implementation has the average with the count of sum elements  `Y=((X1^2 + X2^2 + ... + Xn^2)/n) ^ (1/2)`,  is it an issue of the kernel implementation? BTW, the kernel of l2_norm also has no the average.",2024-08-14T00:51:39Z,type:bug comp:lite,open,1,3,https://github.com/tensorflow/tensorflow/issues/73742,"Yes, this is an issue with the TFLite L2_Pool2d kernel implementation. The correct implementation of L2 pooling should be Y = (X1^2 + X2^2 + ... + Xn^2) ^ (1/2), without averaging by the count of sum elements. The averaging step is not part of the standard L2 pooling operation, and it changes the result. The correct implementation should only sum the squared values and then take the square root, without dividing by the count of elements. It's good that you also noticed the same issue with the L2 norm kernel implementation. Both L2 pooling and L2 norm should follow the same formula without averaging. This discrepancy might cause issues when converting models between different frameworks, or when trying to reproduce results from other frameworks. It's recommended to report this issue to the TFLite developers and ask them to fix the kernel implementation to match the standard L2 pooling and L2 norm formulas.",Current PR: https://github.com/tensorflow/tensorflow/pull/74079,"Thanks, what's the status of this PR, does it have any blocking issue?"
yi,copybara-service[bot],Refactor `DockerImage.pull_and_run` to be simpler,Refactor `DockerImage.pull_and_run` to be simpler This is in preparation for specifying builds that run without a container,2024-08-13T21:56:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73729
yi,copybara-service[bot],Internal change only,Internal change only,2024-08-13T16:42:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73710
yi,copybara-service[bot],Fix vectorization with sequences of apply_indexing.,"Fix vectorization with sequences of apply_indexing. For now, just check that the operands are defined outside the loop. In theory, we could do better, but as explained in the test, it probably doesn't matter in practice.",2024-08-13T12:52:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73700
yi,solidDoWant,Unable to train/take gradient of integer variable under any condition," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version v2.16.119g810f233968c  Custom code Yes  OS platform and distribution Debian 11  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.6  GPU model and memory _No response_  Current behavior? Models with integer variables cannot have gradients computed (or be trained because of this). They fail with `ValueError: No gradients provided for any variable.` Please note that I'm referring to building/training models, _not_ posttraining model quantization. For context, I'm working on a somewhat novel approach to solving a type of engineering problem that includes both discrete and continuous values. In some cases, variables must be one of a set of numeric values (e.g. `1`, `2`, `3000`, etc). This isn't something that can really be split out into separate models and trained independently, as both these variables are used throughout a complex, multipleinput multipleoutput layer based model. An approximation cannot be used either, as even a highly accurate approximation can feasibly result in an incorrect result under certain ",2024-08-12T21:23:47Z,stat:awaiting tensorflower type:bug comp:ops TF 2.16,open,0,1,https://github.com/tensorflow/tensorflow/issues/73631,I was able to reproduce the issue on tensorflow v2.17 and tfnightly. Kindly find the gist of it here.
yi,copybara-service[bot],Internal change only.,Internal change only.,2024-08-12T19:18:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/73625
rag,copybara-service[bot],"Add include-what-you-use 'export' pragma,","Add includewhatyouuse 'export' pragma, to allow clients to continue to include error_reporter.h and verifier.h from their original ""tensorflow/lite"" locations without getting lint warnings, despite that code having now been moved to ""tensorflow/compiler/mlir"". See https://github.com/includewhatyouuse/includewhatyouuse/blob/master/docs/IWYUPragmas.md",2024-08-12T18:41:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/73620
gemma,copybara-service[bot],Add option to prevent `GemmRewriters` from causing excessive inter-instruction dependencies.,"Add option to prevent `GemmRewriters` from causing excessive interinstruction dependencies. Disabling bias prevents using the `beta * C` term in the GEMM, which can remove dependencies between multiple matrix multiplications. This, in turn, can improve the performance of overall computation by allowing multiple GEMMs to be scheduled in parallel. As an example, consider the following computation: `(A * A) + (B * B)`. With bias enabled, the `GemmRewriter` will emit the following GEMMs:  Because the second GEMM depends on the first, they cannot be scheduled in parallel. Instead, with bias disabled, the `GemmRewriter` will emit the following:  In this case, the two GEMMs can be scheduled in parallel.",2024-08-12T13:45:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73608
rag,copybara-service[bot],"Add include-what-you-use 'export' pragma,","Add includewhatyouuse 'export' pragma, to allow clients to continue to include error_reporter.h and verifier.h from their original ""tensorflow/lite"" locations without getting lint warnings, despite that code having now been moved to ""tensorflow/compiler/mlir"". See https://github.com/includewhatyouuse/includewhatyouuse/blob/master/docs/IWYUPragmas.md",2024-08-12T12:05:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73599
rag,Salvo9879,Pip install of Tensorflow may be causing issues with MacOS," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution MacOS Sonoma 14.1  Mobile device _No response_  Python version 3.12.4  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I recently installed tensorflow (2 days ago) using Pip. Ever since downloading Tensorflow I have had an issue with my MacBook. Specifically a process called “fileproviderd” has been using over 110% of my CPU usage. This is causing my laptop to heat up and is draining the battery. I have since deleted tensorflow from my virtual environment and have noticed a slight improvement (averaging out at about 40%90%  which is still not normal). I have just reinstalled Tensorflow and the process’ activity skyrocketed using about 140% of the CPU usage.  I am also contacting Apple support with the issue and will report any solutions that we discover however I do believe that this is an issue with Tensorflow as I’ve never had this issue before installing it.  Also not that I never ran any code the second time I installed tensorflow, ",2024-08-11T21:52:01Z,type:support 2.17,closed,0,7,https://github.com/tensorflow/tensorflow/issues/73561,Can you post the output of `pip list` once TF is installed?,Package                 Version   abslpy                 2.1.0 astunparse              1.6.3 certifi                 2024.7.4 charsetnormalizer      3.3.2 flatbuffers             24.3.25 gast                    0.6.0 googlepasta            0.2.0 grpcio                  1.65.4 h5py                    3.11.0 idna                    3.7 keras                   3.5.0 libclang                18.1.1 Markdown                3.6 markdownitpy          3.0.0 MarkupSafe              2.1.5 mdurl                   0.1.2 mldtypes               0.4.0 namex                   0.0.8 numpy                   1.26.4 opteinsum              3.3.0 optree                  0.12.1 packaging               24.1 pip                     24.2 protobuf                4.25.4 Pygments                2.18.0 requests                2.32.3 rich                    13.7.1 setuptools              72.1.0 six                     1.16.0 tensorboard             2.17.0 tensorboarddataserver 0.7.2 tensorflow              2.17.0 termcolor               2.4.0 typing_extensions       4.12.2 urllib3                 2.2.2 Werkzeug                3.0.3 wheel                   0.44.0 wrapt                   1.16.0,Cannot reproduce with the exact same versions. Is it possible you are syncing the virtual environment to the cloud? You should not. There are many files in these wheels and there's nothing to be gained by syncing them compared to just installing them again when needed.,"Yes, My project is stored using iCloud. However I have deleted the environment completely yet the issue persists. I will investigate further. Thank you for the response. ","It takes a while for the sync to finish. And when you delete your files, the deletion needs to be synced to the cloud too.",Closing this since it is not a TF issue.,Are you satisfied with the resolution of your issue? Yes No
rag,sakshigithubit,AttributeError: 'Sequential' object has no attribute '_get_save_spec'," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. L",2024-08-11T10:55:46Z,stat:awaiting response stale comp:lite TFLiteConverter,closed,0,3,https://github.com/tensorflow/tensorflow/issues/73532,"Hi  , Can you give more details , like which tensorflow version and your os details?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
chat,arielmedinaa,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL)., Issue type Others  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tensorflow 2.17.0  Custom code Yes  OS platform and distribution Windows10  Mobile device a  Python version 3.12  Bazel version a  GCC/compiler version a  CUDA/cuDNN version a  GPU model and memory a  Current behavior? a  Standalone code to reproduce the issue   Relevant log output ,2024-08-11T00:01:14Z,stat:awaiting response type:others 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/73525,"Hi **** , There are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also in order to expedite the troubleshooting process, could you please provide the following information OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: TensorFlow installed from (source or binary): Installed using virtualenv? pip? conda?: Bazel version (if compiling from source): GCC/Compiler version (if compiling from source): CUDA/cuDNN version: GPU model and memory: and the exact sequence of commands / steps that you executed before running into the problem. https://github.com/tensorflow/tensorflow/issues/61887 Thank you!","> Hola****Hay al menos 3 escenarios posibles: Necesita instalar el redistribuible MSVC 2019 Su CPU no admite instrucciones AVX2 Su CPU/Python es de 32 bits Hay una biblioteca que está en una ubicación diferente/no está instalada en su sistema y que no se puede cargar. >  > Además, para acelerar el proceso de resolución de problemas, ¿podría proporcionar la siguiente información? Plataforma y distribución del sistema operativo (por ejemplo, Linux Ubuntu 16.04): Dispositivo móvil (por ejemplo, iPhone 8, Pixel 2, Samsung Galaxy) si el problema ocurre en el dispositivo móvil: TensorFlow instalado desde (fuente o binario): ¿Instalado usando virtualenv? ¿pip? ¿conda?: Versión de Bazel (si se compila desde la fuente): Versión de GCC/compilador (si se compila desde la fuente): Versión de CUDA/cuDNN: Modelo de GPU y memoria: y la secuencia exacta de comandos/pasos que ejecutó antes de encontrarse con el problema. CC(Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.) >  > ¡Gracias! I think it's more because of my cpu, the error I was having was not repeated on other machines that had a Windows operating system (and one of them Linux). But it is already solved, it uses an environment and to install packages I used pip (now that I am on Linux I must use pip3) thats right?","Hi **** ,  pip should work fine in this environment. Please refer to the following link for instructions. I am glad to hear the issue is resolved. Feel free to close this issue. Thank you!",Are you satisfied with the resolution of your issue? Yes No
yi,hanssssssssssssss,"Converting model for on-device training fails with ""LLVM ERROR: Failed to infer result type(s)."""," 1. System information  OS Platform and Distribution: Debian GNU/Linux 10  TensorFlow installation: pip package (Python 3.12.4)  TensorFlow library version: 2.17.0  2. Code I'm trying to follow the instructions that are given here: [](https://www.tensorflow.org/lite/examples/on_device_training/overview) The model can be saved with tf.saved_model.save(), including the custom function signatures (Although the code from the example has to be adapted slightly to work with TensorFlow 2.17.0).  But the conversion step fails with the following output:  Below is a minimal example that recreates the error. ",2024-08-10T11:30:27Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/73517,"It looks like github removed the link to the instructions that I'm referring to. Anyway, it can be found on tensorflow dot org /lite/examples/on_device_training/overview","Hi  , I was wondering if you may be able to resolve your issue by using AIEdgeTorch, you can find more information here: googleblog. I have actually created a simple script for converting your model to tflite   Using this script i was able to get the model.tflite file without any errors. If you want to, you can actually try visualizing the result in modelexplorer as well. Please try them out and let us know if this resolves your issue. If you still need further help, feel free to open a new issue at the respective repo.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
transformer,privatepeople,Issue with softmax warning appearing in Tensorflow 2.17.0," TensorFlow version 2.17.0  OS platform and distribution Google Colab  Current behavior? Hi, everyone. I am practicing implementing a Transformer model that machine translates English into Korean by reading TensorFlow guides and books. However, I am having trouble because an unknown UserWarning appears during the final translation process. In that issue, I've never used softmax before, but warning me about using it. This problem appears after the model has finished training and when making inferences. I searched to see if there were any cases similar to mine, but it seems that no solution was found in any of them.  same issue: https://github.com/tensorflow/tensorflow/issues/67758 This problem did not exist in tensorflow 2.15 but appeared in 2.17.0. I can't even guess what could be causing it. For those of you who are curious about the full code, I am leaving a Colab link. You can easily reproduce it by running it with Ctrl + F9 in Google Colab. The execution time of the entire code is approximately 5 minutes ~ 5 minutes and 30 seconds on a T4 GPU. That issue is at the bottom. Colab Link: https://colab.research.google.com/drive/1IMFWoJ1s5ReKU9LYENROpAsZ47D6cG8T?usp=sharing The data I used is 'koreng.zip'",2024-08-10T07:12:43Z,type:bug 2.17,open,0,3,https://github.com/tensorflow/tensorflow/issues/73516,"I modified some parts of the code to make it easy to reproduce by just running it. And I changed it to a Colab link rather than a Github link. The execution time of the entire code is approximately 5 minutes ~ 5 minutes and 30 seconds on a T4 GPU. That issue is at the bottom. And although not all of them were edited, some comments were edited to English rather than Korean. I will edit the remaining parts so that you do not have any inconvenience while reading them.",I tried to run your code on Colab using TF v2.17.0 and faced the same issue. Please find the gist here for reference. Thank you!,I've the same problem. Is there a solution yet? Can we ignore this warning?
yi,pisarev,Issue with Loading libdelegate.so on Ubuntu 22.04 LTS Using TensorFlow Lite GPU Delegate, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.4  Custom code Yes  OS platform and distribution WSL Linux Ubuntu22.04  Mobile device _No response_  Python version 3.9  Bazel version Bazelisk (automatically manages Bazel version)  GCC/compiler version 10.5.0  CUDA/cuDNN version 11.2.r11.2  GPU model and memory NVidia 4060Ti  Current behavior? (base) master1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu22.04/x64$ LD_DEBUG=libs ./test ... skipped lines with no errors or warnings ...      27014:      27014:     calling init: /usr/lib/x86_64linuxgnu/gconv/UTF16.so      27014:      27014:     /lib/x86_64linuxgnu/libglib2.0.so.0: error: symbol lookup error: undefined symbol: g_object_ref_sink (fatal)      27014:     find library=libgdkx112.0.so [0]; searching      27014:      search path=/usr/local/cuda11.2/lib64:glibchwcaps/x8664v3:glibchwcaps/x8664v2:tls/x86_64/x86_64:tls/x86_64:tls/x86_64:tls:x86_64/x86_64:x86_64:x86_64:          (LD_LIBRARY_PATH)      27014:       trying file=/usr/local/cuda11.2/lib64/libgdkx112.0.so      27014:       trying file=glibchwcaps/x8664v3/libgdkx112.0.so      27014:       trying file=glibchwcaps/x86,2024-08-09T20:17:19Z,stat:awaiting response type:bug stale comp:lite TF 2.4,closed,0,6,https://github.com/tensorflow/tensorflow/issues/73484,"Hi **** , Sorry for the delay. There seem to be version compatibility issues here. Could you please check this documentation? Please update the documentation accordingly and let us know if the issue still persists. Thank you!","Thank you for your response! I have checked the versions of all relevant components and it seems everything is selected correctly:  **Ubuntu Version**: 22.04.4 LTS (Jammy)  **Bazel Version**: 3.7.2 (installed via Bazelisk)  **CUDA Version**: 11.2 (V11.2.152)  **cuDNN Version**: 8.1.1  **Python Version**: 3.9.19 (Conda environment)  **GCC Version**: Default system GCC is used. Configuration details:  CUDA compute capability: 8.9  No ROCm, TensorRT, or Clang as CUDA compiler. The ""libdelegate.so"" library, when loaded by my application, leads to a symbol lookup error, while the ""libtensorflowlite_c.so"" library loads without any issues. Here are the specific commands I ran in the terminal along with their outputs: (tf) master1VM6C4QJVS5:/mnt/c/Users/admin$ lsb_release a No LSB modules are available. Distributor ID: Ubuntu Description:    Ubuntu 22.04.4 LTS Release:        22.04 Codename:       jammy (tf) master1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu24.04/tensorflow$ bazel version bazel 3.7.2 (tf) master1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu24.04/tensorflow$ nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052021 NVIDIA Corporation Built on Sun_Feb_14_21:12:58_PST_2021 Cuda compilation tools, release 11.2, V11.2.152 Build cuda_11.2.r11.2/compiler.29618528_0 (tf) master1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu24.04/tensorflow$ cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR A 2 define CUDNN_MAJOR 8 define CUDNN_MINOR 1 define CUDNN_PATCHLEVEL 1  define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL) endif /* CUDNN_VERSION_H */ (tf) master1VM6C4QJVS5:/mnt/c/Users/admin$ conda activate tf (tf) master1VM6C4QJVS5:/mnt/c/Users/admin$ which python /home/master/miniconda3/envs/tf/bin/python (tf) master1VM6C4QJVS5:/mnt/c/Users/admin$ python version Python 3.9.19 Configuration steps: (tf) master1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu24.04/tensorflow$ ./configure You have bazel 3.7.2 installed. Please specify the location of python. [Default is /bin/python3]: /home/master/miniconda3/envs/tf/bin/python Found possible Python library paths:   /home/master/miniconda3/envs/tf/lib/python3.9/sitepackages Please input the desired Python library path to use.  Default is [/home/master/miniconda3/envs/tf/lib/python3.9/sitepackages] Do you wish to build TensorFlow with ROCm support? [y/N]: n No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: y CUDA support will be enabled for TensorFlow. Do you wish to build TensorFlow with TensorRT support? [y/N]: n No TensorRT support will be enabled for TensorFlow. Found CUDA 11.2 in:     /usr/local/cuda11.2/targets/x86_64linux/lib     /usr/local/cuda11.2/targets/x86_64linux/include Found cuDNN 8 in:     /usr/local/cuda11.2/targets/x86_64linux/lib     /usr/local/cuda11.2/targets/x86_64linux/include Please specify a list of commaseparated CUDA compute capabilities you want to build with. You can find the compute capability of your device at: https://developer.nvidia.com/cudagpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code. Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 8.9]: 8.9 Do you want to use clang as CUDA compiler? [y/N]: n nvcc will be used as CUDA compiler. Please specify which gcc should be used by nvcc as the host compiler. [Default is /bin/gcc]: Please specify optimization flags to use during compilation when bazel option ""config=opt"" is specified [Default is Wnosigncompare]: Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n Not configuring the WORKSPACE for Android builds. Despite these configurations, the ""libdelegate.so"" library still encounters the following errors when loaded by the application: (tf) master1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu24.04/x64$ LD_DEBUG=libs ./eye ...       6411:     /lib/x86_64linuxgnu/libglib2.0.so.0: error: symbol lookup error: undefined symbol: g_object_ref_sink (fatal) ...       6411:     ./eye: error: symbol lookup error: undefined symbol: gtk_widget_device_is_shadowed (fatal) ...       6411:     ./bin/tflite/libdelegate.so: error: symbol lookup error: undefined symbol: _ZN4absl14lts_2020_09_236Status16kMovedFromStringE (fatal) When checking the shared library dependencies of libdelegate.so, the following output was observed: (tf) master1VM6C4QJVS5:/mnt/c/Users/admin/Documents/Lazarus/Projects/ubuntu24.04/tensorflow$ ldd ../x64/bin/tflite/libdelegate.so         linuxvdso.so.1 (0x00007ffd835b6000)         libstdc++.so.6 => /lib/x86_64linuxgnu/libstdc++.so.6 (0x00007ff59dcf6000)         libgcc_s.so.1 => /lib/x86_64linuxgnu/libgcc_s.so.1 (0x00007ff59dcd6000)         libc.so.6 => /lib/x86_64linuxgnu/libc.so.6 (0x00007ff59daad000)         libm.so.6 => /lib/x86_64linuxgnu/libm.so.6 (0x00007ff59d9c6000)         /lib64/ldlinuxx8664.so.2 (0x00007ff59df38000)","Hi , what command did you use to build `libdelegate.so`? Also what command(s)/program(s) did you use to check it (`./eye` ??) can you please explain what that is? Please also format code/terminal with 3 backtick code formatting: ex: \`\`\` \ code/terminal output \`\`\` outputs:  Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,arthursw,Memory leak in training," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.16.1 and 2.17  Custom code Yes  OS platform and distribution Debian 11  Mobile device _No response_  Python version Python 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuda/12.0.0_gcc10.4.0 and cudnn/8.9.7.2912_gcc10.4.0  GPU model and memory different GPU, among which Tesla V100SXM232GB  Current behavior? I have a memory leak (GPU memory and RAM constantly increase) during my training.  **This does not happen with Tensorflow 2.11.1**.  Here is the project. Unfortunately, this is not my code and I do not have time to create a minimal standalone code.  Standalone code to reproduce the issue   Relevant log output  With Tensorflow 2.16.1 and 2.17:  ```",2024-08-09T10:51:49Z,stat:awaiting tensorflower type:bug 2.17,open,0,4,https://github.com/tensorflow/tensorflow/issues/73457,"Hi **** , Apologies for the delay. Could you please provide a Colab gist to help troubleshoot the above issue? The provided code is quite extensive, making it challenging to pinpoint the source of the memory leaks. Thank you!","Hi, thanks for your answer. Here is a minimal reproducible example:  With tensorflow 2.11.1, no memory leak:  With tensorflow==2.16.1, memory leak?:  This with `cuda/12.0.0_gcc10.4.0` and `cudnn/8.9.7.2912_gcc10.4.0` ; on python 3.10.14 (also tested with python 3.9 and tensorflow 2.11.1), on Debian GNU/Linux 11 (bullseye).","Hi All,  Just chiming in that I see the same issue on a different project. When training Tensorflow 2.16.2 or 2.17 and python 3.12.3 on Ubuntu 24.04, Tensorflow suffers a very similar gradual increase in CPU Memory usage until the training is killed.  Our project also trains the model using `model.train_on_batch`.  FWIW.",I tried running your code on Colab using TensorFlow 2.18.0 version and faced the same issue. Please find the gist1 attached here for reference. Thank you!
yi,chungyu1108,adding type annotations in TensorFlow," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.8  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I have started adding type annotations to the TensorFlow codebase, focusing initially on core utility functions that are widely used. The goal is to improve code clarity, enhance IDE support, and enable static analysis tools to catch bugs earlier. Given the size and complexity of the TensorFlow codebase, this will be an incremental process. The annotations are being added in `.pyi` files to maintain compatibility with older Python versions (2.7, 3.3, 3.4). I’ve also ensured that all new annotations are tested with `mypy` to verify their correctness. Please review the initial set of changes and provide feedback on this approach. I encourage other contributors to start adding annotations as they work on different parts of the codebase.  Standalone code to reproduce the issue   Relevant log output ",2024-08-09T07:06:28Z,stat:awaiting response stale type:performance TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/73444,", In the given code snippet you have defined the class and its methods but are not calling them anywhere. Also the tensorflow v2.8 is pretty old, could you please try to upgrade to latest tensorflow v2.17 and provide the update if you are facing the same issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,AhdHazim,Convergence of Actor critic algorthim ," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version V1  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hello,  I am using the following actor critic agent, but it does not converge at all. Please could you help me with this matter. In the following  the code and the results ` import numpy as np import tensorflow as tf import tensorflow.compat.v1 as tf import json tf.disable_v2_behavior() tf.reset_default_graph() class A2CLSTM(object):     def __init__(             self,             sess,             n_actions,             n_features,             lr_a,             lr_c,             entropy_beta,             batch_size=32   Default batch size     ):         self.sess = sess         self.n_actions = n_actions         self.n_features = n_features         self.lr_a = lr_a         self.lr_c = lr_c         self.entroy_beta = entropy_beta         self.batch_size = batch_size   Set the batch size         self.lstm_cell_size = 64      ",2024-08-08T18:17:16Z,stat:awaiting response type:bug stale 1.4.0,closed,0,6,https://github.com/tensorflow/tensorflow/issues/73404,"Hi **** , Sorry for the dealy, I tried to run your code on Colab using the TFnightly version and faced different issue. I suspect this might be because you are using TensorFlow 1.x. Could you please migrate your code to TensorFlow 2.x? I have attached documentation and a replicated gist for reference.  Thank you!","Thank you for your kind reply! You are right,  I am using TensorFlow 1.x . Indeed I have tried before to moved  2.x  but I have faced many issues.  You  can run it with TensorFlow 1.x if you add  the following lines to your run file: tf.compat.v1.disable_eager_execution()  Use ConfigProto with tf.compat.v1 config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True sess = tf.compat.v1.Session(config=config) ________________________________ From: Venkat6871 ***@***.***> Sent: 12 August 2024 09:23 To: tensorflow/tensorflow ***@***.***> Cc: Sabr, Ohood ***@***.***>; Mention ***@***.***> Subject: Re: [tensorflow/tensorflow] Convergence of Actor critic algorthim (Issue CC(Convergence of Actor critic algorthim )) [ATTENTION : Ce courriel provient de l'extérieur de l'ÉTS] Évitez de cliquer sur un lien ou d'ouvrir une pièce jointe si vous ne connaissez pas l'expéditeur du courriel. En cas de doute, veuillez SVP créer un billet Problème de courriel au GUS . Hi  , I tried to run your code on Colab using the TFnightly version and faced different issue. I suspect this might be because you are using TensorFlow 1.x. Could you please migrate your code to TensorFlow 2.x? I have attached documentation and a replicated gist for reference. Thank you! — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you were mentioned.Message ID: ***@***.***>","Hi **** , Apologies for the delay, and thank you for your patience. TensorFlow 1.x is already deprecated, so you need to migrate to TensorFlow 2.x. For your reference, I have attached the  documentation on migrating. Thank you! ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,1778813838,"Regarding lstm's full integer quantisation, does it quantise its intermediate output? The lstm full integer quantisation I get has only one input and one output quantisation parameter (minus the arguments)"," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ![Uploading 企业微信截图_20240808180440.png…]()  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including traceb",2024-08-08T10:05:27Z,stat:awaiting response type:support stale comp:lite TFLiteConverter 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/73369,"Hi , can you please provide a reproducible code example? So that we may verify that what you are seeing is expected or not expected. Though in general, we recommend that people use Transformers instead of LSTMs.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,chrism0dwk,Gradient computation for `vectorized_map` nested inside `while_loop`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version v2.17.0  Custom code Yes  OS platform and distribution Linux Mint 21.3  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to compute gradients for a `tf.vectorized_map`ped function nested within a call to `tf.while_loop` (and hence also `tf.map_fn`) as in this (trivial!) Colab example.  The top level function can compute its return value in all three execution modes (eager, graph, XLA).   I expected to also be able to compute the gradients of the function with respect to its inputs.  This works under eager and graph mode, but not XLA mode where an InvalidArgument exception occurs: > InvalidArgumentError: Input 1 to node `gradient_tape/while/gradients/while/PartitionedCall_grad/PartitionedCall/gradients/pfor/Tile_grad/Reshape_1` with op Reshape must be a compiletime constant. In the example, all shapes are wellspecified (by hard coding in this trivial example), so it feels like some shape information is getting lost somewhere.  Is t",2024-08-08T09:36:08Z,stat:awaiting tensorflower type:bug comp:xla 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/73367,"I was able to reproduce the issue on tensorflower v2.17, tfnightly. Kindly find the gist of it here."
yi,akoushandeh11,"ImportError when trying to run ""from tensorflow import keras""", Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17  Custom code Yes  OS platform and distribution windows 10  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? import keras from tensorflow successfully  Standalone code to reproduce the issue   Relevant log output ,2024-08-08T09:22:12Z,stat:awaiting response type:support stale 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/73366,"Hi **** , I apologize for the delayed response. Could you please install Keras separately using . Then, import it as  instead of  . If you still facing any issue please let us know. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Reverts c6173f108b512affb82abe13227f666d44614a9f,Reverts c6173f108b512affb82abe13227f666d44614a9f FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15749 from zhenyingliu:scheduling_name c8dd378740fa2ed934bbd415337f16c051bedf77,2024-08-08T09:18:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73364
yi,copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15749 from zhenyingliu:scheduling_name c8dd378740fa2ed934bbd415337f16c051bedf77,2024-08-08T09:17:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/73363
yi,copybara-service[bot],[XLA:GPU] NFC: Move more passes to the transforms subdirectory.,[XLA:GPU] NFC: Move more passes to the transforms subdirectory. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15749 from zhenyingliu:scheduling_name c8dd378740fa2ed934bbd415337f16c051bedf77,2024-08-08T08:46:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73360
rag,copybara-service[bot],[tsl:concurrency] Use C++17 operators new and delete to allocate storage for AsyncValue,[tsl:concurrency] Use C++17 operators new and delete to allocate storage for AsyncValue name                     old cpu/op   new cpu/op   delta BM_MakeConstructed    15.8ns ± 1%  11.9ns ± 1%  24.84%  (p=0.000 n=40+40) BM_MakeConstructed    15.8ns ± 1%  11.8ns ± 1%  25.10%  (p=0.000 n=40+40) BM_MakeConstructed    15.8ns ± 1%  11.8ns ± 1%  24.97%  (p=0.000 n=40+37) BM_MakeConstructed   15.7ns ± 1%  11.7ns ± 1%  25.10%  (p=0.000 n=39+35) BM_MakeConstructed   15.9ns ± 1%  12.2ns ± 1%  23.34%  (p=0.000 n=39+39) BM_MakeConstructed   16.6ns ± 1%  12.5ns ± 1%  24.44%  (p=0.000 n=39+39) BM_MakeConstructed  17.1ns ± 1%  13.9ns ± 2%  18.35%  (p=0.000 n=38+39) BM_MakeConstructed  19.8ns ± 2%  16.8ns ± 4%  15.44%  (p=0.000 n=39+39),2024-08-08T01:04:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73340
rag,TheWalkingSea,Tensorflow incompatible shapes.," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version  2.17.0  Custom code Yes  OS platform and distribution Windows  Mobile device _No response_  Python version 3.11.4  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am using a dictionary to pass data into model.fit with the names of the input layers corresponding in the dictionary with data. However, for some reason, tensorflow is mixing up the data in category and amzData but if you look at the output from `input({layer: tf.convert_to_tensor(X_train[layer].to_list()).numpy().shape for layer in layer_names})` it shows that the data is correctly labelled. So why is tensorflow mixing up these two variables despite it being correctly correlated in the dictionary?  Standalone code to reproduce the issue   Relevant log output  data/out/model.png: !model",2024-08-07T16:14:31Z,stat:awaiting response type:support stale comp:keras 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/73301,", I tried to execute the mentioned  code and faced a different error. Kindly find the gist of it here and also looks like this issue is more related to keras, so could you please raise a new issue in kerasteam/keras repo for quick resolution. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No, did this ever get fixed?
yi,RaoufiTech,TensorBuffer does not support data type: INT32, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Windows 11 x64  Mobile device Andorid Studio virtual device: Medium Phone  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? My app is crashing with the error: TensorBuffer does not support data type: INT32. I expected it to work normally and not to crash.  Standalone code to reproduce the issue   Relevant log output ,2024-08-07T14:10:01Z,stat:awaiting response type:bug stale comp:lite Android TF 2.16,closed,0,6,https://github.com/tensorflow/tensorflow/issues/73295,"Hi  , By looking at the error    it looks like DataType.INT32 is not supported by TensorBuffer , can you use DataType.FLOAT32  instead and let me know if it worked for you.",Hey FLOAT32 works but the ai model that I am working with needs INT32...,"Hi  , You will need to use Bytebuffer to accomplish your task . However the output can still handled as a FLOAT32 TensorBuffer . Please take a look at this",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #15749: Update the scheduling name for the created fusion and root instructions,"PR CC([XLA] Define TF_COMPILE_LIBRARY for two libraries): Update the scheduling name for the created fusion and root instructions Imported from GitHub PR https://github.com/openxla/xla/pull/15749 When working on maxtext, there is an error of scheduling name mismatching the instruction name for the rematpolicy of minimal_offloaded. This CL fixes the scheduling names for the instructions created by the StreamAttributeAnnotator pass. Copybara import of the project:  1b045078da10618e1ac0603b9b43bec288eaf5e3 by Jane Liu : Update the scheduling name for the created fusion and root instructions  b8bd7f1ef23bab3005859b1c233db6496927ee3b by Jane Liu : simpler code  c8dd378740fa2ed934bbd415337f16c051bedf77 by Jane Liu : change std::string to absl::string_view when passing arguments Merging this change closes CC([XLA] Define TF_COMPILE_LIBRARY for two libraries) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15749 from zhenyingliu:scheduling_name c8dd378740fa2ed934bbd415337f16c051bedf77",2024-08-07T13:30:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73293
yi,copybara-service[bot],[XLA:GPU] Add passes.td to fusions/triton.,[XLA:GPU] Add passes.td to fusions/triton. It is just a refactoring to make adding passes easier by relying on TableGen. Reverts e6982712b1637e13cf84598de653a0e7bec8274b,2024-08-07T11:51:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73286
yi,copybara-service[bot],PR #15749: Update the scheduling name for the created fusion and root instructions,"PR CC([XLA] Define TF_COMPILE_LIBRARY for two libraries): Update the scheduling name for the created fusion and root instructions Imported from GitHub PR https://github.com/openxla/xla/pull/15749 When working on maxtext, there is an error of scheduling name mismatching the instruction name for the rematpolicy of minimal_offloaded. This CL fixes the scheduling names for the instructions created by the StreamAttributeAnnotator pass. Copybara import of the project:  1b045078da10618e1ac0603b9b43bec288eaf5e3 by Jane Liu : Update the scheduling name for the created fusion and root instructions  b8bd7f1ef23bab3005859b1c233db6496927ee3b by Jane Liu : simpler code  c8dd378740fa2ed934bbd415337f16c051bedf77 by Jane Liu : change std::string to absl::string_view when passing arguments Merging this change closes CC([XLA] Define TF_COMPILE_LIBRARY for two libraries) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15749 from zhenyingliu:scheduling_name c8dd378740fa2ed934bbd415337f16c051bedf77",2024-08-07T05:59:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/73273
yi,eric,[RNN] TFLite does not appear to be using the UnidirectionalSequenceLSTM," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS  TensorFlow installation (pip package or built from source): from pip  TensorFlow library (version, if pip package or github SHA, if built from source): kerasnightly3.4.1.dev2024080603 tbnightly2.18.0a20240806 tfnightly2.18.0.dev20240806  2. Code 1) https://colab.research.google.com/gist/eric/f00f071e527f9fa7b2ed39f8d482fbb4/tensorflowdatasets.ipynb 2) https://colab.research.google.com/gist/eric/a292799568831371b7686a2b8cefcd0b/tensorflowlitedebuggercolab.ipynb model.zip  3. Failure after conversion I've found that any Keras LSTM causes this issue. I expected TFLite to use the `UnidirectionalSequenceLSTM` op, but instead it seems to be doing something else that then requires the use of flex ops, which I would like to avoid trying to get to work with my tflite deployment situation.",2024-08-06T23:15:11Z,stat:awaiting tensorflower type:bug comp:lite TFLiteConverter 2.17,closed,0,13,https://github.com/tensorflow/tensorflow/issues/73254,Is there anything I can do to tweak my model to allow it to work without the flex ops?,"Hi , using only TFLite BuiltIn Ops, will do this but that can be quite constraining. For most LSTM use cases we highly recommend upgrading to Transformers and using PyTorch and AIEdgeTorch and the Generative API to convert to .tflite models. Does that work for you?","So far I've found that using an LSTM has been a good tradeoff for performance and accuracy, so it would be nice if they could work again. Is there a current working example that shows how to use it with tensorflow?","Hi , with raw tensorflow w/o Keras, it does not seem like there is an easy way to create an LSTM. It was sunsetted awhile ago ... I did find a raw_ops for an LSTM Cell: https://www.tensorflow.org/api_docs/python/tf/raw_ops/BlockLSTMV2. I don't believe you will want to reconstruct an LSTM with more primitive ops w/o Keras but that is one possibility.",Is there a way to do it with Keras? I am using Keras so that would  be fine.,"Hi , my current attempts seems to be running into a prior bug ... would you be willing to use AIEdgeTorch instead? I was able to convert an lstm that way: ","I've been using kerastcn for other parts of my model and found that it performed better than LSTM, so I would have to evaluate the pytorch landscape to see if it would work for me.","Understood, let us know if for some reason that workflow doesn't work. You are able to reopen the issue in the future.",Is there any hope of fixing the bug in Keras that is preventing it from working? It would be nice to not have to rewrite everything. ,"Hi , we can turn this issue into that issue for now, if that is preferred. Here's a gist showing the issue with tfnightly (I have attempted on previous versions of tf up to and including 2.15) gist Hi wei, can you please take a look? Thanks.",That would be great. Thanks!,"Hi,   Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/41 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Improve error message readability.,Improve error message readability.,2024-08-06T21:34:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73241
yi,copybara-service[bot],[XLA:GPU] Add passes.td to fusions/triton.,[XLA:GPU] Add passes.td to fusions/triton. It is just a refactoring to make adding passes easier by relying on TableGen.,2024-08-06T12:53:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73215
yi,copybara-service[bot],PR #15491: [XLA:CPU][oneDNN] Enable oneDNN Graph build,PR CC(Branch 179578952): [XLA:CPU][oneDNN] Enable oneDNN Graph build Imported from GitHub PR https://github.com/openxla/xla/pull/15491 This PR enables oneDNN Graph functionality in oneDNN library which is a prep work for supporting fusions such as MHA via oneDNN graph. Copybara import of the project:  06aed953869b5a91f9d2618d5d36eee372a9a18a by Yimei Sun : [XLA:CPU][oneDNN] Enable oneDNN Graph build This PR enables oneDNN Graph functionality in oneDNN library which is a prep work for supporting fusions such as MHA via oneDNN graph. Merging this change closes CC(Branch 179578952) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15491 from Inteltensorflow:yimei/enable_build_onednn_graph 06aed953869b5a91f9d2618d5d36eee372a9a18a,2024-08-06T10:30:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73207
yi,copybara-service[bot],Internal change only,Internal change only,2024-08-05T21:51:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73171
rag,ReemaHVT,Kubeflow Pipeline with Kserve running into errors with Tensorflow,"We ran a kubeflow pipeline and created an endpoint(inferenceservice), it is in ready state, but when I curl it to get predictions i get attached issue, i already saved model in proper format and loaded model from storage uri to make predictions which worked, even tensorflow runtime version for training and kserve is same. We have used same tf version(2.16.1) for training and serving. Here is the link to the Curl command, logs and the yaml file. Tensorflow error with Kubeflow Pipelines",2024-08-05T16:45:01Z,stat:awaiting response stale TF 2.16,closed,0,7,https://github.com/tensorflow/tensorflow/issues/73158,"Hi  could you please provide some insights into the issue, we are facing it for a long time now","also getting same issue from cifar10_model and iris_model, logs:  20240808 08:16:11.407139: I external/org_tensorflow/tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint `roundoff` errors from different computation orders. To turn them off, set the environment variable TF_ENABLE_ONEDNN_OPTS=0. 20240808 08:16:11.500499: I tensorflow_serving/model_servers/server.cc:77] Building single TensorFlow model file config: model_name: cifar10_model model_base_path: /models/cifar10_model 20240808 08:16:11.503247: I tensorflow_serving/model_servers/server_core.cc:474] Adding/updating models. 20240808 08:16:11.503278: I tensorflow_serving/model_servers/server_core.cc:603] (Re)adding model: cifar10_model 20240808 08:16:11.709794: I tensorflow_serving/core/basic_manager.cc:740] Successfully reserved resources to load servable {name: cifar10_model version: 1} 20240808 08:16:11.709855: I tensorflow_serving/core/loader_harness.cc:68] Approving load for servable version {name: cifar10_model version: 1} 20240808 08:16:11.709884: I tensorflow_serving/core/loader_harness.cc:76] Loading servable version {name: cifar10_model version: 1} 20240808 08:16:11.716917: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /models/cifar10_model/1 20240808 08:16:11.729105: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve } 20240808 08:16:11.729159: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /models/cifar10_model/1 20240808 08:16:11.730837: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20240808 08:16:11.775236: I external/org_tensorflow/tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled 20240808 08:16:11.782870: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle. 20240808 08:16:11.948435: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /models/cifar10_model/1 20240808 08:16:11.961415: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 244832 microseconds. 20240808 08:16:11.963256: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:82] No warmup data file found at /models/cifar10_model/1/assets.extra/tf_serving_warmup_requests 20240808 08:16:12.070870: I tensorflow_serving/core/loader_harness.cc:97] Successfully loaded servable version {name: cifar10_model version: 1} 20240808 08:16:12.073985: I tensorflow_serving/model_servers/server_core.cc:495] Finished adding/updating models 20240808 08:16:12.074074: I tensorflow_serving/model_servers/server.cc:121] Using InsecureServerCredentials 20240808 08:16:12.074608: I tensorflow_serving/model_servers/server.cc:388] Profiler service is enabled 20240808 08:16:12.079460: I tensorflow_serving/model_servers/server.cc:423] Running gRPC ModelServer at 0.0.0.0:8500 ... [warn] getaddrinfo: address family for nodename not supported 20240808 08:16:12.083437: I tensorflow_serving/model_servers/server.cc:444] Exporting HTTP/REST API at:localhost:8501 ... [evhttp_server.cc : 250] NET_LOG: Entering the event loop ... 20240808 08:17:38.815509: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: Could not find variable sequential/conv2d/kernel. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/sequential/conv2d/kernel/N10tensorflow3VarE does not exist. [[{{function_node __inference_serving_default_45404}}{{node sequential_1/conv2d_1/convolution/ReadVariableOp}}]] 20240808 08:17:38.815626: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: ABORTED: Stopping remaining executors. 20240808 08:17:45.111537: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: ABORTED: Stopping remaining executors.","Hi **** , I apologize for the delayed response and thank you for bringing this issue to our attention, if possible could you please try by downgrading the TensorFlow version to `2.15.0 `and latest TensorFlow version `2.17.0` and see is it resolving your issue ?  Meanwhile, please refer this issue https://github.com/tensorflow/tensorflow/issues/57803 which is similar in some extent which may help you to solve your issue. If issue still persists please help us with minimal codesnippet/Github repo along with complete steps to reproduce the same behavior from our end to help you further on this issue. Thank you for cooperation and patience.","Hi  , already tried both version 2.15.0  and 2.17.0 but issue still persists, sharing minimal codesnippe along with complete steps to reproduce the same behavior in some time. ","Hey , while preparing minimal code snippet for you, the versioin 2.15.0 worked, previously when I tested it with 2.15.0 in my kubeflow pipeline, it failed cause I used replace all to replace version, it also replaced  tfserving version, tfserving don't have a 2.15.0 version, now it works thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
rag,copybara-service[bot],PR #15671: [ROCM] Enable basic functionality to run MLIR fussion tests,PR CC(Turn check_futures_test into a sanity check): [ROCM] Enable basic functionality to run MLIR fussion tests Imported from GitHub PR https://github.com/openxla/xla/pull/15671 Specific tests will be fixed as we go Copybara import of the project:  c8ce4f7107a7f308bdcb8e252aa6f406aead6fe7 by Dragan Mladjenovic : [ROCM] Fix build after 21311f23028acfd4bc2c3e4a3f76bad8c9e640e8  5ceeec1c3ccbc926fc85330cccfbfbc4a70eb795 by Dragan Mladjenovic : [ROCM] Add basic scaffolding and enable MLIR fusion Merging this change closes CC(Turn check_futures_test into a sanity check) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15671 from ROCm:rocm_mlir 5ceeec1c3ccbc926fc85330cccfbfbc4a70eb795,2024-08-05T06:02:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/73118
yi,copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15749 from zhenyingliu:scheduling_name c8dd378740fa2ed934bbd415337f16c051bedf77,2024-08-04T03:47:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/73104
yi,copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15749 from zhenyingliu:scheduling_name c8dd378740fa2ed934bbd415337f16c051bedf77,2024-08-04T03:19:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/73092
yi,npanpaliya,Proposal: Conditionally substitute boringSSL with system OpenSSL,"Hello All, As we know boringSSL doesn't support IBM Power architecture anymore, we are finding it difficult to build Tensorflow on Power with proper and updated boringSSL. Even if we just patch boringSSL to have Power support (by using older boringSSL or applying patch from previous commits that had Power support), it is difficult to maintain if boringSSL version gets updated with no equivalent changes for Power.  Our Proposal: Use system installed OpenSSL instead of boringSSL based on a flag **only if** set. For e.g. USE_SYSTEM_OPENSSL TF brings in boringSSL dependency directly and indirectly (through curl and grpc) during bazel build process. So, we have tried some changes which does following  1. Use boringSSL and curl from the system (using TF_SYSTEM_LIBS) 2. Patched grpc to fallback to system openSSL include and library paths during its build.  Note: grpc already provides similar mechanism when grpcio's wheel is being built through setup.py. But this mechanism doesn't work when we build grpc through bazel. These changes have enabled us to build TF on Power, and our testing also didn't give any issues so far (some of the bazel tests and tensorflow/models).  We want your opinion on this approach. Als",2024-08-02T15:16:19Z,stat:awaiting tensorflower type:feature type:build/install,open,2,2,https://github.com/tensorflow/tensorflow/issues/73029," , toplay .","Hello   Could you please let us know your opinion on this? Also, kindly help me tag the main Community members and get their views too on this?"
yi,Voivio,[RNN] unable to quantize RNN with customized cell," 1. System information  OS Platform and Distribution: Ubuntu 22.04.4 LTS  TensorFlow installation: pip package  TensorFlow library: 2.18.0dev20240801  2. Code Provide code to help us reproduce your issues using one of the following options: A minimal example is available in this Colab notebook.  3. Failure after conversion None  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. I wanted to implement a RNN using customized cells. When I try to quantize the model, the conversion can not be finished. The error message looks like: ` INFO:tensorflow:Assets written to: /tmp/tmp6jab2_an/assets INFO:tensorflow:Assets written to: /tmp/tmp6jab2_an/assets /home/d/miniconda3/envs/tflite_model/lib/python3.9/sitepackages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.   warnings.warn( 20240801 21:32:47.659189: W tensorflow/compiler/mlir/lite/python/tf",2024-08-02T02:14:21Z,type:bug comp:lite TFLiteConverter 2.17,closed,0,3,https://github.com/tensorflow/tensorflow/issues/73002,"Hi , the easiest way to accomplish this right now is to create a custom RNN in PyTorch and convert using AIEdgeTorch, does this work for you?","Hi , thanks for your quick reply. I have quickly tested this out in the Colab Notebook using this library. This aiedgetorch library can convert the RNN with customized cells into a TFLite model without errors, yet it is not actually converting operations happened inside (though those outside are fine). Operations are still float32based rather than int8. So now I give up on using the RNN structure provided from keras and decide to manually handling all hidden states. I have no more questions regarding this issue and please feel free to close it if there will be no more updates on it!  Many thanks again.",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only.,Internal change only.,2024-08-01T23:28:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72996
yi,copybara-service[bot],internal change only,internal change only,2024-08-01T22:00:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72992
agent,copybara-service[bot],Avoid logging when the coordination agent is successfully shutting down and improve the barrier failure message.,Avoid logging when the coordination agent is successfully shutting down and improve the barrier failure message.,2024-08-01T21:38:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72990
yi,mayurmunshi,TimeDistributed step error, Issue type Others  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15  Custom code Yes  OS platform and distribution Linux Ubuntu 16  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Model fit is not able to take TimeDistributed step  Standalone code to reproduce the issue   Relevant log output _No response_,2024-08-01T13:04:31Z,stat:awaiting response stale comp:dist-strat type:others TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/72954,", I tried with the sample code on tfnightly which it was executed without any issues/error. Kindly find the gist of it here. Also this has been resolved with the respective PR. https://github.com/kerasteam/keras/pull/19799 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Clean up uses of bounds in ApplyIndexingOp since they are no longer necessary,Clean up uses of bounds in ApplyIndexingOp since they are no longer necessary,2024-08-01T08:59:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72945
yi,dependabot[bot],Bump the github-actions group with 8 updates,"Bumps the githubactions group with 8 updates:  Updates `google/osvscanneraction` from 1.8.1 to 1.8.2  Release notes Sourced from google/osvscanneraction's releases.  v1.8.2 What's Changed  Now uses OSVScanner v1.8.2, see https://github.com/google/osvscanner/blob/main/CHANGELOG.md for full changelog. Call analysis is now done by default for golang.     Commits  7ac94f9 Merge pull request  CC(Connectionist Temporal Classification example) from google/updatetov1.8.2 da0991f Update unified workflow example to point to v1.8.2 reusable workflows f1f90c4 Update reusable workflows to point to v1.8.2 actions f8af522 Update actions to use v1.8.2 osvscanner image 4847de9 Merge pull request  CC(Slack Channel) from renovatebot/renovate/workflows b7d4e6e Update actions/uploadartifact action to v4.3.4 dfa8609 Merge pull request  CC(simplify contributing process) from renovatebot/renovate/workflows 7e2f071 Update workflows See full diff in compare view    Updates `actions/setuppython` from 5.1.0 to 5.1.1  Release notes Sourced from actions/setuppython's releases.  v5.1.1 What's Changed Bug fixes:  fix(ci): update all failing workflows by @​mayeut in actions/setuppython CC(SessionOptions should be declared as defined.  ",2024-08-01T08:20:09Z,awaiting review ready to pull size:S dependencies github_actions,closed,0,1,https://github.com/tensorflow/tensorflow/issues/72943,"Hi , Can you please review this PR? Thank you !"
yi,copybara-service[bot],Put the need to clear constraints to the caller of ApplyIndexingOp,Put the need to clear constraints to the caller of ApplyIndexingOp,2024-08-01T07:29:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72939
yi,copybara-service[bot],"Prioritize delaying async-start that has async depth of 0, before looking at","Prioritize delaying asyncstart that has async depth of 0, before looking at ""kLessStall"", with an additional flag.",2024-07-31T21:09:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72916
yi,copybara-service[bot],Simplify GpuStream's interface.,"Simplify GpuStream's interface. 1. Remove IsIdle() method, which was only called during destruction in favor of calling the underlying code during the actual destructor. 2. Remove Destroy(), which was only called in the DeallocateStream which was only called in the destructor.  Instead, just call the code in the destructor. 3. Make Init() return an absl::Status. 4. Just pass the priority std::optional to the constructor rather than have discrete SetPriority methods taking either of the choices.",2024-07-31T17:16:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72899
rag,copybara-service[bot],[xla:cpu] Optimize ThunkExecutor::Execute part #2,"[xla:cpu] Optimize ThunkExecutor::Execute part CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"") Use std::aligned_storage_t trick to avoid defaultinitializing Node struct on a hot path. name                                     old cpu/op   new cpu/op   delta BM_SelectAndScatterF32/128/process_time   791µs ± 4%   720µs ± 2%  8.93%  BM_SelectAndScatterF32/256/process_time  3.20ms ± 4%  2.96ms ± 2%  7.46%  BM_SelectAndScatterF32/512/process_time  13.7ms ± 5%  12.8ms ± 2%  6.80%  name                                     old time/op          new time/op          delta BM_SelectAndScatterF32/128/process_time   790µs ± 5%           719µs ± 1%   9.00%  BM_SelectAndScatterF32/256/process_time  3.20ms ± 3%          2.96ms ± 1%   7.58%  BM_SelectAndScatterF32/512/process_time  13.2ms ± 4%          12.3ms ± 1%   6.82%",2024-07-31T16:03:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72892
yi,saad-koukous,How to reduce the size of the shared library,"**System information**  OS Platform and Distribution : Ubuntu 22.04.4 LTS  TensorFlow version 2.7.4 I need to use the tflite c library in a beaglebone black rev3 with this specs: > processor       : 0 > model name      : ARMv7 Processor rev 2 (v7l) > BogoMIPS        : 995.32 > Features        : half thumb fastmult vfp edsp thumbee neon vfpv3 tls vfpd32 > CPU implementer : 0x41 > CPU architecture: 7 > CPU variant     : 0x3 > CPU part        : 0xc08 > CPU revision    : 2 >  > Hardware        : Generic AM33XX (Flattened Device Tree) > Revision        : 0000 > Serial          : 2218SBB15982 > ldd (Debian GLIBC 2.2810) 2.28 > gcc version 8.3.0 i'm trying to build the library using this toolchain : gccarm8.32019.03x86_64armlinuxgnueabihf I successfully built a shared library, but the size of the .so file is 4 MB, which is too big for me. How can I reduce it? is there any way to limit the operators and types required by my model",2024-07-31T12:09:06Z,stat:awaiting response stale comp:lite TF 2.7,closed,0,4,https://github.com/tensorflow/tensorflow/issues/72878,"Hi **koukous** ,  Could you please use strip command to remove unnecessary symbols from the library. This command can significantly reduce the file size. I giving that command below:   And ensure you are compiling specifically for the ARMv7 architecture to avoid including unnecessary code for other architectures:  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,"Hi   I tried using the strip command as you suggested to remove unnecessary symbols from the libtensorflowlite_c.so However, the size of the library did not change after stripping. Additionally, I tried compiling the library specifically for the ARMv7 architecture Unfortunately, the file size remained the same even after these attempts. It seems that there might not be any significant symbols or unnecessary code that can be stripped from the library. If you have any other suggestions or further optimizations to try, please let me know! Thank you!"
rag,copybara-service[bot],Enable mlir reduction emitter by default.,"Enable mlir reduction emitter by default. This can be disabled with the flag xla_gpu_mlir_emitter_level, setting it to any value < 4. Change some tests to still use the old emitters. We have separate IR tests for the new emitters, and keeping the old tests running with the old emitters ensures we still have coverage for the old emitters, in case we need to rollback. One notable change with enabling emitter level 4 is that the heuristic to avoid code duplication due to cache invalidation is disabled. This was always a a workaround, and the new emitters fixed the problem. This is the most common source of why the tests behave differently between the old and the new emitters.",2024-07-31T10:50:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72876
yi,copybara-service[bot],PR #15444: Fixed some issues around compiling on Windows.,PR CC(Fix lib_strings_str_util_test on Windows): Fixed some issues around compiling on Windows. Imported from GitHub PR https://github.com/openxla/xla/pull/15444 This PR fixes some issues I bumped into when trying to compile XLA on Windows. I still haven't gotten GPU support to work but I'm making progress. The CPU only version compiles fine after some of the changes in this PR. I'll point out some specific issues this PR fixes in comments. There are also TSLspecific changes that are pulled in a separate PR ( CC(Go bindings: No shape inference function exists for op 'CreateSummaryFileWriter')). Copybara import of the project:  eacee95f41abc49a21516ee389861d84a40eca85 by eaplatanios : Fixed some issues around compiling on Windows.  b12e4cf0d23c2690111125a651e486ec6a112e54 by eaplatanios : .  e23ef176de72cf04555242174a19a407884f3f0e by eaplatanios : .  bdae19b9e15c396985703bb7e88a4db6fcddc7f6 by eaplatanios : .  2f90e6ba564f92fafa564b104ed0ce82b7642563 by eaplatanios : .  57009793b74c4d7d51fb39547a70a3ec142dadab by eaplatanios : .  a978b1f7f70d49f1426fe46b107fdcc3618e3085 by eaplatanios : .  d7fe81dc9cf909a6a8d70e2be8cfffca4063493e by eaplatanios : .  fc40d919619330bce596555613e425cb6267eea4 by eaplatanio,2024-07-31T09:15:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72872
yi,copybara-service[bot],PR #15444: Fixed some issues around compiling on Windows.,PR CC(Fix lib_strings_str_util_test on Windows): Fixed some issues around compiling on Windows. Imported from GitHub PR https://github.com/openxla/xla/pull/15444 This PR fixes some issues I bumped into when trying to compile XLA on Windows. I still haven't gotten GPU support to work but I'm making progress. The CPU only version compiles fine after some of the changes in this PR. I'll point out some specific issues this PR fixes in comments. There are also TSLspecific changes that are pulled in a separate PR ( CC(Go bindings: No shape inference function exists for op 'CreateSummaryFileWriter')). Copybara import of the project:  eacee95f41abc49a21516ee389861d84a40eca85 by eaplatanios : Fixed some issues around compiling on Windows.  b12e4cf0d23c2690111125a651e486ec6a112e54 by eaplatanios : .  e23ef176de72cf04555242174a19a407884f3f0e by eaplatanios : .  bdae19b9e15c396985703bb7e88a4db6fcddc7f6 by eaplatanios : .  2f90e6ba564f92fafa564b104ed0ce82b7642563 by eaplatanios : .  57009793b74c4d7d51fb39547a70a3ec142dadab by eaplatanios : .  a978b1f7f70d49f1426fe46b107fdcc3618e3085 by eaplatanios : .  d7fe81dc9cf909a6a8d70e2be8cfffca4063493e by eaplatanios : .  fc40d919619330bce596555613e425cb6267eea4 by eaplatanio,2024-07-31T08:40:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/72868
yi,tanwang2020,Shape Mismatch," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17.0  Custom code Yes  OS platform and distribution Windows  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I have two computers, one with tf 2.11.0, the other with tf.2.17.0. Using tf 2.11.0, the code given above generates no error message, while using tf 2.17.0, the code generates error message. Can anyone help?  Standalone code to reproduce the issue   Relevant log output ",2024-07-30T17:57:02Z,stat:awaiting response type:support stale comp:keras 2.17,closed,0,6,https://github.com/tensorflow/tensorflow/issues/72797,"Hi **** , Please post this issue on kerasteam/keras repo. as this issue is more related to keras Thank you!","I will try to post this issue, but in a slightly different form, as I discovered that a related issue may arise within tf 2.17.0. Below is the code. If this is indeed the case, then I think there might be a bug in tf 2.17.0. Here is the first version of the code: import numpy as np import tensorflow as tf from tensorflow.keras.constraints import non_neg from tensorflow.keras.layers import Input, Dense, concatenate from tensorflow.keras.models import Model print('TensorFlow Version: ',tf.__version__)  Example input data X_train_dict = {     'green_fin_const': np.random.rand(558, 3),     'green_fin_inst': np.random.rand(558, 4),     'gov_sup': np.random.rand(558, 5),     'com_act': np.random.rand(558, 5),     'eco_city': np.random.rand(558, 4),     'type': np.random.rand(558, 1) } Y_train = np.random.rand(558, 2) X_test_dict = {     'green_fin_const': np.random.rand(140, 3),     'green_fin_inst': np.random.rand(140, 4),     'gov_sup': np.random.rand(140, 5),     'com_act': np.random.rand(140, 5),     'eco_city': np.random.rand(140, 4),     'type': np.random.rand(140, 1) } Y_test = np.random.rand(140, 2)  Define input layers inputs_1 = Input(shape=(3,), name='green_fin_const') inputs_2 = Input(shape=(4,), name='green_fin_inst') inputs_3 = Input(shape=(5,), name='gov_sup') inputs_4 = Input(shape=(5,), name='com_act') inputs_5 = Input(shape=(4,), name='eco_city') inputs_6 = Input(shape=(1,), name='type')  Define dense layers score_1 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_1) score_2 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_2) score_3 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_3) score_4 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_4) score_5 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_5)  Concatenate scores and type input concatenated_scores = concatenate([score_1, score_2, score_3, score_4, score_5, inputs_6])  Define output layer outputs = Dense(2, activation='softmax', kernel_constraint=non_neg())(concatenated_scores)  Create the model model = Model(inputs=[inputs_1, inputs_2, inputs_3, inputs_4, inputs_5, inputs_6], outputs=outputs)  Compile model model.compile(optimizer='nadam', loss='mse', metrics=['KLDivergence'])  Fit the model model.fit(     x=X_train_dict,     y=Y_train,     validation_data=(X_test_dict, Y_test),     epochs=100,     batch_size=32,     verbose=0 )  Print input shapes to verify for key, value in X_train_dict.items():     print(f'{key}: {value.shape}') for key, value in X_test_dict.items():     print(f'{key}: {value.shape}') model.summary() The message from the system is: TensorFlow Version:  2.17.0  ValueError                                Traceback (most recent call last) Cell In[6], line 57      54 model.compile(optimizer='nadam', loss='mse', metrics=['KLDivergence'])      56  Fit the model > 57 model.fit(      58     x=X_train_dict,      59     y=Y_train,      60     validation_data=(X_test_dict, Y_test),      61     epochs=100,      62     batch_size=32,      63     verbose=0      64 )      65  Print input shapes to verify      66 for key, value in X_train_dict.items(): File ~\anaconda3\Lib\sitepackages\keras\src\utils\traceback_utils.py:122, in filter_traceback..error_handler(*args, **kwargs)     119     filtered_tb = _process_traceback_frames(e.__traceback__)     120      To get the full stack trace, call:     121      `keras.config.disable_traceback_filtering()` > 122     raise e.with_traceback(filtered_tb) from None     123 finally:     124     del filtered_tb File ~\anaconda3\Lib\sitepackages\keras\src\layers\input_spec.py:227, in assert_input_compatibility(input_spec, inputs, layer_name)     222     for axis, value in spec.axes.items():     223         if value is not None and shape[axis] not in {     224             value,     225             None,     226         }: > 227             raise ValueError(     228                 f'Input {input_index} of layer ""{layer_name}"" is '     229                 f""incompatible with the layer: expected axis {axis} ""     230                 f""of input shape to have value {value}, ""     231                 ""but received input with ""     232                 f""shape {shape}""     233             )     234  Check shape.     235 if spec.shape is not None: ValueError: Exception encountered when calling Functional.call(). Input 0 of layer ""dense_27"" is incompatible with the layer: expected axis 1 of input shape to have value 3, but received input with shape (None, 5) Arguments received by Functional.call():   • inputs={'green_fin_const': 'tf.Tensor(shape=(None, 3), dtype=float32)', 'green_fin_inst': 'tf.Tensor(shape=(None, 4), dtype=float32)', 'gov_sup': 'tf.Tensor(shape=(None, 5), dtype=float32)', 'com_act': 'tf.Tensor(shape=(None, 5), dtype=float32)', 'eco_city': 'tf.Tensor(shape=(None, 4), dtype=float32)', 'type': 'tf.Tensor(shape=(None, 1), dtype=float32)'}   • training=True   • mask={'green_fin_const': 'None', 'green_fin_inst': 'None', 'gov_sup': 'None', 'com_act': 'None', 'eco_city': 'None', 'type': 'None'}   Here is the second version of the code. In this version, I made only one change. I changed the name of the keys to shorter ones and the corresponding names in the specification of the inputs of the model. Now the program works. There is no error message.   import numpy as np import tensorflow as tf from tensorflow.keras.constraints import non_neg from tensorflow.keras.layers import Input, Dense, concatenate from tensorflow.keras.models import Model print('TensorFlow Version: ',tf.__version__)  Example input data X_train_dict = {     'A': np.random.rand(558, 3),     'B': np.random.rand(558, 4),     'C': np.random.rand(558, 5),     'D': np.random.rand(558, 5),     'E': np.random.rand(558, 4),     'T': np.random.rand(558, 1) } Y_train = np.random.rand(558, 2) X_test_dict = {     'A': np.random.rand(140, 3),     'B': np.random.rand(140, 4),     'C': np.random.rand(140, 5),     'D': np.random.rand(140, 5),     'E': np.random.rand(140, 4),     'T': np.random.rand(140, 1) } Y_test = np.random.rand(140, 2)  Define input layers inputs_1 = Input(shape=(3,), name='A') inputs_2 = Input(shape=(4,), name='B') inputs_3 = Input(shape=(5,), name='C') inputs_4 = Input(shape=(5,), name='D') inputs_5 = Input(shape=(4,), name='E') inputs_6 = Input(shape=(1,), name='T')  Define dense layers score_1 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_1) score_2 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_2) score_3 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_3) score_4 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_4) score_5 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_5)  Concatenate scores and type input concatenated_scores = concatenate([score_1, score_2, score_3, score_4, score_5, inputs_6])  Define output layer outputs = Dense(2, activation='softmax', kernel_constraint=non_neg())(concatenated_scores)  Create the model model = Model(inputs=[inputs_1, inputs_2, inputs_3, inputs_4, inputs_5, inputs_6], outputs=outputs)  Compile model model.compile(optimizer='nadam', loss='mse', metrics=['KLDivergence'])  Fit the model model.fit(     x=X_train_dict,     y=Y_train,     validation_data=(X_test_dict, Y_test),     epochs=100,     batch_size=32,     verbose=0 )  Print input shapes to verify for key, value in X_train_dict.items():     print(f'{key}: {value.shape}') for key, value in X_test_dict.items():     print(f'{key}: {value.shape}') model.summary() I also tried a third version, another change from the first version. In this version, I did not change the lengths of the names of keys, but reduced the number of inputs and hence layers. The code also worked without generating error. Here is the code: import numpy as np import tensorflow as tf from tensorflow.keras.constraints import non_neg from tensorflow.keras.layers import Input, Dense, concatenate from tensorflow.keras.models import Model print('TensorFlow Version: ',tf.__version__)  Example input data X_train_dict = {     'green_fin_const': np.random.rand(558, 3),     'green_fin_inst': np.random.rand(558, 4) } Y_train = np.random.rand(558, 2) X_test_dict = {     'green_fin_const': np.random.rand(140, 3),     'green_fin_inst': np.random.rand(140, 4) } Y_test = np.random.rand(140, 2)  Define input layers inputs_1 = Input(shape=(3,), name='green_fin_const') inputs_2 = Input(shape=(4,), name='green_fin_inst')  Define dense layers score_1 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_1) score_2 = Dense(1, activation='sigmoid', kernel_constraint=non_neg())(inputs_2)  Concatenate scores and type input concatenated_scores = concatenate([score_1, score_2])  Define output layer outputs = Dense(2, activation='softmax', kernel_constraint=non_neg())(concatenated_scores)  Create the model model = Model(inputs=[inputs_1, inputs_2], outputs=outputs)  Compile model model.compile(optimizer='nadam', loss='mse', metrics=['KLDivergence'])  Print input shapes to verify for key, value in X_train_dict.items():     print(f'{key}: {value.shape}') for key, value in X_test_dict.items():     print(f'{key}: {value.shape}') model.summary()  Fit the model model.fit(     x=X_train_dict,     y=Y_train,     validation_data=(X_test_dict, Y_test),     epochs=100,     batch_size=32,     verbose=0 )  Print input shapes to verify for key, value in X_train_dict.items():     print(f'{key}: {value.shape}') for key, value in X_test_dict.items():     print(f'{key}: {value.shape}') model.summary()","Hi **** , Apologies for the delay, and thank you for your patience. I tried running your code on Colab using TensorFlow 2.18 and observed the same issue as you. As an alternative, I tried using different shapes, and it worked fine. I hope this will be helpful for you. Please find the gist here for reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,LuvolwethuTokwe,Tensorflow Issue,I would like to be assisted in the issue I arise on when running tensorflow  Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf24.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError                               Traceback (most recent call last) File ~\AppData\Roaming\Python\Python312\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_inter,2024-07-30T13:24:57Z,stat:awaiting response type:bug stale TF 2.4,closed,0,4,https://github.com/tensorflow/tensorflow/issues/72782,"Hi **** ,  Can you please take a look at this comment from the issue with similar error.It helps.Also in order to expedite the troubleshooting process, could you please provide the following information OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: TensorFlow installed from (source or binary): Installed using virtualenv? pip? conda?: Bazel version (if compiling from source): GCC/Compiler version (if compiling from source): CUDA/cuDNN version: GPU model and memory: and the exact sequence of commands / steps that you executed before running into the problem. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Duplicate of CC(Help! I have an issue when importing TF!) ,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Move StreamExecutor::Submit method to GpuCommandBuffer.,"Move StreamExecutor::Submit method to GpuCommandBuffer. The Submit method didn't need anything from any StreamExecutor class, and is fullyimplementable as part of GpuCommandBuffer. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15216 from olupton:numapinning dc6c68e1252fb98cb4d8b4be4dc3833a6d78494c",2024-07-29T20:58:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72722
yi,grep-mb,MirroredStrategy() getting stuck at optimizer level when calling apply_gradients with tf2.16.2," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.2  Custom code Yes  OS platform and distribution Linux Ubuntu 22, MacOS M1, MacOS Intel chip  Mobile device _No response_  Python version 3.11  Bazel version 7.2.1  GCC/compiler version 14.0.3  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The code will get stuck when applying gradients with the optimizer with both `tf.keras.optimizers.legacy.Adam` and `tf.keras.optimizers.Adam`  Standalone code to reproduce the issue   Relevant log output _No response_",2024-07-29T07:33:52Z,stat:awaiting response type:bug stale comp:dist-strat TF 2.16,closed,0,4,https://github.com/tensorflow/tensorflow/issues/72686,"mb, Looks like this issue is happening with the tensorflow v2.16, 2.17 which contains the keras3.0 by default causing the issue. Could you please raise the issue in kerasteam/keras repo for the quick resolution. Also there is a similar issue in the keras repo which is in open state. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Add IndexingMapAttr to ApplyIndexingOp,Add IndexingMapAttr to ApplyIndexingOp,2024-07-29T07:19:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72685
rag,tom2002965,Compare to different GPUs (RTX 4060Ti and GTX 1660Ti) performance," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.2.3/tf2.3/ tf2.5/tf2.10/tf2.12  Custom code Yes  OS platform and distribution Ubuntu 18.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hello, I'm comparing the performance of two GPUs and the differences between different TensorFlow versions. I plan to upgrade from GTX 1660 Ti to RTX 4060Ti. I expected the inference speed to be faster after the upgrade, but surprisingly it wasn't. Below are the results of inferring different models and batch sizes. I used Docker to test different versions of TF. Due to cuda version, it's hard to compare almost tf versions. The number is average inference ms   GTX 1660 Ti    RTX 4060 Ti   GTX 1660 Ti vs RTX 4060 Ti (w/ same tf version)  From the chart above, I've drawn a few conclusions: 1. The higher the TF version, the longer the inference time... why??? 2. I expected the 4060Ti to be faster than the 1660Ti for the same TF version, but this is only observed when bs=8/16, while my use ",2024-07-28T03:32:05Z,stat:awaiting response type:performance TF 2.12,closed,0,4,https://github.com/tensorflow/tensorflow/issues/72635,"Hi **** ,  Here i observed version mismatch. Could you please check with compatibility versions? And Please use latest version for better results. let us know whether the issue persists with recent TF versions. Thank you!","Hi  ,  Very thank for your response.  All the tf/cuda/cudnn are from docker which is tensorflow provided.  Now my gpu drive of RTX 4060Ti is `535.183.01` . I have to update driver to test latest version . So I test tf2.14 first. The result is below.  2.14 is faster than 2.10/2.12, but it still slower than tf 2.5.  Thanks.","Hi **** , Apologies for the delay, and thank you for your patience. It appears there are still compatibility issues with TensorFlow 2.14.0. The compatible version of cuDNN is 8.7. Could you please doublecheck all the compatible versions and verify with the latest ones? If you continue to face issues, please let us know. For your convenience, here is the documentation to help verify the compatible versions. Thank you!",Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],Support returning stack frames in `StackTraceWrapper` without caching any generated data.,"Support returning stack frames in `StackTraceWrapper` without caching any generated data. When instantiating a function and running `MlirFunctionOptimizationPass`, the `GraphDefImporter` leverages `StackTraceWrapper` to get the location of individual graph nodes. The intermediate stack trace could lead to significant host RAM usage when cached in memory, e.g., 35+G in this experiment. With this change, the MLIR importer invokes `ToUncachedFrame` and the frames are discarded after the call. This is recovering an old behavior that was previously deleted in cl/539480407.",2024-07-27T21:02:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72631
yi,copybara-service[bot],Allow using host layout for argument when allocating buffers.,Allow using host layout for argument when allocating buffers. Extends multi_host_runner's running_option to allow using the layout in the host literal for argument when copying arguments to device.,2024-07-27T19:29:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72630
yi,MikeyD-rbg,Failed to load the native TensorFlow runtime.,"Trying to run a captcha solving python script and got this error; Traceback (most recent call last):   File ""C:\Users\micha\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\Users\micha\Documents\PythonProgramming\IG\pytesseract_solver.py"", line 9, in      import tensorflow   File ""C:\Users\micha\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\__init__.py"", line 37, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\micha\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 85, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\micha\AppData\Local\Programs\Python\Python312\Lib\sitep",2024-07-27T16:56:12Z,stat:awaiting response type:build/install type:support stale,closed,0,4,https://github.com/tensorflow/tensorflow/issues/72629,"rbg, Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:  https://github.com/tensorflow/tensorflow/issues/61887 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,Rosie-93,Long error message when trying to run a tensor flow program,"Hey All, Im new to the deep learning field and I'm trying to follow a tutorial. Im using a MacOS and PyCharm IDE.  I have successfully installed tensorFlow package. I wrote a simple ( Hello World) and this error appeared, then I tried to trace back at which line this error appear, simply from the very basic first line  `import tensorflow as tf`   I tried one of the solutions online that I should install `pip install tensorflowcpu` rather than `pip install tensorflow` but same error appear.  I appreciate any tips ",2024-07-27T08:55:28Z,type:support stale,closed,0,10,https://github.com/tensorflow/tensorflow/issues/72622,"It seems like there may be conflicting dependencies of the package and Python version installed. To address the issue, you can simply create a conda environment (Dependency management) with a compatible version of Python for your target TensorFlow version. Next, activate it and install TensorFlow, which should run without problems.  Steps to Resolve the Issue: 1. **Create a Conda Environment with a Specific Python Version**:          Replace `3.x` with the specific Python version you need. 2. **Activate the Conda Environment**:      3. **Follow TensorFlow Installation Guide**:     Follow the instructions from the official TensorFlow installation guide for macOS: TensorFlow Installation Guide","> It seems like there may be conflicting dependencies of the package and Python version installed. To address the issue, you can simply create a conda environment (Dependency management) with a compatible version of Python for your target TensorFlow version. Next, activate it and install TensorFlow, which should run without problems. >  >  Steps to Resolve the Issue: > 1. **Create a Conda Environment with a Specific Python Version**: >     >     >     >         >           >         >     >           >         >     >         >       >    Replace `3.x` with the specific Python version you need. > 2. **Activate the Conda Environment**: >     > 3. **Follow TensorFlow Installation Guide**: >    Follow the instructions from the official TensorFlow installation guide for macOS: TensorFlow Installation Guide thank you for your comment, I have been working with Installing packages through pycharm only, do I need this ( Conda environment)? I mean is there any difference between it and what I have been doing so far with Pycharm only?",There is no difference except for managing your Python and TensorFlow versions. So you can also set another Python version for the Pycharm environment and install your TensorFlow package within that one. ,"Hi **93** ,  Could you please ensure that the tensorflow version you installed is compatible with your macos and python version. And verify that you are working in the correct virtual environment where tensorflow installed. Try to reinstall tensorflow. And follow this documentation for better results. Thank you!","rezaei   I have used Conda as the environment, the first 2 sentences warning appeared but the program was executed successfully, I went and tried Python ,pip required versions and Tensorflow as well using Pycharm default environment however, it did NOT work even with the right versions thats why I found it confusing why wouldn't it work with pycharm environment",My view would be that there are some package dependencies in default environment. It would be resolved if u install a new environment with your system's compatible versions.,"Hi **93** ,  Here I followed below steps to install tensorflow in PyCharm IDE. It runs successfully to me. 1. I installed the PyCharm IDE by using the tool box app from the PyCharm site. 2. After that python venv to create the environment  for the new project. 3. Then I installed tensorflow by using pip.  4. After that import tensorflow is working fine.   Here I am providing screenshots for your reference. , . Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Hi **93** , I am closing this issue as this was resolved. Please feel free to reopen if I am mistaken. Thank you!",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],internal change only,internal change only,2024-07-26T22:50:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72603
rag,copybara-service[bot],[XLA:GPU][NFC] Force `triton_support_test.cc` to run on GPU.,"[XLA:GPU][NFC] Force `triton_support_test.cc` to run on GPU. This is a temporary measure to get OSS coverage while the test fails on CPU. The alternative is completely disabling it in OSS, which is not ideal.",2024-07-26T06:34:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72554
yi,pradeep10kumar,A KerasTensor is symbolic: it's a placeholder for a shape an a dtype. It doesn't have any actual numerical value. You cannot convert it to a NumPy array.,"I am using following version:  TF Version:  2.18.0dev20240716 Eager mode:  True Hub version:  0.16.1 While create the model I am facing the follwoing problem. I tried different version but it did not help. ValueError                                Traceback (most recent call last) Cell In [23], line 1> 1 model = create_model()      2 model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=2e5),      3              loss= tf.keras.losses.CategoricalCrossentropy(),      4              metrices = [tf.keras.metrics.Accuracy])      5 model.summary() Cell In [22], line 10, in create_model()      5 input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,      6                                    name=""input_mask"")      7 input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,      8                                     name=""input_type_ids"")> 10 pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])     11 drop =  tf.keras.layers.Dropout(0.2)(pooled_output)     12 output = tf.keras.Dense(label_list, activation = 'sigmoid', name = ""output"")(drop) File ~\AppData\Roaming\Python\Python310\sitepackages\tf_keras\src\utils\traceback_",2024-07-25T08:49:26Z,stat:awaiting response stale comp:keras 2.17,closed,2,9,https://github.com/tensorflow/tensorflow/issues/72497,"Hi **** ,  Make sure the bert_layer is properly loaded and initialized. If it is a TensorFlow Hub layer, Confirm it is correctly set up. And In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thank you!","import tensorflow as tf import tensorflow_hub as hub  Define IMAGE_SHAPE IMAGE_SHAPE = (224, 224) def create_model(model_url, num_classes=10):     """"""     Takes a TensorFlow Hub URL and creates a Keras model with it using the functional API.     Args:         model_url (str): A TensorFlow Hub feature extraction URL.         num_classes (int): Number of output neurons in the output layer,           should be equal to number of target classes, default 10.     Returns:         An uncompiled Keras model with model_url as feature extractor         layer and Dense output layer with num_classes output neurons.     """"""      Download the pretrained model and save it as a Keras layer     feature_extractor_layer = hub.KerasLayer(model_url,                                              trainable=False,   freeze the already learned patterns                                              input_shape=IMAGE_SHAPE + (3,))      Define the input layer     inputs = tf.keras.Input(shape=IMAGE_SHAPE + (3,))      Pass the inputs through the feature extractor layer     x = feature_extractor_layer(inputs)   Explicitly set training=False      Define the output layer     outputs = tf.keras.layers.Dense(num_classes, activation=""softmax"")(x)      Create the model     model = tf.keras.Model(inputs=inputs, outputs=outputs)     return model  Example usage  Ensure you have a valid URL and the required data resnet_url = ""https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5"" num_classes = 10   For example purposes resnet_model = create_model(resnet_url, num_classes=num_classes)  Print the model summary to verify resnet_model.summary()",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"I am also facing a similar issue with the later TF versions. The same code runs without errors on TF 2.10.0, but with 2.16.1 throws this error.  ValueError: Exception encountered when calling layer 'autoregressive_network' (type AutoregressiveNetwork). A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:  What you should do instead is wrap `tf_fn` in a layer:  Call arguments received by layer 'autoregressive_network' (type AutoregressiveNetwork):   • x=<KerasTensor shape=(None, 2), dtype=float  Can someone else try this code given here : https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/AutoregressiveNetwork   Generate data  as in Figure 1 in [Papamakarios et al. (2017)][2]). n = 2000 x2 = np.random.randn(n).astype(dtype=np.float32) * 2. x1 = np.random.randn(n).astype(dtype=np.float32) + (x2 * x2 / 4.) data = np.stack([x1, x2], axis=1)  Density estimation with MADE. made = tfb.AutoregressiveNetwork(params=2, hidden_units=[10, 10]) distribution = tfd.TransformedDistribution(     distribution=tfd.Sample(tfd.Normal(loc=0., scale=1.), sample_shape=[2]),     bijector=tfb.MaskedAutoregressiveFlow(made))  Construct and fit model. x_ = tfkl.Input(shape=(2,), dtype=tf.float32) log_prob_ = distribution.log_prob(x_) model = tfk.Model(x_, log_prob_) model.compile(optimizer=tf.optimizers.Adam(),               loss=lambda _, log_prob: log_prob) batch_size = 25 model.fit(x=data,           y=np.zeros((n, 0), dtype=np.float32),           batch_size=batch_size,           epochs=1,           steps_per_epoch=1,   Usually `n // batch_size`.           shuffle=True,           verbose=True)  Use the fitted distribution. distribution.sample((3, 1)) distribution.log_prob(np.ones((3, 2), dtype=np.float32))  I am trying to understand if this is a genuine error or due to some installation problems on my end. ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,"Hi **** , **** , Could you please raise a new issue for your concern, so that it will be easier to track. Hi  , Please try to provide the code snippet as I requested above. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,6eanut,error: 'NPY_NTYPES' was not declared in this scope; did you mean 'NPY_TYPES'?," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.13.0  Custom code Yes  OS platform and distribution Linux openeulerriscv42 6.6.0  Mobile device _No response_  Python version 3.11.6  Bazel version 5.3.0  GCC/compiler version 12.3.1  CUDA/cuDNN version no  GPU model and memory no  Current behavior? I am trying to build TensorFlow2.13.0 on aarch64 with bazel5.3.0, but there are problems during the build process：   Standalone code to reproduce the issue   Relevant log output _No response_",2024-07-25T00:38:26Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.13,closed,0,7,https://github.com/tensorflow/tensorflow/issues/72480,", Could you please try to install the tensorflow with the compatible tested build configurations. tensorflow2.13.0, python	3.83.11,	Clang 16.0.0,	Bazel 5.3.0, cuDNN8.6, CUDA11.8 https://www.tensorflow.org/install/sourcegpu And tensorflow v2.13 is an old version, please try to install  with the latest tensorflow version. Thank you!"," thanks for helping! I did configure the tool according to the corresponding version, but I still encountered the above problem",same error,Could you please check whether the same issue you are facing with the latest Tensorflow v2.17 or v2.18 and provide the update where the supported python version has been upgraded to 3.93.12. https://www.tensorflow.org/install/sourcecpu Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,Apprisco,Distributed Training without strategies- can't average gradients," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17  Custom code Yes  OS platform and distribution Ubuntu Server  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm trying to not use a tf.dataset for a distributed learning scenario. As such, using MirroredStrategy is out of the question. However, I can replicate MirroredStrategy by creating multiple instances of my model, one for each GPU. And I can gather gradients and manually send batches to each model using said method. However, once I received all the gradients from tf.gradients(), I am unable to average them. I tried tf.stack, converting to np, etc, but they all fail due to dimensions being different within the gradient itself. Each model gives the correct gradient back for each batch, but as I'm unable to average them I am unable to train the model on multiple gpus. I'm not sure how I'm supposed to average/accumulate gradients between each GPU, and unsure how mirroredstrategy achieves this.  Standalone code to reproduce the issu",2024-07-25T00:27:21Z,stat:awaiting response type:support stale comp:data 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/72477,", Could you please share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
agent,copybara-service[bot],Refactor the heartbeat lambda function in coordination service agent into a private member function.,Refactor the heartbeat lambda function in coordination service agent into a private member function.,2024-07-24T20:30:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72462
yi,copybara-service[bot],Fix `llvm_compiler_test` debug compilation.,"Fix `llvm_compiler_test` debug compilation. When trying to build this test for LLDB debugging purposes, there is a linker error: `ld: error: duplicate symbol: main`. This error is only observed with `dynamic_mode=off` bazel flag. Removing gunit from dependencies fixes that issue.",2024-07-24T13:41:32Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/72438,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
yi,Jacob-yen,tensorflow.python.framework.errors_impl.AbortedError: Exception encountered when calling MaxPooling1D.call()," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17 and 2.18.0.dev20240717   Custom code Yes  OS platform and distribution Ubuntu 20.04.3 LTS (x86_64)  Mobile device _No response_  Python version Python version: 3.10.14   Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered a bug in TensorFlow when using the `tf.keras.layers.MaxPool1D` API after setting the input data format to 'channels_first'. The `tf.keras.layers.MaxPool1D` throws an `tensorflow.python.framework.errors_impl.AbortedError` exception. However, the average pooling operator executes successfully without any exceptions. The code is as follows:  The error message was as follows:  The above code would throw an exception on `tf2.17` and `tfnightly 2.18.0.dev20240717` (nightlybuild).  Standalone code to reproduce the issue   Relevant log output ",2024-07-24T03:09:12Z,stat:awaiting response type:bug comp:keras 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/72411,The same issue occurs with MaxPool2D and MaxPool3D.,"yen, I tried to execute the code on tensorflow tfnightly where it was executed without any issues or errors. Kindly find the gist of it here. The tfnightly(2.18.0dev20240722) contains keras3.0 where we should import keras and keras.layers.MaxPool1D   and try to execute the code. Thank you!","Thank you for your prompt reply. I tried the notebook, and it works well. However, the above code still crashes on my Linux server with tfnightly (2.18.0dev20240722). To investigate the issue, I tested the code on three different servers, installing the same dependency packages with Python 3.10.   I found that two servers with `Python 3.10.14 and GCC 11.2.0` crashed, while the other server with `Python 3.10.4 and GCC 7.5.0` ran the code without any problems. I'm not sure what caused the crash, but it may be a compatibility issue between the TensorFlow version and the GCC version. I will temporarily close the issue until further reasons are found.",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[xla:cpu] Make WhileThunk non-blocking,[xla:cpu] Make WhileThunk nonblocking BlockUntilReady inside Thunk:Execute is illegal and leads to deadlocks. Run while loop asynchronously relying on AndThen callbacks.,2024-07-23T18:28:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72384
yi,JVD9kh96,Custom metrics results returned by 'History' callbacks doesnt work properly," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.10.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I have defined two custom metrics to track SSIM and PSNR in my denoising autoencoder during the training. While the logs printed during the training seems to work properly, it returns weird results at the end by 'History' callback. All the values for training and validation psnr and ssim equal to the last printed value for validtion psnr and ssim, respectively. Like the older versions, I have defined internal variables by using add_variable method. I have also tried with add_weight. Both showed same behaviour.  By the way, history callback used to return metrics as numpy arrays, but for the custom metrics that I defined, it returns KerasVariable instances.  Is there any fixes for this? This is a bit annoying that some metrics like loss are returned as numpy arrays while the custom metrics are KerasVariable instances.   Standalo",2024-07-23T13:34:55Z,stat:awaiting response type:bug stale comp:keras 2.17,closed,0,8,https://github.com/tensorflow/tensorflow/issues/72366,"instead of using tf.keras.metrics.Metric I also tried tf.keras.Metric for defining the metrics, the results were the same",", Thank you for reporting the issue. As this issue is more related to keras, could you please raise the issue in kerasteam/keras for quick resolution. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"For those who are looking for a quick fix, you can use legacy keras: `pip install tf_keras` ",", Glad the issue was resolved with the tfkeras. Kindly find the gist of it here. And tensorflow v2.17 contains Keras3.0, and this is more related to keras please try to raise the issue in kerasteam/keras repo for the quick resolution. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,gajendrahatt,TensorFlow lite micro on Cortex-M7 ," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.13.1  Custom code No  OS platform and distribution S32K344 CortexM7  Mobile device CortexmM7  Python version 3.9  Bazel version _No response_  GCC/compiler version armnoneeabigcc 10.2  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I trained the tensorflow model with layers Conv2d followed bye relu and Maxpool2D.  this is the architecture i used import tensorflow as tf model1 = tf.keras.Sequential()  model1.add(tf.keras.Input(shape=(750,3,1))) model1.add(Conv2D(4,kernel_size=(5,5),strides=(1,1), padding='same',input_shape=(750, 3, 1))) model1.add(tf.keras.layers.Activation(activation='relu')) model1.add(MaxPooling2D(pool_size=(4,4),padding='same'))     model1.add(Dropout(0.25)) model1.add(Conv2D(8,kernel_size=(5,5),padding='same')) model1.add(tf.keras.layers.Activation(activation='relu')) model1.add(MaxPooling2D(pool_size=(4,4), padding='same'))   model1.add(Conv2D(8,kernel_size=(5,5),padding='same')) model1.add(tf.keras.layers.Activation(activation='relu')) model1.add(MaxPooling2D(pool_size=(4,4), padding='same'))   model1.add(Flatten()) model1.add(Dens",2024-07-23T13:26:16Z,stat:awaiting response type:bug stale comp:lite comp:micro TFLiteConverter TF 2.13,closed,0,4,https://github.com/tensorflow/tensorflow/issues/72364,"Hi , I believe https://github.com/tensorflow/tflitemicro/issues will be able to assist you better. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[XLA_GPU][MLIR-based emitters] Add a pass to flatten tensors.,"[XLA_GPU][MLIRbased emitters] Add a pass to flatten tensors. Right now it only supports tensor.extract, tensor.insert, xla_gpu.atomic_rmw, func.func and func.return, scf.for, scf.if and scf.yield.",2024-07-23T09:48:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72354
yi,arteen1000,tf.strided_slice new_axis_mask inconsistency," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tfnightly, 2.17.0, 2.16.2, 2.16.1  Custom code No  OS platform and distribution macOS, Linux  Mobile device _No response_  Python version 3.10.12, 3.12.4  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm seeing that when `spec_size` (as described in the docs as the length of the `begin`, `end`, `strides` arrays) is less than the rank of the `input` tensor, the behavior of `strided_slice` differs when `new_axis_mask` for the bits between `len(begin)` and `tf.rank(input)` is specified. Is this the expected behavior, and if so, is there anything else like this that differs when `spec_size != tf.rank(input)`?  I'm noting that it's currently permitted for `spec_size > tf.rank(input)`, which yields the same result in this case as when `spec_size == tf.rank(input)`.  Standalone code to reproduce the issue  ```  Relevant log output _No response_",2024-07-23T00:44:47Z,stat:awaiting tensorflower type:bug comp:ops 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/72332," I tried to run your code on Colab using TF v2.17.0, nightly and faced the same issue. Please find the gist here for reference. Thank you!"
yi,KnightGOKU,tf.raw_ops.MapUnstage: Check failed: 1 == NumElements() (1 vs. 120)Must have a one element tensor Aborted (core dumped)," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0.dev20240717  Custom code Yes  OS platform and distribution Ubuntu 20.04.3 LTS (x86_64)  Mobile device _No response_  Python version Python version: 3.10.14   Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered a bug in TensorFlow when I used API `tf.raw_ops.MapUnstage`  with randomly generated tensors. The code is as follows:  The error message was as follows:  I have confirmed that above code would crash on `tfnightly 2.18.0.dev20240717` (nightlybuild). Also, I provided that a colab notebook to reproduce the error.  Standalone code to reproduce the issue   Relevant log output ",2024-07-22T14:08:43Z,stat:awaiting tensorflower type:bug comp:ops 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/72295,"I was able to reproduce the issue on tensorflow v2.15, v2.17 and tfnightly. Kindly find the gist of it here."
yi,copybara-service[bot],Prepare tests for MLIR reduction emitter launch.,"Prepare tests for MLIR reduction emitter launch.  hlo tests just hard code the emitter level to 0. We can adjust   these when we remove the flag (or remove them / move them to   )  tests that depend on MOF were adjusted to the new IR, except for   one that tests something that does not occur in real pipelines.   I'm not really sure what that test is trying to test  probably   side outputs, which are covered in the unit test.  one test that verified a failure to vectorize was disabled",2024-07-22T11:54:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72290
yi,siy415,[Android]Failed to run on the given Interpreter: tensorflow/lite/kernels/transpose.cc:63 op_context->perm->dims->data[0] != dims (3 != 2)," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.7, tflite 2.16.1, tensorflowliteselecttfops:2.16.1  Custom code Yes  OS platform and distribution Linux 20.04  Mobile device Android SDK 28  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? My .tflite model works on python but it dosen't work well on android project. It seems no difference between python and android.  What kinds of layer or function changes demention of input?  Standalone code to reproduce the issue  Model: ""sequential_43"" ┏━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━┓ ┃ Layer (type)                        ┃ Output Shape         ┃        Param  ┃ ┡━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━┩ │ embedding_35 (Embedding)│ (None, 256, 10)         │         350,000 │ ├──────────────────┼─────────────┼─────────┤ │ gru_33 (GRU)                         │ (None, 10)                │                 660 │ ├──────────────────┼─────────────┼─────────┤ │ dense_27 (Dense)                  │ (None, 2)                   │                  22 │ └──────────────────┴─────────────┴─────────┘  Total pa",2024-07-22T08:37:24Z,stat:awaiting response type:bug stale comp:lite Android TF 2.16,closed,0,4,https://github.com/tensorflow/tensorflow/issues/72279,"Hi,   I apologize for the delayed response and if possible could you please help us with Google colab notebook in which you converted your model to TensorFlow Lite format and your github repo of android project which will help us to replicate the same behavior from our end to investigate this issue further from our end ?  Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],Legalize mhlo.reduce window to tfl,"Legalize mhlo.reduce window to tfl * Add op view for reduce window * Move some of the conv view logic into common file to be shared * Add new pattern that makes all reduce_window NHWC in prepare * Migrate average pool lowering to mhlo>tfl, use new op view, lots of cleanups  The pattern to transpose reduce windows needs to go at the same time as the avg pool legalizations. This is because inserting the transposes will disrupt the mhlo>tf path, but that logic is covered in the new legalization.",2024-07-22T08:29:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72277
yi,VicB18,TF2 Fast Style Transfer for Arbitrary Styles running problems," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Win 10  Mobile device _No response_  Python version 3.12.4  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I try to run the Fast Style Transfer for Arbitrary Styles https://www.tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization on Windows 10, Python 3.12.4, TensorFlow 2.16.1. When the copied original code was run (Style_transfer.py in this case), the following error appeared for the lines > 79: hub_handle = 'https://kaggle.com/models/google/arbitraryimagestylizationv1/frameworks/TensorFlow1/variations/256/versions/1' >  > 80: hub_module = hub.load(hub_handle)  What to do with `Access is denied.` I did not find. Probably, the solution is simple, but it is hidden in a ton of issues. Next, I tried to download the model with  > os.environ[""TFHUB_MODEL_LOAD_FORMAT""] = ""UNCOMPRESSED"" The error was  There are a lot of posts with the `'gs' not implemented` issue, mainly from 20172018, but nothing solved this. I have no idea how to pr",2024-07-20T21:42:13Z,type:support,closed,0,1,https://github.com/tensorflow/tensorflow/issues/72235,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],"In `build.py`, stop trying to pull container after first successful pull","In `build.py`, stop trying to pull container after first successful pull Reverts aeeeef0ba125dd2b28b59c5d144dd0a237a780c4",2024-07-19T20:57:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72211
yi,copybara-service[bot],internal visibility change only,internal visibility change only,2024-07-19T19:40:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72206
yi,jaskarannagi19,Mmap of '41' at offset '0' failed with error '22'. on movenet, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynbscrollTo=48kW1c2F5l1R I amy trying to run this repo with movenet but I get error. Mmap of '41' at offset '0' failed with error '22'. on movenet load function. No change has been made. I am running this on colab directly please help  Standalone code to reproduce the issue   Relevant log output _No response_,2024-07-19T18:42:23Z,type:bug comp:lite TF 2.15,closed,0,6,https://github.com/tensorflow/tensorflow/issues/72202,"Yes, getting same issue. Can you resolve please?",", I tried to execute the code and observed that it was executed without any fail/error. Kindly find the gist of it here and also please have a look at the issue for the reference. https://github.com/tensorflow/tensorflow/issues/70841 Thank you!", No I think the code you are refering to has a different tf load method. I am more interested in training that notebook on my own examples. I think there is a problem in model that is not compatible ,I got the same error. I replaced the code   !image I highlighted the changes. The model i donwloaded from the github linkhttps://github.com/devfemibadmus/humanposeestimation or even download tflite models from the kaggle link  https://www.kaggle.com/models/google/movenet/tfLite/singleposethunder/1?tfhubredirect=true replace the model movenet = Movenet('downloaded model.tflite') with the one downloaded and the code should run fine!   Load MoveNet Thunder model import utils from data import BodyPart from ml import Movenet movenet = Movenet('downloaded model.tflite'), I think that's what we did followed by change in str() to str()_ in movenetpreprocessor class method to support numpy 1.25.2.  That notebook does need a new fresh link for the model out of the box link does not work,Are you satisfied with the resolution of your issue? Yes No
yi,libofei2004,how to install mediapipe_model_maker0.2.1.4 in windows?," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf2.8  Custom code No  OS platform and distribution windows11  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I run ""pip install mediapipe_model_maker""  , but I can only install version 0.1.0.1. I was trying to install mediapipe_model_maker by ""pip install mediapipe_model_maker0.2.1.4py3noneany.whl"", but also failed. The error: ERROR: Cannot install mediapipemodelmaker and mediapipemodelmaker==0.2.1.4 because these package versions have conflicting dependencies. The conflict is caused by:     mediapipemodelmaker 0.2.1.4 depends on tensorflowtext     tfmodelsofficial 2.15.0 depends on tensorflowtext~=2.15.0     mediapipemodelmaker 0.2.1.4 depends on tensorflowtext     tfmodelsofficial 2.14.2 depends on tensorflowtext~=2.14.0     mediapipemodelmaker 0.2.1.4 depends on tensorflowtext     tfmodelsofficial 2.14.1 depends on tensorflowtext~=2.14.0     mediapipemodelmaker 0.2.1.4 depends on tensorflowtext     tfmodelsofficial 2.14.0 depends on tenso",2024-07-19T18:06:22Z,type:bug TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/72198,     Could you please help me to solve my problem?,"Hi **** ,  Could you please post this issue on https://github.com/googleaiedge/[mediapipe] (https://github.com/googleaiedge/mediapipe/issues) as this issue belongs to mediapipe. Thank you!"," Yes, I have post the issue on mediapipe page.","Hi **** , Closing the issue since it is being tracked in other repo. Please feel free to reopen the issue if necessary. Thank you!",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[XLA:GPU] Pass operand to the construction of TiledHloInstruction.,"[XLA:GPU] Pass operand to the construction of TiledHloInstruction. We use `tile_offset_indexing` to distinguish between tiles of the same instruction.  Composing and simplifying indexing maps is expensive. For instruction inside the fusion that are not load/store, comparing `operand` pointers is a cheaper way to achieve the same effect. This change is a preparation to compute `tile_offset_indexing` only when necessary.",2024-07-19T11:43:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72179
yi,copybara-service[bot],[XLA:GPU][MLIR-based emitters] Add a pattern that can shrink/refine bounds.,"[XLA:GPU][MLIRbased emitters] Add a pattern that can shrink/refine bounds. Right now, it can refine ranges of dims/symbols based on the ranges of the induction variables. This is needed to update apply_indexing ops after the peeling is done.",2024-07-19T10:50:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72174
gemma,copybara-service[bot],PR #13425: [ROCM] gemm precision settings for autotuner,"PR CC(Initialize fetchTensors to fix NullPointerException): [ROCM] gemm precision settings for autotuner Imported from GitHub PR https://github.com/openxla/xla/pull/13425 Here we add a new flag **xla_gpu_autotune_gemm_rtol** which controls the relative precision used by the BufferComparator (defaults to **0.1**). Also I added one more ""paranoid"" level 5 for **xla_gpu_autotune_level** which forces the autotuner to discard solutions with accuracy problems. Long time I was under impression that the autotuner already does it, however this is not the case as outlined here. BufferComparator just prints out the error message but **keeps wrong solutions** as possible candidates which could lead to a great confusion. So, the autotune level 5 is supposed to discard solutions with accuracy problems. Besides, I also did some small refactoring on BufferComparator to simplify the source code and added **verbose** flag in order to mute error messages if needed. rotation: could you please have a look? Copybara import of the project:  cab53b672f9546fe4b811d09cf998b105dc8be01 by Pavel Emeliyanenko : added precision settings for autotuner and buffer_comparator small refactoring, added verbose flag Merging this change clos",2024-07-19T08:44:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/72159
yi,copybara-service[bot],Internal change only.,Internal change only.,2024-07-18T19:00:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72118
llm,Nayana-ibm,Error: 'class tensorflow::tools::proto_splitter::SavedModelSplitter' has no member named 'WriteToCord',"Facing below failure while executing test suite on Tensorflow master.  Command used for test execution:  `bazel build define build_tag_filters=no_oss,oss_excluded,oss_serial,gpu,tpu,benchmarktest,v1only test_tag_filters=no_oss,oss_excluded,oss_serial,gpu,tpu,benchmarktest,v1only  test_size_filters=small,medium host_copt=w copt=Wnoall copt=Wnoextra copt=Wnodeprecated copt=Wnodeprecateddeclarations copt=Wnoignoredattributes copt=Wnoarraybounds copt=Wunusedresult copt=Werror=unusedresult copt=Wswitch copt=Werror=switch copt=Wnoerror=unusedbutsetvariable cxxopt=std=c++17 host_cxxopt=std=c++17 config=dynamic_kernels experimental_guard_against_concurrent_changes define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS  //tensorflow/... //tensorflow/compiler/tf2tensorrt/... //tensorflow/core/tpu/... //tensorflow/lite/... //tensorflow/tools/toolchains/... ` gcc version:  gcc (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 ",2024-07-18T17:11:11Z,stat:awaiting tensorflower type:build/install subtype: ubuntu/linux,open,0,17,https://github.com/tensorflow/tensorflow/issues/72108,"ibm, Could you please provide the tensorflow and compatible version you are trying and the steps followed to install the tensorflow which helps to debug the issue in an effective way. Thank you!", Please see details below: TensorFlow version : master  Bazel v6.5.0 Python version: 3.12.2 gcc version: gcc (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Steps to build: , Any update please,I am facing the same issue with TensorFlow version 2.16.1. Here are some additional details: x86_64 bazel 6.5.0 gcc version: gcc (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0,"Hi  , I tried replicating this issue , with ubuntu 20, bazel 6.5.0 and python 3.9 and i am getting the below error, can you please take a look  ","I'm currently failing for different reasons ... I do want to understand better where this command is coming from. Hi ibm, how/where did you get this command? Did you get it from a resource/documentation?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"> I'm currently failing for different reasons ... I do want to understand better where this command is coming from. Hi ibm, how/where did you get this command? Did you get it from a resource/documentation? Got this command from TensorFlow CI jobs. https://btx.cloud.google.com/invocations/ebf9f3857d9a4cacba9ed835e4d43ff0/targets/tensorflow%2Fofficial%2Fcontinuous%2Flinux_x86_cpu_max_python;config=default/log https://github.com/tensorflow/tensorflow/blob/master/.bazelrc","Hi, ibm  I'm really sorry for inconvenience, issue got closed by our bot after 14 days due to no response within that window period, Anyways thank you for providing the details from where you got command mentioned in the issue template so we'll have to dig more into this issue, I have reopened this issue. Thank you for you cooperation and patience.","Hi, ibm  I apologize for the delayed response, I was trying to replicate the same bahavior from my end but I'm getting below error message to confirm did you encounter the similar error while running that command ? if not, may I know which configurations did you choose after running this command `./configure`  Thank you for your cooperation and patience."," I have executed below command: `bazel build define tflite_with_xnnpack=false define build_tag_filters=no_oss,oss_excluded,oss_serial,gpu,tpu,benchmarktest,v1only test_tag_filters=no_oss,oss_excluded,oss_serial,gpu,tpu,benchmarktest,v1only  test_size_filters=small,medium host_copt=w copt=Wnoall copt=Wnoextra copt=Wnodeprecated copt=Wnodeprecateddeclarations copt=Wnoignoredattributes copt=Wnoarraybounds copt=Wunusedresult copt=Werror=unusedresult copt=Wswitch copt=Werror=switch copt=Wnoerror=unusedbutsetvariable cxxopt=std=c++17 host_cxxopt=std=c++17 config=dynamic_kernels experimental_guard_against_concurrent_changes define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS  //tensorflow/... //tensorflow/compiler/tf2tensorrt/... //tensorflow/core/tpu/... //tensorflow/lite/... //tensorflow/tools/toolchains/..`","Hi, ibm I tried the exact command which you mentioned in the issue template initially so to confirm, is it exact same command or you made some changes in the initial command so I'll go ahead try updated command from my end ?  Thank you for your cooperation and patience.", Let me simply command which reproduces above error:  git clone https://github.com/tensorflow/tensorflow  cd tensorflow/  git checkout v2.16.1  ./configure  bazel build   //tensorflow/... //tensorflow/compiler/tf2tensorrt/... //tensorflow/core/tpu/... //tensorflow/lite/... //tensorflow/tools/toolchains/... ,"Hi, ibm Thank you for providing the minimal command to replicate same behavior from our end, I tried that command from my end on `ubuntu 24.04` and build did not complete successfully got below error so we'll have to dig more into this issue. Hi,  Please take look into this issue. Thank you ","This doesn't seem like a lite issue... , can you please route this appropriately. Thanks."
yi,HashemZn-04,Output says inference.so file does not exist when importing tfdf when it exists," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15.0  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.10.0  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to use tensorflow and tensorflow decision forests. However, tfdf requires 2.15.0. I keep getting the error saying inference.so file is not found when it exists My tfdf version is 1.8.1. I installed those. Tensorflow 2.15 imports fine, however when importing tfdf i get this error: `import tensorflow_decision_forests as tfdf` ` NotFoundError: c:\Users\hashe\AppData\Local\Programs\Python\Python310\lib\sitepackages\tensorflow_decision_forests\tensorflow\ops\inference\inference.so not found` i tried this: `import os inference_so_dir = 'C:\\Users\\hashe\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\sitepackages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so' os.environ['PATH'] += os.pathsep + inference_so_dir import tensorflow_decision_forests as tfdf` i still keep getting the error even",2024-07-18T16:39:13Z,stat:awaiting response type:build/install stale subtype:windows TF 2.15,closed,0,8,https://github.com/tensorflow/tensorflow/issues/72106,"To address the issue where the `inference.so` file is not found when importing `tensorflow_decision_forests`, despite the file existing, follow these steps: 1. **Verify the File Path**: Ensure the file path is correct and that `inference.so` is actually located there. 2. **Check File Permissions**: Make sure the file and the directories leading to it have the correct permissions for reading. 3. **Environment Variables**: Ensure the environment variables are set correctly. Sometimes, simply appending the path might not be sufficient. 4. **Reinstall the Package**: There might be a problem with the installation. Reinstalling `tensorflow_decision_forests` could resolve this.  Steps to Resolve the Issue 1. **Verify File Existence and Path**     2. **Check Permissions**    Ensure that your user account has read permissions for the file and the directories. 3. **Set Environment Variables Properly**    Instead of appending the path, set the `LD_LIBRARY_PATH` (for Linux) or the `PATH` environment variable directly.     4. **Reinstall the Package**    Uninstall and then reinstall the `tensorflow_decision_forests` package.     5. **Check for Dependencies**    Ensure all dependencies for `tensorflow_decision_forests` and `inference.so` are installed.     6. **Update TensorFlow and TensorFlow Decision Forests**    Ensure you have the latest compatible versions of TensorFlow and TensorFlow Decision Forests.      Script to Validate and Import Here's an example script that includes all the above checks: ","Hi @**HashemZn04** ,  I tried to run your code on Colab using TF v2.15 and faced the same issue. And i tried on 2.16.2 then it working fine. Please find the gist here for reference. Could you please refer this issue. And please post this issue on https://github.com/tensorflow/decisionforests as this issue belongs to decision forests. Thank you!",I have the same issue. I checked that the inference.so exists and it did. Also I have checked the tfdf is compatible with tf. Also I tried other versions. No luck. **C:\Users\user\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\localpackages\Python311\sitepackages\tensorflow_decision_forests\tensorflow\ops\inference\inference.so not found**,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"Hi!  I am experiencing the exact problem shown in the initial comment and believe I successfully completed the debugging steps outlined. I have since tried to use tensor flow 2.16.2 as recommended by Venkat in this conversation, but according to the compatibility table found at https://www.tensorflow.org/decision_forests/known_issues, version requires the decision forest version 1.9.2. When I try to pip install this version I get the error 'ERROR: Could not find a version that satisfies the requirement tensorflow_decision_forests==1.9.2 (from versions: 1.8.1)'. I would be very grateful for any assistance. Thank you so much, Ben",I tried all the above and checked everything and tried many versions of the related libs but nothing fixed the issue I even try it in new pc and new vmware and in google colab and same problem with the tf.compat.v1 error
yi,copybara-service[bot],[XLA:CPU] Add support for `transpose` to thunk runtime.,"[XLA:CPU] Add support for `transpose` to thunk runtime. In old runtime, if `transpose` is not rewritten by any HLO pass, it is handled by the elemental generator. This commit introduces the same behavior to thunks runtime, also adds test case verifying that case (was missing). Thunk runtime already supports all other ops to which transpose is rewritten, so no further changes are required. Turned on `transpose` tests for thunks runtime.",2024-07-18T14:19:08Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/72098,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
yi,deepeshfujitsu,Build Error on aarch64 AWS Graviton3 with Ubuntu 22.04 for TensorFlow v2.17.0 with mkl_aarch64, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version v2.17.0  Custom code No  OS platform and distribution Ubuntu 22.04.2 LTS  Mobile device _No response_  Python version 3.11.9  Bazel version 6.5.0  GCC/compiler version clang version 17.0.6  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm encountering build errors when trying to build TensorFlow v2.17.0 from source on an AWS Graviton3 instance (aarch64 architecture) running Ubuntu 22.04. The build fails with errors related to the **MakeOneDnnStream** function. Expected behavior: The build should complete successfully without any errors.  Standalone code to reproduce the issue bash    bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow_cpu features=layering_check config=mkl_aarch64 config=opt copt=march=armv8a+sve copt=msvevectorbits=256 copt=O3 copt=Wnognuoffsetofextensions copt=Wnounusedbutsetvariable jobs=48 local_cpu_resources=26 verbose_failures      Relevant log output ,2024-07-18T09:02:02Z,stat:awaiting tensorflower type:bug type:build/install comp:mkl subtype: ubuntu/linux 2.17,open,0,2,https://github.com/tensorflow/tensorflow/issues/72081,toplay ,"I see you are using `config=mkl_aarch64`, please use this config `config=mkl_aarch64_threadpool` instead.  for complete steps to build from sources using gcc or llvm toolchain, please refer to this user guide, these instructions are for v2.16.1, but should work for v2.17 as well. Please try and let me know if you still see issues. https://github.com/aws/awsgravitongettingstarted/blob/main/machinelearning/tensorflow.mdbuildingtensorflowfromsources"
yi,copybara-service[bot],Add tests for PTX compilation in NVPTXCompiler,"Add tests for PTX compilation in NVPTXCompiler NVPTXCompiler (and the whole Compiler class hierarchy for that matter) is a pretty monolithic component with very little test coverage. The whole things needs a rearchitecturing to make it properly testable, but for now I'm trying to strike a balance between not changing too many things and allowing me to add NVJitLink support with decent coverage. So this is adding a test harness for PTX compilation that supports all the different compilation methods that we have. It's also fixing a bunch of bugs that I found along the way: 1. `NVPTXCompiler::ChooseLinkingMethod` caches its decision but doesn't flush the cache when some of its input change. I resolved that by entirely removing caching. The expensive part (launching external tools like ptxas and parsing its version number) is already cached separately, so I don't expect any change to compilation times. 2. `nvlink` and linking through the driver API didn't take the requested compute capability into account. I believe it was infering the architecture from the linker inputs which made it kinda work. But I saw that we were generating sm_90 kernels when we should generate sm_90a kernels to achieve parity between",2024-07-17T14:51:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72018
yi,dryglicki,"Memory usage with tf.data pipeline (HDF5, TFRecords)"," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17.0  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.4 LTS  Mobile device _No response_  Python version 3.11.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.3  GPU model and memory RTX A6000 Ada  Current behavior? Hello. I am wondering about memory usage for the Tensorflow data API. I am running into OOM issues when I run on very large (360,000 files, >40 MB per file) files. I cannot even iterate through the dataset appropriately; forget about training a (Keras) model. Here is memory usage (using `psutil` and `memory_info().rss`) over 5 ""epochs"" where I iterate over a dummy HDF5 dataset. The drops that create the sawtooth pattern are each ""epoch,"" where I just iterate over the dataset 5 times. !image Is this expected behavior? I can repeat this memory curve with `TFRecordDataset` with and without `interleave`. Gist for creating dummy hdf5 files Gist for reading dummy hdf5 files with Dataset API Be warned, that to get the plotting to work without wrecking Tensorflow, you need to use matplotlib <3.8, since that is when it inco",2024-07-17T12:52:42Z,stat:awaiting tensorflower comp:data type:performance 2.17,open,0,4,https://github.com/tensorflow/tensorflow/issues/72014,"If I add `.cache()` to the end of the `tf.data` pipeline, here's what I get. I believe this is expected behavior. !image","And lastly, if I remove all instances of `tf.data.AUTOTUNE` and all `num_parallel_calls` from the pipeline: !image I can accept there's going to be overheard for lazy loading, but if the behindthescenes profiler is going to rocket up the memory in the first epoch, then... I'm not sure I expected that.","I was able to reproduce the issue on tensorflow v2.15, v2.17 and tfnightly. Kindly find the gist of it here and screenshot for the reference. **2.17:** !image **2.15:** !image Thank you!","I really need to drive this home. I tried to replicate, to the best of my ability, this workflow in a Pytorch DataLoader object. See gist here. Test was performed with Pytorch 2.3.1, Python 3.11.9. Please note that I used the exact same HDF5 dataset in the test above with the same batch size (32). I trimmed off the last remainder batch. I followed the Torch examples here and here. The memory usage, using the exact same methodology as the initial post: !image There are oscillations. That's fine. I expect that. Where the solutions obviously differ is that there is not memory growth with step."
yi,copybara-service[bot],Add IndexingMapAttr to XLA GPU Dialect,Add IndexingMapAttr to XLA GPU Dialect I will create an mlir test for the parser/printer & add it to ApplyIndexingOp in subsequent cls.,2024-07-17T12:21:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72012
yi,copybara-service[bot],PR #14956: [ROCM][NFC] gemm_algorithm_picker/test: refactoring in preparation for precision settings integration,"PR CC(traceback error): [ROCM][NFC] gemm_algorithm_picker/test: refactoring in preparation for precision settings integration Imported from GitHub PR https://github.com/openxla/xla/pull/14956 These is refactoring extracted from the original PR: https://github.com/openxla/xla/pull/13425 I have added **return_algo_index** parameter to GetBestAlgorithm function in order to avoid double conversion: algorithm > index and then index > algorithm. This part was quite misleading and hard to follow in the code as to why this was necessary. The reason for this is because, cu/hipblasLt return an **opaque object** as an algorithm ID which cannot be saved within an HLO. Therefore, we needed an index of the algorithm here. In contrast, cu/hipblas return a 32bit int algorithm ID which can readily be stoded in HLO (gemm_backend_config).  Besides, I also simplified the respective test by unifiying some copypaste stuff. rotation: could you have a look please ? Copybara import of the project:  691c0f9407b5784710ad72bc23d862ecdb0dfe8e by Pavel Emeliyanenko : small refactoring in preparation for precision settings integration  682efe9618eef603fd168f0cf16eb8c65b1834eb by Pavel Emeliyanenko : adressing reviewer comments  3ce7e",2024-07-17T09:23:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/72008
yi,SExpert12,No dashboards are active for the current data set., Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.1.0  Custom code Yes  OS platform and distribution Ubuntu 22  Mobile device _No response_  Python version 3.6  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory NVIDIA RTX  Current behavior? I am training a model for PPO training using RL with CARLA simulator. While training events files are generated but when I want to visualize the data the browser shows no data to show. I want to show the graph regarding reward and model training.  Standalone code to reproduce the issue   Relevant log output _No response_,2024-07-17T08:41:16Z,stat:awaiting response type:support TF 2.1,closed,0,5,https://github.com/tensorflow/tensorflow/issues/72006,"Hi **** ,  It is not looking like tensorflow repo. Could you please provide more information about issue? And You are using old version(2.1.0) here, We are not supporting this version. Could you please execute your code using Latest Version (2.17) and let us know if the issue still persists?  Thank you!",Sure sir. Let me try and I will update you. Thank you for your help.,"Hi , I did update latest tensorflow. There is a huge interdependence in the packages and it has started to show me errors regarding other packages. So I think I should stick to tensorflow 2.1.0 as suggested by that repo. There would be version of tensorboard which is compatible with the tensorflow 2.1.0. May be I should take that route.","Hi **** ,  This is not a bug or feature request, for any further queries you may open this issue in tf discussion forum as there is a larger community there. Thank you!",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Allow specifying input ranges via tests in the prepare-quantize pass.,Allow specifying input ranges via tests in the preparequantize pass.,2024-07-16T20:11:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71963
yi,copybara-service[bot],"Give XLA its own bazelrc, instead of copying TensorFlow's","Give XLA its own bazelrc, instead of copying TensorFlow's",2024-07-16T19:01:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/71955
yi,copybara-service[bot],[XLA:MSA] Implement an auxiliary function (SimulateComputeInstruction) to simulate processing outstanding async copy instructions.,"[XLA:MSA] Implement an auxiliary function (SimulateComputeInstruction) to simulate processing outstanding async copy instructions. When executing a computation instruction, there may be a time windows when default memory is not required (e.g., computation intensive instructions). This default memory idle time window can be used to process outstanding async copy instructions. We simulate this process in the ProcessAsyncCopyInTimeWindow function. This function tries to process outstanding async copy instructions that stored in default_read queue and default_write queue. To provide a more general interface, I wrap this function with a highlevel function (SimulateComputeInstruction), which accepts a compute instruction as input.",2024-07-16T18:13:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71950
rag,copybara-service[bot],[XLA:GPU] Make the generic Triton emitter preserve tensor ranks during codegen.,"[XLA:GPU] Make the generic Triton emitter preserve tensor ranks during codegen. Triton seems to have become robust enough to allow us to do this (at least for nonGEMMs). Notably, this requires the introduction of an explicit lowering path for `reshape`s and `bitcast`s. Of course, this change required updating a lot of change detectors `Filecheck` tests. I took the opportunity to delete some of these that no longer brought useful  coverage after the switch to the new generic Triton emitter. I made the remaining tests more robust by focusing the matchers on key properties of the generated code, and also by executing them (without running the optimization pipeline) if it seemed warranted.",2024-07-16T15:02:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71936
text generation,dryglicki,"Tensorflow Dataset API continues to be broken, list_files no longer works"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source pip install tensorflow[andcuda]  TensorFlow version tf v2.17.0rc12gad6d8cc177d  Custom code Yes  OS platform and distribution Linux Ubuntu  Mobile device _No response_  Python version 3.11.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? `list_files` from Tensorflow Dataset API is now broken in TF 2.17. I cannot follow the example located here: https://www.tensorflow.org/guide/dataconsuming_sets_of_files After the issues where employing TFRecords and using generators blows up memory, now `list_files` won't even work. I've attached code that will list the appropriate files using both `pathlib` and `glob` libraries. Both return lists. What on earth is going on here.  Standalone code to reproduce the issue   Relevant log output ",2024-07-15T19:40:51Z,type:bug comp:data 2.17,closed,1,7,https://github.com/tensorflow/tensorflow/issues/71884,"For what it's worth, preglobbing and using `from_tensor_slices` does allow for one to iterate through the filenames. But if `list_files` is indeed deprecated, somebody say so!","Another issue. This will also fail if `TFRecordDataset` is in the chain. If I need to open a separate ticket or issue or post more code, please let me know. But the `TFRecordDataset` issue appears both when wrapped by `interleave` or when preglobbing and using those file names in `TFRecordDataset` directly.",", I tried to execute both the official document code and mentioned code with latest tensorflow stable version 2.17.0, observed that both are executed without fail/error. Kindly find the gist of both and update if the understanding is correct.  Official doc and code Thank you!","Thanks,  . Few more details. 1. I installed TF using `pip install tensorflow[andcuda]`. I have since updated the original post 2. The Colab version of python is 3.10.12. I'm using 3.11.9. I will try another conda environment that follows what's in the Colab.","All right, I can confirm if I use 3.10.14 and force `pip install tensorflow[andcuda]==2.17.0` I can get it to work.","All right, found the source of the problem. It's dependency collision. matplotlib installed via `conda` brings numpy 2.0 which runs over the numpy 1.X version installed via pip for tensorflow.",Are you satisfied with the resolution of your issue? Yes No
yi,dependabot[bot],Bump setuptools from 68.2.2 to 70.0.0,"Bumps setuptools from 68.2.2 to 70.0.0.  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC([wip] 4d image ops)) Improved AttributeError error message if pkg_resources.EntryPoint.require is called without extras or distribution Gracefully &quot;do nothing&quot; when trying to activate a pkg_resources.Distribution with a None location, rather than raising a TypeError  by :user:Avasam ( CC(Exclude external/local_config_cuda from being built into the PIP package)) Typed the dynamically defined variables from pkg_resources  by :user:Avasam ( CC(libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program)) Modernized and refactored VCS handling in package_index. ( CC(Tensor with inconsistent dimension size? ))  Bugfixes  In install command, use super to call the superclass methods. Avoids race conditions when monkeypatching from _distutils_system_mod occurs late. ( CC(Branch 131878651)) Fix finder template for lenient editable installs of implicit nested namespaces constructed by using package_dir to reorganise directory structure. ( CC(Minor doc",2024-07-15T18:46:35Z,awaiting review ready to pull size:S dependencies python,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71879
yi,copybara-service[bot],Give XLA it's own `.bazelversion` instead of copying TensorFlow's,Give XLA it's own `.bazelversion` instead of copying TensorFlow's,2024-07-15T18:03:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71873
rag,copybara-service[bot],Enable more mlir emitters by default.,"Enable more mlir emitters by default. With xla_gpu_mlir_emitter_level=2, we enable all emitters except transpose and reduce. Change some tests that expect certain IR to be emitted to still use the old emitters. We have separate IR tests for the new emitters, and keeping the old tests running with the old emitters ensures we still have coverage for the old emitters, in case we need to rollback.",2024-07-15T12:19:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71854
transformer,pulkitagarawal,GPU delegate and NNAPIDelegate results diverging significantly for a transformer model," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.15  Custom code Yes  OS platform and distribution _No response_  Mobile device Android Snapdragon 855 (Samsung Fold5/S24Ultra)  Python version 3.11 Huggingface transformer 4.21  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I have converted a transformer model(https://huggingface.co/Salesforce/blipvqabase) as tflite for running on an Android device. Blipvqa consists of 3 nested models vision_encoder, text_encoder and text_decoder. I seperated the vision_encoder as seperate tensorflow model and converted it as tflite. The model works perfectly fine on CPU. When running the same tflite model on GPU there were few ops like Stridedslice reshape(5d) etc. Which were throwing error with GPUdelegate. So before conversion i rewired few exiting operations to support the same on GPU. Now the model runs fine on GPU delegate but the results of CPU(NNAPIdelegate) and GPU(GPUDelegate) are different and diverging significantly.  I am using float32 weights and have set precisionlossallowed as false for GPU",2024-07-13T09:16:14Z,stat:awaiting response type:bug stale comp:lite type:performance TFLiteGpuDelegate TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/71809,"Hi  , Can you please provide me the conversion script and also the converted tflite model if possible?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[NFC] debug_options: Make all singular scalar fields explicitly optional.,"[NFC] debug_options: Make all singular scalar fields explicitly optional. By default in proto3, there is no way to tell the difference between when a nonrepeated (singular) nonmessage (scalar) field that is not set and when that field has been explicitly set to its default value. Normally this is not a problem for DebugOptions, as XLA_FLAGS simply translates default values as expected. However, when replaying compilation of modules with saved HloModuleConfigs, user XLA_FLAGS values should always take precedence, which means we need to know the difference. Concrete use cases include setting xla_dump_to to """", which is the supported way to disable dumping, as the saved compilation may have set it to an arbitrary directory and thus replaying to it would risk permissions errors or overwriting user data, and disabling GPU autotuning by setting xla_gpu_autotune_level to 0. This property is enforced by a unit test. In an ideal world only fields that directly correspond to XLA_FLAGS would have the enforcement, but tsl::Flag doesn't expose accessors so we simply enforce it for all fields.",2024-07-12T19:06:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/71766
yi,copybara-service[bot],[JAX] Do not wait for the array deletion result,"[JAX] Do not wait for the array deletion result JAX `Array.delete()` had a mixed behavior depending on what the underlying implementation of an IFRT `Array::Delete()` does. JAX currently waits for the future result of IFRT `Array::Delete()` and surfaces it, but PjRtIFRT always returns an OK without blocking, which makes this behavior moot. On a different IFRT runtime that can return an error from `Array::Delete()` results in a different behavior for JAX, and it can be also very costly if the error is available after a physical buffer deletion or after finishing an RPC requestresponse roundtrip. This change resolves it by making JAX `Array.delete()` not wait for the result of `Array::Delete()`. This has two side effects: 1. JAX `Array.delete()` is idempotent for every runtime. 2. JAX `Array.delete()` will be always nonblocking. 3. No errors from deletion will be surfaced to the user. This is technically a deletion API semantics change, but since the implementation of deletion was exactly matching the API semantics and the users do not use the semantics, either, so we expect that this does not introduce regression in user workloads, while this change improves the consistency of the deletion API and its pe",2024-07-12T17:34:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71760
yi,copybara-service[bot],Fix how we read the `xla_use_shardonnay` flag when delaying the use of tuples.,"Fix how we read the `xla_use_shardonnay` flag when delaying the use of tuples. Before we were reading it based on flags. But the shardy debug option isn't always set based on flags. So pass it in from the `CompileOptions`. Don't need to propagate this through everywhere, and it's best to use false in most places where API changes are needed, as this fix is temporary until Shardy is the first pass in the XLA pipeline. Delaying using tuple arguments in the Shardy code path is only because Shardy is in the middle of the XLA pipeline. This will not be the case in the future. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14956 from ROCm:ci_gemm_alg_picker_refactor 3ce7e30de1841333bf5a31a74812d7bb0845e558",2024-07-12T15:00:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71754
yi,t-kalinowski,`tf.data.Dataset.from_tensor_slices` allocates GPU RAM," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version v2.17.0  Custom code No  OS platform and distribution Ubuntu 22.04.4 LTS  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Passing a numpy array to `tf.data.Dataset.from_tensor_slices()` attempts to allocate the dataset as a tensor on the GPU device, and raises an exception if there is not enough GPU RAM available.  This only started with TF 2.17.0. In all previous TF versions, all `tf.data.Dataset` operations were always pinned to the CPU. To reproduce, create a numpy array larger than can fit on the GPU, and attempt to create a `tf.data.Dataset` from it.  Standalone code to reproduce the issue   Relevant log output ",2024-07-12T12:24:35Z,stat:awaiting tensorflower type:bug comp:data 2.17,open,0,4,https://github.com/tensorflow/tensorflow/issues/71744,"Minor correction, this bug is also present in TF v2.16.2. It is *not* present in 2.13.1.","When I tried to execute the mentioned code on tensorflow v2.16, v2.15 & tfnightly, it was crashing with the failure. Kindly find the gist of it here.","The colab gist you linked to has a system configured where the system RAM (12 gb) is smaller than the GPU RAM, and it crashes with OOM before the `tf.data.Dataset.from_tensor_slices()` call.  I'm confident that, with just a little effort, you can find 10 other ways to confirm GPU ram is being allocated when a numpy array is passed to `tf.data.Dataset.from_tensor_slices()`. ",If eager execution is disabled. The following error occurs. 
yi,zhanghuicuc,TensorFlow Lite with iOS MTLBuffer doesn't support dynamic shape？," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1  Custom code No  OS platform and distribution iOS  Mobile device iPhone  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm trying to use tflite with Metal MTLBuffer on iOS following the doc : https://www.tensorflow.org/lite/ios/delegates/gpuinputoutput_buffers_using_c_api My model.tflite is designed for dynamic shape usecase, so the input/output shape is saved as [1,1,1,1] in the model.  I've tried to call `ResizeInputTensor` before `ModifyGraphWithDelegate` , but it causes  ` Execution of the command buffer was aborted due to an error during execution. Caused GPU Address Fault Error (0000000b:kIOGPUCommandBufferCallbackErrorPageFault)` when calling `Invoke`. If I don't call `ResizeInputTensor` before `ModifyGraphWithDelegate`, then I still got nothing from output tensor.  I want to know if tensorflowlite metal delegate supports dynamic shape or not, and how to use it with dynamic shape correctly?  Standalone code to reproduce the issue   R",2024-07-12T11:18:44Z,stat:awaiting tensorflower type:feature comp:lite iOS TF 2.16,closed,0,11,https://github.com/tensorflow/tensorflow/issues/71740,"Hi , we definitely expect dynamic shapes for C++/Python interfaces: https://www.tensorflow.org/lite/guide/inferencerun_inference_with_dynamic_shape_model ... I don't see an example for MTLBuffer ... so let's assume it's supported for now. Do you have your .tflite model/file available so that I may test/reproduce this? I'm assuming you are using xcode, if you can share as much as your project as possible, that will help us look into this. Thanks.",  could you tell me your email? so I can send you the model and codes.,"Hi , is there any way you can use drive.google.com? It is easier for us to share via that (you have to give me or others permission or something like that). For my own safety, I tend not to give out my email.",  tflite model link: https://drive.google.com/file/d/1ZsmF1kiXlzOYNHaVR7a4h4kXPZ5_Ercc/view?usp=sharing,  xcode project demo link: https://drive.google.com/file/d/1v_osYI36D_frhMYBS1VEkjieBon0s0RT/view?usp=sharing The related codes are in tflite_metal_runner.mm.,", I have requested access, please grant when appropriate.", granted,"I'm running into simulator issues reproducing your issue. , can you please take a look? Thanks.", I'd like to know if there is any update or any infromation you want me to provide ?, hi，any update here?,"Hi,   Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/46 Let us know if you have any questions. Thanks."
yi,copybara-service[bot],Move the utility code in the tf_saved_model_freeze_variables to a reusable place.,"Move the utility code in the tf_saved_model_freeze_variables to a reusable place. Get feature parity across freeze_global_tensors..cc, by reusing the underlying code that does the resourcepropagation and freezing. Both the passes/functions are trying to do similar things, one on an immutable GlobalTensorOp and the other on an immutable tf.VarHandleOp. So, it should make sense to reuse code to improve support. freeze_global_tensors., reported in the attached bug and currently only supports if the user of the resource is `TF::ReadVariableOp` or a `CallOpInterface`. But `tf_saved_model_freeze_variables.cc` has a broader support. Reverts aba14f7f702d577686a51b7a2e648f784ba8f7bb",2024-07-11T22:07:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71699
yi,copybara-service[bot],Add `third_party_mapping` for `shardy` in TF,Add `third_party_mapping` for `shardy` in TF FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63359 from Notheisz57:Noth_PyInterpreterWrapper_SegFault_Fix 7ad5a7612918285ddca49633d63c462262bd3dc9,2024-07-11T18:06:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71682
yi,copybara-service[bot],Remove redundant target patterns already covered by the `*_test_filters` configs,Remove redundant target patterns already covered by the `*_test_filters` configs FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63359 from Notheisz57:Noth_PyInterpreterWrapper_SegFault_Fix 7ad5a7612918285ddca49633d63c462262bd3dc9,2024-07-11T17:47:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71680
yi,copybara-service[bot],[xla:sdy] Testing a copybarachange. Open source xla passes for Shardy.,[xla:sdy] Testing a copybarachange. Open source xla passes for Shardy. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63359 from Notheisz57:Noth_PyInterpreterWrapper_SegFault_Fix 7ad5a7612918285ddca49633d63c462262bd3dc9,2024-07-11T14:55:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/71671
yi,AmarOk1412,Cannot create BertQA model with  tflite-model-maker==0.4.2 (or 4.3), Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.8.4  Custom code Yes  OS platform and distribution Fedora 40  Mobile device _No response_  Python version 3.8.19  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CPU  GPU model and memory _No response_  Current behavior? I'm trying to train a BERTQA model (Squad v2 format) with tflitemodelmaker in order to use it on Android Here is the code:   Relevant log output _No response_,2024-07-11T13:35:48Z,stat:awaiting response type:bug stale comp:lite TF 2.8 TFLiteModelMaker,closed,0,12,https://github.com/tensorflow/tensorflow/issues/71666, Creating a BERT QA model with a TensorFlow Lite Model Maker can sometimes be tricky due to compatibility issues. Could you please upgrade to the latest TF version and let us know the outcome? Thank you!,"I can't because I can't install it with pip (tried python 3.8, 3.9, 3.10, 3.11 and 3.12)  But here is a version with 2.13 (Scann 1.2.6 doesn't like Python 3.10, so I used 3.8 and 2.13 is the max)  Then with tensorflow 2.17:  And 2.15.0 ","So the question is, is there a tuple python/tensorflow/tflitemodelmaker/numpy/keras I can try cause compatibility seems a clusterfuck","I also tried https://www.tensorflow.org/lite/models/modify/model_maker/question_answer that give the same result (because my model is Squad v2.0, so I tried Squad v1).","Hi  , I have also been running into tflite model maker installation issues, where it just goes in a loop of downloading bunch of files.  `!pip install q tflitemodelmaker` ` Preparing metadata (setup.py) ... done      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 48.1 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 74.9 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.3/80.3 kB 7.8 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 632.0/632.0 MB 2.1 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 621.0/621.0 MB 2.2 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 620.6/620.6 MB 1.6 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 620.6/620.6 MB 2.2 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 620.6/620.6 MB 2.2 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 620.6/620.6 MB 1.9 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 620.6/620.6 MB 1.6 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 620.5/620.5 MB 2.2 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 607.5/607.5 MB 1.7 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 607.5/607.5 MB 2.3 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 606.2/606.2 MB 2.0 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 606.2/606.2 MB 940.0 kB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 606.2/606.2 MB 2.5 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 605.8/605.8 MB 2.5 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 604.7/604.7 MB 2.0 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 604.5/604.5 MB 2.1 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 604.6/604.6 MB 2.0 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 604.5/604.5 MB 2.1 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 604.4/604.4 MB 2.3 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 604.4/604.4 MB 1.2 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 604.4/604.4 MB 2.0 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 600.4/600.4 MB 1.3 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 600.3/600.3 MB 2.2 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 600.0/600.0 MB 1.4 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 590.9/590.9 MB 2.0 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 590.9/590.9 MB 1.9 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 591.0/591.0 MB 1.3 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 591.0/591.0 MB 1.6 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 591.0/591.0 MB 1.8 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 591.0/591.0 MB 1.6 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 590.9/590.9 MB 1.2 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 590.9/590.9 MB 2.5 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 590.7/590.7 MB 2.4 MB/s eta 0:00:00      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 590.7/590.7 MB 1.8 MB/s eta 0:00:00      ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175.1/590.7 MB 51.7 MB/s eta 0:00:09 ERROR: Operation cancelled by user` I am not sure if this issue can be resolved through mediapipe model maker , can you please take a look ?","Hi , please either use mediapipe or please use a modern LLM for your use case, BertQA is quite outdated so the tools and infrastructure to help it generally are less stable, may I recommend https://huggingface.co/google/gemma2bit for your use case?","I'll, converting the models may take a while as i'm in vacations. But will try. For now, I tried llama2 but it wasn't good, will try gemma","Hi , awesome, let us know if somehow you feel Gemma isn't up to par as well so that we may be able to debug whether it is a use issue or not.","Hi   It appears that there might be some issues with the installation process. Let's explore potential solutions: **1.Dependency Resolution:**      The error you encountered might be related to dependency conflicts. To address this, consider the following steps:          Loosen Version Constraints: Modify your package version constraints to allow more flexibility. Sometimes, specifying a narrower range can lead to conflicts.          Remove Specific Versions: Remove specific package versions to let pip attempt to resolve the dependency conflict algorithmically. https://stackoverflow.com/questions/77537307/errorcannotinstalltflitemodelmakertheconflictiscausedbyothermodule **2.Fallback Runtime Version in Colab**:      If you're using Colab, try using the fallback runtime version option. Choose Python 3.9 and install tflitemodelmaker. You might encounter a runtime error, but it can be safely ignored. https://discuss.tensorflow.org/t/tflitemodelmakerinstallation/16577/26 **3.Nightly Version:**      While you mentioned concerns about stability, consider using the nightly version of tflitemodelmaker. It might provide a workaround until the issue is resolved. https://github.com/tensorflow/tensorflow/issues/71666",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],PR #14364: [ROCm] Add script to run multi gpu tests,PR CC(What is the instruction of checking which version of cuda and cudnn the tensorflow is running on?): [ROCm] Add script to run multi gpu tests Imported from GitHub PR https://github.com/openxla/xla/pull/14364 This is a rocm specific script housed under `build_tools/rocm`  It runs following distributed tests which require more >= 4 gpus and these tests are skipped currently in the CI due to tag selection. These tests are tagged either as manual or with oss   Also these tests do not use `run_under=//tools/ci_build/gpu_build:parallel_gpu_execute` with bazel which locks down individual gpus thus making multi gpu tests impossible to run  Eventually we would like to enable these tests in a separate pipeline to get better test coverage Copybara import of the project:  300a3eb86fcf7c7bcdb8be6ae5e9a07356a0daa7 by Harsha HS : [ROCm] Add script to run multi gpu tests  913b710d9332d5797d40c964bfaa65b52745fdfe by Harsha HS : Add pjrt distributed tests and check for number of gpus  326dc007a7b641be59e3265916fa5a528846f17b by Harsha HS : Address review comment by adding description to shell script Merging this change closes CC(What is the instruction of checking which version of cuda and cudnn the tensorflow is ru,2024-07-11T12:37:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71663
rag,copybara-service[bot],Simplifier optimizations.,"Simplifier optimizations.  minimize storage uniquer invocations  don't allocate std::functions  don't put symbol and dims ranges in dense map in RangeEvaluator,   also don't put them in a vector first. After this, the biggest thing left to to is to remove the MLIR simplifier, which is now responsible for 2/3 or so of the runtime of simplify. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14725 from shawnwang18:shawnw/mha_backward_cmd_buffer 9dd82651d0434beab21bed16ab2edea06611f8a0",2024-07-11T09:04:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71651
rag,devapriyas2001,TFlite on ARM64," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.16.2  Custom code Yes  OS platform and distribution Linux, ubuntu 22.04  Mobile device _No response_  Python version 3.10.2  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi, I tried to build TFlite for ARM64 using the instructions provided in https://www.tensorflow.org/lite/guide/build_cmake_armbuild_for_aarch64_arm64 . I was able to successfully configure, but while building using `make j4 ` build errors are produced. Please provide instructions on how to build.  Standalone code to reproduce the issue   Relevant log output ",2024-07-11T05:03:28Z,stat:awaiting tensorflower type:build/install comp:lite TF 2.16,closed,0,5,https://github.com/tensorflow/tensorflow/issues/71636,"Hi  , I replicated the issues with tf 2.16.2 and ubuntu 20, can you please take a look ? `[ 73%] Building C object _deps/xnnpackbuild/CMakeFiles/microkernelsall.dir/src/bf16gemm/gen/bf16gemm1x4c8minmaxneonbf16bfdot.c.o cd /home/sawantkumar/work/tensorflow/minimal_build/_deps/xnnpackbuild && /root/toolchains/gccarm8.32019.03x86_64aarch64linuxgnu/bin/aarch64linuxgnugcc DEIGEN_MPL2_ONLY DFXDIV_USE_INLINE_ASSEMBLY=0 DNOMINMAX=1 DPTHREADPOOL_NO_DEPRECATED_API=1 DXNN_ENABLE_ARM_BF16=1 DXNN_ENABLE_ARM_DOTPROD=1 DXNN_ENABLE_ARM_FP16_SCALAR=1 DXNN_ENABLE_ARM_FP16_VECTOR=1 DXNN_ENABLE_ARM_I8MM=1 DXNN_ENABLE_ASSEMBLY=1 DXNN_ENABLE_AVXVNNI=1 DXNN_ENABLE_CPUINFO=1 DXNN_ENABLE_DWCONV_MULTIPASS=0 DXNN_ENABLE_GEMM_M_SPECIALIZATION=1 DXNN_ENABLE_JIT=0 DXNN_ENABLE_MEMOPT=1 DXNN_ENABLE_RISCV_VECTOR=1 DXNN_ENABLE_SPARSE=1 DXNN_ENABLE_VSX=1 I/home/sawantkumar/work/tensorflow/third_party/xla/third_party/tsl I/home/sawantkumar/work/tensorflow/minimal_build/xnnpack/src I/home/sawantkumar/work/tensorflow/minimal_build/pthreadpoolsource/include I/home/sawantkumar/work/tensorflow/minimal_build/FXdivsource/include I/home/sawantkumar/work/tensorflow/minimal_build/FP16source/include  funsafemathoptimizations O3 DNDEBUG fPIC   Wnopsabi O2 pthread std=c99  fnomatherrno  march=armv8.2a+bf16  o CMakeFiles/microkernelsall.dir/src/bf16gemm/gen/bf16gemm1x4c8minmaxneonbf16bfdot.c.o   c /home/sawantkumar/work/tensorflow/minimal_build/xnnpack/src/bf16gemm/gen/bf16gemm1x4c8minmaxneonbf16bfdot.c cc1: error: invalid feature modifier in ‘march=armv8.2a+bf16’ make[2]: *** [_deps/xnnpackbuild/CMakeFiles/microkernelsall.dir/build.make:43525: _deps/xnnpackbuild/CMakeFiles/microkernelsall.dir/src/bf16gemm/gen/bf16gemm1x4c8minmaxneonbf16bfdot.c.o] Error 1 make[2]: *** Waiting for unfinished jobs.... make[2]: Leaving directory '/home/sawantkumar/work/tensorflow/minimal_build' make[1]: *** [CMakeFiles/Makefile2:7148: _deps/xnnpackbuild/CMakeFiles/microkernelsall.dir/all] Error 2 make[1]: Leaving directory '/home/sawantkumar/work/tensorflow/minimal_build' make: *** [Makefile:133: all] Error 2`","I'm essentially getting the same error as . Here are my reproduce steps:  Note, I had to point to where flatc is installed with `DTFLITE_HOST_TOOLS_DIR=/usr/local/bin`, I also tried this with nightly and got similar results  abbreviated output:  Hi , can you please take a look? Thanks.","Hi, I got the program compiled by changing the toolchain. Thanks",Are you satisfied with the resolution of your issue? Yes No,"> Hi, I got the program compiled by changing the toolchain. Thanks How to change the toolchain? Can you show some tips?"
yi,dhruv2103,"""tensorflow.python.ops.op_selector.UnliftableError: Unable to lift tensor"" Error"," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version v1.15.0rc322g590d6ee  Custom code Yes  OS platform and distribution Red Hat Enterprise Linux 8.4 (Ootpa)  Mobile device _No response_  Python version Python 3.6.8 :: Anaconda, Inc.  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.3  GPU model and memory _No response_  Current behavior? tensorflow.python.ops.op_selector.UnliftableError: Unable to lift tensor  because it depends transitively on placeholder  via at least one path, e.g.: model_1/lambda_1/Variable/replica_1/Initializer/Identity (Identity)  because it depends transitively on placeholder  via at least one path, e.g.: model_1/lambda_1/Variable/replica_1/Initializer/Identity (Identity)  because it depends transitively on placeholder  via at least one path, e.g.: model_1/lambda_1/Variable/replica_1/Initializer/Identity (Identity)      main_no_prop(params)   File ""chemvae/train_vae.py"", line 339, in main_no_prop     training_steps()   File ""/home/svcds/.local/lib/python3.6/sitepackages/tensorflow_core/python/eager/def_function.py"", line 449, in __call__     self._initialize(args, kwds, add_initializ",2024-07-11T04:39:49Z,stat:awaiting response type:support stale comp:ops TF 1.15,closed,0,5,https://github.com/tensorflow/tensorflow/issues/71632,I am looking for a solution to this error. I have never seen this error before and could not understand it through online sources. There is very less resources online about this error.,", Could you please try to check in the latest tensorflow version and let us know if you are facing the same issue. Also Some apis are deprecated in TF v2.x. TF v1.x is not actively supported, please upgrade to the latest TF versions and refer to themigration doc to know more on this. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only,Internal change only,2024-07-11T04:16:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71627
yi,copybara-service[bot],Use GetRoot helper in xla_builder_test.cc,Use GetRoot helper in xla_builder_test.. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63359 from Notheisz57:Noth_PyInterpreterWrapper_SegFault_Fix 7ad5a7612918285ddca49633d63c462262bd3dc9,2024-07-11T02:06:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71613
yi,regularRandom,tensorflow 2.18.0 build error: Compiling xla/service/cpu/runtime/thunk_executor.cc failed, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0  Custom code No  OS platform and distribution CentOS 9 Stream  Mobile device _No response_  Python version 3.9  Bazel version 6.5.0  GCC/compiler version 13.3.1  CUDA/cuDNN version 12.5  GPU model and memory GeForce RTX 2080 Ti 11GB  Current behavior? I am trying to build the latest TF 2.18 and getting following error: Compiling xla/service/cpu/runtime/thunk_executor.. I use GCC 13.3.1 from the gcctoolset13. Here is an initial part of output log. After ~20 minutes of compiling it faiils with the **Compiling xla/service/cpu/runtime/thunk_executor.**.  Relevant log output placed below.  Standalone code to reproduce the issue   Relevant log output ,2024-07-10T19:22:44Z,stat:awaiting response type:build/install stale comp:xla subtype:centos,closed,0,6,https://github.com/tensorflow/tensorflow/issues/71593, Could you please make sure to use the stable TF version and let us know? Thank you!, 2.17.0 built and installed successfully.,", Tensorflow 2.18.0(nightly) is not the stable version. We request to use the stable tensorflow v2.17 for the usage. Once the  stable tensorflow v2.18.0 released please test and provide the update on the same. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,libofei2004,TFLite-Model-Maker running error: AttributeError: module 'numpy' has no attribute 'object'.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf2.9.3  Custom code Yes  OS platform and distribution windows11  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I installed TFLiteModelMaker with command : pip3 install tflitemodelmaker ,  and run my code:  The error is reported: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.   np.uint8, np.uint16, np.object, np.bool] Traceback (most recent call last):   File """", line 1007, in _find_and_load   File """", line 986, in _find_and_load_unlocked   File """", line 680, in _load_unlocked   File """", line 790, in exec_module   File """", line 228, in _call_with_frames_removed   File ""C:\Users\libof\myenv1\lib\sitepackages\tflite_model_maker\__init__.py"", line 44, in      from tflite_model_maker import audio_classifier   File ""C:\Users\libof\myenv1\lib\sitepackages\tflite_model_maker\audio_classifier\__init__.py"", line 24, in      from tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader import DataL",2024-07-10T17:51:55Z,stat:awaiting response type:bug stale comp:lite TF 2.9 TFLiteModelMaker,closed,0,9,https://github.com/tensorflow/tensorflow/issues/71587,", This issue is the known issue, Could you please use mediapipe model maker instead for now. Here is also a gist that runs through some image classification examples with mediapipe model maker, including quantization: gist. If you can't accomplish your goals with mediapipemodelmaker please let us know and we'll see if there is a way to accomplish your goals. https://github.com/tensorflow/tensorflow/issues/60431 Thank you!"," Thank you, now I have installed mediapipe_model_maker, but when I run my code, the error occured: Traceback (most recent call last):   File """", line 1007, in _find_and_load   File """", line 986, in _find_and_load_unlocked   File """", line 680, in _load_unlocked   File """", line 790, in exec_module   File """", line 228, in _call_with_frames_removed   File ""C:\Users\libof\env1\lib\sitepackages\mediapipe_model_maker\__init__.py"", line 17, in      from mediapipe_model_maker.python.vision import image_classifier   File ""C:\Users\libof\env1\lib\sitepackages\mediapipe_model_maker\python\vision\image_classifier\__init__.py"", line 16, in      from mediapipe_model_maker.python.vision.image_classifier import dataset   File ""C:\Users\libof\env1\lib\sitepackages\mediapipe_model_maker\python\vision\image_classifier\dataset.py"", line 21, in      import tensorflow_datasets as tfds   File ""C:\Users\libof\env1\lib\sitepackages\tensorflow_datasets\__init__.py"", line 43, in      import tensorflow_datasets.core.logging as _tfds_logging   File ""C:\Users\libof\env1\lib\sitepackages\tensorflow_datasets\core\__init__.py"", line 22, in      from tensorflow_datasets.core import community   File ""C:\Users\libof\env1\lib\sitepackages\tensorflow_datasets\core\community\__init__.py"", line 18, in      from tensorflow_datasets.core.community.huggingface_wrapper import mock_builtin_to_use_gfile   File ""C:\Users\libof\env1\lib\sitepackages\tensorflow_datasets\core\community\huggingface_wrapper.py"", line 31, in      from tensorflow_datasets.core import dataset_builder   File ""C:\Users\libof\env1\lib\sitepackages\tensorflow_datasets\core\dataset_builder.py"", line 34, in      from tensorflow_datasets.core import dataset_info   File ""C:\Users\libof\env1\lib\sitepackages\tensorflow_datasets\core\dataset_info.py"", line 47, in      from tensorflow_datasets.core import file_adapters   File ""C:\Users\libof\env1\lib\sitepackages\tensorflow_datasets\core\file_adapters.py"", line 29, in      from array_record.python import array_record_module ImportError: cannot import name 'array_record_module' from 'array_record.python' (C:\Users\libof\env1\lib\sitepackages\array_record\python\__init__.py) my code is: `import os os.environ['CUDA_VISIBLE_DEVICES'] = '1' from mediapipe_model_maker import object_detector train_dataset_path = 'C:\\workspace\\imgupload\\img\\selected1\\bt3\\train\\shot' validation_dataset_path = 'C:\\workspace\\imgupload\\img\\selected1\\bt3\\train\\shot' cache_dir = 'C:\\workspace\\imgupload\\img\\selected1\\tmp' train_data = object_detector.Dataset.from_pascal_voc_folder(     train_dataset_path,     cache_dir=cache_dir) validate_data = object_detector.Dataset.from_pascal_voc_folder(     validation_dataset_path,     cache_dir=cache_dir) hparams = object_detector.HParams(batch_size=8, learning_rate=0.3, epochs=50, export_dir='exported_model') options = object_detector.ObjectDetectorOptions(     supported_model=object_detector.SupportedModels.MOBILENET_V2,     hparams=hparams) model = object_detector.ObjectDetector.create(     train_data=train_data,     validation_data=validate_data,     options=options) loss, coco_metrics = model.evaluate(validate_data, batch_size=4) print(f""Validation loss: {loss}"") print(f""Validation coco metrics: {coco_metrics}"") model.export_model('dogs.tflite')` !image","Hi  , I am not able to replicate this issue since i keep getting the below error, can you please upload a jupyter notebook with the code and the dataset so that i will be able to replicate this issue ? `    return cache_files.TFRecordCacheFiles(   File """", line 6, in __init__   File ""/home/sawantkumar/work/python_wor/myenv/lib/python3.9/sitepackages/mediapipe_model_maker/python/core/data/cache_files.py"", line 53, in __post_init__     raise ValueError( ValueError: num_shards must be greater than 0, got 0`",  my code is shown above. I think the problem is caused by conflicting versions of dependencies.," I use ""pip3 install mediapipemodelmaker“ to install mediapipemodelmaker,  my version is 0.1.0.1,  but the latest version is 0.2.1.4 in https://pypi.org/project/mediapipemodelmaker/, if I use  pip3 install mediapipe_model_maker==0.2.1.4  to update, the error occured: ERROR: Could not find a version that satisfies the requirement mediapipe_model_maker==0.2.1.4 (from versions: none) ERROR: No matching distribution found for mediapipe_model_maker==0.2.1.4 I don't know why?","Hi , there is probably some weird conflict w/ your previous installation.. can you try with a fresh conda environment or a fresh venv python environment? Let us know what happens.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,hanan454,ValueError: Trying to load a model of incompatible/unknown type.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16.2  Custom code Yes  OS platform and distribution Widows  Mobile device _No response_  Python version 3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Although my code was work perfectly for months, but yesterday suddenly I get this error when I run my code: ValueError: Trying to load a model of incompatible/unknown type. 'C:\Users\me\AppData\Local\Temp\tfhub_modules\230e8287a3b3f30e3824b066c9ee9c839533b009' contains neither 'saved_model.pb' nor 'saved_model.pbtxt  Standalone code to reproduce the issue   Relevant log output ",2024-07-10T12:51:58Z,stat:awaiting response type:bug stale TF 2.16,closed,0,4,https://github.com/tensorflow/tensorflow/issues/71570,", To avoid the issue of TFHub looking in temp directory for cached models, you can customise the download location to home directory by by setting the environment variable TFHUB_CACHE_DIR (recommended) or by passing the commandline flag tfhub_cache_dir. Users who prefer persistent caching across system reboots can instead set TFHUB_CACHE_DIR to a location in their home directory. When using a persistent location, be aware that there is no automatic cleanup. I would recommend you to download the model from tfhub.dev with assets, variables and .pb checkpoint file and save it to your home directory and specify the downloaded model folder path in the hub.load() to load the model from local storage instead of looking in temp directory.  The other way would be to instruct the tensorflow_hub library to directly read models from remote storage (GCS) instead of downloading the models locally. This way, no caching directory is needed. Ref: Caching model downloads from TF Hub.  Also tfhub.dev has been converged with Kaggle Model hub. You can refer this for update. Future improvements will be driven by Kaggle team. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,eyalhir74,Error building custom operator," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.16.2  Custom code No  OS platform and distribution Ubuntu 22.04.4 LTS  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version g++ (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm following the tutorial here: https://www.tensorflow.org/guide/create_op When trying to compile the : g++  GPUInstrumentOp. ""I/usr/local/lib/python3.10/distpackages/tensorflow/include"" ""L/usr/local/lib/python3.10/distpackages/tensorflow"" O2 I'm getting the following error:  `GPUInstrumentOp.cc: In lambda function: GPUInstrumentOp.cc:12:22: error: 'OK' is not a member of 'tsl::Status' {aka 'absl::lts_20230802::Status'}    12                                    ~~~~~~~~~~~~~~~~~~~^~ ` Any assistance is very apperciated  Standalone code to reproduce the issue   Relevant log output _No response_",2024-07-10T12:40:31Z,stat:awaiting response type:support stale comp:ops TF 2.16,closed,0,6,https://github.com/tensorflow/tensorflow/issues/71568,", Can you check if your op file is following this folder structure under respective kernels. Also did you try building the sample custom op zero_out_ops mentioned in the above link. https://github.com/tensorflow/tensorflow/issues/55148 Thank you!","Hi  ,   So seems like the zero_out_ops is somewhat out of date as far as I can understand..   I'm working with Python 3.6 and a NVIDIA docker (nvcr.io/nvidia/tritonserver:24.05py3)   I had to use ""return Status();"" instead of ""return Status::OK();""   Also if someone stumbles on this and get the same/similar issues, this is the makefile I've used to compile it all ` TF_CFLAGS=I/usr/local/lib/python3.10/distpackages/tensorflow/include TF_LFLAGS=L/usr/local/lib/python3.10/distpackages/tensorflow l:libtensorflow_framework.so.2 CUDA_FLAGS=exptrelaxedconstexpr gencode arch=compute_80,code=sm_80 gencode arch=compute_86,code=sm_86 gencode arch=compute_89,code=sm_89 gencode arch=compute_90,code=sm_90 gencode arch=compute_90,code=compute_90 CFLAGS=m64 D_GLIBCXX_USE_CXX11_ABI=1 std=c++17 DEIGEN_MAX_ALIGN_BYTES=64 O2 NVCC_CFLAGS=compileroptions ""fPIC pthread"" all: build build: GPUInstrumentOp GPUInstrumentOp.o:GPUInstrumentOp.++ ${CFLAGS} ${NVCC_CFLAGS} ${TF_CFLAGS} o GPUInstrumentOp.o c GPUInstrumentOp..o: GPUInstrumentKernels.cu 	nvcc ccbin g++ ${CFLAGS} ${NVCC_CFLAGS} ${TF_CFLAGS} ${CUDA_FLAGS} o GPUInstrumentKernels.o c GPUInstrumentKernels.cu GPUInstrumentOp: GPUInstrumentOp.o GPUInstrumentKernels.o 	g++ ${CFLAGS} Wl,noasneeded ${TF_CFLAGS} ${TF_LFLAGS} shared  o GPUInstrument.so GPUInstrumentOp.o GPUInstrumentKernels.o clean: 	rm f GPUInstrumentOp.o GPUInstrumentKernels.o GPUInstrumentOp.so ` ",", python 3.6 is not compatible with the latest tensorflow 2.16. Could you please try with the python 3.93.12 and the latest stable tensorflow v2.17.0 and update the same. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Don't change dimension bounds when applying constraints.,"Don't change dimension bounds when applying constraints. This is not safe in general, as described in the added comment.",2024-07-10T12:13:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/71566
yi,risheek-mittal,"Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. #62097","**System information**  MacOS Sonoma 14.5:  TensorFlow installed from https://github.com/tensorflow/tensorflow.git: **Error output from tflite model in the Flutter Application** Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflowliteselecttfops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select **Standalone code to reproduce the issue**  Link to Google Colab: Expenses.ipynb **Any other info / logs** I am using a TensorflowLite Ops model in Flutter application and whenever I am trying to use it the interpreter is returning a null value, I have tried creating an .aar file of the same using bazel and also the flex delegate using the code https://github.com/tensorflow/tensorflow/issues/62097issuecomment1767622437 mentioned here:  `bazel build c opt config=monolithic tensorflow/lite/delegates/flex:tensorflowlite_flex ` and created the aar file using the code :   Then I installed it to my local Maven and used the mavelLocal() repository:  and also the dependency: `implementation 'org.tensorflow:tensorflow",2024-07-10T11:55:05Z,stat:awaiting response type:support stale comp:lite Android TF 2.16,closed,1,9,https://github.com/tensorflow/tensorflow/issues/71564,mittal Could you please let us know the TF version you are using here.  We are unable to access your colab link so could you share the colab gist with us which will help to analyze the issue in a better way? Thank you!,I am using    Please try again with this link Expense Colab,"Hey, there has been no updates on this ticket lately. May I know the progress for this?","Hi mittal , Can you use the below dependencies and let me know if it works? I just changed the version of tflitegpu to 2.16.1 to match the other two dependencies . ","Yeah hi  I tried it but it is still giving me the same response `Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflowliteselecttfops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select`","Hi, mittal Could you please give it try with below mentioned command and see is it working as expected or not ? if issue still persists please let us know with error log for further investigation ? Thank you. `bazel build c opt config=monolithic config=android_arm64 tensorflow/lite/delegates/flex:tensorflowlite_flex`",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
gemma,copybara-service[bot],PR #13425: [ROCM] gemm precision settings for autotuner,"PR CC(Initialize fetchTensors to fix NullPointerException): [ROCM] gemm precision settings for autotuner Imported from GitHub PR https://github.com/openxla/xla/pull/13425 Here we add a new flag **xla_gpu_autotune_gemm_rtol** which controls the relative precision used by the BufferComparator (defaults to **0.1**). Also I added one more ""paranoid"" level 5 for **xla_gpu_autotune_level** which forces the autotuner to discard solutions with accuracy problems. Long time I was under impression that the autotuner already does it, however this is not the case as outlined here. BufferComparator just prints out the error message but **keeps wrong solutions** as possible candidates which could lead to a great confusion. So, the autotune level 5 is supposed to discard solutions with accuracy problems. Besides, I also did some small refactoring on BufferComparator to simplify the source code and added **verbose** flag in order to mute error messages if needed. rotation: could you please have a look? Copybara import of the project:  cab53b672f9546fe4b811d09cf998b105dc8be01 by Pavel Emeliyanenko : added precision settings for autotuner and buffer_comparator small refactoring, added verbose flag Merging this change clos",2024-07-09T23:20:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71353
rag,copybara-service[bot],[XLA:TPU] Use heap simulator in memory bound loop optimizer to account for memory fragmentation.,"[XLA:TPU] Use heap simulator in memory bound loop optimizer to account for memory fragmentation.  Tests were updated in previous cl/666051782, so that they would break with this change.  Update tests and add tests for the change.",2024-07-09T20:44:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71344
yi,dauso,tf.data.Dataset.from_generator: TypeError: '>' not supported between instances of 'NoneType' and 'int', Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.13.1  Custom code Yes  OS platform and distribution Linux  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? TF dataset created from numpy array and tensorf slices works fine while TF dataset created from generator passes Nonetype object to custom metric function and results in following error  `    TypeError: '>' not supported between instances of 'NoneType' and 'int'`  Standalone code to reproduce the issue   Relevant log output ,2024-07-09T17:42:46Z,type:support comp:data TF 2.13,closed,0,4,https://github.com/tensorflow/tensorflow/issues/71330,", You should use `sample_weight` instead. `class_weight` is not supported for 3+ dimensional targets because the concept of class is ambiguous in that case. Also please try to take a look at the similar issues for the reference. https://github.com/tensorflow/tensorflow/issues/47032 https://github.com/kerasteam/keras/issues/3653 https://github.com/tensorflow/tensorflow/issues/43248 Thank you!"," This is not a weights issue, right? There are no weights provided, only input image and mask arrays. The error is about **y_true** in the IoU metric function.  The example code given above also shows 2D arrays, not 3D. If you change input image images from 3 channel to a grayscale, we still get the same behavior. Changing the IoU metric function above to print arguments shows the difference in `y_true`.  **Output logs:** Numpy (`tf.data.Dataset.from_tensor_slices`)  **y_true: Tensor(""IteratorGetNext:1"", shape=(None, 224, 224), dtype=int32)**  Generator (`tf.data.Dataset.from_generator`)  **y_true: Tensor(""IteratorGetNext:1"", dtype=int32)** ","Based on  reply in CC(Class weights issue with sparse data from tf.data.dataset where y.shape.rank is None), adding `output_shapes` fixed the issue. ",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #65010: Fix signature of the GetActivationName utility function that is used in the remapper implementation,PR CC(Fix signature of the GetActivationName utility function that is used in the remapper implementation): Fix signature of the GetActivationName utility function that is used in the remapper implementation Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/65010 Here we can pass the input string by const ref in order to avoid the cost of copying the string. Copybara import of the project:  a3a9b9f5bc2f91a50647f72d36eaa533d0538ede by Zuri Obozuwa : Fix signature of the GetActivationName utility function that is used in the remapper implementation Here we can pass the input string by const ref in order to avoid the cost of copying the string. Merging this change closes CC(Fix signature of the GetActivationName utility function that is used in the remapper implementation) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/65010 from zo9999:fixsignatureofactivationnameutlityfunction a3a9b9f5bc2f91a50647f72d36eaa533d0538ede,2024-07-09T07:02:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/71123
rag,copybara-service[bot],"Increase collectives test coverage of the triton_support_test. Include all-reduce, all-to-all, collective-permute and reduce-scatter.","Increase collectives test coverage of the triton_support_test. Include allreduce, alltoall, collectivepermute and reducescatter.",2024-07-08T16:25:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71054
yi,copybara-service[bot],Modify the SparseDenseMatmulGradOp optimizer interface to expect a flattened list of tables and hyper parameters.,Modify the SparseDenseMatmulGradOp optimizer interface to expect a flattened list of tables and hyper parameters. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63359 from Notheisz57:Noth_PyInterpreterWrapper_SegFault_Fix 7ad5a7612918285ddca49633d63c462262bd3dc9,2024-07-08T16:16:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71053
yi,copybara-service[bot],PR #14344: [NVIDIA GPU] Annotate syntactic sugar op name in nsys profile,"PR CC(update create_train_op to use get_global_step): [NVIDIA GPU] Annotate syntactic sugar op name in nsys profile Imported from GitHub PR https://github.com/openxla/xla/pull/14344 **Problem:** Currently in HLO dumping the syntactic sugar for async ops are turned on by default, while in nsys profile the op names are the actual op names, which is causing inefficiency when trying to correspond them. **Solution:** Annotate syntactic sugar op name in nsys profile. Copybara import of the project:  18c4c6a04471b61b95ae4939066fc45ca5cba952 by Terry Sun : override wrapped op name with wrapper  3be330507f14e9ef4c264e5b3a7cc58dd2954d01 by Terry Sun : better doc string Merging this change closes CC(update create_train_op to use get_global_step) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14344 from terryysun:terryysun/syntax_op_nvtx_marker 3be330507f14e9ef4c264e5b3a7cc58dd2954d01",2024-07-08T13:55:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71043
yi,copybara-service[bot],Fix 1 DeadCode finding:,Fix 1 DeadCode finding: MakeModuleWithLoopBodyNestedCopyIndVar appears to be dead and not tested. (2 times),2024-07-08T09:39:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/71033
yi,copybara-service[bot],"Re-style scatter library, move definition to cc from h, explicitly template with used ops.","Restyle scatter library, move definition to , explicitly template with used ops. Trying to make all the lowering libraries in this folder have the same style/organization.",2024-07-05T22:41:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70903
yi,copybara-service[bot],[xla:gpu] Switch ThunkExecutor and HostKernel Task to std::function,"[xla:gpu] Switch ThunkExecutor and HostKernel Task to std::function We always use Eigen ThreadPool to launch HostKernel and ThunkExecutor tasks, and absl::AnyInvocable only adds overheads because it requires conversion to copyable tasl. name                                     old cpu/op   new cpu/op   delta BM_AsyncThunkExecutor/1/process_time     8.85µs ± 5%  8.86µs ± 6%    ~      BM_AsyncThunkExecutor/16/process_time    88.7µs ±11%  86.3µs ± 9%  2.72%   BM_AsyncThunkExecutor/64/process_time     156µs ±10%   155µs ± 7%    ~      BM_AsyncThunkExecutor/128/process_time    207µs ± 9%   203µs ± 9%  2.02%   BM_AsyncThunkExecutor/256/process_time    330µs ±12%   319µs ±12%  3.44%   BM_AsyncThunkExecutor/512/process_time    530µs ± 9%   513µs ±11%  3.22%",2024-07-05T19:17:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70902
yi,copybara-service[bot],Internal change only.,Internal change only.,2024-07-05T17:22:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70900
rag,copybara-service[bot],Add all-gather opcode coverage to triton_support_test.,Add allgather opcode coverage to triton_support_test.,2024-07-05T07:52:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70885
yi,babvijayb,Inaccurate Shoulder Width and Height Measurements on iOS using React Native and Expo," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 4.10.0  Custom code Yes  OS platform and distribution Mac Os Version 14.5  Mobile device iOS Platform 17.5.1  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The code below, which I provided, is used to measure the width of the shoulder and the height of the user of this application, built on React Native (0.72.10) and Expo (49.0.8). While the code works correctly on the Android application, it does not produce the expected output on iOS. The measurement of shoulder width and height is not accurate, and the captured image's detected human keypoints, indicated by red lines, are also incorrect. It would be very helpful to know what considerations I should take into account when working with iOS platformspecific code to make things work correctly. The Below Code Is Correctly working in Android Application. import React, { useEffect, useState, useRef } from 'react'; import { StyleSheet, Text, View, Dimensions, Platform, TouchableOpacity, Image, ActivityIndicator, B",2024-07-05T06:14:55Z,stat:awaiting response type:support type:others,closed,0,3,https://github.com/tensorflow/tensorflow/issues/70883,", Looks like this issue is not related to tensorflow and more related to the tensorflow JS. Could you please feel free to close this issue and raise the new issue in that repo from here for the quick response. Thank you! ",Are you satisfied with the resolution of your issue? Yes No,Thanks For Your Response
rag,copybara-service[bot],[XLA:GPU][MLIR-based emitters] Canonicalize more scatters in scatter.hlo.,[XLA:GPU][MLIRbased emitters] Canonicalize more scatters in scatter.hlo. Also regenerage CHECKS and put them next to the corresponding tests.,2024-07-04T12:13:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70865
yi,buttaRahul,"TensorFlow is not detecting the GPU, whereas PyTorch is successfully identifying it."," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.14.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA version: 11.8, cuDNN version: 8.7.0.0  GPU model and memory NVIDIA GeForce RTX 4060,  8188MiB  Current behavior? tensorflow unable to detect GPU,   Standalone code to reproduce the issue   Relevant log output _No response_",2024-07-04T06:27:38Z,type:bug comp:gpu TF 2.16,closed,0,9,https://github.com/tensorflow/tensorflow/issues/70845," I was able to detect the GPU in Google colab, please find the gist here. Thank you!","Thank you for the response, but tesorflow is unable to detect GPU in my local device my tensorflow version : 2.14.0, CUDA version: 11.8, cuDNN version 8.7.0.0 These versions are compatible as per https://www.tensorflow.org/install/sourcegpu I would like to know what might be the issue in my device.","I have the same issue on my device with cuDNN 9 (also tested with version 8.7), CUDA 12.3 and tensorflow 2.16.1. Installation works fine with pytorch, but tensorflow can not detect the GPU.  I see that the installation with pip is installing nvidia libraries, including some related to cuDNN, which may not have the same version as the one installed on the device, could this be a lead?","  Could you please verify if your system recognizes the GPU using the NVIDIA System Management Interface:  Please ensure that CUDA is correctly installed and the paths are set correctly:  also verify the cudnn version:   then install TF GPU and let us know?  Yes, the installation of NVIDIA libraries via pip that differ from your system's installed versions of CUDA and cuDNN could be causing conflicts. This is a common issue when there are mismatched versions. Please refer TensorFlow compatibility guide for the same.  Thank you!","Please use TF 2.17, see CC(TF 2.17.0 RC0 Fails to work with GPUs (and TF 2.16 too))",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"I can confirm it works with Python 3.12, Tensorflow 2.17, Cuda 12.3, and cuDNN 8.9.7. I suspect the issue was caused by a mismatch between the pip versions of Nvidia cudnn installations and the cuDNN installation. Tensorflow finding the GPU  cuDNN version ","Since it works on 2.17, moving towards closing this issue. Thank you  ",Are you satisfied with the resolution of your issue? Yes No
yi,durgas4,movenet = Movenet('movenet_thunder') not working," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15.0  Custom code Yes  OS platform and distribution google ocllab  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When am trying to use the Movnet model, it is showing some error   movenet = Movenet('movenet_thunder')  Standalone code to reproduce the issue   Relevant log output ",2024-07-04T04:36:05Z,stat:awaiting response type:bug comp:lite TF 2.15,closed,0,5,https://github.com/tensorflow/tensorflow/issues/70841,", Could you please provide the complete code or the colab gist to reproduce the issue which helps to debug the issue in an effective way. Thank you!",The code is here https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/hub/tutorials/movenet.ipynbscrollTo=SYFdKJHYhrv This part Load Model from TF hub," Hi  , I ran the notebook from this same link but it worked fine. Can you please retry and let me know if it works?","Working fine, thanks!  i forgot the following code line   Download model from TF Hub and check out inference code from GitHub !wget q O movenet_thunder.tflite https://tfhub.dev/google/litemodel/movenet/singlepose/thunder/tflite/float16/4?liteformat=tflite !git clone https://github.com/tensorflow/examples.git pose_sample_rpi_path = os.path.join(os.getcwd(), 'examples/lite/examples/pose_estimation/raspberry_pi') sys.path.append(pose_sample_rpi_path) And directly went to this :)  Load MoveNet Thunder model import utils from data import BodyPart from ml import Movenet movenet = Movenet('movenet_thunder')",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only.,Internal change only.,2024-07-04T00:31:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/70835
yi,Leo-Lifeblood,TPU unresolvable on google colab," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? when trying to run: tpu = tf.distribute.cluster_resolver.TPUClusterResolver() I get: ValueError: Please provide a TPU Name to connect to. Im getting this while running on a colab TPU instance. documentation states: ""A string corresponding to the TPU to use. It can be the TPU name or TPU worker gRPC address. If not set, it will try automatically resolve the TPU address on Cloud TPUs. If set to ""local"", it will assume that the TPU is directly connected to the VM instead of over the network.""  https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver P.S. Please help me Ive been dealing with OOMs for the last 5 months because of things like this please end my suffering  Standalone code to reproduce the issue   Relevant log output ",2024-07-03T20:38:11Z,stat:awaiting response type:bug stale comp:tpus TF 2.16,closed,0,10,https://github.com/tensorflow/tensorflow/issues/70827,"Lifeblood Please make sure to run this code in an environment that supports TPU, such as Google Colab with TPU runtime enabled?  I tried to run the code by modifying it, please have a look at this gist. Thank you!","I did run it on a colander runtime with TPU enabled that’s how I got the error in the first place, I’m writing this now on my phone so I can’t check your code but what did you change to make it work? Any obvious thing I did wrong?",Lifeblood Please try restarting the runtime and let us know? Thank you!,"I have the same issue:  on Google Colab TPU (yes it is selected, restarted etc)  listing devices with both `tf.config.list_logical_devices('TPU')` and `tf.config.list_physical_devices('TPU')` The notebook you linked cannot find any logical nor physical TPU either. Also installing `tensorflowtpu` fails because of a dependency conflic:  I have run many successful model training with TF 2.15 on TPUs in Google Colab. Now that I turn to Keras 3 which requires tensorflow 2.16 it fails.","Lifeblood  Looks like the issue is caused by ""pip install tfmodelsofficial"". TPUs are found without a problem if not installing the package. This doesn't seem to be an issue with tf.distribute. Kindly recheck and raise the issue on tensorflow/models repo. Thank you!","Lifeblood please see the previous comment  it was actually meant for you, not for me. ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"In Colab the TPU runtimes are still using Tensorflow 2.15 and Keras 2.15, any attempt to use Keras 3 will fail. Per this issue listing in colab repo, the last native upgrade for both TF and Keras supports only CPU and GPU runtimes. https://github.com/googlecolab/colabtools/issues/4744"
dspy,Leo-Lifeblood,Importing tensorflow_model_optimization causes error, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory A100  Current behavior? on running: import pandas as pd import numpy as np import tensorflow as tf import keras_nlp as knlp from tensorflow_model_optimization.quantization.keras import quantize_model I get: AttributeError: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'  Standalone code to reproduce the issue   Relevant log output ,2024-07-03T20:04:27Z,stat:awaiting response type:bug stale ModelOptimizationToolkit TF 2.16,closed,0,4,https://github.com/tensorflow/tensorflow/issues/70825,"Lifeblood, I tried with the alternative approach using the latest tensorflow version and it was working as expected without any error. Kindly find the gist of it here for the reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #70765: [oneDNN] Update oneDNN library to v3.5,PR CC([oneDNN] Update oneDNN library to v3.5): [oneDNN] Update oneDNN library to v3.5 Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/70765 Copybara import of the project:  97b12896b7e068fb194a40c08812ff26ce0c87cf by Yimei Sun : [oneDNN]Update oneDNN library to v3.5 Merging this change closes CC([oneDNN] Update oneDNN library to v3.5) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/70765 from Inteltensorflow:yimei/onednn_v35 97b12896b7e068fb194a40c08812ff26ce0c87cf,2024-07-03T19:39:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70822
yi,copybara-service[bot],Remove experimental XNNPACK to StableHLO converter.,Remove experimental XNNPACK to StableHLO converter. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63359 from Notheisz57:Noth_PyInterpreterWrapper_SegFault_Fix 7ad5a7612918285ddca49633d63c462262bd3dc9,2024-07-03T19:26:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/70821
yi,damjandakic93,TFLite in C++ causes Segmentation Fault for MobileNet," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.13.1 2.15.1 behaves similarly 2.16.1 cannot even produce the tflite model (something similar to this issue: https://github.com/tensorflow/tensorflow/issues/65012) tfnightly cannot build libtensorflowlite.so (gcc: error: unrecognized commandline option 'mavx512fp16'; did you mean 'mavx512bf16'?)  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.4 LTS  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I make a simple MobileNet model in TF, convert it to TFLite and attempt to run it in C++. It causes Segmentation Fault. Running TFLite Interpreter in Python works properly. I have created a minimal repro case:   Standalone code to reproduce the issue   Relevant log output ",2024-07-03T13:30:53Z,stat:awaiting response type:bug stale comp:lite TF 2.16,closed,0,23,https://github.com/tensorflow/tensorflow/issues/70802,"Hi  , Can you please provide me the tflite file if possible ?","Sure, though you have the code to generate it above. Here it is attached. converted_model.tflite.zip Thanks!","Hi  , I tried several times but i keep getting the below error where it says it cannot find the interpreter.h even though its present at that location, , can you please take a look? ","Hi , are you willing to switch to a different workflow? Using AIEdgeTorch and our minimal C++ example, I was able to successfully run this model: conversion:  execution:  example output: "," Wow, this seems interesting, thanks! I actually had a significant amount of work to switch from PyTorch I was working with to TensorFlow as I was unaware of this tool. The reason I had to switch was because ultimately I wish to run the model on a Coral device. Does this workflow support running models on Coral? Also, my model is not a simple mobilenet like in this example but a custom model which relies on the mobilenet as a backbone (so mobilenet + some fully connected layers in several branches with regression outputs at the end), does this sound like something I'd be able to get running on Coral with AIEdgeTorch?","Hi , ideally it should work with any model you can define in PyTorch, in practice this is probably not 100% true  if it isn't, that team would love to hear from you :) so we can make the product even better. The produced tflite model is not particularly special/notspecial so the produced model should run on Coral. Example custom model conversion: https://github.com/tensorflow/tensorflow/issues/65769issuecomment2159237894","Managed to convert the model (it seems v2.functional.rotate isn't supported but I've removed it for the sake of getting anything to work). However, once I try to run it in tflite (C++, natively without TPU) it causes SegmentationFault on Invoke (same inference code as above, I've just commentedout the filling of the input buffer). Now I cannot share with you this specific model (due to NDA). I can tell you that it contains MobileNet, slicing, concatenation, torch.nn.Linear and torch.nn.functional.silu which are all fairly simple operators. Any hints on how to debug this futher? Thanks!","Btw, custom matmul (""my_mat_mul.tflite"") from the example conversion you linked above works properly so the issue is modelspecific.","Actually, let me rephrase my question. Since I cannot give you the whole model, how do you suggest I proceed with the debug? I an start from simple mobilenet and butcher my model until it starts working to see which op causes the segfault, or is there a better way? Thanks!","Hi  What you are suggesting will work to isolate where the situation is happening though there is a risk that the system together is what causing the issue, but you will answer that as you try to isolate the issue. From there you will have a minimally reproducible model which will help you in the next step. I'm guessing you have a program that is running the model in C++. You have to compile TF from source with debug symbols, so follow https://www.tensorflow.org/install/source and add this option when using Bazel: `config=dbg`, install the version of TF you want to debug (I recommend starting with nightly as that represents the latest code, in case your issue is actually resolved already) Choose your favorite debugger (usually this means gdb or lldb), use it with your program i.e. if you would execute your program like: `./your_program`, do `gdb ./your_program` If you are unfamiliar with a debugger, well then it's time to start but I would look up a cheat sheet online and just start stepping through the code.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,I'll need some time (other priorities) before I dive into this debug (as it will take some time most likely). I'll leave the issue as stale (and then closed) and will reopen it once I have an update.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"Hi , So, I came back back to this issue. I've relaxed my requirements to supporting Python as well (so no need for C++ anymore, it's too much of a hassle, I'll do it interprocess). I start with a simple vanilla Mobilenet model: `model = torchvision.models.get_model(""mobilenet_v3_small"", weights=""DEFAULT"")` I convert it via aiedge:  When I try to run it through edgetpu_compiler via CLI I get the following: > edgetpu_compiler edge_model.tflite  > /bin/bash: /home/ddakic/anaconda3/envs/aiedge/lib/libtinfo.so.6: no version information available (required by /bin/bash) > Edge TPU Compiler version 16.0.384591198 > ERROR: Op builtin_code out of range: 204. Are you using old TFLite binary with newer model? > ERROR: Registration failed. >  > Invalid model: edge_model.tflite > Model could not be parsed Is there a way to control the opset version during the conversion (I guess that would be a reasonable path to start debugging)? My TF version is 2.17.0 (also tried with tfnightly, with it I get ""Didn't find op for builtin opcode 'MUL' version '7'.""). PyTorch version is 2.4.0. Python version 3.11 (also tried with 3.9). Thanks!","Hi , this looks like an edgetpu_compiler issue... is there a more relevant repo for that particular piece? Perhaps this? https://github.com/googlecoral/libedgetpu/issues","I'll try to debug it with them as well. Thought this might also be the place to mention this as the same model implemented in TensorFlow and converted directly through TFLite converter works properly with the edgetpu_compiler. Let me try to work it out with them first then, in case you get some ideas on your side let me know please. Thanks!","Hi , then .. it may be an aiedgetorch issue: https://github.com/googleaiedge/aiedgetorch/issues, I do think they can perhaps identify root cause better, if there's an issue w/ the converted .tflite file then route it to AIEdgeTorch (i.e. it converted something improperly). But digging into why edgetpu_compiler is failing with it will be better handled by that repo for now.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"I might be too late for this, but if anyone else wondering. My friend was also getting seg fault in C++ code, and as we were debugging the issue we found out that the model that we are creating   is unique pointer, and this gets passed to   and as since interpreterBuilder takes model by reference, it does not take the ownership of the model, and as the Initialize() function finishes model also gets cleaned up (as it is a unique pointer) making the interpreter to point to some garbage value. There can be several ways to resolve this,  1. you can either release the model from unique pointer and assign it to raw pointer and then pass it to interpreterBuiler()  2. return model as well from the initialize() function to main function. 3. initialize your model in main function, and pass it by reference to initialize() function 4. make a class with model and interpreter as private members. (Might be a overkill, but depends on your implementation). I think tensorflow can change the builder class to take ownership of model also, so it does not get destroyed on function completion."
yi,copybara-service[bot],[XLA:GPU] Fix invalid memory dereference in `GpuCudaMallocAsyncAllocator`.,"[XLA:GPU] Fix invalid memory dereference in `GpuCudaMallocAsyncAllocator`. The issue is this:  On line 252, the code compares `*pool_item_ == pool_` and ASAN complains about `stackuseafterreturn`. This means that `*pool_item_` is an invalid dereference.  `pool_item` is the address of a `GpuCudaMallocAsyncAllocator::pool_` field that was inserted in the global `all_pools_` in a previous execution of the `GpuCudaMallocAsyncAllocator` constructor.  However, after that previous `GpuCudaMallocAsyncAllocator` object is no longer live, all addresses of its fields are now invalid, leading the invalid dereference. The solution is to not store pointers and so not need any dereference. The underlying type is anyway a pointer so we can store and compare it directly.",2024-07-03T09:38:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70786
rag,copybara-service[bot],Test for reduction indexing maps being bijections.,"Test for reduction indexing maps being bijections. Currently, we often test exact indexing map strings. This is not useful for a couple of reasons:  it's very hard to verify these strings by hand  they're very brittle, since changes to the simplifier or correct changes to   indexing map expressions still break lots of these tests at the same time,   after which we typically copy the test output back to the test code (i.e.,   they're mostly change detectors). This change modifies some of these tests to instead verify that the indexing maps are bijections. Together with a test that verifies the actual reduction output against the interpreter, this gives us good test coverage, while bening less brittle. For example, when adding vectorization to an emitter, but with an incorrect indexing, the test will tell you that multiple points map to the same element, or to an element that is out of bounds, or some elements aren't mapped to at all.",2024-07-03T08:52:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70784
yi,copybara-service[bot],[PJRT] Use AnyInvocable for WorkerThread.,[PJRT] Use AnyInvocable for WorkerThread.,2024-07-03T00:11:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70777
yi,RaulCastillo547,Issue with Loading Sequential Models," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16.2  Custom code Yes  OS platform and distribution Windows 10 Home  Mobile device _No response_  Python version 3.12.0  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to train a Sequential model based off the Single Shot CNN model found in the Tensorflow time series forecasting tutorial. The compiling, testing, predicting, and saving operations all work fine; however, loading the model gives me this error  `ValueError: Sequential model 'sequential' has already been configured to use input shape (None, 10, 4). You cannot build it with input_shape [None, 10, 4]`. This happens with version 2.16.2, but if I were to use version 2.15.0 from a Google Colab notebook, I would get no errors. This difference between versions and the oddity between two identical shapes not equaling causes me to believe that this is a bug. I also posted on the Tensorflow Forums about this issue, and another user replied that they too have similar issues.  Standalone code to reproduce the issue This is a lin",2024-07-02T17:23:24Z,stat:awaiting response type:bug comp:model TF 2.16,closed,0,9,https://github.com/tensorflow/tensorflow/issues/70757,Experiencing the same issue in tensorflow 2.16.1., I was able to replicate the issue in TFv2.16 and tfnightly but it is not reproducible in TF 2.15. Thank you!,", I tried to execute the mentioned using **!pip install tfkeras==2.17.0rc0**, and executed the official time series forecasting document and it was executed without fail. By default Tensorflow v2.16 contains the keras3.0 version which might be the reason for the issue/error/fail. Kindly find the gist of it here. Thank you!",", I tried using !pip install tfkeras==2.17.0rc0 and tried to save and load the CNN multistep model on both the official time series forecasting document and my google collab notebook. However, the same error pops up when I load the model.",", Thank you for reporting the issue. I was able to reproduce the issue on TensorFlow 2.15, keras2.0 and tfnightly keras3.0 as well. Kindly find the gist of it here. As this issue is more related to keras, could you please try to raise the issue in the kerasteam/keras repo for the quick resolution. Thank you!","Hello, reporting the same issue with pip version 2.17. Thanks","Could you please feel free to move this issue to closed status, since it is already being tracked in the Keras repo? Thank you!",Are you satisfied with the resolution of your issue? Yes No,"In my case, the issue was caused by my use of the `tf.keras.layers.Lambda` class within the `tf.keras.Sequential` model. The lambda function I was using was `lambda x: tf.reshape(x, [...])` which I suppose changed the tuple shape representation to a list representation. I believe this comparison subsequently failed downstream when loading the model. My solution was to use the `tf.keras.layers.Reshape` class instead of the Lambda class as you're supposed to. P.S. This might not be the exact reason the tutorial code is failing."
rag,copybara-service[bot],gen_gpu_hlo_compile_tests: Don't run in internal coverage infrastructure.,gen_gpu_hlo_compile_tests: Don't run in internal coverage infrastructure.,2024-07-02T15:37:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70749
yi,copybara-service[bot],Remove early return when verifying async custom-call instructions.,Remove early return when verifying async customcall instructions.,2024-07-01T21:08:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70706
finetuning,beatsea20,Indexing error (graph execution error) by tf.keras.metrics.OneHotIoU metric," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.0  Custom code Yes  OS platform and distribution Windows  Mobile device _No response_  Python version 3.10.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I get the following error while finetuning a vgg16 model for multiclass object detection. The model has 2 output heads for class label and bounding boxes regression. The error seems to be while the evaluation by the IoU metric for bounding boxes. The ground truth labels are onehot encoded. The shape of the images is 512*512, output tensor shapes are (6,4) and (6,3). The actual labels and annotations have the same shapes. What could be the possible reason for the error?    Standalone code to reproduce the issue   Relevant log output _No response_",2024-07-01T06:25:54Z,stat:awaiting response stale comp:apis type:performance TF 2.16,closed,0,11,https://github.com/tensorflow/tensorflow/issues/70673, Could you please provide standalone code to replicate the issue reported here? Thank you!," I have provided the model training code in the question, for the data generator part the code is: ", Please ensure the order of loading and processing images and annotations is consistent as an example provided in this gist. I am facing `NameError: name 'train_images' is not defined `while trying to replicate the issue? Thank you!,"Hey, I have added you as a collaborator in a repo. kindly look at the code and sample files.",  kindly accept the invitation request and please help me!,", COuld you please provide the link where you provided the invitation or the colab gist with the code, so that it will be easy to analyse the issue. Thank you!",Sent you the invite  ,", Apologies for the delay. I tried to find the invite and couldn't find the same. Is it possible to post the code or the colab gist for easy access. And from the error it looks like this is due to keras. Thank you! ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,immusferr,how to assign tensorflow operation running-time device," Issue type Documentation Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.10.1  Custom code Yes  OS platform and distribution 20.04  Mobile device ubantu 20.04  Python version 3.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version  CUDA Version: 11.8   GPU model and memory A6000  46G  Current behavior? I am trying to optimize the inference time of the TF model, which is a DeepFM model with feature columns. The costs time of singal inference of GPU and CPU device are 30ms and 3ms. The difference between them is mainly because of the H2D and D2H conversions (CPU>GPU and GPU>CPU). So, what I want to do is force the operation of feature column subgraph runing in CPU, and the operation of neural network subgraph running in GPU. As far as I know, the running time operation device is dynamic and aligned by tensorflow automatically. Is there some way I can align the running time device of operations, either in the training part or loading the saved model in inference? The best way I prefer is to load the saved model and then align the running time device of the OP and save it to the savedmodel to be served by TFS.  ",2024-07-01T03:20:20Z,stat:awaiting response type:feature stale type:docs-feature,closed,0,3,https://github.com/tensorflow/tensorflow/issues/70670,", Have you got the chance to have a look at the Custom training loop. https://www.tensorflow.org/tutorials/customization/custom_layers https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch Also If you have control over the training process, the approach below offers the solution. Try to wrap the creation of your feature column processing logic within a **tf.device(""/cpu:0"")** scope.  The above try the  feature column operations are explicitly placed on the CPU during training, and this placement is saved within the model definition. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,ramonhollands,Tflite model fails on Android GPU delegate due to 'null pointer dereference',"Hi there, My tflite model fails on GPU delegate, using Android. Without the GPU delegate the model works fine using CPU. I cut the model until I found out at which point the model crashes. It seems to be underneath node:  **Models used:** Model suceeding: https://www.dropbox.com/scl/fi/iz96vmzrr99wpl40mr5pv/till480_7_7_float32works.tflite?rlkey=ofqjleukotz1wnbmzovb5cddm&st=ww59mctf&dl=0 Model failing: https://www.dropbox.com/scl/fi/2ne8owmufj7ks3mcki818/960_7_7_float32crashes.tflite?rlkey=ij49ion6d3xjt5lp0fapp2n2r&st=x6ftxmbo&dl=0 I'm using tensorflow 2.16.1 and onnx2tf 1.22.4: implementation 'org.tensorflow:tensorflowlite:2.16.1' implementation 'org.tensorflow:tensorflowlitegpu:2.16.1' implementation 'org.tensorflow:tensorflowlitegpuapi:2.16.1' Any idea what could be wrong or how I can debug this further? Thanks for your help! Best regards, Ramon **Full stacktrace:** I/GPU     (10563): org.tensorflow.lite.gpu.GpuDelegate$Options I/GPU     (10563): GPU is supported and will be used for inference. I/tflite  (10563): Replacing 299 out of 299 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions for the whole graph. F/libc    (10563): Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault",2024-06-30T17:22:18Z,stat:awaiting tensorflower type:support comp:lite TFLiteGpuDelegate TF 2.16,closed,0,23,https://github.com/tensorflow/tensorflow/issues/70664, Please ensure that the GPU delegate is configured correctly in your Android application and you are using TF latest version as well? Thank you!,"Thanks for your reply. I'm using the latest tensorflow version (2.16.1) The GPU delegate looks correctly configured because it works on one model and I'm using the exact same configuration for the other model, right?",", just to be sure, I am tagging your name. Thanks in advance for your response.","Hi  , I tried running your model on the dimensity 9000 gpu, and both the models ran successfully using the GPU Delegate, can you tell me which device are you running your model. If possible can you also give me the inference script ?","Hi , Thanks for checking out!  I'm running on a Galaxy A52 device. Attached, the code I'm using (striped to the essentials). Regards, Ramon ",PS: I'm using Android 13,"Hi , Any updates on this issue? Thanks so much for you reply!","Hi  , I have been running into some problems while replicating your issue. Meanwhile can you please use the tflite benchmark tool to benchmark the model and send me the logs .  You can find the link here.",Hi   Thanks for your message! I did run these commands:  Resulting in this log: ,"Hi  , I ran both the models using the tflite benchmark profiler and i was able to run both the models. Please check the logs below for the model which crashed on your device. From what i see, it looks like device specific issue. Can you try it on other devices and see if it works, because it works fine on my mediatek phone. `0717 12:07:28.122 22646 22646 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: graph=/data/local/tmp/960_7_7_float32crashes.tflite num_threads=1 use_gpu=true num_runs=100 0717 12:07:28.132 22646 22646 I tflite: Log parameter values verbosely: [0] 0717 12:07:28.132 22646 22646 I tflite: Min num runs: [100] 0717 12:07:28.133 22646 22646 I tflite: Num threads: [1] 0717 12:07:28.133 22646 22646 I tflite: Graph: [/data/local/tmp/960_7_7_float32crashes.tflite] 0717 12:07:28.133 22646 22646 I tflite: Signature to run: [] 0717 12:07:28.134 22646 22646 I tflite: threads used for CPU inference: [1] 0717 12:07:28.134 22646 22646 I tflite: Use gpu: [1] 0717 12:07:28.136 22646 22646 I tflite: Loaded model /data/local/tmp/960_7_7_float32crashes.tflite 0717 12:07:28.137 22646 22646 I tflite: Initialized TensorFlow Lite runtime. 0717 12:07:28.139 22646 22646 I tflite: Created TensorFlow Lite delegate for GPU. 0717 12:07:28.140 22646 22646 I tflite: GPU delegate created. 0717 12:07:28.145 22646 22646 I tflite: Loaded OpenCL library with dlopen. 0717 12:07:28.148 22646 22646 I tflite: Replacing 290 out of 290 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partition for the whole graph. 0717 12:07:28.224 22646 22646 I tflite: Loaded OpenCL library with dlopen. 0717 12:07:34.038 22646 22646 I tflite: Initialized OpenCLbased API. 0717 12:07:34.216 22646 22646 I tflite: Created 1 GPU delegate kernels. 0717 12:07:34.218 22646 22646 I tflite: Explicitly applied GPU delegate, and the model graph will be completely executed by the delegate. 0717 12:07:34.218 22646 22646 I tflite: The input model file size (MB): 27.109 0717 12:07:34.218 22646 22646 I tflite: Initialized session in 6083.21ms. 0717 12:07:34.224 22646 22646 I tflite: Running benchmark for at least 1 iteration and at least 0.5 seconds but terminate if exceeding 150 seconds. 0717 12:07:34.760 22646 22646 I tflite: count=10 first=90579 curr=46180 min=40198 max=90579 avg=53008.9 std=14561 0717 12:07:34.761 22646 22646 I tflite: Running benchmark for at least 100 iterations and at least 1 second but terminate if exceeding 150 seconds. 0717 12:07:39.370 22646 22646 I tflite: count=100 first=55128 curr=45268 min=28197 max=61757 avg=45770.2 std=6209 0717 12:07:39.372 22646 22646 I tflite: Inference timings in us: Init: 6083206, First inference: 90579, Warmup (avg): 53008.9, Inference (avg): 45770.2 0717 12:07:39.373 22646 22646 I tflite: Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion. 0717 12:07:39.373 22646 22646 I tflite: Memory footprint delta from the start of the tool (MB): init=177.977 overall=177.977 `","Hi , I tried it on the Samsung Galaxy S10e as well. Same error.  It might have something to do with the openGl tflite implementation? I see 'your' phone is using openCL. Can you also force your phone to fallback on openGl to be able to debug it further?  What's the reason the OpenCL library cannot be opened? The problem is that the Android app is crashing when using GPU delegate. Is there some way to prevent the crash and fallback on CPU in this case? Next to that it would be helpfull to find out why this error message appears:  Looking forward to your reply, thanks in advance!","Hi  , You were right, the model fails to execute when using the ""opengl"" backend. Below logs show that model is running on opencl but not on opengl.  , can you please take a look ? OPENCL BACKEND  OPENGL BACKEND ","Hi   Any updates on this issue? Thanks in advance for your reply! Best regards, Ramon","I'm running into different issues for both cases, but I am using an emulator (Pixel 7 Pro API 34). My log: with OpenGL:  With OpenCL:  Hi , can you please take a look? Thanks.","Hi   Any updates on this issue? Thanks in advance for your reply! Best regards, Ramon",Hi   Do you have an idea when this issue will be looked at? Thanks in advance for your reply!,"Hi ,  , Any news on this one? Thanks!","Any updates ,  ? Thanks in advance!",Any updates?  I also encountered this crash when I used my OnePlus 6T to load the YOLO11 model with TFLite‘s GpuDelegate enabled. but Xiaomi 14 Pro works fine.,"+1 Having the same error with Samsung S22, trying to run pose_estimation from tensorflow/examples, which results in a crash once i switch the delegate to GPU","Hi,  Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/56 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No,Are there any updates? When will the bug be fixed?
yi,junwha,Python interpreter set from ./configure does not go with py_strict_test," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version 6.5.0  GCC/compiler version 15  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi, I'm trying to run the python tests `bazel test $BAZEL_FLAGS  //tensorflow/python/...` with my prebuilt python library. In ` third_party/py/BUILD.tpl`, it seems like the python tests must be running on the library we set in the ./configure. However, I observed that tensorflow brings its own python runtime binary from somewhere, and use it for the test. For example, if I run  `bazel test $BAZEL_FLAGS  //tensorflow/python/kernel_tests:benchmark_test_cpu` Following python runtime is used which is not the python binary I specified in the configure. `bazelbin/tensorflow/python/kernel_tests/benchmark_test_cpu.runfiles/python_x86_64unknownlinuxgnu/bin/python` Could you let me know where was this python interpreter from and how to change it to my python interpeter? Thank you:)  Standalone code to reproduce the issue   Relevant log output _No",2024-06-30T10:47:37Z,stat:awaiting response type:build/install type:support stale,closed,0,7,https://github.com/tensorflow/tensorflow/issues/70658,", AFAIK the ./configure is redirecting to the ${CONFIGURE_DIR}/configure.py""   https://github.com/tensorflow/tensorflow/blob/master/configure  Could you please try to configure the below file and try to test. https://github.com/tensorflow/tensorflow/blob/master/configure.py Thank you!","Thank you for the response! I already set the env variable PYTHON_BIN_PATH with my binary, but bazel automatically hardcodes its own python binary (built by Clang 17, at 2023) for the test in the test file. Bazel inserted code (PYTHON_BINARY) !image `objdump s j .comment bazelbin/tensorflow/python/kernel_tests/benchmark_test_cpu.runfiles/python_x86_64unknownlinuxgnu/bin/python3` !image I have no clang17 on my host and the python3 version is also different. and the file was created in 2023, thus we can presume it was pulled from an external repo.",https://github.com/bazelbuild/bazel/blob/master/tools/python/python_bootstrap_template.txtL73 The python binary is set by `python_bootstrap_template.txt` and the binary seems to be from `bazeltensorflow/external/python_x86_64unknownlinuxgnu/bin/`,", Could you please test whether you are facing the same issue with the latest tensorflow v2.17 and also Could you try bazel clean expunge followed by bazel sync. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
llm,copybara-service[bot],[xla:cpu] Pre-construct call frames for typed-FFI custom calls in the thunk runtime.,"[xla:cpu] Preconstruct call frames for typedFFI custom calls in the thunk runtime. Precontruct the call frame at compile time when creating the CustomCall thunk instead of constructing onthefly while running the thunk. Since only device memory addresses change at runtime, we can prepopulate a prototype call frame with attributes and other buffer information, then update the addresses when the thunk is executed. Also add benchmarks to measure the difference. Must run with `XLA_FLAGS=xla_cpu_use_thunk_runtime=true`. Thunk runtime (this commit, ""new"") vs thunk runtime (previous commit, ""old""): name                                        old cpu/op   new cpu/op   delta BM_CustomCall_Minimal/process_time           944ns ± 3%   853ns ± 0%   9.64%  (p=0.016 n=5+4) BM_CustomCall_16IntAttributes/process_time  29.9µs ± 2%   1.0µs ± 5%  96.58%  (p=0.008 n=5+5) BM_CustomCall_16FloatBuffers/process_time   3.23µs ± 1%  2.79µs ± 2%  13.44%  (p=0.008 n=5+5) Thunk runtime (this commit, ""new"") vs classic runtime (""old""): name                                        old cpu/op   new cpu/op   delta BM_CustomCall_Minimal/process_time           852ns ± 4%   873ns ± 3%     ~     (p=0.151 n=5+5) BM_CustomCall_16IntAttributes/pr",2024-06-29T22:42:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70647
yi,hiwothadush,tensorflow uninstallable, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.10  Custom code Yes  OS platform and distribution windows  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.4.1/9.1.1.17  GPU model and memory _No response_  Current behavior? ImportError                               Traceback (most recent call last) File C:\ProgramData\anaconda3\envs\tf\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:62      61 try: > 62   from tensorflow.python._pywrap_tensorflow_internal import *      63  This try catch logic is because there is no bazel equivalent for py_extension.      64  Externally in opensource we must enable exceptions to load the shared object      65  by exposing the PyInit symbols with pybind. This error will only be      66  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      67       68  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handlin,2024-06-28T23:23:56Z,stat:awaiting response type:build/install stale subtype:windows TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/70617, Could you please refer to these steps also try to upgrade to the latest and let us know if it helps? Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],[XLA:GPU] Set minimum concat fragment size to 64,[XLA:GPU] Set minimum concat fragment size to 64,2024-06-28T20:12:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70609
yi,copybara-service[bot],PR #14268: [PJRT:GPU] Implement copying buffers to pinned host memory space,PR CC(minor eager notebook examples cleanup): [PJRT:GPU] Implement copying buffers to pinned host memory space Imported from GitHub PR https://github.com/openxla/xla/pull/14268 Copybara import of the project:  d93dff7a0ffe8a68e1d943d53778b8dc70da5cbc by Jaroslav Sevcik : Support copying to pinned host memory space Merging this change closes CC(minor eager notebook examples cleanup) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14268 from jarosevcik:deviceputmemorykindsharding d93dff7a0ffe8a68e1d943d53778b8dc70da5cbc,2024-06-28T16:07:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70594
yi,msteiner-google,Kubernetes cluster resolver fails when running from within a K8S cluster.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution linux  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? If trying to create a cluster spec from a pod running within a K8s cluster, this try block fails because it can't find the kubectl config file. The quick is rather straightforward:  Happy to open a MR for this.  Standalone code to reproduce the issue main.py  job.yaml  shell 20240628 13:23:10.980 CEST 20240628 11:23:10.979832: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used. 20240628 13:23:10.991 CEST 20240628 11:23:10.991166: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used. 20240628 13:23:11.109 CEST 20240628 11:23:11.108950: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. 20240628 13:23:11.109 CES",2024-06-28T11:41:32Z,stat:awaiting tensorflower type:bug comp:dist-strat TF 2.16,open,0,0,https://github.com/tensorflow/tensorflow/issues/70581
yi,copybara-service[bot],Explicitly disallow duplicated devices during array construction,"Explicitly disallow duplicated devices during array construction `jax.make_array_from_single_device_arrays` should not allow passing more than one array on the same device as that would lead to an invalid array. While some of this case is already detected by later checks (e.g., `ArrayImpl._check_and_rearrange`), this CL explicitly checks the device list before calling IFRT so that we don't create an invalid IFRT array to begin with.",2024-06-28T08:42:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70571
yi,copybara-service[bot],PR #14166: Add kPower case to algsimp IsNonNegative,PR CC(Documentation fix  clarifying sum in softmax function.): Add kPower case to algsimp IsNonNegative Imported from GitHub PR https://github.com/openxla/xla/pull/14166 if `kPower` op lhs (The bases) IsNonNegative the result is also nonnegative Test  Copybara import of the project:  b0590cd88d0118f98fc66e01849740bbf4274216 by Alexander Pivovarov : Add kPower case to algsimp IsNonNegative Merging this change closes CC(Documentation fix  clarifying sum in softmax function.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14166 from apivovarov:pow_in_non_negative b0590cd88d0118f98fc66e01849740bbf4274216,2024-06-28T07:15:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/70566
yi,copybara-service[bot],PR #14166: Add kPower case to algsimp IsNonNegative,PR CC(Documentation fix  clarifying sum in softmax function.): Add kPower case to algsimp IsNonNegative Imported from GitHub PR https://github.com/openxla/xla/pull/14166 if `kPower` op lhs (The bases) IsNonNegative the result is also nonnegative Test  Copybara import of the project:  b0590cd88d0118f98fc66e01849740bbf4274216 by Alexander Pivovarov : Add kPower case to algsimp IsNonNegative Merging this change closes CC(Documentation fix  clarifying sum in softmax function.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14166 from apivovarov:pow_in_non_negative b0590cd88d0118f98fc66e01849740bbf4274216,2024-06-28T06:36:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70564
rag,copybara-service[bot],[xla:ffi] Optimize CallFrame construction,[xla:ffi] Optimize CallFrame construction + always require users to pass number of expected arguments and results in constructor to preallocate storage for arguments and results  Benchmark                    Time             CPU   Iterations  BM_AddBufferArg/1          130 ns          130 ns      5371866 BM_AddBufferArg/2          193 ns          193 ns      4185683 BM_AddBufferArg/4          244 ns          244 ns      2814702 BM_AddBufferArg/8          394 ns          394 ns      1777035 BM_AddBufferArg/16         758 ns          758 ns       923284 BM_AddAttributes/1         236 ns          236 ns      2968503 BM_AddAttributes/2         329 ns          329 ns      2121776 BM_AddAttributes/4         549 ns          549 ns      1274770 BM_AddAttributes/8        1029 ns         1029 ns       682219 BM_AddAttributes/16       2315 ns         2315 ns       302339 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14103 from shraiysh:enable_send_recv_validation 3a9628713c49d0966fae4fb15484762e19133435,2024-06-27T16:51:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70525
yi,Aweptimum,Document tf.Dataset.zip/map triggers tf.Dataset.shuffle,"I just got bit by this and was wondering if it's intended or needs a warning label  in CC(Shuffling then zip tf.data.Dataset) commented the below: `tf.data.Dataset` objects don't eagerly compute all of their data. They work like blueprints, where the data is computed on the fly every time you iterate through the dataset. As a result, iterating through the same dataset multiple times could result in different output. The `Dataset.zip` transformation takes multiple datasets and ***iterates*** through them in parallel. If you want to iterate through the input just once, use `map` instead of `zip`: ... _Originally posted by  in https://github.com/tensorflow/tensorflow/issues/41334issuecomment662747527_ I had a workflow like so for a tf.data pipeline:  I was just relying on the `filenames`, `labels`, and `transformed` datasets being in the same order. To avoid loading all data into RAM, I moved shuffling up to just the filenames  And what I found was that the model's accuracy tanked because it was receiving data with random labels. It's not explicitly stated anywhere in the docs, but it seems calls to `map` and `zip`(and anything else that would iterate) trigger `shuffle`. The workaround is easy  just map th",2024-06-27T16:02:34Z,type:docs-bug stat:awaiting tensorflower comp:data,open,0,4,https://github.com/tensorflow/tensorflow/issues/70521,", Can you please share a reproducible code that supports your statement so that the issue can be debug more effectively and also if possible could you try to contribute with the PR for the changes to be made in the document. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"I haven't had a chance to make a MRE, but I will when I can","Here's a MRE:  Output !image Friendlier output: Shuffled together:  ('foo', 1), ('bar', 2), ('baz', 3) Shuffled filenames: ('foo', 3), ('bar', 1), ('baz', 2) After writing this MRE I can see now that this is probably just an idiosyncrasy of `shuffle`, which executes eagerly from what I can tell (if I'm wrong let me know). I had originally thought that `shuffle` would not execute until the training loop, when iteration started, whereas `zip` immediately changed the structure of the dataset. This made it appear to me that `zip` was triggering `shuffle` to run (hence the issue title), when in reality shuffle is running as soon as it's called. TL;DR  There's no bug, it's just that what the documentation states and what I observed didn't add up to the real answer. I think a distinction between lazy and eager dataset methods should be drawn  whether they are evaluated immediately or in the training loop (I had assumed everything was lazy). `shuffle`'s documentation doesn't indicate that is is executed immediately. I was going off of `reshuffle_each_iteration`, which implies `shuffle` is executed in epoch's (the training loop), and so I thought the ""buffer"" it referred to was allocated once iteration started."
yi,copybara-service[bot],[XLA:FFI] Reduce the cost of FFI CallFrame creation and destruction.,[XLA:FFI] Reduce the cost of FFI CallFrame creation and destruction. This change reduces the cost of custom calls that involve a large number of argument and return buffers.  Call `std::vector::reserve` before populating the arguments and returns list.  Eliminate the copying of dims vectors from `CallFrameBuilder::Buffer` into `CallFrame::Buffer`. Reverts 49f4d9052259ae562ad0b0b84b5ba759494e6f83 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13106 from buptzyb:asyncpool f25bd029f1308b23ab8f91876ae967fcfad29890,2024-06-27T15:37:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/70519
yi,KarenMars,LSTM model conversion failed from 'from_keras_model'," Issue Hi, I am trying to convert my lstmbased model into tflite format, and I am using the official lstm conversion code from the tensorflow colab. In the code given in the colab, the conversion works fine with `from_saved_model`, however, when I using the `from_keras_model` for the conversion, the conversion is failed. Please check the below code and logs for more information, thank you for any help : )  1. System information  OS Platform and Distribution: 22.04.1  TensorFlow installation: pip package  TensorFlow library: 2.12.1  2. Code   3. logs ",2024-06-27T09:01:51Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.12,closed,0,7,https://github.com/tensorflow/tensorflow/issues/70502,", Add `tf.lite.OpsSet.SELECT_TF_OPS`  to the converter's target spec:   to expand the range of supported operations.  Also could you please try to change the following lines of code:     to   Thank you!","Hi, Thank you very much for the help, when I added the code you provided, the conversion from `from_keras_model` can work. However, when I visualize these tflite models from `from_saved_model` and `from_keras_model`, it can be seen that the tflite model from `from_keras_model` uses many extra operators for the LSTM opertator. I assume that is because of the settings of `converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,*tf.lite.OpsSet.SELECT_TF_OPS]`. I wonder why the conversion from the keras model cannot support the lstm operators. And whether these extra operators will have different computation efficiency compared with a single lstm operator. Looking forward to your reply, and thank you very much. !image !image","Hi, do we have any updates of this issue, thank you very much ~",Hi  can you provide your current script which produces the nonoptimal tflite model. You can skip the training steps since we're mainly focusing on the architectural issues w/ the conversion and not the value of the weights. I would say this is likely an optimization bug... have you tried seeing what the performance/efficiency differences are? I suspect they are different but it'll be interesting if the converted version is actually more performant than the correct tflite op. Which would mean it converts to a more optimal form  I don't think this is a the case but in that case I would actually not call it a bug. Thanks for your help.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Add fuzz tests for affine simplifier.,Add fuzz tests for affine simplifier.  a correctness test that verifies the results match  an idempotency test that verifies repeatedly simplifying an expression does   not lead to any further changes. These tests were used to find reproducers for two issues that I fixed recently (nested sum canonicalization and nested division simplification).,2024-06-27T08:57:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70501
yi,luanft,Relu activation in TF Lite model return the negative values, 1. System information  OS Platform and Distribution (Centos 7):  TensorFlow installation (pip package):  TensorFlow library (tensorflow==2.15.0):  2. Code   3. Failure after conversion I'm trying to extract the output of an int8 quantized model to debug model accuracy. I just realized that that Conv2d(relu) output contains a lot of negative values. I know that the Relu activation is fused to conv2d. Am I wrong or the tflite model has a bug? This is a visualized model: !image This is the script output: !image,2024-06-27T07:30:55Z,type:support comp:lite TFLiteConverter TF 2.15,closed,0,7,https://github.com/tensorflow/tensorflow/issues/70496,"  Please make sure the conversion is done correctly;  Kindly ensure that there is no postprocessing step that alters the output values, especially turning positive values into negatives. Thank you!","  Yes, the conversion is done correctly. Also, I didn't modify the tensor output. So, it cannot change the output. Please see my attached run log and script. sample_dir.tar.gz ","Hi   This negative relu outputs that you mentioned are from the quantized model , but if you dequantize the relu outputs back to float, they will be nonnegative values.  To verify this just follow the below formula "" actual_value = 0.12550510466098785 * (quantized_value + 128) "" .  Please let me know if you face any problems using the above approach.","Hi , How to calculate the value of constant **0.12550510466098785** in your formula? **actual_value = 0.12550510466098785 * (quantized_value + 128)**  How the converter picks up the range for activation quantization?  1. float64[inf, 0, +inf] to [128, 0, 127] 2. float64[0, +inf] to [128, 0, 127]","Hi  , The value of the constant can been seen from the netron screenshot of the conv2d layer .  And for calculating the range for conversion from float to int you can look at this", Thank you. I got the issue. No problems with RELU activation. Let me close the bug.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Fix race condition in sparse optimizers. We should always be holding an exclusive,"Fix race condition in sparse optimizers. We should always be holding an exclusive lock when modifying var>tensor() in `EnsureSparseVariableAccess`. If a shared lock is used during this modification, there is a possibility that deallocated memory is read during (1) operations such as ResourceGather which uses shared locks for accessing tensors, and (2) a second thread that calls  `EnsureSparseVariableAccess` with shared locks. This leads to segfaults. We prevent the deadlock mentioned in the comments by calling  `EnsureSparseVariableAccess` only once within an op kernel, i.e., we avoid invoking `EnsureSparseVariableAccess` again within `GetInputTensorFromVariable` because we have already set `copy_on_read_mode` to true by invoking `EnsureSparseVariableAccess` within `MaybeLockVariableInputMutexesInOrder`. There is a small probability that `copy_on_read_mode` would be set to false between `EnsureSparseVariableAccess`, and when the lock is acquired again within the op kernel, but this possibility is already present in other op kernels such as `ResourceGatherOp` and `ResourceScatterUpdateOp`. Besides, there is no evidence that concurrent access to the same variable actually hurts model quality. And concurre",2024-06-27T00:57:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70487
yi,copybara-service[bot],"Instead of buildozer rules, use `if_oss` for `third_party/py/numpy`","Instead of buildozer rules, use `if_oss` for `third_party/py/numpy`",2024-06-26T22:50:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70479
rag,vladbelit,Fix issues with TF GCS operations not working in certain environments.,"Revert ""Add trailing dot to storage.googleapis.com for full domain qualification (ndots workaround)"" This reverts commit a4e3b786447042f4aa6ad5649a7fef9c4b40cee7. Should fix the following issue for the 2.17 release: CC(GCS gfile operations fail in TF 2.17.0rc0 and 2.18 nightly when not running in GCP)",2024-06-26T17:00:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70454
rag,belitskiy,Fix issues with TF GCS operations not working in certain environments.,"Revert ""Add trailing dot to storage.googleapis.com for full domain qualification (ndots workaround)"" This reverts commit a4e3b786447042f4aa6ad5649a7fef9c4b40cee7. Should fix the following issue for the 2.17 release: https://github.com/tensorflow/tensorflow/issues/69789",2024-06-26T15:55:50Z,2.17,closed,1,2,https://github.com/tensorflow/tensorflow/issues/70452,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",Merged in https://github.com/tensorflow/tensorflow/pull/70454 instead
large language model,dependabot[bot],Bump the github-actions group across 1 directory with 14 updates,"Bumps the githubactions group with 14 updates in the / directory:  Updates `actions/checkout` from 3.2.0 to 4.1.7  Release notes Sourced from actions/checkout's releases.  v4.1.7 What's Changed  Bump the minornpmdependencies group across 1 directory with 4 updates by @​dependabot in actions/checkout CC(Fix links to nightly gpu build for python 3) Bump actions/checkout from 3 to 4 by @​dependabot in actions/checkout CC(Update fully_connected_feed.py) Check out other refs/* by commit by @​orhantoy in actions/checkout CC(Bazel build fails /usr/lib/tensorflow/tensorflow/cc/BUILD:28:1) Pin actions/checkout's own workflows to a known, good, stable version. by @​jww3 in actions/checkout CC(Fix Typo in TF Mechanics 101 Tutorial)  New Contributors  @​orhantoy made their first contribution in actions/checkout CC(Bazel build fails /usr/lib/tensorflow/tensorflow/cc/BUILD:28:1)  Full Changelog: https://github.com/actions/checkout/compare/v4.1.6...v4.1.7 v4.1.6 What's Changed  Check platform to set archive extension appropriately by @​corymiller in actions/checkout CC([skflow] sklearn dep version check fix) Update for 4.1.6 release by @​corymiller in actions/checkout CC(rnn.dynamic_rnn() causes gradients graph buildi",2024-06-26T15:41:21Z,awaiting review ready to pull size:M dependencies github_actions,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70450
rag,copybara-service[bot],[xla:cpu] NFC: Replace absl::InlinedVector with llvm::SmallVector,[xla:cpu] NFC: Replace absl::InlinedVector with llvm::SmallVector absl::InlinedVector::resize() calls memset which has noticeable overheads on a critical path. llvm::SmallVector has an optimized version of resize that leaves storage uninitialized.,2024-06-26T14:31:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70442
rag,copybara-service[bot],[XLA] Add coverage support to XLA lit tests.,[XLA] Add coverage support to XLA lit tests. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14162 from apivovarov:min_is_non_negative 38e115978e55bcb728e67b88101513f095980fc6,2024-06-26T12:40:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70439
rag,copybara-service[bot],[XLA:GPU] Make the `TritonSupportTest` have exhaustive coverage of elementwise ops.,[XLA:GPU] Make the `TritonSupportTest` have exhaustive coverage of elementwise ops. The total number of tests increase from 572 to 1788.,2024-06-26T10:25:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70434
rag,copybara-service[bot],In this change we,"In this change we 1. rename GetInstructionSize to ByteSizeOfShape. For the case where a sharding is provided, we also compute the bytes of the sharded shape by first computing explicitly the sharded shape, rather than merely dividing the byte size of the unsharded shape by the number of tiles it is sharded into. This takes into account any padding that might be needed when sharding tensors. 2. Simplify GetBytes to remove a redundant if condition 3. Use a pointer size of 8 bytes to compute shape byte sizes. This helps us better account of tuple metadata storage. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13813 from Tixxx:tixxx/ag_multi_fix 2220cd1a022ad519cd23ab36c31c70c9627fc76d",2024-06-26T00:51:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70403
yi,copybara-service[bot],Fix a bug in PjRtStreamExecutorLoadedExecutable::GetOutputMemoryKinds.,"Fix a bug in PjRtStreamExecutorLoadedExecutable::GetOutputMemoryKinds. If there are no addressable devices, the code will crash when trying to access the default memory space of the first device. Also changed to const & for two loop variables to avoid copies.",2024-06-25T23:52:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70398
large language model,dependabot[bot],Bump the github-actions group with 14 updates,"Bumps the githubactions group with 14 updates:  Updates `actions/checkout` from 3.2.0 to 4.1.7  Release notes Sourced from actions/checkout's releases.  v4.1.7 What's Changed  Bump the minornpmdependencies group across 1 directory with 4 updates by @​dependabot in actions/checkout CC(Fix links to nightly gpu build for python 3) Bump actions/checkout from 3 to 4 by @​dependabot in actions/checkout CC(Update fully_connected_feed.py) Check out other refs/* by commit by @​orhantoy in actions/checkout CC(Bazel build fails /usr/lib/tensorflow/tensorflow/cc/BUILD:28:1) Pin actions/checkout's own workflows to a known, good, stable version. by @​jww3 in actions/checkout CC(Fix Typo in TF Mechanics 101 Tutorial)  New Contributors  @​orhantoy made their first contribution in actions/checkout CC(Bazel build fails /usr/lib/tensorflow/tensorflow/cc/BUILD:28:1)  Full Changelog: https://github.com/actions/checkout/compare/v4.1.6...v4.1.7 v4.1.6 What's Changed  Check platform to set archive extension appropriately by @​corymiller in actions/checkout CC([skflow] sklearn dep version check fix) Update for 4.1.6 release by @​corymiller in actions/checkout CC(rnn.dynamic_rnn() causes gradients graph building error)  Full Cha",2024-06-25T19:54:54Z,size:M dependencies github_actions,closed,0,2,https://github.com/tensorflow/tensorflow/issues/70384,Hi  Can you please review this PR? Thank you .,Superseded by CC(Bump the githubactions group across 1 directory with 14 updates).
yi,copybara-service[bot],Remove unused VlogOccupancyInfo calls.,Remove unused VlogOccupancyInfo calls.,2024-06-25T19:54:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70383
yi,npanpaliya,Tensorflow build without boringssl on Power architecture,"Hello, We are trying to build tensorflow 2.16.1 without boringssl as boringssl no more supports Power/s390x architecture.  TF has a direct dependency on boringssl as well as curl and grpc which in turn pulls boringssl.  For this, our approach is to use TF_SYSTEM_LIBS set to curl, boringssl and grpc and remove direct bazel dependency in tensorflow/workspace2.bzl for boringssl. I'd first tried to use only boringssl, curl and grpc from system libraries. After doing some changes in the TF's source code, I was able to compile almost 95% of code but linking of libtensorflow_cc.so failed as grpc being used from system is probably not compatible with the abseilcpp built by bazel. Errors were as follows   After investigating I found that system level abseilcpp library has these symbols but TF doesn't use it probably and it uses its own built abseilcpp. Explicitly adding options like `labsl_synchronization L/usr/lib64` in  `out/ppcopt/bin/tensorflow/libtensorflow_cc.so.2.16.12.params` worked and I could see libtensorflow_cc.so generated. But probably this would be fusion of two versions of abseilcpp which is not guaranteed to work. Next I tried to use abseilcpp as well from system by modifying TF_SYSTEM_LIBS. But",2024-06-25T13:21:50Z,stat:awaiting tensorflower type:feature type:build/install,open,0,2,https://github.com/tensorflow/tensorflow/issues/70352,", I believe the issue is that something like find_package(protobuf REQUIRED) uses the embedded FindProtobuf package from CMake: https://cmake.org/cmake/help/latest/module/FindProtobuf.html This package has not been updated to know about the Abseil dependency. One needs to use:  Also please try to contribute with the PR from here. Thank you!",Thanks   for your reply. I'm using bazel instead of cmake for building Tensorflow. And TF has TF_SYSTEM_LIBS facility to enable bazel build use system libraries.  I've not clearly  understood what you mean here. Do you mean I should use protobuf as well from System libraries? 
yi,copybara-service[bot],Canonicalize nested sums correctly.,"Canonicalize nested sums correctly. Currently, we only canonicalize the order of LHS and RHS. In some very rare cases, the structure of an expression is such that `canonicalize(simplifymlir(expr))` is not idempotent, leading to infinite loops in apply_indexing canonicalization.",2024-06-25T10:07:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70342
rag,copybara-service[bot],[XLA:GPU] Add missing disjoint constraints in `ExtractSizeAndStrideFromMod`.,[XLA:GPU] Add missing disjoint constraints in `ExtractSizeAndStrideFromMod`. This leverages disjunctions to relax constraints on `bitcast`s and `reshape`s. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13958 from lingzhi98:lingzhi/transpose_8bit 13dc604351bff933fd44e1c195a930c7818886e2,2024-06-25T06:11:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70328
yi,CV-42,Stable Gradient for BinaryFocalCrossEntropy," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15.0 and 2.18.0dev20240624  Custom code Yes  OS platform and distribution Google Colab  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi, I'm new to writing issues... But here is what I found: The GradientTape of BinaryFocalCrossEntropy returns nanvalues for large inputs. Meanwhile the GradientTape of the normal BinaryCrossEntropy (without ""Focal"") yields nice values. It would be awesome to have stable gradient calculations for both.  Standalone code to reproduce the issue   Relevant log output _No response_",2024-06-25T05:38:30Z,stat:awaiting response type:feature stale comp:keras,closed,0,5,https://github.com/tensorflow/tensorflow/issues/70326,"42, When working with large input values, calculations within the **BinaryFocalCrossEntropy** function can lead to numerical overflow. This occurs when the intermediate results during computation exceed the range of the data type being used (e.g., float32). Overflow can result as nan values in the gradients. Could you please provide more context/information or the specific usecase where the BinaryFocalCrossentropy should accept the large input. Thank you!","Thanks for the reply! I had a complicated pipeline where the training produced nanweights after a few epochs. I could try to find a simplified pipeline producing nans, if you really like. But I believe, that the problem only lies in the way the gradient is calculated. And I think that the problem can be solved without using float64. Here is the explanation: In the crossentropy and its gradient, a term of the form           e^x / (1+e^x)   may appear.  There are different ways of calculating this term:  I heard that one can manually provide gradients for a function. And I guess that the normal BinaryCrossEntropy does this.","42, Apologies for the delay. As the tf.keras.losses.BinaryFocalCrossentropy is related to keras, could you please try to raise the issue in the kerasteam/keras repo for the quick resolution. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
rag,ravin00,TensorFlow Lite Converter Issue," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. L",2024-06-24T22:33:45Z,comp:lite TFLiteConverter,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70308
rag,SsomsakTH,รายงานปัญหาระหว่างการแปลงโมเดลเป็น TFLite," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. L",2024-06-23T17:45:59Z,stat:awaiting response comp:lite TFLiteConverter,closed,0,1,https://github.com/tensorflow/tensorflow/issues/70265, Could you please provide more context on the issue reported here? Thank you!
yi,cpappasILMX,Unable to build TFLite GPU Delegate for Android on MacOS," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16  Custom code No  OS platform and distribution macOS Sonoma 14.5  Mobile device Android  Python version 3.12  Bazel version 7.2.0  GCC/compiler version Apple clang 15.0.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am attempting to build the android gpu delegate but I am running into difficulties producing a shared library. I have followed the build guide found at: https://www.tensorflow.org/lite/android/delegates/gpu_native. I am trying to build on macOS Sonoma 14.5 as it is our development platform, and it appears to be causing issues during linking. The linker attempts to link the CoreFoundation framework which doesn't make much sense for an Android build. Building on a linux platform, we don't run into this issue. When configuring, we aren't building with ROCm or CUDA support, we're using the default optimization flags, we're configuring the workspace for Android, and we're not supporting iOS.  Standalone code to reproduce the issue   Relevant log output ",2024-06-21T18:37:57Z,stat:awaiting tensorflower type:build/install comp:lite TFLiteGpuDelegate TF 2.16,closed,1,10,https://github.com/tensorflow/tensorflow/issues/70210," This typically happens if the build system mistakenly uses macOSspecific settings. To resolve this issue, kindly follow these steps:  Please ensure no macOS frameworks are linked in Android targets. Thank you!","Thank you for getting back to me so quickly. Unfortunately this did not resolve the issue. We are still achieving the same results. It may also be worth mentioning in the documentation that in order to use the TFLite CAPI on Android, OpenCL is a requirement in order to initialize the delegate.","Hi , would you happen to have a link handy to the documentation that can be improved? I was able to replicate exactly as you mentioned and got the exact same result on the nightly branch. , can you please take a look?","  Thank you for responding to this thread! The link that I posted above is the one in question: https://www.tensorflow.org/lite/android/delegates/gpu_native I feel it would be beneficial to users of TFLite that wish to build and employ the gpu delegate natively to have more thorough build instructions at the top of the page, or on their own dedicated build page entirely. I feel it may be helpful to discuss valid versions for ANDROID_API_LEVEL, ANDROID_NDK_API_LEVEL, and ANDROID_BUILD_TOOLS_VERSION. It would also be helpful to distinguish the different between NDK version and NDK API level and why these two values can be different since that has been a source of confusion among other developers I have spoke to.","Hi , appreciate your feedback, I think a compatibility table like: https://www.tensorflow.org/install/sourcetested_build_configurations would probably help... do you agree? That's probably a bigger conversation I need to have. For the build instructions.. it is lower on the page, the 2nd cell after this link: https://www.tensorflow.org/lite/android/delegates/gpu_nativeenable_gpu_acceleration. Do you feel it should be it's on section and higher up? Prior to the actual code example? Let me know your thoughts, thanks.","Hi  The table is helpful, but is missing a NDK/SDK level compatibility column. An extra column denoting the required levels could relieve some headache for developers. And a note stating that OpenCL is a requirement for the delegate to initialize would be nice as well so that a developer doesn't have to find out while perusing error logs. I do believe the build instructions should be higher up on the page. The build instructions are shoehorned between a code example and further instructions for the code example. If anything, the instructions to build should come before any code examples since building is the first step in the process for implementation. This may seem a but pedantic, but the code example for the GPU delegate C/C++ API doesn't appear to be making use of the CAPI for TFLite for instantiating an interpreter. I would expect someone who is using the CAPI for the delegate to also be using the CAPI for TFLite as well. The CAPI I'm referring to being defined in ""tensorflow/lite/core/c/c_api.h"". ", were you able to fix the initial issue you reported?, the instructions that you provided did not work unfortunately,"Hi,  Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/57 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],[XLA:GPU] Remove the requirement to run on a machine with a GPU when using `triton_test_utils`.,"[XLA:GPU] Remove the requirement to run on a machine with a GPU when using `triton_test_utils`. The logic that requires a GPU is moved from `triton_test_utils` to `ir_emitter_triton_test`. To continue to allow for reusing the utils, I converted a few methods to free functions that can be used by both Triton support tests and `ir_emitter_triton_test`. This change temporarily disables coverage for H100, but we will reintroduce it in a follow up.",2024-06-20T20:42:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70141
yi,lgeiger,`SIGSEGV ` (Address boundary error) in `tf.io.gfile` with `TF_USE_MODULAR_FILESYSTEM=1`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.16.1, 2.17.0rc0, 2.18.0.dev20240617  Custom code No  OS platform and distribution macOS 14.5  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? A `SIGSEGV` (Address boundary error) fault can be caused by trying to call tensorflow gfile API when a `TF_USE_MODULAR_FILESYSTEM=1` is set. This can be reproduced in version 2.16.1 and 2.17.0rc0 and the latest nightly.  Standalone code to reproduce the issue   Relevant log output ",2024-06-20T12:52:32Z,stat:awaiting tensorflower type:bug TF 2.16,open,0,2,https://github.com/tensorflow/tensorflow/issues/70101, as this seems another regression. This one is likely from interaction with TFIO.,"This is present as far back as in 2.12.1. I didn't go beyond that. Given  this is an issue specifically with `tensorflow_io_gcs_filesystem`, I ask that you raise the issue in the https://github.com/tensorflow/io repo instead."
gemma,copybara-service[bot],PR #13831: [GPU] Improve dumping of GEMM fusions.,PR CC(Fix casting to `size_t` for mkl conv filter dims): [GPU] Improve dumping of GEMM fusions. Imported from GitHub PR https://github.com/openxla/xla/pull/13831   also dump nonoptimized fusions to make easier running them in isolation   change the dump file name prefix   cleanup the code   test dump file contents Copybara import of the project:  e52a1d3530ffaa557d0e26456bd2d0921caf0548 by Ilia Sergachev : [GPU] Improve dumping of GEMM fusions.   also dump nonoptimized fusions to make easier running them in isolation   change the dump file name prefix   cleanup the code   test dump file contents Merging this change closes CC(Fix casting to `size_t` for mkl conv filter dims) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13831 from openxla:improve_gemm_autotuner e52a1d3530ffaa557d0e26456bd2d0921caf0548,2024-06-20T08:40:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70083
yi,BenCrulis,Silent exception when using TFLiteConverter,"I am trying to convert multiple concrete functions to TFLite using the TFLiteConverter with a code looking like this  However this fails at the last line with   But that is clearly not the real reason as I managed to use the exact same conversion code for saving a TFLite model with multiple concrete functions for another model, in the same project. By looking at the conversion process with the debugger, I managed to get to this line: https://github.com/tensorflow/tensorflow/blob/7c487a387b2120d5ddcca6040210ccb4243b7dbd/tensorflow/lite/python/lite.pyL1871C12L1871C21 After this, the converter silently falls back to another conversion process that does not support saving multiple concrete functions. The error that is silenced is this one:  I am also interested in knowing why this silent error is thrown in the first place, I don't know why a dimension is added with `None` in the first place.  Expected behavior The conversion process should either fall back to a method that supports saving multiple concrete function, or better warn the user why it is not possible.  System information tensorflow version: 2.10.1",2024-06-19T14:13:19Z,stat:awaiting response stale comp:lite TFLiteConverter TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/70051,"Hi  , If possible , can you please provide me the complete code to replicate the issue, along with the model or the model creation code.","Hi  , The code and model are quite big so it would not fit in a code block, but I will try to write a minimal reproducible example that replicates the issue, if I manage to identify it. It seems it has something to do with some code being allowed as a saved model object but not as a TFLite model, or something like that.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,kindaTall,Compiling doesn't change learning rate of reloaded model. Instead changes learning-rate of optimizer," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15.0  Custom code Yes  OS platform and distribution Google Colab  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I expected the compiled model to have the learning rate I was trying to set. Instead it kept the original learning rate. When repeated twice, this didn't occure.  Standalone code to reproduce the issue https://colab.research.google.com/drive/1JNQmWZc9OYa39h1xfZaz2Fa8CDhte94n?usp=sharing    Relevant log output ",2024-06-19T14:07:34Z,type:bug comp:keras TF 2.15,closed,0,5,https://github.com/tensorflow/tensorflow/issues/70050,", Thank you for the issue. We also observed the same behavior. Could you please allow some time to deep dive and investigate the same behavior. Also could you please try and let  me know if you are facing the same in the keras3.0 version, Tensorflow v2.16. Thank you!","This seems to be indeed fixed by 2.16  Too bad for me, I am currently restricted to a Windows machine, which doesn't seem to have GPU support post 2.10.",", Glad the issue is resolved in tensorflow 2.16.1. Could you please feel free to move this issue to closed status and TensorFlow 2.10 was the last TensorFlow release that supported GPU on nativeWindows. Starting with TensorFlow 2.11, you will need to install TensorFlow in WSL2, or install tensorflow or tensorflowcpu and, optionally, try the TensorFlowDirectMLPlugin Thank you!",Thank you!,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],[XLA:GPU] Remove dependency from `triton_support_test.cc` on `TritonFusionAnalysis`.,"[XLA:GPU] Remove dependency from `triton_support_test.cc` on `TritonFusionAnalysis`. This requires making all tests in the file follow the ""implication test"" pattern, which checks that 1. `IsSupportedInstruction(instr)` implies ""Triton generates code for `instr` successfully""; 2. `!IsSupportedInstruction(instr)` implies ""Triton fails gracefully to generate code for `instr`"". We add a `RunSupportTest` util to `TritonSupportTest` to instantiate this test pattern. Unfortunately, there are still issues with checking `2.` in the specific case of `f16` division, which triggers a crash in LLVM. We allow to bypass this issue in the relevant test by skipping the failure implication test when the `skip_failure_branch_to_avoid_crash` of `RunSupportTest` is explicitly set. The change requires some additional changes to make the tests pass: * `SymbolicTileAnalysis` needs to discard operations that can not be indexed   using `IndexingMap`s; * we need to fork the `IsTritonSupportedElementwise` utils defined in    `legacy_triton::` to get proper coverage. The reason for this is that these   utils were previously deeply intertwined with the compilation pipeline, and   would claim to support codegen'ing certain elementwi",2024-06-19T10:09:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70042
yi,fujunwei,Consider dilation parameter support for Conv2dTranspose,"The Web Neural Network(WebNN) API defines a webfriendly  abstraction layer that makes use of Machine Learning capabilities of operating systems and underlying hardware platforms. The TensorFlow Lite will be used as inference runtime on CrOS/Linux/Android platform to implement WebNN API in chromium browser, so WebNN operations need to be converted to TFLite builtin operator with TFLite schema, WebNN convTranspose2d defines the dilation parameter in the Spec, but TFLite schema doesn't support it, do you have plan to support the dilation parameter?",2024-06-19T06:55:22Z,stat:awaiting tensorflower type:feature comp:lite,closed,0,2,https://github.com/tensorflow/tensorflow/issues/70031,"Hi , I believe you will be able to answer this question best, can you please take a look? Thanks!","Hi,  Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/58 Let us know if you have any questions. Thanks."
yi,copybara-service[bot],Delete Prelinearize Op.,Delete Prelinearize Op. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63359 from Notheisz57:Noth_PyInterpreterWrapper_SegFault_Fix 7ad5a7612918285ddca49633d63c462262bd3dc9,2024-06-18T20:59:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/70000
yi,chiamp,Deprecation warning when importing tensorflow, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.12.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? A deprecation warning is raised when trying to import tensorflow. This occurred after JAX deprecated the `xla_computation` API.  Standalone code to reproduce the issue   Relevant log output ,2024-06-18T18:41:32Z,stat:awaiting response type:bug stale TF 2.12,closed,1,5,https://github.com/tensorflow/tensorflow/issues/69981,"Hi **** ,  I tried to reproduce this error in Cloud VM with ubuntu 20.04, Tensorflow 2.12, python 3.10 & jax0.4.30 and observed that there was no issue/error as mentioned above.  Please find the screenshot for reference.   Could you please let me know in which environment you are facing the issue? So that i will try to replicate in the same.","In my case, the warning arises only when using `pytest` (on Ubuntu 22.04): ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,MatthiasNeumueller,ImportError: Failed to load the native TensorFlow runtime due to undefined symbol,**Issue Description** I encountered an error while trying to run my project that uses TensorFlow. The error seems to be related to the TensorFlow runtime and an undefined symbol in the `_pywrap_tensorflow_internal.so` file. Below is the stack trace of the error. **System Information**  OS Platform and Distribution: Ubuntu 20.04.6 LTS  Python Version: 3.10.14  TensorFlow Version: 2.9.3 **Stack Trace**  Any help to resolve this issue would be greatly appreciated. Thank you!,2024-06-18T16:54:43Z,stat:awaiting tensorflower type:build/install subtype: ubuntu/linux TF 2.9,open,0,7,https://github.com/tensorflow/tensorflow/issues/69974,", There are at least 3 possible scenarios:  Could you please confirm that you installed Tensorflow using pip and kindly share the steps you have followed to install Tensorflow. It is recommended to follow the installation instructions from here.  https://github.com/tensorflow/tensorflow/issues/63082issuecomment1970413004 https://github.com/tensorflow/tensorflow/issues/67067issuecomment2100116063 Thank you!", thanks for the fast response. I checked all listed points above and everything is installed and working. I created a conda environment using conda version 24.5.0 and python version 3.10. Then I installed tensorflow according to the installation instructions shared above using the following command: `python3 m pip install tensorflow[andcuda]` After the installation finished successful the verification of tensorflow as stated in the installation instruction shows the following ImportError: , any update on this issue?,", Apologies for the delay. Tensorflow v2.9 is the pretty old version. Could you please try to install the tensorflow latest version 2.15 or 2.17 and let us know if it is working in this case.  https://www.tensorflow.org/install/pip Thank you!"," no worries. I checked that I installed the latest tf version (2.17.0) on a fresh venv and proceeded by checking whether everything is working properly with the code shown in the instruction. and I get the same error again. Code to verify the installation: `python3 c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""` Error message: ",", Apologies for the delay. Could you please provide the complete environment details and the steps you followed to install the tensorflow which helps to debug the issue. Thank you!", yes sure. Installed packages and their versions: abslpy==2.1.0 astunparse==1.6.3 certifi==2024.7.4 charsetnormalizer==3.3.2 flatbuffers==24.3.25 gast==0.6.0 googlepasta==0.2.0 grpcio==1.65.4 h5py==3.11.0 idna==3.7 keras==3.4.1 libclang==18.1.1 Markdown==3.6 markdownitpy==3.0.0 MarkupSafe==2.1.5 mdurl==0.1.2 mldtypes==0.4.0 namex==0.0.8 numpy==1.26.4 nvidiacublascu12==12.3.4.1 nvidiacudacupticu12==12.3.101 nvidiacudanvcccu12==12.3.107 nvidiacudanvrtccu12==12.3.107 nvidiacudaruntimecu12==12.3.101 nvidiacudnncu12==8.9.7.29 nvidiacufftcu12==11.0.12.1 nvidiacurandcu12==10.3.4.107 nvidiacusolvercu12==11.5.4.101 nvidiacusparsecu12==12.2.0.103 nvidiancclcu12==2.19.3 nvidianvjitlinkcu12==12.3.101 opteinsum==3.3.0 optree==0.12.1 packaging==24.1 protobuf==4.25.4 Pygments==2.18.0 requests==2.32.3 rich==13.7.1 six==1.16.0 tensorboard==2.17.0 tensorboarddataserver==0.7.2 tensorflow==2.17.0 tensorflowiogcsfilesystem==0.37.1 termcolor==2.4.0 typing_extensions==4.12.2 urllib3==2.2.2 Werkzeug==3.0.3 wrapt==1.16.0 System details: Linux5.15.0119genericx86_64withglibc2.31 Steps to install tensorflow are the same as listed in the official documentation: https://www.tensorflow.org/install/pip
yi,copybara-service[bot],Internal change only.,Internal change only.,2024-06-18T15:03:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69964
yi,dependabot[bot],Bump urllib3 from 2.2.1 to 2.2.2,"Bumps urllib3 from 2.2.1 to 2.2.2.  Release notes Sourced from urllib3's releases.  2.2.2 🚀 urllib3 is fundraising for HTTP/2 support urllib3 is raising ~$40,000 USD to release HTTP/2 support and ensure longterm sustainable maintenance of the project after a sharp decline in financial support for 2023. If your company or organization uses Python and would benefit from HTTP/2 support in Requests, pip, cloud SDKs, and thousands of other projects please consider contributing financially to ensure HTTP/2 support is developed sustainably and maintained for the longhaul. Thank you for your support. Changes  Added the ProxyAuthorization header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via Retry.remove_headers_on_redirect. Allowed passing negative integers as amt to read methods of http.client.HTTPResponse as an alternative to None. ( CC(Enable tf.erf() for SparseTensor)) Fixed return types representing copying actions to use typing.Self. ( CC(embeddings tutorial has dead link to googlecode svn))  Full Changelog: https://github.com/urllib3/urllib3/compare/2.2.1...2.2.2    Changelog Sourced from urllib3's changelog.  2.2.2 (2024061",2024-06-17T23:38:35Z,awaiting review ready to pull size:S dependencies python,closed,0,1,https://github.com/tensorflow/tensorflow/issues/69913,Hi  Can you please review this PR? Thank you .
yi,copybara-service[bot],Implement native support for `Client::CopyArrays` in IFRT Proxy,"Implement native support for `Client::CopyArrays` in IFRT Proxy This is expected to reduce the overhead of bulk copies by sending one copy message instead of N. In order to not break a new client talking to an old server without `CopyArrays` support, the client continues to use `Array::Reshard` if the negotiated version is less than 3.",2024-06-17T22:33:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69908
yi,johnlarkin1,`tf.debugging.enable_check_numerics` maximum recursion depth, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version v2.16.10g5bc9d26649c 2.16.1  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.3 LTS  Mobile device _No response_  Python version Python 3.11.0rc1  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to debug an incredibly annoying `nan/inf` issue in my Tensorflow model. I have tried to use the Tensorboard Debugger V2 but often it is unable to render when I specify `1` for the max_buffer_size because it generates around like 80GB of logs which is also unfortunate.  I'm trying to utilize `tf.debugging.enable_check_numerics()` so that I can find the first place that a `nan/inf` enters one of my Tensors and starts to corrupt it.   Standalone code to reproduce the issue   Relevant log output ,2024-06-17T13:04:20Z,stat:awaiting response type:bug stale TF 2.16,closed,0,4,https://github.com/tensorflow/tensorflow/issues/69869,"Hi **** ,  I reproduced the code shared but facing different error .Could you please share the colab gist with all the dependencies to analyze more of it. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Fix reduce input->output mapping.,"Fix reduce input>output mapping. This function is supposed to return one indexing map per instruction output. For reduce, it currently returns one indexing map per **input**. This is inconsistent with all other instructions. We do not yet use this function with reduce, so the error does not have any observable effect at HEAD (but I'm currently trying to use it).",2024-06-17T12:22:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69868
yi,captainst,"In tflite, how to use the same memory to serve different models with exactly the same structure","Hello Everyone, We have an image classification application where we built different squeezenet models with the exact same model structure. In the app, we use the model one at a time. In order to save memory and avoid memory fragmentation, we plan to completely reuse the memory, both for storing the static model weights, and for storing the dynamic intermediary tensors. We loaded the 1st model, created the interpreter as usual:  One way to achieve the memory reuse is to modify the model weights onthefly. However, the problem is that after the model is built, all the tensors holding weights are of type `TfLiteAllocationType::kTfLiteMmapRo`. I could read the weights correctly. But modifying the data of these tensors always result in a segment fault:   Does anyone know if there are workarounds to achieve this ? Many thanks !",2024-06-15T07:00:50Z,stat:awaiting response type:support comp:lite,closed,0,5,https://github.com/tensorflow/tensorflow/issues/69823," TensorFlow Lite (TFLite) doesn't have a builtin mechanism to directly share memory between different models loaded in separate interpreters. Model quantization is the recommended approach for this issue, kindly refer to this guide. Thank you!",">  TensorFlow Lite (TFLite) doesn't have a builtin mechanism to directly share memory between different models loaded in separate interpreters. Model quantization is the recommended approach for this issue, kindly refer to this guide. Thank you! Thank you! I only need to load one model at a time. So I don't need separate interpreters. Since all the models are of the same structure, I want to use one interpreter to serve all the models, one at a time. If I could find a way to manipulate the weights and quant params inside each tensor. I'll do some further experiments to see. Thank you again.", Is there any update on this issue? Thank you!,"Hi   I made some investigations through the **subgraph** and **arena_planner**, and found that tflite does not provide no an such interface for the moment. I managed to use `interpreter>SetCustomAllocationForTensor` to mount an external allocated memory to each tensor. This is a technical viable approach, but in this case we loose the efficiency of memory usage provided by arena_planner. The amount memory we needed to provide externally is 2x to 4x, which does not make practical sense. A more intelligent solution could be to make arena_planner use a memory pool provided externally. This could help to greatly alleviate the memory fragmentation problem, when we need to load and offload different models frequently. This involves some modifications to arena.  For the moment I would think that the issue being closed. Probably we could contribute with PR when it is done from our side. Thank you.",Are you satisfied with the resolution of your issue? Yes No
yi,orikkwak,Help Needed: AttributeError in tf2onnx Conversion from ONNX to TensorFlow Model," 1. System information OS Platform and Distribution: Windows 10 / flutter 3.19.5 / dart 3.3.3 / python 3.12.4 TensorFlow installation: pip package TensorFlow library: 2.16.1  2. Code import tensorflow as tf import torch import torch.onnx import onnx import tf2onnx import torch.nn as nn import torchvision.models as models  PyTorch 모델 정의 및 로드 model = models.resnet18(weights=None) num_features = model.fc.in_features model.fc = nn.Linear(num_features, 6) model.load_state_dict(torch.load('checkpoint/ResNet18Transferred.pt')['net']) model.eval()  더미 입력 생성 dummy_input = torch.randn(1, 3, 224, 224)  ONNX로 변환 onnx_model_path = ""resnet18.onnx"" torch.onnx.export(model, dummy_input, onnx_model_path, verbose=True, input_names=['input'], output_names=['output']) print(""ONNX 모델이 성공적으로 저장되었습니다."")  ONNX 모델 검사 onnx_model = onnx.load(onnx_model_path) onnx.checker.check_model(onnx_model) print(""ONNX 모델이 올바르게 저장되었습니다."")  ONNX 모델의 입력 및 출력 노드 이름 확인 input_names = [input.name for input in onnx_model.graph.input] output_names = [output.name for output in onnx_model.graph.output] print(""Input Names:"", input_names) print(""Output Names:"", output_names)  ONNX 모델을 TensorFlow 모델로 변환 spec = (tf.TensorSpec((1, 224, 224, 3), tf.float32, ",2024-06-15T01:33:16Z,stat:contribution welcome stat:awaiting response stale comp:lite TFLiteConverter TF 2.16,closed,0,3,https://github.com/tensorflow/tensorflow/issues/69812,"Hi  , we're wondering if you may be able to resolve your issue by using AIEdgeTorch, you can find more information here: googleblog. So instead of converting your model to onnx and then to tflite, you can directly convert your model from pytorch to tflite. I have actually created a simple script for converting a resnet model here:  If you want to, you can actually try visualizing the result in modelexplorer as well. Please try them out and let us know if this resolves your issue. If you still need further help, feel free to open a new issue at the respective repo.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,orikkwak,Immediate Assistance Required: Issue with Converting Keras Model to TFLite," 1. System information  OS Platform and Distribution : Windows 10 / flutter 3.19.5 /dart 3.3.3 / python 3.12.4  TensorFlow installation : pip package  TensorFlow library : 2.16.1  2. Code  Option B: Paste your code here or provide a link to a custom endtoend colab import os import tensorflow as tf import tensorflow_datasets as tfds import tensorflow_hub as hub import warnings  경고 메시지 무시 warnings.filterwarnings(""ignore"", category=DeprecationWarning) warnings.filterwarnings(""ignore"", category=FutureWarning) os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'   TensorFlow의 INFO 및 WARNING 메시지 무시  데이터 로드 및 전처리 함수 def load_data():     def preprocess(image, label):         try:             image = tf.image.resize(image, (224, 224))             image = image / 255.0   정규화         except Exception as e:             print(f""Error in preprocessing: {e}"")             return None, None         return image, label     train_dataset, test_dataset = tfds.load('cifar10', split=['train', 'test'], as_supervised=True)     train_dataset = train_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)     train_dataset = train_dataset.filter(lambda x, y: x is not None)   예외 처리된 데이터 제외     train_dataset = train_dataset.batch(32).pre",2024-06-15T01:14:41Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.16,closed,0,4,https://github.com/tensorflow/tensorflow/issues/69811,"Hi  , I tried replicating your issue in colab, but it keeps crashing because i run out of VRAM while training on the Tesla T4 GPU . Can you please share me the saved model instead ?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,lgeiger,GCS gfile operations fail in TF 2.17.0rc0 and 2.18 nightly when not running in GCP," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.17.0rc0, 2.17.0dev20240320, 2.17.0.dev20240528, 2.18.0.dev20240613, 2.18.0.dev20240617  Custom code No  OS platform and distribution Debian bookworm, macos 14.5  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When trying to run GCS operations with `tf.io.gfile` on nightly 2.17 or 2.18 anywhere outside of a GCP VM the command hangs and eventually fails after 10 retries with the error message as below. I can't seem to reproduce this issue on either colab or a GCP VM. But it will consistently fail locally on my mac, inside a `python:3.11` docker container, on GitHub actions or inside a kaggle notebook. The same code works fine with TensorFlow 2.16 so I don't think this is due to my local setup. It also seems like other people are running into this with the latest TF nightly: https://github.com/tensorflow/datasets/issues/5360 Would be great to get this fixed before the 2.17 stable release.  Standalone code to reproduce the issue   Relevant log o",2024-06-14T16:21:43Z,stat:awaiting tensorflower type:bug comp:cloud 2.17,closed,0,11,https://github.com/tensorflow/tensorflow/issues/69789,"The only commit that comes to mind that might influence this is a4e3b786447042f4aa6ad5649a7fef9c4b40cee7, but I can't easily test this since tfnightly wheels on pypi older than this date are not available.","toplay , this is another regression in TF 2.17 RC0",Please consider filing an issue in TensorFlow IO. To my knowledge there are similar issues already reported to TensorFlow IO. FYI    ,> Please consider filing an issue in TensorFlow IO. To my knowledge there are similar issues already reported to TensorFlow IO. toplay I already did at https://github.com/tensorflow/io/issues/2016. But I'm not sure that it's related to TensorFlow IO since I can only reproduce this issue with TF 2.17 but not TF 2.16 even when both of them use the same `tensorflow_io_gcs_filesystem` package version., Could you PTAL and help route to the right person?,"This is from tsl layer, not TFIO.","Indeed, using a wheel from Feb 20 works: `gs://tensorflowpublicbuildartifacts/prod/tensorflow/official/release/nightly/linux_x86_cpu/wheel_py311/391/20240220042013/github/tensorflow/build_output/tf_nightly_cpu2.17.0.dev20240220cp311cp311manylinux_2_17_x86_64.manylinux2014_x86_64.whl` (there's more wheels in that bucket) I've been able to reproduce  this in one place, but it does work in a Docker container on a nonVM PC, so it's not necessarily VMrelated. So it does appear https://github.com/tensorflow/tensorflow/commit/a4e3b786447042f4aa6ad5649a7fef9c4b40cee7 is the cause. We're looking into whether it can be rolled back","Probably could be ""reverted"" via copybara","Thanks for looking into it and reproducing the issue. Also thanks for sharing the link to the older tfnightly wheels, that's very useful to know.",This has been fixed in the latest nightly and on 2.17.0rc1 🎉 ,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],Add a tag to remove a few targets from internal code coverage computation.,"Add a tag to remove a few targets from internal code coverage computation. They time out and there isn't a good way to detect this state with select(), and long/large tests are banned within TF directories.",2024-06-14T15:28:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69787
yi,dingjieliu,error: no such file or directory: 'v2' when Bazel build for macOS," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.13.1  Custom code No  OS platform and distribution macOS 14.3.1  Mobile device no  Python version 3.9  Bazel version 5.3.0  GCC/compiler version Apple clang version 15.0.0 (clang1500.1.0.2.5)  CUDA/cuDNN version np  GPU model and memory no  Current behavior? Hi, I am trying to install TensorFlow C lib with Bazel as recommended. I run: git clone git checkout v2.13.1 bazel build config opt //tensorflow/tools/lib_package:libtensorflow it says:   Standalone code to reproduce the issue   Relevant log output ",2024-06-14T15:24:10Z,stat:awaiting response type:build/install stale subtype:macOS TF 2.13,closed,0,10,https://github.com/tensorflow/tensorflow/issues/69786,"Hi **** ,  I found version compatibility here. Could you please go through this documentation once. You need to use Clang 16.0.0 for tf 2.13.1. But you are using other version. Please update the version. Thank you!","Hi Venkat   I have updated the Clang version to 16.0.0 as recommended. Besides, I run the commend with:  I met an error said no toolchain is configured. (detailed info will be post later) So I modify .bazelrc file by changing build:tf_fuzztest action_env=CC=/usr/local/bin/clang and build:tf_fuzztest action_env=CXX=/usr/local/bin/clang++ but the error remaines:  How should I do to set the toolchain configs? Thanks for your help!!",I just found the Chinese and English pages are different significantly. I will try later. Thanks!,"Hi **** ,  I guess Chinese page documentation may be not updated. Could you please try to follow the english documentation page as it updated? Thank you!","Hi  , I have updated my clang to 16.0.0  Then I run Bazel to build:  it says:  How should I do? Is it caused by wrong protobuf version or failed download or something else? Does it have some version requirement for protobuf? Thank you!","> Hi  , >  > I have updated my clang to 16.0.0 >  >  >  > Then I run Bazel to build: >  >  >  > it says: >  >  >  > How should I do? Is it caused by wrong protobuf version or failed download or something else? Does it have some version requirement for protobuf? >  > Thank you! tried with v2.12.1 and the error remains.","Hi **** , Apologies for the delay, and thank you for your patience. It looks like you are using an older version of TensorFlow (2.13). Many bugs have been fixed in the latest version. Could you please try running your code with the latest version and let us know if the issue still persists? Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],"[TSL] Remove apparently unnecessary ""template"" keywords that are yielding a clang warning.","[TSL] Remove apparently unnecessary ""template"" keywords that are yielding a clang warning.",2024-06-14T14:52:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69783
rag,tirk999,tf.py_function does not output ragged tensors," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15  Custom code Yes  OS platform and distribution Windows  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hello everyone, I am having a problem with the function tf.py_function. I want it to output a ragged tensor but I cannot manage to make it happen. The longer context is that I am training a neural network, YOLO, for object detection. I want to use some data augmentations techniques that do not support bounding boxes. Therefore, I moved to albumentations for data augmentation. The problem is that albuminations works on numpy arrays. I use tf.py_function to output my defined data augmentation but I would need the bounding boxes to be a ragged tensor.  Reference code: https://keras.io/examples/vision/yolov8/   Inside of the map_augmentation function:   I can modify the Tout in order to output a tf.TensorRaggedSpec but I always get errors. Moreover, I tried to find a solution with this topic https://github.com/tensorflow/tensorflow/issues/2645",2024-06-14T13:24:10Z,stat:awaiting response type:bug stale comp:ops TF 2.15,closed,0,6,https://github.com/tensorflow/tensorflow/issues/69777,"Hi **** ,  I tried to run your code on Colab using TF v2.16.1 and nightly faced the same issue. Please find the gist here for reference.   Thank you!","Hello  , do you think that the issue will be solved? Thank you!","Hi,   I apologize for the delayed response, I have tried with lambda function something like below which is returning the `tf.RaggedTensor` please refer these similar issues https://github.com/tensorflow/tensorflow/issues/27679, https://github.com/tensorflow/tensorflow/issues/26453 which may help you to solve your issue.  **Output** : `` If I have missed something here please let me know. Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Introduce nested tuple support in FFI,Introduce nested tuple support in FFI This change allows passing (nested) tuples to FFI. Tuples are flattened acting as a logical buffer separator which enables them to be treated as flat buffer arguments. All tuple arrangements are supported as long as the underlying bare buffers are valid for input and output.,2024-06-14T12:46:43Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/69776,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
rag,sanowl,Add New Features and Enhance TensorFlow ConstOp Tests," Overview This pull request introduces several new features to the TensorFlow constant operation tests, enhancing the coverage and robustness of the tests. Key additions include tests for complex numbers, boolean values, large tensors, mixed data types, and range values.  New Features 1. **Complex Numbers**: Added a test case to check the creation of tensors with complex numbers. 2. **Boolean Values**: Added a test case to verify the creation of tensors with boolean values. 3. **Large Tensor**: Added a test case to create and verify a large tensor. 4. **Mixed Data Types**: Added a test case to check behavior with mixed data types (should fail). 5. **Range Values**: Added a test case to create tensors from a range of values.  Detailed Changes  **Complex Numbers**: Ensured support for complex number tensors and verified with appropriate tests.  **Boolean Values**: Verified the creation of boolean value tensors.  **Large Tensor**: Tested the creation and validation of a large tensor to ensure performance and correctness.  **Mixed Data Types**: Checked the behavior of tensors with mixed data types to ensure appropriate error handling.  **Range Values**: Created tensors from a range of values to validate cor",2024-06-14T10:42:46Z,comp:ops size:S,closed,0,6,https://github.com/tensorflow/tensorflow/issues/69770,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",Hi  Can you please sign CLA? Thank you!,Hi  Can you please sign CLA? Thank you!,"Hi , Can you please sign CLA? Thank you!","Hi , Can you please sign CLA? Thank you!","Hi  I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen. Thank you for your contribution!"
rag,copybara-service[bot],Replace the use of `xla::ifrt::Array::Reshard()` in JAX Python binding with `xla::ifrt::Client::CopyArrays()`,"Replace the use of `xla::ifrt::Array::Reshard()` in JAX Python binding with `xla::ifrt::Client::CopyArrays()` IFRT API now distinguishes reshard vs. copy, so this CL is reflecting such a semantics change to the JAX Python binding. Since pjit input sharding is relatively easy to batch, it was also rewritten to leverage the batched `CopyArrays` API.",2024-06-14T08:19:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69758
yi,roshni-warrantylife,Object Detection in Android using front camera: the detected bounding boxes are drawn incorrectly," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 0.4.0  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When I move an object (initially placed at the center of the screen) from a far distance towards the camera lens, the left position of the bounding box gradually shifts to the right side of the screen instead of staying centered. Here is the recording of the issue, https://drive.google.com/file/d/144zCu8yPXYSeVPKk4RjDTG40YOcbLAtX/view This issue is reproducable in the https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android.  I have changed the camera lens to ""LENS_FACING_FRONT"" in the CameraFragment class. Additionally, to resolve the issue with the inverted (mirrored) coordinates, I have flipped the coordinates horizontally by adding the following code in the OverlayView class. `              Standalone code to reproduce the issue   Relevant log output _No response_",2024-06-13T20:55:44Z,stat:awaiting tensorflower type:bug comp:lite Android,closed,0,11,https://github.com/tensorflow/tensorflow/issues/69734,"Hi, warrantylife  I apologize for the delayed response and thank you for bringing this issue to our attention, if possible could you please help us with your github repo (with changes made by you in this example) which will be easy for us to replicate the same behaviour from our end and investigate this issue further ? Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"> Hi, warrantylife >  > I apologize for the delayed response and thank you for bringing this issue to our attention, if possible could you please help us with your github repo (with changes made by you in this example) which will be easy for us to replicate the same behaviour from our end and investigate this issue further ? >  > Thank you for your cooperation and patience. Thank you for your understanding. I’ve shared the GitHub repository link below, which includes the changes I made to the example. Please feel free to use it to replicate the behavior on your end and investigate the issue further: https://github.com/roshniwarrantylife/objectdetection/tree/master/lite/examples/object_detection/android Let me know if you need any additional information, and I’ll be happy to assist. Thank you for your time and support!","Hi, warrantylife I apologize for the delayed response, I was trying to replicate the same behavior from my end with your provided Github repo and I'm getting error messages please refer this error log file so could you please try from your end and see is it working as expected or not ? it seems there might be some package versions compatibility or Gradle version mismatch ? Thank you for your cooperation and patience.","Thanks for sharing the error log. I've updated the code in the GitHub repo to address possible Gradle or package version issues. Please try again using this link and let me know if it works as expected. If the issue persists, feel free to share the updated logs for further review. Thanks for your patience!","Hi, warrantylife Thank you for providing the updated github repo, I'm able to build and run your project successfully with Android virtual device with `Pixel 5 API 35` but I'm getting below output so are you using actual device or AVD ( I gave permission to access  camera for device from AVD) ? or Am I missing something here ? Thank you for your cooperation and patience. !image","Thank you for your feedback and for successfully building and running the project on the Android Virtual Device (Pixel 5 API 35). To clarify, I am testing the project on an actual physical device. This might explain the difference in the output you're seeing. If possible, could you try running the app on an actual device to see if the output differs?  Thank you for your patience and cooperation as we troubleshoot this!","Hi,  Please take look into this issue. Thank you","Hi , can you please take a look? Thanks.","Hi, warrantylife  Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/59 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[XLA:MSA] Sync copy replacement,"[XLA:MSA] Sync copy replacement This change adds a feature to MSA that replaces synchronous copies (between default and alternate memory) with asynchronous copies, when possible. The feature is disabled by default for now. The feature is implemented by jointprocessing of intervals that are connected to through synchronous copy instructions that we are trying to replace. If all allocation values of the jointprocessed intervals can be allocated successfully, and the sync copy instruction replacement correctness conditions are met, the sync copy will be replaced with an async copy instruction. If not, the sync copy instruction is kept as is, and will be ignored in replacement efforts of future closures. For a sync copy to be consider replaceable, beside not being in the ignored list because of the past failures, it must not change the layout between its input and output, it must not have a preassigned memory space for its operand or output, and its operand value must only show up in one position. In order to replace a copy, we modify the segments to extend from the AllocationValue to the uses of its sync copy uses that are considered for replacement.",2024-06-13T20:25:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69730
yi,ludiusvox,Installing Tensorflow on Fedora 40," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.5  Custom code Yes  OS platform and distribution WSL Fedora 40  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.4  GPU model and memory 4080RTX NVIDIA  Current behavior? I am trying to figure out how to build a wheel for Tensorflow 2.5+ for CUDA 12.4, I am having difficulty, the Fedora developers think they need Bazel, but I explained that Bazel is unneccesary, but I am looking for a procedure to install with CMAKE and build from source and upload a container.   Pytorch is easy to install, and the team lead on Fedora 40 AI/ML is more interested in supporting ROCm so I am just coming in here see if I can get some advice.  I had completed a graduate thesis in TF and i am really comfortable with the Software, but it's limited pipeline support for all the distros is difficult.   I would hate to have to spend $1000 on a new GPU I really like fedora and Pytorch works out of the box, can anything be done?  Standalone code to reproduce the issue   Relevant log output ",2024-06-13T18:00:32Z,stat:awaiting response type:bug TF 2.5 wsl2,closed,0,6,https://github.com/tensorflow/tensorflow/issues/69718,"I got it fixed with tensorflow nightly in docker containers, forget building from source!!! this worked!! StepbyStep Instructions Ensure Podman is Installed and Configured: If Podman isn't installed or configured yet, follow these steps: sh sudo dnf install y podman Install NVIDIA Container Toolkit: Install the NVIDIA Container Toolkit to enable GPU support in Podman. sh distribution=$(. /etc/osrelease;echo $ID$VERSION_ID) curl s L https://nvidia.github.io/nvidiadocker/gpgkey __/ WARNING: You are running this container as root, which can cause new files in mounted volumes to be created as the root user on your host machine. To avoid this, run the container by specifying your user’s userid: $ docker run u $(id u):$(id g) args… /sbin/ldconfig.real: /usr/lib/wsl/drivers/nvmdsi.inf_amd64_23a2cede5f1383ec/libnvidiaml.so.1 is not a symbolic link root:/ python c “import tensorflow as tf; print(‘TensorFlow version:’, tf.version); print(‘Num GPUs Available:’, len(tf.config.experimental.list_physical_devices(‘GPU’)))” 20240613 21:27:38.985917: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable TF_ENABLE_ONEDNN_OPTS=0. 20240613 21:27:39.009506: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. TensorFlow version: 2.18.0dev20240612 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1718314059.869098 17 cuda_executor.cc:990] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node Your kernel may have been built without NUMA support. I0000 00:00:1718314059.872207 17 cuda_executor.cc:990] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node Your kernel may have been built without NUMA support. I0000 00:00:1718314059.872254 17 cuda_executor.cc:990] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node Your kernel may have been built without NUMA support. Num GPUs Available: 1","Hi  ,  Could you please upgrade your tensorflow version. And here there is version miss match also. Please go through this documentation once. Thank you!",Thank you!,"I found a solution with an IDE 1 line of code.  Very useful one line of code Found the solutions. https://jupyterdockerstacks.readthedocs.io/en/latest/using/selecting.html For example, you can use an image quay.io/jupyter/pytorchnotebook:cuda12python3.11.8 or quay.io/jupyter/tensorflownotebook:cudalatest `sudo podman pull quay.io/jupyter/tensorflownotebook:cudalatest` very fast shortcut one line of code fixed the problem ","Hi **** ,  Glad to see this issue is resolved.  Please feel free to close the issue if it is resolved ?  Thank you!",Are you satisfied with the resolution of your issue? Yes No
yi,nrad,Significant Performance Drop When Training Sequential Model Using `tf.data.Dataset.from_generator`, Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.13.1 2.16.1 tfnightly (2.18.0dev20240613)  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I noticed a significant drop in the performance of the Sequential model in a binarycross entropy problem when I switched to using tf.dataset.from_generator from using pd.DataFrame during the training. Here is link to a google colab notebook which demonstrate the issue: ~~https://colab.research.google.com/drive/1AoysBY3nL6Tti1fKnCb6ch9if20iz6?usp=sharing~~ https://colab.research.google.com/drive/1kdEQJpRpORiF9ornGzzqn02f6pUVims?usp=sharing  Standalone code to reproduce the issue  shell Performance using pd.DataFrame: 20/20 [==============================]  0s 3ms/step  loss: 0.1074  accuracy: 0.9617 Performance using tf.dataset: 20/20 [==============================]  0s 3ms/step  loss: 0.6245  accuracy: 0.6626 ```,2024-06-13T15:05:14Z,stat:awaiting tensorflower comp:data type:performance 2.17,open,0,4,https://github.com/tensorflow/tensorflow/issues/69702,", I can see you are using tensorflow 2.13 which is little older than the latest. I tried with the tfnightly and couldn't found any difference in the performance with tf.data.dataset.from_generator. Kindly find the gist of it here.  Thank you!",  Thank you for the quick response! Sorry but in the code I had posted the actual training line `history = model.fit(...)` was commented out! I just tried the code again with the tfnightly and also with v2.16.1 and unfortunately the I see the issue persisting. Here is the corrected google colab. I also edited the original post.,Any ideas?  ,"I tried to execute the code on tensorflow v2.15 & tfnightly, and observed that the performance was dropping when using **tf.data.dataset.from_generator**. Kindly find the gist of it here. Also there is another issue which was for the similar issue.  https://github.com/tensorflow/tensorflow/issues/49727 Thank you!"
yi,x0w3n,Segmentation fault in `tf.raw_ops.CollectiveAllToAllV2`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.3 LTS (x86_64)  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Under specific input, tf.raw_ops.CollectiveGatherV2 encounters ""Segmentation fault (core dumped)"".  Standalone code to reproduce the issue   Relevant log output ",2024-06-13T14:38:21Z,stat:awaiting tensorflower type:bug comp:ops TF 2.16,open,0,1,https://github.com/tensorflow/tensorflow/issues/69700,"Hi  ,  I was able to reproduce the issue on Colab using TF v2.16.1 and TFnightly ,Please find the gist here for reference. Thank you!"
yi,x0w3n,Segmentation fault in `tf.raw_ops.CollectiveGatherV2`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.3 LTS (x86_64)  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Under specific input, tf.raw_ops.CollectiveGatherV2 encounters ""Segmentation fault (core dumped)"".  Standalone code to reproduce the issue   Relevant log output ",2024-06-13T14:35:21Z,stat:awaiting tensorflower type:bug comp:ops TF 2.16,open,0,1,https://github.com/tensorflow/tensorflow/issues/69699,I was able to reproduce the issue on tensoflow v2.16 and tfnightly. Kindly find the gist of it here and the screenshot for the reference. !image
yi,dhruvsingla273,TFlite usage on android,"I am using a TFlite model to do some video processing frame by frame on android. The below is the method I have to use to convert Bitmap to ByteBuffer. ` val bitmap = Bitmap.createScaledBitmap(yourInputImage, 224, 224, true) val input = ByteBuffer.allocateDirect(224*224*3*4).order(ByteOrder.nativeOrder()) for (y in 0 until 224) {     for (x in 0 until 224) {         val px = bitmap.getPixel(x, y)         val r = Color.red(px)         val g = Color.green(px)         val b = Color.blue(px)         val rf = (r  127) / 255f         val gf = (g  127) / 255f         val bf = (b  127) / 255f         input.putFloat(rf)         input.putFloat(gf)         input.putFloat(bf)     } } ` But this is taking a lot of time and as I have to do Real time processing I dont have such time, How can I optimise this? And also while copying the image from byteBuffer to Bitmap it is also taking a lot of time Inbuilt functions given don't work because of how the ByteBuffer is created. So I have to make custom loop to copy the pixels back which takes a lot of time. Please suggest alternative strategies.",2024-06-13T09:32:47Z,comp:lite type:performance Android,closed,0,6,https://github.com/tensorflow/tensorflow/issues/69680," Can you please have a look, Thanks","Hi  , I  don't have much idea about this , can you please take a look?","I made a few optimisations like using array and using bulk get put and get methods. I got a delegate for my model QNN Htp, but i am not able to set it up Any chance you could help?","Hi , using array/bulk/vectorized methods and preprocessing libraries will probably help. You may want to see if tflitesupport can help you: https://www.tensorflow.org/lite/inference_with_metadata/lite_support . How are you > not able to set it up ? I.e. what is blocking you?, what's your environment? and How can I reproduce your error?","I am able to make the QNN HTP delegate now. I changed the android NDK version and it worked. There was some SOC file that was missing. Thanks, I will checkout the tflite_support for the preprocessing",Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],[XLA:GPU] Use predication instead of branching in topk_kernel.,"[XLA:GPU] Use predication instead of branching in topk_kernel. This change shrinks the size of the JAX CUDA 12 wheel (*compressed*) by 24MiB from 105MiB to 81MiB. It also reduces the compile time for the topk_kernel to less than 5 minutes for the longer shard; previously it was well over 10 minutes. nvcc/clang were emitting the branching in Push() as branches, which caused a lot of unnecessary register moves at SSA phi nodes in the program. If we change the program slightly to encourage nvcc to use predication instead of branching, we generate a significantly more compact program in both PTX and SASS forms. Reverts 5969602d6a1114d538aa98dbde9f27304fc6b22f",2024-06-12T18:48:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69628
rag,copybara-service[bot],[XLA:GPU] Use predication instead of branching in topk_kernel.,"[XLA:GPU] Use predication instead of branching in topk_kernel. This change shrinks the size of the JAX CUDA 12 wheel (*compressed*) by 24MiB from 105MiB to 81MiB. It also reduces the compile time for the topk_kernel to less than 5 minutes for the longer shard; previously it was well over 10 minutes. nvcc/clang were emitting the branching in Push() as branches, which caused a lot of unnecessary register moves at SSA phi nodes in the program. If we change the program slightly to encourage nvcc to use predication instead of branching, we generate a significantly more compact program in both PTX and SASS forms.",2024-06-12T16:18:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69614
transformer,copybara-service[bot],PR #13278: Fix _xla_send_recv_validation_attribute in loop-double-buffer-transformer,"PR CC(tf_cc_binary() makes opencv unable to load an image): Fix _xla_send_recv_validation_attribute in loopdoublebuffertransformer Imported from GitHub PR https://github.com/openxla/xla/pull/13278 This patch fixes the value of valid iteration for GPUs in the loopdoublebuffertransformer pass. If the original iteration bounds for a particular GPU are {a,b} then the transformed attribute is:    If number of iterations is even:      For collective.permute.1: {floor((a+1)/2), floor(b/2)}      For collective.permute.2: {floor(a/2), max(0, floor((b1)/2))}    If the number of iterations is odd: then the first iteration is peeled out      For collective.permute.1: {floor(a/2), max(0, floor((b1)/2))}      For collective.permute.2: {max(0, floor((a1)/2)), max(0, floor((b2)/2)} For collective.permute.peeled, only one instance of the collective is valid. It is valid for all the devices that had iteration 0 as valid iteration earlier. So, if the attribute was {{0,4},{0,5},{1,5},{1,6}} for the old instruction, then for the peeled instruction, the attribute would be {{0,0},{0,0},{1,0},{1,0}} because it is valid for the first two devices, but invalid for the rest. More details for these are added in comments as docstrin",2024-06-12T15:58:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69609
gemma,copybara-service[bot],PR #13310: [NVIDIA GPU] Added a rewrite logic in gpu_windowned_einsum_handler to handle all2all,"PR CC(Feature Request: Mixed Sparse and Dense Tensors): [NVIDIA GPU] Added a rewrite logic in gpu_windowned_einsum_handler to handle all2all Imported from GitHub PR https://github.com/openxla/xla/pull/13310 Added a rewrite logic in gpu_windowned_einsum_handler to decompose a2a+gemm or gemm+a2a into smaller a2a+gemm/gemm+a2a to hide communuication overhead. Partial results will be aggregated at the end. An example will be:  decomposed into  All partial gemms will be dispatched to parallel streams too to achieve gemmgemm overlap. Performance metrics: For an unit with just a2a+gemm or gemm+a2a, we see from 515% speedup depending on the size by doing this type of composition. Copybara import of the project:  557c540df51b3c238f87ae01262f1a6000ee4499 by TJ : Added a rewrite logic in gpu_windowned_einsum_handler to decompose a2a+gemm or gemm+a2a into smaller a2a+gemm/gemm+a2a to hide communuication overhead.  d3e9b2fc28b484f263609c21b6177ea948aa8e01 by TJ : Changed testing to use file check  3de9fbb962438837e77353c3c0b2a96e3e0d397e by TJ Xu : Added e2e tests address recent changes to thunk emission with execution stream id  d7790ed5e206c5e1ebf33afa8e34d7faedff4d47 by TJ Xu : Added file check to BUILD file Merg",2024-06-12T15:52:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69606
rag,BenCrulis,"The constant folding pass of the TFLite converter prevent storing packed tensors, stores dequantized tensors instead"," 1. System information WSL Linux l 5.14.0427.18.1.el9_4.x86_64 GNU/Linux tensorflow==2.10.1 tensorflowcpu==2.10.1 installed using pip  2. Code Ignore the fact that the dequantization process is currently wrong, this is just for testing.   3. conversion The conversion is successful, but the model size is that of a model which stores the full 1000*500 weight matrix in int8 format, which is about 500KB, when it should store the packed weights and weigh ~63KB. I assume this is the result of the constant folding pass of the converter which stores the weights that have just been casted to float32, instead of storing the int8 weights and redoing the dequantization process each time. This can be seen in the graph of the resulting tflite file: !image (I am not sure why the tensor is not saved after the transpose instead) This is also evidenced by the fact that I can replace `compressed_weights = self.kernel` with `compressed_weights = self.kernel + tf.cast(x[0,0], dtype=tf.int8) * 0` and have the compressed weights saved on disk this way, because `x` cannot be constantfolded. However, this cost extra operations and forces me to activate additional supported ops in the converter, which is not ideal. Note that I a",2024-06-12T13:17:17Z,stat:awaiting tensorflower type:feature comp:lite TFLiteConverter TF 2.10,closed,0,5,https://github.com/tensorflow/tensorflow/issues/69598,"Hi   , I replicated his code and the converted model size comes out to be of 500kb . Can you please take a look?","I am unable to find a configuration option which accomplishes the stated goals... here's where you can find most of the converter options: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/lite.pyL614, for now I will consider this as a feature request. Using a current version of TF also does not resolve this. wei can you please take a look? Thanks.","Thanks everyone for looking into this.  to me, the best way to solve this problem would not be a global configuration option, because optimizing some parts of the graph can still be very useful, but instead an operator that just prevents constant folding from being applied on this part of the graph. Last week I managed to find a way to create such an operator, but it is a bit of a hack that might not work in future versions of Tensorflow:  It is used like this in the original code:  With this, the resulting tflite file is indeed 64KB, here is what the graph looks like: !image I think it would be best if Tensorflow introduced a similar operator, which ideally does not require a witness runtime value. Maybe it could be an option to `tf.identity`, or a dedicated operator that is just like `tf.identity` but prevents constant folding.","Hi,   Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/60 Let us know if you have any questions. Thanks.","Hi  , I was not aware of the migration to LiteRT, but I noticed that the TFLite Engine was seemingly gradually abandoned in favor of something else. Thank you for reopening the issue there. Currently I am solving my original problem by modifying Larq, which is based on the TFLite Engine to include a custom native operator which does not cause the converter to store dequantized tensors in the `.tflite`. So I am wondering, will LiteRT easily modified to include custom operators? Ideally keeping the same operator API. And since we are there, is there any plan to implement the multithreaded batch matrix multiplication operator for ARM devices, or allow the replacement of the native operators of the library by custom native operators?"
yi,copybara-service[bot],PR #11583: Weight offloading of Jax memories: support memory_kind for GPUs,"PR CC(tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[49] = 60000 is not in [0, 60000)): Weight offloading of Jax memories: support memory_kind for GPUs Imported from GitHub PR https://github.com/openxla/xla/pull/11583 The first step to implement weight offloading is to support the existing host memory kind for GPUs: pinned_host.   Below are examples of the supported Jax APIs:  The next step will be supporting Jax API of device_put() via PjRtStreamExecutorClient::BufferFromHostBuffer() in XLA. Each tensor can only have one memory space or memory kind. This will be implemented in PjRtBuffer to get the mapped memory space from NamedSharding() or with_memory_kind(). Copybara import of the project:  7ac05a767e23755f8505bef9d1f077d1a293ab0c by Jane Liu : Support NamedSharding with memory_kind by adding host memory spaces. Merging this change closes CC(tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[49] = 60000 is not in [0, 60000)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/11583 from zhenyingliu:weightoffloading 7ac05a767e23755f8505bef9d1f077d1a293ab0c",2024-06-12T07:55:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69576
llama,copybara-service[bot],Turn on layer scanning for llama2-7b on GPU.,Turn on layer scanning for llama27b on GPU. This better utilizes recent optimizations to collective approximation in the XLA latency hiding scheduler. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13639 from philipphack:u_layer_uaf_xla 91ebf7b4a2ac90ebadce27d1a73e88fb4513aed4,2024-06-11T16:04:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69536
yi,copybara-service[bot],[XLA:GPU] Initial simplification of Triton Support test.,"[XLA:GPU] Initial simplification of Triton Support test. This is a first change that demonstrates my intention for simplifying the Triton support tests. Once we agree on the general direction, I will submit additional CLs to simplify all tests.",2024-06-11T15:14:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69534
yi,sokolyaka,Selectively Build TensorFlow Lite with Docker ends with an error.," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version latest  Custom code No  OS platform and distribution AWS m5a.xlarge: CPU 4, Memory 16, Linux Ubuntu 24.04 LTS  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi there, I'm trying to reduce TensorFlow Lite binary size, selectively building TensorFlow Lite with Docker using the guide. I have the same error log as in the issue, which was closed due to inactivity, and I'm not sure how to proceed.  Standalone code to reproduce the issue  4. Downloaded the TensorFlow Git repository and stay on the master branch `https://github.com/tensorflow/tensorflow.git`  5. Run the build_aar_with_docker.sh script with params (check log output) shell ubuntu:~/tflite$ ./build_aar_with_docker.sh \ input_models=/home/ubuntu/tflite/models/model.tflite \  target_archs=x86,x86_64,arm64v8a,armeabiv7a \ src_dir=/home/ubuntu/tflite/tensorflow +++ dirname ./build_aar_with_docker.sh ++ cd . ++ pwd + SCRIPT_DIR=/home/ubuntu/tflite + ARGUMENTS='input_models=",2024-06-11T14:54:37Z,stat:awaiting response type:build/install comp:lite subtype: ubuntu/linux TF 2.16,closed,0,4,https://github.com/tensorflow/tensorflow/issues/69532,"I ran the 2.13 version and got the same error at the end, but the logs a bit differed. ","Hi  , Can you please share me the tflite file also ?","Hi  the issue was with the tflite file itself, not with the TensorFlow.",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Fix fallback to RunHloModuleIterationLiterals.,Fix fallback to RunHloModuleIterationLiterals. Trying to parse RunHloModuleLiterals may already add one iteration. So we need to clear it before adding the RunHloModuleIterationLiterals proto. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13639 from philipphack:u_layer_uaf_xla 91ebf7b4a2ac90ebadce27d1a73e88fb4513aed4,2024-06-11T13:02:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69527
rag,copybara-service[bot],PR #13491: [ROCm] Fix profiler deadlock due to 3d7a4720b02e643faccd13f9a450213b5…,PR CC(Feature Request: support tf.diag on GPU): [ROCm] Fix profiler deadlock due to 3d7a4720b02e643faccd13f9a450213b5… Imported from GitHub PR https://github.com/openxla/xla/pull/13491 …dfe17b7 Copybara import of the project:  5e584a13c226de0707dccf763501fccbf37dcc32 by Dragan Mladjenovic : [ROCm] Fix profiler deadlock due to 3d7a4720b02e643faccd13f9a450213b5dfe17b7 Merging this change closes CC(Feature Request: support tf.diag on GPU) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13491 from ROCm:rocm_profiler_deadlock 5e584a13c226de0707dccf763501fccbf37dcc32,2024-06-11T08:31:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69516
gpt,Hozzu,YOLOv5n model does work on python tflite but not C++ tflite,"**System information** Linux with tensorflow v2.16.1 **Standalone code to reproduce the issue** I exported YOLOv5n integer quantized model from ultralytics. I tested with python tflite code, and it worked fine. But when I ported it to C++ tflite code, it did not work. All the output score value is 0. The output x,y,w,h value is not 0. **Any other info / logs** I attatched the exported YOLOv5n model. https://1drv.ms/u/s!AnqHHtrBqwyUg8QHviF7T1KqPkXeog?e=c3fasi The code I used it for C++ is as bellows.  ",2024-06-11T06:57:11Z,type:bug comp:lite TF 2.16,closed,0,1,https://github.com/tensorflow/tensorflow/issues/69510,Are you satisfied with the resolution of your issue? Yes No
finetuning,lbo462,Transfer learning and fine-tuning doc seems to have unexpected results," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version latest  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The python documentation notebook at https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb outputs unexpected results. As the tutorial explains how to finetune a model to differentiate cats and dogs, the resulting output fails and always detect dogs, no matter the pet. Here's a screenshot directly taken at https://www.tensorflow.org/tutorials/images/transfer_learning : !image One would expect that the result is a correct detection of cats and dogs.  Standalone code to reproduce the issue   Relevant log output _No response_",2024-06-10T17:35:28Z,type:docs-bug stat:awaiting response comp:keras,closed,0,7,https://github.com/tensorflow/tensorflow/issues/69480,", Thank you for reporting the issue. We observed the same with the Tensorflow.org page of transfer learning. We will try to fix it. Meanwhile for the alternative you can try using https://keras.io/guides/transfer_learning/ which is providing the correct output. Thank you!",Is this a duplciate of CC(Doc(Transfer learning and finetuning) is quite different from real executive result.)?,", I have raised the PR mentioning the **TF.nn.sigmoid(predictions)** should be either removed or commented. And the PR is under awaiting review. Once it is merged, the changes will be available in the respective document. https://github.com/tensorflow/docs/pull/2314 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,", The PR raised for the mentioned change has been merged. Could you please check and provide the update if the issue got resolved. https://github.com/tensorflow/docs/pull/2314 Thank you!",The issue got resolved,Are you satisfied with the resolution of your issue? Yes No
yi,x0w3n,Aborted (core dumped) in `tf.raw_ops.SparseReduceSum\tf.raw_ops.SparseReduceMax`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.3 LTS (x86_64)  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Under specific input, tf.raw_ops.SparseReduceSum\tf.raw_ops.SparseReduceMax encounters ""Aborted (core dumped)"".  Standalone code to reproduce the issue   Relevant log output ",2024-06-10T12:55:29Z,stat:awaiting response type:bug stale comp:ops TF 2.16,closed,0,6,https://github.com/tensorflow/tensorflow/issues/69470,", I tried to execute the mentioned code on tfnightly and it was not aborted as above, and fails with the same error as the input is very long(Encountered overflow when multiplying). Kindly find the gist of it here.  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Thank you for your response. I found that this crash only triggers in tf version 2.16. Thanks.  here is gist: https://colab.research.google.com/drive/1IxrskRrvNvxuiObCry6LOUxPaKALSqNT?usp=sharing,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #13542: [NVIDIA] Change DCE to replay control deps for GTE-fusion simplification.,"PR CC(Update version strings for TensorFlow 1.4rc0): [NVIDIA] Change DCE to replay control deps for GTEfusion simplification. Imported from GitHub PR https://github.com/openxla/xla/pull/13542 We have seen a case when some optimization pipelines(ie tuple simplifier) add control deps to a GTE output of a fusion kernel to preserve execution order, DCE's unused GTE simplification logic will error out trying to remove the GTE. Instead of replacing and removing, we can use ReplaceInstruction with relay_control_dependency set to true to move the control deps to the fusion instruction. Copybara import of the project:  1de482f5a80f4e9a9b2c8203129111d2aaa63f28 by TJ : Change DCE to replay control deps for GTEfusion simplification. Merging this change closes CC(Update version strings for TensorFlow 1.4rc0) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13542 from Tixxx:tixxx/dce_control_dep 1de482f5a80f4e9a9b2c8203129111d2aaa63f28",2024-06-10T06:04:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69458
yi,yuvch98,AttributeError: 'ModelCheckpoint' object has no attribute '_implements_train_batch_hooks' in MoViNet Streaming Model Training," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.1  Custom code No  OS platform and distribution Google Colab  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an AttributeError while trying to fit the model using the MoViNet Streaming Model Training and Inference notebook provided in the TensorFlow Model Garden repository. The specific error message is as follows: !image in addition, there is a minor adjustment needed to be done in this code: !image  Standalone code to reproduce the issue   Relevant log output ",2024-06-09T17:48:09Z,stat:awaiting response type:bug stale comp:model TF 2.16,closed,0,4,https://github.com/tensorflow/tensorflow/issues/69448, Could you try creating a new virtual environment to avoid conflicts with existing project dependencies and if you still face the issue then please post this issue on models repository for further assistance. Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],Make `pxla.shard_arg` batch calls to `xc.copy_array_to_devices_with_sharding`,"Make `pxla.shard_arg` batch calls to `xc.copy_array_to_devices_with_sharding` This CL changes `shard_arg_handlers` to be batched, in that it now receives a list of objects and a list of shardings and returns a list of array. This makes it possible to batch backend calls whenever it's beneficial to do so. Based on the above, the batched shard arg for arrays leverages the newly added `xla::ifrt::Client::CopyArrays()` (https://github.com/tensorflow/tensorflow/pull/69096) to make bulk copy cheaper in some backend implementations. Since `Client::CopyArrays()` requires batched arrays to have the same set of source/destination devices, `PyArray::BatchedCopyToDeviceWithSharding()` internally groups arrays by their source/destination devices and memory kinds. The grouping is pushed all the way to C++ for performance in case we have lots of arrays.",2024-06-07T17:11:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69389
rag,chudur-budur,"Compilation of mlir:tf-opt fails with error ""The repository '@llvm_zlib' could not be resolved""", Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version dd0c582  Custom code No  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11.9  Bazel version 6.5.0  GCC/compiler version 9.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Compiling using the docker image `latestdevel` and `devel`.  When I try to compile:  I get this error:  How do I compile it?  Standalone code to reproduce the issue   Relevant log output ,2024-06-07T10:25:21Z,stat:awaiting tensorflower type:build/install type:support subtype: ubuntu/linux,open,0,6,https://github.com/tensorflow/tensorflow/issues/69367,Possible workaround. Add these dependecies in the top level `WORKSPACE` file:  The compilation works but then it generates duplicate symbols from `llvm_zstd` and `net_zstd` and fails during the linking: ,bump?,"Hi budur ,  Thank you for the reporting issue. There are other issues in the same line which are assigned to eng team, we will provide the resolution once it resolved. ",CC: toplay,"That method doesn't seem to work anymore, likely because the `llvmproject` source is patched when retrieved by Bazel, which doesn't happen when a local repo is used in accordance with those instructions.    However, with some minimal hacking, you can specify a particular commit/tag like so: https://github.com/vladbelit/tensorflow/commit/a17f4c1022d6fc0886d5465f66df00789501eeab   (if you use a tagged archive, you might need to change `strip_prefix`, etc.) In order to retrieve the SHA256 for a particular archive:  Btw, even though the Docker image you're using works fine in your case, it's no longer supported. You should use one from here: https://hub.docker.com/r/tensorflow/build/tags   Something like: `docker pull tensorflow/build:latestpython3.12`   It already contains Clang 17 (Clang 18 soonish).   Installing Python + setting up a Virtual env is no longer necessary, as Tensorflow's Python has been hermetic for some time now (except maybe in some cases??)","For a more robust solution, handing this off to MLIR  "
yi,Inv4lidn4m3,Add stop_evaluating attribute to Model class and add associated modification to evaluate method," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16.1  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The Model class implements the stop_training attribute, but no stop_evaluating one. In fit, there is a correct action taken to stop training when stop_training is set to True, in a callback for instance:  and validation:  The evaluate method doesn't include this behaviour:  The current way to stop evaluation in a callback is to force raise a  error as it will trigger the  method. But you can only do it after step 1, otherwise the logs will be empty. This is clearly not clean. Also, the  except text is only related to the training. A more general text would help to debug:   Standalone code to reproduce the issue   Relevant log output _No response_",2024-06-07T09:02:14Z,type:feature,closed,0,1,https://github.com/tensorflow/tensorflow/issues/69360,"Sorry, I was in a wrong Conda env and using TF 2.15.1 It seems to have been implemented with keras 3 and TF 2.16.1"
yi,copybara-service[bot],Updating auto generated .pyi files when updating mypy to v1.10.0.,Updating auto generated .pyi files when updating mypy to v1.10.0.,2024-06-06T13:36:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/69318
rag,copybara-service[bot],PR #13461: typo: fix transpse,PR CC(using ExponentialMovingAverage differs in CPU & GPU): typo: fix transpse Imported from GitHub PR https://github.com/openxla/xla/pull/13461 Copybara import of the project:  e7c6c23be44648237a31d17bf2574c4f26510889 by knightXun : typo: fix transpse Merging this change closes CC(using ExponentialMovingAverage differs in CPU & GPU) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13461 from knightXun:transposetypo e7c6c23be44648237a31d17bf2574c4f26510889,2024-06-06T11:26:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69312
yi,DaraOrange,MobileNetV3 quantization,"Hi! I'm trying to quantize MobileNetV3 with tflite, but int8model performs very poor. I think, it is because of linear quantization, which is too simple method not appropriate for any weights distributions. What else can I try? Are you going to support logarithmic scale for quantization in the future?",2024-06-06T11:18:18Z,stat:awaiting response type:support stale comp:lite type:performance TFLiteConverter,closed,0,10,https://github.com/tensorflow/tensorflow/issues/69311,You might want to look into QuantizationAware Training,"I think no, because I train Pytorch models and then convert them to onnx and tflite","Hi  , this issue may be resolved by following a different work flow. Have you tried using AIEdgeTorch?, you can find more information here: googleblog. I have actually created a simple script for converting a CV model here:  If you want to, you can actually try visualizing the result in modelexplorer as well. Please try them out and let us know if this resolves your issue. If you still need further help, feel free to open a new issue at the respective repo.","I have no problems with conversion, but results after quantization are poor. Does this tool support quantization? ","Hm, I see that it just wraps tflite quantizer and don't provide more complicated methods of quantization","Hi , correct: these are the only options available: https://www.tensorflow.org/lite/performance/model_optimization, can you provide the data for us for, ""results after quantization are poor"".",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No, did you find the solution for the issue?
gpt,xujuntwt95329,Failed to build TF Lite benchmark tool on Windows," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version latest from git  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version Visual Studio 2022  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Build failed for Windows, reporting several errors:  Code require C++ 20 but cmake set std to 17  conflict `MT_StaticRelease` and `MD_DynamicRelease` flag during linking  symbol redefinition  Standalone code to reproduce the issue   Relevant log output ",2024-06-06T02:19:15Z,stat:awaiting response type:build/install stale comp:lite subtype:windows,closed,0,5,https://github.com/tensorflow/tensorflow/issues/69292,"Hi,   I apologize for delayed in my response and I'm able to replicate the same behavior from my end by following your steps so we'll have to dig more into this issue, for reference I've added output log below Hi,   Please take look into this issue ? Thank you.  Thank you for your cooperation and patience.","Hi , are you willing to use WSL instead? You will probably run into less issues if we go that route. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,erik-fauna,Build from source C doesn't produce .tar.gz archive," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.16  Custom code No  OS platform and distribution Linux Ubuntu 20.04 (arm64)  Mobile device _No response_  Python version _No response_  Bazel version 6.5.0  GCC/compiler version gcc  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The readme within the the lib_package folder indicates that a tar.gz archive should be created upon building with bazel to finish installation. Currently trying to build on arm64 architecture (jetson orin) as there are no docker images on dockerhub for Tensorflow C with that architecture. I would have preferred to use 2.15 to stay the same as the version used by the amd64 desktops, but that failed to build because of stubs being too large (using bazel 6.1.0, gcc, cxxopt flag used). There are no logs because the build claims it completed successfully, there are the usual shared library files within `bazelbin/tensorflow`, but porting them and all the dependencies to the right locations sounds like a nightmare. Any thoughts why the archive isn't being built?  Standalone code to reproduce the issue   Relevant log output _No response_",2024-06-05T20:37:39Z,stat:awaiting response type:bug type:build/install subtype: ubuntu/linux TF 2.16,closed,0,7,https://github.com/tensorflow/tensorflow/issues/69266,"fauna Bazel allows for conditional build rules based on configurations like target architecture. It's possible the archive creation step is defined only for specific architectures (e.g., amd64). Kindly search for conditional statements (""if"", ""select"") within the Bazel build files in the lib_package folder.  Also try to replace `your_target` with the specific build target defined in the Bazel files. Thank you!","I see that there are some differences in the build commands defined in the lib_package, so I've made some adjustments to my build line. It should finish building by the morning, so I can report back then. I added the verbose_failures flag as well.","fauna Yes, please! Let us know kindly. Thank you!","This worked to get the tar.gz archive. `../bazel6.5.0linuxarm64 build verbose_failures jobs=12 cxxopt=""D_GLIBCXX_USE_CXX11_ABI=1"" c opt //tensorflow/tools/lib_package:libtensorflow` For anyone who comes across this looking for number of jobs, bazel maxes out at 8.  I guess rather than just building //tensorflow:libtensorflow.so, the entire library package was needed. I will eventually go and checkout 2.15 and see if the package for that builds successfully (with bazel 6.1.0), because that failed when trying to build the .so file directly.",fauna Please let us know if the issue got resolved and if not then kindly share the error log to analyze the issue? Thank you!,"Using the code from my previous comment, the bug that I specifically raised this issue about is resolved. I haven't tested the other packages to confirm those work, but will raise another issue if they don't, as it's not exactly the same issue seen here.",Are you satisfied with the resolution of your issue? Yes No
yi,cpappasILMX,Unable to build TensorFlowLite GPU Delegate for Android," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15  Custom code No  OS platform and distribution Android  Mobile device Android  Python version 3.9  Bazel version 7.1.2  GCC/compiler version Apple Clang 15.0.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On a Mac machine, I am trying to build the tensorflowlite gpu delegate for android. I have followed the build instructions without docker found at https://www.tensorflow.org/lite/android/lite_build to build tflite for android and was successful. I attempted to build the native gpu delegate as a shared library as well following the instructions found at https://www.tensorflow.org/lite/android/delegates/gpu_native. The goal is to build the delegate as a shared library to simply copy over into my own project. The problem appears to be when building a dependency of the delegate, dependency declarations are missing.  Standalone code to reproduce the issue   Relevant log output ",2024-06-05T17:31:46Z,stat:awaiting response type:bug stale comp:ops TF 2.15,closed,0,5,https://github.com/tensorflow/tensorflow/issues/69252, Please make sure that Android NDK has a version compatible with your target Android API level. Bazel build system. Could you check TensorFlow documentation for specific version requirements for these tools for building the GPU Delegate? Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,I was able to build the gpu delegate using NDK version 25 with NDK API Level set to 26.,Are you satisfied with the resolution of your issue? Yes No,> I was able to build the gpu delegate using NDK version 25 with NDK API Level set to 26. can u share details how it work to compile for gpu ? not wokring for me . can u share ur text file may be thanks.
yi,copybara-service[bot],Improve reduction vectorization.,"Improve reduction vectorization.  v4 loads should not be used with f32 or larger inputs. I measured   significant performance degradation from this.  stop relying on unrolling for vectorization, so we can   remove the MayPreventVectorization heuristic. This enables   more vectorization, e.g. of variadic reductions.",2024-06-05T11:41:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69230
yi,PhyllisJi,Request for groups parameter support in Conv2DTranspose/Conv1DTranspose Layer," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.14.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I request the addition of the groups parameter to the tf.keras.layers.Conv2DTranspose layer. This feature is crucial for the following reasons: **Enhanced Model Flexibility and Efficiency:** The groups parameter allows for the implementation of grouped transposed convolutions, which can significantly enhance model flexibility and computational efficiency. By splitting the input and output channels into separate groups, it enables the creation of more complex and diverse neural network architectures, such as those found in stateoftheart models like ResNeXt and MobileNetV2. **Consistency with Other Frameworks:** PyTorch, a widely used deep learning framework, supports the groups parameter in its ConvTranspose2d layer. Including this feature in TensorFlow would align its functionality with PyTorch, facilitating easier migratio",2024-06-05T03:28:56Z,stat:awaiting response type:support stale comp:keras TF2.14,closed,0,4,https://github.com/tensorflow/tensorflow/issues/69201,"Hi **** ,  Sorry for the delay, Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,ngostin,Module Not Found: yggdrasil_decision_forests.model.gradient_boosted_tree," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Mac OS Big Sur 11.3.1  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I've been wrestling trying to install/implement some form of Decision Forests for weeks now, and keep running into issues with Both ydf and tfdf. Both issues seem to happen on import. For tfdf, I get a module not found error, relating to yggdrasil_decision_forests (less related, the issue I'm having with ydf gives an error ""no suitable image found"" for ydf)  Standalone code to reproduce the issue   Relevant log output ",2024-06-05T02:31:54Z,stat:awaiting response type:support stale comp:apis TF 2.16,closed,0,4,https://github.com/tensorflow/tensorflow/issues/69200,", I tried to import the tensorflow_decision_forests in the latest version Tensorflow 2.16.1 and could not face any error/issue while importing. Kindly find the gist of it here.  Also there is another repo for the TensorFlow decision forests for the quick resolution. Please try to post in that repo. https://github.com/tensorflow/decisionforests/issues Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Slightly modernized the annotation in xla_client.pyi,Slightly modernized the annotation in xla_client.pyi,2024-06-04T10:08:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69136
rag,copybara-service[bot],Make fragile autotuner tests work / disable them on Hopper,Make fragile autotuner tests work / disable them on Hopper,2024-06-04T10:07:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69135
yi,PhyllisJi,errors in the descriptions of the parameters in the documentation for tf.keras.layers.Conv2DTranspose," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.14.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The current documentation describes dilation_rate parameter as：  And when we set dilation_rate to a tuple, the error message is reported as：  In addition, the documentation describes _strides_ as having to be integers, but the code actually constrains them to have to be positive integers.   The constrains of output_padding and kernel_size are also not mentioned.   Standalone code to reproduce the issue    Relevant log output _No response_",2024-06-04T02:40:48Z,type:docs-bug stat:awaiting response stale comp:keras,closed,0,4,https://github.com/tensorflow/tensorflow/issues/69098,", In the **tf.keras.layers.Conv2DTranspose**  for the strides it is mentioned that int or tuple/list of 1 integer, specifying the stride length of the transposed convolution. **strides > 1** is incompatible with **dilation_rate > 1** and for the dilation_rate  int or tuple/list of 1 integers, specifying the dilation rate to use for dilated transposed convolution. https://keras.io/api/layers/convolution_layers/convolution2d_transpose/ Please feel free to  contribute with PR for the changes needed in the document. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Introduce `Client::CopyArrays()` for batched device-to-device copy,"Introduce `Client::CopyArrays()` for batched devicetodevice copy This CL introduces an IFRT client API that transfers a list of IFRT arrays to a set of new devices and memory kind. The API is intentionally designed to impose a constraint that all arrays in a single call must share the same destination devices and memory kind. The rationale is that we don't expect batching benefit across transfers that do not share the same set of source/destination devices. Callers can always invoke `CopyArrays` multiple times if they have multiple sets of destination devices. This API is intended as a batched version of `Array::Reshard()` (now marked as deprecated), but with a narrower interface that allows just device and memory kind changes and not ""resharding"". Since none of the existing IFRT implementations currently implement true resharding, we expect `Client::CopyArrays()` to be a sufficient replacement of `Array::Reshard()` for the current use case. The actual resharding is a much more involved operation that may deserve an API that is closer to execution than transfer and can be reintroduced with a separate design. Since `CopyArrays` takes a device list instead of a sharding object, `Sharding` objects of the r",2024-06-04T02:01:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69096
rag,marcelobenedito,TFLite Select OPS aar build failed," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.12  Custom code No  OS platform and distribution Windows 10  Mobile device _No response_  Python version Python 3.11.9  Bazel version 5.3.0  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The TFLite model I use in my Android project must use TF Select OPS aar. So, I would like to reduce its size. Then, I tried to follow the Set up build environment using Docker link. But the build fails while running tensorflow/lite/tools/build_aar.sh. I note that some downloads got 404 responses. But I don't know if it's causing that issue. Could you please help me? Thanks.  Standalone code to reproduce the issue   Relevant log output ",2024-06-03T22:14:17Z,stat:awaiting response type:build/install stale comp:lite,closed,0,5,https://github.com/tensorflow/tensorflow/issues/69080,"I cleaned Bazel by running ""bazel clean expunge"". Then the download warning didn't display anymore. But the build failed with the same error. I checked ""dx.jar"" and it doesn't exist though.","Hi  , If possible , can you share me your model file because I am trying to replicate your issue and everything ran fine till the step 4 of the instructions that you are following. ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,x0w3n,Aborted in `tf.reduce_mean` occurs when gpu is not available," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.3 LTS (x86_64)  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The crash occurs when calling tf.reduce_mean when my machine has no gpu. interestingly when I run the code under the latest TensorFlow Nightly version on the colab, the colab throws an error instead of a crash. I'm not sure if the cause of this problem is because of my python version, or some other environmental effect, I'm running the code on the same machine with tf version 2.16 which also causes a crash.  Standalone code to reproduce the issue   Relevant log output ",2024-06-03T15:15:20Z,stat:awaiting response type:bug stale comp:ops TF 2.16,closed,0,7,https://github.com/tensorflow/tensorflow/issues/69054,"I pulled the latest tensorflow through docker again and then after running the code, the results were the same with the following output: !image",", I tried to execute the mentioned code on tensorflow v2.15, v2.16, tfnightly on GPU and CPU with colab and other Linux environement, and observed that it is not anorted as mentioned, and it is providing the error message which was expected. Kindly find the gists of both environments and the error log of the linux environments as well. ","Thank you! I've noticed this. I ran the code in colab and it really didn't trigger a crash. and pulled the latest tensorflow and ran the code using docker on a new physical machine (no gpu) and it didn't trigger a crash either. however, on my original machine(with gpu but installed tensorflowcpu version ) I still got an error running this string of code. Here's the environment info I've gathered, hope it's useful. ",", As mentioned, the tf.reduce_mean is not aborted and it is failing with the error which is expected. As the error(convolution input must be 4dimensional) mentioned the API is expecting the input as the 4dimensional. Could you please try with the expected input and try to execute.  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Use mlir_converter::ApplyIndexing in loop_mlir.,"Use mlir_converter::ApplyIndexing in loop_mlir. Currently, we still generate multiresult apply_indexing ops there. We need to ban this.",2024-06-03T14:35:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/69053
yi,FutureGoose,Utilize GPU for tf 2.15," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15  Custom code No  OS platform and distribution WSL Ubuntu 22.04.3 (Windows 10)  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 11.8/8.6  GPU model and memory 4095MB NVIDIA GeForce RTX 2070 SUPER  Current behavior? TensorFlow is not finding the GPU.  Standalone code to reproduce the issue   Relevant log output  Some other, perhaps, relevant information:   ```",2024-06-03T10:53:12Z,stat:awaiting response type:build/install stale subtype:windows TF 2.15,closed,0,9,https://github.com/tensorflow/tensorflow/issues/69042," Please ensure you have a compatible Nvidia GPU and drivers installed. Also kindly verify that the paths to CUDA, cuDNN, and TensorFlow DLLs are added to your system's environment variable PATH as well. Thank you!","Thanks for the reply ! Yes I put the nvidiasmi output above. You mentioned the environment variable path, and I take it as .bashrc in WSL. However when you say ""system"" and ""as well"" it makes me think you mean on the windows side?","Hi **** ,  Apologies for the delay. Please try using TF version 2.17.0 and let us know if the issue persists. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"> Hi **** , >  > * Apologies for the delay. Please try using TF version 2.17.0 and let us know if the issue persists. >   Thank you! The specific project requirements is 2.15 unfortunately.","Hi @**FutureGoose** , Apologies for the delay. There are some known issues with GPU support in TensorFlow 2.15.0, which is why we recommend using the latest versions. Many bugs have been fixed in the newer releases, so upgrading will likely provide better results. https://www.tensorflow.org/install/source_windows Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,fsamekl,Running an Integrated Image Segmenter in Java,"The segmentation model I trained using YOLOv5S was converted to tflite and deployed using TensorFlow Lite on Android. However, when it ran, it reported an error and crashed, which seems to have two issues. The first one is: Java. lang. IllegalArgumentException: Error occurred when initializing ImageSegment: Image segmentation models are expected to have only 1 output, found 2 The second one: Error getting native address of native library: task_vision_jni, I would like to ask how to solve this problem? thanks Here is my code: public class MainActivity extends AppCompatActivity {     private ImageView iv_Y_result, iv_H_result;     private TextView tv_result;     private Button btn_result;     private String MODEL_YOLOV5SSEG = ""yolov5sseg.tflite"";     private Context context;          protected void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);         setContentView(R.layout.activity_main);         context = this;         iv_H_result = findViewById(R.id.iv_H_result);         iv_Y_result = findViewById(R.id.iv_Y_result);         btn_result = findViewById(R.id.btn_result);         tv_result = findViewById(R.id.tv_result);         Bitmap bitmap = BitmapFactory.decodeResour",2024-06-02T13:02:07Z,stat:awaiting response stale comp:lite TFLiteConverter Android,closed,0,3,https://github.com/tensorflow/tensorflow/issues/69021,"Hi  , Can you also provide me the tflite model please so that i can replicate the issue ?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,copybara-service[bot],"Adds a validation check to BasicStringArray to ensure that the underlying buffers an array, when they get ready, are consistent with the sharding specified at the time of its (array's) construction.","Adds a validation check to BasicStringArray to ensure that the underlying buffers an array, when they get ready, are consistent with the sharding specified at the time of its (array's) construction.",2024-05-31T22:53:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68992
yi,copybara-service[bot],PR #11583: Weight offloading of Jax memories: support memory_kind for GPUs,"PR CC(tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[49] = 60000 is not in [0, 60000)): Weight offloading of Jax memories: support memory_kind for GPUs Imported from GitHub PR https://github.com/openxla/xla/pull/11583 The first step to implement weight offloading is to support the existing host memory kind for GPUs: pinned_host.   Below are examples of the supported Jax APIs:  The next step will be supporting Jax API of device_put() via PjRtStreamExecutorClient::BufferFromHostBuffer() in XLA. Each tensor can only have one memory space or memory kind. This will be implemented in PjRtBuffer to get the mapped memory space from NamedSharding() or with_memory_kind(). Copybara import of the project:  7ac05a767e23755f8505bef9d1f077d1a293ab0c by Jane Liu : Support NamedSharding with memory_kind by adding host memory spaces. Merging this change closes CC(tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[49] = 60000 is not in [0, 60000)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/11583 from zhenyingliu:weightoffloading 7ac05a767e23755f8505bef9d1f077d1a293ab0c",2024-05-31T18:49:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68968
llama,copybara-service[bot],Change norm sharding for llama2-7b to fsdp.,Change norm sharding for llama27b to fsdp.,2024-05-31T13:29:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68952
yi,copybara-service[bot],[IFRT] Add an IFRT API for Topology.,"[IFRT] Add an IFRT API for Topology. Also add a (currently unused) Compile() method to ifrt::Compiler that produces an ifrt::Executable given a ifrt::Topology. At the moment this is a thin wrapper around the the PJRT TopologyDescription API, and does not attempt to change that API or its semantics. The change prepares for allowing the two to diverge. Add a PJRTIFRT implementation of ifrt::Topology that is for now a direct wrapper of PjRtTopologyDescription.",2024-05-30T15:37:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68884
yi,copybara-service[bot],[XLA] Limit DCE MoF cleanup to GPU backend,"[XLA] Limit DCE MoF cleanup to GPU backend We've had other backends blowup due to modifying the fusion body postfusion, as now the fusion metadata is incorrect. Reverts c82138f5a838ca67b55697d12a137b4662c08525",2024-05-29T22:25:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68847
rag,copybara-service[bot],[xla:cpu] Pack constants of sub-byte element type into dense storage compatible with XLA format,[xla:cpu] Pack constants of subbyte element type into dense storage compatible with XLA format + added a few more tests that now pass with thunks runtime,2024-05-29T19:16:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68833
yi,copybara-service[bot],PR #13018: [ROCm] Distinguish between AMD and NVIDIA GPUs with relevant tags,PR CC(Add `Dataset.from_variable_length` to accept numpy arrays with varying length): [ROCm] Distinguish between AMD and NVIDIA GPUs with relevant tags Imported from GitHub PR https://github.com/openxla/xla/pull/13018 Add `requiresgpuamd` for AMD gpus and unit tests that require AMD backend. Use `requiresgpunvidia` for any nvidia gpu backend. Copybara import of the project:  8c3c52760c37b6c9fc17910174df6bfa396b5b5e by Harsha HS : [ROCm] Distinguish between AMD and NVIDIA GPUs with relevant tags  0bf9ab80d06284883df19f17f421eb6a691c0dd8 by Harsha HS : Change ALL_GPU_BACKENDS to GPU_BACKENDS Merging this change closes CC(Add `Dataset.from_variable_length` to accept numpy arrays with varying length) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13018 from ROCm:ci_amdtags_20240523 0bf9ab80d06284883df19f17f421eb6a691c0dd8,2024-05-29T16:13:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68815
yi,Calabashypr,Training model with the Poisson loss function and the Adam optimizer resulted in NaN loss," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.0.0  Custom code Yes  OS platform and distribution Ubuntu 20.04.4 LTS  Mobile device _No response_  Python version 3.6.5  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA Version: 12.2  GPU model and memory _No response_  Current behavior? When I tried to train model with the Poisson loss function and the Adam optimizer,the loss value of the model became NaN. The model file and test data are attached here.In order to simplify the process of identifying the bug, I have only included a portion of the code for the forward and backward propagation. 000116.zip  Standalone code to reproduce the issue   Relevant log output _No response_",2024-05-29T14:06:36Z,stat:awaiting response type:support stale TF 2.0,closed,0,6,https://github.com/tensorflow/tensorflow/issues/68806,"Hi **** ,  It looks like you are using an older Version of Tensorflow (2.0). Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version (2.15.0 or 2.16.1) and let us know if the issue still persists? Thank you!","> Hi **** , >  > * It looks like you are using an older Version of Tensorflow (2.0). Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version (2.15.0 or 2.16.1) and let us know if the issue still persists? >   Thank you! Hello,I have updated the Version of Tensorflow to 2.15.0,but I got an ERROR below: `[ERROR] Crash when training model with tensorflow Traceback (most recent call last):   File ""train.py"", line 254, in      model, input_objects_names, output_layers_names, x, y, ins, ins_value = __prepare(flags.loss, flags.optimizer,   File ""train.py"", line 69, in __prepare     ins = model._feed_inputs + model._feed_targets + model._feed_sample_weights AttributeError: 'Functional' object has no attribute '_feed_targets' ` Could you help me solve the problem?","Hi **** , Apologies for the delay. The code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? or Could you please share the Colab gist with all dependencies? Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
gemma,copybara-service[bot],[XLA:GPU] Use priority fusion in TritonGemmAutotunerExtractor.,[XLA:GPU] Use priority fusion in TritonGemmAutotunerExtractor.,2024-05-29T13:20:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68803
yi,copybara-service[bot],PR #13018: [ROCm] Distinguish between AMD and NVIDIA GPUs with relevant tags,PR CC(Add `Dataset.from_variable_length` to accept numpy arrays with varying length): [ROCm] Distinguish between AMD and NVIDIA GPUs with relevant tags Imported from GitHub PR https://github.com/openxla/xla/pull/13018 Add `requiresgpuamd` for AMD gpus and unit tests that require AMD backend. Use `requiresgpunvidia` for any nvidia gpu backend. Copybara import of the project:  8c3c52760c37b6c9fc17910174df6bfa396b5b5e by Harsha HS : [ROCm] Distinguish between AMD and NVIDIA GPUs with relevant tags  0bf9ab80d06284883df19f17f421eb6a691c0dd8 by Harsha HS : Change ALL_GPU_BACKENDS to GPU_BACKENDS Merging this change closes CC(Add `Dataset.from_variable_length` to accept numpy arrays with varying length) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13018 from ROCm:ci_amdtags_20240523 0bf9ab80d06284883df19f17f421eb6a691c0dd8,2024-05-28T17:45:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68739
yi,copybara-service[bot],[XLA:GPU] Add a util to reset programs count and deflake `gpu_compiler_test`.,"[XLA:GPU] Add a util to reset programs count and deflake `gpu_compiler_test`. Previously, running `gpu_compiler_test` in different orders would yield different results for `GpuCompilerTest.CompiledProgramsCount`.",2024-05-28T15:29:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68728
yi,copybara-service[bot],Add missing newline in the output of run_hlo_module,Add missing newline in the output of run_hlo_module This is very annoying if one uses the tool frequently Reverts df40f8d088774e2e938896b7f7cf2e2051d00a3a FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/13088 from ROCm:ci_reduce_atomic_min f894f1954513019f0ca6890a27e09e0fee9d462e,2024-05-28T13:34:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68718
rag,383156098, There was no error when converting the lite model but an error occurred when calling the Interpreter allocate_tensors() method. It will appear if the Conv1D data_format parameter is set to channels_first and the dilation_rate parameter > 1," 1. System information  Windows 10 and Centos 7  Tensorflow 2.10.0 ~ 2.10.1   2. Code     conv1d = tf.keras.layers.Conv1D(data_format=""channels_first"",                                     filters=128,                                     kernel_size=3,                                     strides=1,                                     padding=""same"",                                     groups=128,                                     dilation_rate=2)     model = tf.keras.Sequential(conv1d)     model.build(input_shape=[1, 128, 2000])     model.summary()     converter = tf.lite.TFLiteConverter.from_keras_model(model)     converter.experimental_new_converter = True     converter.target_spec.supported_ops = [         tf.lite.OpsSet.TFLITE_BUILTINS,   enable TensorFlow Lite ops.         tf.lite.OpsSet.SELECT_TF_OPS   enable TensorFlow ops.     ]     tflite_model = converter.convert()     with open('../tflite_checkpoint/conv_tas_net.tflite', 'wb') as wb:         wb.write(tflite_model)     interpreter = tf.lite.Interpreter(model_content=tflite_model)     interpreter.allocate_tensors()     input_details = interpreter.get_input_details()     output_details = interpreter.get_output_details()  5. error logs     inter",2024-05-28T08:57:36Z,stat:awaiting response stale comp:lite TFLiteConverter TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/68713," Could you please try to upgrade to the latset TF version as you are using an older version which is not actively supported. If your application allows it, kindly consider changing the data_format parameter in your Conv1D layer to channels_last. This is the more common format for TFLite and might avoid the allocation issue. Thank you!","import tensorflow as tf  Define the Conv1D layer with 'channels_last' conv1d = tf.keras.layers.Conv1D(data_format=""channels_last"",                                 filters=128,                                 kernel_size=3,                                 strides=1,                                 padding=""same"",                                 groups=128,                                 dilation_rate=2)  Build the model model = tf.keras.Sequential([conv1d]) model.build(input_shape=[1, 2000, 128])   Update input shape accordingly model.summary()  Convert to TFLite model converter = tf.lite.TFLiteConverter.from_keras_model(model) converter.experimental_new_converter = True converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS,     tf.lite.OpsSet.SELECT_TF_OPS ] tflite_model = converter.convert()  Save the TFLite model with open('conv_tas_net.tflite', 'wb') as wb:     wb.write(tflite_model)  Load and allocate tensors with TFLite interpreter interpreter = tf.lite.Interpreter(model_content=tflite_model) interpreter.allocate_tensors() input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() print(""Model loaded and tensors allocated successfully."")",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,zhanghuicuc,dynamic input shape with InferenceRunner," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16  Custom code No  OS platform and distribution android  Mobile device android  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm trying to use tflite with opengl ssbo, just like the following code does https://github.com/googleaiedge/mediapipe/blob/master/mediapipe/calculators/tensor/inference_calculator_gl_advanced.cc. https://github.com/googleaiedge/mediapipe/blob/master/mediapipe/util/tflite/tflite_gpu_runner.'m using `InferenceRunner` and `InferenceBuilder`, just like `ResizeInputTensor`  when using `tflite::Interpreter`?  Standalone code to reproduce the issue   Relevant log output _No response_",2024-05-27T12:03:53Z,type:support comp:lite TF 2.16,closed,0,1,https://github.com/tensorflow/tensorflow/issues/68695,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[XLA:GPU] Use xla_gpu.apply_indexing in ir_emitter_triton.,[XLA:GPU] Use xla_gpu.apply_indexing in ir_emitter_triton. `xla_gpu.apply_indexing` is the new preferred way to emit IndexingMaps.,2024-05-27T11:32:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68691
yi,ViratCh04,     TypeError: len is not well defined for a symbolic Tensor (rnn_decoder_1/gru_1/Squeeze:0). Please call `x.shape` rather than `len(x)` for shape information.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to train my encoderdecoder(GRU) with attention mechanism but on calling the  `trainStep()`, tf gives this error where it claims to be unable to measure a tensor's length using len and refers me to use tf.shape() instead. How can one do such changes in a package's code on a cloud compiler if there is no workaround for this? Please let me know if I have to provide more resources to address this problem as this is my first raised issue on github  Standalone code to reproduce the issue   Relevant log output ",2024-05-27T10:38:01Z,stat:awaiting response type:bug comp:keras TF 2.15,closed,0,3,https://github.com/tensorflow/tensorflow/issues/68690,",  Looks like this is an issue from the Keras side. Also there is an issue raised in the Kerasteam/Keras repo for the similar error on the RNN_decoder/gru and assigned to the Developer. Kindly take a look and try to follow the same for the quick response. Thank you!",Oh alright . Thanks for your response. I will try to contribute towards this issue there then!,Are you satisfied with the resolution of your issue? Yes No
yi,Kullka,TFLite for LSTM: Downscale accumulation from 32-bit to 16-bit before applying to activation,"Hello everyone, I have converted LSTM model from TensorFlow to TensorFlow Lite. I realized that Inputs and Weights (include input_input(forget, cell, output)_weights and recurrent_input(forget, cell, output)_weights) are quantized to Int8 but Biases are quantized to 32bit. After performing Weights*Inputs + Biases, I have 32bit accumulation. These results are applied to activation (Sigmoid and Tanh) but the input of these activation is 16bit. I wonder how to downscale the accumulation from 32bit to 16bit before applying activation. Can you help me. Thanks!",2024-05-26T10:07:03Z,stat:awaiting response stale comp:lite TFLiteConverter,closed,0,5,https://github.com/tensorflow/tensorflow/issues/68670,"hi  , Can you please share the code to reproduce the issue ? ","Hi , Thank you for responding me. I have converted LSTM model from TensorFlow to TensorFlow Lite by following code: !image After converting, I used netron app to visualize the model and use the parameters on tflite model to implement inference LSTM on FPGA.  !image The Input and Weight are quantized as int8, so I found in https://github.com/tensorflow/tensorflow/blob/e33a5ea470d1ef02107b595737d23670f7a1eda4/tensorflow/lite/kernels/unidirectional_sequence_lstm.ccL1482L1497 the method lstm_eval::EvalInteger8x8_16 match with the model's parameters. In this method, I found the code here with many parameters and not sure how to get these parameters. https://github.com/tensorflow/tensorflow/blob/e33a5ea470d1ef02107b595737d23670f7a1eda4/tensorflow/lite/kernels/lstm_eval.ccL2438 I saw the way to find the parameter input_to_gate_effective_bias in method _prepare_ in file unidirectional_sequence_lstm.'t found anywhere.  Finally, assume I have all parameters, should I follow method https://github.com/tensorflow/tensorflow/blob/e33a5ea470d1ef02107b595737d23670f7a1eda4/tensorflow/lite/kernels/lstm_eval.ccL1420 and then https://github.com/tensorflow/tensorflow/blob/e33a5ea470d1ef02107b595737d23670f7a1eda4/tensorflow/lite/kernels/lstm_eval.ccL554 to understand the way to implement LSTM?  Correct me if I wrong! Thank you.","Hi , we're wondering if you may be able to resolve your issue by using AIEdgeTorch, you can find more information here: googleblog. I have actually created a simple script for converting an LSTM model here:  If you want to, you can actually try visualizing the result in modelexplorer as well. Please try them out and let us know if this resolves your issue. If you still need further help, feel free to open a new issue at the respective repos.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,copybara-service[bot],Implement `AssembleArrayFromSingleDeviceArrays` for BasicStringArray.,Implement `AssembleArrayFromSingleDeviceArrays` for BasicStringArray.,2024-05-25T18:47:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68661
yi,saad-koukous,can't crosscompile tensorflow lite c library using camke, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 22.04.4 LTS  Mobile device _No response_  Python version _No response_  Bazel version cmake version 3.22.1  GCC/compiler version gcc (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I need to use the tflite c library in a beaglebone black rev3 with this specs:  i'm trying to build the library using this toolchain : gccarm8.32019.03x86_64armlinuxgnueabihf so like this tutto https://www.tensorflow.org/lite/guide/build_cmake_arm i used this command first:  but then i have errors when i run `cmake build . j`   Standalone code to reproduce the issue   Relevant log output _No response_,2024-05-24T21:21:22Z,stat:awaiting tensorflower type:build/install comp:lite subtype: ubuntu/linux,closed,0,7,https://github.com/tensorflow/tensorflow/issues/68616,And I just test using Ubuntu 16.04.7 LTS and cmake version 3.20.5 but same problem,koukous Could you ensure your toolchain includes the necessary libraries and headers for building TensorFlow Lite on ARMv7? Please share the exact error messages or relevant excerpts if possible. Thank you!,"Hi , thank you for your response. I am using the same toolchain presented in this tutorial TensorFlow Lite Guide: Build with CMake for ARM for the ARMv7 case. Attached are the logs from the `cmake build . j` command. logs.log erros.txt","I tried building following your command on ubuntu 20.04.6, cmake version 3.16.3,  but i ran into the below error , can you please take a look   ","I ran into the same issue as , I have tested on master, nightly, and r2.17: complete reproduce steps: Install toolchain:  Install flatc:  Download tensorflow:  Build:  Hi , can you please take a look? Thanks.","Hi, koukous Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/62 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
yi,Nicholas-B1,Why does my GPU trained model on Jetson Nano preform so much worse than CPU., Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.4.1  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8.1  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Accuracy is staying the same when training on GPU but I want the speed of that training.  Standalone code to reproduce the issue   Relevant log output ,2024-05-24T18:44:55Z,stat:awaiting response stale comp:gpu type:performance TF 2.4,closed,0,6,https://github.com/tensorflow/tensorflow/issues/68610,"Hi **B1** ,  I tried to replicate your issue on colab, But it is asking directory. Could you please share the proper colab gist with all the dependencies to analyze more of it. Thank you!","I actually tried to run my model with CPU and it is giving the same bad results however, this model worked fine on my other machine that was running a newer version of Tensorflow. I had to downgrade to be able to use my GPU on my jetson nano. Is there anyone I could ask about this model issue? I have a feeling it maybe because of outdated libraries. So it was not a GPU issue after all.","Hi **B1** , It seems TensorFlow 2.4 might be causing the issue. This version is outdated and could be the reason for the poor results you are seeing. I strongly recommend upgrading to a newer TensorFlow version. Since your model worked fine on a newer version previously, this is likely the solution. And GPU is not related to tensorflow. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Introduce the support for the greedy (kMinimizeTpuCostPerRequest) batch policy.,Introduce the support for the greedy (kMinimizeTpuCostPerRequest) batch policy. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15340 from shawnwang18:shawnw/fix_2node_error 5f0be15d22aa01acaf8d7d1845690d8c9ffac438,2024-05-24T14:35:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68602
yi,x0w3n,Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence when call some methods of `tf.data`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.3 LTS (x86_64)  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When calling and iterating over the results of some of tf.data's methods it outputs ""Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence"".  Standalone code to reproduce the issue   Relevant log output ",2024-05-24T11:24:11Z,stat:awaiting tensorflower type:bug comp:data TF 2.16,open,0,22,https://github.com/tensorflow/tensorflow/issues/68593,", I tried to execute the mentioned code on the tfnightly GPU environment and haven't found out_of_range issues. Kindly find the gist of it here and let me know if it is working as expected. Thank you!","> , I tried to execute the mentioned code on the tfnightly GPU environment and haven't found out_of_range issues. Kindly find the gist of it here and let me know if it is working as expected. Thank you! It seems that colab is not outputting debug messages, I can see the debug messages output when I run it with the idea tool on a physical machine.  As the last line of the debug message in the image shows: !image",i have the same problem!!,", I tried to execute the mentioned code on the tensorflow 2.16 GPU environment and haven't faced the issue. Also I observed that the mentioned OUT_OF_RANGE is the warning which might not effect the execution of the code and the output. !Screenshot 20240604 122243 Thank you!","I think this output may be caused by gpu unavailability. Although it does not affect the execution of the code, the reason for this output is still unknown. Thank you!",", The above code was executed in the GPU environment, and the output was also expected. In the screenshot it was available. Could you please provide the GPU details which you are trying. I suspect it might the GPU issue for the different output in your machine. Thank you!","I'm going to add some color here, since I'm running into the same issue. Here is my code to load my TFRecords dataset (a dummy dataset I created for another issue):  Since I am using Keras, the first epoch looks like this: > 8/Unknown 6s 531ms/step  loss: 25.4941  mae: 4.4993  mse: 25.494120240614 14:08:00.703161: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence For the second Epoch, it then says the correct number of steps (8), but at the end of the epoch, I get the same warning thrown. So this raises a few questions. 1. How do I prescribe the number of files in my dataset before I get to fit? 2. What do I have to do to stop this warning? 3. Are TFRecords files not meant to be oneatatime? My true dataset has many larger values, with files around 30 MB/each. Before we get to the GPU issue, I'd point out  TF v2.16.1 Keras v3.3.3",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,I am running into the same issue. This issue was not there with tf 2.13. ,I am still running into the same issue in 2.17.0,"  Hit by this in 2.17.0 as well, It makes it impossible for newbies like me to go through the examples, like: https://www.tensorflow.org/tutorials/images/classification Must say, as this issue seems to have been here since May (!), I am a bit worried that Google prioritize Collab and other own offers a bit too much here? ","I am also bumping into this issue when trying to distribute my training over 4 GPUs, it seems to be working though and just act as a warning. TF = 2.17.0",I receive the same warning (error?) with TF 2.16.2 in local Conda environment with NVIDIA Quadro RTX 6000.,Issue also persits in new envrionment with tensorflow 2.18 and python 3.10,I have the same warning on TF 2.16 and python 3.10,I have the same warning on tf 2.17 and python 3.9.6,"Same on Python 3.12, TF 2.18","Also seeing this with TF 2.16.1, Keras 2, Python 3.11. It doesn't obviously impact training, though, but it's annoying. I'm using a tf dataset with caching, shuffling, repeating, prefetching, etc. My warning logs sound similar to what  described above.","Well, it impacted the recording of the training history, since the metrics loss etc alternated between 0 and the ?real? values. Beeing a relative newbie to machine learning, the bug made me so unsecure to continue with my script, i switched to pytorch.","Same issue trying to run multiGPU distributed training. Training proceeds, but only one GPU seems to being used, whilst other sits at idle."
yi,nikechase3,I was using tf lite and trying to get the examples but cant, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.11  Custom code No  OS platform and distribution _No response_  Mobile device Linux raspberrypi 6.1.21v7  Python version 3.7.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version   GPU model and memory _No response_  Current behavior? ive tried using the setup `sh setup.sh` but it will revert my opencv version to 4.5.x and wont detect it which in turn result in an import error. so i use an opencv with version 4.6.x and it shows the code under.   Standalone code to reproduce the issue   Relevant log output ,2024-05-24T07:47:39Z,type:bug,closed,0,6,https://github.com/tensorflow/tensorflow/issues/68585,"Hi  , Can you provide the code for detect.py or can you provide the link for the same , also can you tell me what is setup.sh about ?","it is on the tensorflow lite example repository, i tried yesterday, and it worked but i fear that next time the same thing will happen again","hi  , Can you please retry one more time in a different python enviornment and let me know .",i ve tried again and the results are unreliable between just opening the window on display without any output or just lagging,Are you satisfied with the resolution of your issue? Yes No,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Use apply_indexing for GEP index calculations.,Use apply_indexing for GEP index calculations. Reverts 1165601296e2baba5535c8f9ba7510e1373b9d6c,2024-05-23T10:56:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68523
yi,copybara-service[bot],Fold sequences of apply_indexing ops.,Fold sequences of apply_indexing ops. But only if this leads to simplification.,2024-05-23T10:02:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68516
yi,copybara-service[bot],Fold sequences of apply_indexing ops.,Fold sequences of apply_indexing ops. But only if this leads to simplification.,2024-05-23T06:21:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68496
yi,filip-halt,Android Tflite model fails to load on GPU Delegate: CL_OUT_OF_HOST_MEMORY," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version org.tensorflow:tensorflowlite:2.16.1  Custom code Yes  OS platform and distribution Android  Mobile device Samsung S23  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Currently trying to get a larger model to load on an S23 but I am running into OOM errors. When initializing an Interpreter using a GPUDelegate with factory options grabbed from CompatibilityList.getBestOptionsForThisDevice(), the Interpreter crashes with `Failed to apply delegate: Failed to build program executable  Out of host memoryError: Program not built!`.  This seems to pop up from an OpenCL error that is parsed with: https://github.com/tensorflow/tensorflow/blob/dd5c42638ac3c19c7facffb4c3cdadd2524cc6a5/tensorflow/lite/delegates/gpu/cl/util.ccL42 My best guess is that this is due to hitting the Dalvikheap memory limit of 512mb found on my device with Runtime.maxMemory(). I profiled the memory usage and it seems to crash around the 450mb  mark. Does Tflite on android not use native memory to g",2024-05-22T21:10:26Z,stat:awaiting response type:bug stale comp:lite type:performance TFLiteGpuDelegate Android TF 2.16,closed,0,10,https://github.com/tensorflow/tensorflow/issues/68470,"Hi halt , Can you please provide the tflite model file so that i can replicate the issue?","> Hi halt , >  > Can you please provide the tflite model file so that i can replicate the issue? You can find a copy here: https://github.com/filiphalt/tflite_bug It was too large to upload directly into this chat.","Turns out that this is most likely due to a conv2dtranspose layer in the model.  I was under the impression that conv2dtranpose was supported but I could be wrong.  Another interesting thing that happens is that when you convert the model with:  the resulting binary is 2x as large as float32 and 20% slower on mobile. When I inspected the graph with Netron, it looks like nothing was converted to float16, not even the conv2d's","Hi  halt , I ran your model using GPU delegate on dimensity 9000 and it ran fine without giving any issues.  Can you please try it out with a different device and let me know if it worked there. However the list of supported operators for tflite is here and TRANSPOSE_CONV is listed there.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,I believe that this is a grouped TRANSPOSE_CONV conversion problem. Tensorflow seems to barely support this and is what is breaking when converting onnx to tf. The default conversion creates a large amount of layers that ultimately cause an OOM on the phone when loading.  ,"Hi halt  , we're wondering if you may be able to resolve your issue by using AIEdgeTorch, you can find more information here: googleblog. I have actually created a simple script for converting a mobilenet model here:  If you want to, you can actually try visualizing the result in modelexplorer as well. Please try them out and let us know if this resolves your issue. If you still need further help, feel free to open a new issue at the respective repo.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],"Fix warp order calculations for MMA_V3 layouts. Hopper forces a [0,1] order for mma_v3 layouts (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#matrix-fragments-for-wgmma-mma-async-m64nnk8) while keeping a [1,0] order for thread layouts so these two things have to be kept separate.","Fix warp order calculations for MMA_V3 layouts. Hopper forces a [0,1] order for mma_v3 layouts (https://docs.nvidia.com/cuda/parallelthreadexecution/index.htmlmatrixfragmentsforwgmmammaasyncm64nnk8) while keeping a [1,0] order for thread layouts so these two things have to be kept separate.",2024-05-22T14:47:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68431
yi,copybara-service[bot],Clean up xla_gpu_ops.,"Clean up xla_gpu_ops.  Move & and * next to the type, where they belong, as   everyone knows.  fix `reserve` calls in FoldApplyIndexingOperands",2024-05-22T14:20:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68428
yi,ShabbirMarfatiya,RuntimeError when invoking TFLite INT8 model with tile operation,"I'm facing an issue while trying to run inference with a TensorFlow Lite model quantized to INT8 precision. The model was trained using CenterNet MobileNet for hand keypoint detection, and I'm getting a `RuntimeError` when invoking the interpreter, with the following error message:  Environment:  TensorFlow version: 2.7.0  TensorFlow GPU version: 2.7.0  Python version: 3.8.10  Operating System: Ubuntu 20.04 Steps to Reproduce: 1. Train a CenterNet MobileNet model for hand keypoint detection 2. Convert the trained model to TensorFlow Lite INT8 precision using the following code:  3. Run inference with the converted INT8 model using the following code:   Expected Behavior: The TensorFlow Lite INT8 model should run inference successfully without any errors. Actual Behavior: The RuntimeError is raised when invoking the interpreter, indicating that the tile operation is not supported in INT8 precision. Additional Information: I've followed the recommended steps for INT8 quantization, including setting the optimization flags, providing a representative dataset, and setting the target specs for INT8 operations. However, the issue persists.     , Could you please assist me in resolving this issue? I would great",2024-05-22T11:53:06Z,type:bug comp:lite,closed,0,4,https://github.com/tensorflow/tensorflow/issues/68474,"Hi  , Thanks for filing an issue, Could you please file a bug in the TensorFlow where you can get the issue resolution faster.It is not related to the Model Garden models. Thanks.","Hi , Thanks for the reply. I have filed a bug at the TensorFlow repo but haven't received a resolution yet.","Hi  , There is already an issue with the same bug, so I am closing this one since its a duplicate. Please track this for reference .",Are you satisfied with the resolution of your issue? Yes No
gpt,copybara-service[bot],PR #12438: [XLA:GPU] add force inline option to get better llvm splits,"PR CC(Update Readme.md): [XLA:GPU] add force inline option to get better llvm splits Imported from GitHub PR https://github.com/openxla/xla/pull/12438 Add option to XLA to enforce inlining before llvm splitModule or set preserveLocals=False to get more balanced splits in parallel compilation case. Some data of GPT3 5B model with different setting:  However, the runtime per step of perserve_locals=False vs other three setup is 1.4s vs 1.0s. Copybara import of the project:  5f61718691d265c8609a89dc2349b1362045f7c5 by Cjkkkk : add force inline and no preserve local option to get better llvm splits  f32419eb792206de332be481bc59650c9d37a564 by Cjkkkk : remove preserveLocals option Merging this change closes CC(Update Readme.md) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/12438 from Cjkkkk:force_inline_before_split f32419eb792206de332be481bc59650c9d37a564",2024-05-22T09:05:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68418
yi,copybara-service[bot],Fix auto_sharding_runner dependencies.,Fix auto_sharding_runner dependencies. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/12912 from openxla:aportnoy/improvefficapidatatypeerrormessage 32908d2d90a89269c655dff6d999bfc974f405a9,2024-05-21T23:28:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68395
yi,copybara-service[bot],PR #12912: Overload operator<< for XLA_FFI_DataType,PR CC(TSNE Suddenly Not Working in Projector): Overload operator: Overload operator<< for XLA_FFI_DataType Merging this change closes CC(TSNE Suddenly Not Working in Projector) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/12912 from openxla:aportnoy/improvefficapidatatypeerrormessage 32908d2d90a89269c655dff6d999bfc974f405a9,2024-05-21T22:28:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68392
yi,copybara-service[bot],Split sum-modification from sum-visitation.,"Split summodification from sumvisitation. Currently, we're both computing values and modifying sums in one step, which is not very nice. This splits the modification parts from the traversal parts where feasible. There's some almostduplication in the mod/div simplifiers which could probably be cleaned up, but so far I failed to find a good name for it.",2024-05-21T14:55:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68343
yi,MohanKrishnaGR,Couldn't resolve TF-TRT Warning: Could not find TensorRT,"Couldn't resolve TFTRT Warning: Could not find TensorRT I'm using wsl2 in windows 11.  Previously, I wasn't able to have GPU as the backend, I had tried all the methods in installing tensorflow in wsl.  But, trying with python3.11 and tensorflow  2.15.1, got me access to the GPU backend.   versions:  nvidiasmi:   I've tried all ways of installing tensorrt  pip, .deb, .tar; updated path variable, etc. Its importing in python shell  All the files are available (.tar installation method)  I've also used the files from .tar method to install using pip:  But still I get this error  _Originally posted by  in https://github.com/tensorflow/tensorflow/issues/64809issuecomment2122328887_",2024-05-21T11:32:10Z,stat:awaiting tensorflower type:build/install wsl2 TF 2.15,open,1,11,https://github.com/tensorflow/tensorflow/issues/68335," The standard approach of installing TensorRT directly on your Windows host might not work for WSL 2. Instead, you'll need to install it within the WSL 2 environment itself. Please make sure to follow these instructions.  Within your WSL 2 environment, please print the value of LD_LIBRARY_PATH to confirm it includes the TensorRT directory and let us know? Thank you!","I still did not solve TensorRT problem (""TFTRT Warning: Could not find TensorRT"") WSL2, Ubuntu 22.04, RTX 3090, Python 3.11, TensorFlow 2.15.1 (I installed TensorFlow as `pip install tensorflow[andcuda]==2.15.1`) Once I execute   I get the following error  So, obviously, I need 8.6.1 version of TensorRT and I downloaded ""TensorRT8.6.1.6.Linux.x86_64gnu.cuda12.0.tar.gz"" After extract it, moved folder to ""~/local/opt/TensorRT8.6.1.6"" For example,   Following Nvidia's installation instruction (https://docs.nvidia.com/deeplearning/tensorrt/installguide/index.htmlinstallingtar) I did  I added followings after open `sudo nano ~/.bashrc` export LD_LIBRARY_PATH=/home/enhorse/local/opt/TensorRT8.6.1.6/lib:$LD_LIBRARY_PATH export TensorRT_ROOT=/home/enhorse/local/opt/TensorRT8.6.1.6 When I execute the commend `strace e open,openat python c ""import tensorflow as tf"" 2>&1 O_CLOEXEC) = 3 20240528 11:41:21.209619: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT ``` I still have no idea what I can do more. Is there something wrong in my installation?", The library paths mentioned (/home/enhorse/local/opt/TensorRT8.6.1.6/lib) might be incorrect or not set properly in the environment variables. Kindly verify that and let us know? Thank you!,"Thanks,  ! for   I got:  But still the TRT Warning:  Could not find TensorRT exits: ",", Could you please try to execute the below commands and let us know if it working in Tensorflow v2.16.1  https://github.com/tensorflow/tensorflow/issues/64809issuecomment2147599600 Thank you!",>  This worked for me. But the other issue I am getting is my GPU is not detected despite no error on Tensorrt.,", Could you please raise the new request from here with all the details required for the quick resolution. Thank you!","Thank you very much   I resolved it by:  export TF_CPP_MAX_VLOG_LEVEL=3  Then, looking which library is not detected.  Finding where it is:    Adding that path to $LD_LIBRARY_PATH ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Same Warning on Ubunt22.04, python3.10, miniconda3, tensorflow 2.16.2 $ pip install tensorrt $ echo $LD_LIBRARY_PATH /home/rkuo/miniconda3/lib/python3.10/sitepackages/tensorrt_libs:/usr/local/cuda/lib64: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT","> , Could you please try to execute the below commands and let us know if it working in Tensorflow v2.16.1 >  >  , this helped. Thank you.  > strace e open,openat python c ""import tensorflow as tf"" 2>&1 TFTRT"" This was helpful to detect my issue which was solved by the above. Thank you  "
yi,copybara-service[bot],Replace all remaining uses of affine.apply with apply_indexing.,"Replace all remaining uses of affine.apply with apply_indexing. Also split indexing maps into one map per result, to not block LICM.",2024-05-21T09:38:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68332
rag,dependabot[bot],Bump requests from 2.31.0 to 2.32.0,"Bumps requests from 2.31.0 to 2.32.0.  Release notes Sourced from requests's releases.  v2.32.0 2.32.0 (20240520) 🐍 PYCON US 2024 EDITION 🐍 Security  Fixed an issue where setting verify=False on the first request from a Session will cause subsequent requests to the same origin to also ignore cert verification, regardless of the value of verify. (https://github.com/psf/requests/security/advisories/GHSA9wx4h78vvm56)  Improvements  verify=True now reuses a global SSLContext which should improve request time variance between first and subsequent requests. It should also minimize certificate load time on Windows systems when using a Python version built with OpenSSL 3.x. ( CC(Branch 143639671)) Requests now supports optional use of character detection (chardet or charset_normalizer) when repackaged or vendored. This enables pip and other projects to minimize their vendoring surface area. The Response.text() and apparent_encoding APIs will default to utf8 if neither library is present. ( CC(Deadlock when decoding TFRecords))  Bugfixes  Fixed bug in length detection where emoji length was incorrectly calculated in the request contentlength. ( CC(tensorflow.python.framework.errors_impl.NotFoundError: logs)) Fix",2024-05-21T05:56:07Z,size:S dependencies python,closed,0,2,https://github.com/tensorflow/tensorflow/issues/68314,Hi  Can you please review this PR ? Thank you!,"Looks like requests is uptodate now, so this is no longer needed."
yi,copybara-service[bot],Allow specifying compilation environment for ifrt compilation,Allow specifying compilation environment for ifrt compilation,2024-05-20T23:59:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68301
llm,copybara-service[bot],Add missing parentheses in XLA custom calls doc.,"Add missing parentheses in XLA custom calls doc. When reading through custom_call.md, I noticed these two missing parentheses.",2024-05-20T17:59:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68276
yi,copybara-service[bot],Update np.float_ and np.complex_ to match Numpy 2.0.,"Update np.float_ and np.complex_ to match Numpy 2.0. Numpy 2.0 removed the aliases np.float_ and np.complex_. Direct usages of np.float_ and np.complex_ are replaced with np.float64 and np.complex128, respectively.  See https://github.com/numpy/numpy/issues/24743 and https://numpy.org/devdocs/numpy_2_0_migration_guide.htmlchangestonamespaces. Consistent with JAX, TF retains dtype wrappers for float_ and complex_. This is considered useful for determining the default float/complex type, providing compatibility with older code that might still rely on these aliases.",2024-05-20T16:53:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68271
yi,copybara-service[bot],[PJRT][IFRT] Move topology discovery into PJRT-IFRT.,"[PJRT][IFRT] Move topology discovery into PJRTIFRT. Currently each PJRT backend is responsible for figuring out the global topology. This has several downsides: * it is a layering violation: PJRT is responsible for communicating with the devices local to each host, whereas IFRT is the layer that has a global view of a cluster. Topology discovery properly belongs at the IFRT layer. The current topology discovery is really useful only to the PJRTIFRT implementation of the IFRT API, because other distributed runtimes will have their own way to discover their cluster topology, and hence it should go there. * duplication: e.g., the CPU and GPU backends have almost identical code to discover topologies. If we move this code into PJRTIFRT, we can avoid redundancy. * inconsistency: currently the TPU implementation of the PJRT API does its own topology discovery. But this leads to an inconsistency: the process_id values attached to each device are determined by the TPU physical topology, and they don't match the process_id that the user otherwise sees in JAX. While it is easy for each PJRT client to figure out its own process_id, in order to correctly determine the mapping between nonaddressable TPUs and their p",2024-05-20T16:28:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68260
transformer,guillaumelorre28,ValueError: as_list() is not defined on an unknown TensorShape. during training, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17.0dev20240517  Custom code Yes  OS platform and distribution macOS Sonoma   Mobile device _No response_  Python version 3.10.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I get the following error when training a transformer for translation:   Standalone code to reproduce the issue   Relevant log output _No response_,2024-05-18T15:26:10Z,stat:awaiting tensorflower type:bug comp:ops TF 2.16,open,0,6,https://github.com/tensorflow/tensorflow/issues/68217,"Hi  , I have reproduced the reported issue.Attached gist for same.",", Looks like the custom layer sub classed from LSTM layer has a problem when loading in .keras format but with .tf format it is working fine. The problem exists in tfnightly also. Also there is an issue for the same error which is still open and assigned to the Developer. Could you please check and follow the same for the updates. https://github.com/tensorflow/tensorflow/issues/61270 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No, You may have answered the wrong issue. The issue is not related to LSTM and saving models.
yi,vivekjoshy,TPU v3-8 CrossReplicaSum_33 Error," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm getting this error when I run model.fit  I have a very large code base, and am unable to reproduce. I was using AUC metric, but removed it as seen here https://github.com/tensorflow/tensorflow/issues/33890. The issue still persists.  Another relevant issue is https://github.com/tensorflow/tensorflow/issues/41590. I have attached an MVCE example to reproduce, but it's still not enough to know that explicit shapes are needed as explained in CC(Jpeg decoding (for example when loading TFRecords from files) causes error on TPU when trying to fit a model) since I don't know where I'm using transpose. I need a stack trace to point to where in the code it's causing the issue.  Standalone code to reproduce the issue   Relevant log output _No response_",2024-05-18T11:04:38Z,type:bug comp:tpus TF 2.15,closed,0,5,https://github.com/tensorflow/tensorflow/issues/68210,"What's even more weirder is that I'm defining the input sizes explicitly like so:  ~But the batch size shown in the error log is 256 in [70, 256] which I assume is the transposed tensor.~ Ignore this since it appears that Hidden Size * 2 is also 256. So it could be a transpose inside a Dense layer. Edit: I've tracked it down to this code: ","It seems to be an issue with `tf.expand_dims(x, axis=1)`. Any axis other than 0 will cause this error. `tf.transpose` also causes this error. ",", When I tried to execute the code by explicitly setting the size after you decode, using tf.reshape, the code was executed successfully. While with the other approach it was executed with the error.   Kindly find the gist of it here.",Fixed by passing in explicit shapes everywhere.,Are you satisfied with the resolution of your issue? Yes No
yi,declanoller,Memory leak from using tf.constant in loop in TF 2.16," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary from pip  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I believe I tracked down a memory leak in my code to a `tf.constant` creation in a short lived object of a class. I can reproduce it in an even simpler way by just creating the constant in a loop. No `.function` decoration or model training necessary to cause it. It seems to happen if I replace the `tf.constant` with a `tf.random.uniform()`. I tried suggestions I've seen elsewhere, like trying to use `del` on the variable followed by `gc.collect()`. Some similar looking bug reports:  https://github.com/tensorflow/tensorflow/issues/57982   https://github.com/tensorflow/tensorflow/issues/50765  https://discuss.tensorflow.org/t/tensorflowmemoryleakduringinferenceinloop/23044 It looks like upgrading to 2.15 solved it for some people, but it appears to be back in 2.16? is this expected and I'm doing something wrong?  St",2024-05-18T04:43:28Z,stat:awaiting tensorflower comp:ops type:performance TF 2.16,open,1,4,https://github.com/tensorflow/tensorflow/issues/68196,"Hi  , I have tested the code in `tfnightly` (dev20240520) and found no memory leak. Please refer to attached gist.",Tested with Tf2.16 also and found no memory leak.Gist for reference.," what python version? I have two machines. Both have TF 2.16.1. However, running the same code on the one with python 3.10 does *not* leak, while the machine (in the OP) with 3.12 does leak. edit: I added a `print(sys.version)` to your gist, it looks like it's using 3.10:  Can you test it with 3.12 somehow?","I have also encountered this bug. It can be triggered using the following code.  Once the code is run, memory usage can be monitored using a utility such as HTOP. I have tested the code on the following x86/64 virtual machines on Google Cloud and found the following results. "
yi,copybara-service[bot],[XLA] Refactor HostOffloader.,[XLA] Refactor HostOffloader. Change HostOffloader's algorithm for identifying host memory offloading. This approach supports every conceivable host memory offloading pattern (as of today).,2024-05-17T23:41:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/68082
yi,allogic,"Tensorflow Generate ""ops_to_register.h"" Without Graph"," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf.2.16.1  Custom code No  OS platform and distribution Windows 11 x64  Mobile device _No response_  Python version 3.13  Bazel version 6.5.0  GCC/compiler version MSVC 19.39.33520 for x64  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm trying to build tensorflow as a static library. When I create the root scope it tells me that I didn't register any operators or kernels. Specifically the ""NoOp"" is required. I've read that with the tool ""tensorflow/python/tools/print_selective_registration_header"" I can generate the missing header file ""ops_to_register.h"" which registers all operators and kernels when I build the C++API again with the flag cxxopt=”DSELECTIVE_REGISTRATION”. But I need a graph definition in order to produce the header file. I don't have a graph since I want to build my model in C++ instead. What can I do to register all operators and kernels. (I don't care about file size!)  Standalone code to reproduce the issue   Relevant log output _No response_",2024-05-17T21:32:38Z,stat:awaiting response type:build/install stale comp:ops subtype:windows TF 2.16,closed,0,7,https://github.com/tensorflow/tensorflow/issues/67960,"Hi **** ,  Sorry for the delay, Here i am seeing compatibility mismatch. Could you go through this documentation once? Please let us know if issue still persists. Thank you!","Look, I was going through the documentation for a whole week and still could not use tensorflow as a simple static library. Here is a simple step by step guide to reproduce the behavior I'm experiencing. NOTE: `session_header.lib` is not being built by default. It is only registered in bazel tests, but is not linked into the final binary!  It seems clang is the preferred compiler starting with tensorflow 2.16.1. But when building with clang using the following command, clang produces a linker error which I have never encountered. It seems to be a problem on the LLVM site. NOTE: I've not tested newer versions of clang, only the one described in the documentation!  Regardless, when I build with MSVC on the other hand, it works as expected. It generates all the static libraries that I can link against without error. The monolithic option is described in the `.bazelrc` file and is used to create a mostly static build. It also states that it will DISABLE modular op registration which is the problem I am facing currently. All tho it ""states"" it will be disabled, it does not. Modular op registration is still enabled and my guess is that all the operators get optimized away during the build to safe binary size.  Last but not least I'm generating the headers.  Here are all the software versions that I use. They are strictly limited to the versions described in the documentation!  I'm forced to leave it like this as I have no proper experience with Bazel and Tensorflow as a whole. But I would be happy if someone could explain to me what I'm missing or doing wrong. It can't be that big of a problem, since the modular op registration has to be disabled somewhere...","Hi , I am not able to reproduce the error. It is passing on my end Please try the command below bazel build config=win_clang  config=monolithic repo_env=TF_PYTHON_VERSION=3.11 //tensorflow:tensorflow.lib","Hi **** ,  Sorry for the delay. We need Python 3.9 to 3.12 for Tensorflow 2.16.1. However, you are using Python 3.13.0a6. Could you please change your Python version and let us know the update. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],Add unit test coverage for IFRT call op kernel impl,Add unit test coverage for IFRT call op kernel impl,2024-05-17T18:39:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67862
yi,copybara-service[bot],IsSimplifiedScatter should also return false if update_window_dims is unsorted.,"IsSimplifiedScatter should also return false if update_window_dims is unsorted. We started relying on that with LayoutNormalization creating Scatters that don't have sorted update_window_dims, and want ScatterSimplifier to clean this up.",2024-05-17T13:05:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67840
rag,mciprian13,[TensorFlowLite] Add new data types in the TFLite format schema,"Add new data types in the TFLite format schema that could be leveraged by custom accelerators, such as:  Custom floating point format, having configurable number of bits for exponent and mantissa (something like **TensorType_FLOAT8** using custom format options) This is relevant only for the TFLite format (schema) used for model representation and not necessary for the TFLite tools or runtime. This is relevant when industry has proprietary tools that rely on the TFLite format but not on the TFLite tools or runtime (interpreter),",2024-05-17T09:28:12Z,stat:awaiting tensorflower type:feature comp:lite,closed,0,2,https://github.com/tensorflow/tensorflow/issues/67821,"Seems like a reasonable request, , can you please take a look? Thanks. ","Hi,   Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/63 Let us know if you have any questions. Thanks."
rag,copybara-service[bot],[pt composite lowerings],"[pt composite lowerings] Add lowering from composite for average pool when ceil_mode=true and count_pad=true Additionally, the following refactors: * Simplify padding string determination * Use intermediate struct to parse attrs into c types * Rename some functions to be more cleared where they are used * Constrain enum usage to within one function, so remove enum",2024-05-17T07:14:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67805
yi,copybara-service[bot],[IFRT] Use llvm::DenseSet instead of llvm::SmallSet when verifying devices.,[IFRT] Use llvm::DenseSet instead of llvm::SmallSet when verifying devices.,2024-05-17T04:40:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67791
yi,tanpengshi,"Unable to Force-load TensorFlowLiteSelectTfOps.framework, created with Selective Build, in iOS","IDE: Xcode 15 Platform: iOS17 TensorFlow version: r2.9 I am developing both iOS and Android apps that are running with TensorFlow Lite model. Because my model uses LSTM, I have to make use of TFSelectOps. In addition, because the TensorFlowLiteSelectTFOps library is large in memory size, I have to do a selective build. After much effort, I have succeeded in making my TensorFlowLite model running smoothly on the Android app based on the selectively built libraries. On the iOS however, I used a bazel build: bash tensorflow/lite/ios/build_frameworks.sh \   input_models=model1.tflite,model2.tflite \   target_archs=x86_64,armv7,arm64 to generate the:    1. TensorFlowLiteSelectTfOps.framework    2. TensorFlowLiteC.framework After that I edited the TensorFlowLiteSwift.podspec to include the framework: ``` Pod::Spec.new do    s.name             = 'TensorFlowLiteSwift'   s.version          = '2.14.0'   s.authors          = 'Google Inc.'   s.license          = { :type => 'Apache' }   s.homepage         = 'https://github.com/tensorflow/tensorflow'   s.source           = { :git => 'https://github.com/tensorflow/tensorflow.git', :tag => ""v{s.version}"" }   s.summary          = 'TensorFlow Lite for Swift'   s.descript",2024-05-17T04:34:35Z,stat:awaiting tensorflower type:build/install comp:lite TF 2.16,closed,0,12,https://github.com/tensorflow/tensorflow/issues/67790,"I would appreciate any working solution to this, as long i can run my model on Select Ops based on a TensorFlowLiteSelectTFOps library selectively built based on the tflite model itself. Thanks a lot in advance! :)","Hi , it looks like the system is unable to properly link the google protobuf library ... can you ensure you are following all the directions here correctly: https://www.tensorflow.org/lite/guide/build_iosusing_local_swift_or_objectivec_apis particularly I see that your path does not point to the TF root directory  Where as the source says:  Let me know if that works. Thanks.","I have tried the above and I am still getting the same error. In addition I did :  Then replace i replaced the TensorFlowLiteSelectTfOps.framework with my selectively build TensorFlowLiteSelectTfOps.framework, and I still get the same error when building! I suspect the error comes inherently from the selectively built framework. Are you able to replicate the error I get? Thank you!","Hi , are you willing to share your model(s) that are used to make your selective build? I'm wondering if any particular ops are causing the issue. Perhaps you can just make a dummy model which includes at least one of all the ops you are using and share that.","tflite_CNN_LSTM_dummy.zip Hi , here is the dummy model that contains all the ops!","Hi , I attempted to reproduce it with your model... I'm actually failing on the build the select framework step  considering you were able to build the Select framework, can you share the steps that you use to build it? I did this:  my output: ","Hi  I made a mistake, the tensorflow version should be r2.16 instead of r.2.14. When I used r2.16, I can successfully build the model, while r2.14 produced similar error.",Hmm I'm now currently running into a different issue... which python version are you using? I'm using an M1 and Xcode = 15.0.1. Any other potential environmental difference you can possibly think of? ,"My apologies again, if you use TensorFlow r2.16, you will run into the issue later on in Xcode: [](https://github.com/tensorflow/tensorflow/issues/63952) The TensorFlow version should instead be r2.9, with the .frameworks bazel installed with Python 3.9 installed with Anaconda with M3 and Xcode 15.3","Hey , as 2.9 is quite old at this point, I think we need to fix HEAD, going to change this into a build/install issue for the current state. I tried again in tfnightly, reproduce steps, on M1 Mac  my output:  Hi , can you please take a look? Thanks.","Hi,   Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/64 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
yi,ShabbirMarfatiya,RuntimeError when invoking TFLite INT8 model with tile operation,"I'm facing an issue while trying to run inference with a TensorFlow Lite model quantized to INT8 precision. The model was trained using CenterNet MobileNet for hand keypoint detection, and I'm getting a `RuntimeError` when invoking the interpreter, with the following error message:  Environment:  TensorFlow version: 2.7.0  TensorFlow GPU version: 2.7.0  Python version: 3.8.10  Operating System: Ubuntu 20.04 Steps to Reproduce: 1. Train a CenterNet MobileNet model for hand keypoint detection 2. Convert the trained model to TensorFlow Lite INT8 precision using the following code:  3. Run inference with the converted INT8 model using the following code:   Expected Behavior: The TensorFlow Lite INT8 model should run inference successfully without any errors. Actual Behavior: The RuntimeError is raised when invoking the interpreter, indicating that the tile operation is not supported in INT8 precision. Additional Information: I've followed the recommended steps for INT8 quantization, including setting the optimization flags, providing a representative dataset, and setting the target specs for INT8 operations. However, the issue persists.     , Could you please assist me in resolving this issue? I would great",2024-05-17T04:06:39Z,stat:awaiting response stale comp:lite TFLiteConverter,closed,2,11,https://github.com/tensorflow/tensorflow/issues/67789,"Hi  , I will replicate your issue and will get back to you.","Hey , Thanks for looking into this issue. If you need any more details, feel free to let me know.","Hi , Have you happened to find anything related to my issue that might be helpful?","Hi  , I am in the process of replicating your issue. Meanwhile did you try out with the latest tensorflow version , 2.16.1 ?","Hi , No, I didn't try that, but I'll try and let you know the outcome.","Hi  , I see that you have already filed a bug report for this, right? Anyway , just in case can you also also provide all the other assets like model file and dataset. ","Hi , I have uploaded all necessary folders and files to GDrive. Here's the link for you: https://drive.google.com/drive/folders/1Ov1qVyreDWq0q3VQbAq0BIvrxRSB4dRs?usp=sharing If you need any more details, feel free to let me know. ","Hi  , I am getting a lot of protobuf library issues. It will take me some time to resolve and replicate your issue. I will let you know once i am done. Thank you ","Hi  , we're wondering if you may be able to resolve your issue by using AIEdgeTorch, you can find more information here: googleblog. I have actually created a simple script for converting a mobilenet model here:  If you want to, you can actually try visualizing the result in modelexplorer as well. Please try them out and let us know if this resolves your issue. If you still need further help, feel free to open a new issue at the respective repo.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,sp00N221,Need Help with a Softmax Warning in TensorFlow 2.16," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hey everyone, I'm running into a bit of a headache with TensorFlow 2.16 and could really use some help. I'm getting this annoying warning about a Softmax operation over an axis with size 1. This pops up when I'm using a custom TransformerBlock layer that includes MultiHeadAttention. What I've Tried: Debugging Dimensions: Added print statements to check tensor shapes at different stages. Used tf.squeeze to remove dimensions of size 1 before passing the tensor to MultiHeadAttention. What I Need: Is this a bug in TensorFlow 2.16? If yes, any workarounds or patches? Best practices for handling tensor dimensions in MultiHeadAttention to avoid this? Should I downgrade or wait for an update? If yes, which version should I try? Additional Info: Using LSTM and GRU layers followed by the custom TransformerBlock. Running on Windows with Pytho",2024-05-16T18:35:33Z,stat:awaiting response type:bug stale subtype:windows TF 2.16,closed,0,7,https://github.com/tensorflow/tensorflow/issues/67758,"I am experiencing the same issue when implementing my own transformer encoder decoder. So far, i am still missing positional encoding and some masking layers, i don't know whether those would affect it in any way.  Here is my code, showing the same warning with tfnightly, and python 3.11.9     import keras     import numpy as np     def possitionalEmbedding(input_dim, output_dim):   TODO         return keras.layers.Embedding(input_dim=input_dim, output_dim=output_dim)     def model_func(encoder_vocab_len, decoder_vocab_len, encoder_maxlen, decoder_maxlen, params):         num_heads, key_dim, d_v, d_ff, d_model, n = params         encoder_input = keras.Input(shape=(None,))         decoder_input = keras.Input(shape=(None,))          encoder part         embedded = possitionalEmbedding(encoder_vocab_len, d_model)(encoder_input)  todo possitional embedding         embedded = keras.layers.Dropout(0.1)(embedded)         encoded = embedded         for i in range(n):             attended_encoded = keras.layers.MultiHeadAttention(num_heads,                                             key_dim,                                             dropout=0.1,                                             use_bias=True,                                             output_shape=(d_model,))(encoded, encoded, encoded)   todo padding_mask             attended_encoded_d = keras.layers.Dropout(0.1)(attended_encoded)             add = encoded + attended_encoded_d             normalised = keras.layers.LayerNormalization()(add)             fed_f = keras.layers.Dense(d_ff)(normalised)   feed forward 1 part             fed_ff = keras.layers.Dense(d_model)(keras.activations.relu(fed_f))   feed forward 2 part             fed_ff_d = keras.layers.Dropout(0.1)(fed_ff)             add2 = normalised + fed_ff_d             normalised2 = keras.layers.LayerNormalization()(add2)             encoded = normalised2   and the loop is repeated         encoder_output = encoded   output from encoder          decoder part         de_embed = possitionalEmbedding(decoder_vocab_len, d_model)(decoder_input)         de_embed = keras.layers.Dropout(0.1)(de_embed)         for i in range(n):             self_attention = (keras.layers.MultiHeadAttention(num_heads,                                             key_dim,                                             dropout=0.1,                                             use_bias=True,                                             output_shape=(d_model,))                                 (de_embed, de_embed, de_embed))             self_attention_d = keras.layers.Dropout(0.1)(self_attention)             add = de_embed + self_attention_d             normalised1 = keras.layers.LayerNormalization()(add)             cross_attention = (keras.layers.MultiHeadAttention(num_heads,                                             key_dim,                                             dropout=0.1,                                             use_bias=True,                                             output_shape=(d_model,))                                (normalised1, encoder_output,encoder_output))             cross_attention_d = keras.layers.Dropout(0.1)(cross_attention)             add2 = normalised1 + cross_attention_d             normalised2 = keras.layers.LayerNormalization()(add2)             fed_f = keras.layers.Dense(d_ff)(normalised2)   feed forward 1 part             fed_ff = keras.layers.Dense(d_model)(keras.activations.relu(fed_f))   feed forward 2 part             fed_ff_d = keras.layers.Dropout(0.1)(fed_ff)             add3 = normalised2 + fed_ff_d             normalised3 = keras.layers.LayerNormalization()(add3)             de_embed = normalised3         decoder_dense_output = keras.layers.Dense(decoder_vocab_len, activation='softmax', name='decoder_output')(de_embed)         return keras.Model(inputs=[encoder_input, decoder_input], outputs=decoder_dense_output)     if __name__ == '__main__':         params = (8, 64, 64, 256, 512, 6)         model = model_func(10000, 10000, 100, 100, params)         model.summary()          Generate random input data with appropriate shapes         encoder_input_data = np.random.randint(0, 10000, (2, 1))   (batch_size, sequence_length)         decoder_input_data = np.random.randint(0, 10000, (2, 4))   (batch_size, sequence_length)          Call the model with the random input data         output = model.call([encoder_input_data, decoder_input_data], training=False)          Print the shape of the output         print(f'Output shape: {output.shape}') UserWarning: You are using a softmax over axis 3 of a tensor of shape (2, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?   warnings.warn( UserWarning: You are using a softmax over axis 3 of a tensor of shape (2, 8, 4, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?   warnings.warn( Output shape: (2, 4, 10000)","Hi **** ,  Sorry for the delay, Can you please check with recent TF compatibility versions? I tried with TF2.16.1 and I cannot reproduce the error.  Please check the screenshot !test1 here. Thanks!","Hey, Thank you for taking the time to review my issue. I've had nothing but problems with my task over the past few days. I had a combination of a TransformerBlock and LSTM layers. Coupled with Optuna, it was probably just too many variables and possibilities, causing the model to become unstable. I have now switched to this task: def objective(trial, features, target):     n_estimators = trial.suggest_int('n_estimators', 50, 300)     max_depth = trial.suggest_int('max_depth', 3, 15)     learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)     subsample = trial.suggest_float('subsample', 0.5, 1.0)     colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)     gamma = trial.suggest_float('gamma', 0, 5)     min_child_weight = trial.suggest_int('min_child_weight', 1, 10)     reg_lambda = trial.suggest_float('lambda', 1e8, 10.0, log=True)     reg_alpha = trial.suggest_float('alpha', 1e8, 10.0, log=True)     model = XGBClassifier(         n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,         subsample=subsample, colsample_bytree=colsample_bytree, gamma=gamma,         min_child_weight=min_child_weight, reg_lambda=reg_lambda, reg_alpha=reg_alpha,         random_state=42     )     x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)     numeric_features = x_train.columns     preprocessor = ColumnTransformer(         transformers=[             ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')), ('scaler', StandardScaler())]),              numeric_features)         ])     x_train = preprocessor.fit_transform(x_train)     x_test = preprocessor.transform(x_test)     model.set_params(early_stopping_rounds=10, eval_metric='logloss')     model.fit(x_train, y_train, eval_set=[(x_test, y_test)], verbose=False)     predictions = model.predict(x_test)     accuracy = accuracy_score(y_test, predictions)     return accuracy With this, I have no problems. Have a nice day!","Hi **** ,  Could you please confirm if this issue is resolved for you? Please feel free to close the issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Enable weight-only quantization with StableHLO opset in TF Quantizer,Enable weightonly quantization with StableHLO opset in TF Quantizer This CL integrates StableHLO weightonly quantization preset to TF Quantizer API. The weightonly quantization is enabled when the quantization method is set to `METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8` and opset is set to `quant_opts_pb2.STABLEHLO`.,2024-05-16T13:27:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67738
yi,copybara-service[bot],PR #11895: Offloading 3/3: enable rematerialization using xla flags,PR CC(Fix typos): Offloading 3/3: enable rematerialization using xla flags Imported from GitHub PR https://github.com/openxla/xla/pull/11895 This enables activation offloading automatically on GPU through HLO Rematerialization. There is one xla flag needed to trigger rematerialization offloading: 1. A bool to enable host memory offloading. It's false by default to disable the offloading. The other information is achieved by querying the GPU device: 2. Memory bandwidth in bytes per second. 3. The number of floating point operations on a device.  Copybara import of the project:  46b6232dd0c9dd34d7b170f049e6074ccee002bd by Jane Liu : Offloading 3/3: enable rematerialization using a bool xla flag. Query the memory bandwidth and flops from GPUs. Merging this change closes CC(Fix typos) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/11895 from zhenyingliu:offloadingflags 46b6232dd0c9dd34d7b170f049e6074ccee002bd,2024-05-16T12:41:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67732
gemma,copybara-service[bot],[XLA:GPU] Remove GOOGLE_CUDA and TENSORFLOW_USE_ROCM defines from gemm_algorithm_picker.,[XLA:GPU] Remove GOOGLE_CUDA and TENSORFLOW_USE_ROCM defines from gemm_algorithm_picker. These defines are not needed since the BUILD target is anyway guarded by if_gpu_is_configured.,2024-05-16T11:22:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67730
yi,copybara-service[bot],Use apply_indexing for input index computations.,"Use apply_indexing for input index computations. For this to work, we need a folder, so add that too. We kind of rely on the folder to work most of the time (for caching of instructions). There may be some dragons here, I'll probably rewrite that stuff at some point. This works around the remaining known miscompiles. The issue must be somewhere in LLVM and be related to GEPs with subtractions, but I lack the skills and resources to actually track it down.",2024-05-16T10:13:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67726
yi,copybara-service[bot],PR #11895: Offloading 3/3: enable rematerialization using xla flags,PR CC(Fix typos): Offloading 3/3: enable rematerialization using xla flags Imported from GitHub PR https://github.com/openxla/xla/pull/11895 This enables activation offloading automatically on GPU through HLO Rematerialization. There is one xla flag needed to trigger rematerialization offloading: 1. A bool to enable host memory offloading. It's false by default to disable the offloading. The other information is achieved by querying the GPU device: 2. Memory bandwidth in bytes per second. 3. The number of floating point operations on a device.  Copybara import of the project:  46b6232dd0c9dd34d7b170f049e6074ccee002bd by Jane Liu : Offloading 3/3: enable rematerialization using a bool xla flag. Query the memory bandwidth and flops from GPUs. Merging this change closes CC(Fix typos) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/11895 from zhenyingliu:offloadingflags 46b6232dd0c9dd34d7b170f049e6074ccee002bd,2024-05-16T10:01:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67724
rag,copybara-service[bot],[Testing] Code Coverage: Increase Code Coverage of //third_party/tensorflow/core/runtime_fallback/util to at least 75%,[Testing] Code Coverage: Increase Code Coverage of //third_party/tensorflow/core/runtime_fallback/util to at least 75%,2024-05-15T20:28:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67680
yi,copybara-service[bot],PR #10803: Expose CompiledMemoryStats on CPU and GPU,"PR CC(TensorBoard Histogram Dashboard link broken): Expose CompiledMemoryStats on CPU and GPU Imported from GitHub PR https://github.com/openxla/xla/pull/10803 CompiledMemoryStats is supposed to summarize the amount of memory required to run a given executable. Bufferrelated fields in CompiledMemoryStats are currently missing for both the GPU and the CPU backend. This PR computes the missing fields using the executable's underlying BufferAssignment / buffer allocations.  Example  Note the use of `donate_argnums=(0,)` which allows XLA to modify the first input buffer inplace and reuse it as the output. This is reflected in `alias_size_in_bytes`  without buffer donation that number is zero.  Open questions  Does this match the semantics for the TPU backend?  `dynamic_cast`ing on `gpu::Executable` ain't great. Do we want `Executable`s to expose their buffer allocations publicly (i.e. GpuExecutable's GetAllocations but for all Executables)? Copybara import of the project:  f9ae2c4e367aed813af74ae50edfd27a6f9304eb by Georg Stefan Schmid : Expose CompiledMemoryStats on CPU and GPU Merging this change closes CC(TensorBoard Histogram Dashboard link broken) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/ope",2024-05-15T09:53:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67627
yi,copybara-service[bot],"In `xla::ShardingPropagation`, remove users and the parameters in the called computations in `already_inferred_from_operands` when clearing the cache.","In `xla::ShardingPropagation`, remove users and the parameters in the called computations in `already_inferred_from_operands` when clearing the cache. Before this cl, shardings may not be completely propagated.",2024-05-15T05:33:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67608
rag,copybara-service[bot],"Expands the test coverage of GetReducedIntervals(), since this is now the preferred way of interacting with the Memory Term Reducer.","Expands the test coverage of GetReducedIntervals(), since this is now the preferred way of interacting with the Memory Term Reducer.",2024-05-14T22:03:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67590
yi,kralka,Unexpected behavior in tf.data.Dataset.from_generator when there is a naming collision, Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15.0  Custom code Yes  OS platform and distribution colab  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version 11.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When creating a `tf.data.Dataset` object using `from_generator` a naming collision with an iterable used in the `generator` parameter causes a crash. On Ubuntu I got the following error (not in colab): date: F external/local_tsl/tsl/platform/default/env.cc:74] Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed.  Standalone code to reproduce the issue   Relevant log output ,2024-05-14T16:46:32Z,stat:awaiting tensorflower type:bug comp:data TF 2.16,closed,0,5,https://github.com/tensorflow/tensorflow/issues/67566,"Hi  , I have replicated the issue with `TF2.15v` and `tfnightly` as well. On Colab the code executes indefinitely due to naming collision of generator and finally runtime will crash without any logs. Attaching gist for reference.", thank you for your response. One can get a crash (as opposed to indefinite execution without errors) in an official docker (`docker pull tensorflow/tensorflow:latestgpu`). I will provide concrete steps if needed.,Hi  is there some plan to resolve this issue? Do you know what causes it? In case resolving would require too large changes do you think it is worth putting a note in the documentation since it is rather hard to figure out what causes this behavior? Personally I would be interested in the cause of this since some magic needs to be happening here :),Sorry my fault. This is not a TensorFlow issue. ,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Fix a bug in `validate_colocate` when a variable is not created in a strategy scope.,"Fix a bug in `validate_colocate` when a variable is not created in a strategy scope. In some cases `v._distribute_strategy` can be None, which was causing an AttributeError when trying to check if the `extended` field matched the expected value.",2024-05-14T14:26:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67561
yi,AceAbhi147,Bazel - Tensorflow build failed,"I have a ubuntu 22.0.4 LTS machine running on virtual box. I am trying to build tensorflow by cloning the repo and following the steps defined here: [build from source].(https://www.tensorflow.org/install/source) I have checkout the master branch. And I have installed llvm14 clang14 rather than 17 as I was getting some not found error. This is the command I am using to build tensorflow:  `bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow_cpu` But after waiting for what seems like hours, I am getting this error:  `[12,514 / 15,964] 3 actions running     Compiling tensorflow/compiler/mlir/tensorflow/ir/tf_ops_n_z.cc; 720s local     Compiling tensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc; 681s local ERROR: /home/aceventura/tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:424:11: Compiling tensorflow/compiler/mlir/tensorflow/ir/tf_ops.: (Exit 1): gcc failed: error executing command (from target //tensorflow/compiler/mlir/tensorflow:tensorflow_ops) /usr/bin/gcc U_FORTIFY_SOURCE fstackprotector Wall Wunusedbutsetparameter Wnofreenonheapobject fnoomitframepointer g0 O2 'D_FORTIFY_SOURCE=1' DNDEBUG ffunctionsections ... (remaining 187 arguments skipped) gcc: fatal error: Kil",2024-05-14T13:05:25Z,stat:awaiting response type:build/install stale subtype:bazel,closed,0,4,https://github.com/tensorflow/tensorflow/issues/67554, Could you please confirm that you have followed the steps properly and also referred the tested build configurations? Please let us know! !72Wi6T878zwwfuW Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
agent,LongZE666,Segmentation fault (core dumped) in `tf.raw_ops.SoftmaxCrossEntropyWithLogits`, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.1  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Triggered when input parameters `features` and `labels` are incorrect.  Standalone code to reproduce the issue   Relevant log output  ```,2024-05-14T08:47:15Z,stat:awaiting response type:bug stale comp:ops TF 2.16,closed,0,6,https://github.com/tensorflow/tensorflow/issues/67531,"Hi **** ,  Sorry for the delay, Can you please try with recent TF version? I tried with TFnightly and I cannot reproduce the error. It might be solved in upcoming version. Please check the gist here. Thank you!","I can reproduce this problem on tensorflow version 2.16.1, and it can be executed normally on tfnightly.","Hi **** ,  Yes, I also reproduce same error with version 2.16.1. But it is working fine with tfnightly. So, It might be solve in upcoming version. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,LongZE666,Segmentation fault (core dumped) in `tf.raw_ops.FusedResizeAndPadConv2D`, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Illegal `size` will trigger a segfault.  Standalone code to reproduce the issue   Relevant log output  ASAN report:  ```,2024-05-14T08:39:21Z,stat:awaiting tensorflower type:bug comp:ops TF 2.16,open,0,1,https://github.com/tensorflow/tensorflow/issues/67529, I was able to replicate the issue reported here. Thank you!
rag,copybara-service[bot],Improve test coverage of fallback_state.cc,Improve test coverage of fallback_state.cc,2024-05-13T23:52:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67514
agent,copybara-service[bot],Migrate coord agent and service to use absl  mutex/condvar libraries directly + some clang fixes.,Migrate coord agent and service to use absl  mutex/condvar libraries directly + some clang fixes.,2024-05-13T20:58:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67493
rag,copybara-service[bot],[Testing] Improve testing coverage to > 75% for all files under google3/third_party/tensorflow/core/tfrt/runtime/,[Testing] Improve testing coverage to > 75% for all files under google3/third_party/tensorflow/core/tfrt/runtime/,2024-05-13T19:59:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67487
yi,copybara-service[bot],PR #10803: Expose CompiledMemoryStats on CPU and GPU,"PR CC(TensorBoard Histogram Dashboard link broken): Expose CompiledMemoryStats on CPU and GPU Imported from GitHub PR https://github.com/openxla/xla/pull/10803 CompiledMemoryStats is supposed to summarize the amount of memory required to run a given executable. Bufferrelated fields in CompiledMemoryStats are currently missing for both the GPU and the CPU backend. This PR computes the missing fields using the executable's underlying BufferAssignment / buffer allocations.  Example  Note the use of `donate_argnums=(0,)` which allows XLA to modify the first input buffer inplace and reuse it as the output. This is reflected in `alias_size_in_bytes`  without buffer donation that number is zero.  Open questions  Does this match the semantics for the TPU backend?  `dynamic_cast`ing on `gpu::Executable` ain't great. Do we want `Executable`s to expose their buffer allocations publicly (i.e. GpuExecutable's GetAllocations but for all Executables)? Copybara import of the project:  260fff9e66d32c86692f6851786d2d41e028f4fe by Georg Stefan Schmid : Expose CompiledMemoryStats on CPU and GPU Merging this change closes CC(TensorBoard Histogram Dashboard link broken) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/ope",2024-05-13T15:10:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67456
yi,copybara-service[bot],PR #10803: Expose CompiledMemoryStats on CPU and GPU,"PR CC(TensorBoard Histogram Dashboard link broken): Expose CompiledMemoryStats on CPU and GPU Imported from GitHub PR https://github.com/openxla/xla/pull/10803 CompiledMemoryStats is supposed to summarize the amount of memory required to run a given executable. Bufferrelated fields in CompiledMemoryStats are currently missing for both the GPU and the CPU backend. This PR computes the missing fields using the executable's underlying BufferAssignment / buffer allocations.  Example  Note the use of `donate_argnums=(0,)` which allows XLA to modify the first input buffer inplace and reuse it as the output. This is reflected in `alias_size_in_bytes`  without buffer donation that number is zero.  Open questions  Does this match the semantics for the TPU backend?  `dynamic_cast`ing on `gpu::Executable` ain't great. Do we want `Executable`s to expose their buffer allocations publicly (i.e. GpuExecutable's GetAllocations but for all Executables)? Copybara import of the project:  f9ae2c4e367aed813af74ae50edfd27a6f9304eb by Georg Stefan Schmid : Expose CompiledMemoryStats on CPU and GPU Merging this change closes CC(TensorBoard Histogram Dashboard link broken) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/ope",2024-05-13T14:00:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67450
gpt,copybara-service[bot],PR #11563: [NVIDIA GPU] Improve GPU collective matmul to support all-gather having multiple users,"PR CC([DO NOT MERGE] Remove RTLD_GLOBAL when importing pywrap_tensorflow): [NVIDIA GPU] Improve GPU collective matmul to support allgather having multiple users Imported from GitHub PR https://github.com/openxla/xla/pull/11563 We have identified another optimization opportunity for gpt3 using collective matmul, in the backward pass, the allgather has multiple dot users but current spmd will duplicate multiple collective matmul loops. We'd like this transformation: before:  This is advantageous since the chained dot can fully utilize all the resource on the GPU while comm is hidden by the first collective matmul loop. We introduced an option to turn off CM loop duplication in SPMD and rewrite the graph to desired pattern in the gpu_windowed_einsum_handler pass. Copybara import of the project:  986ac94ab44d31f6d11ec6f135f6cfb2e5636d80 by TJ : Moved most of changes to gpu pass  44e81df91c235cac635f334c89d1d8a117ac6511 by TJ : Added e2e test for windowed einsum Minimized unit test hlo  8fc24a479de7515f532f36de8ffbcce49516c154 by TJ : Added explanations for spmd tests and dot_handler to skip multiple consumers  142d84d54db2b6291484443e43913d86c44a485c by TJ : move windowed einsum test to stateful_rng_spmd_pa",2024-05-13T13:02:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67445
yi,h-tye,Building binary for Sparkfun Edge,"I am trying to upload micro speech detection onto the SparkFun Edge device using this tutorial: https://codelabs.developers.google.com/codelabs/sparkfuntensorflow/ CC(JVM, .NET Language Support) . However, the directory needed to build the binary in the first part of step 4 no longer exists, has anyone found a way around this? The issue is tensorflow has tflitemicro as a standalone repository now but it no longer has the same tools.",2024-05-12T15:27:14Z,stat:awaiting response type:support stale comp:lite,closed,0,7,https://github.com/tensorflow/tensorflow/issues/67418,"tye You're right, the original TensorFlow Lite for Microcontrollers tutorial relies on an older directory structure that's no longer included. Kindly refer to the workaround as follows; The SparkFun Edge device often comes preloaded with a sketch called ""micro_speech.ino"". This sketch implements basic micro speech detection using TensorFlow Lite. You can try the following: 1. Connect your SparkFun Edge to your computer using the SparkFun USBC Serial Basic programmer. 2. Open the Arduino IDE. 3. In the Arduino IDE, go to **_File > Examples > SparkFun Edge > micro_speech_**. This should open the preloaded sketch. 4. Upload the sketch to your SparkFun Edge. 5. Open the Serial Monitor in the Arduino IDE. You should see the blue LED blinking. 6. Speak a keyword like ""yes"" or ""no"" near the microphone. If the speech is recognized, the serial monitor will display a message and the yellow LED will blink. Also refer the TensorFlow Lite Micro getting started guide for more information.  Thank you!"," Unfortunately, I am not able to use the Arduino IDE as when I run any code  even the examples I get an error that reads ""Unsupported Board"". I believe that Arduino no longer supports the Sparkfun Edge which is why I was trying to flash binary onto the device. Thanks for the help","Correction: It looks like the Arduino examples for Sparkfun edge are no longer supported by TensorFlow, not Arduino. https://github.com/tensorflow/tflitemicroarduinoexamples?tab=readmeovfilehowtoinstall as per this github repo, the peripherals are made purely for the Arduino Nano 33 BLE, which means that anytime I run one of the examples it produces a ""unsupported board"" error.","tye There might not be official Arduino libraries for the SparkFun Edge specifically, TFLite Micro offers support for various Arm CortexM processors, which the SparkFun Edge uses (https://www.tensorflow.org/lite/microcontrollers).  For any further queries could you please post this issue in this  repository ? Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,samrajput1143,"issue with loss_weights parameter of model.compile() , when model returns multiple output"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.11.0  Custom code Yes  OS platform and distribution ubuntu 18.04  Mobile device _No response_  Python version 3.7.5  Bazel version _No response_  GCC/compiler version 7.5.0  CUDA/cuDNN version cuda_11.8/cuDNN 9.1.0.70  GPU model and memory Tesla P40/ 24 gb  Current behavior? Even if i have provided the **loss_weights** parameter , TensorFlow 2.11.0  is calculating loss for  each of the output variable in the outputs list against the true value .  Standalone code to reproduce the issue   Relevant log output ",2024-05-12T06:17:16Z,stat:awaiting response type:support stale comp:apis TF 2.11,closed,0,6,https://github.com/tensorflow/tensorflow/issues/67405,"Hi  , Could you please test with tfnightly and let us know whether this is replicable with nightly version as well?","I have python version 3.7.5 , can i install tfnightly?",", Looks like this is not the issue related to TensorFlow. Also Tensorflow v2.11 is a pretty older version, Could you please try to execute the code with the latest TensorFlow v2.16 which supports the python v3.93.12. Kindly open a tensorflow discussion forum[https://discuss.tensorflow.org/] issue for this as it is not a bug or feature request",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],"Raise a runtime error when trying to convert the `jax.Array` wrapped by `jax.core.Token` to a numpy array, as it is an internal implementation detail and the buffer has XLA token shape.","Raise a runtime error when trying to convert the `jax.Array` wrapped by `jax.core.Token` to a numpy array, as it is an internal implementation detail and the buffer has XLA token shape.",2024-05-10T23:04:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67366
yi,Hell576,"Why tf.data.Dataset.choose_from_datasets() chooses only one element from dataset of size-element 5, I want to unite with other dataset of size-element 5 the same. If I want to merge dataset with all their elements and get  <ChooseDataset ...> with 10 elements inside"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf v2.16.0rc018g5bc9d26649c 2.16.1  Custom code Yes  OS platform and distribution Windows 10 Home  Mobile device _No response_  Python version 3.11.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory Ryzen 5 5600U 8 gb RAM  Current behavior? I expect to have one dataset with 21 elements from SUB_DATASETS trainSubDs***(unpack it):  [Uploading SUB_DATASETS.7z…]() link if archive didn't upload: https://drive.google.com/drive/folders/1yg2QL6uXUwNikSVNduBl0tMvOSqTa050?usp=sharing It looks like(console could show only last elements to output, it could copy only last snippet): [[0.5359075],         [0.2795821],         [0.0720736],         ...,         [0.0077176],         [0.1932825],         [0.0856282]],        [[0.5283639],         [0.2746144],         [0.0710207],         ...,         [0.0052901],         [0.1952176],         [0.0862649]],        ...,        [[0.       ],         [0.       ],         [0.       ],         ...,         [0.       ],         [0.       ],         [0.       ]],        [[0.       ],         [0.       ],",2024-05-10T10:53:08Z,stat:awaiting response type:bug stale comp:data TF 2.16,closed,0,10,https://github.com/tensorflow/tensorflow/issues/67327," tf.data.Dataset.choose_from_datasets() isn't designed to merge datasets elementwise. It actually picks elements one at a time, deterministically choosing from the provided datasets based on a separate ""choice"" dataset. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"If It actually picks elements one at a time, that means TF should either fix it or edit documentation for this function",", To merge two datasets and get all their elements combined, you can try using **tf.data.Dataset.concatenate** or **tf.data.Dataset.flat_map**.   Use tf.data.Dataset.concatenate to create a new dataset by appending elements from the second dataset to the first  If your datasets are nested (e.g., each element is a dataset itself), use tf.data.Dataset.flat_map to flatten them into a singlelevel dataset.  https://www.tensorflow.org/api_docs/python/tf/data/Datasetconcatenate https://www.tensorflow.org/api_docs/python/tf/data/Datasetflat_map Thank you!","I know concatenate() method, However there was troubles after doing such operation. When I cocncatenated two datasets (every has 5 elements, 5+5 totally) tried to check tf.data.Dataset It threw in the 1st element: tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__MakeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes at component 0: expected [150,128,1] but got [].  I still try to reproduce this error using numpy arrays included to datasets, however this error does not repoduce. In real code I used tensors you can see in my example above from RaggedTensor, then used tf.data.Dataset.zip(), then  tf.data.Dataset.choose_from_dataset() then tf.data.Dataset.concatenate(). But my error slips away if I use after concatenate function tf.data.Dataset.save() and tf.data.Dataset.load() after.",Are you satisfied with the resolution of your issue? Yes No,", Glad the above methods worked with numpy arrays without any issue. For ragged batch, could you please take a look at this reference document and try to execute the code.  https://www.tensorflow.org/api_docs/python/tf/data/Datasetragged_batch Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,saturn-drm,Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.9  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I have a function as below. I have `N = tf.shape(y_true)[0]` where `y_ture` is ``. I later iterate over N as `for i in tf.range(N)`, but get an error.  But I get error like this:  How can I solve that?  Standalone code to reproduce the issue   Relevant log output _No response_",2024-05-10T10:28:34Z,type:bug TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/67323,"Hi **drm** ,  Sorry for the delay, I tried to run your code on colab using TF v2.9, 2.16.1 and i am not facing any issue. Please find the gist here for reference.  And it looks like you are using an older Version of Tensorflow (2.9). Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version (2.16.1) and let us know if the issue still persists? Thank you!",Thank you for the reply. I will try it out as you recommended.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],"Raise a runtime error when trying to convert the `jax.Array` wrapped by `jax.core.Token` to a numpy array, as it is an internal implementation detail and the buffer has XLA token shape.","Raise a runtime error when trying to convert the `jax.Array` wrapped by `jax.core.Token` to a numpy array, as it is an internal implementation detail and the buffer has XLA token shape.",2024-05-10T07:58:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67308
yi,copybara-service[bot],Implement optional memory space support for PjRt stream executor and use it to add memory space support to GPU,"Implement optional memory space support for PjRt stream executor and use it to add memory space support to GPU This CL implements an optional interface that allows PjRt stream executor implementations to add basic memory space support. The simplest form is one memory space per device, which requires just a memory space implementation and nothing else (memory space variants of transfer methods fall back to the corresponding device versions if the memory space is associated with just one device). More sophisticated mapping is possible, but requires overriding methods that take a memory space as a destination. Using the above, this CL also adds basic memory space support for SE:GPU. Right now, there's just one memory kind, HBM (`StreamExecutorGpuHbmMemorySpace`), and every PjRt GPU device has a corresponding HBM memory space as its default memory space. While this does not bring any new feature, it simplifies the framework and runtime support since the caller does no longer need to check whether the underlying implementation supports memory space or not.",2024-05-10T05:49:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67296
yi,jakirkham,NumPy 2.0 support," Problem description NumPy 2.0 is coming out soon ( https://github.com/numpy/numpy/issues/24300 ). NumPy 2.0.0rc1 packages for conda & wheels came out a month ago ( https://github.com/numpy/numpy/issues/24300issuecomment2030603395 )  Feature description NumPy has put out a migration guide. More details are in the release notes If TensorFlow make use of NumPy's C API (and produces wheels that use it), having a release of TensorFlow with wheels built against NumPy 2.0.0rc1 would be helpful to ensure NumPy 1 & 2 compatible wheels (as wheels built against NumPy 1 won't be compatible with NumPy 2). More details in this NumPy 2 ABI doc Also would recommend incorporating NumPy 2 in TensorFlow's CI Also as NumPy is tracking ecosystem support for NumPy 2.0, it would be helpful to share TensorFlow's current support status in issue (with any plans): https://github.com/numpy/numpy/issues/26191  Additional information There has been some discussion about TensorFlow's NumPy 2 plans here: https://github.com/numpy/numpy/issues/26191issuecomment2078401952 Also the Keras team is interested in this question as they depend on TensorFlow: https://github.com/kerasteam/keras/issues/19691issuecomment2103062310 Lastly was unabl",2024-05-10T04:33:03Z,stat:awaiting tensorflower type:feature type:others override-stale,closed,4,41,https://github.com/tensorflow/tensorflow/issues/67291,Am curious if anyone has had a chance to look into this. No worries if not It is worth noting that NumPy is planning to release 2.0 on June 16th: https://github.com/numpy/numpy/issues/24300issuecomment2125643448,Friendly nudge 😉 Do we have a sense of where this is in the queue?,"Sorry for the late reply, TF is working on the upgrade but it may not make the upcoming branchcut to ensure all cases around datatype promotion are handled correctly.   is working on the upgrade and ensuring test owners verify updates.  We will give another status update by Friday on if it will make the 2.17 branchcut and when it can be expected in the nightly branch. ",Thanks Michael! 🙏 No worries Are Kanglan's changes in a PR? Or could they be added to one? Maybe members of the community could help review 🙂 FWIW did see these merged PRs: * https://github.com/tensorflow/tensorflow/pull/63317 * https://github.com/tensorflow/tensorflow/pull/67852 * https://github.com/tensorflow/tensorflow/pull/67864 * https://github.com/tensorflow/tensorflow/pull/68271 Are there any others?,No other PRs right now that i think are ready yet.   I can say that at this point it looks like we will not make the target of next week's branchcut.  We need to verify if any of the fixes or modifications cause api level changes given TF's API compatibility guidelines.  We will target to get it into nightly as soon as we can.  We will look to use this issue to give regular updates until have made the 2.0 upgrade. ,Would recommend that any released packages (wheels or otherwise) have an upper bound to disallow NumPy 2 Mentioning this as NumPy is planning to release on June 16th: https://github.com/numpy/numpy/issues/24300issuecomment2125643448,Upper bounds will just cause solvers like pip to select older versions of tensorflow without the bound.,"Am aware that is true of Conda, but does the pip solver also have this behavior? Are there examples of this?","Ahh, yes, if nothing causes a backtrack, it does work. It can still backtrack in some cases and produce unhelpful solves, but by default it would help to have the limit. The simple case of `pip install tensorflow` will be broken tomorrow without a limit.",It won't be broken  TensorFlow has always used upper bounds for all of its dependencies AFAIK. I am pretty sure there are no versions of TensorFlow for Python 3.93.12 that depend on an unpinned `numpy`.,"Yeah, `tensorflow` `v2.16.1` had the following requirements (with caps on `numpy < 2.0.0` introduced in https://github.com/tensorflow/tensorflow/commit/92b81403305ddba765d3fb9a64bae8f52bf90dfd on 20231016) https://github.com/tensorflow/tensorflow/blob/5bc9d26649cca274750ad3625bd93422617eed4b/tensorflow/tools/pip_package/setup.pyL85L126 but you only have to go back to `tensorflow` `v2.14.0` in September, 2023 to reach an uncapped `numpy` requirement https://github.com/tensorflow/tensorflow/blob/4dacf3f368eb7965e9b5c3bbdd5193986081c3b2/tensorflow/tools/pip_package/setup.pyL93 which gets you a broken TensorFlow:  (That's not the worst obviously, but still some motivation to want to see a compatible release)","Unfortunately, the team won't upgrade any TF below 2.16, now that the process for 2.17 started and the builds for the last patch release on 2.16 have been triggered (it used to be that patches were given for a full year, now it's just one patch on the next release and that's all :shrug: )",Yeah that's all totally reasonable. 👍 I hope it didn't come across like I was hoping for anything to get back ported.,"No, it's all good. In the old system, 2.14 and 2.13 would still have been updated to had the upper bound.","As long as the latest one has a bound, that’ll fix most cases. Weird backsolves will happen if someone requires >=2, but that isn’t possible to fix without NumPy 2 support, so it’ll just be a slightly stranger error message, which is totally fine I think.",Kind ping on this front. > We will target to get it into nightly as soon as we can. We will look to use this issue to give regular updates until have made the 2.0 upgrade. I think there is still the fact that setup.py declares that 2.0 isn't supported: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.pyL88 Has the full test suite been run with success? Maybe we can start to use the newstyle pinning?,"There is some work on this front, see CC([numpy] Fix build failures in TensorFlow under NumPy 2.0.)",Sorry for the late reply. We're conducting thorough internal testing and expect to provide an update within the next few weeks.,"hello, any news on this ?","Hi  , we've addressed most of the compatibility issues with NumPy 2, and you can expect TensorFlow nightly wheels compiled against NumPy 2 within the next week. 🎉",will it be announced somewhere when Tensorflow works with Numpy 2?  I upgraded to Python 3.12.5 because another plugin that I use did an upgrade and only works with Numpy 2 and higher.  So when I then install tensorflow it automatically uninstalls my Numpy 2.1 version and installs 1.26. So I  took down uninstalled Tensorflow for the time being. Of course I can still use my Python 3.8.5 version but I run my Python code from another program that requires Numpy 2 or higher. Where will it be announced when Tensorflow has been upgraded to work with Numpy 2.0 and higher? Thanks,Most likely in this issue,is there a specific PR or commits we can backport on 2.17 for numpy 2 support ?,"I think that's not possible. The development process results in individual commits to main branch and not all of them are tagged with the feature they are working towards. And, unfortunately, some are just tagged ""internal change"", so not even by reading the commit message would you be able to determine if the commit is relevant or not. I think the number of changes is quite significant, so you would have to cherrypick quite a large number of commits if you want to go that way and hope that the compilation process won't fail. If possible, wait a little bit, 2.18 is around the corner and might have numpy 2 by default.","fair enough, do you know the release schedule for 2.18 ?",I _estimate_ (not on the team) that branch cut would happen by the end of this month and then in around at most a month there will be a final release. Definitely before Thanksgiving (end of Nov),"ok, thanks","Thank You On Sun, Sep 15, 2024, 3:56 PM Julien Schueller ***@***.***> wrote: > ok, thanks > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you are subscribed to this thread.Message > ID: ***@***.***> >","A quick update: TensorFlow nightly wheels are now compiled with NumPy 2.0, starting with the latest release 2.18.0.dev20240919!  * https://pypi.org/project/tfnightly/2.18.0.dev20240919/ * https://pypi.org/project/tfnightlycpu/2.18.0.dev20240919/","did not yet work for me. I did: pip install tfnightly==2.18.0.dev20240919 but seems it is installing 2.17 Installing collected packages: numpy, tbnightly, kerasnightly, tfnightlyintel, tfnightly   Attempting uninstall: numpy     Found existing installation: numpy 2.1.1     Uninstalling numpy2.1.1:       Successfully uninstalled numpy2.1.1   WARNING: Failed to remove contents in a temporary directory 'C:\Users\win 10\AppData\Local\Programs\Python\Python312\Lib\sitepackages\~~mpy'.   You can safely remove it manually. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. **tensorflowintel 2.17.0 requires numpy=1.26.0; python_version >= ""3.12"", but you have numpy 2.0.2 which is incompatible.** Successfully installed kerasnightly3.5.0.dev2024092003 numpy2.0.2 tbnightly2.18.0a20240919 tfnightly2.18.0.dev20240919 tfnightlyintel2.18.0.dev20240919"
yi,copybara-service[bot],Give `xla_cc_test` a `use_gpu` option instead of manually specifying tags,Give `xla_cc_test` a `use_gpu` option instead of manually specifying tags,2024-05-09T21:39:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67278
yi,copybara-service[bot],Remove the default `PjRtBuffer::CopyRawToHostFuture` implementation,"Remove the default `PjRtBuffer::CopyRawToHostFuture` implementation It is infeasible to implement `CopyRawToHostFuture` correctly in the base `PjRtBuffer` class just with other public methods because keeping the underlying buffer alive requires knowing about internal representation of a buffer. Instead, this CL removes the buggy default implementation and lets it always return an `UNIMPLEMENTED` error.",2024-05-09T21:38:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67277
dspy,LeeYunhang,Protobuf does not support 4.x," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered the following error when loading tensorflow with protobuf==4.25.3 installed File ""/opt/python3.10.11/lib/python3.10/sitepackages/tensorflow/__init__.py"", line 45, in from tensorflow._api.v2 import __internal__ File ""/opt/python3.10.11/lib/python3.10/sitepackages/tensorflow/_api/v2/__internal__/__init__.py"", line 8, in from tensorflow._api.v2.__internal__ import autograph File ""/opt/python3.10.11/lib/python3.10/sitepackages/tensorflow/_api/v2/__internal__/autograph/__init__.py"", line 8, in from tensorflow.python.autograph.core.ag_ctx import control_status_ctx  line: 34 File ""/opt/python3.10.11/lib/python3.10/sitepackages/tensorflow/python/autograph/core/ag_ctx.py"", line 21, in from tensorflow.python.autograph.utils import ag_logging File ""/opt/python3.10.11/lib/python3.10/sitepackages/tensorflow/python/autog",2024-05-09T12:44:54Z,type:bug,closed,0,3,https://github.com/tensorflow/tensorflow/issues/67247,"  Hi, Do you know why protobuf 4.x hasn't been supported yet?","Hi  , Please find below the list of protobuf versions supported for nightly version. https://github.com/tensorflow/tensorflow/blob/f29f62111ecb98a54951377bb0d7069a7d8c2314/tensorflow/tools/pip_package/setup.pyL94 >  Hi, Do you know why protobuf 4.x hasn't been supported yet? I am not sure which versions are in pipeline now ? I need to check with concern developer team and will update.",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[xla:ffi] Add Python-based tests for type-safe custom calls on CPU platform,"[xla:ffi] Add Pythonbased tests for typesafe custom calls on CPU platform Implements CC(C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed). This change only adds test cases, all other changes required for Python support are already merged. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/44950 from Inteltensorflow:yimei/fuse_old_bn e09d450b1fec79c547e6599040aad5ccbb8b96fc",2024-05-09T08:18:04Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/67229,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
yi,copybara-service[bot],Introduced Op `GetTpuHostId` and `UpdateTaskIdAndGlobalCoreArray`.,Introduced Op `GetTpuHostId` and `UpdateTaskIdAndGlobalCoreArray`.,2024-05-09T07:27:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67223
rag,zwikst,Why don't you provide a standard go-lang get install?,"Of course I know that a manual scripted installation is provided at the following link: build from source However, I would like to say that Tensorflow is a Google product and so is Golang, but my desire to use Golang for Tensorflow would require me to manually patch a few dozens of patches. I'd also like to say that the patch doesn't always work, as in the following error:  Apparently the .proto source files you provide are not compatible with the go generate command in scripts. It's really puzzling, I've seen very small studios or small workshops that might offer their products to collaborators this way. But this is Google, this is Tensorflow, this is Golang. And  the product is for the public. No intention to disparage third party developers, in fact I'd rather appreciate them. But recommending a community product with 30+ stars on your product's official page is still baffling. The intention is not to fix these patch scripts, but I still provide my working environment to easily reproduce the problem: ",2024-05-09T06:54:43Z,stat:awaiting response type:support stale,closed,0,4,https://github.com/tensorflow/tensorflow/issues/67221," Starting with Go version 1.17, the functionality of go get for installing executables has been deprecated. This means using go get to directly install and compile packages is no longer recommended. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,gustavla,"TFLite GPUv2: ADD(x, 1e-5) results in severely wrong output","**System information**  Samsung Galaxy S23 / Android 13 /Snapdragon® 8 Gen 2 | SM8550  GPUv2 delegate  TFLite 2.16.1 Assets:  * Model: https://qaihubpublicissues.s3.uswest2.amazonaws.com/tflite/67216_post_add_numerical_issues.tflite * Inputs: https://qaihubpublicissues.s3.uswest2.amazonaws.com/tflite/67216_post_add_numerical_issues_inputs.npz (saved with `numpy.savez`) Please take a look at two outputs in particular of this network: * `key = ""model_13/featurefusion_network/encoder/query_layer/norm/LayerNormalization/moments/variance""` (variance) * `key2 = ""model_13/featurefusion_network/encoder/query_layer/norm/LayerNormalization/batchnorm/add""` (add) The variable `variance` gets fed into ADD(x, 0.000009999999747378752) and comes out as `add`.  I ran this on the CPU (xnnpack) and the GPU (GPUv2) and got totally different results. `variance` looks like this across CPU and GPU (so far consistent):  `add` looks like this across CPU and GPU:  Here, the values on the GPU has gone completely off the rails. They do not look random though, since there is a periodicity to the output (error alternates between around 1.6 and 0.6). **Standalone code to reproduce the issue**  This should be simple to set up through ",2024-05-09T03:06:22Z,stat:awaiting response type:bug stale comp:lite TFLiteGpuDelegate TF 2.16,closed,0,12,https://github.com/tensorflow/tensorflow/issues/67216,"Hi  , I replicated your issue using Qualcom Ai hub, and i got the same results as you. Let me verify the same through an Android app and I will get back to you.","1. Is this OpenGL or OpenCL? 2. What is the precision?  FP16 or FP32? 3. The GPU delegate works on 4D tensors of shape [B, H, W, C].  It looks like it starts with [1, H, W, C] but throughout the network, you see tensor shapes like 16x1x256, which will then be auto expanded to [16, 1, 1, 256], but this is obviously wrong, because B is expected to be 1 (the way I read your network).  Can the tensor dimensions be carefully reviewed and made consistent, wellformed 4D tensors?","Hi  , I tried reproducing your issue using an Android app but I kept running into issues with passing the inputs to the tflite model. If possible can you please reproduce this error through an Android app?","  1. OpenCL 2. FP16. I've attached the GPUv2 configuration below, which will allow FP16 execution. Note that I have confirmed that this is not a precision issue at all. It may be a bug in the FP16 implementation only, but it's not because of the lower precision. I can try running this with the FP32 mode too. 3. > Can the tensor dimensions be carefully reviewed and made consistent, wellformed 4D tensors?     This is part of a large model that was compiled from Tensorflow to TFLite using the TFLiteConverter. Making sure this model is 4D at the TFLite level is a nontrivial task. I am also under the impression that the current 3D tensors are perfectly wellformed as far as TFLite is concerned (if not, that's a bug in TFLiteConverter then). Yes, GPUv2 may translate them to rank 4 as an implementation detail, but if the rank 3 tensors are valid, then isn't it a bug in GPUv2 if this translation to rank 4 is causing issues?  I do not have a standalone app that reproduces this. However, the original repro did run through an app using TFLite on a real Android device. Feel free to resave the npy file in another format that you are more used to using for repros like this. For instance, you can do packed rowmajor as `inputs[""image""].tofile(""image.raw"")`. How do you usually feed in specific data in situations like this? GPUv2 configuration: inference_preference = TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED inference_priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY inference_priority2 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_MEMORY_USAGE inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION","Hi  ,  If possible, can you share me the model tensorflow to Tflite model conversion script. The issue could lie there also.","Sorry for the late turn around. Took a look at the network.  You are tapping into the intermediate tensors, but we reuse intermediate tensors, no matter whether it's a graph output tensor (sorry, that's a limitation that requires some engineering resources to fix and we never prioritized that).  If you really want to tap into an intermediate tensor, you have to add a small noop.  For example, if you want to read what you have named as ""variance"" in the above picture (output of MEAN), you have to add a small nonzero value, e.g. `ADD(x, 1e6)` (not to be confused with the` ADD(x, 0.000009999999747378752)` you already have) and make a tensor a true terminal tensor.","> Sorry for the late turn around. >  > Took a look at the network. You are tapping into the intermediate tensors, but we reuse intermediate tensors, no matter whether it's a graph output tensor (sorry, that's a limitation that requires some engineering resources to fix and we never prioritized that). If you really want to tap into an intermediate tensor, you have to add a small noop. For example, if you want to read what you have named as ""variance"" in the above picture (output of MEAN), you have to add a small nonzero value, e.g. `ADD(x, 1e6)` (not to be confused with the` ADD(x, 0.000009999999747378752)` you already have) and make a tensor a true terminal tensor.  could you elaborate a little more about adding a noop. How will it help in making the results of cpu and gpu consistent? Thanks in advance ","Let `a` be the input tensor and assume the network consists of two ops OP1 & OP2. > b = OP1(a) > c = OP2(b) Then, `b` is an intermediate tensor and its value will be overwritten during the computation because memory buffers are shared.  if you really want to inspect `b`, you have to add a noop: > b = OP1(a) > b' = ADD(b, 1e6) > c = OP2(b) and inspect `b'` instead of reading `b`.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"Not sure why this closed. This is clearly a bug and it hasn't been resolved. I understand the argument that most users won't hit this, because most users won't attach outputs inside common fusion patterns like this. Those are good arguments for making it a low priority bug. However, it's still a bug, is it not?  This is as far as I can tell a valid TFLite model with functionally incorrect behavior in the runtime. What is there to fix on the compiler side? > Took a look at the network. You are tapping into the intermediate tensors  I get what you are saying that these are ""intermediate tensors"" in a common fusion pattern. However, these tensors are not intermediate. They are networklevel outputs. That is what the user specified in the graph.  Considering whether intermediates are used elsewhere is a standard consideration when writing fusion graph passes to ensure that the graph is still functionally correct after the pass. There are typically two approaches: * Make no assumption whether or not the intermediates are used elsewhere. Do not remove any op in the pass. Let DCE clean it up. That way, the fusion will trigger, but some ops will remain to also produce the intermediate. * Break the pattern if intermediates are used elsewhere. Technically the second approach can avoid some unnecessary overhead, but first one is generally a better approach from a software maintenance point of view."
yi,copybara-service[bot],[XLA:Python] Fix a memory corruption bug in the tp_name attribute of ArrayImpl and PjitFunction for Python 3.10 or earlier.,"[XLA:Python] Fix a memory corruption bug in the tp_name attribute of ArrayImpl and PjitFunction for Python 3.10 or earlier. This works around https://github.com/python/cpython/issues/89478, which was fixed in Python 3.11.",2024-05-09T00:37:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67209
transformer,copybara-service[bot],Introduce ReplaceWhileOperandShape api to HloInstruction. This function provides an interface to change the shape an tuple operand in while loops.,"Introduce ReplaceWhileOperandShape api to HloInstruction. This function provides an interface to change the shape an tuple operand in while loops. The default implementation (ShapeTransformer) propagates shape change through instructions in which the output shape is directly inferred from operands, namely, gte, tuple, and nested while loops. As the result, if the changed operand of the while loop is only used by the mentioned instructions, the call to ReplaceWhileOperandShape guarantees validity of the hlo graph after shape replacement. Currently, the replace function simply bails and returns false if there are any users other than the mentioned instructions.",2024-05-07T22:37:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67127
yi,copybara-service[bot],[XLA:GPU][MLIR-Based emitters] Fix canonicalization patterns for ApplyIndexingOp.,[XLA:GPU][MLIRBased emitters] Fix canonicalization patterns for ApplyIndexingOp.,2024-05-07T19:54:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67113
transformer,copybara-service[bot],[XLA:GPU] Rename LoopDoubleBufferTransformer to DoubleBufferLoopUnrolling.,[XLA:GPU] Rename LoopDoubleBufferTransformer to DoubleBufferLoopUnrolling.,2024-05-07T11:41:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67092
yi,copybara-service[bot],[XLA:GPU][NFC] Make GpuCompilerTest.CollectivePermuteDecompositionAndPipelining less brittle.,"[XLA:GPU][NFC] Make GpuCompilerTest.CollectivePermuteDecompositionAndPipelining less brittle. Previously, the test was relying on both 1. exact instruction names, and 2. explicit ordering of many pure and reorderable HLO operations. Both of these things can not be relied upon for testing, and this came up as a blocker when trying to remove passes from the pipeline. This change removes all the matches on exact instruction names and minimizes the number of instructions that need to be explicitly orderedwhile aiming to preserve the spirit of the test.",2024-05-07T11:11:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67089
yi,rkdtmdals7710,problem with converted custom model at TensorFlow Lite,"I created a tflite model from yolov5n  [export.py  weights mymodel.pt  include tflite] for the learning model I learned from yolov5n. By the way, /home/pi1/Desktop/project3/examples/lite/examples/object_detection/raspberry_pi. It works fine when I run the existing detect.py , but when I use the above model, I get the following error.  (0507) pi1:~/Desktop/project3/examples/lite/examples/object_detection/raspberry_pi $ python detect.py model mymodel.tflite Traceback (most recent call last):   File ""detect.py"", line 15, in      import tensorflow_io as tfio   File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/sitepackages/tensorflow_io/__init__.py"", line 17, in      from tensorflow_io.python.api import *   pylint: disable=wildcardimport   File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/sitepackages/tensorflow_io/python/api/__init__.py"", line 19, in      from tensorflow_io.python.ops.io_dataset import IODataset   File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/sitepackages/tensorflow_io/python/ops/__init__.py"", line 24, in      import tensorflow as tf   File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/sitepackages/tensorflow/__init__.py"", line 37, in      from tensorflow.python.tools import module_u",2024-05-07T10:10:13Z,stat:awaiting response type:support stale comp:lite TFLiteConverter,closed,0,4,https://github.com/tensorflow/tensorflow/issues/67086," For such issues the recommended solution is to upgrade your protobuf to version 3.19.0 or later. You can usually do this using your package manager (e.g., `pip install upgrade protobuf`). Also please verify the TensorFlow IO Installation,  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[XLA:GPU][NFC] Make GpuCompilerTest.CollectivePermuteDecompositionAndPipelining less brittle.,"[XLA:GPU][NFC] Make GpuCompilerTest.CollectivePermuteDecompositionAndPipelining less brittle. Previously, the test was relying on both 1. exact instruction names, and 2. explicit ordering of many pure and reorderable HLO operations. Both of these things can not be relied upon for testing, and this came up as a blocker when trying to remove passes from the pipeline. This change removes all the matches on exact instruction names and minimizes the number of instructions that need to be explicitly orderedwhile aiming to preserve the spirit of the test.",2024-05-07T09:14:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/67079
rag,theomeb,GlobalAveragePooling1D fails with empty inputs and a mask," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15.0  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? `tf.keras.layers.GlobalAveragePooling1D` cannot be called on an empty tensor with an empty mask. This can cause issue when using a model that uses this layer under a distributed strategy, e.g. `MirroredStrategy`, which will distribute the data over multiple GPUs. For instance, for a dataset with 3 samples, a batch_size of 2 and 3 GPUs, one of the GPUs will have empty batches which will cause the error. This is due to the casting `math_ops.cast(mask, inputs[0].dtype)` in `GlobalAveragePooling1D::call()` which implies a nonempty inputs tensor, while `math_ops.cast(mask, inputs.dtype`) should do the trick without causing the error.  Standalone code to reproduce the issue   Relevant log output ",2024-05-06T14:57:57Z,stat:awaiting tensorflower type:bug comp:keras TF 2.15,open,0,2,https://github.com/tensorflow/tensorflow/issues/67023,A colab environment with the bug and the fix: https://colab.research.google.com/drive/196CeYNTFTXdSj21sBlqQvrWIER3Q36L?usp=sharing,  I was able to replicate the issue reported here. Thank you!
yi,NOORLEICESTER,RQA features,"I am trying to run the python code to extract RQA features. I am getting on the console window the following: [Platform 'Portable Computing Language'] Vendor: The pocl project Version: OpenCL 3.0 PoCL 3.0rc2  Linux, Release, RELOC, LLVM 14.0.6, SLEEF, DISTRO, POCL_DEBUG Profile: FULL_PROFILE Extensions: cl_khr_icd cl_pocl_content_size   [Device 'pthreadAMD EPYC 7532 32Core Processor'] Vendor: AuthenticAMD Type: 2 Version: OpenCL 3.0 PoCL HSTR: pthreadx86_64unknownlinuxgnuznver2 Profile: FULL_PROFILE Max Clock Frequency: 2395 Global Mem Size: 268025274368 Address Bits: 64 Max Compute Units: 64 Max Work Group Size: 4096 Max Work Item Dimensions: 3 Max Work Item Sizes: [4096, 4096, 4096] Local Mem Size: 524288 Max Mem Alloc Size: 68719476736 Extensions: cl_khr_byte_addressable_store cl_khr_global_int32_base_atomics   cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics   cl_khr_local_int32_extended_atomics cl_khr_3d_image_writes cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_khr_fp64,,   How can I prevent this from appearing (printing) in the console window?",2024-05-05T15:59:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66986
yi,burntato,"Android Tensorflow lite cannot use models from path, gives ""TFLite failed to load model with error"" error."," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.8  Custom code Yes  OS platform and distribution Linux Ubuntu 23.04  Mobile device Android 10 device and Emulated android 10 device  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am currently trying to make a different version of tensorflow lite android(java) app that can classify images from here https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android_java the changes that I made is that I added a button to upload a file(tensorflow lite model) to the app, and then the app will use that file as the model.  I'm having difficulties on trying to use the model, the file upload section is able to upload and rename it just fine, but somehow even if the file is there it still gives  20240505 02:13:33.449 2600326003 CameraFragment          org....examples.imageclassification  D  Received URI: content://com.android.providers.downloads.documents/document/msf%3A25 20240505 02:13:33.466 2600326003 CameraFragment          org....examples.ima",2024-05-04T19:19:50Z,stat:awaiting response type:bug stale comp:lite comp:lite-examples TF 2.8 Android,closed,0,8,https://github.com/tensorflow/tensorflow/issues/66979,"Hi  , I will replicate the issue and will get back to you.","Hi  , Can you also share the model.tflite file please ","> Hi  , >  > Can you also share the model.tflite file please I have updated the google drive folder to contain the custom model I've used","Any news? or do you have any difficulties trying to replicate the issue?  , maybe I could help by providing more information","Hi,   I apologize for the delayed response and as per our codelab documentation and recommended approach is to add the TensorFlow Lite model to `assets` folder, if you place TensorFlow Lite model to other folder you may face issues loading that model so I would suggest you to keep your TensorFlow Lite model in `assets` folder and you confirmed that after keeping the TensorFlow Lite model to `assets` folder things as working expected if I have not mistaken Please refer official codelab tutorial) where it clearly mentioned **Add the TensorFlow Lite model to assets folder** If I have missed something here please let me know.  Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],[xla:ffi] Added support for token-typed arguments and results,"[xla:ffi] Added support for tokentyped arguments and results This change is necessary to migrate the custom calls behind jax.*_callback APIs. I decided to encode tokens as scalar buffers with void storage type. Another possibility was to introduce a dedicated argument/return type for tokens and change all the internals to allow both buffers *and* tokens, but I thought the extra complexity is not worth it.",2024-05-03T17:53:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66944
transformer,mdfaijul,[oneDNN] Quantized batch-matmul and fusions,"Reopened the rollback https://github.com/tensorflow/tensorflow/commit/cd6c5aac83fc2d14c9861d9e50d94dbf9cda6b65 This PR enables support for quantized batched matrices in the BatchMatMul op. A few binary ops can also be fused to it. Transformers based models frequently uses it and the fusions (e.g., BatchMatMul + Mul + Add) in their selfattention.",2024-05-03T16:22:40Z,comp:mkl size:XL,closed,0,12,https://github.com/tensorflow/tensorflow/issues/66934, Just a headsup that https://github.com/tensorflow/tensorflow/pull/58204 PR got reverted due to ARM unit test failures. I have pushed a fix in this PR.,Hi  Can you please rebase your branch and resolve the conflicts? Thank you!,  I have resolved all the merge conflicts. Can you please review this PR? Thanks.,Hi  Can you please review this PR? Thank you!,Hi  Can you please review this PR? Thank you!,Hi  Can you please review this PR? Thank you!,Hi  Can you please review this PR? Thank you!,"Hi  , Can you please review this PR? Thank you!","Hi  , Can you please review this PR? Thank you!","Hi  , Can you please review this PR? Thank you!","Hi  , Can you please review this PR? Thank you!",Quantization support will provided through XLA.
yi,copybara-service[bot],Inline single usages of `SymbolTable` to avoid excessive copy.,Inline single usages of `SymbolTable` to avoid excessive copy.,2024-05-03T04:49:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66908
yi,mmayers88,libhexagon_interface.so for non Android - eLinux platform,**System information**  OS Platform and Distribution :  Linux Ubuntu 20.04  Mobile Device: Qualcomm QRB5165/RB5  TensorFlow installed from Source  TensorFlow version 2.16.1  Bazel version (if compiling from source): 6.5.0  GCC/Compiler version (if compiling from source): aarch64linuxgnu 11.3.0 **Describe the problem** Hello. I'm trying to get hexagon delgate to work on Qualcomm's QRB5165/RB5 platform. This platform is based on Snapdragon 8xx series. But the problem is that this platform works on a Debian Linux. The currently available libhexagon_interface.so doesn't work as it has Android dependencies. I cannot find the source code of libhexagon_interface.so in tensorflow git. Is it possible to get a complied binary of libhexagon_interface.so for aarch64 Linux without Android dependencies?  It seems like this has been done before  CC(libhexagon_interface.so for non Android  eLinux platform) and I would appreciate a binary as well. Thank you,2024-05-02T20:19:14Z,stat:awaiting response type:build/install stale comp:lite TFLiteGpuDelegate,closed,0,13,https://github.com/tensorflow/tensorflow/issues/66872,"Maybe I should edit the question. I am having a problem running the benchmark with the Hexagon DSP chip. I am using a Debian Linux, and not Android.  I can access and run other scripts on the Hexagon DSP, however I am having a lot of problems with Tensorflow. The Hexagon Delegate instructions seem to be out of date and are only for Android, even though it seems that I should be able to compile the necessary files. The Skeleton files: libraries “libhexagon_nn_skel.so”, “libhexagon_nn_skel_v65.so” and “libhexagon_nn_skel_v66.so” are all precompiled, so I am stuck with the latest version being  v1.20.0.1, and I can't even find that version of Tensorflow. The error I am running into is:  How Can I get a libehexagon_interface, libhexagon_nn_skel, and libhexagon_delegate that all work together?","Hi  , From what i know, i think the libhexagon_nn_skel.so and other "".so"" files are provided by Qualcomm itself and is a part of snapdragon SDK instead of tensorflow team. Can you please check on Qualcomm forums and let me know if you are able to find them.","I've already checked on there. From what I understand the interface and delegate come from you. I have successfully generated a libhexagon_nn_skel.so, but I'm not sure about the other files and generating them. Is it possible to generate the libhexagon_interface.so as was done for other people?","Sorry, i misunderstood the question first time, can you please check this link and let me know if this is the one you are looking for .",I've tried that. Would I just build the interface with bazel as:  Would the same be true for building the delegate?: ,"Hi.  , Can you please suggest, which one of the above two commands will be able to build the gpu delegate ?","Hi , I think you're almost right... I can only find a config option for `linux_aarch64` https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILDL555 Can you try these and let me know if you run into any issue? ",I will try that and report back.,"Reporting back, I get this error: ","Hi , apologies, you were actually right it has an e before it:  For future reference you can find the options under your .bazelrc file in the tensorflow root. I would also run the ./configure script prior to building and answer as appropriately as you can.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only.,Internal change only. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/62750 from mattbahr:implementsampledaddmmv2 c295a0e649e86304c084b7c9b3c77bdf0d2ed8fb,2024-05-02T19:28:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66866
yi,andremfreitas,Dataset sharding warning," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.3 LTS  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory A100 40GB  Current behavior? Hi, I am using a mirrored strategy for gpu parallelisation. This is how I prepare my dataset:  Which is resulting in the warning below (see relevant log output).  This (closed) issue mentions something similar:  CC(Dataset sharding in MultiWorker Mirrored Strategy). I would expect this warning not to appear in tf2.16 anymore. I am not trying to do data sharding of a a file or files so this warning shouldn't be thrown in my opinion (and it's annoying). But please let me know if it makes sense to be thrown in a case like this. Thanks, Andre  Standalone code to reproduce the issue   Relevant log output ",2024-05-02T15:32:01Z,stat:awaiting response type:support stale comp:data TF 2.16,closed,0,4,https://github.com/tensorflow/tensorflow/issues/66851, Could you please provide a standalone code to replicate the issue reported here as it would help us to analyze the issue? Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[XLA:GPU][MLIR-Based emitters] Use xla_gpu.apply_indexing instead of affine.apply.,[XLA:GPU][MLIRBased emitters] Use xla_gpu.apply_indexing instead of affine.apply.,2024-05-02T14:43:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66849
yi,copybara-service[bot],[XLA:GPU][MLIR-based emitters] Add a pattern to fold constant operands for apply_indexing op.,[XLA:GPU][MLIRbased emitters] Add a pattern to fold constant operands for apply_indexing op.,2024-05-02T13:02:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66843
yi,copybara-service[bot],[XLA:GPU][MLIR-based emitters] Remove unused results from apply_indexing op.,[XLA:GPU][MLIRbased emitters] Remove unused results from apply_indexing op.,2024-05-02T11:15:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66841
rag,RSRcoder001,libtensorflow-cpu-windows-x86_64-2.15.0,ref Windows CPU link: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflowcpuwindowsx86_642.15.0.zip The zip file for the C api seems to be missing files e.g. tf_buffer.h.,2024-05-02T10:37:13Z,stat:awaiting response type:bug type:build/install stale subtype:windows,closed,0,5,https://github.com/tensorflow/tensorflow/issues/66835,"Hi **** ,  Thanks for reporting issue i also observed the same, We will check with developers and provide the resolution. Thank you!","Hi **** ,  Could you please check with latest version?. it might be fix with latest version. i am adding **commit** ,  CC(also missing from the include ""patch"" is file ""tensorflow/tsl/c/tsl_status.h"" (there might be others)) for here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only.,Internal change only.,2024-05-01T10:34:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66781
yi,LongZE666,Aborted (core dumped) in `tf.raw_ops.SparseBincount`, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Multiplication of `dense_shape` and `size` causes integer overflow  Standalone code to reproduce the issue   Relevant log output ,2024-05-01T03:53:11Z,stat:awaiting tensorflower type:bug comp:ops TF 2.16,open,0,1,https://github.com/tensorflow/tensorflow/issues/66766, I was able to replicate the issue reported here. Thank you!
agent,LongZE666,Segmentation fault (core dumped) in `tf.raw_ops.FractionalMaxPoolGrad`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? If `row_pooling_sequence` is a minimal negative number, `tf.raw_ops.FractionalMaxPoolGrad` encounters ""Segmentation fault (core dumped)"".  Standalone code to reproduce the issue   Relevant log output ",2024-05-01T03:27:14Z,stat:awaiting response stat:awaiting tensorflower type:bug stale comp:ops TF 2.16,closed,0,5,https://github.com/tensorflow/tensorflow/issues/66760,", Thank you for reporting the issue. I was able to reproduce the issue on tensorflow v2.15, v2.16 and tfnightly. Kindly find the gist of it here and the screenshot of the check fail for the reference. !image",", I tried to execute the mentioned code on the Tensorflow v2.16 in the alternative approach and it was failing with the error which was intended. Kindly find the gist of it here.  row_pooling_sequenceA Tensor of type int64. row pooling sequence, form pooling region with col_pooling_sequence Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Fix HloAyncStartInstruction::ClassOf,Fix HloAyncStartInstruction::ClassOf Ignore verifying computation parameter/result shapes for customcall async ops,2024-05-01T01:27:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66754
yi,gustavla,GPUv2 numerical inaccuracy in simple Add + Mul,"**System information**  Google Pixel 7 / Android 14 / Google Tensor G2  TensorFlow 2.16.1 Running a simple model with a Add + Mul on both GPU and CPU gives vastly different results (on device CPU confirmed separately to be correct):  **Standalone code to reproduce the issue**  * Model asset: [tflite_66740_add_mul_gpu_numerically_incorrect.tflite * Generate some data and run on device on GPU (and optionally CPU for baseline). * This results in the above severe numerical discrepancy **Inference repro** I reproduced this using https://aihub.qualcomm.com/ out of convenience (but I'm sure it will repro through other means as well), so I'm attaching the script here.  **Model visualization**  **Any other info / logs**  Logs from the GPU inference job (via https://aihub.qualcomm.com/): ",2024-04-30T22:23:44Z,stat:awaiting tensorflower comp:lite TFLiteGpuDelegate Android,open,0,9,https://github.com/tensorflow/tensorflow/issues/66740,"Hi  , I am in the process of replicating your issue . I will get back to you as soon as poosible. ","Hi  , I replicated your issue and i got the same result as yours . I will try to replicate the issue through an android app and will let you know . ","Hi  , Can you please reproduce this error through an Android app please and let me know?  That way it will be easier for me to debug the issue.", I do not have a standalone app with a repro. How do you usually feed in data to TFLite for the purpose setting up repros? It is easy to dump out the inputs from numpy to another format you need (happy to show how).,"Hi  , Usually its either images or video data which I use as input . But here i need to create .npy files or put the inputs in a text file and then through the java code i need to load and do inference on it. I am having error loading .npy files in Java/Kotlin in Android. ","Hi,   Please take look into this issue. Thank you","I don't trust the emulator GPU delegate workflow to test this properly... , can you please take a look? Thanks.",Reproduced the error with run_delegate_testing.sh It looks like the CPU is outputting zeros for many of the elements (at least for the inputs generated by run_delegate_testing) whereas the GPU is producing nonzero values. Curious how that occurs for the ops in the model... Will investigate next week,"Hi , do you have any updates on this issue? I've encountered similar issue in much larger network but probably root cause is the same as in this simple example."
rag,copybara-service[bot],[xla:ffi] Revised the list of supported attribute types and added tests,"[xla:ffi] Revised the list of supported attribute types and added tests Turns out the previous commit did not update the runtime, this the FFI failed with an error when constructing a frame with e.g. bool attributes. Here I decided to * Only support signed integer attributes, i.e. leave out unsigned integer   types until we have a usecase which needs them. * Drop support for boolean arrays as attributes. The reasons are purely   techincal: MLIR stores them as std::vector which has   implementationdefined storage. Thus, we have to copy the vector to be able   to expose it as XLA_FFI_Array.",2024-04-30T22:12:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66739
rag,gestalone,2.12.0: memory leak in TFLite's tflite::Interpreter::Invoke()," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.12.0   Custom code Yes  OS platform and distribution Crossbuild from 'Windows:x86_64' to 'Android:armv8'  Mobile device Android with Snapdragon 820  Python version _No response_  Bazel version _No response_  GCC/compiler version  CXX compiler identification is Clang 14.0.7  CUDA/cuDNN version no  GPU model and memory Snapdragon 820 with Adreno 530  Current behavior? Running the invoke for a tflite model using the gpu delegate, with opencl backend.  It goes fast and well, the problem is that exist a memory leak, that is increasing, not sure how to fix it. Not sure if it's an error on the opencl implementation, on the drivers of the adreno gpu or in the delegate implementation. !image  Standalone code to reproduce the issue   Relevant log output _No response_",2024-04-30T21:44:09Z,type:bug comp:lite TFLiteGpuDelegate TF 2.12,closed,0,12,https://github.com/tensorflow/tensorflow/issues/66736,I am able to reproduce this in the benchmark android_aarch64_benchmark_model of the tf nightly build, can you take a look?, Could you please try to upgrade to the latest TF version as memory leak issues are often addressed in subsequent versions. Kindly let us know if it is appearing in the latest and try to explore if your stable delegate library supports alternative backends besides OpenCL? Thank you!,"Hi , I was able to reproduce it with the benchmark 2.16 version. I tried the opengl backend of the gpu delegate, but unfortunately is not working.  it's quite easy to test, just use the android benchmark, with any tensorflow approved model using the gpu delegate.  The tutorial can be followed from here.  https://www.tensorflow.org/lite/performance/measurement I think should be fixed, as memory leaks give quite a lot of issues.",", any help here?","Hi  , Sure thing, let me replicate your issue . However gpu delegates today primarily use ""openCL"" as their backend instead of ""openGL"" . I will get back to you .","Hi ! The issue I had was using opencl backend, i was not able to use opengl","Hi  , I used ""android_aarch64_benchmark_model"" on pixel 6a to test a  tflite model using the below command      I used android  stuido pofiler to check the memory used by the ""tflite benchmark activity ""  process and it didn't show any memory leaks . The memory usage spiked up to 130 MB when using the benchmark tool but it came back to normal once the benchmarking was complete.  Can you please try out your code on a different phone and let me know if you are able to replicate this issue on a different phone. Also if possible , can you provide your tflite model for easier debugging for me.","!image !image Hi! I am using a different device and I am not able to reproduce, then I guess is device related. MAybe the opencl drivers? I will check. both run with the same command: !image","I was looking a bit and I found this: https://developer.qualcomm.com/forum/qdnforums/software/adrenogpusdk/35473 for the model that i Found, I will try to change the buildoptions of opencl for adreno 530. I will let you know","  Hi! I found the issue. Seems the snapdragon profiler program that i use to check the memory make a bad interaction with the opencl thing. I check it with a different method to get the memory and is not reproducible. Then I guess the issue was to run the benchmark and my code with the snapdragon profiler.  really strange interaction, should be reporte to qualcomm.  Best you can close and thanks for all the help",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only.,Internal change only.,2024-04-30T21:08:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66733
yi,gustavla,TFLiteConverter produces model that doesn't conform to GPUv2 (TfLiteGpuDelegate Init: FULLY_CONNECTED: Amount of input channels should match weights width),"**System information**  Compilation: Ubuntu Linux  Execution: Google Pixel 7 / Android 14 / Google Tensor G2  Tensorflow 2.16.1 **Standalone code to reproduce the issue**  * Generate model with script below (note, this fails with the reshape, but the TFLiteConverter ends up with a model without the reshape) * Run on GPUv2 on device (for instance via benchmark tool) * Falls back to run 100% on CPU  **Generate model script**  **Any other info / logs** Runtime log (executed by https://aihub.qualcomm.com/)  ",2024-04-30T20:20:59Z,stat:awaiting response type:bug stale comp:lite TFLiteGpuDelegate TF 2.16,closed,0,14,https://github.com/tensorflow/tensorflow/issues/66729,"hi  , I tired to create the tflite model from the code you provided but i ran into this error and no tflite model was created.  ValueError                                Traceback (most recent call last) Cell In[1], line 9       5 output_shape = [1, 21]       7 tf_input = keras.Input(input_shape[1:], batch_size=input_shape[0]) > 9 tf_z = tf.reshape(tf_input, (1, 7))      10 tf_output = tf.keras.layers.Dense(21)(tf_z)      12 model = keras.Model(inputs=[tf_input], outputs=[tf_output]) File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\ops\weak_tensor_ops.py:88, in weak_tensor_unary_op_wrapper..wrapper(*args, **kwargs)      86 def wrapper(*args, **kwargs):      87   if not ops.is_auto_dtype_conversion_enabled(): > 88     return op(*args, **kwargs)      89   bound_arguments = signature.bind(*args, **kwargs)      90   bound_arguments.apply_defaults() File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\util\traceback_utils.py:153, in filter_traceback..error_handler(*args, **kwargs)     151 except Exception as e:     152   filtered_tb = _process_traceback_frames(e.__traceback__) > 153   raise e.with_traceback(filtered_tb) from None     154 finally:     155   del filtered_tb File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\keras\src\backend\common\keras_tensor.py:91, in KerasTensor.__tf_tensor__(self, dtype, name)      90 def __tf_tensor__(self, dtype=None, name=None): > 91     raise ValueError(      92         ""A KerasTensor cannot be used as input to a TensorFlow function. ""      93         ""A KerasTensor is a symbolic placeholder for a shape and dtype, ""      94         ""used when constructing Keras Functional models ""      95         ""or Keras Functions. You can only use it as input to a Keras layer ""      96         ""or a Keras operation (from the namespaces `keras.layers` ""      97         ""and `keras.operations`). ""      98         ""You are likely doing something like:\n\n""      99         ""\n\n""     104         ""What you should do instead is wrap `tf_fn` in a layer:\n\n""     105         ""\n""     111     ) ValueError: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:  What you should do instead is wrap `tf_fn` in a layer: ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,I wrote the repro script against tensorflow 2.15 (even though I ran on TFLite 2.16 runtime). Model: https://qaihubpublicissues.s3.uswest2.amazonaws.com/tflite/66729_model.tflite,"Hi  , The script that you used to create the tflite model seems to be the issue. The ""input_shape"" and ""output_shape"" should be a tuple when you are passing it to the input layer. Also because of an issue keras tensors are incompatible with tf tensors. I will suggest you to use the below script to create the model. The tflite file created with the below script should run perfectly fine using the gpu delegate. Please run this colab notebook to create your tflite model"," Thanks! I tried changing to tuples, and that doesn't seem to make any difference. As for keras vs tflite tensors, I am unable to view your colab link (I requested access).","Hi  , I have given you the access to view the colab notebook, Please review it and let me know if the tflite model created through this colab link solves your problem.","TF 2.16 uses keras 3, which results in the above error. add   to the above script to make it compile","Hi  , can you confirm if the code provided in the above colab link worked for you?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,keeping this thread alive while  is out.,"Hi aarts , The compatibility issue between keras3 and tf has been resolved in the tf version 2.17.0 . You can create your model using this version and let me know if it works out . ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Mark dots to be unpropogatable on space-to-batch conversion. Delaying the conversion from dots to convs to occur post layout assignment caused this.,Mark dots to be unpropogatable on spacetobatch conversion. Delaying the conversion from dots to convs to occur post layout assignment caused this.,2024-04-30T20:11:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66728
yi,gustavla,Op support request: Matmul with constant left hand side,**System information**  Samsung Galaxy S23 / Android 13 / Snapdragon® 8 Gen 2 | SM8550  TFLite 2.16.1 (stock) **Standalone code to reproduce the issue**  Code to generate model:  **Any other info / logs** Runtime log (executed on https://aihub.qualcomm.com/):   Full log: ,2024-04-30T19:58:24Z,stat:awaiting response type:support stale comp:lite,closed,1,7,https://github.com/tensorflow/tensorflow/issues/66727,"Hi  , I ran the code you provided to create the tflite model and i am getting the below error. Can you share me the tflite model itself.  AttributeError                            Traceback (most recent call last) Cell In[6], line 21      19  Convert the model.      20 converter = tf.lite.TFLiteConverter.from_keras_model(model) > 21 tflite_model = converter.convert()      23  Save the model.      24 with open('model.tflite', 'wb') as f: File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py:1175, in _export_metrics..wrapper(self, *args, **kwargs)    1172 .wraps(convert_func)    1173 def wrapper(self, *args, **kwargs):    1174    pylint: disable=protectedaccess > 1175   return self._convert_and_export_metrics(convert_func, *args, **kwargs) File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py:1129, in TFLiteConverterBase._convert_and_export_metrics(self, convert_func, *args, **kwargs)    1127 self._save_conversion_params_metric()    1128 start_time = time.process_time() > 1129 result = convert_func(self, *args, **kwargs)    1130 elapsed_time_ms = (time.process_time()  start_time) * 1000    1131 if result: File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py:1641, in TFLiteKerasModelConverterV2.convert(self)    1637 if saved_model_convert_result:    1638   return saved_model_convert_result    1640 graph_def, input_tensors, output_tensors, frozen_func = ( > 1641     self._freeze_keras_model()    1642 )    1644 graph_def = self._optimize_tf_model(    1645     graph_def, input_tensors, output_tensors, frozen_func    1646 )    1648 return super(TFLiteKerasModelConverterV2, self).convert(    1649     graph_def, input_tensors, output_tensors    1650 ) File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\convert_phase.py:215, in convert_phase..actual_decorator..wrapper(*args, **kwargs)     213 except Exception as error:     214   report_error_message(str(error)) > 215   raise error from None File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\convert_phase.py:205, in convert_phase..actual_decorator..wrapper(*args, **kwargs)     202 .wraps(func)     203 def wrapper(*args, **kwargs):     204   try: > 205     return func(*args, **kwargs)     206   except ConverterError as converter_error:     207     if converter_error.errors: File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py:1582, in TFLiteKerasModelConverterV2._freeze_keras_model(self)    1573  If the model's call is not a `tf.function`, then we need to first get its    1574  input signature from `model_input_signature` method. We can't directly    1575  call `trace_model_call` because otherwise the batch dimension is set    1576  to None.    1577  Once we have better support for dynamic shapes, we can remove this.    1578 if not isinstance(self._keras_model.call, _def_function.Function):    1579    Pass `keep_original_batch_size=True` will ensure that we get an input    1580    signature including the batch dimension specified by the user.    1581    TODO(b/169898786): Use the Keras public API when TFLite moves out of TF > 1582   input_signature = _model_input_signature(    1583       self._keras_model, keep_original_batch_size=True    1584   )    1586  TODO(b/169898786): Use the Keras public API when TFLite moves out of TF    1587 func = _trace_model_call(self._keras_model, input_signature) File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\tflite_keras_util.py:84, in model_input_signature(model, keep_original_batch_size)      82   input_specs = input_specs[0][0]      83 else: > 84   input_specs = model._get_save_spec(   pylint: disable=protectedaccess      85       dynamic_batch=not keep_original_batch_size)      86   if input_specs is None:      87     return None AttributeError: 'Functional' object has no attribute '_get_save_spec'","Sorry, I think the repro script requires tensorflow 2.15. Here is the model: https://qaihubpublicissues.s3.uswest2.amazonaws.com/tflite/66727_model.tflite","Hi,   I apologize for the delayed response, I was trying to replicate the same behavior from my end, I was able to run your code successfully with TensorFlow version `2.15.0` and TFLite model got created here gistfile for reference after that I did login into https://aihub.qualcomm.com/ and created API token so if you don't mind could you please help me with further steps to replicate the same behavior from my end that will be great ?  Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,gustavla,GPUv2 segfaults on split-head attention CLIP model,**System information**  Google Pixel 7 / Android 13 / Google Tensor G2  TFLite 2.16.1 (stock) **Standalone code to reproduce the issue**  Model asset: tflite_66721_sha_clip_gpuv2_segfault.tflite Run model through TFLite (GPUv2) on an Android device (for instance through benchmark tool). **Any other info / logs** Runtime log (executed on https://aihub.qualcomm.com/)  ,2024-04-30T17:44:14Z,stat:awaiting tensorflower type:support comp:lite TFLiteGpuDelegate TF 2.16,closed,0,7,https://github.com/tensorflow/tensorflow/issues/66721,"Hi  , Did you try out this model on other devices? Also can you please provide me the inference code you used on qualcom ai hub ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"I just tried it on a few devices and it only repros on the Pixels that I tried: * Google Pixel 7: Segfaults * Google Pixel 8: Segfaults * Samsung Galaxy S23: OK (100% GPUV2 (OpenCL), 81.6 ms / 0  366 MB peak memory) * Xiaomi Redmi Note 10 5G: OK (100% GPUv2 (OpenCL), 376.2 ms, 0  352 MB peak memory)  Here is the submission:  Or ","Hi,  I apologize for the delayed response, I am able to replicate similar behavior from my end for reference I've attached runtime log file so we'll have to dig more into this issue.  Hi,  Please look into this issue. Thank you","Emulator is unable to properly test GPU delegates. , can you please take a look? Thanks.","Hi,  Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/65 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
yi,gustavla,GPUv2 syntax error in shader generation in broadcast with different batch sizes,"**System information**  Samsung Galaxy S23 / Android 13 / Snapdragon® 8 Gen 2 | SM8550   TFLite 2.16.1 (also repros on many older versions) **Standalone code to reproduce the issue**  Compile the model that fails:  Execute on GPUv2 (for instance through benchmark_tool). It falls back to CPU and executes without a problem there. **Any other info / logs** Runtime log (executed through https://aihub.qualcomm.com/):  There are many input shapes that will trigger this (NG), but some execute just fine (OK): ",2024-04-30T16:15:55Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TFLiteGpuDelegate TF 2.16,closed,0,10,https://github.com/tensorflow/tensorflow/issues/66712,"Hi  , Have you tried replicating this particular issue on other android devices or this happens only on galaxy s23?", I also tried it on a Google Pixel 7 and it also falls back to CPU.,"Hi  , I ran the code to create the tflite model but it gave me the below error:  AttributeError                            Traceback (most recent call last) Cell In[8], line 23      21  Convert the model.      22 converter = tf.lite.TFLiteConverter.from_keras_model(model) > 23 tflite_model = converter.convert()      25  Save the model.      26 with open('model.tflite', 'wb') as f: File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py:1175, in _export_metrics..wrapper(self, *args, **kwargs)    1172 .wraps(convert_func)    1173 def wrapper(self, *args, **kwargs):    1174    pylint: disable=protectedaccess > 1175   return self._convert_and_export_metrics(convert_func, *args, **kwargs) File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py:1129, in TFLiteConverterBase._convert_and_export_metrics(self, convert_func, *args, **kwargs)    1127 self._save_conversion_params_metric()    1128 start_time = time.process_time() > 1129 result = convert_func(self, *args, **kwargs)    1130 elapsed_time_ms = (time.process_time()  start_time) * 1000    1131 if result: File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py:1641, in TFLiteKerasModelConverterV2.convert(self)    1637 if saved_model_convert_result:    1638   return saved_model_convert_result    1640 graph_def, input_tensors, output_tensors, frozen_func = ( > 1641     self._freeze_keras_model()    1642 )    1644 graph_def = self._optimize_tf_model(    1645     graph_def, input_tensors, output_tensors, frozen_func    1646 )    1648 return super(TFLiteKerasModelConverterV2, self).convert(    1649     graph_def, input_tensors, output_tensors    1650 ) File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\convert_phase.py:215, in convert_phase..actual_decorator..wrapper(*args, **kwargs)     213 except Exception as error:     214   report_error_message(str(error)) > 215   raise error from None File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\convert_phase.py:205, in convert_phase..actual_decorator..wrapper(*args, **kwargs)     202 .wraps(func)     203 def wrapper(*args, **kwargs):     204   try: > 205     return func(*args, **kwargs)     206   except ConverterError as converter_error:     207     if converter_error.errors: File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\lite.py:1582, in TFLiteKerasModelConverterV2._freeze_keras_model(self)    1573  If the model's call is not a `tf.function`, then we need to first get its    1574  input signature from `model_input_signature` method. We can't directly    1575  call `trace_model_call` because otherwise the batch dimension is set    1576  to None.    1577  Once we have better support for dynamic shapes, we can remove this.    1578 if not isinstance(self._keras_model.call, _def_function.Function):    1579    Pass `keep_original_batch_size=True` will ensure that we get an input    1580    signature including the batch dimension specified by the user.    1581    TODO(b/169898786): Use the Keras public API when TFLite moves out of TF > 1582   input_signature = _model_input_signature(    1583       self._keras_model, keep_original_batch_size=True    1584   )    1586  TODO(b/169898786): Use the Keras public API when TFLite moves out of TF    1587 func = _trace_model_call(self._keras_model, input_signature) File ~\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\lite\python\tflite_keras_util.py:84, in model_input_signature(model, keep_original_batch_size)      82   input_specs = input_specs[0][0]      83 else: > 84   input_specs = model._get_save_spec(   pylint: disable=protectedaccess      85       dynamic_batch=not keep_original_batch_size)      86   if input_specs is None:      87     return None AttributeError: 'Functional' object has no attribute '_get_save_spec'","Just like in the other issue, I think the repro only works on tensorflow 2.15. Here is the model: https://qaihubpublicissues.s3.uswest2.amazonaws.com/tflite/66712_model.tflite","Hi  , Can you please use the code that i provided for model conversion in this issue and let me know if it resolved this issue.", I do not have access to the colab that you posted in that issue.,"Hi  , Given you the access.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,AS005209,404 - page not found,"I couldn't find anywhere else to post, so I'll just put it here. Good afternoon I am a 2nd year student, studying to be a software engineer I decided to study the topic of sound recognition, I would like to see and parse a ready project to work with tflite, which is offered by teachablemachine. Here is the link it gives, but unfortunately the empty example has been removed from the github thread, is there any way to get it or put it back in the thread. https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/android I have also looked at more complex projects, but have not been able to fully understand them yet.",2024-04-30T14:56:50Z,stat:awaiting response stale comp:lite comp:lite-examples,closed,0,3,https://github.com/tensorflow/tensorflow/issues/66708,"Hi  , Please check this link for the audio classification android example and let me know if this is the one you were looking for. ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,shenyxcode,FarthestPointSimple issue when importing meta  graph," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 1.14  Custom code Yes  OS platform and distribution Ubuntu 16.04  Mobile device _No response_  Python version 3.6  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuda 11.1  GPU model and memory _No response_  Current behavior? when loading the pretrained model 'RPointNet' from repository https://github.com/ericyi/GSPN ,it has the problem when running code `saver = tf.compat.v1.train.import_meta_graph('gspn/GSPNmaster/models/model.ckpt.meta', clear_devices=True)`, I've tried to switch several tf versions including 1.x and 2.x ,but still have the similar problem, when using tf 1.14 the traceback is KeyError, full traceback is provided below  Standalone code to reproduce the issue   Relevant log output ",2024-04-30T12:50:16Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/66705,Are you satisfied with the resolution of your issue? Yes No
finetuning,lida2003,Doc(Transfer learning and fine-tuning) is quite different from real executive result.," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.15.0+nv24.03 GPU version, check this link: https://forums.developer.nvidia.com/t/multipleexecutivewarningsafterswitchingtensorflowfrom2161cputov60dptensorflow2150nv2403gpuversion/291208  Custom code No  OS platform and distribution Jetson Orin Nano ubuntu 22.04 Jammy  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA12.2.140/cuDNN8.9.4.25  GPU model and memory sm90 8GB  Current behavior? The executive result (trend of curve and abosolute value) is quite different from document.  My result: transfer_learning.zip !图片  Document (https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb) says: !图片 !图片  Standalone code to reproduce the issue   Relevant log output Also tried Colab, which is consistent with documentation: !图片",2024-04-30T11:08:24Z,stat:awaiting response stale comp:keras type:performance TF 2.15,closed,0,16,https://github.com/tensorflow/tensorflow/issues/66696,", Could you please confirm whether the difference in Accuracy & Loss has happened in GPU/CPU with both tensorflow v2.15, v2.16? Also I will also try to debug more on this issue and provide the resolution. Thank you!","> , > Could you please confirm whether the difference in Accuracy & Loss has happened in GPU/CPU with both tensorflow v2.15, v2.16?   NVIDIA 2.15.0+nv24.03 GPU version faild 100%  Colab is consistent with tensorflow document (But I don't know the version) PS: Jetson Orin Nano 8GB, CPU&GPU shared memory > v2.16 Well, on the very begining, I have installed (pip binary installation) 2.16 CPU version on Jetson Orin Nano. Runing KerasFineTuningPreTrainedModels without any resource warning. It might be the way CPU using swap area. The result is also different from the document, See link below:  Multiple executive warnings after switching tensorflow from 2.16.1 CPU to v60dp tensorflow==2.15.0+nv24.03 GPU version When I switched to NVIDIA 2.15.0+nv24.03 GPU version: Tensorflow v2.16.1 GPU version local build on Jetson Orin Nano failed  The resource issue, we have confirmed with NVIDIA. > Also I will also try to debug more on this issue and provide the resolution. Thank you! There are also a copule of other things might be a clue for you. Here is a link on NVIDIA forum:   Tensorflow v2.16.1 GPU version local build on Jetson Orin Nano failed  Multiple executive warnings after switching tensorflow from 2.16.1 CPU to v60dp tensorflow==2.15.0+nv24.03 GPU version Please take a look at those warnings and memory issue. I think we need a sanity check before software is packed for release (put on repo). EDIT: Keep sync with NVIDIA feedback.  NVIDIA claimed their binary compiled following tensorflow guide.  Those warnings seems OK with NVIDIA 2.15 binary version.  Inconsistency of NVIDIA 2.15.0+nv24.03 v.s. Colab v.s. Tensorflow Documentation",", As you mentioned, I tried to execute the transfer learning code on the colab with the Tensorflow v2.16, v2.15 and observed that the loss and Accuracy is the same as the document. I suspect there might be an issue with the GPU. I will recheck and deep dive on the same.  !image Thank you!",  Please check Inconsistency of NVIDIA 2.15.0+nv24.03 v.s. Colab v.s. Tensorflow Documentation  JetPack 6.0 DP and tf2.15.0+nv24.04  ==> BAD  JetPack v6.0 + tensorflow2.15.0+nv24.05  ==>OK,", Could you please let me know if you are facing the same issue on TensorFlow v2.16 where keras3.0 is by default included. eras 3 is a full rewrite of Keras that enables you to run your Keras workflows on top of either JAX, TensorFlow, or PyTorch, and that unlocks brand new largescale model training and deployment capabilities. Also please try to comment this particular line and execute the code. predictions = tf.nn.sigmoid(predictions) Thank you!","> Could you please let me know if you are facing the same issue on TensorFlow v2.16 where keras3.0 is by default included. Sorry, I have met difficulties on build from source code (I have asked about some build steps, but I built without any luck)  https://forums.developer.nvidia.com/t/howtocompiletensorflowforjetsonorinnano/291321  https://forums.developer.nvidia.com/t/tensorflowv2161gpuversionlocalbuildonjetsonorinnanofailed/291195 so I have used NVIDIA binary.   2.15 is their latest build for v60 !图片  2.15 is their latest build for v60dp !图片 But I think it might be some issue with NVIDIA V60DP version, check this for details: Inconsistency of NVIDIA 2.15.0+nv24.03 v.s. Colab v.s. Tensorflow Documentation",", Is this happening with the latest tensorflow v2.17 and the kerasnightly as well. Please try to test and let us know if you are facing the same issue. Thank you!","> Is this happening with the latest tensorflow v2.17 and the kerasnightly as well. I will try if there is a 2.17 release. But right now, It's NOT supported from Jetson binary release. !图片",", https://developer.download.nvidia.com/compute/redist/jp/v60/tensorflow/  Tensorflow is not handling this files. COuld you please check with the Nvidia team on the 2.17 whl. Thank you!","OK, I have made a post for this. But I'm NOT sure when they will provide a binary for me to test. https://forums.developer.nvidia.com/t/istheretensorflow217supportedbyjetpack60/303264",", As the NVIDIA team is handling the above mentioned files, where tensorflow doesn't have not related to the same, I request  Could you please check with the Nvidia team on the 2.17 whl. Thank you? ","Thanks for remind. I checked navidia repo https://developer.download.nvidia.com/compute/redist/jp/v60/tensorflow/   As forum support(we don't have other support here), I didn't see any binary 2.17 release yet.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"> Are you satisfied with the resolution of your issue? > Yes > No >  No response from  Nvidia binary release, right now."
rag,easyhardhoon,TFLiteGPUDelegate : FirstNLargestPartitions : It would not be very good solution," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.4.1  Custom code Yes  OS platform and distribution Linux Ubuntu 18.04  Mobile device JetsonXavier nx  Python version 3.6.9  Bazel version 3.1.0  GCC/compiler version 7.5.0  CUDA/cuDNN version x  GPU model and memory _No response_  Current behavior?  Let's suppose that the input tflite model is accelerated using GPUs. If there are incompatible layers for GPU backend (called Fallback layer in tflite) in the middle of the input model, ""partition_helper class"" divides the input model into delegatable & nondelegatable partition using graph_info class. Then, Apply the FirstNLargestPartitions functions to select the part to be delegated. In FirstNLargestPartitions logic, partitions with many layers are chosen first.  Here, the variable N can be set directly by the user, and the default is 1. In this tflite's gpu delegation policy, there are two major problems, I think.   First, It may not be appropriate to preferentially select a partition with many layers containing it. In most cases, such partitions will have the highest computational amount, but there may be partitions with a small number",2024-04-30T07:47:48Z,stat:contribution welcome stat:awaiting tensorflower type:feature comp:lite type:performance TFLiteGpuDelegate,closed,0,8,https://github.com/tensorflow/tensorflow/issues/66677,"Hi  , There is a option to select the start and end nodes which you want to be delegated in the tflite benchmarking tool as you can see below.  _first_delegate_node_index: int (default=0) The index of the first node that could be delegated. Debug only. Add 'define=tflite_debug_delegate=true' in your build command line to use it. Currently only supported by CoreML delegate. last_delegate_node_index: int (default=INT_MAX) The index of the last node that could be delegated. Debug only. Add 'define=tflite_debug_delegate=true' in your build command line to use it._ This is intended for debugging purposes only.  However you raise some excellent points regarding the current delegation policy and partition selection strategy. I will get back to you if i find something related to this.","This seems like a reasonable request, , can you please take a look? Thanks.","This seems good to me. This is pretty low priority because there is a workaround as listed above if need be. And moreover, in general we will delegate the entire graph to GPU because context switching is so slow. However, if you were to implement a clean solution, I would approve it.","Thank you for your reply      . It would be reasonable to delegate the entire graph to the GPU, but in reality, I think there could be enough layers that are incompatible with the GPU. For models with such conditions, I think there is room for a little more improvement on the current policy of delegating to the GPU. I'll think about a more efficient policy in terms of inference latency and user convenience. I'll get back to you. Thank you","I faced a similar issue when trying to provide a better CPUGPU op assignment to speed up latency. Would like to provide some of my views on this topic about FirstNLargestPartitions. In my case my model graph is partitioned as below, each number representing the number of operators in the partition:  Firstly, I agree that letting the user tune the N value (more precisely `max_delegated_partitions` as exposed in the delegate options API) is kind of confusing. The main issue is that from the API level, there is currently no way for the user to understand how the graph is being partitioned, unless we manually insert logs in the `PartitionImpl()` and do a custom build (which is what I did). So I think exposing the partitions, and the op index in each partition, might be helpful. Secondly, given that we have the partition information, I think one way to determine the optimal N is by using the elbow method), i.e. choose a point where diminishing returns (decreasing number of ops) are no longer worth the additional cost (CPUGPU data transfer). In my case it's obvious that N = 2, which is to put the 935 + 854 operators on GPU. I would suggest to use elbow method to determine a default N value, while still allowing users to overwrite it (at their own risk) by passing in `max_delegated_partitions`. Thirdly, regarding the issue  mentioned about ""computation cost of ops"" > ""quantity of ops"", I personally think in most of the cases, more ops > more computation cost, so it's a good enough heuristic. If we really want to account for computation cost, one way is to keep a lookup table of the estimated cost of each op, and take the cost as weights when sorting the partitions. But I suppose that would add complexity from a code maintenance point of view, so I am not sure if it's worth doing it. Hope this help, , happy to contribute on this.","Hello, I haven't been able to focus on this issue due to working on another paper. I submitted this issue to a conference held in Korea. The attached PDF is the poster presented at the time. Like the discussion in the poster, I believe a more refined delegation policy is needed. As  mentioned, adding a lookup table for the estimated cost of each op would increase TensorFlow Lite's code complexity. Therefore, I see two possible improvement directions.  A benchmarking tool (profiling tool) This would perform delegation for all possible combinations in models with fallback (nonGPU compatible) layers to find the optimal setup.  FLOPsbased partition selection policy (an advanced version of FirstNLargest) While FirstNLargest prioritizes partitions with more layers, this can be ineffective. I'm researching formulas to calculate MAC and FLOPs for ML layers. A FLOPsbased policy could lead to more efficient inference than the current method. Unlike the benchmarking (profiling) tool, the FLOPsbased partition selection policy has the advantage of not requiring a preprocess. However, as shown in the attached PDF, always selecting the partition with the highest FLOPs may not always be effective, even though it's more efficient than FirstNLargest. I would appreciate your feedback on these two proposed approaches. Thank you.      Cons_of_FirstNLargest.pdf","Hi,  Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/66 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Define GetDefaultLayoutForDevice for CompileOnlyIfRtClient.,Define GetDefaultLayoutForDevice for CompileOnlyIfRtClient.,2024-04-29T22:02:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66651
yi,copybara-service[bot],PR #11880: InitializeBuffer: use maximum 2 h2d copies,"PR CC(fix minor typo): InitializeBuffer: use maximum 2 h2d copies Imported from GitHub PR https://github.com/openxla/xla/pull/11880 This provides a significant speedup when autotuning, where buffers are repeatedly initialized. Instead of repeatedly copying the same data to different locations on the device, it is copied once (using up to two host to device copies) and then replicated on the device using a custom kernel. Using the paxml container from JAXToolbox and the 5B configuration on H100, the total runtime of the `gemmalgorithmpicker` pass for the main JITed function decreased from around 4.8s to 0.16s. With the same model and `xla_gpu_triton_gemm_any=true` to make sure the Triton autotuner does a lot of work, that pass speeds up from 17.2s to 14.2s with this change. Copybara import of the project:  bc9de3720516b4450d3a9791eae8a6979a3a08a9 by Olli Lupton : InitializeBuffer: use maximum 2 h2d copies This provides a significant speedup when autotuning, where buffers are repeatedly reinitialized. Instead of repeatedly copying the same data to different locations on the device, it is copied once (using up to two host to device copies) and then replicated on the device using a custom kernel.  ee92fc52d",2024-04-29T17:35:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66634
yi,abcd123abcd,make error in Tensorflow Lite Micro ,"System information Linux Ubuntu 18.04 Python version: 2.7.17 The problem I'm trying to generate the example projects from the TensorFlow Lite for Microcontrollers C++ library with Make and I run in this error: Traceback (most recent call last):   File ""tensorflow/lite/micro/tools/make/generate_keil_project.py"", line 122, in      parse_args()   File ""tensorflow/lite/micro/tools/make/generate_keil_project.py"", line 118, in parse_args     main(unparsed, flags)   File ""tensorflow/lite/micro/tools/make/generate_keil_project.py"", line 40, in main     six.ensure_str(flags.executable), AttributeError: 'module' object has no attribute 'ensure_str' tensorflow/lite/micro/examples/network_tester/Makefile.inc:41: recipe for target 'tensorflow/lite/micro/tools/make/gen/linux_x86_64_default/prj/network_tester_test/keil/keil_project.uvprojx' failed make: *** [tensorflow/lite/micro/tools/make/gen/linux_x86_64_default/prj/network_tester_test/keil/keil_project.uvprojx] Error 1 Sequence of commands git clone https://github.com/tensorflow/tflitemicro cd tflitemicro make f tensorflow/lite/micro/tools/make/Makefile generate_projects",2024-04-29T12:18:03Z,stat:awaiting response type:build/install stale comp:lite comp:micro,closed,0,4,https://github.com/tensorflow/tensorflow/issues/66616,"Hi  , I tried replicating your error but i kept running into library issues. Can you please update your python version and then try installing tflite micro and let me know if the issue still persists for you .",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,harrylal, TfLiteGpuDelegate Init: BATCH_MATMUL: Node 333 is already a consumer of the value 334," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16  Custom code Yes  Mobile device Android 10  GPU model and memory Mali  G72*3   Tflite Model tflite model  Current behavior? Expected Behavior: The model should initialize and run on the GPU as it does on the CPU. Actual Behavior: When attempting to run the model on the GPU using the TensorFlow Lite GPU delegate, the initialization fails.  Standalone code to reproduce the issue   Relevant log output ",2024-04-29T03:53:38Z,stat:awaiting response type:support comp:lite TF 2.16,closed,0,3,https://github.com/tensorflow/tensorflow/issues/66597," Could you try for disabling specific GPU delegate optimizations related to Batch Matrix Multiplication, if applicable? Thank you!",  Thank you for the response.  Exporting the model on a different enviornement (ultralytics docker) resolved this issue.,Are you satisfied with the resolution of your issue? Yes No
text generation,Coder-HuangBH,IllegalArgumentException: Internal error: Failed to run on the given Interpreter," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version org.tensorflow:tensorflowlite:2.11.0  Custom code Yes  OS platform and distribution win10  Mobile device Android12  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I want to output the text normally, but it's going to be an exception  Standalone code to reproduce the issue   Relevant log output ",2024-04-29T02:27:23Z,stat:awaiting response type:bug stale comp:lite Android,closed,0,4,https://github.com/tensorflow/tensorflow/issues/66594,"hi HuangBH , Can you please briefly describe the error, i can see two inference results from the logs,  but i don't understand if they are from the same interpreter or different interpreter , also can you share your inference code from the android .",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,kotKolil,ImportError: DLL load failed while importing _pywrap_tf2," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution windows 10   Mobile device _No response_  Python version 3.12.1  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version not have  GPU model and memory Intel UHD Graphics  Current behavior? Hello evereone! I tried to run this code snippet:  with this libraries, installed via pip. List is here: https://pastebin.com/jM46gp9T But i get this error:     **from tensorflow.python.platform import _pywrap_tf2 ImportError: DLL load failed while importing _pywrap_tf2: Произошел сбой в программе инициализации библиотеки динамической компоновки (DLL).**  Standalone code to reproduce the issue   Relevant log output ",2024-04-28T13:35:54Z,stat:awaiting response type:bug subtype:windows TF 2.16,closed,0,2,https://github.com/tensorflow/tensorflow/issues/66588,"Hi  , It seems you are using tensorflow 2.16.1 which is a GPU package. GPU support not available for Windows OS since TF2.11v. Please refer the doc source for same. Either you need to install WSL2 for enabling GPU support or use tensorflow_cpu package. For more details please refer to the documentation attached.",Are you satisfied with the resolution of your issue? Yes No
yi,Raaed-Ilham,TFLite Multipose model input error for android,"I have been trying to work with the movenet multipose lightning model. The model gives the following error  ` java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (serving_default_input:0) with 3 bytes from a Java Buffer with 122880 bytes.`  The model, downloaded from the official tfhub/kaggle page [](https://www.kaggle.com/models/google/movenet/tfLite),   requests inputs of (1,1,1,3) of Datatype.UINT8  However in the documentation on the model page it asks for inputs in the form of 1*H*W*3 where h and w are multiples of 32  ideally 160,256 for example.  I have opened this issue on recommendation from  after viewing another user with a similar issue given here:  [](https://github.com/tensorflow/tensorflow/issues/53127) CC(TFLite Movenet multipose/lightning input error)   Currently if 1,1 are given as H and W it results in a bunch of random points on the left hand side of the phone screen which do not seem to move with the user. Any changes to the input size of the image and model only change the latter java buffer size, the tensor image size always remains at 3 bytes in the error messages.  Please let me know if its possible to resolve this issue or if theres something wrong with t",2024-04-27T14:54:55Z,stat:awaiting response type:bug stale comp:lite Android,closed,0,7,https://github.com/tensorflow/tensorflow/issues/66564,"Hi Ilham , I will replicate the issue and i will get back to you .","Hi Ilham , Did you check out this . This example uses the movenet thunder model similar to yours.","y > Hi Ilham , >  > Did you check out this . This example uses the movenet thunder model similar to yours. Yes that was what I initially tried but I cant build the sample app due to a gradle build issue which is a whole other issue same error for someone who tried building the sample app for another classifier CC(Build a handwritten digit classifier app with TensorFlow Lite, step 3  FAIL)  will look into it a bit more then but it uses the same model so it shouldnt be an issue I have compared the code as well.","Hi Ilham , Try with the gradle version 7.3.3 and let me know if you were able to build the project.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[pjrt] NFC: Always use concrete async value in PjRtFuture<T>,[pjrt] NFC: Always use concrete async value in PjRtFuture We plan to make absl::StatusOr an implicit payload of PjRtFuture and for consistency always use AsyncValueRef with concrete payload instead of relying on AsyncValue error semantics. PjRtFuture can't be constructed from async values passed from a user (only from a promise) so we can safely ignore the error bit as we never use it. This is a first CL in preparation for making absl::StatusOr implicit in PjRtFuture.,2024-04-27T04:33:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66548
yi,stellarpower,"Suspected Corner Case in XLA Compilation - vectorized_sum, conditional, scatter_nd_update Complains about Dynamic Shape when we should Know it"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.15.0; nightly  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I seem to have hit what I think is a really weird cornercase in XLA compiling. My code may have a problem in it, this is the first time I'm using XLA, but, I seem to need a particular combination of factors for the bug to reproduce. The compiler is complaining about not being able to operate using dynamic shapes, although these should be known (or inferred) by the graph compiler. I don't know if some errors that could have come up are elided away as the optimisations are applied, but reproducing it required a combination of a conditional, a vectorised map, and a matrix generated using reduce_sum, as well as passing these between functions. Change some or all of these elements to try and pin the bug down and the code compiled fine. I'm working on an implementation of Soft Dynamic Time Warping as a loss function. ",2024-04-26T20:00:08Z,stat:awaiting response type:bug stale comp:xla TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/66535, I was able to replicate the issue reported here. Could you please confirm the results? Also please try without vectorization? We will analyze if a nonvectorized version compiles correctly? Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],Remove invalid quantized tests,Remove invalid quantized tests Quantized tensors have a constraint that `sizeof(storage_type) < sizeof(expressed_type)`. This CL removes quantized tensor tests that were quantizing from BF16/F16 to SI16 and BF16/F16/F32 to SI32.,2024-04-26T17:08:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66522
yi,copybara-service[bot],PR #10782: Offloading 2/3: generate multi-stream async copies using events on GPUs,PR CC(Fix 9392): Offloading 2/3: generate multistream async copies using events on GPUs Imported from GitHub PR https://github.com/openxla/xla/pull/10782 Emit asynchronous memory copies between hosts and devices using additional streams while the main stream does the computation. The async copies are guarded by RecordEvent() and WaitForEvent() created by the copystart/copydone thunks respectively. A hash table is utilized to map copystart instructions to events. The corresponding events will be waited at copydone and extracted from the hash table. Copybara import of the project:  a9271ee9b99eb43a0a0806c8dbdfc7753aef4644 by Jane Liu : Add the multistream implementation for copystart and copydone. Merging this change closes CC(Fix 9392) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10782 from zhenyingliu:async_copy a9271ee9b99eb43a0a0806c8dbdfc7753aef4644,2024-04-26T06:34:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66492
yi,copybara-service[bot],PR #10782: Offloading 2/3: generate multi-stream async copies using events on GPUs,PR CC(Fix 9392): Offloading 2/3: generate multistream async copies using events on GPUs Imported from GitHub PR https://github.com/openxla/xla/pull/10782 Emit asynchronous memory copies between hosts and devices using additional streams while the main stream does the computation. The async copies are guarded by RecordEvent() and WaitForEvent() created by the copystart/copydone thunks respectively. A hash table is utilized to map copystart instructions to events. The corresponding events will be waited at copydone and extracted from the hash table. Copybara import of the project:  a9271ee9b99eb43a0a0806c8dbdfc7753aef4644 by Jane Liu : Add the multistream implementation for copystart and copydone. Merging this change closes CC(Fix 9392) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10782 from zhenyingliu:async_copy a9271ee9b99eb43a0a0806c8dbdfc7753aef4644,2024-04-26T06:20:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66491
yi,advnpzn,`tensorflow::RunOptions::RunOptions(void)` symbol missing in built tensorflow.dll (Windows)," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.1  Custom code No  OS platform and distribution Windows 11 23H2 Build 22631.3296  Mobile device N/A  Python version 3.11.2  Bazel version 6.5.0  GCC/compiler version LLVM/clang 17.0.6 & MSVC 19.29.30154  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? I was trying to build the `tensorflow.dll` in Windows to use the C++ API in my code. After successfully building `tensorflow.dll` and trying to use it in the code, I had faced several missing symbols error while linking. But by adding `TF_EXPORT` I was able to remove most of it. But there was still one symbol that was missing, which is `tensorflow::RunOptions::RunOptions(void)`. As far as I observed this is from a dynamically generated `config.pb.h` from `tensorflow/core/protobuf/config.proto`. What I have tried is to apply `TF_EXPORT` to the generated `config.pb.h` file in relevant places. But as soon as I run the bazel build command for the target, The `config.pb.h` and the `config.pb.cc` files are regenerated, thus whatever changes I made for exporting those symbols invalid. And after the build I still get the",2024-04-26T05:47:00Z,stat:awaiting response type:build/install stale subtype:windows TF 2.16,closed,0,6,https://github.com/tensorflow/tensorflow/issues/66490,", Could you please provide the complete error log of the installation process which helps us to debug the issue in an effective way. Thank you!",Hi   Could you please specify which logs do you want specifically? I could provide you the bazel build log for `tensorflow.dll` and the error output I get while compiling/linking a sample program with that `tensorflow.dll`.,", The logs which are available while trying to build the tensorflow on WIndows would be helpful for us to debug the issue and provide the resolution. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Fix HloAyncStartInstruction::ClassOf,Fix HloAyncStartInstruction::ClassOf Ignore verifying computation parameter/result shapes for customcall async ops  Reverts 838b2f22e36d483be7fd6cccbedd30d46497c7f3,2024-04-25T20:39:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66469
yi,copybara-service[bot],[XLA:GPU][MLIR-based emitters] Add a pattern to simplify apply_indexing op.,"[XLA:GPU][MLIRbased emitters] Add a pattern to simplify apply_indexing op. Right now it also removes unused symbols, but later unused dims will be added as well.",2024-04-25T20:38:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66468
yi,copybara-service[bot],Some preparation changes for enabling async dispatch on JAX CPU.,"Some preparation changes for enabling async dispatch on JAX CPU. To prevent too much parallelism for nonparallel computations, we add a enqueue event to make sure next computation won't be enqueued until last one is done per user thread. In `~PyArray_Storage()`, we now release the Python GIL then destroy the underlying buffer to prevent deadlock caused by interactions between argument donations and host callbacks on CPU backend.",2024-04-25T19:37:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66465
yi,Bhavi-cd,Tensorflow compatibility with pyinstaller," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Windows  Mobile device _No response_  Python version 3.11.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I have encountered a problem while running my custom trained YOLOv8obb model for object detection using an executable (.exe) file generated with PyInstaller. The model loads properly, but when attempting to perform inference, the code halts without providing any output. Steps to Reproduce:     Compile YOLOv8obb model with custom training data.     Generate an executable file using PyInstaller.     Run the executable file on a machine with compatible dependencies.     Attempt object detection using the executable.  Standalone code to reproduce the issue   Relevant log output  124851 WARNING: Failed to collect submodules for 'keras.src.backend.torch' because importing 'keras.src.backend.torch' raised: AttributeError: module 'torch' has no attribute 'float8_e4m3fn' ",2024-04-25T05:07:08Z,stat:awaiting response type:support stale subtype:windows TF 2.16,closed,0,6,https://github.com/tensorflow/tensorflow/issues/66421,"Hi **cd** ,  Ensure that the version of PyTorch used by Ultralytics (indirectly through YOLO) is compatible with the other components in your environment. If possible, try downgrading or upgrading PyTorch to a version that is known to work well with YOLOv8obb and your other dependencies.  Verify if the version of Ultralytics (8.2.2) you are using is compatible with the specific version of YOLOv8obb you have trained. Sometimes, newer or older versions of object detection frameworks like YOLO may not work seamlessly with the corresponding versions of their supporting libraries. And et us know if the issue still persists? Thank you!","Hi   I have tried downgrading the version, also when I run the .py file it is running perfectly but when I convert it to .exe these problem occur.","Hi cd ,  Could you please check your code with the recent TF versions (2.17 and tfnightly) and let us know whether the issue persists with recent TF versions. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,stellarpower,"Profiler does not Seem to Output Timesteps in xplane.pb - ""No step marker observed and hence the step time is unknown"" from Tensorboard"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15.0   (cuda120py39hb94c71b_3  from condaforge)  Custom code No  OS platform and distribution Ubuntu Jammy in podman Container  Mobile device _No response_  Python version 3.9.18  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.4 (cudacupti 12.4.127  @ h59595ed_1   from condaforge)  GPU model and memory RTX 3090, 24GiB  Current behavior? I am in the process of writing a custom loss function, and trying to profile it to see where resources are currently used. I have installed newer CUDA drives, the latest release of Tensorflow (2.15), the CUDA PTI libraries, and other dependencies needed for the Tensorboard profiler plugin. I can run an example with the profiler and get what looks like reasonable data. With my own code, I get a warning back from `_pywrap_profiler.xspace_to_tools_data()` that no timesteps are contained in the file, and thus, some of the useful profiling information is absent/unusable. I cut back the example and found that if I use the MSE loss, the profile is complete; if I change to my own loss function, the timesteps are no longer output. G",2024-04-24T23:48:35Z,comp:tensorboard stat:awaiting tensorflower type:bug comp:apis TF 2.15,open,0,4,https://github.com/tensorflow/tensorflow/issues/66410,  Thank you for reporting the issue. We are also trying to replicate the issue in our environment. Please allow sometime to reproduce and get back to you the same. Thank you!,"Thanks! I can try to push a container image if it helps, let me know if that's be useful. Or I guess I can try a colab notebook too.","As a side note, is there an outofthebox way to create a colab notebook using a tensorflow nightly build? If not, given that's one of the main requests I see asked, before people ping some gists back and forth, I think that would be useful to have to support creating MREs. I assume I can pip install within the notebook, but I guess that having a template to work from would help save some time on both sides.","It is still happening in Tensorflow 2.18. Pretty much any straight forward profiling appears to be broken with warning ""No step marker observed and hence the step time is unknown."""
rag,copybara-service[bot],Adding PRED x BF16 test to provide better coverage.,Adding PRED x BF16 test to provide better coverage.,2024-04-24T21:53:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66402
yi,copybara-service[bot],[XLA:GPU][MLIR-based emitters] Add verifier for apply_indexing op.,[XLA:GPU][MLIRbased emitters] Add verifier for apply_indexing op.,2024-04-23T23:52:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66342
yi,copybara-service[bot],[XLA:GPU][MLIR-based emitters] Add xla_gpu.apply_indexing op.,[XLA:GPU][MLIRbased emitters] Add xla_gpu.apply_indexing op. This first CL just adds printer/parser.,2024-04-23T20:58:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66327
yi,andrewmabc,which version of keras is used by latest kws_streaming,"I'm trying to run kws_streaming's 00_check_data.ipynb/01_train.ipynb/02_inference.ipynb. docker image tensorflow/tensorflow:nightlygpujupyter is used to run the demo. And the following packages are installed via pip: tfmodeloptimizationnightly tfkerasnightly tfanightly scipy pydot graphviz The following package is install via aptget: aptget install graphviz tf._keras_internal.utils.control_flow_util.smart_cond is replaced by tensorflow.python.keras.utils.control_flow_util.smart_cond. tf._keras_internal.models._clone_layers_and_model_config is replaced by tensorflow.python.keras.models._clone_layers_and_model_config. tf._keras_internal.models._clone_layer is replaced by tensorflow.python.keras.models._clone_layer. tf._keras_internal.engine.functional.reconstruct_from_config is replaced by tensorflow.python.keras.engine.functional.reconstruct_from_config. If TF_USE_LEGACY_KERAS=1, 01_train.ipynb can be run successfully. But the following calling is always running infinitely, i.e. don't stop. Why? model_non_stream = utils.to_streaming_inference(model_non_stream_batch, flags, Modes.NON_STREAM_INFERENCE) For the latest kws_streaming, tf.keras.backend.learning_phase() is called in several python files. It see",2024-04-23T07:22:19Z,type:support type:others comp:keras,closed,0,11,https://github.com/tensorflow/tensorflow/issues/66267,  Could you refer to this for more information on kws_streaming.  Thank you!,Thanks for your reply. I have read the webpage of 'Streaming Aware neural network models'. I  just want to run jupyter sample code of 'Quick onboarding with toy demo'. I think I should install correct packages. But I can't get the reason why utils.to_streaming_inference() can't return in 02_inference.ipynb., Could you try with Keras 3 as tf_keras will be legacy Keras? Thank you!,"I use keras 3 and don't install tf_keras. 00_check_data.ipynb report error when executing fourth cell. The fourth cell is : import tensorflow as tf import tensorflow.compat.v1 as tf1 import logging from kws_streaming.models import model_flags from kws_streaming.models import models from kws_streaming.layers.modes import Modes from kws_streaming.train import test from kws_streaming.models import utils from kws_streaming.data import input_data from kws_streaming.data import input_data_utils as du from kws_streaming.models import model_params Reported error message : import tensorflow as tf import tensorflow.compat.v1 as tf1 import logging from kws_streaming.models import model_flags from kws_streaming.models import models from kws_streaming.layers.modes import Modes from kws_streaming.train import test from kws_streaming.models import utils from kws_streaming.data import input_data from kws_streaming.data import input_data_utils as du from kws_streaming.models import model_params 20240425 07:03:24.703360: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. WARNING:tensorflow:Your environment has TF_USE_LEGACY_KERAS set to True, but you do not have the tf_keras package installed. You must install it in order to use the legacy tf.keras. Install it via: `pip install tf_keras`  ImportError                               Traceback (most recent call last) Cell In[4], line 5       3 import logging       4 from kws_streaming.models import model_flags > 5 from kws_streaming.models import models       6 from kws_streaming.layers.modes import Modes       7 from kws_streaming.train import test File /export/Intel_ssd/debug_kws/colab/./googleresearch/kws_streaming/models/models.py:17       1  coding=utf8       2  Copyright 2024 The Google Research Authors.       3     (...)      13  See the License for the specific language governing permissions and      14  limitations under the License.      16 """"""Supported models."""""" > 17 import kws_streaming.models.att_mh_rnn as att_mh_rnn      18 import kws_streaming.models.att_rnn as att_rnn      19 import kws_streaming.models.bc_resnet as bc_resnet File /export/Intel_ssd/debug_kws/colab/./googleresearch/kws_streaming/models/att_mh_rnn.py:18      16 """"""BiRNN model with multihead attention.""""""      17 from kws_streaming.layers import modes > 18 from kws_streaming.layers import speech_features      19 from kws_streaming.layers.compat import tf      20 import kws_streaming.models.model_utils as utils File /export/Intel_ssd/debug_kws/colab/./googleresearch/kws_streaming/layers/speech_features.py:26      24 from kws_streaming.layers import random_shift      25 from kws_streaming.layers import random_stretch_squeeze > 26 from kws_streaming.layers import spectrogram_augment      27 from kws_streaming.layers import spectrogram_cutout      28 from kws_streaming.layers import windowing File /export/Intel_ssd/debug_kws/colab/./googleresearch/kws_streaming/layers/spectrogram_augment.py:20      17 from typing import Any, Dict      19 import tensorflow as tf > 20 import tensorflow_model_optimization as tfmot      22 from kws_streaming.layers.compat import tf      23 from tensorflow.python.ops import array_ops   pylint: disable=gdirecttensorflowimport File /export/Intel_ssd/debug_kws/colab/venv3/lib/python3.11/sitepackages/tensorflow_model_optimization/__init__.py:86      82  To ensure users only access the expected public API, the API structure is      83  created in the `api` directory. Import all api modules.      84 from tensorflow_model_optimization.python.core import version > 86 from tensorflow_model_optimization.python.core.api import clustering      87 from tensorflow_model_optimization.python.core.api import experimental      88 from tensorflow_model_optimization.python.core.api import quantization File /export/Intel_ssd/debug_kws/colab/venv3/lib/python3.11/sitepackages/tensorflow_model_optimization/python/core/api/__init__.py:16       1  Copyright 2021 The TensorFlow Authors. All Rights Reserved.       2        3  Licensed under the Apache License, Version 2.0 (the ""License"");    (...)      13  limitations under the License.      14  ==============================================================================      15 """"""Import API modules for Tensorflow Model Optimization."""""" > 16 from tensorflow_model_optimization.python.core.api import clustering      17 from tensorflow_model_optimization.python.core.api import experimental      18 from tensorflow_model_optimization.python.core.api import quantization File /export/Intel_ssd/debug_kws/colab/venv3/lib/python3.11/sitepackages/tensorflow_model_optimization/python/core/api/clustering/__init__.py:16       1  Copyright 2020 The TensorFlow Authors. All Rights Reserved.       2        3  Licensed under the Apache License, Version 2.0 (the ""License"");    (...)      13  limitations under the License.      14  ==============================================================================      15 """"""Module containing code for clustering."""""" > 16 from tensorflow_model_optimization.python.core.api.clustering import keras File /export/Intel_ssd/debug_kws/colab/venv3/lib/python3.11/sitepackages/tensorflow_model_optimization/python/core/api/clustering/keras/__init__.py:19      16  pylint: disable=gbadimportorder      17 from tensorflow_model_optimization.python.core.clustering.keras import experimental > 19 from tensorflow_model_optimization.python.core.clustering.keras.cluster import cluster_scope      20 from tensorflow_model_optimization.python.core.clustering.keras.cluster import cluster_weights      21 from tensorflow_model_optimization.python.core.clustering.keras.cluster import strip_clustering File /export/Intel_ssd/debug_kws/colab/venv3/lib/python3.11/sitepackages/tensorflow_model_optimization/python/core/clustering/keras/cluster.py:22      19 import tensorflow as tf      21 from tensorflow_model_optimization.python.core.clustering.keras import cluster_config > 22 from tensorflow_model_optimization.python.core.clustering.keras import cluster_wrapper      23 from tensorflow_model_optimization.python.core.clustering.keras import clustering_centroids      24 from tensorflow_model_optimization.python.core.keras.compat import keras File /export/Intel_ssd/debug_kws/colab/venv3/lib/python3.11/sitepackages/tensorflow_model_optimization/python/core/clustering/keras/cluster_wrapper.py:23      21 from tensorflow_model_optimization.python.core.clustering.keras import cluster_config      22 from tensorflow_model_optimization.python.core.clustering.keras import clusterable_layer > 23 from tensorflow_model_optimization.python.core.clustering.keras import clustering_centroids      24 from tensorflow_model_optimization.python.core.clustering.keras import clustering_registry      25 from tensorflow_model_optimization.python.core.keras.compat import keras File /export/Intel_ssd/debug_kws/colab/venv3/lib/python3.11/sitepackages/tensorflow_model_optimization/python/core/clustering/keras/clustering_centroids.py:22      20 from tensorflow.python.ops import clustering_ops      21 from tensorflow_model_optimization.python.core.clustering.keras import cluster_config > 22 from tensorflow_model_optimization.python.core.keras.compat import keras      25 k = keras.backend      26 CentroidInitialization = cluster_config.CentroidInitialization File /export/Intel_ssd/debug_kws/colab/venv3/lib/python3.11/sitepackages/tensorflow_model_optimization/python/core/keras/compat.py:41      37     keras_internal = tf.keras      38   return keras_internal > 41 keras = _get_keras_instance()      43 def assign(ref, value, name=None):      44   if hasattr(tf, 'assign'): File /export/Intel_ssd/debug_kws/colab/venv3/lib/python3.11/sitepackages/tensorflow_model_optimization/python/core/keras/compat.py:33, in _get_keras_instance()      30 os.environ['TF_USE_LEGACY_KERAS'] = '1'      32  Use Keras 2. > 33 version_fn = getattr(tf.keras, 'version', None)      34 if version_fn and version_fn().startswith('3.'):      35   import tf_keras as keras_internal   pylint: disable=gimportnotattop,unusedimport File /export/Intel_ssd/debug_kws/colab/venv3/lib/python3.11/sitepackages/tensorflow/python/util/lazy_loader.py:181, in KerasLazyLoader.__getattr__(self, item)     179   return super(types.ModuleType, self).__getattribute__(item)     180 if not self._tfll_initialized: > 181   self._initialize()     182 if self._tfll_keras_version == ""keras_3"":     183   if (     184       self._tfll_mode == ""v1""     185       and not self._tfll_submodule     186       and item.startswith(""compat.v1."")     187   ): File /export/Intel_ssd/debug_kws/colab/venv3/lib/python3.11/sitepackages/tensorflow/python/util/lazy_loader.py:173, in KerasLazyLoader._initialize(self)     169   super().__init__(     170       self._tfll_name, self._tfll_parent_module_globals, package_name     171   )     172 else: > 173   raise ImportError(   pylint: disable=raisemissingfrom     174       ""Keras cannot be imported. Check that it is installed.""     175   ) ImportError: Keras cannot be imported. Check that it is installed. I notice some tf_keras functions are used in latest kws_streaming, such as tf.keras.backend.learning_phase(). backend.learning_phase() can't be found in keras 3.","Hi  , I have checked the requirements.txt file from kws_streaming and it is using `tf_nightly==2.3.0.dev20200515` version which is quiet older. Hence it will not work with Keras3. Enabling tf_keras (i.e.Keras2) might work. We are not supporting kws_training repo here. If you have any issues on same please report at concern repo. Thanks!","what/where is concern repo? From the issue(https://github.com/googleresearch/googleresearch/issues/783, rybakov said, 'use tfnightly. kws_streaming is always in sync with the latest version of tfnightly,' But in the file of requirements.txt, the version of tf_nightly is too old.","> what/where is concern repo? >  > From the issue(googleresearch/googleresearch CC(Gradient computation erroneously returns None), rybakov said, 'use tfnightly. kws_streaming is always in sync with the latest version of tfnightly,' But in the file of requirements.txt, the version of tf_nightly is too old. I think you need to raise an issue there itself. The attached issue is quiet older and may be current issue is fresh. Kindly make a note that if other libraries has dependency on TF or keras and its not working then it should be addressed there itself. May be they will communicate internally or do required changes.  I suggest you raise an issue there and let hear the reply from there.",Thanks for your reply. But I still don't understand what is concern repo. How can I raise an issue on the concern repo?,"> what/where is concern repo? >  > From the issue(googleresearch/googleresearch CC(Gradient computation erroneously returns None), rybakov said, 'use tfnightly. kws_streaming is always in sync with the latest version of tfnightly,' But in the file of requirements.txt, the version of tf_nightly is too old. Please note that kws_streaming is not a dependency of keras.May be it is using keras but not vice versa. You may please raise the issue in googleresearch repo and let's hear from there.",Thanks for your reply. I raise the same issue in googleresearch repo.(https://github.com/googleresearch/googleresearch/issues/2051),Are you satisfied with the resolution of your issue? Yes No
yi,Nicky9319,Tensorflow Building from Source Code," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.9  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.10.0  Bazel version 5.0.0  GCC/compiler version 9.2.0  CUDA/cuDNN version 11.2 , 8.1  GPU model and memory _No response_  Current behavior? I am trying to build Tensorflow from source because i want to run tensorflow using c++ and i am trying to download the GPU version So I ran ./Configure command, and choose the option to enable GPU Support I ran the bazel command given on the tensorflow Build From source on windows https://www.tensorflow.org/install/source_windows and ran into the following error, What does it imply and how can i Solve it  Standalone code to reproduce the issue   Relevant log output _No response_",2024-04-23T02:13:14Z,stat:awaiting response type:build/install type:support stale subtype:windows TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/66238,"Hi **** ,  Ensure that you have visual studio installed on your system with the necessary components for building tensorflow. This typically includes the ` desktop development with C++` workload and the `MSVC v142  VS 2019 C++ x64/x86 build tools` component.  Make sure that the visual studio environment variables are set correctly. You can do this by opening a developer command prompt for visual studio and running the `vcvarsall.bat` script for your target architecture (x64 or x86) and platform (Debug or Release).  Try to clean the Bazel cache by running this command.  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #11306: [XLA:GPU] Fix not unique name issue in sanitize constant pass,"PR CC(adds missing ""pass"" to empty method in tf.contrib.learn.DNNClassifier doc): [XLA:GPU] Fix not unique name issue in sanitize constant pass Imported from GitHub PR https://github.com/openxla/xla/pull/11306 Fix `Instruction name is not unique` error reported by JAX UT `ShardMapTest.test_matmul_reduce_scatter` in XLA:GPU.  Background Error message:  This error is reported after `PrepareHloModuleForIrEmitting()`. The nonunique name `param_1` is generated from 2 different passes `GpuSanitizeConstantNames` and `FusionWrapper`. The related HLO changes are as follows: 1. Original HLO, there's no `param_1` and only got `param_0` in `async_computation`:  2. `param_0` was changed to `param_1` after `GpuSanitizeConstantNames` pass:  3. Another `param_1` was generated after `FusionWrapper` pass: ```ll %async_computation (param_1: s32[4,8]) > s32[2,8] {   %param_1 = s32[4,8]{1,0} parameter(0)   ROOT %reducescatter.2 = s32[2,8]{1,0} reducescatter(s32[4,8]{1,0} %param_1), channel_id=1, replica_groups={{0,2},{1,3},{4,6},{5,7}}, use_global_device_ids=true, dimensions={0}, to_apply=%region_0.7, metadata={op_name=""jit(fwd)/jit(main)/jit(shmap_body)/reduce_scatter[axis_name=y scatter_dimension=0 axis_index_groups=None ",2024-04-22T17:37:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66221
yi,copybara-service[bot],PR #11721: [XLA:CPU] Add  MpiCollectives to the the .pyi stubs,PR CC(Memory Leak from Training Step): [XLA:CPU] Add  MpiCollectives to the the .pyi stubs Imported from GitHub PR https://github.com/openxla/xla/pull/11721 we forgot this in https://github.com/openxla/xla/pull/7849. Copybara import of the project:  3924cc0fbbb63e9503f38a59aede3b8e817b17fa by Clemens Giuliani : [XLA:CPU] add missing type annotations for the mpi collectives Merging this change closes CC(Memory Leak from Training Step) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/11721 from inailuig:mpicollectives_pytype 3924cc0fbbb63e9503f38a59aede3b8e817b17fa,2024-04-22T16:16:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66214
yi,copybara-service[bot],Refactoring: Separate the model exporting of calibration and debugging models,"Refactoring: Separate the model exporting of calibration and debugging models Currently, we create the wholemodel debugging model by modifying the calibration model. This is no longer suitable as it will be affected by coming changes to prepare the calibration model.",2024-04-22T06:28:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66185
rag,copybara-service[bot],[tsl:concurrency] Add special handling of Map functors returning StatusOr<U> results,[tsl:concurrency] Add special handling of Map functors returning StatusOr results AsyncValueRef> is discouraged and should be replaced with AsyncValueRef because async value is already an container with a absl::Status or a value and Status of Status makes APIs ambiguous. Add specializations to AsyncValueRef::Map to handle functors returning StatusOr objects.,2024-04-22T00:31:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66169
rag,copybara-service[bot],Create basic server coverage and model tests.,Create basic server coverage and model tests.,2024-04-20T18:44:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66148
yi,CarlosNacher,"Wrong quantized_dimension (axis) when ""per-channel"" quantization"," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Microsoft Windows 11 Home 10.0.22631 version 22631 compilation   TensorFlow installation (pip package or built from source): `pip install tensorflow==2.15.0`  TensorFlow library (version, if pip package or github SHA, if built from source): 2.15.0  2. Code   3. Failure after conversion The conversion is successful, but the generated model is wrong: **the inferred output (`tflite_inference`) is all the same value in the fully int8 quantized model and I think it could be because quantization perchannel is performing perbatchaxis instead**: If I inspect the .tflite model, I found that in each operation (for example convolution) the number of `scale` and `zero_point` values are the same as number of axis=0 (batch i.e 256) not the number of channels (axis=1 i.e 512) !image ... !image **I miss one `quantized_dimension` parameter, as stated in docs:** https://www.tensorflow.org/lite/performance/quantization_specperaxis_vs_pertensor ",2024-04-19T15:26:37Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.15,closed,0,19,https://github.com/tensorflow/tensorflow/issues/66081,"Hi  , Can you give me the full code to replicate this issue. The library imports and some other things are not clear . I will be able to debug quicker if you provide me the above details. ","Hey  , For sure! I have uploaded the code to a Colab file so that you can fully reproduce it. I have also written some comments in the notebook explaining the actual behaviour and the expected one. The link to the data you will need to fully reproduce (the SavedModel and calibration data) is here: https://we.tl/tMUNhBovDZr Thank you so much for your response and for taking the time to help me!","Hi  , Sorry for the late response, but the link for your savedModel seems to have expired. can you please provide another link to the model and data. ","Hi  , For sure, here: https://we.tl/tiUsgchLLL7 Thank you.","Hi  , could you please give me some feedback aboout this? The links only last 7 days. If you need it, I can send it in a different format. Thanks!","Hi  , I replicated your script and i got the tflite and i inspected it and confirmed that its quantizing the 0th dimension.    ","Hi  , I will raise a PR to fix your issue and I will update you. Thank you","Hi  , Okay, thank you so much! It's great that we can all contribute together. I look forward to your response.","Hi  , Sure thing. However can you please confirm if this bug can be reproduced in tf 2.16.0 or tfnightly . ","Hey  ! I confirm it, I have replicated using tf 2.16 and tfnightly (2.17.0.dev20240528), the output trace of the code execution is in the Colab I attached. Thank you.","Hello  ,  Any update on this? If there is a PR, could I havee the link to keep updated?  Thank you in advance!","Hi , this issue may be resolved by following a different work flow.  Have you tried using AIEdgeTorch?, you can find more information here: googleblog. I have actually created a simple script for converting a CV model here:  If you want to, you can actually try visualizing the result in modelexplorer as well. Please try them out and let us know if this resolves your issue. If you still need further help, feel free to open a new issue at the respective repo.","Hi   I didn’t know that library, thank you so much for the information! I’ll check it out and let you know, however my final application of interest is to run the model in coral TPU USB accelerator, do you know if with the tool you comment it is achievable?  On the Readme, CPU and GPU support is mentioned but not TPU. As far I know, the only thing you need to run your .tflite on TPU is that it is FULLY INT8, so I suppose that it’ll work, but the no mention of TPU confuses me. Can you give me feedback about that? Anyway, again, it’ll try, and many thanks!","Hi , I would say, go ahead and try it... if it's not fully int8, you should be able to quantize at least in the same way as before by passing in tfl_converter_flags: quantization. Additionally some PT2E quantization may work better for your use case. Does that help?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,wip,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,qaletka,Homepage sections displaying incorrectly on TensorFlow website,"**Description:** I've noticed an issue with the homepage sections on the TensorFlow website. The sections seem to be displaying incorrectly, with some elements overlapping and others not loading properly. **Steps to Reproduce:** 1. Go to the TensorFlow website homepage (https://www.tensorflow.org/?hl=pl). 2. Scroll through the sections on the homepage. 3. Observe the layout and display issues. **Expected Behavior:** The homepage sections should display correctly with all elements properly formatted and visible. **Actual Behavior:** Some sections are overlapping, and certain elements are not loading or displaying as expected. This makes it difficult to navigate and access the content effectively. **Screenshots/Additional Information:** !tensorflow org_BUG **Environment:** Browser: [Google Chrome (124.0.6367.60 )] Operating System: [Windows 10] Device: [Desktop]",2024-04-19T15:26:31Z,type:docs-bug stat:awaiting tensorflower TF 2.16,open,0,4,https://github.com/tensorflow/tensorflow/issues/66080,"Hi  , I have observed the issue with the page display in Polski Language. Switching to English looks fine. CC :  , For a look into into this and comment if any.","Hi , Yes, the English version of the website looks good. Other versions also have problems on the home page and individual subpages. Examples:  https://www.tensorflow.org/?hl=es  https://www.tensorflow.org/lite?hl=es  https://www.tensorflow.org/?hl=id  https://www.tensorflow.org/?hl=it", manages our relationship with the translation service.,   so has anything changed?
yi,miticollo,[Question] Is it possibile to use `jit_compile=True` (XLA) when input is a string?, TL; DR See this notebook on Colab.  Long version Let's consider the following function:  Then we convert it into graph:  Finally run them:  Unfortunately the last statement fails:   About question I don't think this is an issue. But since I'm learning I would want to know why this happens.,2024-04-19T11:53:25Z,stat:awaiting response type:support comp:xla,closed,0,4,https://github.com/tensorflow/tensorflow/issues/66074," No, you cannot directly use jit_compile=True (XLA) with string inputs in TensorFlow. XLA is designed for numerical computations, and strings are not directly supported. Thank you!",Thank u for your clarification!, Thank you for your response here. Could you please move this issue to closed status if it is resolved? Thank you! ,Are you satisfied with the resolution of your issue? Yes No
chat,illaoiiiii,Basic regression: Predict fuel efficiency Probelm," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15.0  Custom code Yes  OS platform and distribution google colab  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In the tutorial beginner part Regression part ,the document have some problem shows like below !image The code after this segment is not running. But i use ChatGPT to fix it .and the right code below !image i think this is a easy problem and i hope official can fix it.  Standalone code to reproduce the issue   Relevant log output ",2024-04-19T08:24:04Z,type:docs-bug stat:awaiting response stale comp:model,closed,0,8,https://github.com/tensorflow/tensorflow/issues/66057,", As you mentioned the document regression.ipynb is failing with the tensorflow v2.15. But when I tried to execute the same official document with the latest tensorflow stable version 2.16, the code was executed without any issue/error. Kindly find the gist of it here. Thank you!","  i just obey the tutorial and i use the colab , colab automatically instal the v2.15,I am learning the TensorFlow framework,maybe is not a very serious bug or problem",", Yeah, It is not a bug and moreover the official docs will be upgraded to the latest stable version in upcoming days. As this issue is resolved in tensorflow v2.16, could you please feel free to move this issue to closed status. Thank you!",Hello :),This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"I run into a similar issue. Different cell in the same notebook !image  Versions are:  So I am using Tensorflow 2.17, not 2.16"
yi,copybara-service[bot],Add PrivacyInfo.xcprivacy for TFLite ios xcframeworks. Apple is enforcing apps and sdks to provide [Privacy Manifests files](https://developer.apple.com/documentation/bundleresources/privacy_manifest_files) and the enforcement will go into effect in three weeks.,Add PrivacyInfo.xcprivacy for TFLite ios xcframeworks. Apple is enforcing apps and sdks to provide Privacy Manifests files and the enforcement will go into effect in three weeks.,2024-04-18T21:43:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66030
rag,jakubdolejs,Inference time using Interpreter API on Android inconsistent and 10–50 times slower than same tflite model on iOS," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.15.0  Custom code Yes  OS platform and distribution _No response_  Mobile device Google Pixel 4a running Android 13  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm running inference on Yolov8based tflite model on Android using the Interpreter API. I noticed that the first 30 or so calls to the `Interpreter.run()` function take much longer than the subsequent calls. The difference is quite marked, starting at about 3500ms per run and ending at about 500ms. I thought perhaps it was something about the input data so I tried a test with running the same call with the same input 100 times in a loop. Same behaviour, the first handful of inference runs take around 3 seconds, slowly speeding up to about 500–700ms by the 100th iteration. I wanted to find out whether there is a specific combination of the interpreter options causing this behaviour so I wrote a test matrix initialising interpreters with different options:  Using GPU delegate    Using Google Pl",2024-04-18T20:40:06Z,stat:awaiting tensorflower comp:lite type:performance Android,closed,0,19,https://github.com/tensorflow/tensorflow/issues/66025,"Hi   There could be a bunch of reasons behind performance issues on pixel 4a compared to the iPhone 12. When you use Core ML delegate on the iPhone, it is using NPU which is much faster compared to the gpu on pixel 4a . Can you also benchmark your model on the pixel 4a using tensorflow profiler which will give you detailed information regarding your model execution like how many partitions of the model are created before execution and how many layers are falling back to the cpu in case of gpu delegate. Also pixel 4a's GPU is not optimised for fp32 calculations , it is only optimised for fp16 operations , so that could be the culprit behind poor gpu performance while using fp32.  Can you share the tensorflow lite profiler results once you benchmark your tflite model on the pixel using profiler.",Thank you . I'll try the profiler and upload the results here.,"Hello , I ran the benchmark tool with different options on the float32 and float16 models. Please see the attached results. The file names ending with `gpu` are from runs that had the `use_gpu` flag set to `true`. The ones ending with `nnapi` had the `use_nnapi` flag set to `true`. The commands used to invoke the tests are included in the txt files. Please let me know if you see anything unexpected in the results. fp16_gpu.txt fp16_nnapi.txt fp16.txt fp32_gpu.txt fp32_nnapi.txt fp32.txt","Hello , I've reviewed the log files, and everything appears as expected, except for the discrepancies noted in the files fp32_gpu.txt and fp16_gpu.txt. While the average latency GPU numbers from the TFLite profiler seem almost identical for both fp16 and fp32 models, the logs from your Android code indicate a clear difference between fp32 and fp16 GPU numbers. To facilitate a more accurate comparison, could you also profile your models on an iPhone 12 using TFLite Profiler for iOS? Regarding the inconsistency in inference numbers during the first few runs on the Pixel 4a, could you integrate a few warmup loops in your Android code before benchmarking and let me know the results? Please feel free to reach out if you encounter any difficulties during this process.",Thank you . I'll try the iOS app and report back. I really appreciate you helping me through this.,"Hi , Here are the benchmarks from iOS (iPhone 12 mini). It looks like the app runs the inference on the UI thread. For all the models I get this warning in the log: `This method should not be called on the main thread as it may lead to UI unresponsiveness.`. I redacted the above messages from the log output for brevity.  FP16 Model on CPU   FP16 model with CoreML delegate:  Note that when running the FP16 model with CoreML delegate I got a `EXC_BAD_ACCESS` error here: https://github.com/tensorflow/tensorflow/blob/4615e171a2517812d32a3a4b66a838076d3e339b/tensorflow/lite/tools/benchmark/ios/TFLiteBenchmark/TFLiteBenchmark/BenchmarkViewController.mmL133 That's why the log is truncated.  FP32 model on CPU:   FP32 model with CoreML delegate: ","Hi , I ran a test on the Pixel 4a with the different model combinations. I ran 50 iterations but this time I included a warmup of 10 inference runs. The first few runs are still very slow. Is this to be expected? How do you recommend the warmup is handled in production? The app I'm building will need to run inference on a few images at a time but it shouldn't take 3 seconds per image. Here is the test function that produced the results in this CSV file: ","Hi , Apologies for the delay; I wasn't available over the weekend. After analyzing the iOS numbers, it's evident that the Core ML delegate on the iPhone 12 Mini outperforms the GPU delegate on the Pixel 4a by approximately 7x for fp32 models. Additionally, the iPhone 12 Mini's CPU executes models roughly 2x faster than the Pixel 4a's CPU. These results clearly indicate that the iPhone 12 Mini offers faster model execution both on CPU and GPU compared to the Pixel 4a. However, if you're aiming to maximize performance on your Pixel device, consider utilizing its DSP. Please ensure thirdparty access to the DSP is permitted on the Pixel phone, then optimize performance using SNPE provided by Qualcomm.","Also regarding handling GPU warm up runs in production , from my experience i have also seen that the first few inference runs on Android TFLite GPU can be slower because of initialization Overhead that is when you run  inference for  first  few times, TensorFlow Lite needs to initialize various components, such as loading the model, allocating memory, and setting up the GPU context. This initialization process can take some time, causing the first few inferences to be slower.  To handle such scenarios in production you can perform the GPU warmup runs during the app's startup. This could be something like an inference loop of 50 or 100 iterations on dummy data on app's startup .  Please let me know if you have any further issues or questions.","Hello ,  I've done some more testing and profiling. I built an Android app that lets me change between the FP16 and FP32 models and toggle the different options. Here are my findings:  The initial slowdown only happens on some devices (e.g., Pixel 4a, Elo touch). On some devices (Pixel 6, Galaxy Tab S6 Lite) the inference runs at consistent speed from start to finish with any given options.  The initial slowdown happens regardless of which delegates are used.  I tried using the same model converted to NCNN and it runs at consistent speed on any device. The speed is comparable to the TfLite model after the ""warmup"".  The slower devices I mentioned like Pixel 4a run first 50 or so iterations in about 3500 ms, after which the speed increases to about 400–500 ms per inference. Even this is not consistent. Sometimes the inference keeps running slowly at over 3000 ms even after hundreds of iterations. For comparison, the faster devices run inference at roughly 300 ms from the get go.  On iOS, the story is slightly different. It takes about 2 seconds to load the model but afterwards the inference runs consistently at about 70 ms (with the CoreML delegate). From using NCNN I can see that even the underpowered devices don't require a warmup to run at acceptable speeds. I believe there may be a bug in TfLite. It shouldn't take 3 minutes to ""warm up"". Would you like me to file a separate issue with a bug report or can you escalate this one?","Hi  , When it comes to speed and performance, NCNN is generally considered to be faster than TFLite in many scenarios so your results are somewhat as expected. However i will replicate the issue on my available pixel phone using tflite  and i will get back to you . I don't think there is a need to file a separate issue yet.","Hi , Just checking whether you were able to repeat the issue on your device. I'm able to consistently reproduce the behaviour on Google Pixel 4a. I got a colleague to try it on a newer Pixel device (6 I believe). It runs slower than the same model converted to NCNN but fast enough for my needs. The newer Pixel also doesn't need any warmup. It runs at the same speed from the get go. Cheers, Jakub","Hi  , I couldn't get my hands on a pixel 4a so i was not able to fully replicate the issue.   can you please take a look at this.","Hi , I am also not able to use a real device to test this out. I did not see an uploaded model file of both types you are testing. If you can share that, that'll be great and help us zero in on what's going on here. Thanks.","Hi , Please see the attached zip archive with the model file. You can find the code to test the inference speed in my post from Apr 19. ARC_PSD001_1.1.122_bst_yl80201_float16.tflite.zip Please let me know if you have any questions."," Thanks for sharing! It always helps immensely. , can you please take a look? Thanks.","Hi,  Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/67 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No,Thank you  for bringing it to my attention. I wasn't aware of LiteRT. I'll follow the issue on the LiteRT Github.
rag,copybara-service[bot],Extend gpu_only_cc_library to handle backend specific targets,Extend gpu_only_cc_library to handle backend specific targets Example:  The macro simplifies and encourages a strict separation of CUDA/ROCm specific code. It generates separate targets for each backend that have to be compileable on its own. It fulfills the following requirements:  Separate independently buildable targets for each backend  build_cleaner compatible  No preprocessor branches needed Note that this is only the first step towards better backend code separation. It forces the user to put some kind of abstraction between backendagnostic and backendspecific code. For now this can be as simple as a free function interface. In later steps we can move towards abstract base classes and factory functions.,2024-04-18T14:35:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/66003
yi,copybara-service[bot],Refactor GEMM autotuner,"Refactor GEMM autotuner The refactoring is trying to fix two issues: 1) Expose API for compiling and autotuning dot fusions  previously the implementation was hidden in an anonymous namespace and used a global object for storing the results (autotuning cache). This might be useful for modelling matmul performance. 2) Reduce code duplication  previously the filtering of the triton configs was done separately for default and exhaustive search. Some configs need to be adjusted to be correctly compiled by Triton, for example:  when there are less elements in a tensor than the number of threads (happens for sparse dot metadata with small tiles);  when compiling for Hopper with `wgmma` enabled (block_m or num_warps may need to be adjusted). Notably, `kDefaultGemmTiling` could need such adjustments, so returning it asis may result in fatal compilation errors. With this patch, the same config generator is used for all code paths. There are a few more changes:  do not handle reference implementation (cuBLAS) separately; instead, use it in the `Config` variant along with the other options (cuDNN, triton).  do not repeat cuDNN conditions  put them in one place.  use actual tile sizes and SM count for calculating t",2024-04-18T10:42:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65992
rag,copybara-service[bot],#shlo_ref Factor `Storage` template definition.,shlo_ref Factor `Storage` template definition.,2024-04-18T09:17:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65986
yi,Afganitia,Incapable of loading a tf v1 model," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Windows 10  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I expected tf to load the model (it is a v1 model, but in theory this is houw you should load them?). Model in question: https://drive.google.com/file/d/1TnN1j34owIPMyi7gRZJO3UAwZhNtXQ9i/view?usp=drive_link  Standalone code to reproduce the issue   Relevant log output ",2024-04-17T16:49:19Z,stat:awaiting response type:support stale comp:apis TF 2.16,closed,0,4,https://github.com/tensorflow/tensorflow/issues/65945, Could you please consider upgrading your TF v1 model to TF2 format. Please refer to this guide for migration.  Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
text generation,phamvanvuhb1999,TensorFlow lite Whisper model get worse inference result.," 1. System information  OS Platform and Distribution: Windows 10  TensorFlow installation: via Pypi   TensorFlow library: 2.16.1  2. Conversion success  Tflite model have been save to .tflite file, with size x4 smaller Code for model conversion:   3. Inference results gets worse compared to TensorFlow model.  After conversion, inference Tflite model and TensorFlow model with first row of the dataset:   **Results**:   + True label: ""CONCORD RETURNED TO ITS PLACE AMIDST THE TENTS""   + TensorFlow model result: "" CONQUERED RETURNED TO ITS PLACE AMIDST THE TENTS.""   + Tflite model result: ""!!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...""",2024-04-17T09:05:37Z,TFLiteConverter,closed,0,2,https://github.com/tensorflow/tensorflow/issues/65917,Please take a look.,Model working well with below converting script: 
yi,copybara-service[bot],Allow per-channel quantization without specifying quantization dimension,"Allow perchannel quantization without specifying quantization dimension Current QuantizedType in QuantizationConfig can only specify perchannel quantization with specified quantization dimension. But quantization dimension may vary depending on the dimension layout within a model, but at the same time, it is straight forward to decide quantization dimension for the most of the times(output feature dimension for convolution, and nonbatching and noncontracting dimension for dot_general  usually it is unique for ODML cases). Therefore, allowing option to do perchannel quantization without specifying quantization dimension will make it more accessible.",2024-04-17T08:22:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65907
rag,ShifengJin,cl 命令行 无法打开 calibration_statistics_collector_average_min_max.obj.params, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1  Custom code Yes  OS platform and distribution Windows10  Mobile device _No response_  Python version 3.11  Bazel version 6.5  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? cl: 命令行 error D8022 :无法打开 calibration_statistics_collector_average_min_max.obj.params  Standalone code to reproduce the issue   Relevant log output ,2024-04-17T07:26:46Z,stat:awaiting response type:bug stale subtype:windows type:performance TF 2.16,closed,0,13,https://github.com/tensorflow/tensorflow/issues/65905,same issue  cl : Command line error D8022 : cannot open 'bazelout/x64_windowsopt/bin/tensorflow/compiler/mlir/quantization/tensorflow/calibrator/_objs/calibration_statistics_collector_average_min_max/calibration_statistics_collector_average_min_max.obj.params'  TF Version 2.15.1,"Seems that the latest version of TF doesn't support MSVC 2019/MSVC2022 compiler any more,  please use CLANG to Build. ","  do you have any update on this issue? I had turned on Developer mode, it is not access right problem.  I checked the BUILD file under calibrator folder, no problem of this file. and calibration_statistics_collector_average_min_max.obj.params file had existed, why cl.exe cannot open this file.  it is format issue or not? BR ","Hi **** , Sorry for the delay, Could you verify that the version of the `C` compiler used by Bazel are compatible with TensorFlow. Sometimes, mismatched compiler versions can lead to build errors. And if you are using an older version of the C++ compiler, consider upgrading to a newer version that may have better memory management. Please delete any existing build files and directories to ensure a fresh start. Thank you!",Microsoft (R) C/C++ Optimizing Compiler Version 19.39.33523 for x64 Copyright (C) Microsoft Corporation.  All rights reserved. it should be latest version of MSVC,MSVC\14.39.33519,"I'm also facing exact error and it is not actually bind to this particular `calibration_statistics_collector_average_min_max.obj.params` file, but it occurs for some other *.params file as well. Faced this while trying to build the following target, `bazel build config=opt cxxopts=""std=c++17"" //tensorflow:tensorflow_cc.dll`","that's a problem why we cannot use c++17 to build,  i used winclang, it can be build successfully.  actually, this file is existed on the bazel_out folder already, if you run cl.exe manually, this file can be opened. I suspected it related with build setup. but we had followed the instruction in release note.  I also met another problem of smallvector like below:  tensorflow/dtensor/mlir/expansions/save_restore_spmd_expander.cc(812): error C2976: 'llvm::SmallVector': too few template arguments. I checked the code, find that for TFV2.15.0 they use llvm17.0.6. (I want to use the latest llvm with override_repository=llvmraw= failed also) and in save_restore_spmd_expander.cc:812 StatusOr> GetLayoutsFromAssignVariableOps I am mad with TF c++ windows build, I gave up windows build , and works on linux environment now.  "," please don't wait the his response, we met the same issue, I can provided the info you wanted","As  said, I do not get any errors when compiling in LLVM/clang 17.0.6. Getting it only in MSVC. But I'm facing other error because of missing symbols while trying to link the tensorflow.dll. The problem is I could solve missing symbols by simply adding `TF_EXPORT` in the respective source and recompile, but the missing symbol is from a dynamically generated proto file C++ header stub.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,xl1043237213,`Overflow` in `tf.raw_ops.SparseReduceSumSparse` when there are too large values in `input_shape`. , Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.15  Custom code No  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? `Overflow` in `tf.raw_ops.SparseReduceSumSparse` when there are too large values in `input_shape`.  See the colab link below for details.  Standalone code to reproduce the issue   Relevant log output ,2024-04-16T19:18:25Z,stat:awaiting response type:bug stale comp:ops TF 2.16,closed,0,5,https://github.com/tensorflow/tensorflow/issues/65871,", Thank you for reporting the issue. I was able to reproduce the issue on tensorflow v2.15, v2.16 and tfnightly. Kindly find the gist of it here.","Hi , I was reproduced the issue or error with latest Tensorflow version (TF 2.18.0). Please refer to this gist file. The issue occured because TensorFlow is attempting to calculate a very large product (1700000000000 * 6000000) as part of the  sparse tensor operations, leading to integer overflow. The result exceeds the maximum value of a 64bit integer, causing the calculation  to fail. To resolve this error, could you please reduce the scale of the input_shape values if possible. Thank you.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,xl1043237213,`Overflow` in `tf.raw_ops.SparseReduceMaxSparse` when there are too large values in `input_shape`., Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.15  Custom code No  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? `Overflow` in `tf.raw_ops.SparseReduceMaxSparse` when there are too large values in `input_shape`. See the colab link below for details.  Standalone code to reproduce the issue   Relevant log output ,2024-04-16T18:40:34Z,stat:awaiting tensorflower type:bug comp:ops TF 2.16,closed,0,9,https://github.com/tensorflow/tensorflow/issues/65865,", Thank you for reporting the issue. I was able to reproduce the issue on tensorflow v2.15, v2.16 and tfnightly. Kindly find the gist of it here.",Not a bug  the shape is too large and informing you of this.,Are you satisfied with the resolution of your issue? Yes No,"> Not a bug  the shape is too large and informing you of this. Can you tell me in more detail why the issue is not considered a bug? `INVALID_ARGUMENT` is an error type defined by TensorFlow but causes the program to crash, should this be avoided?","> > Not a bug  the shape is too large and informing you of this. >  > Can you tell me in more detail why the issue is not considered a bug? `INVALID_ARGUMENT` is an error type defined by TensorFlow but causes the program to crash, should this be avoided? Throwing an error is not a crash.  The error is telling you that your input argument is invalid.","> > > Not a bug  the shape is too large and informing you of this. > >  > >  > > Can you tell me in more detail why the issue is not considered a bug? `INVALID_ARGUMENT` is an error type defined by TensorFlow but causes the program to crash, should this be avoided? >  > Throwing an error is not a crash. The error is telling you that your input argument is invalid. Error is thrown by C/C++ code, causing the Python interpreter to crash, and similarly `Check failed`. Are these errors not considered bugs?🤔","> > > > Not a bug  the shape is too large and informing you of this. > > >  > > >  > > > Can you tell me in more detail why the issue is not considered a bug? `INVALID_ARGUMENT` is an error type defined by TensorFlow but causes the program to crash, should this be avoided? > >  > >  > > Throwing an error is not a crash. The error is telling you that your input argument is invalid. >  > Error is thrown by C/C++ code, causing the Python interpreter to crash The interpreter does not crash  an exception is thrown, and you can catch it in python. >, and similarly `Check failed`. Are these errors not considered bugs?🤔 We don't consider those serious bugs, especially if you intentionally provide obviously invalid data with the sole intention of trying to trigger a crash.","Thanks for your patient answer.  I'd like to know how to catch the exception. When you run the code in colab, the web page will throw error message ""Your session crashed for an unknown reason"". When you run the code using `subprocess`, you can get return code 6. I think the exception is thrown by C/C++ code but cannot be caught by the Python process.  It is true that this is an artificially constructed invalid input, but improper handling of the boundary case may affect the stability of the program execution😔","> Thanks for your patient answer. >  > I'd like to know how to catch the exception. When you run the code in colab, the web page will throw error message ""Your session crashed for an unknown reason"". When you run the code using `subprocess`, you can get return code 6. I think the exception is thrown by C/C++ code but cannot be caught by the Python process. >  > It is true that this is an artificially constructed invalid input, but improper handling of the boundary case may affect the stability of the program execution😔 Thanks, the error was not propagated correctly to python.  It was a quick fix, so should now work. But in general, we now deprioritize any artificial edgecase issues like this that are not encountered by realworld models."
yi,xl1043237213,`Overflow` in `tf.compat.v1.nn.ctc_loss` and `tf.compat.v1.nn.ctc_loss_v2` when there are too large values in `labels`., Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.15  Custom code No  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? `Overflow` in `tf.compat.v1.nn.ctc_loss` and `tf.compat.v1.nn.ctc_loss_v2` when the value of `labels` is too large. See the colab link below for details.  Standalone code to reproduce the issue   Relevant log output ,2024-04-16T18:32:12Z,stat:awaiting response type:bug stale comp:ops TF 2.16,closed,0,5,https://github.com/tensorflow/tensorflow/issues/65864,"Hi  , I have replicated the reported behaviour and attached screenshot below. The overflow warning is expected but it follows a crash which might needs fix. ","Hi , Could you please try to use this `tf.keras.ops.ctc_loss` which is also a similar ops for calculating the ctc loss.  https://www.tensorflow.org/api_docs/python/tf/keras/ops/ctc_loss Thank You.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Refactor GEMM autotuner,"Refactor GEMM autotuner The refactoring is trying to fix two issues: 1) Expose API for compiling and autotuning dot fusions  previously the implementation was hidden in an anonymous namespace and used a global object for storing the results (autotuning cache). This might be useful for modelling matmul performance. 2) Reduce code duplication  previously the filtering of the triton configs was done separately for default and exhaustive search. Some configs need to be adjusted to be correctly compiled by Triton, for example:  when there are less elements in a tensor than the number of threads (happens for sparse dot metadata with small tiles);  when compiling for Hopper with `wgmma` enabled (block_m or num_warps may need to be adjusted). Notably, `kDefaultGemmTiling` could need such adjustments, so returning it asis may result in fatal compilation errors. With this patch, the same config generator is used for all code paths. There are a few more changes:  do not handle reference implementation (cuBLAS) separately; instead, use it in the `Config` variant along with the other options (cuDNN, triton).  do not repeat cuDNN conditions  put them in one place.  use actual tile sizes and SM count for calculating t",2024-04-16T14:51:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65844
rag,xl1043237213,`Segmentation fault` in `tf.raw_ops.RaggedRange` and `tf.ragged.range` when `starts` or `limits` has large (postive/negative) values., Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.15  Custom code No  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? `Segmentation fault` in `tf.raw_ops.RaggedRange` and `tf.ragged.range` when `starts` or `limits` has large (postive/negative) values.  Standalone code to reproduce the issue   Relevant log output ,2024-04-16T05:05:06Z,stat:awaiting tensorflower type:bug comp:ops TF 2.15,open,0,1,https://github.com/tensorflow/tensorflow/issues/65790,"Hi **** , I tried to run your code on colab using TF v2.16.1 and faced the same issue. Please find the gist here for reference. Thank you!"
yi,copybara-service[bot],"[XLA:TPU] Refactor TryInputStreaming into a top-down approach, starting from entry computation parameters with host memory space.","[XLA:TPU] Refactor TryInputStreaming into a topdown approach, starting from entry computation parameters with host memory space. This refactor allows for a more direct traversal of the graph for streamed parameters, but should not change behavior in any way.",2024-04-15T22:48:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65761
chat,dorishyc3,Fail to build TFLite shared library with GPU delegates," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16.1  Custom code No  OS platform and distribution MacOS  Mobile device Android  Python version _No response_  Bazel version _No response_  GCC/compiler version Clang 17.0.2  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Building TFLite with CMake following this guide, with the option to build shared lib and GPU delegates fails.  Standalone code to reproduce the issue   Relevant log output ",2024-04-15T18:27:30Z,stat:awaiting response type:build/install stale comp:lite subtype:macOS TFLiteGpuDelegate TF 2.16,closed,1,15,https://github.com/tensorflow/tensorflow/issues/65744,"Hi , can you please share whether you have an M series or an intel chip for your Mac? Thanks.","Hello, I’m using an M1 mac","Hi , which version of the NDK are you using? Thanks.",my ndk version is 26,"I'm running into a different issue, I tried with ndk version = 26 and 25b and the errors were similar but not exactly the same. My reproduce step (on MacOS M1 and ndk=25b):  End of my output:  Hi , can you please take a look? Thanks.","I can reproduce this on Linux 5.15.0112generic (Ubuntu 22.10). Appending these to `TFLITE_DELEGATES_GPU_SRCS`solves most of them, however few remains:   Compiled wth: Android NDK 25.2.9519653 CMake 3.30",Looks like some sources got lost in CMake build. Below is a minimal patch file with which I've managed to get this to work:   You can download above patch file and apply it to v2.16.1 `patch apply `,"If someone else is encountering a similar issue, where you get undefined symbols related to GL and GPU when using TFLite shared lib with GPU delegate (2.16.1) in Android studio (so on linking step of your Android app!), the patch https://github.com/tensorflow/tensorflow/issues/65744issuecomment2188247881 also helps.","> Looks like some sources got lost in CMake build. Below is a minimal patch file with which I've managed to get this to work: >  >  >  > [](https://github.com/dorishyc3) You can download above patch file and apply it to v2.16.1 `patch apply ` Base on your changes, there have more file need to add in v2.18.0. The final patch tested on my local. ","Hi All, the process to build has changed since this issue was active, please find the current instructions here: https://ai.google.dev/edge/litert/build/arm.  can you try your preferred workflow and let me know if this resolves your issue. If the above solution works for you, please feel free to use it as well. If not, please post your current result, thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No, the issue was not solved I think. Did anyone add the lost sources to the CMakeLists?,"Hi , can you create a new issue after following the new instructions in the LiteRT repo? That will help us organize this better. Thanks."
yi,xl1043237213,`Overflow` in `tf.raw_ops.SerializeManySparse` when there are too large values in `sparse_shape`.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.15  Custom code No  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior?  When there are too large values in `sparse_shape`  while `sparse_indices` and `sparse_values` are empty, ``tf.raw_ops.SerializeManySparse`` triggers overflow error.  Standalone code to reproduce the issue   Relevant log output ",2024-04-15T14:50:33Z,stat:awaiting tensorflower type:bug comp:ops TF 2.16,open,0,2,https://github.com/tensorflow/tensorflow/issues/65727,", I tried it on a VM with Ubuntu 22.04 OS and executed the code multiple times which executed without crash in multiple runs and outputs intended error. !Screenshot 20240416 6 13 50 PM And also observed that the issue/error is occuring whenever the input size is **>=8 digit number. With the <7 digit number** as input, the code was executed without any issue and the output is also as expected. Kindly find the gist of it here. Thank you!",Thanks for your response. I will keep tracking the issue and waiting for the root cause of the bug. I think it's an unexpected execution behavior and hoping that TensorFlow will fix the bug soon.
yi,xl1043237213,Some errors in `tf.raw_ops.SparseTensorDenseMatMul` when there are negative or too large values in `a_shape`.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.15  Custom code No  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When there are negative values in `a_shape`, `tf.raw_ops.SparseTensorDenseMatMul` triggers `INVALID_ARGUMENT` error. When there are too large values in `a_shape`, `tf.raw_ops.SparseTensorDenseMatMul` triggers `overflow` error.  Standalone code to reproduce the issue   Relevant log output ",2024-04-15T14:40:25Z,stat:awaiting tensorflower type:bug comp:ops TF 2.16,open,0,2,https://github.com/tensorflow/tensorflow/issues/65724,"Hi  , I have replicated the reported behaviour with tfnightly and attached screenshot below and gist for reference.", 
yi,copybara-service[bot],[XLA:GPU] Print the correct error message when dumping llvm ir in trtion.,"[XLA:GPU] Print the correct error message when dumping llvm ir in trtion. If `log_stream.emplace` returns error, we destruct `log_stream` and keep trying to use it later. Changed to fail earlier on and print a corresponding error message.",2024-04-15T11:59:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65718
yi,copybara-service[bot],Pass a flag whether we are in code emission phase.,"Pass a flag whether we are in code emission phase. When trying to bisect failing fusions with the new mlir emitters, we have the flags xla_gpu_max_mlir_kernels and xla_gpu_skip_mlir_kernels. However the function GetFusionEmitter() is also called during fusion passes. It seems we have some nondeterminism regarding how often it is called during fusion passes, so it is not easy to determine what value should be used for xla_gpu_skip_mlir_kernels to skip all the calls during fusion passes. Instead, better only start counting in the code emission phase.",2024-04-15T05:59:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65695
yi,OleksiyA,Link error when building flex:tensorflowlite_flex in debug mode for Windows, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version master  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Build should produce dll.  Standalone code to reproduce the issue   Relevant log output ,2024-04-15T01:37:01Z,stat:awaiting response type:bug stale comp:lite,closed,0,7,https://github.com/tensorflow/tensorflow/issues/65688," Could you try to build flex:tensorflowlite_flex in release mode (without c dbg flag) and see if the error persists. If it disappears, the issue is likely related to missing debug information. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Release mode build work without issue., Thank you for the confirmation. Could you please move this issue to closed status if it is resolved? Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
llm,cohaegen,Memory leak in tf.data when iterating over Dataset.from_generator," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version v1.12.1108954g88310ddcbdd 2.17.0dev20240412  Custom code Yes  OS platform and distribution Docker: tensorflow/tensorflow:nightly  Mobile device _No response_  Python version 3.11.0rc1  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I discovered what appears to be a memory leak when iterating over a tf.data.Dataset created with from_generator. Process memory usage continues to grow out of hand. The effect only appears in certain combinations of Tensorflow and Python and it may have appeared in Python 3.11. Here are some examples I've tested: Python 3.10.10, tensorflow 2.13.0: yes Python 3.10.10, tensorflow 2.16.1: no Python 3.10.12, tensorflow v2.15.00g6887368d6d4: no Python 3.11, tensorflow 2.16.1: yes Python: 3.11.0rc1, tensorflow v1.12.1109002g2c2c0a17f05: yes Maybe related to https://docs.python.org/3/whatsnew/3.11.htmlfastercpython? I thought maybe Python is reusing the memory and not freeing it, but usage grows ridiculously (I noticed it because it started taking up tens of GB in one cas",2024-04-14T19:02:53Z,stat:awaiting tensorflower type:bug comp:data TF 2.16,open,1,5,https://github.com/tensorflow/tensorflow/issues/65675,"Hi **** , I tried to run your code on colab using TF v2.15, 2.16.1, and nightly. But i am not facing any issue. Please find the gist here for reference. Thank you!","Hi Venkat thanks for checking it out. I’ve only seen the issue in newer versions of TF with Python 3.11 it seems ok with 3.10. It looks like you ran it with 3.10 (correct me if I’m mistaken). Can you try with 3.11? Thanks, Nick On Mon, Apr 15, 2024 at 2:16 AM Venkat6871 ***@***.***> wrote: > Hi ***@***.*** * , > I tried to run your code on colab using TF v2.15, 2.16.1, and nightly. But > i am not facing any issue. Please find the gist >  > here for reference. > > Thank you! > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","Hi **** ,  I tried to run your code on windows with python 3.11 and facing the same issue. Here i am adding screenshots for reference. !Screenshot 20240603 163119 !Screenshot 20240603 163153 (1) !Screenshot 20240603 163225 Thank you!","To resurface this, I am also seeing memory leaks when using Python 3.11 and Tensorflow 2.16.1, but it's not with the generator. I'm using the following pipeline:  I haven't spent too much time debugging, but this is what I am facing."," ,  ."
yi,xl1043237213,`Check failed` in `tf.raw_ops.TensorScatterSub` and `tf.tensor_scatter_nd_sub` when the rank of `indices` > 2.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.15  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? `Check failed` in `tf.raw_ops.TensorScatterSub` and `tf.tensor_scatter_nd_sub` when the rank of `indices` > 2, which causes the program to crash. See the colab link below for details.  Standalone code to reproduce the issue   Relevant log output ",2024-04-14T16:41:30Z,stat:awaiting tensorflower type:bug comp:ops TF 2.16,open,0,1,https://github.com/tensorflow/tensorflow/issues/65670,", I was able to reproduce the issue on TensorFlow v2.15, v2.16 and tfnightly. Kindly find the gist of it here. !image"
yi,xl1043237213,`Check failed` in `tf.raw_ops.TensorScatterMax` and `tf.tensor_scatter_nd_max` when the rank of `indices` > 2.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.15  Custom code No  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? `Check failed` in `tf.raw_ops.TensorScatterMax` and `tf.tensor_scatter_nd_max` when the rank of `indices` > 2, which causes the program to crash. See the colab link below for details.  Standalone code to reproduce the issue   Relevant log output ",2024-04-14T16:16:43Z,stat:awaiting response type:bug stale comp:ops TF 2.15,closed,0,5,https://github.com/tensorflow/tensorflow/issues/65668, I was able to replicate the issue reported here. Thank you!,Hi   The issue you're encountering is due to how the indices and updates values are being initialized. You can refer to the documentation link for guidance on properly initializing indices and updates.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,garywillcodeit,Error when trring a second training :  WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.13.0  Custom code Yes  OS platform and distribution MAC OS 13.5 Ventura  Mobile device _No response_  Python version 3.11.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi everyone ! I am trying to train a pretrained TensorFlow 2 Detection Model Zoo. Everything works well after the first training, new checkpoints are generated and when I try detecting objects, it finds perfectly the objects in every images. The problem is that when I start a new training (a second training), I get this message and the training stops : **WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter** Here is my tree structure: _tensorflow ____models (TF API) ____workspace ________annotations _____________label_map.pbtxt _____________test.record _____________train.record ________images _____________train _________________IMG1.jpg _________________IMG1.xml ... _________________IMG50.jpg _________________IMG50.xml _____________test _________________IMG1.jpg __________",2024-04-14T09:14:40Z,stat:awaiting response type:bug stale TF 2.13,closed,0,4,https://github.com/tensorflow/tensorflow/issues/65657,"Hi  , The error message seems generating from the destructor method from here. I am not sure how and why destructor method being called here. I request you to provide more context and minimal repro code for same. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,walker-ai,Getting [java.lang.IllegalArgumentException: Internal error: Error applying delegate] error while applying GPU Delegate," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.12.0  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device android 10.0 emulator  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 11.4 cuDNN 8  GPU model and memory _No response_  Current behavior? **Sorry about the type 'bug', I have no idea to solve it, I don' mean to offend that.** I am using the official tf example named generative_ai which from: https://github.com/tensorflow/examples/tree/master/lite/examples/generative_ai/android.  And I tried to add GPU delegate to the interpreter, and my dependencies's version are:  The test case is:  I have only add these lines(Added `wyt` commented section). But the app cannot be installed rightly. It will crash. But when I delete the param `options` to `interpreter = Interpreter(it)`, it works well. It stands to reason that gpu delegate should not be supported.  Standalone code to reproduce the issue   Relevant log output ",2024-04-14T04:30:41Z,stat:awaiting response type:support stale WIP comp:lite-examples Android,closed,0,11,https://github.com/tensorflow/tensorflow/issues/65648,,">  Thank you for your reply. I have tried this code above. It does catch the exception. However, the log convert from 'Error' to 'Warning"":  The problem seems to still exist. ","Hi ai , I have been trying to replicate your issue but i don't have autocomplete.tflite file, can you provide me the autocomplete.tflite file or the script you used to create it .","> Hi ai , >  > I have been trying to replicate your issue but i don't have autocomplete.tflite file, can you provide me the autocomplete.tflite file or the script you used to create it . Thank you for your reply. The **autocomplete.tflite** was converted from `quant_generate_tflite`, you can check the example: ","Hi ai , I followed the same example while creating the autocomplete.tflite file , for some reason the ""generate"" function under the decorator ""tf.function""  gave me numpy related error . I am trying to debug that, but if you have a dummy autocomplete.tflite file that you can provide, it will save me some time.","> Hi ai , >  > I followed the same example while creating the autocomplete.tflite file , for some reason the ""generate"" function under the decorator ""tf.function"" gave me numpy related error . I am trying to debug that, but if you have a dummy autocomplete.tflite file that you can provide, it will save me some time. Sorry, I haven't provided more details. You can open the colab and run through the notebook to get the `autocomplete.tflite`. And I have got the file for you.","Hi ai , Thank you for the model file. I replicated your issue on my emulator with android 10 and it also crashed when i tried running the model on GPU. I will try out some other tensorflow examples which make use of gpu acceleration and i will get back to you .","Hi ai, I modified your code to check for compatibility as shown here: https://www.tensorflow.org/lite/android/delegates/gpuenable_gpu_acceleration_2 My final code looks like this:  If I didn't include this compatibility check, I get your same error.... this tells me your GPU or system is not compatible with your setup (probably b/c of emulation). Perhaps try on a real device which is supported. Let me know if you have any questions.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
chat,Praveer1981,Suggestion: Chatbot Integration for TensorFlow API Assistance,"Hi, I'm learning TensorFlow and sometimes struggle with finding the correct API name. Could a chatbot be integrated into the tensorflow.org website to assist with this? I could ask it a question, and it would provide the accurate API name with all the necessary information, and in fact, it could give a link to navigate to the API page. Kind Regards, Praveer.",2024-04-14T02:20:19Z,type:docs-feature,open,0,1,https://github.com/tensorflow/tensorflow/issues/65643,"Hi **** , Thanks for your suggestion, We will discuss this with our engineering team for update belongs to your suggestion. Thank you!"
yi,xl1043237213,`Overflow` in `tf.raw_ops.DenseBincount` when the value of `size` is too large.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.15  Custom code No  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When the value of `size` is close to the maximum value for the dtype, an integer overflow error will occur in following multiplication calculations.  Standalone code to reproduce the issue   Relevant log output ",2024-04-13T18:07:22Z,stat:awaiting response type:bug stale comp:ops TF 2.16,closed,0,5,https://github.com/tensorflow/tensorflow/issues/65634,"Hi  , Thanks for reporting. I have reproduced the reported behaviour and attached screenshot below. ","Hi , I have tried to implement the bincount using the Jax API and observed the similar result as DenseBincount. please refer to this gist. Thank You.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],Full ICI flag coverage,Full ICI flag coverage,2024-04-12T22:19:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/65565
yi,copybara-service[bot],Pass a flag whether we are in code emission phase.,"Pass a flag whether we are in code emission phase. When trying to bisect failing fusions with the new mlir emitters, we have the flags xla_gpu_max_mlir_kernels and xla_gpu_skip_mlir_kernels. However the function GetFusionEmitter() is also called during fusion passes. It seems we have some nondeterminism regarding how often it is called during fusion passes, so it is not easy to determine what value should be used for xla_gpu_skip_mlir_kernels to skip all the calls during fusion passes. Instead, better only start counting in the code emission phase.",2024-04-12T13:26:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65532
rag,copybara-service[bot],PR #11458: [ROCm] Build fixes,PR CC(Add a code of conduct.): [ROCm] Build fixes Imported from GitHub PR https://github.com/openxla/xla/pull/11458 Contains fix for compile failure after e21311e52a0be80042d4c807847ec17919f2f954 Contains fix for link failure after 596745eca6b9fb1553ee71b63c71e92527f1d7f4 Copybara import of the project:  b253cbac2da4515ff4437b820f37a2b097a287c1 by Dragan Mladjenovic : [ROCm] Build fixes Contains fix for compile failure after e21311e52a0be80042d4c807847ec17919f2f954 Contains fix for link failure after 596745eca6b9fb1553ee71b63c71e92527f1d7f4 Merging this change closes CC(Add a code of conduct.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/11458 from ROCm:build_fix_11042024 b253cbac2da4515ff4437b820f37a2b097a287c1,2024-04-12T08:40:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65519
yi,ripjohnbrown1859,"loss growing to negative infinity val accuracy remaining constant, testing accuracy above 100%"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.16.1  Custom code Yes  OS platform and distribution wsl ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 8.6.0  GPU model and memory sli titan x maxwell  Current behavior? currently my loss is increasing to negative infinity, my test accuracy is sitting around 1.85 and my val accuracy is staying at .9374 no matter how may epochs i run. i am training my model to read ekgs, i have 64 not mutually exclusive outputs that correspond with different diagnoses. each input is a 12 lead ekg with 5000 time series datapoints for each lead.  Standalone code to reproduce the issue   Relevant log output ",2024-04-11T16:29:04Z,stat:awaiting response type:bug stale comp:gpu TF 2.16,closed,0,9,https://github.com/tensorflow/tensorflow/issues/65468, Could you please have a look at this gist and confirm the error reported here.  Thank you!,that is because I am running 2 gpus and the notebook does not have access to 2 gpus so it says the list index for gpus is out of range. it should work if you simply cut out all the gpu[1] stuff.,"this is not a gpu issue, the issue persists whether or not i run it on my gpus. or cpu. ","Hi  , Since the submitted code snippet has dependencies with your dataset could you please submit a minimal code with some dummy set. There seems to be issue as I can notice accuracy is >1 from the attached logs.",heres the link to the dataset. https://physionet.org/content/ecgarrhythmia/1.0.0/,"Hi @**ripjohnbrown1859** , Apologies for the delay. You provided a very large code snippet, which makes it hard for us to debug. Could you please try to provide a simpler code example where you are facing the issue? Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,ankur2210,"Segmentation fault when using tflite_model_maker searcher.TextDataLoader.create(EmbeddingModel, l2_normalize=True)"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.8.4  Custom code Yes  OS platform and distribution Ubuntu 22.04  Mobile device Ubuntu 22.04  Python version 3.9.19  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to make TextSearcher object using tflite_model_maker python library. `data_loader = searcher.TextDataLoader.create(EmbeddingModel, l2_normalize=True)` At this step, getting segmentation fault error. Embedding model is tflite model converted from https://www.kaggle.com/models/google/universalsentenceencoder/tensorFlow2/multilingual using below code  `import tensorflow as tf  These imports are required to load operators' definition. import tensorflow_text as tf_text import sentencepiece as spm path = '/path/to/embedding/model' converter = tf.lite.TFLiteConverter.from_saved_model(path) converter.target_spec.supported_ops = [   tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS ] model_data = converter.convert() with open('model.tflite', 'wb') as f:   f.write(model_data)`   On analysing tflite model using     `imp",2024-04-10T13:27:25Z,stat:awaiting response type:bug stale comp:lite subtype: ubuntu/linux TF 2.8 TFLiteModelMaker,closed,0,10,https://github.com/tensorflow/tensorflow/issues/65409,"Hi **** , Sorry for the delay, I reproduced the code shared but facing different error .Could you please share the colab gist with all the dependencies to analyze more of it. And could you please execute your code using Latest Version (2.16.1) and let us know if the issue still persists? Thank you!","> Hi **** , >  > Sorry for the delay, I reproduced the code shared but facing different error .Could you please share the colab gist with all the dependencies to analyze more of it. And could you please execute your code using Latest Version (2.16.1) and let us know if the issue still persists? >  > Thank you! Hi   Thanks for the reply. I am using 'tflite_model_maker' library to create the searcher model. from tflite_model_maker import searcher on installing 'tflite_model_maker' using `pip install tflitemodelmaker` It install tensorflow 2.8.4 and uninstalls latest tensorflow.", Did you check the Segmentation fault issue ?,"Hi  , I will replicate the issue and will get back to you. ","Hi  , I am having issues installing tensorflow_text using pip . Did you face any such issue ? Either way i will get back to you once i am able to reproduce the issue .","I am not able to replicate this because i still can't install tflitemodelmaker using pip, can you please take a look  ","Hi , TFLiteModelMaker is effectively not working right now... can you use mediapipe instead? https://ai.google.dev/edge/mediapipe/solutions/model_maker",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Temporarily disable newer cuDNN fMHA features due to miscompile,"Temporarily disable newer cuDNN fMHA features due to miscompile I'm trying to make XLA compatible with cuDNN 9 and stumbled upon a miscompile in the fMHA rewriter. It seems to be related with the cause mask pattern matcher, at least forcing the causal mask flag to true, makes the broken test path. I extract a reproducer and added an (integration) test which can be removed or converted into a proper unit test once this is fixed. IN the meantime I disable all fMHA features that require a version of cuDNN 8.9.6+.",2024-04-10T11:30:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65402
yi,damienwojtowicz,Documentation for RaggedTensors does not match Input layer documentation," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16.1  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Documentation of RaggedTensors in section TensorFlow APIs > Keras claim the following, and provides an example where argument `ragged=True` is passed to the constructor of a `tf.keras.Input` object: > Ragged tensors may be passed as inputs to a Keras model by setting ragged=True on tf.keras.Input or tf.keras.layers.InputLayer. However, `ragged` is neither an argument of `tf.keras.Input`, `tf.keras.layers.InputLayer`, nor any of their parent classes. Their respective documentation does not mention it. Trying to execute the example provided with either TF v. 2.16.1 ir nightly (see the MWE) raises a `TypeError` stating that `ragged` is an unexpected keyword for `Input`. It is puzzling as several sources on this internet as well as issues of this very repository mention it. As a consequence, it seems that ragged",2024-04-10T10:22:57Z,type:docs-bug stat:awaiting response type:feature stale comp:keras,closed,1,5,https://github.com/tensorflow/tensorflow/issues/65399,"Hi  , Starting from TF2.16v tensorflow bundles with Keras3. Since Kera3 is supporting multi backend now ragged tensors are not supported on other frameworks. Keras team also has plans to support Ragged Tensors with tensorflow backend in nearby future. Since then if you want to use ragged tensors I recommend to install tf_keras package and set environment varaibale TF_USE_LEGACY_KERAS=1 . ",Please refer this for more details.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,keyur2maru,"[Android x86_64] [TFLite] `Fatal signal 11 (SIGSEGV), code 128 (SI_KERNEL), fault addr 0x0` crash from `libtensorflowlite.so`"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1  Custom code Yes  OS platform and distribution Ubuntu Host Build Machine  Mobile device Android 11 X86_64   Python version _No response_  Bazel version 6.5.0  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I was testing https://github.com/nyadlasys/whisper.tflite / https://github.com/nyadlasys/whisper.tflite/tree/main/whisper_android on Android x86_64 Initially I was able to run it without any issues on real x86_64 Android device. It was correctly running the inference and returned back with transcribed text. But after about 30 mins of using it, it just mysteriously started crashing without much visibility from the logs  Standalone code to reproduce the issue   Relevant log output ",2024-04-09T10:47:23Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/65314,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #11182: [NVIDIA GPU] Optimize collective permute using memcpy,"PR CC(Quantize weights causes accuracy to plunge when run in mobile but not in computers?): [NVIDIA GPU] Optimize collective permute using memcpy Imported from GitHub PR https://github.com/openxla/xla/pull/11182 When the source and target pair of a collective permute instruction are located within the same node, we will try to use cudaMemCpyPeerToPeer to send data instead of invoking nccl send and recv kernels. Because memcpy will use the copy engine on the GPU instead of launching a kernel, copy engine can fully saturate the bandwidth without occupying any SM. Since memcpyP2p needs the destination pointer of the other device, we first communicate the pointer values with peer using nccl calls and use the pointer to invoke memcpy. Some perf comparison: sending a tensor of this size bf16[512,24576]{1,0}: Using collectivepermute: 445 us using memcpy: 100us Copybara import of the project:  73a2568520d411eb6f62b6383a3841cb0f8edf22 by TJ : optimize collective permute using memcpy  1dd896f544852fa898e937af8c0b94e075db3ec2 by TJ : Remove nccl calls and use Async value  a2fbc7d29d3d8106753f36e2a477e0889da6cb58 by TJ : wrap async map and mutex with a class Merging this change closes CC(Quantize weights causes acc",2024-04-09T09:18:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65305
rag,rafimps18,"__dict__ error when saving a model using tf.saved_model.save(model, modelSavedPath)"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16.1  Custom code No  OS platform and distribution Windows 10  Mobile device _No response_  Python version 3.12.2  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I was training MobileNetV3 on a custom dataset and tried to save it as a SavedModel because I get errors when converting .keras files to .tflite. So I retrained my model and to save the model I used `tf.saved_model.save(model, modelSavedPath)`. After the code was done, it displayed this error: `TypeError: this __dict__ descriptor does not support '_DictWrapper' objects`  Standalone code to reproduce the issue   Relevant log output ",2024-04-09T00:38:10Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/65279,Are you satisfied with the resolution of your issue? Yes No
yi,6CRIPT,Tensorflow does not recognice GPU no matter what," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.16.1 or 2.12.0 or 2.10.0  Custom code Yes  OS platform and distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.9.19  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 11.5   GPU model and memory i7 12700k, 32 GB RAM  Current behavior? So I am trying in all the ways that are written on the internet to tensorflow detect my GPU. I am currently using a conda enviroment on WSL2. I was following this guide: https://medium.com/.thanapol/tensorflowwithgpuonlinuxorwsl210b02fd19924 and then this: https://www.tensorflow.org/install/pip?hl=es419windowswsl2 I have spent like 15 hours of trying tensorflow detect my GPU. I really really really need help. I am using it to my final project of CS. Thanks in advice.  Standalone code to reproduce the issue   Relevant log output ",2024-04-08T22:10:46Z,type:build/install subtype: ubuntu/linux TF 2.16,closed,0,5,https://github.com/tensorflow/tensorflow/issues/65269,", I suspect you are trying to install the every tensorflow version on CUDA 11.5 only which is not compatible. Could you please uninstall the installed libraries and try to install CUDA 12.3 and cuDNN 8.9 for the tensorflow v2.16.1 and check the below process.  https://github.com/tensorflow/tensorflow/issues/63341 https://github.com/tensorflow/tensorflow/issues/63948 Thank you!","Hi, thanks for your reply. As you can see on my nvidiasmi output I was using CUDA 12.4 I will retry the entire process anyways, any updates will be posted :p ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Hi I finally solved it. It was all about installing exactly the acurrate versions. Thanks for your responses ! :D I have uploaded a video to youtube explaining the process: https://www.youtube.com/watch?v=iIYHfCh1rmU&t=5s,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Remove StreamExecutorInterface::GetUnderlyingExecutor function.,Remove StreamExecutorInterface::GetUnderlyingExecutor function.,2024-04-08T20:19:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65262
yi,AmanMehta199816,Semantic Segmentation using Deep Learning By Aman Mehta,"**Project Description: Semantic Segmentation using Deep Learning By Aman Mehta** **Overview:** Semantic segmentation is a computer vision task that involves assigning semantic labels to each pixel in an image, enabling precise understanding and analysis of its contents. This project aims to develop a robust deep learning model capable of accurately segmenting objects and scenes within images. **Dataset Selection:** The project begins with the selection of a suitable dataset tailored for semantic segmentation tasks. Datasets such as Pascal VOC, Cityscapes, or COCO provide labeled images with pixellevel annotations, facilitating model training and evaluation. **Data Preprocessing:** Before training the model, extensive preprocessing is necessary to prepare the dataset. This involves tasks such as resizing images to a consistent resolution, normalizing pixel values to a standard range, and generating corresponding segmentation masks for training samples. **Model Architecture:** The core of the project lies in designing an effective neural network architecture capable of performing semantic segmentation. Common architectures like UNet, Fully Convolutional Networks (FCNs), or DeepLab are wellsuited for this ",2024-04-08T16:57:31Z,size:S,closed,0,4,https://github.com/tensorflow/tensorflow/issues/65239,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","**Project Description: Semantic Segmentation using Deep Learning** **Overview:** Semantic segmentation is a computer vision task that involves assigning semantic labels to each pixel in an image, enabling precise understanding and analysis of its contents. This project aims to develop a robust deep learning model capable of accurately segmenting objects and scenes within images. **Dataset Selection:** The project begins with the selection of a suitable dataset tailored for semantic segmentation tasks. Datasets such as Pascal VOC, Cityscapes, or COCO provide labeled images with pixellevel annotations, facilitating model training and evaluation. **Data Preprocessing:** Before training the model, extensive preprocessing is necessary to prepare the dataset. This involves tasks such as resizing images to a consistent resolution, normalizing pixel values to a standard range, and generating corresponding segmentation masks for training samples. **Model Architecture:** The core of the project lies in designing an effective neural network architecture capable of performing semantic segmentation. Common architectures like UNet, Fully Convolutional Networks (FCNs), or DeepLab are wellsuited for this task. These architectures typically consist of an encoderdecoder structure with skip connections to capture both local and global context information. **Model Training:** Once the model architecture is defined, it is trained using the prepared dataset. Training involves optimizing the model parameters using techniques like stochastic gradient descent (SGD) or Adam optimization. Additionally, data augmentation techniques such as random rotation, flipping, and scaling are employed to improve the model's generalization ability. **Evaluation:** The trained model's performance is evaluated using appropriate metrics such as Intersection over Union (IoU), Pixel Accuracy, or Mean Intersection over Union (mIoU). This evaluation helps gauge the model's accuracy and generalization ability on unseen data. **Inference:** After training and evaluation, the model is deployed to perform semantic segmentation on new images. The inference pipeline involves feeding input images to the trained model and obtaining segmentation maps as output. Visualization techniques are used to interpret and analyze the segmentation results, enabling insights into the model's performance. **Conclusion:** Semantic segmentation using deep learning is a challenging yet rewarding task with diverse applications across various domains. By undertaking this project, participants gain valuable experience in designing, training, and deploying deep learning models for realworld image analysis tasks. Moreover, the project contributes to advancing the field of computer vision by developing models capable of understanding and interpreting visual information at a pixel level.","**Project Description: Semantic Segmentation using Deep Learning** **Overview:** Semantic segmentation is a computer vision task that involves assigning semantic labels to each pixel in an image, enabling precise understanding and analysis of its contents. This project aims to develop a robust deep learning model capable of accurately segmenting objects and scenes within images. **Dataset Selection:** The project begins with the selection of a suitable dataset tailored for semantic segmentation tasks. Datasets such as Pascal VOC, Cityscapes, or COCO provide labeled images with pixellevel annotations, facilitating model training and evaluation. **Data Preprocessing:** Before training the model, extensive preprocessing is necessary to prepare the dataset. This involves tasks such as resizing images to a consistent resolution, normalizing pixel values to a standard range, and generating corresponding segmentation masks for training samples. **Model Architecture:** The core of the project lies in designing an effective neural network architecture capable of performing semantic segmentation. Common architectures like UNet, Fully Convolutional Networks (FCNs), or DeepLab are wellsuited for this task. These architectures typically consist of an encoderdecoder structure with skip connections to capture both local and global context information. **Model Training:** Once the model architecture is defined, it is trained using the prepared dataset. Training involves optimizing the model parameters using techniques like stochastic gradient descent (SGD) or Adam optimization. Additionally, data augmentation techniques such as random rotation, flipping, and scaling are employed to improve the model's generalization ability. **Evaluation:** The trained model's performance is evaluated using appropriate metrics such as Intersection over Union (IoU), Pixel Accuracy, or Mean Intersection over Union (mIoU). This evaluation helps gauge the model's accuracy and generalization ability on unseen data. **Inference:** After training and evaluation, the model is deployed to perform semantic segmentation on new images. The inference pipeline involves feeding input images to the trained model and obtaining segmentation maps as output. Visualization techniques are used to interpret and analyze the segmentation results, enabling insights into the model's performance. **Conclusion:** Semantic segmentation using deep learning is a challenging yet rewarding task with diverse applications across various domains. By undertaking this project, participants gain valuable experience in designing, training, and deploying deep learning models for realworld image analysis tasks. Moreover, the project contributes to advancing the field of computer vision by developing models capable of understanding and interpreting visual information at a pixel level.",Hi  Can you please sign CLA. Thank you!
yi,saad-koukous,TensorFlow lite compilation error under Ubuntu 22.04.4 LTS, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.10.0baz  Custom code Yes  OS platform and distribution Ubuntu 22.04.4 LTS  Mobile device _No response_  Python version _No response_  Bazel version bazel 6.5.0  GCC/compiler version gcc (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I built the tflite library using bazel and i'm just trying to test if it work but can't compile   Standalone code to reproduce the issue   Relevant log output ,2024-04-08T09:08:13Z,type:bug,closed,0,2,https://github.com/tensorflow/tensorflow/issues/65222,1Clone the TensorFlow Lite repository. 2Build the library using this command: `bazel build c opt //tensorflow/lite/c:libtensorflowlite_c.so`. 3Copy **libtensorflowlite_c.so** to your folder from `bazelbin/tensorflow/lite/c/`. 4Compile: `gcc o hello_tf hello_tf.c I. Llib/tensorflow/lite ltensorflowlite_c`. 5Copy and paste your dynamic library to **/usr/lib**: `sudo cp lib/tensorflow/lite/libtensorflowlite_c.so /usr/lib`. 6Almost done: `./hello_tf`.,Are you satisfied with the resolution of your issue? Yes No
agent,sgowroji,"[Bazel CI] Bazel build error: Target '//tensorflow/tools/pip_package:build_pip_package' contains an error with Bazel@HEAD
",CI: https://buildkite.com/bazel/bazelatheadplusdisabled/builds/2004 CC(C api)eb729e4a746e0b68c4b90fcac20cc  Platform: Ubuntu  Logs:     Culprit:   Steps:      CC Greenteam ,2024-04-08T07:12:43Z,stat:awaiting tensorflower type:build/install subtype:bazel,closed,0,6,https://github.com/tensorflow/tensorflow/issues/65220,"/cc toplay,  google, [](https://github.com/chsigg)",Having the same error when trying to compile `Tensorflow2.15.1` with `Bazel7.2.0` ... Any ideas?,Encounter the same issue arch is aarch64 Tensorflow2.17.1 with Bazel7.2.1,"Tensorflow requires 6.5.0 at the moment:   https://github.com/tensorflow/tensorflow/blob/dd446b3de1759d64c80b636a8d41ce341782219c/.bazelversionL1 This will be updated to 7+, but it's not completely trivial.   In the meantime, using Bazelisk is helpful, which will automatically pick the version set in `.bazelversion`:   https://github.com/bazelbuild/bazelisk",Are you satisfied with the resolution of your issue? Yes No,"Thanks to toplay, in this case, the problem could be not Bazel 7 (though that won't work), but the target, at least for the first comment  it no longer exists.   See https://github.com/tensorflow/tensorflow/issues/63298issuecomment1995159235"
rag,copybara-service[bot],[xla:gpu] Fix the dimension order propagation for kPad,[xla:gpu] Fix the dimension order propagation for kPad Previously trivial fragments are not handled because we didn't support matrices with trivial dimensions.,2024-04-07T18:48:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65210
rag,copybara-service[bot],KV Cache storage is now a CacheBuffer in the resource map.,KV Cache storage is now a CacheBuffer in the resource map.,2024-04-07T15:17:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65206
yi,esonde,ERROR: @local_config_cuda//:enable_cuda :: Error loading option @local_config_cuda//:enable_cuda: 'NoneType' value has no field or method 'replace'," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.10.0  Custom code No  OS platform and distribution windows 11  Mobile device _No response_  Python version 3.9.12  Bazel version 5.1.1 (bazelisk)  GCC/compiler version msvc 2019  CUDA/cuDNN version 11.1 8.1  GPU model and memory RTX4090 32GB  Current behavior? I was trying to build tensorflow for c++ api use on visual studio, with support for gpu. CUDA/cuDNN/TensorRT. It seems there is a problem with finding the path for the compiler. But if i run cl on console it finds it and its correctly added to path  Standalone code to reproduce the issue   Relevant log output ",2024-04-06T21:35:50Z,stat:awaiting response type:build/install stale subtype:windows TF 2.10,closed,0,6,https://github.com/tensorflow/tensorflow/issues/65195,", The error seems to be a bad path. Can you please try to edit the file **""tensorflow/third_party/gpus/cuda_configure.bzl""**, line 1400 to identify what path the code is looking for and then validate that it exists? I also suspect that the problem comes from the global environment, have you added the MVSC_tool to your path? and if so check if you have the cl.exe file inside it.  Could you try **bazel clean expunge** followed by bazel sync. Also try to set the **BAZEL_SH** environment variable, restarting the pc and run the cmd with Administrator rights. Thank you!","if i change at line 100 def _get_msvc_compiler(repository_ctx):     vc_path = find_vc_path(repository_ctx)     print(""vc_path: "", vc_path"")     return find_msvc_tool(repository_ctx, vc_path, ""cl.exe"").replace(""\\"", ""/"") i get  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64 which exist and points to cl.exe. If i simply write cl it executes, so globally it is also set. If i run bazel sync i get a bunch of  errors... the first 2 are C:\Users\aless\dev\tensorflow>bazel sync INFO: Repository build_bazel_rules_swift_local_config instantiated at:   C:/users/aless/dev/tensorflow/WORKSPACE:23:14: in    C:/users/aless/dev/tensorflow/tensorflow/workspace0.bzl:125:29: in workspace   C:/users/aless/_bazel_aless/ozqc7q5z/external/build_bazel_rules_swift/swift/repositories.bzl:123:11: in swift_rules_dependencies   C:/users/aless/_bazel_aless/ozqc7q5z/external/build_bazel_rules_swift/swift/repositories.bzl:32:18: in _maybe Repository rule swift_autoconfiguration defined at:   C:/users/aless/_bazel_aless/ozqc7q5z/external/build_bazel_rules_swift/swift/internal/swift_autoconfiguration.bzl:302:42: in  ERROR: An error occurred during the fetch of repository 'build_bazel_rules_swift_local_config':    Traceback (most recent call last):         File ""C:/users/aless/_bazel_aless/ozqc7q5z/external/build_bazel_rules_swift/swift/internal/swift_autoconfiguration.bzl"", line 300, column 32, in _swift_autoconfiguration_impl                 _create_linux_toolchain(repository_ctx)         File ""C:/users/aless/_bazel_aless/ozqc7q5z/external/build_bazel_rules_swift/swift/internal/swift_autoconfiguration.bzl"", line 212, column 13, in _create_linux_toolchain                 fail(""ERROR: rules_swift uses Bazel's CROSSTOOL to link, but Swift "" + Error in fail: ERROR: rules_swift uses Bazel's CROSSTOOL to link, but Swift requires that the driver used is clang. Please set `CC=clang` in your environment before invoking Bazel. ERROR: C:/users/aless/dev/tensorflow/WORKSPACE:23:14: fetching swift_autoconfiguration rule //external:build_bazel_rules_swift_local_config: Traceback (most recent call last):         File ""C:/users/aless/_bazel_aless/ozqc7q5z/external/build_bazel_rules_swift/swift/internal/swift_autoconfiguration.bzl"", line 300, column 32, in _swift_autoconfiguration_impl                 _create_linux_toolchain(repository_ctx)         File ""C:/users/aless/_bazel_aless/ozqc7q5z/external/build_bazel_rules_swift/swift/internal/swift_autoconfiguration.bzl"", line 212, column 13, in _create_linux_toolchain                 fail(""ERROR: rules_swift uses Bazel's CROSSTOOL to link, but Swift "" + Error in fail: ERROR: rules_swift uses Bazel's CROSSTOOL to link, but Swift requires that the driver used is clang. Please set `CC=clang` in your environment before invoking Bazel. INFO: Repository sigbuildr2.10python3.7_config_nccl instantiated at:   C:/users/aless/dev/tensorflow/WORKSPACE:15:14: in    C:/users/aless/dev/tensorflow/tensorflow/workspace2.bzl:879:19: in workspace   C:/users/aless/dev/tensorflow/tensorflow/workspace2.bzl:86:27: in _tf_toolchains   C:/users/aless/dev/tensorflow/tensorflow/tools/toolchains/remote_config/configs.bzl:257:24: in initialize_rbe_configs   C:/users/aless/dev/tensorflow/tensorflow/tools/toolchains/remote_config/rbe_config.bzl:184:30: in sigbuild_tf_configs Repository rule remote_nccl_configure defined at:   C:/users/aless/dev/tensorflow/third_party/nccl/nccl_configure.bzl:134:40: in  ERROR: An error occurred during the fetch of repository 'sigbuildr2.10python3.7_config_nccl':    Traceback (most recent call last):         File ""C:/users/aless/dev/tensorflow/third_party/nccl/nccl_configure.bzl"", line 77, column 35, in _create_local_nccl_repository                 cuda_config = find_cuda_config(repository_ctx, find_cuda_config_path, [""cuda""])         File ""C:/users/aless/dev/tensorflow/third_party/gpus/cuda_configure.bzl"", line 649, column 41, in find_cuda_config                 exec_result = _exec_find_cuda_config(repository_ctx, script_path, cuda_libraries)         File ""C:/users/aless/dev/tensorflow/third_party/gpus/cuda_configure.bzl"", line 643, column 19, in _exec_find_cuda_config                 return execute(repository_ctx, [python_bin, ""c"", decompress_and_execute_cmd])         File ""C:/users/aless/dev/tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute                 fail( Error in fail: Repository command failed java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(""C:\users\aless\_bazel_aless\ozqc7q5z\external\sigbuildr2.10python3.7_config_nccl\usr\bin\python3"" c ""from zlib import decompress;from base64 import b64decode;from os import system;script = decompress(b64decode('eJzdPGtz48aR3/krJtjbEqilIMmXcuV4kVOytHvWRZG2JO76UhLDjIAhBS8I8ACQEpPKf093zwODAUC9LHvLqvKaAKZ7unv6Oa837ChbrPN4dluyb/b2/4uNbgUbibTI8g9JdscOl+VtlhcBO0wSdoHNCnYhCpGvRBT03vTesNM4hOYiYss0EjkrAf5wwUP4n/oyYJ9FXsRZyr4J9piPDTz1yev/N2BYZ0s252uWZiVbFgJQxAWbxolg4j4Ui5LFKQuz+SKJeRoKdheXt9SNQgJk(...)): Impossibile trovare il file specificato.  (error: 2) i dont really understand why i read linux while im on windows",", I suspect there are some discrepancies happening because of multiple environments in the system. Could you please try to create the virtual environment and try to follow the steps for the installation. Also Have you tried **config=nccl**  command during Tensorflow build run. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,perphyguo,Cannot use @TaskAction annotation on method IncrementalTask.taskAction$gradle_core() because interface org.gradle.api.tasks.incremental.IncrementalTaskInputs is not a valid parameter to an action method.," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version donot know  Custom code No  OS platform and distribution windows 10  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I use android studio chipmunk to run several tflite examples. build failed Here is the example I tried to run https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android FAILURE: Build failed with an exception. * What went wrong: Could not determine the dependencies of task ':app:packageDebug'. > Could not create task ':app:mergeDebugAssets'.    > Cannot use  annotation on method IncrementalTask.taskAction$gradle_core() because interface org.gradle.api.tasks.incremental.IncrementalTaskInputs is not a valid parameter to an action method. I asked chatgpt, and tell me it should be gradle or apg version issue. I checked, the version should be fine. I already change the jdk to java 11. so this is not problem thanks  Standalone code to reproduce the issue   Relevant log output _No res",2024-04-06T09:24:56Z,stat:awaiting response type:build/install stale comp:lite Android,closed,5,37,https://github.com/tensorflow/tensorflow/issues/65187,Same issue. https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android I tried too and failed with issue above.,"Hi , Please specify which Gradle version used for your project. Once change the `Android Gradle Plugin Version = 7.4.2` and `Gradle Version = 8.0` in your project structure and let us know the response. Thank You","I'm on Mac (Sonoma 14.4.1,  Android Studio Iguana | 2023.2.1 Patch 2 ) and get the same problem with ""gesture_classification), using AGP 7.2.0, Gradle 8.4. !Screenshot 20240418 at 3 01 15 PM I've searched around for a couple of hours without making any progress, including trying to upgrade my gradle plugin, kotlin gradle plugin successfully to AGP 8.1.0 / 8.4 as per this suggestion, but that resulted in an additional compiler error, and I was also unable to complete the last step of running ""Tools > AGP Upgrade Assistant"" (when I select the menu item, nothing happens). ",Same Issue happens here also https://github.com/tensorflow/examples/tree/master/lite/examples/model_personalization/android,I faced same problem on https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android_java. Modifying the gradle version from gradle8.4 to gradle7.4 solved the problem but I don‘t know why。 !image,"Same problem (with gradle versions 8.4, and also the latest 8.7), also with `audio_classification`; same solution:  downgrading to 7.4 fixed it. Using gradle from the command line (no Android Studio or plugin).","Hi  , I cloned the https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android and was able to build the project with the android with the ""Android Gradle Plugin version =72.1"" and ""gradle version = 7.3.3"" .  Let me know if you have any further issues  ![Uploading Screenshot 20240522 at 11.59.08 AM.png…]()",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"> I'm on Mac (Sonoma 14.4.1, Android Studio Iguana | 2023.2.1 Patch 2 ) and get the same problem with ""gesture_classification), using AGP 7.2.0, Gradle 8.4. !Screenshot 20240418 at 3 01 15 PM >  > I've searched around for a couple of hours without making any progress, including trying to upgrade my gradle plugin, kotlin gradle plugin successfully to AGP 8.1.0 / 8.4 as per this suggestion, but that resulted in an additional compiler error, and I was also unable to complete the last step of running ""Tools > AGP Upgrade Assistant"" (when I select the menu item, nothing happens). I can build with this setup! Thank you~~~","Hi  , Can you please let me know if the provided solution worked for you?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Same issue, fixed by change distributionUrl gradle8.5 to gradle7.4","Sam issue, but when i downgrade the gradle vertion to 7.4 (or 7.5), i got another problem. another solution?","> Same issue. https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android I tried too and failed with issue above. As  said, if you are working on a TensorFlow lite android example, just revert the last commit that upgraded the gradle version. In android > gradle > wrapper > gradlewrapper.properties, revert `distributionUrl` to 7.6.2. ","Same issue after upgrading to Android Studio Ladybug 2024.2.1 (Build AI242.21829.142.2421.12409432, built on September 24, 2024). My flutter project was using gradle 7.5: after upgrading Android Studio, it could no longer compile and so I had to also upgrade to gradle 8.5 as suggested by flutter fix, but now I got this issue and cannot find any solution", absolutely same situation expect it compiles locally on my emulator but once I try to build an apk/aab totally crashes,Same issue after upgrading to Android Studio Ladybug ...  any fix?,Getting this error ,"Experiencing the same with Android Studio Ladybug, forced me to upgrade to 8.5 and now I have the error above.","Hi,   Please take look into this issue. Thank you","Today I meet this problem again.. I don't know why it worked. Sorry to make confusion If I get the other solution, i'll update new comment > My Environment > * Android Studio Koala 2024.1.2(It still works with LadyBug) > * Flutter 3.19.6 > * Java 17 >  > The way to fix this issue > 1. In ``android/gradle/wrapper/gradlewrapper.properties``, change your distributionUrl into 7.6.3 > ``distributionUrl=https\://services.gradle.org/distributions/gradle7.6.3bin.zip`` > !image >  > 2. In , change your com.android.application version into 7.3.0 > !image >  > 3.  flutter clean & flutter pub get","> I had same a issue but maybe I did it..! >  > My Environment >  > * Android Studio Koala 2024.1.2(It still works with LadyBug) > * Flutter 3.19.6 > * Java 17 >  > The way to fix this issue >  > 1. In `android/gradle/wrapper/gradlewrapper.properties`, change your distributionUrl into 7.6.3 >    `distributionUrl=https\://services.gradle.org/distributions/gradle7.6.3bin.zip` >    !image > 2. In `android/setting.gradle`, change your com.android.application version into 7.3.0 >    !image > 3. flutter clean & flutter pub get It unfortunately does not work on latest Android Studio ladybug,  downgrading from 8.4 to what you recommended 7.6 give the following message in flutter fix ""Your project's Gradle version is incompatible with the Java version that Flutter is using for Gradle""",> Getting this error >  >  > Getting this error >  >  same error.,"Hi  or anyone else, have you guys tried with this repo? https://github.com/googleaiedge/litertsamples/tree/main/examples . These examples are more up to date.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Fixed by upgrading androidGradlePlugin to 8.7.1 and gradle to 8.10.2. My Android studio version is Android Studio Ladybug | 2024.2.1 Patch 1,"same error, somebody can solve this problem?",I managed to resolve the issue by downgrading Java version that my Android Studio is using in gradle settings and then resync Gradle in Android Studio. Hope it helps. ,I modified the JDK version to solve the problem. My environment:   Android Gradle Plugin Version : 7.4.2   Gradle Version : 7.6.3 ** Gradle JDK : 21 > 17 to resync fix problem. ,"After upgrading Android Studio to Ladybug | 2024.2.1 Patch 2, AGP 8.7.2, and Gradle version 8.9. Set namespace in the Gradle file. I have successfully run the TensorFlow example image classification."
yi,copybara-service[bot],Redefines the identifier API of the kind of a `PjRtMemorySpace`:,"Redefines the identifier API of the kind of a `PjRtMemorySpace`: 1) `PjRtMemorySpace::kind()` returns a verbose string identifying the kind of a `PjRtMemorySpace` instance. For instance, ""device"", ""pinned_host"". 2) `PjRtMemorySpace::kind_id()` returns a unique ID identifying the kind ID of a `PjRtMemorySpace` instance. This is useful for quickly comparing the kinds of memory spaces.",2024-04-06T07:49:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65174
rag,copybara-service[bot],[IFRT] Add fast pointer equality test for `DeviceList` internal state,"[IFRT] Add fast pointer equality test for `DeviceList` internal state Copied `DeviceList`s share their internal state with the source `DeviceList`. We leverage it to make comparison faster between two `DeviceList` that are a (transitive) copy of the other. The singledevice `DeviceList` does not use pointer equality test because the internal state is not wrapped in a shared pointer, but this case will not benefit from pointer equality test anyway.",2024-04-05T20:47:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65153
yi,copybara-service[bot],#shlo_ref Add F16 wrapper type,"shlo_ref Add F16 wrapper type _Float16 is not available on the GCC until version 13, which is very recent. This change transforms the F16 into a wrapper type that will always be used regardless of compiler support. It defines conversion and arithmetic operators. If _Float16 is available, that will be used as the underlying type. If it is not, we'll use a uint16_t bits format and the FP16 library for converting between float and uint16_t.",2024-04-05T17:30:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65135
yi,copybara-service[bot],Prepare move of CUDA compilation functionality,"Prepare move of CUDA compilation functionality Currently PTX to SASS compilation functions are exported by the target `stream_executor/gpu:asm_compiler`, but implemented in `stream_executor/cuda:cuda_asm_compiler`. I'm trying to clean this up by moving everything into `:cuda_asm_compiler`. Since we can't do atomic cross cutting changes between Jaxlib and XLA, I'm only preparing `:cuda_asm_compiler` as an alias for Jaxlib to depend on. In a subsequent change I will move all the code over.",2024-04-05T17:27:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65133
yi,copybara-service[bot],Add the priority_isolation policy to the BatchFunction op to disallow mixing the high priority and low priority tasks,Add the priority_isolation policy to the BatchFunction op to disallow mixing the high priority and low priority tasks,2024-04-05T17:24:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65132
yi,cfasana,Interpreter API (Java) - GpuDelegateV2 support,"Hi, I am trying to run a TFLite model on the GPU of an Android device. According to this documentation, it is possible to use both the `Interpreter API` and the `Native c++ API` to achieve this. At the moment, I am using the following dependencies:  I was able to successfully run my model using the `GPUDelegate` provided by the Java Interpreter API. However, this delegate does not allow to specify inference priority options (`TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY`, `TFLITE_GPU_INFERENCE_PRIORITY_MIN_MEMORY_USAGE`, `TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION`). These options can be specified if the Native C++ API is used given the presence of GpuDelegateV2. However, at the moment I don't see this option in the Interpreter API since there is no class named `GpuDelegateV2`. Is there a way to make use of this new delegate without the need of using the Native C++ API?",2024-04-05T10:11:23Z,stat:awaiting tensorflower type:feature comp:lite TFLiteGpuDelegate Android TF 2.15,closed,0,5,https://github.com/tensorflow/tensorflow/issues/65114,"Hi , The Java Interpreter API currently doesn't have GpuDelegateV2 directly. However if you would like to achieve the faster inference speed you can use `GpuDelegate`  class by setting the `isPrecisionLossAllowed` flag to `true` in the following way as a workaround. But for memory usage and max precision, feature requests will be raised. Thanks for letting us know.  or  Also try with tflite_flutter library which will provide access to GpuDelegateV2 through DART API. Hi , As  mentioned,  `GpuDelegateV2` need to be included in Java Interpreter API with the support of  . Raised a feature request. Thank You","Hi , thanks for the feedback. I will proceed as you suggested while awaiting the Java Interpreter API update.","Hi , can you please take a look a this feature request? Thanks.","Hi,   Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/68 Let us know if you have any questions. Thanks.","Hi , thanks for pointing this out. I will follow the progress as you suggested"
yi,Ikerlandarech,JUCE LNK2001 - LNK2019: UNRESOLVED EXTERNAL SYMBOL," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.11  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hey there and thank you for reading this. I've been trying to work on a VST for my final thesis project which consists of timbre transfer voice to instruments, I've been using JUCE, and all source files are loaded from the Projuicer, the problem I'm facing is those Linker Errors that I think have to do with reading/accessing the Tensorflow files. I have no idea on how to solve it since I tried changing all the ""includes"" so that filepath were accessible and also tried importing all TF into the Projuicer but then it crashes. The framework that I'm working with is DDSPVST from the Magenta Team which uses Tensorflow to perform all the ML operations. The original DDSP code framework is implemented in Python with Tensorflow but for DDSPVST everything is rewritten in C++ using the JUCE framework and Tensorflow Lite. I'm",2024-04-04T20:40:58Z,stat:awaiting response type:build/install TF 2.11,closed,0,2,https://github.com/tensorflow/tensorflow/issues/65082, Could you please try creating a minimal example project outside of your VST that only includes TensorFlow Lite and JUCE to see if the linker errors persist? Please let us know! Thank you!,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],[xla:gpu] Fix the dimension order propagation for bitcast,"[xla:gpu] Fix the dimension order propagation for bitcast If bitcast's destination contains a trivial dimension, it will create an unnecessary trivial fragment after propagation. For example: %p = f32[5,1] parameter(0) %bitcast = f32[5] bitcast(%p) When propagating dimension order from bitcast to p, it creates a fragment of size 1 for %p.",2024-04-04T17:58:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/65069
yi,kimbyungeun,"When performing inference, the error 'DNN library not found.' occurs intermittently."," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version TF 2.11  Custom code Yes  OS platform and distribution Ubuntu 20.04 LTS 5.15.046generic  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 11.3.0 / 8.2.0.53  GPU model and memory NVIDIA A16  Current behavior? When attempting inference, the error 'DNN library is not found.' occurs, leading to inference failure. This error does not occur every time but happens randomly, and sometimes it does not occur at all.  Standalone code to reproduce the issue   Relevant log output ",2024-04-04T04:15:05Z,stat:awaiting response type:bug subtype: ubuntu/linux TF 2.11,closed,0,5,https://github.com/tensorflow/tensorflow/issues/65033,"Hi **** ,  Sorry for the delay, Check your environment variables related to CUDA and cuDNN. Set LD_LIBRARY_PATH to include the directories containing the CUDA and cuDNN libraries.   Run a script to check if tensorflow can detect your GPU and if it's using the correct CUDA/cuDNN versions.   Clearing the tensorflow cache can resolve intermittent issues. You can do this by deleting the '~/.keras' directory.  If the issue persists, consider reinstalling TensorFlow and its dependencies from scratch. Use a virtual environment to isolate the installation. Thank you!","Hi   I am planning to proceed with the following two tasks: * Set the LD_LIBRARY_PATH environment variable in a Docker container. * Downgrade CUDA version from 11.3.0 to 11.2.2.    * https://github.com/googlecolab/colabtools/issues/2600issuecomment1061312120 Additionally, I have a question: * Between CUDA 11.3.1 and CUDA 11.2.2, which direction would be better? Please recommend. Thank you!","Hi  ,  As per documentation. For TF 2.11 compatible version is CUDA 11.2.2. So i can say this compatible version for smooth installation. Thank you!",Are you satisfied with the resolution of your issue? Yes No,"The base image has been changed from `nvidia/cuda:11.3.1cudnn8develubuntu20.04` to `nvidia/cuda:11.2.2cudnn8develubuntu20.04`, and so far the same error has not occurred. It is considered to be resolved. https://hub.docker.com/layers/nvidia/cuda/11.3.1cudnn8develubuntu20.04/images/sha256278b6f43436029c8fa7b4905970457156774178b99509aa753b08547fb721cd9?context=explore https://hub.docker.com/layers/nvidia/cuda/11.2.2cudnn8develubuntu20.04/images/sha2567d4850083e83853f757014e9fb852934907317a4830013591ec71480181232e7?context=explore  👍 "
yi,zo9999,Fix signature of the GetActivationName utility function that is used in the remapper implementation,Here we can pass the input string by const ref in order to avoid the cost of copying the string.,2024-04-03T18:49:34Z,awaiting review ready to pull comp:grappler size:XS,closed,0,4,https://github.com/tensorflow/tensorflow/issues/65010,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!
yi,eltimen,GPU delegate linker errors when building TFLite 2.16.1 for Android with CMake, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version Clang 14.0.6 (Android NDK r25b)  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm trying to build TFLite 2.16.1 for Android using CMake and Android NDK r25b and got the following linker errors related to GPU delegate. It looks like some source files was not added to CMake configuration files.   Standalone code to reproduce the issue   Relevant log output _No response_,2024-04-03T13:48:41Z,stat:awaiting tensorflower type:build/install comp:lite,closed,0,14,https://github.com/tensorflow/tensorflow/issues/64994,"Hi , Enabling GPU support requires additional libraries and dependencies related to OpenCL. This might be the reason for   the error ` tflite::gpu::gl and tflite::gpu::OptionalAndroidHardwareBuffer`.  Please try to add the following to the Android manifest in order to detect GPU delegate.  Also Verify that the NDK version (r25b) is compatible with the TensorFlow Lite version you are trying to build. Thank You","Hi , I'm not using Gradle or Android Studio. I just tried to build `libtensorflowlite.so` for Android using CMake following the documentation and I got the linker errors above. I tried to build it with Bazel as mentioned here and build was successful. According to this page NDK r25b should be compatible with this TFLite version.  According to the documentation, TensorFlow Lite supports both ways to build it for Android: with CMake and with Bazel. Looks like CMake building is broken with enabled GPU delegate in this version.","This looks like a duplicate of https://github.com/tensorflow/tensorflow/issues/65744, investigating.","Hi , I was able to build fine with these steps:  Please note $ANDROID_NDK is the path to my NDK, which is 25b. I am building on MacOS, you didn't specify an OS so that could be an issue. Can you please let us know what OS you are using? Additionally please review my reproduce steps and let me know if I did something different than what you are doing? Thanks.","Hi , I use Ubuntu 22.04. I'm building TFLite from the tag `v2.16.1`, but you are building it from the branch `r2.16`.   Can you try to reproduce this using the release 2.16.1 version?","Hi , I am using a Debian system, and went to that tag, but am using NDK=r25c, I am still not reproducing the error:  It looks like my Clang is different (14.0.7) probably from the different r25c. Can you try with a fresh repo, perhaps fresh NDK, and ensure you follow the same steps exactly? Let me know if somehow that resolves your issue.","Hello , I followed your step exactly, the error does not come from the generating the build file but from actual building with `cmake build . j`, can you verify this is the correct command to build?","Also just double checking, when building with CMake, are the steps in this guide still needed to build GPU delegate?","Hi , I believe you are in the wrong thread, please follow your issue in that thread instead so we don't confuse the issues (Same error but different OS right now), to answer your question  that is the correct command if you are following these directions: https://www.tensorflow.org/lite/guide/build_cmake. That guide is following directions with Bazel, I don't recommend you mix build systems unless you truly understand both and what they are doing. For `cmake` I would follow this: https://www.tensorflow.org/lite/guide/build_cmakeopencl_gpu_delegate. If you want to try Bazel or see if you can complete your workflow with it instead, please feel free to do so.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Hi, . Sorry for the delay. I tried building the TFLite exactly following your steps using NDK r25c, but I'm still reproducing this error. My CMake version is 3.27.7","Hi , It seems we have been miscommunicating slightly :). The original problem didn't include the `cmake build . j` step, so I wasn't doing that  as seen in my previous comment. After doing that step, I was able to reproduce. To recompile to ensure there is no further miscommunication the reproduce steps are as follows:  Hi , can you please take a look? Thanks.","Hi,   Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/69 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
gpt,LarocheC,Feature Request: Integrate different Digital Signal Processing into tf.signal," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Currently, TensorFlow offers a range of signal processing functions under tf.signal, but there's a noticeable gap in advanced audio data augmentation functionalities directly comparable to those available in the PyTorch ecosystem, particularly in the julius library. The julius library provides a suite of efficient DSP operations. These are crucial for building robust audio recognition and processing models, as they allow for the creation of diverse training datasets, enhancing model generalization. Integrating similar functionalities into the TensorFlow ecosystem would greatly benefit researchers and developers working on audiorelated projects by providing them with native tools for complex audio data augmentation. This integration would streamline workflows, allowing for seamless data preprocessing and augmenta",2024-04-03T12:50:37Z,stat:awaiting tensorflower type:feature comp:signal,open,0,3,https://github.com/tensorflow/tensorflow/issues/64989,Triage Notes: I was able to reproduce the issue on tensorflow v2.15 and 2.16. Kindly find the gist of it here.,"  can i contribute here by ,Checking if similar functions exist in tf.signal or libraries like SciPy (scipy.signal). Decide whether to implement in pure TensorFlow or use custom ways for efficiency.  ",Hi   No problem. I have not worked on that so far but feel free to take the idea further :)
yi,eltimen,Cannot build TFLite 2.16.1 for RISC-V due to the bug in cpuinfo (missing sys/hwprobe.h), Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tflite 2.16.1  Custom code Yes  OS platform and distribution Linux  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version GCC 13.2.0 RISCV GNU Compiler Toolchain link: https://github.com/riscvcollab/riscvgnutoolchain/releases/download/2023.11.17/riscv64glibcubuntu20.04gccnightly2023.11.17nightly.tar.gz  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm trying to build TensorFlow Lite 2.16.1 using CMake for RISCV using GCC toolchain for this platform. The build fails with the following error inside `cpuinfo` dependency:   This bug is already fixed in cpuinfo by this commit. Can you bump cpuinfo to the latest commit (or to any version including this fix) in the next TF release?  Standalone code to reproduce the issue   Relevant log output _No response_,2024-04-03T11:43:02Z,stat:awaiting response type:build/install stale comp:lite TF 2.16,closed,0,8,https://github.com/tensorflow/tensorflow/issues/64987,"Hi , The commit  you have  provided for cpuinfo was merged under pytorch  repo. But under TensorFlow repo (tensorflow/lite/tools/cmake/modules/cpuinfo.cmake), a PR  for `cpuinfo.cmake` still under open. Once it is reviewed and merged,  it will be updated on the next coming release. Thank You","Hi , Thank you, I hadn't noticed that a PR to fix this had already been created.","Hi , Thank you for the response. We will update you once the PR CC(update cpuinfo) is merged. Also please keep  track of the same PR . Thank You","hi All!  tensorflow 2.16.1  tflite build does not generate a static libraries in **/clogbuild**. I see that the clogbuild in the version r2.10. Do you have an any idea how can i fix it? error output: gmake[2]: *** No rule to make target '../../tflite_build/_deps/clogbuild/libclog.a', needed by 'testapp/testapp'.  Stop.","Hi NominBa, Could you please open a new issue. Thank You",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],Add ODML e2e coverage test for `jax.lax.add`.,Add ODML e2e coverage test for `jax.lax.add`.,2024-04-03T11:02:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64986
rag,copybara-service[bot],Support lifting of same shape bias for `stablehlo.convolution`.,"Support lifting of same shape bias for `stablehlo.convolution`. Although it is common to write models where 1D constant would be broadcasted, it is possible to explicitly give bias with the desired shape. The examples in `odml_coverage_test` are such. Handle such cases where the bias has the same shape as target accumulation. Additionally, add `FindOperandType` for finding an operand of specific type and respective tests.",2024-04-03T10:21:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64983
yi,dieterd-sentea,No optimized fast float implementation for `Logistic`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.15  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Commit https://gitlab.com/libeigen/eigen//commit/a30ecb7221a46824b85cad5f9016efe6e5871d69 of the thirdparty Eigen library disabled the fast float implementation of the `Logistic` (or sigmoid if you like) operator for most (all?) Tensorflow Lite backends. The fallout of this is more severe than just the speed impact: relying on overflow behavior of `exp(x)` for large `x`, as used in the generic fallback implementation, triggers a bug on our embedded platform, causing glitches where `logistic(x)` in some cases becomes 0 instead of 1 for large `x` (around 88.9). I would propose to bump the Eigen dependency if this gets fixed upstream (see also https://gitlab.com/libeigen/eigen//merge_requests/1576), as this effectively fixes the issue.  Standalone code to reproduce the issue   Relevant log output _No response_",2024-04-03T09:23:27Z,type:bug comp:lite comp:ops TF 2.15,closed,0,9,https://github.com/tensorflow/tensorflow/issues/64981,sentea Could you please update your build process to use the new Eigen version and let us know? Thank you!,"Hi , I assume that TFLite built with the newest Eigen version, with https://gitlab.com/libeigen/eigen//merge_requests/1576 now merged, does not exhibit the issue. The (arguably confusing) workaround that we use at the moment, until the Eigen dependency in this repository gets bumped, is the following patch:  Of course, this abuses the fact that `EIGEN_GPU_CC` is only used to enable the fast float logistic implementation again in the buggy Eigen versions (and is used nowhere else). In the newest Eigen version, this workaround is not needed anymore, and is in fact undesirable (in case Eigen want to refactor `EIGEN_CPUCC` to `EIGEN_CPU_CC` at some point, to give an example).",Correction on previous message: I have not explicitly tested if latest Eigen master (which might contain other unrelated changes) is working for us. I'll find out and let you know.,"I can confirm that we experience no issues on tfnightly (a700cea6b22840a3656f951f16603657685e7d89) with the following patch, which updates Eigen to latest master https://gitlab.com/libeigen/eigen//commit/b2c9ba2beef4b5fd61513d73911c678e93c8dd9d:  Funnily enough, the overflow issue that we observed does not appear on tfnightly (but does occur on v2.14.0 and v2.15.0), but this is orthogonal to this issue which is about the fast float implementation for `Logistic`.","So I've investigated this a bit further and we observe a nice speedup with the above patch (up to 6% for a certain model that contains a sigmoid layer). Note btw that we do not use XNNPACK delegate, i.e., we configure the project with `cmake DTFLITE_ENABLE_XNNPACK=OFF`. This is important to note, as otherwise float32 logistic goes through XNNPACK instead of Eigen, as can be seen here: https://github.com/tensorflow/tensorflow/blob/ade45358646f2c7ed2ce6792d0234d83ad2dca3f/tensorflow/lite/kernels/activations.ccL1130","Hi sentea, can we say that this is resolved now on the master branch at least? Let me know if you disagree or if there is any other action needed from our side. Thanks.", LGTM,So indeed 476aaacc4298e4f5a772fa585ce57604e6522f6e resolves this. Thanks!,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[xla][gpu] Implement pipelined-p2p-rewriter.,"[xla][gpu] Implement pipelinedp2prewriter. This pass rewrite pipelined pointtopoint communication by rotating the SendDone and RecvDone operations in a whilebody to the beginning of the next iteration. The SendDone and RecvDone operations for the last iteration are moved to the whileop calling computation, after the whileop. Add the pass to the GPU postscheduler pipeline. This is another approach to achieve the code pattern to pipeline two SendRecv chains decomposed from a collectivepermute with a sourcetarget pair cycle for performance. The pipelined SendRecv pattern puts SendDone and RecvDone before Send and Recv in the whilebody, and if we generate such code pattern too early in the GPU compilation pipeline, copyinsertion may generate copies of Send causing Send and SendDone with different buffers and thus correctness problem.",2024-04-02T23:42:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64956
rag,copybara-service[bot],PR #11122: [ROCm] Fix RCCL hang on rocm5.7,PR CC(Building Tensorflow from source with XLA enabled and without GPU): [ROCm] Fix RCCL hang on rocm5.7 Imported from GitHub PR https://github.com/openxla/xla/pull/11122 Copybara import of the project:  94f7dc85f6ab07343ddb565b0aac0ed908fd66b4 by Dragan Mladjenovic : [ROCm] Allow ncclCommInitRankConfig on rocm5.7  2d196b645ccac046309d44f747d42777f00f1c9b by Dragan Mladjenovic : [ROCm] Fix sporadic hangs in CommInitRanks Pointer to comm_handle must stay valid untill GroupEnd call on pre 2.18 nccl. Only after that comm_handle contains a valid value. Enforce this by using std::vector instead of temp local. Merging this change closes CC(Building Tensorflow from source with XLA enabled and without GPU) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/11122 from ROCm:5.7rcclhang 2d196b645ccac046309d44f747d42777f00f1c9b,2024-04-02T18:18:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64942
llm,copybara-service[bot],Convert unquantized XlaCallModule to func.call,Convert unquantized XlaCallModule to func.call Some composite ops may be unquantized due to not supported or selective quantization. We still want to convert them to func.call. This CL converts any unquantized XlaCallModule ops to func.call. This covers both srq and weightonly cases.,2024-04-02T17:41:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64937
yi,MDENDAN,TypeError: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 1.14  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.6  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cudatoolkit=10.1  GPU model and memory Nvidia rtx 3050 4gb  Current behavior? I created a new env using conda with python version 3.6 using command: conda create n myenv c condaforge strictchannelpriority python=3.6 Next I activate the environment and install tensorflowgpu=1.14 and cudatoolkit=10.0 On trying to import tensorflow i am getting meta class conflict error C:\Users\MD IMRAN\miniconda3\envs\aai\lib\sitepackages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.   _np_qint8 = np.dtype([(""qint8"", np.int8, 1)]) C:\Users\MD IMRAN\miniconda3\envs\aai\lib\sitepackages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (t",2024-04-02T16:54:03Z,stat:awaiting response type:bug stale comp:apis TF 1.14,closed,1,5,https://github.com/tensorflow/tensorflow/issues/64926, TF v1.x is not actively supported so we request you to kindly upgrade to the latest TF version. Please follow the migration document here. Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,pip install abslpy==1.1.0
rag,copybara-service[bot],#shlo_ref Add 4 bit with 8 bit storage support.,shlo_ref Add 4 bit with 8 bit storage support.,2024-04-02T16:40:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64925
yi,copybara-service[bot],Add F16 wrapper type,"Add F16 wrapper type _Float16 is not available on the GCC until version 13, which is very recent. This change transforms the F16 into a wrapper type that will always be used regardless of compiler support. It defines conversion and arithmetic operators. If _Float16 is available, that will be used as the underlying type. If it is not, we'll use a uint16_t bits format and the FP16 library for converting between float and uint16_t.",2024-04-02T15:50:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64924
yi,MinaBabahaji-ML,tf.linalg.normalize generates wrong output in tflite version running on mobile GPU," 1. System information  Android S10 snapdragon 855  TensorFlow built from source  TensorFlow 2.15.0  2. Issue Summary: When running a TensorFlow Lite model (`tf.linalg.normalize `layer) on an Android 12 device with TensorFlow version 2.15 built from source, using the OpenCL delegate, I am experiencing wrong outputs (This is not hapenning for alll the inputs, but some inputs). Model is converted with `fp16` quantization. This issue can't be reproduced with the XNNPACK delegate. The model is shared in the github repository, but I could regenerate the issue with the following dummy model:   2. Code I did not encounter any issues in converting the model, but when running the converted model on the phone with the OpenCL delegate, the output has a big distance from XNNPACK delegate and keras model. The code, model and steps to reproduce the problem can be found in the following GitHub repository: GitHub Repository Link  3. logs: ",2024-04-02T15:27:57Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter comp:lite-xnnpack TF 2.15,closed,0,7,https://github.com/tensorflow/tensorflow/issues/64922,"Hi ML, I followed your directions exactly with a fresh git clone, on this step:  I get this error:  I am using an emulator but I do not believe this issue is related to that, perhaps you are more familiar with your code so if you know how to fix this please let me know.","Thank you pkgoogle. Could you please confirm that after cloning the repository, you checked out the right branch? (git checkout normalize_layer). It seems that you are on main branch","Hi ML, Thank you... I have openCL installed on my mac and my AVD has graphics set to Automatic and I run into this issue:  This is likely an emulator problem, Hi , can you please take a look? Thanks.","Hi ML, we have a new workflow with AIEdgeTorch, you can find more information here: googleblog. Perhaps this issue does not exhibit itself if you use this workflow. I have actually created a simple script for converting your model here:  If you want to, you can actually try visualizing the result in modelexplorer as well. Please try them out and let us know if this resolves your issue. If you still need further help, feel free to open a new issue at the respective repo.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,TimiOkusi,Unknown operation used in model that is not explicitly declared," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.15 and 1.8  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am working on a uni project, a fall detection app built using Android studio and machine learning model. The model uses time series data gotten from mobile sensors. I have trained the model using model architecture LSTM > TimeDistributedLayer > Dense layer > Dense layer (Output) I did this initially on colab which uses tensorflow version 2.15, but when I tried to export it as a tensorflow lite model I was getting some errors during the conversion suggesting me to use SelectOps or flex ops which are because an operation is not supported by tensorflow lite’s standard operations. Doing that and importing into Android studio, I couldn’t initialize the model without getting error saying “Unsupported data type 14 in tensor” which was probably as a result of Android not having the package to run the added operation. I",2024-04-02T15:12:30Z,stat:awaiting response type:bug stale comp:ops 1.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/64920,"Hi  , The 1.x versions are not supported now. Please provide the context and error log for Tf2.x versions.  As I understand the reported error is with TF1.8 version, and if so we can't support it. Please test with Tf2.x versions and preferably with latest versions and report the problems faced with all the required details for debug. Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
gpt,copybara-service[bot],PR #10965: [GPU] Make xla_gpu_enable_nccl_per_stream_comms false by default,"PR CC(Correct the learning rate as per the code snippet): [GPU] Make xla_gpu_enable_nccl_per_stream_comms false by default Imported from GitHub PR https://github.com/openxla/xla/pull/10965 In CC(Fix typos in high performance models docs) I added a flag to toggle between perstream communicators. The default option was true to preserve the current behavior. While perstream comms can provide a speedup at the cost of additional memory, it doesn't seem to help for many common workloads which I have tested. I am proposing that we set the default value to false to reduce the amount of GPU memory used by XLA. The table below shows how the total GPU memory usage changes (absolute value) and how the steps/sec changes (relative value) by changing xla_gpu_enable_nccl_per_stream_comms **from true to false**. In all of these cases, we use less memory and the performance stays in general within 1%.  Test settings * T5X container: ghcr.io/nvidia/jax:t5x20240315 * PAX container: ghcr.io/nvidia/jax:pax20240315 * Performed using DGXH100 * NCCL_NVLS_ENABLE=1 * XLA_PYTHON_CLIENT_MEM_FRACTION=0.8 (except paxml GPT3pp which uses 0.7 because it goes OOM otherwise when enable_nccl_per_stream_comms is true) Copybara import of th",2024-04-02T07:33:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64883
yi,copybara-service[bot],[SavedModelBundleLite] Avoid copying the GraphDef during the load path.,"[SavedModelBundleLite] Avoid copying the GraphDef during the load path. Previously, we reused the ""legacy"" SavedModelBundle loading path, then discarded the unused MetaGraphDef. An old TODO called out the opportunity to avoid copying the MetaGraphDef's GraphDef, and this change does that.",2024-04-01T20:24:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64858
yi,copybara-service[bot],[xla][gpu] Add tuple-simplifier to post-scheduling copy-insertion pipeline.,[xla][gpu] Add tuplesimplifier to postscheduling copyinsertion pipeline. The copyinsertion pass invokes tuplesimplifier in a similar way.,2024-04-01T02:12:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/64829
yi,Ikerlandarech,JUCE LNK2001 - LNK2019: UNRESOLVED EXTERNAL SYMBOL,"Hey there and thank you for reading this. I've been trying to work on a VST for my final thesis project which consists on timbre transfer voice to instruments, I've been using JUCE and all source files are loaded from the Projuicer, the problem I'm facing is those Linker Errors that I think have to do with reading/accessing the Tensorflow files. I have no idea on how to solve it since I tried changing all the ""includes"" so that filepath were accessible and also tried importing all TF into the Projuicer but then it crashes. If anyone can help with this I would really appreciate some help, thank u so very much in advance and have a beautiful day whoever is reading this :) !image",2024-03-31T16:17:35Z,stat:awaiting response comp:lite,closed,0,1,https://github.com/tensorflow/tensorflow/issues/64818,"Hi , Upon inspecting the screenshot, these errors indicate that the code is referencing functions or variables that are not defined or available in the linked libraries.  Check library versions for compatibility, make sure necessary headers are included.  In order to reproduce the issue, please fill the template and elaborate more the issue and the environment which you are working.  Thank You"
yi,paya-alavi77,efficient_serving.ipynb has some bugs, Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi. I was trying to run this guide on tensorflow site and got error when tried to create the Movielensmodel. I didn't change any code and just ran it on a google colab server.  The code that I'm getting error is on 10th cell: `model = MovielensModel()` which is supposed to create the retrieval model however I get below error I tried to work it out but I couldn't fix it. I found out that the installation of scann brings this problem. If we don't install scann the project will run just fine.:  ValueError                                Traceback (most recent call last) [](https://localhost:8080/) in () > 1 model = MovielensModel()       2 model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1)) 6 frames /usr/local/lib/python3.10/distpackages/keras/src/backend/common/variables.py in standardize,2024-03-31T16:02:35Z,type:docs-bug stat:awaiting response type:bug stale TF 2.16,closed,0,5,https://github.com/tensorflow/tensorflow/issues/64817, I was able to replicate the issue reported here. Thank you!,"Hi alavi77 , This needs to be addressed at recommenders repo. Please file an issue there. Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,theraghavjuneja,tf Normalization ," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Windows  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Referring to tf.keras.layers.Normalization In the documentation, I read that adaptdata is used so that tensorflow can use the adapt data for computing means and variance corresponding to each axes. And axes can be specified as None if you want to compute the overall mean and variance of adapt data. It was mentioned either u can use the adapt data or explicitly specify mean and variance . Explicitly specifiying mean and variance doesnt work until or unless data is of the format np.array([[6.], [7.], [3.],[4.],[5.]] But if it is of np.array([[6,7,3,4,5]])/ A little difference in shape, This explicit will not work Although It used to work in the back.  Standalone code to reproduce the issue   Relevant log output ",2024-03-31T03:52:32Z,type:support,closed,0,2,https://github.com/tensorflow/tensorflow/issues/64814,Are you satisfied with the resolution of your issue? Yes No,Oh sorry! That was my fault
yi,janeon,Generating Images with BigGAN -- demo in docs no longer compatible with tensorflow (hub?)," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 1.7.0  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.711  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When first running demo in docs, the following bug was encountered when running cell under section: Load a BigGAN generator module from TF Hub `AttributeError: module 'tensorflow_hub' has no attribute 'Module'` Following Johnny Tang's solution to change `hub.Module` to `hub.load`, the below error is met: `AttributeError: 'AutoTrackable' object has no attribute 'get_input_info_dict'` This persists after trying multiple versions of tf, tensorflow_hub as well as Python  Standalone code to reproduce the issue   Relevant log output ",2024-03-30T19:45:58Z,type:docs-bug stat:awaiting response type:bug stale comp:model,closed,0,4,https://github.com/tensorflow/tensorflow/issues/64810,", The model Generating Images with BigGAN which you are trying to execute is compatible with the 1.x version. There are multiple changes that happened from 1.x to 2.x and also this is related to tfhub.  Please try to connect with the tfhub repo for the quick solution.  https://github.com/tensorflow/hub/issues Thank you! ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,Jacob-Bishop,Tf_nightly fails to install in docker," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tfnightly  Custom code No  OS platform and distribution Linux  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory Nvidia 3090 ti  Current behavior? Actual Behavior: When trying to install tf_nightly in a docker container, pip starts iterating backwards in time, downloading previous days' tensorflow nightly versions. I stopped the process after it made it back to March 9th... Expected behavior: tf_nightly installs or produces an error Note that I can install tf_nightly_cpu just fine. IIRC, GPUs are not available during buildtime for docker by default, so maybe the error is related to the build environment silently failing a check for some gpurelated support? If so, (1) this shouldn't happen silently and (2) it's unlikely to be something that downloading a previous version would fix?  Standalone code to reproduce the issue   Relevant log output _No response_",2024-03-30T18:23:47Z,stat:awaiting tensorflower type:build/install subtype: ubuntu/linux,closed,0,3,https://github.com/tensorflow/tensorflow/issues/64807,It looks like it would be due to the nightly getting a version flagged as a release version when we switched over how we built the pip package for 2.16's release.  We will need to fix that but in the mean time you should be able to get around this by changing the install to  `pip install tfnightly pre`,I have pulled the bad release from the tfnightly repo.  The replication code provided now correctly pulls the latest nightly for me on the first attempt.,Are you satisfied with the resolution of your issue? Yes No
yi,ripjohnbrown1859,tensorflow not utilizing gpu memory and stating limit is 137gb," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.16.1  Custom code Yes  OS platform and distribution wsl ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 8.6.0  GPU model and memory sli titan x maxwell  Current behavior? i have 2 titan x maxwells and am trying to run a CNN on my machine in wsl2, however when I try to run it i get the attached error, which seems to indicate tensorflow is not using any gpu memory and then throwing an error. it runs out of memory while converting the training input tensor  Standalone code to reproduce the issue   Relevant log output ",2024-03-29T21:34:04Z,stat:awaiting response type:support stale TF 2.16,closed,0,6,https://github.com/tensorflow/tensorflow/issues/64764,", Could you please provide the steps you have followed to install the tensorflow 2.16 and also provide the CUDA, cudNN, Bazel, Environment where you are trying which helps us to analyse the issue in an effective way. Thank you!","i managed to fix this specific problem by processing the data on the cpu and under 'with tf.device('/CPU:0'):' and training under 'with strategy'. now i have a problem where every other epoch reports a bunch of rendezvous errors, skips abunch of data, and gives a val accuracy of 1 and an increasingly high loss. also my epochs are reporting greater than 1 accuracy. Should i open a new issue?",", Glad the GPU issue was resolved. For the val loss, If the validation loss (error) is going to increase so means overfitting. You must set the number of epochs as high as possible and avoid the overfitting and terminate training based on the error rates. . As long as it keeps dropping training should continue. Till model start to converge at nth epochs. Indeed it should converge quite well to a low val_loss. Also please take a look at this references. https://discuss.tensorflow.org/t/whydoesmyvalidationlossincreasebutvalidationaccuracyperfectlymatchestrainingaccuracy/4283 ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Change include order in `ml_dtypes.cc` to prevent errors.,"Change include order in `ml_dtypes.cc` to prevent errors. Trying to prevent `error: ""Using deprecated NumPy API, disable it with define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION""`",2024-03-29T17:29:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64749
rag,copybara-service[bot],Add a Resource for KV Cache buffer storage,Add a Resource for KV Cache buffer storage,2024-03-28T22:56:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64709
yi,suyash-narain,TfLite C/C++ shared library via Bazel,"Hi, I am trying to build Tensorflowlite v2.14.0 using Bazel and I do obtain libtensorflowlite.so shared library. I am using Ubuntu OS 22.04 I want to know is it possible for the generated libtensorflowlite.so to contain the c_api symbols as well? Whenever i build tensorflowlite using bazel, i need to specifically build libtensorflowlite.so and libtensorflowlite_c.so to use c_api.  I though libtensorflowlite.so should contain the c_api symbols as well. Is it possible? Is there a way via bazel to include c_api symbols in libtensorflowlite.so instead of generating a separate libtensorflowlite_c.so?  I think the build using CMAKE generates libtensorflowlite.so containing c_api symbols as well. How to do it via bazel? thanks",2024-03-28T19:39:14Z,stat:awaiting response type:support comp:lite comp:runtime,closed,0,5,https://github.com/tensorflow/tensorflow/issues/64693,narain Could you please refer to the official TensorFlow Lite documentation for complete Bazel build and let us know if it helps? Thank you!,"Hi  , the documentation link you mentioned points to cmake build, not bazel. How do i build a combined tf .so file containing both c++ and c symbols using bazel?",narain Bazel doesn't directly support creating a single shared library (.so) file containing both C++ and C symbols. To achieve this you can try creating separate libraries for your C++ and C code and linking them together during your main program's build process. Thank you!,thanks,Are you satisfied with the resolution of your issue? Yes No
yi,Wasim04,Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed. Aborted (core dumped)," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15.0.post1  Custom code Yes  OS platform and distribution Rocky Linux 8.9  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 8.9.2.26  GPU model and memory  NVIDIA A100SXM480GB   Current behavior? I am trying to run a simple denoising autoencoder. my training data and label data are 900 samples of healpy maps with nside 64 resolution, loaded as numpy array. After normalising the maps, I used tf.data.Dataset.from_tensor_slices, to create dataset. when I used random noise to create these maps and ran on jupyter notebook, although took ages to initiate training after doing model.fit(), but it did run and produced some result. knowing that the model works, I tried to run on GPU with real data. this is where the issue started.  it shows the following error: Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed. Aborted (core dumped), and the process stops.   Standalone code to reproduce the issue   Relevant log output ",2024-03-28T16:15:23Z,stat:awaiting tensorflower type:bug comp:data TF 2.15,open,4,4,https://github.com/tensorflow/tensorflow/issues/64681,", I am trying to execute the mentioned code with the GPU and CPU environments and the time taken to execute the code is more than expected. Could you please allow some duration to analyse the issue and provide the update on the same. Thank you!",  Thanks for your response. Please take your time. ,"  Hi, just wondering if you had a chance to look into this issue?",Same issue here but not with tensorflow  the error seem to come from jax==0.4.30
agent,LongZE666,`tf.raw_ops.AvgPoolGrad`: Heap buffer overflow, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version gcc version 11.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? tf.raw_ops.AvgPoolGrad can lead to heap buffer overflow.  Standalone code to reproduce the issue   Relevant log output  ```,2024-03-28T03:17:39Z,stat:awaiting response type:bug stale comp:ops TF 2.16,closed,0,4,https://github.com/tensorflow/tensorflow/issues/64642,", I request you to take a look at this issue where a similar issue has been proposed and it is still open.Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,sangyo1,interpreter->Invoke(); Segmentation fault in c++ inference,"!image !image I am trying to deploy own object detection model in c++. Once I run the code I am kept getting segmentation fault. seems like `cerr typed_input_tensor(0), image.data, image.total() * image.elemSize())Invoke() This is what I am getting on debugger ` CC(Installation over pip fails to import with protobuf 2.6.1)  0x00007ffff7cda369 in tflite::optimized_ops::Conv(tflite::ConvParams const&, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, int const*, tflite::RuntimeShape const&, unsigned char*, tflite::RuntimeShape const&, unsigned char*, tflite::CpuBackendContext*) ()` ` CC(Java interface)  0x00007ffff7cdabc9 in void tflite::ops::builtin::conv::EvalQuantized(TfLiteContext*, TfLiteNode*, TfLiteConvParams*, tflite::ops::builtin::conv::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) () ` I am getting stuck and I am not sure how to fix this problem",2024-03-27T18:54:07Z,stat:awaiting response stale comp:lite,closed,0,3,https://github.com/tensorflow/tensorflow/issues/64608,"Hi , can you please share the file or code snippets/text instead, Also please include your steps to compile and execute. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,shabnamso,Error with loading a .pb model for prediction: Op type not registered 'DecodeProtoSparseV4'," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version latest version  Custom code Yes  OS platform and distribution Windows  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Encountered an issue while trying to load a trained .pb model using tf.saved_model.load. I'm receiving a RuntimeError stating that the Op type not registered 'DecodeProtoSparseV4'. Here are the details: Error Message: RuntimeError: Op type not registered 'DecodeProtoSparseV4' in binary running on [Machine's Name]. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.  Standalone code to reproduce the issue   Relevant log output _No response_",2024-03-26T22:20:28Z,stat:awaiting response type:support stale type:performance,closed,0,8,https://github.com/tensorflow/tensorflow/issues/64553,"Hi  , Actually the mentioned Op DecodeProtoSparseV4 is not exists in latest Official TF binaries. It might be possible that this particular Op is a user created Op. Tensorflow provides the flexibility to users to create their own Op and gradient for same. You can refer this guide on same. I believe you are trying to reload a model that is trained on custom built TF binary with latest Official release. May be you have to use same binary wheel upon which the model was trained initially. Unfortunately we can't provide support for custom ops of users. Thanks!",Hi  The model was trained by Vertex AI. I downloaded the model to my local machine.,"Hi  , Could you please share the model details. You mean the model is a pretrained model of Vertex AI ?",Hi   I trained the model on vertex AI using my data and the files and folders of the model were downloaded to local machine. The saved_model.pb was automatically in the folder /predict/001. ,Hi  . Have you used any pretrained/readily available model from VertexAI.If so which one? VertexAI can host user built models also if I am not wrong? ,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,Altrastorique,Problem with Tensorflow 2.16.1 and gpu," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.12 and also tried with 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory RTX 3070  Current behavior? Hello, I was trying to make TensorFlow work with my GPU, but the CUDA installation crashed every time. So I tried using Anaconda, but they don't have CUDA version 12.3 and CuDNN version 8.9. So I succeeded using the Anaconda PowerShell and downloading TensorFlowGPU 2.10.  I heard that TensorFlowGPU is an old version we should not use. Does someone have any idea which could help me? Thank you very much!  Standalone code to reproduce the issue   Relevant log output ",2024-03-26T17:10:15Z,stat:awaiting response type:build/install stale comp:gpu TF 2.16,closed,0,13,https://github.com/tensorflow/tensorflow/issues/64525,"Hi  , After creating Conda environment please use pip to install TF package. For GPU package please use pip install `tensorflow[andcuda]` that will install all the required compatible cuda/cudnn packages. Please ensure compatible cuda driver installed manually. Please check and come back if still having problem. Thanks.","Okay, but when I run this in the annaconda power shell I have this: `WARNING: tensorflow 2.13.1 does not provide the extra 'andcuda'` So when I run this code:   I have: `[ ]` The same happen when I run it in my terminal.","> Okay, but when I run this in the annaconda power shell I have this: `WARNING: tensorflow 2.13.1 does not provide the extra 'andcuda'` The 'andcuda' package introduced in TF2.14 onwards. You are trying for Tf2.13v as warning states.Please upgrade to higher version.",The tensorflow version >2.13 are not available on the anaconda powershell. So I had to use the terminal. But I still have this error when installing tensorflow[andcuda]  When trying the 2.14 version I had this: ,"Hi  , If you are using Windows OS cuda package is not available for windows from TF2.11 onwards. Please refer the caution on same in documentation. If you want to use GPU then you need to install WSL2. Please refer the instructions for wsl2 here.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"I had same troble   Could not find a version that satisfies the requirement nvidiancclcu12==2.19.3; extra == ""andcuda"" (from tensorflow[andcuda]) (from versions: 0.0.1.dev5)","> I had same troble >  > Could not find a version that satisfies the requirement nvidiancclcu12==2.19.3; extra == ""andcuda"" (from tensorflow[andcuda]) (from versions: 0.0.1.dev5) same here as well. WSL2 is installed with all the prereqs for 2.16.1  https://www.tensorflow.org/install/sourcegpu","same issue running pip install ""tensorflow[andcuda]==2.16.1""  and see  INFO: pip is looking at multiple versions of tensorflow[andcuda] to determine which version is compatible with other requirements. This could take a while. ERROR: Could not find a version that satisfies the requirement nvidiancclcu12==2.19.3; extra == ""andcuda"" (from tensorflow[andcuda]) (from versions: 0.0.1.dev5) ERROR: No matching distribution found for nvidiancclcu12==2.19.3; extra == ""andcuda""",Has someone been able to solve this issue?,"> same issue running pip install ""tensorflow[andcuda]==2.16.1"" and see >  > INFO: pip is looking at multiple versions of tensorflow[andcuda] to determine which version is compatible with other requirements. This could take a while. ERROR: Could not find a version that satisfies the requirement nvidiancclcu12==2.19.3; extra == ""andcuda"" (from tensorflow[andcuda]) (from versions: 0.0.1.dev5) ERROR: No matching distribution found for nvidiancclcu12==2.19.3; extra == ""andcuda"" I get the same error."
yi,copybara-service[bot],PR #10376: Fix expm1 inaccuracies on complex inputs with small absolute values. Add Cosm1.,PR CC(Lack support of qint32 in tf.nn.tanh): Fix expm1 inaccuracies on complex inputs with small absolute values. Add Cosm1. Imported from GitHub PR https://github.com/openxla/xla/pull/10376 As in the title. Tests and improvement reports are in https://github.com/google/jax/pull/20144. Accuracy tests are enabled in https://github.com/google/jax/pull/20436 Copybara import of the project:  42e222a436787c52adf453c8c0c39125b010e2b2 by Pearu Peterson : Fix expm1 inaccuracies on complex inputs with small absolute values. Add Cosm1. Fix expm1(x+yi) when x is large and y is zero. Merging this change closes CC(Lack support of qint32 in tf.nn.tanh) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10376 from pearu:pearu/expm1 42e222a436787c52adf453c8c0c39125b010e2b2,2024-03-26T16:43:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64523
yi,copybara-service[bot],Add basic support for folding constant RTVars,Add basic support for folding constant RTVars This allows turning dynamic HLO operands into a index expressions when the HLO yields some constant value. As a first step this is adding support for the HLO ops `constant` and `iota`.,2024-03-26T15:09:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64517
yi,asapsmc,protobuf fatal error in Tensorflow 2.16.1 on macos M1," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.16.1  Custom code No  OS platform and distribution macOS Sonoma 14.4  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm trying to run a test snippet, and I get protobuf FATAL error `[libprotobuf FATAL google/protobuf/message_lite.cc:353] CHECK failed: target + size == res:  libc++abi: terminating due to uncaught exception of type google::protobuf::FatalException: CHECK failed: target + size == res: `  Standalone code to reproduce the issue   Relevant log output ",2024-03-26T12:58:34Z,stat:awaiting response type:bug stale subtype:macOS TF 2.16,closed,0,10,https://github.com/tensorflow/tensorflow/issues/64507,", Thank you for the issue. Could you please provide the steps you have followed to install the tensorflow 2.16.1 on macos and allow some time to debug the issue. Thank you!","I'm using miniforge, so it was `conda install c apple tensorflowdeps`, then activate and then `python m pip install tensorflow`followed by `python m pip install tensorflowmetal`.","Hi, I understand that you are using GPU to build your model. This is an issue with the `tensorflowmetal` library, as it only supports `tensorflow` up to version 2.14. Therefore, it seems that we need to wait for a new version of the `tensorflowmetal` library. For now, if you want to use `tensorFlow` >2.14, you will need to uninstall` tensorflowmetal` or refrain from using the GPU.",Thanks for your help. Now I define tensorflow==2.14 and everything works. ,", AFAIK the version of tensorflowmetal==1.1.0 must be along with tensorflowmacos==2.14.0. So, If you use ""pip install tensorflowmacos"" alone. You will get 2.15.0 or the latest. So you must install the version which tensorflowmetal is supported. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"Thank you for this post, I also ran into the same issue! As another source posted, the tensorflowmetal compatibility listing is here: https://pypi.org/project/tensorflowmetal/ After I outright deleted my virtual environment and started over, here are the commands I ran:  I then ran the test code shown on https://developer.apple.com/metal/tensorflowplugin/ and the run was successful! On my Macbook epoch 1 took 41s to run, epochs 25 each took ~32s. For comparison, in Google Colab CPU only mode, each epoch was gonna take WAAAAAAAY longer than that.",> 感谢这篇文章，我也遇到了同样的问题！正如另一个来源所发布的那样，tensorflowmetal 兼容性列表在这里：https ://pypi.org/project/tensorflowmetal/ >  > 在我彻底删除虚拟环境并重新开始之后，我运行了以下命令： >  >  >  > 然后我运行了https://developer.apple.com/metal/tensorflowplugin/上显示的测试代码，运行成功！在我的 Macbook 上，epoch 1 运行了 41 秒，epoch 25 每次运行大约 32 秒。相比之下，在 Google Colab 仅 CPU 模式下，每个 epoch 都要花费更长的时间。 使用 pip install tensorflowmacos==2.14 似乎更有效
rag,copybara-service[bot],[XLA:GPU] Expand test coverage for SymbolicTile derivation.,"[XLA:GPU] Expand test coverage for SymbolicTile derivation. Added tests to cover failure paths for mod, floordiv, and non0 offset across untiled dimensions. Also contains some cosmetic/style changes.",2024-03-26T10:03:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64499
yi,copybara-service[bot],[xla] hlo_computation: compact instructions' vector on Cleanup(),"[xla] hlo_computation: compact instructions' vector on Cleanup() tl;dr: this gives a 1.26x compilation time speedup for a large, dense model in XLA:GPU. The largest perf leaf seen in profiles of a large, dense model is related to computing the post order. Surprisingly, it is not the DFS itself what's most expensive; rather, most of the time is spent on scanning through HloComputation::Instructions() to identify DFS roots. The reason this scan becomes expensive as instructions are removed is that the vector holding HloInstructionInfo (introduced in cl/600130708  https://github.com/openxla/xla/commit/247280ab727) is not shrunk as it flows through the pipeline, making us having to walk through many deleted ""tombstone"" entries. Here is the histogram of  of tombstones encountered during post order computations for this model:  To ameliorate this, this CL shrinks the vector periodically, so far only between passes. This is done by running compaction on the vector during HloComputation::Cleanup(), which is called after every pass. Note that the removal operation must be stable because some users depend on it  I have added comments to the code about this because it took me some time to realize the extent of our",2024-03-26T04:45:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64473
yi,PowerToThePeople111,Saving a model defined by model subclassing can not be saved," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.15.1  Custom code No  OS platform and distribution Mac 14.2.1   Mobile device _No response_  Python version 3.10.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hey all, _Remark:_  I am using simple CPU computations and installed tf using pip (so i did not build it myself). Therefor, I did not provide any Cuda version. _The problem I am facing:_  I am currently trying to save a model which was created by subclassing which throws me an error. (see below) This is the code for the definition of the model ... 1. first using function API 2. secondly using model subclassing The first one runs fine, the second throws an error during the last step where I try to save the model. As you can see, I called the model with the data before saving it (just as the error message suggests), but that still does not help. It seems to me that this is a bug, but maybe I have also missed something important.   Thanks for looking into this in advance. best,  Standalone code to reproduce the issue The code is in ",2024-03-25T14:50:28Z,type:bug comp:apis TF 2.15,closed,0,5,https://github.com/tensorflow/tensorflow/issues/64415, Please refer to the TensorFlow documentation for the latest recommendations on saving subclassed models: https://www.tensorflow.org/tutorials/keras/save_and_load and kindly provide the access to your notebook. Thank you! ,"Hi sushreebarsa, thank you for your reply. You should now be able to access the notebook. Also I put the whole code neccessary below the link to it. I read the documentation again and was lead by the following parts: > You can switch to the SavedModel format by: >  > Passing save_format='tf' to save() > Passing a filename without an extension >  > ... >  > *Custom objects (for example, subclassed models or layers) require special attention when saving and loading. Refer to the Saving custom objects section below. >  > ... >  So for me it seemed that the above is basically everything i need to do since later on in the ""Custom objects""section you can read at the very top: > If you are using the SavedModel format, you can skip this section.  So actually I should be able to store the model by using `save()` with `save_format 'tf'` and providing a filename without ending, which is what I did: Neither `dp2.save(""delteme"", save_format=""tf"") ` nor `dp2.save(""delteme"") ` did the trick for me and result in the error message i posted in my initial statement. Did I miss something here?","Hi  , I don't think the problem in saving is actually coming from model subclassing. Most probably, it comes from passing `df` as argument to the `call` method. By slightly changing your code to pass a list instead of a Pandas dataframe, the model can be saved. ","Thank you , that solution works. :) ",Are you satisfied with the resolution of your issue? Yes No
gemma,copybara-service[bot],Consistently handle algorithm selection in GemmAlgorithmPicker,Consistently handle algorithm selection in GemmAlgorithmPicker,2024-03-25T08:47:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64398
yi,AGFACBNNR,CategoryEncoding layer lacks consideration of 0-dimension inputs," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.15, tf2.16.1  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When provided a 0dim tensor as an input, CategoryEncoding layer will generate an output whose shape is inconsistent with the result of `compute_output_shape()`.  The document does not define the expected behavior of CategoryEncoding layer when the input is a 0dim tensor.  If it's not an acceptable input, maybe a Raise is needed, otherwise `compute_output_shape()` should have generated a shape matching the layer output.      Standalone code to reproduce the issue   Relevant log output _No response_",2024-03-25T06:32:59Z,stat:awaiting response awaiting review type:bug comp:keras TF 2.16,closed,0,6,https://github.com/tensorflow/tensorflow/issues/64392,"Hi  , Thanks for reporting. I have checked and acknowledge the reported behaviour. The code works fine with Keras2 but has inconsistent behaviour with Keras3. Attached keras2gist and keras3gist for reference. As per documentation the inputs should be: > inputs:	A 1D or 2D tensor of integer inputs. The behaviour as follows:  IMO, This behaviour of Keras3 should be consistent with keras2. Looking for probable fix. ",Update: Since Keras3 supports multi backend the Keras3(TF backend) behaviour is different wrt Keras2(TF backend) but inline with Torch and Jax. But there is issue with `compute_output_shape` method with below cases which is not same as actual output shape:  I am going to propose a fix for same. Thanks!,Shall be addressed in Keras PR19388,"Hi  , The proposed PR got merged into master. Could you please test in Kerasnightly and let us know. Thanks!", I've tested it in Kerasnightly. Thanks a lot.,Are you satisfied with the resolution of your issue? Yes No
rag,cv-on-device,XNNPACK is not supported in Linux armv7-a when compile source code,"Hi, I followed the instructions in the website:https://tensorflow.google.cn/lite/guide/build_cmake_arm to cross compile and find a question XNNPACK is not supported in Linux armv7a when compile source code.    **part error log:** `/tensorflow/build_arm/xnnpack/src/qs8qc8wigemm/gen/qs8qc8wigemm2x2c4minmaxfp32armsimd32.c:80:15: error: unknown type name ‘int16x2_t’          const int16x2_t va1c02 = __sxtb16(va1); `",2024-03-23T11:41:36Z,stat:awaiting response stale comp:lite TFLiteConverter,closed,0,5,https://github.com/tensorflow/tensorflow/issues/64358, You're right that XNNPACK is not officially supported for Linux armv7a architecture when compiling the source code. This is because armv7a processors generally lack hardware acceleration features that XNNPACK leverages for performance optimization. You can refer to the XNNPACK documentation here for prebuilt wheels if available. Thank you!," Thank you for your help. The old Version XNNPACK from https://github.com/google/XNNPACK/tree/test_282285243 can support ARMv7 (with NEON) on Android and **Linux**, but the new version only provide script ARMv7 (with NEON) on Android, **not Linux**. I think the problem is related to this.", You are right! It is currently not supported. You can download the specific commit (`test_282285243`) from XNNPACK's GitHub repository. This version should work with ARMv7 NEON on Linux. Please use this with caution because it is not the stable version. It might have unexpected behavior. Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,copybara-service[bot],[XLA:GPU][TileAnalysis] Re-think symbolic tiles and their parameter extraction.,"[XLA:GPU][TileAnalysis] Rethink symbolic tiles and their parameter extraction. Previously, symbolic tiles could take offsets and strides as input parameters. While there is a use for it at codegen time (e.g. to use the symbolic tile to directly produce the overall offset for loading a parameter, and similarly for the strides), the benefit is marginal. However, it turns out to be highly  problematic for more important use cases, such as concrete size propagation; indeed, a combination of a nontrivial reshape with an unknown stride gives rise to stridedependent sizesthus completely preventing concrete size propagation until a constant stride is chosen. If we need to choose a constant stride anyway, then there is no use in carrying around this parameter. There is likely no such problem with offsets, but we likewise elide them from the list of input parameters in order to simplify things. We take this opportunity to simplify our extraction logic as well, and implement logic able to handle merge reshapes correctly. Thanks to the simplification of the structure of symbolic tiles, we can now make stronger assumptions about our intermediate expressions (e.g. where constants appear), and as a result the logic is",2024-03-22T19:16:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64324
yi,copybara-service[bot],[xla][gpu] Make the p2p-schedule-preparation pass to not rely on certain,"[xla][gpu] Make the p2pschedulepreparation pass to not rely on certain pattern to find the corresponding Send/Recv for SendDone/RecvDone. Previously, we relied on certain code patterns to find the Send/Recv for Senddone/Recvdone for a pipelined loop. Now that we realize copyinsertion can complicate such code patterns and make such pattern matching fragile. We modify the p2pschedulepreparation pass to record Send/Recv in additional to SendDone/RecvDone for each p2p communication group.",2024-03-22T17:10:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64307
yi,copybara-service[bot],[XLA:GPU] Fix possible bug in libdevice path logic.,"[XLA:GPU] Fix possible bug in libdevice path logic. * When copying a C string, we need to include the null terminator byte. It seems likely this code reads off the end of the array since it only copies nonnull bytes. But if we use a std::string it will handle this for us. * dirname() may not be threadsafe. On Linux it likely is, but better to avoid it and just use the TSL utility for this. This may be the cause of a flaky test failure in JAX CI.",2024-03-22T15:12:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64291
yi,copybara-service[bot],PR #10316: [NVIDIA GPU] Add a mechanism in GpuAsyncTracker to force delaying scheduling of an instruction.,"PR CC([OpenCL] Cleans control_flow_ops.): [NVIDIA GPU] Add a mechanism in GpuAsyncTracker to force delaying scheduling of an instruction. Imported from GitHub PR https://github.com/openxla/xla/pull/10316 latency hiding scheduler has a property for each HloScheduleNode called SetForceDelay, setting this to true for a node will result in forcing LHS to ignore all cost info and schedule the instruction to the very beginning of the instruction sequence. This is useful in manually enforcing an instruction to run before anything else in the scheduled graph. Passes can set `should_force_delay` attribute in the GpuBackendConfig to instruct how this instruction will be scheduled. It's set to false by default. An example usage:  Copybara import of the project:  74267e60142c5161310580db5b760a3fe6d4f625 by TJ : Add a mechanism in GpuAsyncTracker to force delaying scheduling of an instruction.  0aac4cf8c42f2ca210450581f05239378d8a33c2 by TJ : Fix failing backend config test  4deab8393ecb01ca010e0cf4ea30930ac448b815 by TJ : Use force delay for windowed einsum loops  a1321e88fd336f83a4fcfcc89037b8d4a3d1903f by TJ : Revert changes in gpu_windowed_einsum_handler, moving them to a separate pr  86ffb97db8ce167438562f99731",2024-03-22T11:16:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64275
yi,omontf,tflite converter crashes with version 2.16.1,"Hello ,  I've got a crash when trying to quantize mnist model with tf 2.16.1 (tested with python 3.10 & 3.11) Same example works with tf 2.15.0 (python 3.10)  1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.5 LTS  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): 2.16.1  python 3.11  2. Code Provide code to help us reproduce your issues using one of the following options:   3. Error Log   warnings.warn( WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1711105049.664360 1507500 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format. W0000 00:00:1711105049.664404 1507500 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency. 20240322 11:57:29.664698: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpm0lgi7rh 20240322 11:57:29.665327: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve } 20240322 11:57:29.665351: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpm0lgi7rh 20240322 11:57:29.671257: ",2024-03-22T10:58:44Z,stat:awaiting response type:bug comp:lite TFLiteConverter TF 2.16,closed,0,6,https://github.com/tensorflow/tensorflow/issues/64273,", By default Tensorflow v2.16 uses keras v3.0 which might be the reason for the error you are facing. As the workaround, Could you please try to install keras 2 where the code was executed without any issue/error. Kindly find the gist of it here.  Thank you!",Hi   I tried your suggestion. I confirm that it works well with Keras v2. Thank you ! Olivier,", Glad the issue is resolved for you, Could you please feel free to move this issue to closed status. Thank you!",Are you satisfied with the resolution of your issue? Yes No,Same issue here. macOS M2. Above solution is not working for me. ,This issue should be reopened till the problem is really fixed.  The converter should be updated to work with latest Keras as well.  Downgrading Keras version(and so losing all the improvements introduced with Keras 3) to prevent crash of Tflite model converter is just a temporary workaround not a proper solution. 
yi,hvaria,Support for DT_INT64 in Reciprocal and ReciprocalGrad Operations,"This pull request introduces support for the DT_INT64 data type in the Reciprocal and ReciprocalGrad operations within TensorFlow. Previously, these operations did not support DT_INT64, which could lead to NotFoundError exceptions when attempting to use these operations with tensors of type int64. This limitation could affect a range of computational graphs where int64 tensors are used, especially in scenarios requiring reciprocal transformations of integerbased tensors. Problem: CC(tf.math.reciprocal lacks support of int input on CPU, inconsistent with the documentation) The Reciprocal operation was found to be unsupported for DT_INT64 tensors, resulting in a NotFoundError when attempting to execute this operation on an int64 tensor. Solution: The proposed solution involves modifying the kernel registration in cwise_op_reciprocal.. By adding support for DT_INT64, TensorFlow can handle reciprocal operations on int64 tensors without throwing a NotFoundError, thus extending the operation's applicability and enhancing usability for a broader range of applications. The proposed solution involves modifying the kernel registration in cwise_op_reciprocal.. By adding support for DT_INT64, TensorFlow can handle ",2024-03-22T05:11:53Z,size:S comp:core,closed,0,10,https://github.com/tensorflow/tensorflow/issues/64255,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",Hi  Can you please sign CLA. Thank you!,"I signed, Maybe it takes time to update",Hi  Can you please check 's comments and keep us posted ? Thank you!,"Sorry For delay, directory is deleted",Reciprocal also doesn't make sense for any integer types the result will always be zero (except if the input is 1).  What is the usecase?,I added int64 support to the Reciprocal operation to make TensorFlow more consistent in how it handles different data types. This change helps prevent errors and simplifies processing when working with integers and floatingpoint numbers together.,"> I added int64 support to the Reciprocal operation to make TensorFlow more consistent in how it handles different data types. This change helps prevent errors and simplifies processing when working with integers and floatingpoint numbers together. Except it doesn't really help at all, since the reciprocal op only makes sense for floatingpoint types.  There's never a case where you would instead replace a float with an int in this case.","Thanks for your input. You're absolutely right that the reciprocal operation makes the most sense for floatingpoint types due to their noninteger results. The idea behind adding int64 support isn’t to replace floatingpoint operations with integers but rather to ensure TensorFlow can handle any data type consistently. This helps prevent errors and keeps the door open for any future functionalities that might need this capability, even if it’s a rare case.",">  This helps prevent errors and keeps the door open for any future functionalities that might need this capability, even if it’s a rare case. No, you can't just swap types likes this and expect it to work.  The ops have meanings, and some ops will only work with some times.  Closing this without a valid usecase."
yi,copybara-service[bot],[xla][gpu] Change related to the handling of pipelined Send/Recv.,"[xla][gpu] Change related to the handling of pipelined Send/Recv. Previously, we relied on certain code patterns to find the Send/Recv for Senddone/Recvdone for a pipelined loop. Now that we realize copyinsertion can complicate such code patterns and make such pattern matching fragile. We modify the collectivepermutedecomposer to annotate the pipeline decision on Senddone/Recvdone, in additional to Send/Recv. We also change the HLO scheduler and verifier to use such annotation instead of relying on code pattern matching.",2024-03-21T22:32:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64231
yi,Sur3,using tf.linalg.det in custom loss function yields matrix inversion error, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.16.1  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Calculating the determinant should not require a matrix inversion.. The code worked fine in tensorflow 2.15.0 but tensorflow 2.16.1 yields a matrix inversion error when the determinant is included in the loss function:   Standalone code to reproduce the issue  together with `kernel_initializer=tf.keras.initializers.Zeros()` in a Dense layer triggers the bug.  Relevant log output _No response_,2024-03-21T21:30:58Z,stat:awaiting response type:bug stale comp:ops TF 2.16,closed,0,9,https://github.com/tensorflow/tensorflow/issues/64225,"'''      import tensorflow as tf          from keras.losses import Loss           class DeterminantZeroLoss(Loss):           def __init__(self, name=""determinant_zero_loss""):           super().__init__(name=name)     def call(self, y_true, y_pred):         try:              Calculate the determinant of the predicted matrix             det = tf.linalg.det(y_pred)         except tf.errors.InvalidArgumentError as e:              Handle the case where the matrix inversion fails              You can print the error message or take any other appropriate action             print(""Matrix inversion failed:"", e)              Return a large loss value to indicate the failure             return 1e6          Penalize when the determinant is close to zero         loss = tf.abs(det)         return loss '''  hope it helps ","> except tf.errors.InvalidArgumentError as e: Sadly that didn't catch the error, but I found out the error seems to only occur if I use `kernel_initializer=tf.keras.initializers.Zeros()` in one of the layers as well as the determinant=0 loss. PS: >         return 1e6 >      Penalize when the determinant is close to zero >     loss = tf.abs(det) You seem to have missunderstood that, when the matrix is not invertible the determinant should be zero, therefore the loss should be low, if the determinant is close to zero that is good and not penalized ;)","Hi  , Could you able to submit a reproducible code snippet to debug this? Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Sorry I didn't find time to produce a full code example yet, maybe next week.",", Could you please share colab link or simple standalone code to reproduce the issue in our environment. It helps to debug the issue in an effective way. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,QuesarVII,Tensorflow 2.16.1 fails to compile with -march=native on Xeon 4410Y CPU," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16.1  Custom code No  OS platform and distribution x86_64 Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version 6.5.0  GCC/compiler version clang 17.0.6 (ubuntu package)  CUDA/cuDNN version 12.4 / 8.9.7  GPU model and memory (2) RTX A4500  20GB  Current behavior? Tensorflow 2.16.1 fails to compile with march=native when building with CUDA support on Xeon 4410Y CPUs with clang 17.0.6. I run configure saying yes to cuda support, then edit .tf_configure.bazelrc and add these lines: build:cuda copt=march=native build:cuda host_copt=march=native build:cuda copt=Wnoerror=unusedcommandlineargument  (see issue 62459 regarding ""unused"" and set TF_PYTHON_VERSION=3.10, the bazel build errors out on tensorflow/core/kernels/matmul_op_real./eigen_archive/Eigen/src/Core/MathFunctions.h.  The error output is in the log field. This appears to be the same problem as issue 62047.  Standalone code to reproduce the issue   Relevant log output ",2024-03-21T20:42:20Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.16,closed,0,10,https://github.com/tensorflow/tensorflow/issues/64221,"I just tested building without cuda. I ran configure again, provided ""march=native"" as the ""opt"" compiler flag, and then built with config=opt, without modifying anything other than setting TF_PYTHON_VERSION=3.10.  That failed with the same static_cast error in MathFunctions.h.  I changed the issue subject to remove the mention of CUDA since it happens without it too.",Can you try disabling avx512_fp16? For `gcc` use `copt=mnoavx512fp16`.,", Incorrectly mark vectorized casting as available with **EIGEN_VECTORIZE_AVX512FP16**. Since it doesn't exist, it's triggering all the other errors. Could you please try disabling the avx512_fp16 and try to compile. Thank you!","Yes, avx512_fp16 triggers the issue.  Using mnoavx512fp16 results in a successful build. Thanks.","  Glad the issue is resolved for you, Could you please feel free to move this issue to closed status. Thank you!",Shouldn't the code base be fixed to resolve this issue before closing it?  Disabling compiler flags is more of a workaround than a real fix.  Thanks!,", This issue has been taken to the developer notice and apparently it might be disabled in the upcoming version releases. Also it is tracking internally. So that it will be resolved in the next versions. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #10316: [NVIDIA GPU] Add a mechanism in GpuAsyncTracker to force delaying scheduling of an instruction.,"PR CC([OpenCL] Cleans control_flow_ops.): [NVIDIA GPU] Add a mechanism in GpuAsyncTracker to force delaying scheduling of an instruction. Imported from GitHub PR https://github.com/openxla/xla/pull/10316 latency hiding scheduler has a property for each HloScheduleNode called SetForceDelay, setting this to true for a node will result in forcing LHS to ignore all cost info and schedule the instruction to the very beginning of the instruction sequence. This is useful in manually enforcing an instruction to run before anything else in the scheduled graph. Passes can set `should_force_delay` attribute in the GpuBackendConfig to instruct how this instruction will be scheduled. It's set to false by default. An example usage:  Copybara import of the project:  74267e60142c5161310580db5b760a3fe6d4f625 by TJ : Add a mechanism in GpuAsyncTracker to force delaying scheduling of an instruction.  0aac4cf8c42f2ca210450581f05239378d8a33c2 by TJ : Fix failing backend config test  4deab8393ecb01ca010e0cf4ea30930ac448b815 by TJ : Use force delay for windowed einsum loops  a1321e88fd336f83a4fcfcc89037b8d4a3d1903f by TJ : Revert changes in gpu_windowed_einsum_handler, moving them to a separate pr  86ffb97db8ce167438562f99731",2024-03-21T19:01:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64205
gemma,copybara-service[bot],"Ignore nested tuples, if present, during post processing. ","Ignore nested tuples, if present, during post processing.  This is a hack for some cases (specifically observed for the llama and gemma models) where nested tuples as used as inputs/outputs of the kOptimizationBarrier instruction.",2024-03-21T16:06:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64181
yi,arnava13,Issue with file reading when using TPU enabled colab machine," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15.0  Custom code Yes  OS platform and distribution Google Colab   Mobile device _No response_  Python version Python 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm encountering an UnimplementedError related to the file system scheme '[local]' not being implemented when attempting to read files from the local filesystem in a Google Colab environment while using TPUs. The error occurs when trying to execute tf.io.read_file(file_path) within a data generator class, even when explicitly setting tf.device('/cpu:0') around the file reading operation. This issue arises despite attempts to ensure file operations are directed to execute on the CPU within a TPU environment. The expected behavior is for TensorFlow to successfully read the local file when specified to do so on the CPU, even in a TPU context, without encountering a file system scheme error. Full repository: https://github.com/arnava13/SeniorHonoursProject (Training run in training_colab.ipynb on a colab machine)  Standalone",2024-03-21T05:03:44Z,stat:awaiting response type:bug comp:tpus TF 2.15,closed,0,3,https://github.com/tensorflow/tensorflow/issues/64117,", Could you please try loading the file from local file when using TPU  read them as a normal python file.read() not with tf.io. Also please have a look at this issue which is similar https://stackoverflow.com/questions/62870656/filesystemschemelocalnotimplementedingooglecolabtpu   https://github.com/tensorflow/models/issues/8265issuecomment610535054 Thank you!",Thank you. Reverting to numpy I/O.,Are you satisfied with the resolution of your issue? Yes No
agent,copybara-service[bot],Create new PjRt GPU client with remote devices after coordination service agent is available,"Create new PjRt GPU client with remote devices after coordination service agent is available This does not modify the previous PjRt GPU client. The test environment case where multiple threads are used to simulate multiple workers is supported. For the PjRt GPU MultiWorkerMirroredStrategy (MWMS) case where there are multiple workers each with a GPU or GPUs, the symptom of a client without this fix is a error message like the following where the number at the end is the ID of the first remote device, e.g. 8 when there are eight local GPUs with IDs 0..7. (Another example is 1 when there is one local GPU with ID 0.) `INVALID_ARGUMENT: No matching device found for device_id 8` Note that while the primary purpose of `BaseGPUDeviceFactory::CreateDevices` is to do onetime initialization, it is often called multiple times. In the typical production MWMS case, it is called both when creating a TF Context and when creating GRPC servers when enabling collectives. In the unit test added by this CL (which uses two GPUs), it is first called when a TF Context is created when starting up the test environment (with both GPUS) and then two worker TF processes are created which both call it twice during MWMS startup (each",2024-03-20T23:59:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64096
gemma,copybara-service[bot],"Ignore nested tuples, if present, during post processing. ","Ignore nested tuples, if present, during post processing.  This is a hack for some cases (specifically observed for the llama and gemma models) where nested tuples as used as inputs/outputs of the kOptimizationBarrier instruction.",2024-03-20T23:56:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64095
yi,QuantumCoder4,"slient overflow occurs in tf.range leading to incorrect result, here is a possible fix"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following the documentation https://www.tensorflow.org/api_docs/python/tf/range, tf.range should have consistent result with np.arange.  However, given the following inputs, tf.range outputs `tf.Tensor([1136033460], shape=(1,), dtype=int32)`, which is **incorrect** The correct result is np.arange's output `[1136033460 713794229]`. I did some analysis and find the root cause lies in here: https://github.com/tensorflow/tensorflow/blob/06e37b8d7f2ba0338efd0f588f068122414f0722/tensorflow/core/kernels/sequence_ops.ccL100 When computing the `limit  start`, the actual value overflows. To solve this problem, we can change the original code to a new one by applying the division first origin:  new:   Standalone code to reproduce the issue  ```  Relevant log output _No response_",2024-03-20T20:15:57Z,stat:contribution welcome stat:awaiting tensorflower type:bug comp:ops TF 2.15,closed,0,13,https://github.com/tensorflow/tensorflow/issues/64081,"Your analysis and solution seem valid. Open a PR if you'd like, otherwise I can handle it","I am afraid I don't have time in these few days, I would appreciate it if you can help me to open one.","Hi  ,   I acknowledge the issue of overflow and attached gist for reference.The proposed fix might work it seems. Please feel free to raise a PR.","Hi  , I have a question. What happens if the value of delta =1 here keeping other two params same. It still might cause overflow ?","Line 83 in `sequence_ops.cc`:  In that case, this if statement would get triggered since delta = 1, and then `start <= limit` would get evaluated to `false` so the function will raise an error",The proposed fix above won't work  it leads to different behavior.  See the discussion in the (now closed) PR CC(Fix silent overflow in tf.range). Someone can pick that up if they like.,Is it ok if I take this over?  ,I believe I have a solution for this. Can open up a PR in a bit if  isn't still actively working on it,Are you satisfied with the resolution of your issue? Yes No,Reopening as the fix in CC(Fix integer overflow in tf.range) is getting rolled back due to it introducing a signed integer overflow issue.,Are you satisfied with the resolution of your issue? Yes No,"Reopening once more, see https://github.com/tensorflow/tensorflow/pull/77018issuecomment2412560383",Are you satisfied with the resolution of your issue? Yes No
yi,summa-code,"Working code broke after deploying to new installation. ValueError: When using `stateful=True` in a RNN, the batch size must be static. Found dynamic batch size: sequence.shape=(None, xx, xx)"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.16.1  Custom code No  OS platform and distribution Ubuntu 22.04  Mobile device No  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.4  GPU model and memory _No response_  Current behavior? So the existing code broke, with this error ValueError: When using `stateful=True` in a RNN, the batch size must be static. Found dynamic batch size: sequence.shape=(None, XX, XX),  but when changed to (XX, XX, XX) I get this, ValueError: Input 0 of layer ""lstm"" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 1, XX, XX) If static shape is passed, not sure why additional dimension is added. Need help in resolving this. I tried both the release version and nightly version. The issue persists And also the input is 3D tensor produced by timeseries_dataset_from_array function. Is there any example on training stateful LSTM using these 3D tensors? Not sure if this is a bug really. I am stuck.  Standalone code to reproduce the issue   Relevant log output _No response_",2024-03-20T16:50:17Z,stat:awaiting response type:bug stale subtype: ubuntu/linux TF 2.16,closed,0,16,https://github.com/tensorflow/tensorflow/issues/64061,"Hi **code** ,  Ensure that your input data has a static batch size. When using stateful=True, the batch size should not change between batches. You can achieve this by padding or truncating sequences to a fixed length.  When defining your LSTM layer, set stateful=True and specify the batch_input_shape parameter with a fixed batch size and sequence length. example:   If you are using timeseries_dataset_from_array to prepare your data, make sure that all sequences have the same length. You can achieve this by setting the sequence_length parameter when creating the dataset. example:  Thank you!","So that was the working code, but after I updated to latest in fresh ubuntu install, ""batch_input_shape"" not a recognized attribute.  This is what I am getting  And yes, I am using sequence_length, please refer to timeseries example on this Tensorflow site. https://www.tensorflow.org/tutorials/structured_data/time_series",Facing the exact same issue with `tensorflow 2.16.1` and `Python 3.9.16`,"Looks like either it is a bug or they got rid of the function, but either way, it is like a bug in how they configure the shape internally for statefulness. And also, there are no examples on how to use stateful LSTM in addition to lack of documentation on statefulness.",Anyone? ,"Try using the input layer instead of batch_input_shape.  `InputLayer(batch_shape=(1,None,4))`","If I add a input layer before LSTM layer, I get this, File ""/.local/lib/python3.11/sitepackages/keras/src/utils/traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File "".local/lib/python3.11/sitepackages/tensorflow/python/eager/execute.py"", line 53, in quick_execute     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error: Detected at node sequential_1/lstm_0_1/CudnnRNN defined at (most recent call last):   File ""/.local/lib/python3.11/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler   File ""/.local/lib/python3.11/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 325, in fit   File ""/.local/lib/python3.11/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 118, in one_step_on_iterator   File ""/.local/lib/python3.11/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 106, in one_step_on_data   File ""/.local/lib/python3.11/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step   File ""/.local/lib/python3.11/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler   File ""/.local/lib/python3.11/sitepackages/keras/src/layers/layer.py"", line 814, in __call__   File ""/.local/lib/python3.11/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler   File ""/.local/lib/python3.11/sitepackages/keras/src/ops/operation.py"", line 48, in __call__   File ""/.local/lib/python3.11/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler   File ""/.local/lib/python3.11/sitepackages/keras/src/models/sequential.py"", line 202, in call   File ""/.local/lib/python3.11/sitepackages/keras/src/models/functional.py"", line 194, in call   File ""/.local/lib/python3.11/sitepackages/keras/src/ops/function.py"", line 151, in _run_through_graph   File ""/.local/lib/python3.11/sitepackages/keras/src/models/functional.py"", line 578, in call   File ""/.local/lib/python3.11/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler   File ""/.local/lib/python3.11/sitepackages/keras/src/layers/layer.py"", line 814, in __call__   File ""/.local/lib/python3.11/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler   File ""/.local/lib/python3.11/sitepackages/keras/src/ops/operation.py"", line 48, in __call__   File ""/.local/lib/python3.11/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler   File ""/.local/lib/python3.11/sitepackages/keras/src/layers/rnn/lstm.py"", line 537, in call   File ""/.local/lib/python3.11/sitepackages/keras/src/layers/rnn/rnn.py"", line 398, in call   File ""/.local/lib/python3.11/sitepackages/keras/src/layers/rnn/lstm.py"", line 511, in inner_loop   File ""/.local/lib/python3.11/sitepackages/keras/src/backend/tensorflow/rnn.py"", line 823, in lstm   File ""/.local/lib/python3.11/sitepackages/keras/src/backend/tensorflow/rnn.py"", line 934, in _cudnn_lstm Invalid input_h shape: [1,512,48] [1,512,48] 	 [[{{node sequential_1/lstm_0_1/CudnnRNN}}]] [Op:__inference_one_step_on_iterator_4048]","Hi code ,  Apologies for the dealy, In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,I have this same problem. I've been having this for a while now. ,"having same issue, is this solve?",Same issue here,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"almost one year later, the bug is still there, with the most recent versions of all the packages involved."
yi,copybara-service[bot],Update TFRT dependency to use revision,Update TFRT dependency to use revision http://github.com/tensorflow/runtime/commit/3d4d5024419cc94ad3898895ce43264105cb4620. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10636 from zhenyingliu:offloading/annotator 11148fe63588f58c85ab2530bda64807bf400476,2024-03-20T15:56:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64051
yi,copybara-service[bot],PR #10636: Offloading 1/3: Add annotation for copy-start/copy-done,PR CC(Nondeterminism Docs (2732)): Offloading 1/3: Add annotation for copystart/copydone Imported from GitHub PR https://github.com/openxla/xla/pull/10636 Add stream id in the backend_config of copystart instruction. The stream id is obtained from hlo_query::NextChannelId().  The corresponding copydone instruction which is the use of copystart instruction will be traversed and added the stream id in the backend_config too. This part is automatically done by the subsequent AnnotateStreamAttributesForUsers() existing in the function. The bool data member copy_start_done_ is used to differentiate copystart/copydone from other collective instructions and go through two different paths. https://github.com/openxla/xla/pull/10450 is split and the current PR is the first 1 out of 3 PRs. Copybara import of the project:  ff99c161a634b868d4265204d10d0b80adf2e772 by Jane Liu : Add annotation of stream id for copystart and its use of copydone instruction  64c746a5de6aa4c9370034ed712192dfd78e3a6e by Jane Liu : Enable the annotator for copystart/copydone in gpu compiler  7972b987e45cd165834483c4350e953094b5dbe8 by Jane Liu : Add the annotator pass after HLO rematerialization pass  2a9e317aac85e4de3f0ff5f1558885408ab45,2024-03-20T13:52:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64045
yi,JuliaUReddy,"‘Val_loss’ is not generated and training is getting interrupted before the completion of the last epoch for a multi-input multi-output model. This is happening when my checkpoint directory is checkpoint_dir + ""/checkpoint-{epoch:02d}.tf"""," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.12.0  Custom code Yes  OS platform and distribution MacOS Sonoma 14.1.2  Mobile device _No response_  Python version 3.11.5  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to train a multiinput multioutput tensorflow model but the training gets interrupted(along with a warning) before the completion of the last epoch. Also after the 1st epoch, the val_loss is not getting generated for the rest of the following epochs.  This behavior is seen only when I give the checkpoint directory as checkpoint_dir + ""/checkpoint{epoch:02d}.tf"" and if I give the directory as checkpoint_dir + ""/checkpoint{epoch:02d}{val_loss:.2f}.tf"", I get another another error mentioned here.  Both training and validation data are a list of numpy arrays and I am passing the validation data while training.  Standalone code to reproduce the issue   Relevant log output ",2024-03-20T10:53:43Z,stat:awaiting response type:bug stale comp:apis TF 2.12,closed,0,7,https://github.com/tensorflow/tensorflow/issues/64035,"This is the error that we see when the checkpoint directory is checkpoint_dir + ""/checkpoint{epoch:02d}{val_loss:.2f}.tf"", "," Thank you for raising this issue.  If the issue is the same as this one then we will track the issue in Keras repo. Meanwhile please ensure your checkpoint directory has enough space to store all checkpoints. If training is unexpectedly interrupted, the last checkpoint might be corrupted. Try deleting existing checkpoints and restarting training. Also try with the latest TF version? Thank you!", its not the same issue as the issue CC(Failed to load the native TensorFlow runtime.). Yes tried couple of times to by clearing out space in checkpoint directory but end up with the same issue.The main issue is that val_loss is not generated for all the epochs after the 1st epoch because of which there is an interruption. Yes tried with latest version of TF as well. Getting the same issue.," You've set steps_per_epoch = 213 for training, indicating it expects 213 batches per epoch. Here the training gets interrupted due to ""Your input ran out of data"" warnings. This happens in both the first and ninth epoch. If your dataset is finite data then, use the repeat() method while building your dataset as follows for streaming continuous data;  Please let us know if that works? Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #10636: Offloading 1/3: Add annotation for copy-start/copy-done,PR CC(Nondeterminism Docs (2732)): Offloading 1/3: Add annotation for copystart/copydone Imported from GitHub PR https://github.com/openxla/xla/pull/10636 Add stream id in the backend_config of copystart instruction. The stream id is obtained from hlo_query::NextChannelId().  The corresponding copydone instruction which is the use of copystart instruction will be traversed and added the stream id in the backend_config too. This part is automatically done by the subsequent AnnotateStreamAttributesForUsers() existing in the function. The bool data member copy_start_done_ is used to differentiate copystart/copydone from other collective instructions and go through two different paths. https://github.com/openxla/xla/pull/10450 is split and the current PR is the first 1 out of 3 PRs. Copybara import of the project:  ff99c161a634b868d4265204d10d0b80adf2e772 by Jane Liu : Add annotation of stream id for copystart and its use of copydone instruction  64c746a5de6aa4c9370034ed712192dfd78e3a6e by Jane Liu : Enable the annotator for copystart/copydone in gpu compiler  7972b987e45cd165834483c4350e953094b5dbe8 by Jane Liu : Add the annotator pass after HLO rematerialization pass  2a9e317aac85e4de3f0ff5f1558885408ab45,2024-03-20T08:07:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64026
yi,yerriswa,android.support.v13.app does not exist while building TfLiteCameraDemo, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.8.0  Custom code No  OS platform and distribution ubuntu 20.04  Mobile device container ubuntu 20.04  Python version 3.11.8  Bazel version 6.5.0  GCC/compiler version 9.4.0  CUDA/cuDNN version NA  GPU model and memory _No response_  Current behavior? Trying to build TfLiteCameraDemo.apk using below command    Cmd:bazel build c opt //tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo    Error: error: package android.support.v13.app does not exist Please let me know the steps to resolve above issue.  Standalone code to reproduce the issue   Relevant log output ,2024-03-20T04:36:46Z,stat:awaiting response type:build/install stale comp:lite TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/64019," You are using an older version of  TF  so please consider using the latest version of the TensorFlow Lite Support Library, which is built with AndroidX compatibility. The error messages ""package android.support.v13.app does not exist"" and ""package FragmentCompat does not exist"" point to the usage of deprecated libraries from the Android Support Library. Thank you!",I have cloned from https://github.com/tensorflow/tensorflow.git link and the branch is master. I guess this is the latest. Please let me know if any other latest code is available.," Since you've built the TensorFlow Lite library successfully, please try updating the TfLiteCameraDemo project to use modern libraries. Also kindly replace the deprecated android.support libraries with their AndroidX counterparts. Thank you!"," Thanks for your response, I wil try to update from my side and let you know if I need any help in upgrading the TfLiteCameraDemo project.", Could you please let us know if the issue has been resolved so that we could close the ticket? If not please update us on the same? Thank you! ,", The android.support.v13.app framework was used only for the dynamic permision request which was not very critical for our application. I have commented as of now and went ahead with other development activites. Let you know once I update this code base in future.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only,Internal change only,2024-03-20T04:10:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/64018
yi,copybara-service[bot],Call unbounded broadcast only when necessary.,"Call unbounded broadcast only when necessary. Here's a before & after HLO for `max(f32[?,10], f32[?,10])` where implicit broadcasting is not necessary: Before:  After:  FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10631 from Inteltensorflow:yimei/fix_cpu_bf16_support 9b4f9136f1976b4432e1a60f2deb4a17c150dd3f",2024-03-19T21:23:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63996
rag,copybara-service[bot],Increase Code Coverage of //third_party/tensorflow/core/tfrt/run_handler_thread_pool/,Increase Code Coverage of //third_party/tensorflow/core/tfrt/run_handler_thread_pool/,2024-03-19T20:54:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63989
rag,copybara-service[bot],Add test coverage for attr_util.cc,Add test coverage for attr_util.cc,2024-03-19T17:45:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63974
rag,copybara-service[bot],Pass XNN_FLAG_REDUCE_DIMS to global sum pooling and global average pooling,Pass XNN_FLAG_REDUCE_DIMS to global sum pooling and global average pooling Remove deprecated flag in graph builder,2024-03-19T16:11:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63966
yi,copybara-service[bot],PR #10631: [XLA:CPU][oneDNN] Minor fix in CPU bfloat16 support,"PR CC(i get an error when i build tensorflow by bazel on windows): [XLA:CPU][oneDNN] Minor fix in CPU bfloat16 support Imported from GitHub PR https://github.com/openxla/xla/pull/10631 Align the usage of oneDNN bf16 cpu float normalization with the same guarding condition as for oneDNN rewrites in compilation step, i.e., only when oneDNN is enabled and in nonAOT mode. Copybara import of the project:  8166205b6491dd53dcd65e3898b4b9edfa641517 by Yimei Sun : [XLA:CPU][oneDNN] Minor fix in CPU bfloat16 support  9b4f9136f1976b4432e1a60f2deb4a17c150dd3f by Yimei Sun : Move CpuFloatSupport reference under guard condition Merging this change closes CC(i get an error when i build tensorflow by bazel on windows) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10631 from Inteltensorflow:yimei/fix_cpu_bf16_support 9b4f9136f1976b4432e1a60f2deb4a17c150dd3f",2024-03-19T14:46:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63960
gemma,copybara-service[bot],PR #10562: Add missing warmup run when autotuning.,"PR CC([Bash] Moving multiple parameters out of shebang): Add missing warmup run when autotuning. Imported from GitHub PR https://github.com/openxla/xla/pull/10562 The `xla/service/gpu:gemm_algorithm_picker_test` test, run on V100, was hitting the delay kernel timeout because of this. See CC(Large page fault causes slow performance while using gpu) for explanation of why the best practice is to execute a warmup run **without the GpuTimer active**. Copybara import of the project:  b4ccf2928ee45ec9139db003378095b948bb73d5 by Olli Lupton : Add missing warmup run when autotuning. The xla/service/gpu:gemm_algorithm_picker_test test, run on V100, was hitting the delay kernel timeout because of this. Merging this change closes CC([Bash] Moving multiple parameters out of shebang) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10562 from olupton:missingwarmup b4ccf2928ee45ec9139db003378095b948bb73d5",2024-03-18T17:01:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63887
rag,copybara-service[bot],PR #9873: [ROCm] Don't use CUDA PTX for ROCM in ComputationIdCmd,PR CC(Can't not find the tutorial code for Convolutional Neural network): [ROCm] Don't use CUDA PTX for ROCM in ComputationIdCmd Imported from GitHub PR https://github.com/openxla/xla/pull/9873 Copybara import of the project:  818077159230e06ce8e94b3c556d1d68fa125b09 by Dragan Mladjenovic : [ROCm] Don't use CUDA PTX for ROCM in ComputationIdCmd Merging this change closes CC(Can't not find the tutorial code for Convolutional Neural network) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/9873 from ROCm:comp_id 818077159230e06ce8e94b3c556d1d68fa125b09,2024-03-18T15:55:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63874
rag,schwefeljm,"TFLite 2.16.1 conversions fail with ""AttributeError: 'Sequential' object has no attribute '_get_save_spec'"" "," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 13  Trixie  TensorFlow installation (pip package or built from source): pip  TensorFlow library (version, if pip package or github SHA, if built from source): 2.16.0rc0 & 2.16.1 I am brand new to TensorFlow and welcome any suggestions.  I tested the exact same code on 2.15.0.post1 and 2.16.x. It runs on 15 and errors out on 16  2. Code Provide code to help us reproduce your issues using one of the following options:  Option B: Paste your code here or provide a link to a custom endtoend colab     (You can paste links or attach files by dragging & dropping them below)  Include code to invoke the TFLite Converter Python API and the errors.  Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.  3. Failure after conversion Conversion completely fails with ""AttributeError: 'Sequential' object has no attribute '_get_save_spec'. Did you mean: '_set_save_spec'?"" See below for complete log and traceback. I have had to revert to 2.15.0.post1 to get model to convert and save as TFLite.  5. (optional) Any other info / logs  ",2024-03-18T14:34:37Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.16,closed,2,28,https://github.com/tensorflow/tensorflow/issues/63867,"> ```python >     model = Sequential([ >         data_augmentation, >          layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)), >          layers.Conv2D(16, 3, padding='same', activation=activation1), >          layers.MaxPooling2D(), >         layers.Conv2D(32, 3, padding='same', activation=activation1), >         layers.MaxPooling2D(), >         layers.Conv2D(48, 3, padding='same', activation=activation2), >         layers.MaxPooling2D(), >         layers.Conv2D(64, 3, padding='same', activation=activation1), >         layers.MaxPooling2D(), >         layers.Dropout(0.15), >         layers.Flatten(), >         layers.Dense(128, activation=activation2), >         layers.Dense(num_classes) >     ]) `Sequential` here might be imported from something other than `tf.keras`. Please take a look at CC('Sequential' object has no attribute '_get_save_spec') ","Hi , I am trying to reproduce the code  meanwhile I encountered some  other error. Is it `args = argparser(). Please provide complete reproducible code/link to debug the issue.  Thank You","> Hi , >  > I am trying to reproduce the code meanwhile I encountered some other error. Is it `args = argparser(). Please provide complete reproducible code/link to debug the issue. >  > Thank You Hi  , I updated to code to remove the depency on 'argsparser()' The dataset I used is from: https://www.kaggle.com/datasets/gpiosenka/100birdspecies Though, I expect it work on any dataset. Jason ","> >  >  > `Sequential` here might be imported from something other than `tf.keras`. Please take a look at CC('Sequential' object has no attribute '_get_save_spec')   I went through and forced 'tf.keras.models.Sequential' and it had no effect. Thank you for the suggestions, though. Jason","I was able to replicate on tfnightly as well as 2.16.1. gist, I reduced the reproducible sample to what mattered i.e. the training process is actually irrelevant. This appears to happen when using Keras with the TFLite converter in 2.16.1 onward. Hi , can you please take a look?",I'm experiencing the same error  here   Is it gonna help if I downgrade to an older version of Tensorflow?,> I'm experiencing the same error here >  >  >  > Is it gonna help if I downgrade to an older version of Tensorflow?   For me it did.,"Hi , it seems like 2.15 does not exhibit this behavior currently."," I downgraded to 2.15 and it worked, thanks.",Referencing legacy Keras worked for me: https://blog.tensorflow.org/2024/03/whatsnewintensorflow216.html,"As  stated, using Legacy Keras is the preferred workaround for now: ","I'm having the same issue, but Legacy Keras isn't working either. ","Hi , do you have a notebook or code snippet to share? Additionally have you tried using 2.15 for now? Thanks for any information you can provide.","I  met the same problem when I using tensorflow2.6.1 in python3.12.3, I tried to downgrade tensorflow to 2.15.0, while it not support python3.12.3. I am trying to downgrade python now...",I have the same issue when trying to convert a keras model to TFLite exactly like in the docs. ,I used model.export() to deal with  the problem. ,"Hi , have you heard of AIEdgeTorch?, you can find more information here: googleblog. You will not run into this issue if you go this route. I have created a script for converting your model here:  You will still need to modify your training code but I have tested this and the conversion does work w/o issue. If you want to, you can actually try visualizing the result in modelexplorer as well. Please try them out and let us know if this resolves your issue. If you still need further help, feel free to open a new issue at the respective repo.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,">  I downgraded to 2.15 and it worked, thanks. I am facing trouble in downgrading my TF, can you share how you did and what was your python version?",I have the same issue with a Keras 3 model saved into `.keras` format and reloaded with `keras.models.load_model()`:  This is the error message: ,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,I'm having this issue as well.,"Some folks are mentioning that the legacy keras switch doesn't work for them.  If you're having trouble with it, confirm that it is placed before your keras import statements.  For reference, I am using Python 3.12.3 and Keras 3.2.1 and have no issues when the flag is set at the top of my script as import os os.environ[""TF_USE_LEGACY_KERAS""] = ""1""",I coudn't downgrade tf version beacuse of python version ;however upgrading tf to 2.17 solves the problem.,downgrading to the following versions resolved the issue: TensorFlow version: 2.13.0 Here's how you can install these versions: pip install tensorflow==2.13.0,"Hi All, this more or less has been fixed with the 2.17 release, that is the preferable method. If you are having problems with the latest release like this, please create a new issue for it. Thanks."
yi,copybara-service[bot],Internal change only,Internal change only,2024-03-18T07:38:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63858
yi,copybara-service[bot],Internal code change for maintainability.,Internal code change for maintainability.,2024-03-18T00:42:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63855
rag,AaronEggert,"""ValueError: Layer 'dense' expected 1 input(s). Received 2 instead"" on tf.keras.models.load_model", Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version v2.16.0rc018g5bc9d26649c 2.16.1  Custom code Yes  OS platform and distribution Windows 11 (KB5035853)  Mobile device _No response_  Python version  3.11.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I want to load a saved model directly after training. But when it executes tf.keras.models.load_model it says `ValueError: Layer 'dense' expected 1 input(s). Received 2 instead.`  Standalone code to reproduce the issue   Relevant log output ,2024-03-17T20:17:06Z,stat:awaiting response type:bug stale comp:keras TF 2.16,closed,0,27,https://github.com/tensorflow/tensorflow/issues/63853,"Hi **** , Sorry for the delay, This error typically occurs when the input shape of the loaded model does not match the expected input shape of the layers. Here i am providing gist with updated code for your reference. Please go through it once. Thank you!","Thank you for your help.  When I define the input size while loading the model, I get the same Error. input_shape = (224, 224, 3) model = tf.keras.models.load_model('modelv1.keras', compile=False, custom_objects={'input_shape': input_shape}) Traceback (most recent call last):   File ""C:\Users\AaronDesktop\OneDrive\Dokumente\Meine Projekte\Deep Learning\Hund erkennung\code\train.py"", line 64, in      n_model = tf.keras.models.load_model('modelv1.keras', compile=False, custom_objects={'input_shape': input_shape})               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\AaronDesktop\AppData\Roaming\Python\Python311\sitepackages\keras\src\saving\saving_api.py"", line 176, in load_model     return saving_lib.load_model(            ^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\AaronDesktop\AppData\Roaming\Python\Python311\sitepackages\keras\src\saving\saving_lib.py"", line 155, in load_model     model = deserialize_keras_object(             ^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\AaronDesktop\AppData\Roaming\Python\Python311\sitepackages\keras\src\saving\serialization_lib.py"", line 711, in deserialize_keras_object     instance = cls.from_config(inner_config)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\AaronDesktop\AppData\Roaming\Python\Python311\sitepackages\keras\src\models\sequential.py"", line 336, in from_config     model.add(layer)   File ""C:\Users\AaronDesktop\AppData\Roaming\Python\Python311\sitepackages\keras\src\models\sequential.py"", line 117, in add     self._maybe_rebuild()   File ""C:\Users\AaronDesktop\AppData\Roaming\Python\Python311\sitepackages\keras\src\models\sequential.py"", line 136, in _maybe_rebuild     self.build(input_shape)   File ""C:\Users\AaronDesktop\AppData\Roaming\Python\Python311\sitepackages\keras\src\layers\layer.py"", line 224, in build_wrapper     original_build_method(*args, **kwargs)   File ""C:\Users\AaronDesktop\AppData\Roaming\Python\Python311\sitepackages\keras\src\models\sequential.py"", line 177, in build     x = layer(x)         ^^^^^^^^   File ""C:\Users\AaronDesktop\AppData\Roaming\Python\Python311\sitepackages\keras\src\utils\traceback_utils.py"", line 123, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""C:\Users\AaronDesktop\AppData\Roaming\Python\Python311\sitepackages\keras\src\layers\input_spec.py"", line 156, in assert_input_compatibility     raise ValueError( ValueError: Layer 'dense' expected 1 input(s). Received 2 instead.","I have the exact same error. Although the platform is different (MacBook Pro w/ M3), the TF version is the same (2.16.1). My models are also really similar to the ones described here (CNN Backbone + GlobalAveragePooling2D + Dense). The proposed fix also did not seem to work properly. Do we have any updates on this issue?",I found a solution to that: You need to define the input_size on loading the model This Code whould help you: ,Are you satisfied with the resolution of your issue? Yes No,"Well, it doesn't seem to work properly. Am I missing something? ",I am having the shape issue on a Windows 10 machine with TF 2.16.1 and similar model. The proposed solution yields the same ValueError as before. ,Have exactly the same issue on Mac. Setting the input shape does't help,"same here with tf 2.16.1， I've also tried tf2.15 with keras 3.0&3.1.1 but with no luck. My trained and saved .keras model （trained with 2.16.1）is not able to load with keras.saving.load_model()  a walkround is to save the trained model with .weights.h5 and build model , load weights(but you need to retrain the model and save weights only which is sometimes not acceptable) `model.save_weights(""mobilenetv2_weights_only.weights.h5"")` then "," I think that we reached the same workaround. The only thing working for now is saving the weights and loading them to the model:  Not the optimal solution, but it works.",Any update on this issue?,"есть ли фид бек по тому когда эта ошибка появилась , потому что раньше все работало отлично и никаких проблем не наблюдалось",Version 2.15.0.post1 is working fine for *.keras extensions. The model should also be trained on this version. It's working fine on 2.15.0.post1,Thanks . It worked for me!,"I am also having the shape issue on a Ubuntu 20.4 machine with TF 2.16.1  this how i trained my model, here is when i load my model happens  here is error ",Also having the same issue. Windows 10 machine with TF 2.16.1. Here is how I trained my model:  and here is my error: ,cord try to use version 2.15 it should work as expected. ,"i have solved this problem by lower my tensorflow version to 2.12.0, maybe u can have a try &nbsp;原始邮件&nbsp; 发件人: ***@***.***&gt;;  发送时间: 2024年4月24日(星期三) 上午10:10 收件人: ***@***.***&gt;;  抄送: ***@***.***&gt;; ***@***.***&gt;;  主题: Re: [tensorflow/tensorflow] &quot;ValueError: Layer & CC(is a Python 3 support coming soon ?);dense& CC(is a Python 3 support coming soon ?); expected 1 input(s). Received 2 instead&quot; on tf.keras.models.load_model (Issue CC(""ValueError: Layer 'dense' expected 1 input(s). Received 2 instead"" on tf.keras.models.load_model)) Also having the same issue. Windows 10 machine with TF 2.16.1. Here is how I trained my model:  import tensorflow as tf from tensorflow.keras.applications import EfficientNetV2S V2S_model = EfficientNetV2S(weights='imagenet',                         include_top=False,                         input_shape=(224, 224, 3)) for layer in V2S_model.layers:   layer.trainable = False from tensorflow.keras import layers image_preprocess = tf.keras.Sequential([     tf.keras.Input((None,None,3)),     layers.Resizing(224,224, crop_to_aspect_ratio = True),     layers.RandomFlip(""horizontal_and_vertical""),     layers.RandomRotation(0.2) ], name = ""image_aug"") transfer_model = tf.keras.Sequential([     tf.keras.Input((None,None,3)),     image_preprocess,     V2S_model,     layers.GlobalAveragePooling2D(),     layers.Dense(512, activation='relu'),     layers.Dropout(0.2),     layers.Dense(1, activation = 'sigmoid') ]) transfer_model.summary() metrics = [tf.keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy'),            tf.keras.metrics.AUC(),            tf.keras.metrics.TruePositives(),            tf.keras.metrics.TrueNegatives(),            tf.keras.metrics.FalsePositives(),            tf.keras.metrics.FalseNegatives()           ] transfer_model.compile(loss=tf.keras.losses.BinaryCrossentropy(),                        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),                        metrics=metrics) BATCH_SIZE = 16 IMAGE_SIZE = (224,224) SEED = 42  This sets up a training and validation set from our ../data/ directory train_dataset = tf.keras.utils.image_dataset_from_directory(     '../data/',     class_names = ['not_niblet','niblet'],     color_mode='rgb',     batch_size=BATCH_SIZE,     image_size=IMAGE_SIZE,     shuffle=True,     validation_split=0.2,     subset='training',     seed=SEED)  This is the validation set. Notice `shuffle = FALSE` and `subset = validation` val_dataset = tf.keras.utils.image_dataset_from_directory(     '../data/',     class_names = ['not_niblet','niblet'],     color_mode='rgb',     batch_size=BATCH_SIZE,     image_size=IMAGE_SIZE,     shuffle=True,     validation_split=0.2,     subset='validation',     seed=SEED) from tensorflow.keras.callbacks import EarlyStopping es = EarlyStopping(patience=15, monitor='val_loss') history = transfer_model.fit(train_dataset, epochs=100,                              validation_data=val_dataset,                              callbacks=[es]) transfer_model.save('../models/transfer_model_gt_2024_04_23.keras') transfer_model_loaded = tf.keras.models.load_model('../models/transfer_model_gt_2024_04_23.keras')   and here is my error:   ValueError                                Traceback (most recent call last) Cell In[16], line 2       1 input_shape = (None,None,3) &gt; 2 transfer_model_loaded = tf.keras.models.load_model('../models/transfer_model_gt_2024_04_23.keras', compile=False, custom_objects={'input_shape': input_shape}) File ~\anaconda3\envs\CS380_final\Lib\sitepackages\keras\src\saving\saving_api.py:176, in load_model(filepath, custom_objects, compile, safe_mode)     173         is_keras_zip = True     175 if is_keras_zip: &gt; 176     return saving_lib.load_model(     177         filepath,     178         custom_objects=custom_objects,     179         compile=compile,     180         safe_mode=safe_mode,     181     )     182 if str(filepath).endswith(("".h5"", "".hdf5"")):     183     return legacy_h5_format.load_model_from_hdf5(filepath) File ~\anaconda3\envs\CS380_final\Lib\sitepackages\keras\src\saving\saving_lib.py:152, in load_model(filepath, custom_objects, compile, safe_mode)     147     raise ValueError(     148         ""Invalid filename: expected a `.keras` extension. ""     149         f""Received: filepath={filepath}""     150     )     151 with open(filepath, ""rb"") as f: &gt; 152     return _load_model_from_fileobj(     153         f, custom_objects, compile, safe_mode     154     ) File ~\anaconda3\envs\CS380_final\Lib\sitepackages\keras\src\saving\saving_lib.py:170, in _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode)     168  Construct the model from the configuration file in the archive.     169 with ObjectSharingScope(): &gt; 170     model = deserialize_keras_object(     171         config_dict, custom_objects, safe_mode=safe_mode     172     )     174 all_filenames = zf.namelist()     175 if _VARS_FNAME + "".h5"" in all_filenames: File ~\anaconda3\envs\CS380_final\Lib\sitepackages\keras\src\saving\serialization_lib.py:711, in deserialize_keras_object(config, custom_objects, safe_mode, **kwargs)     709 with custom_obj_scope, safe_mode_scope:     710     try: &gt; 711         instance = cls.from_config(inner_config)     712     except TypeError as e:     713         raise TypeError(     714             f""{cls} could not be deserialized properly. Please""     715             "" ensure that components that are Python object""    (...)     719             f""\n\nconfig={config}.\n\nException encountered: {e}""     720         ) File ~\anaconda3\envs\CS380_final\Lib\sitepackages\keras\src\models\sequential.py:339, in Sequential.from_config(cls, config, custom_objects)     334     else:     335         layer = serialization_lib.deserialize_keras_object(     336             layer_config,     337             custom_objects=custom_objects,     338         ) &gt; 339     model.add(layer)     340 if (     341     not model._functional     342     and build_input_shape     343     and isinstance(build_input_shape, (tuple, list))     344 ):     345     model.build(build_input_shape) File ~\anaconda3\envs\CS380_final\Lib\sitepackages\keras\src\models\sequential.py:120, in Sequential.add(self, layer, rebuild)     118 self._layers.append(layer)     119 if rebuild: &gt; 120     self._maybe_rebuild()     121 else:     122     self.built = False File ~\anaconda3\envs\CS380_final\Lib\sitepackages\keras\src\models\sequential.py:139, in Sequential._maybe_rebuild(self)     137 if isinstance(self._layers[0], InputLayer) and len(self._layers) &gt; 1:     138     input_shape = self._layers[0].batch_shape &gt; 139     self.build(input_shape) File ~\anaconda3\envs\CS380_final\Lib\sitepackages\keras\src\layers\layer.py:222, in Layer.__new__.<locals&gt;.build_wrapper(*args, **kwargs)     219 (original_build_method)     220 def build_wrapper(*args, **kwargs):     221     with obj._open_name_scope(): &gt; 222         original_build_method(*args, **kwargs)     223      Record build config.     224     signature = inspect.signature(original_build_method) File ~\anaconda3\envs\CS380_final\Lib\sitepackages\keras\src\models\sequential.py:180, in Sequential.build(self, input_shape)     178 for layer in self._layers[1:]:     179     try: &gt; 180         x = layer(x)     181     except NotImplementedError:     182          Can happen if shape inference is not implemented.     183          TODO: consider reverting inbound nodes on layers processed.     184         return File ~\anaconda3\envs\CS380_final\Lib\sitepackages\keras\src\utils\traceback_utils.py:122, in filter_traceback.<locals&gt;.error_handler(*args, **kwargs)     119     filtered_tb = _process_traceback_frames(e.__traceback__)     120      To get the full stack trace, call:     121      `keras.config.disable_traceback_filtering()` &gt; 122     raise e.with_traceback(filtered_tb) from None     123 finally:     124     del filtered_tb File ~\anaconda3\envs\CS380_final\Lib\sitepackages\keras\src\layers\input_spec.py:156, in assert_input_compatibility(input_spec, inputs, layer_name)     154 inputs = tree.flatten(inputs)     155 if len(input_spec) != len(inputs): &gt; 156     raise ValueError(     157         f""Layer '{layer_name}' expected {len(input_spec)} input(s). ""     158         f""Received {len(inputs)} instead.""     159     )     160 for x in inputs:     161      Having a shape/dtype is the only commonality of the various     162      tensorlike objects that may be passed. The most common kind of     163      invalid type we are guarding for is a Layer instance (Functional API),     164      which does not have a `shape` attribute.     165     if not hasattr(x, ""shape""): ValueError: Layer 'dense' expected 1 input(s). Received 2 instead.   — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***&gt;",> الإصدار 2.15.0.post1 يعمل بشكل جيد مع امتدادات *.keras. يجب أيضًا تدريب النموذج على هذا الإصدار. إنه يعمل بشكل جيد على 2.15.0.post1 Thank you it wored .,"I have exactly the same problem on Keras 3.4.1 and Tensorflow 2.16.2  I save a model and can't load it because of that ""ValueError: Layer 'dense' expected 1 input(s). Received 2 instead"" error. I think the bug is in Keras, not Tensorflow: * Downgrading Tensorflow to 2.16.1 did NOT help, the problem persists. * Downgrading Keras to 3.3.3 helped  the problem is solved. Interestingly, Keras 3.3.3 could load a model saved in 3.4.1 (which 3.4.1 couldn't load) just fine.","What version of Keras did you have this bug with? I just reported a regression in Keras 3.4, which didn't exist in 3.3.3, with exactly these symptoms. It was fixed today in https://github.com/kerasteam/keras/commit/0be3e78459c4c85766fe31ed31b0f98ab67c5de4 The bug involved a Functional layer inside a Sequential  exactly what you used in your example.","Could you please check whether this issue is resolved with the latest tensorflow v2.17 which contains Keras 3.0 version. As Keras3 now built to support multiple backends there are some changes in design. Also this issue is more related to Keras, please try to raise the issue in kerasteam/Keras repo for the quick resolution. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"> I have exactly the same problem on Keras 3.4.1 and Tensorflow 2.16.2  I save a model and can't load it because of that ""ValueError: Layer 'dense' expected 1 input(s). Received 2 instead"" error. >  > I think the bug is in Keras, not Tensorflow: >  > * Downgrading Tensorflow to 2.16.1 did NOT help, the problem persists. > * Downgrading Keras to 3.3.3 helped  the problem is solved. Interestingly, Keras 3.3.3 could load a model saved in 3.4.1 (which 3.4.1 couldn't load) just fine. Wow thank you so much.. you really help me out.. I was stuck in this error like the entire day then turns out the problem was in keras itself. thanks again haha",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
llm,QuantumCoder4,"tf.linalg.svd([inf, -inf]) throws internal error logs to the console, and generates inconsistent results on different dtypes."," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.15.0  Custom code Yes  OS platform and distribution Linux5.14.0362.18.1.el9_3.x86_64x86_64withglibc2.34  Mobile device AlmaLinux 9  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? An internal error log was thrown to the console. Also, we found that API generates inconsistent results on different input dtypes. The same error log has appeared in CC(Error happend in 'tf.linalg.svd'.), but I am not sure whether we are talking about the same problem.  Standalone code to reproduce the issue   Relevant log output ",2024-03-17T18:58:12Z,stat:awaiting tensorflower type:bug comp:ops TF 2.15,open,0,4,https://github.com/tensorflow/tensorflow/issues/63852,", Looks like this is a duplicate of the issue  CC(linalg.svd  complex64  NaN  bug). Could you please close this issue, since it is already being tracked there? Thank you!","Thanks for reply. I have checked the issue CC(linalg.svd  complex64  NaN  bug). Although the problem seems to be not exactly the same, I will be happy if this issue can also be tracked there.",Are you satisfied with the resolution of your issue? Yes No,"Hello. It seems that issue CC(linalg.svd  complex64  NaN  bug) has been closed as completed. However, this internal error log still exists in the latest version.  And it will produce the following log:  Also, I found that this issue occurs also on `float64`, not limited to `complex64`. Please kindly check whether this is expected and whether it's actually the same issue as mentioned one CC(linalg.svd  complex64  NaN  bug)."
yi,QuantumCoder4,tf.math.angle(nan) returns inconsistent results on dtype float64 / complex128.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.15.0  Custom code Yes  OS platform and distribution Linux5.14.0362.18.1.el9_3.x86_64x86_64withglibc2.34  Mobile device AlmaLinux 9  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? tf.math.angle gives inconsistent outputs on NaN value: if tensor dtype=complex128 then NaN, while for dtype=float64 it is 0. Although doc states that for any real input the output will be zero, I would like to list some points to argue. 1. It does not make sense for ending up with a regular value for calculating NaN. 2. The nature of NaN should not change, regardless of the dtype of the tensor it's staying in. 3. Such behavior may let NaN error to escape, causing trouble in debugging. Expected behavior: the result is NaN when input is NaN.  Standalone code to reproduce the issue   Relevant log output _No response_",2024-03-17T08:26:25Z,stat:awaiting response type:bug stale comp:ops TF 2.15,closed,0,6,https://github.com/tensorflow/tensorflow/issues/63848,"import tensorflow as tf import numpy as np def angle_with_nan_handling(input_tensor):      Check if input_tensor contains NaN values     contains_nan = tf.reduce_any(tf.math.is_nan(input_tensor))      If input_tensor contains NaN values, return NaN     if contains_nan:         return tf.fill(tf.shape(input_tensor), tf.constant(np.nan, dtype=tf.float64))     else:          Calculate angle for nonNaN values         return tf.math.angle(input_tensor)  Test with float64 input input_float64 = tf.constant([np.nan], dtype=tf.float64) out_float64 = angle_with_nan_handling(input_float64) print(""Output for float64 input:"", out_float64.numpy())   Output: [nan]  Test with complex128 input input_complex128 = tf.constant([np.nan], dtype=tf.complex128) out_complex128 = angle_with_nan_handling(input_complex128) print(""Output for complex128 input:"", out_complex128.numpy())   Output: [nan]",  I was able to replicate the issue here. Thank you!,", As for the code in the complex number gist, the model outputs are float32 instances which explains why their output returns nans. https://github.com/tensorflow/tensorflow/issues/50854 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],[xla:hlo] NFC: Add benchmark for HloDfsReachability::IsReachable,[xla:hlo] NFC: Add benchmark for HloDfsReachability::IsReachable,2024-03-16T02:09:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63840
yi,jonas-eschle,TensorFlow 2.16 API docs not available online," Issue type Documentation Bug  TensorFlow version 2.16 The website displaying the 2.16 API of TensorFlow is not available, see https://www.tensorflow.org/api/r2.16 Only the 2.15 appears if clicking through",2024-03-15T23:15:50Z,type:docs-bug stat:awaiting response comp:apis,closed,4,11,https://github.com/tensorflow/tensorflow/issues/63834,"eschle, Thank you for reporting this issue. Please allow some duration to discuss with the developer on this and will provide the update on the same issue. Thank you!","eschle, There is a change request for this issue and we are tracking it internally. Once it is resolved, the documentation will be updated to v2.16 and will update the same. Thank you!",Thanks a lot for taking care of it!,"eschle, If you feel the issue is addressed, Could you please feel free to move this issue to closed status. Thank you!","I will check, but at this moment in time, the docs don't show up yet","eschle, Yeah. As mentioned the issue was informed to the respective team and will update the docs in upcoming days. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Hi, we are also interested in the docs for v2.16 API. I am commenting to keep this issue open. ","eschle, , When I checked the official document, I could able to check the 2.16 api docs now. https://www.tensorflow.org/api_docs/python/tf Could you please check and confirm the same. Screenshot for the reference. !image","I confirm that the docs are visible now, thanks for the fix! closing",Are you satisfied with the resolution of your issue? Yes No
gemma,copybara-service[bot],PR #10562: Add missing warmup run when autotuning.,"PR CC([Bash] Moving multiple parameters out of shebang): Add missing warmup run when autotuning. Imported from GitHub PR https://github.com/openxla/xla/pull/10562 The `xla/service/gpu:gemm_algorithm_picker_test` test, run on V100, was hitting the delay kernel timeout because of this. See CC(Large page fault causes slow performance while using gpu) for explanation of why the best practice is to execute a warmup run **without the GpuTimer active**. Copybara import of the project:  b4ccf2928ee45ec9139db003378095b948bb73d5 by Olli Lupton : Add missing warmup run when autotuning. The xla/service/gpu:gemm_algorithm_picker_test test, run on V100, was hitting the delay kernel timeout because of this. Merging this change closes CC([Bash] Moving multiple parameters out of shebang) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10562 from olupton:missingwarmup b4ccf2928ee45ec9139db003378095b948bb73d5",2024-03-15T11:29:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63796
llm,yutkin,Unable to retrieve BLAS factory in Tensorflow 2.16.1," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Linux, Ubuntu 20.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuda:12.2.2cudnn8  GPU model and memory NVIDIA T4  Current behavior? We tried to upgrade our current 2.13.0 `libtensorflow` binary in our Golang application to the latest 2.16.1. However, the new `libtensorflow` does not work and starts to produce errors during the model inference:  The only change we made is to replace the Googledistributed `libtensorflow` 2.13.0  with `libtensorflow` 2.16.1 from here and change the base docker image:   Standalone code to reproduce the issue   Relevant log output ",2024-03-15T08:09:48Z,stat:awaiting tensorflower type:build/install subtype: ubuntu/linux TF 2.16,open,0,6,https://github.com/tensorflow/tensorflow/issues/63785,"Hi  , Just FYIP, we are not currently supporting GOLANG. If you are looking for any support of it please contact the third party contributor as mentioned here.","> Hi  , >  >  >  > Just FYIP, we are not currently supporting GOLANG. If you are looking for any support of it please contact the third party contributor as mentioned here. Hi! It's not a Golang problem. The errors come from libtensorflow. ",I also have this problem. All was working fine with 2.15 and became a problem with 2.16 Application is Blurexterminator plugin of PixInsight. I believe it is C++ application.,", Could you please try to check with the latest tensorflow v2.17 and provide the update whether you are facing the same issue? Thank you!","> , Could you please try to check with the latest tensorflow v2.17 and provide the update whether you are facing the same issue? Thank you! Hi! I can't try until https://github.com/tensorflow/tensorflow/issues/63384 fixed :(",I'm also experiencing this issue. Here's a standalone test case. It works on tensorflow2.15.0 C API downloaded from Tensorflow on my PC running WSL2 Ubuntu. It fails on tensorflow2.16.1 C API downloaded from Tensorflow on a LambdaCloud instance. So my project is completely stalled at the moment. CMakeLists.txt main.c.txt outputtensorflow2.15.0.txt outputtensorflow2.16.1.txt
agent,copybara-service[bot],Cleanup CoordinationServiceAgent ownership,Cleanup CoordinationServiceAgent ownership,2024-03-15T05:42:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63780
yi,copybara-service[bot],PR #10316: [NVIDIA GPU] Add a mechanism in GpuAsyncTracker to force delaying scheduling of an instruction.,"PR CC([OpenCL] Cleans control_flow_ops.): [NVIDIA GPU] Add a mechanism in GpuAsyncTracker to force delaying scheduling of an instruction. Imported from GitHub PR https://github.com/openxla/xla/pull/10316 latency hiding scheduler has a property for each HloScheduleNode called SetForceDelay, setting this to true for a node will result in forcing LHS to ignore all cost info and schedule the instruction to the very beginning of the instruction sequence. This is useful in manually enforcing an instruction to run before anything else in the scheduled graph. Passes can set `should_force_delay` attribute in the GpuBackendConfig to instruct how this instruction will be scheduled. It's set to false by default. An example usage:  Copybara import of the project:  74267e60142c5161310580db5b760a3fe6d4f625 by TJ : Add a mechanism in GpuAsyncTracker to force delaying scheduling of an instruction.  0aac4cf8c42f2ca210450581f05239378d8a33c2 by TJ : Fix failing backend config test  4deab8393ecb01ca010e0cf4ea30930ac448b815 by TJ : Use force delay for windowed einsum loops  a1321e88fd336f83a4fcfcc89037b8d4a3d1903f by TJ : Revert changes in gpu_windowed_einsum_handler, moving them to a separate pr Merging this change closes C",2024-03-14T22:26:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63756
rag,copybara-service[bot],hlo_dfs_reachability: reserve indices' storage at Build() time,hlo_dfs_reachability: reserve indices' storage at Build() time name                               old time/op  new time/op  delta BM_HloDfsReachabilityBuild/1        105ns ± 1%   105ns ± 1%     ~     (p=0.151 n=5+5) BM_HloDfsReachabilityBuild/64      3.51µs ± 3%  2.38µs ± 2%  32.18%  (p=0.008 n=5+5) BM_HloDfsReachabilityBuild/128     6.93µs ± 2%  4.79µs ± 3%  30.82%  (p=0.008 n=5+5) BM_HloDfsReachabilityBuild/256     13.8µs ± 1%   9.5µs ± 1%  30.68%  (p=0.008 n=5+5) BM_HloDfsReachabilityBuild/512     28.0µs ± 1%  19.3µs ± 1%  30.86%  (p=0.008 n=5+5) BM_HloDfsReachabilityBuild/4096     268µs ± 1%   188µs ± 1%  29.90%  (p=0.008 n=5+5) BM_HloDfsReachabilityBuild/32768   2.74ms ± 3%  2.03ms ± 2%  25.99%  (p=0.008 n=5+5) BM_HloDfsReachabilityBuild/262144  43.8ms ± 2%  38.9ms ± 2%  11.15%  (p=0.008 n=5+5),2024-03-14T21:34:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63749
llm,copybara-service[bot],Preserve Aliased Functions in StableHLO Quantizer,Preserve Aliased Functions in StableHLO Quantizer Aliased functions are represented as StatefulPartitionedCall in the TF graph. They will be marked tf._noinline=true so that Inliner pass will skip them. In this CL I: * modified TF>StableHLO pass to skip lowering them to func.call op. * modified ReplaceStablehloOpsInMainFunctionWithXlaCallModuleOpsPass. So that StableHLO graph in each StatefulPartitionedCall will be lifted.,2024-03-14T20:09:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63742
yi,copybara-service[bot],Add a knob to HloCSE to only sink scalar instructions.,Add a knob to HloCSE to only sink scalar instructions. HloCSE is executed to identifying unroallable loops.,2024-03-14T18:50:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63738
rag,copybara-service[bot],#shlo_ref Remove impossible storage/expressed type combinations from `DISPATCH_QUANTIZED`.,shlo_ref Remove impossible storage/expressed type combinations from `DISPATCH_QUANTIZED`.,2024-03-14T18:46:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63737
yi,Dobiasd,Enormous memory usage after batched forward passes with TensorFlow 2.16.1,"The following minimal example reproduces the effect. (No GPU, just CPU.)   (Dockerfile to reproduce) And it's not just `tf.keras.applications.ResNet152V2()`. It also happens (for example) with `tf.keras.applications.inception_resnet_v2.InceptionResNetV2` and `tf.keras.applications.inception_v3.InceptionV3`. And it also happens when using Python `3.12.2` instead of `3.11.8`. With TensorFlow `2.15.1` (instead of `2.16.1`), however, the memory usage does not explode:  (Dockerfile to reproduce)",2024-03-14T09:00:19Z,stat:awaiting tensorflower comp:runtime type:performance TF 2.16,closed,1,16,https://github.com/tensorflow/tensorflow/issues/63701,"As a workaround (I just found), one can replace `model(images).numpy()` with `model.predict(images)`. This tames the memory usage.",", As you mentioned there was memory usage with tensorflow 2.16.1 while executing the mentioned code. When compared with 2.15, I can see the difference in memory usage. Could you please allow some time to check with the developer and provide the solution for the same. Meanwhile you can try using model.predict(images) where the memory is the same as 2.15. Kindly find the gist of it here. Click to expand for the screenshots! 2.15: !Screenshot 20240315 2 26 23 PM 2.16: !Screenshot 20240315 2 11 36 PM",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Thanks for reproducing the effect. I know that using `model.predict(images)` is a workaround for the problematic `model(images)` (wrote it already above). However, I don't feel that this is a reason to close this issue. `model(images)` is still broken.",", By default tensorflow v2.16 uses keras 3.0 version. I tried to import the keras as **import tf_keras as keras** and the code given execution time with the **model(images)** is similar as tensorflow v2.15. Kindly find the gist of it here.   Thank you!",Thanks! Does this mean this issue should be moved from the `tensorflow/tensorflow` repo to the `kerasteam/keras` repo?,"Btw., `model.predict(images)` also leaks a bit of memory:   (Dockerfile to reproduce)",", Looks like this is also happening with the tensorflow v.2.16 & keras3.0. With the tensorflow v2.15 and tfkeras I was not able to find any issue with the memory. Kindly please raise the request in kerasteam/keras repo. Thank you!","Thanks. As suggested, I've opened this issue in `kerasteam/keras`.","The conclusion so far of the discussion in the Keras repo is, that it's a problem with TensorFlow, not with Keras.","This is what I get when running 's dockerfile image on my machine:  This pattern seems to repeat: slowly growing, then a drop.  It never seems to monotonically increase: only up to some limit before dropping again. I'm not sure it's a leak, since it stays below a certain (perhaps machinespecific) limit before it drops down.  Outside of docker, running locally on my machine, I see more stable results on CPU:  which is very similar to 's colab. I don't even think it's a keras 3 vs legacy keras issue, since using legacy `tfkeras` leads to the same memory profile for me both in docker and my local machine. My guess is its just TensorFlow's BFC allocator slowly growing.  Something might have changed in TF 2.16 related to memory allocations that reduces memory reuse before the allocator consolidates freed blocks.  If you have a lot of memory available, it might grow more before doing this on your particular machine.","Thanks for testing! > If you have a lot of memory available, it might grow more before doing this on your particular machine. That's very unfortunate if you're running something like that in a Kubernetes pod with a (cgroupsmanaged) memory limit, and TensorFlow uses the memory as if it were the only process on the whole node. At least that's how I ran into this problem, i.e., my K8s pods were constantly OOMKilled.","> Thanks for testing! >  > > If you have a lot of memory available, it might grow more before doing this on your particular machine. >  > That's very unfortunate if you're running something like that in a Kubernetes pod with a (cgroupsmanaged) memory limit, and TensorFlow uses the memory as if it were the only process on the whole node. At least that's how I ran into this problem, i.e., my K8s pods were constantly OOMKilled. Sorry, I made a mistake  using legacy keras _does_ fix the issue.  Setting the environment variable:  _does_ restore the original memory profile.  So keras 3 is a contributing factor.  I think it has to do with tracking information for training (since this is a trainable model).  Note that the initial jump in memory use is just the model loading the weights for the first time, since you report your first memory usage _before_ your first predict call. I still don't think it's a ""leak"".  Modifying your script a bit to use `tracemalloc` and recording memory use _after_ the first predict iteration:  I get the following with Keras 3 by exporting `TF_USE_LEGACY_KERAS=0`:  The `constant_op` is allocating the weight and shape tensors.  Then there's a bunch of ""tracking"" allocations. If I use legacy keras by exporting `TF_USE_LEGACY_KERAS=1`, I get:  The main difference to me looks like mostly `tracking`related allocations in keras 3.  Every keras layer is `Trackable`, so my guess is the increased memory usage lies there.  I believe converting to a `tf.function` essentially disables this trainable tracking, so you get the similar results as your `model.predict(...)` call. I still don't get unbounded growth though, just vastly increased memory usage.  As for what to do: can you live with legacy keras and set the environment variable `TF_USE_LEGACY_KERAS=1`?  Can you freeze your model into an inferenceonly TF model and use that?  Otherwise I think the keras team might be better at debugging.","Actually, rewriting your reproducer in terms of `keras` alone and setting  it to use JAX as the backend, I get similar high memory usage:   Changing the model to use `model.predict(...)` instead yields much lower memory usage:  which seems to confirm that all the memory is the keras model storing training information. So I'm pretty confident this isn't a TensorFlow bug, and it may or may not be a Keras bug  depending on if that memory use for training is actually needed by keras.","> As for what to do: can you live with legacy keras and set the environment variable `TF_USE_LEGACY_KERAS=1`? Can you freeze your model into an inferenceonly TF model and use that? Otherwise I think the keras team might be better at debugging. Can conform, in my original minimal example, `TF_USE_LEGACY_KERAS=1` helps. I'll try it in PROD next week. :+1: Thanks!",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Make error message about disabling streaming enqueue more accurate by querying the environment variable,Make error message about disabling streaming enqueue more accurate by querying the environment variable,2024-03-14T05:50:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63692
yi,copybara-service[bot],Add a knob to WhileLoopConstantSinking to only sink scalar constants.,Add a knob to WhileLoopConstantSinking to only sink scalar constants. We use this to prepare the module for urolling while loops. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10631 from Inteltensorflow:yimei/fix_cpu_bf16_support 9b4f9136f1976b4432e1a60f2deb4a17c150dd3f,2024-03-13T23:39:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63670
yi,aws-yishanm,How to plugin a PJRT/common runtime device to Tensorflow?," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.13.1  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi all, I am trying to plugin a PJRT device to tensorflow. I saw https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/framework/load_library.pyL160, but that seems to be for the StreamExecutor C API. I also took a look at https://github.com/tensorflow/tensorflow/blob/6c20138314183635f70dfff454d065f25e833ff1/tensorflow/core/common_runtime/pluggable_device/pluggable_device_plugin_init.ccL75, which is what I think is what want to call. Is there a python binding for this function or am I supposed to call it directly? Jax has something like this listed in step 3 here: https://github.com/openxla/xla/blob/b3745173ea3d15306663c8a6f2124da8efea4289/xla/pjrt/c/docs/pjrt_integration_guide.md Is there something similar for TensorFlow? Thanks for the help.  Standalone code to reproduce the issue   Relevant log o",2024-03-13T21:10:37Z,stat:awaiting tensorflower type:support comp:core TF 2.13,open,0,0,https://github.com/tensorflow/tensorflow/issues/63655
rag,QuantumCoder4,Incorrect result when using tf.norm to conduct p-norm computation on float16 precision," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? tf.norm outputs incorrect result under float16 computation when the ord is large. Here is the code to reproduce:  Instead, if I change the dtype to 'float32', tf.norm outputs `inf`. It seems that tf.norm overflows during this computation https://github.com/tensorflow/tensorflow/blob/9ce1d5f2f7e5e8866a55633c182cc2dda1a2d6b8/tensorflow/python/ops/linalg_ops.pyL784 Although fixing this issue by letting tf.norm also outputs `inf` in `float16` (which is consistent with the result in `float32`. I am suggesting another possible fix according to this post https://timvieira.github.io/blog/post/2014/11/10/numericallystablepnorms/ Indeed, the original implementation may not be numerical stable, a more stable one is to normalize the input before doing the actual computation. Here is the naive implementation of original tf.norm and t",2024-03-13T19:26:58Z,stat:awaiting tensorflower type:bug comp:ops TF 2.15,open,0,2,https://github.com/tensorflow/tensorflow/issues/63643, Thank you for raising this issue. Could you please have a look at this gist and confirm the results? Thank you!,"Yes, I confirm the result. I would be appreciated if you could fix it."
yi,MoFHeka,How can I exit the XLAControlFlowContext when inside a jit_compile tf.function? Exit() function take no effect.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.15  Custom code Yes  OS platform and distribution Ubuntu 22.04  Python version 3.10  CUDA/cuDNN version CUDA12.3/cuDNN9.0  GPU model and memory GTX4090 24GiB  Current behavior? I have a custom op that is a normal OpKernel unable to be compile by XLA cluster. But unfortunately, when the user use this custom op, they have to wrap it into a tf.funtion. So somehow when the use Keras jit_compile or something else, errors happened.  Standalone code to reproduce the issue   Relevant log output ",2024-03-13T18:29:08Z,stat:awaiting tensorflower type:bug comp:xla comp:tf.function TF 2.15,open,0,3,https://github.com/tensorflow/tensorflow/issues/63632,"Hi **** , Sorry for the delay, I tried to run your code on colab using tf 2.15, 2.16 and tfnightly versions faced same issue. Please find the gist here for reference. We need to work on it, In few days i will provide a solution for it. Thank you!","Hi **** ,  The ctx.Exit() and ctx.Enter() methods are internal and their direct manipulation is generally not advisable due to potential instability and undefined behavior.  I tried alternative approaches manually. I hope it will use for you. Here i am providing gist for your reference. Thank you!", I'm afraid this proposal is not suitable for me as I am the developer of tensorflow recommender addon. I wanted to be able to be compatible with keras full graph XLA training without rewriting the new XlaOpKernel code.
llm,copybara-service[bot],#tf-data Test symbolic checkpoints in BatchGlobalShuffleCheckpointTest.,tfdata Test symbolic checkpoints in BatchGlobalShuffleCheckpointTest. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/9319 from jarosevcik:nocublasortritonforsmallmatmuls 690bb0e1d8b5894f64134d68a031d387399c638f,2024-03-13T17:03:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63628
llm,copybara-service[bot],[XLA:GPU] Replace TF_RET_CHECKs with return Status.,[XLA:GPU] Replace TF_RET_CHECKs with return Status. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/9319 from jarosevcik:nocublasortritonforsmallmatmuls 690bb0e1d8b5894f64134d68a031d387399c638f,2024-03-13T14:28:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63620
rag,Supriob9,"""Loss/regularization_loss' and ""Loss/classification_loss"" are high is high during the training of a object detection model with ""ssd_mobilenet_v2_320x320_coco17_tpu-8"""," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.10  Custom code Yes  OS platform and distribution windows 10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 11.8, 8.1  GPU model and memory Nvidia Quadro p3200  Current behavior? ""Loss/regularization_loss' is high during the training of a object detection model with ""ssd_mobilenet_v2_320x320_coco17_tpu8"".  !Screenshot 20240313 120121 How can I reduce the loss. Specially the regularization loss and classification_loss are high. I have tried reducing the learning rate to .08 and increasing the batch size to 24. I have added my configurations here.  Standalone code to reproduce the issue   Relevant log output ",2024-03-13T11:04:23Z,stat:awaiting response stale comp:model subtype:windows type:performance TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/63602,"Hi **** , Sorry for the delay, Experiment with different learning rates. Instead of directly setting it to 0.08, try a range of values and monitor the impact on loss. Increase the variety and intensity of data augmentation techniques. This can help the model generalize better and reduce overfitting, potentially reducing regularization loss. Experiment with different base feature extractors or backbone architectures. You can try variations of MobileNet, EfficientNet, or other architectures to see if they improve performance. Adjust the hyperparameters of the feature extractor, such as depth multiplier and minimum depth, to find optimal settings for your dataset. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,copybara-service[bot],PR #10050: [ROCm] Don't use dsabled nodes as command buffer barriers,PR CC(Function img_to_array() gives error for JPEG on Windows): [ROCm] Don't use dsabled nodes as command buffer barriers Imported from GitHub PR https://github.com/openxla/xla/pull/10050 Using disabled kernel node as a root of the graph may cause stream to desynchronize. Copybara import of the project:  d70da0531db5e9777765cceadb648c5a1c4e6309 by Dragan Mladjenovic : [ROCm] Don't use dsabled nodes as command buffer barriers Using disabled kernel node as a root of the graph may cause stream to desynchronize. Merging this change closes CC(Function img_to_array() gives error for JPEG on Windows) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10050 from ROCm:cmd_barrier d70da0531db5e9777765cceadb648c5a1c4e6309,2024-03-12T21:53:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63559
llm,iamthebot, TypeError: this __dict__ descriptor does not support '_DictWrapper' objects during trivial model save," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No (bug doesn't exist in tfnightly 2.17.0.dev20240312)  Source source  TensorFlow version v2.16.1  Custom code Yes  OS platform and distribution OSX  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When calling ` tf.saved_model.save(model, saved_model_path)` we see:  I suspect this is related to CC(TypeError: this __dict__ descriptor does not support '_DictWrapper' objects) which was supposedly fixed. However, in 2.16.1 TF removes the pin on wrapt and the issue indeed persists. I've even tried downgrading wrapt to 1.14.1 and the issue remains.  Standalone code to reproduce the issue  shell Traceback (most recent call last):   File ""/Users/alfredo_luque/repos/git.musta.ch/airbnb/bigheadservice/packages/mlframeworks/tensorflow/tests/minimal_repro.py"", line 44, in      tf.saved_model.save(online_tower_model, tmpdirname)   File ""/Users/alfredo_luque/miniforge3/envs/localbigheadv0.0.1/lib/python3.12/sitepackages/tensorflow/python/saved_model/save.py"", line 1392, in save     save_and_retu",2024-03-12T19:52:24Z,stat:awaiting tensorflower type:bug comp:model TF 2.16,open,7,17,https://github.com/tensorflow/tensorflow/issues/63548,I can verify this bug is not present in 2.15.1. Nor is it present in `tfnightly==2.17.0.dev20240312` with `kerasnightly==3.1.0.dev2024031203`,"Same issue, even simpler example:  Python: 3.12.2  TensorFlow: 2.16.0rc0, 2.16.1, and 2.17.0dev20240312   TensorFlow 2.16.0rc0 and 2.16.1 are the only `tensorflow` versions currently available using pip install.",I am having the exact same issue. , yeah the only workaround we have right now is `tfnightly` to enable model saving w/ py3.12... but that's obviously not a viable workaround for production use.,Running into a similar issue.,"Hi **** , Sorry for the delay, I tried to run your code on Colab using TF v2.15 and 2.16, But I don't faced any issue here. Could you check again. Please find the gist here for reference. Thank you!"," I have similar issue and I think the issue is with python 3.12, with python 3.11 it works","I confirm this to be linked to python 3.12. I tested tensorflow 2.16, 2.16.1 and 2.17.0.dev20240317. There problem occured with all of them.", yep looks like Python 3.12 is required to reproduce this issue. Your collab is running Python 3.10.,By the way disabling wrapt by setting an environment variable `WRAPT_DISABLE_EXTENSIONS` (as hinted by wrapt developer https://github.com/GrahamDumpleton/wrapt/issues/231issuecomment1455800902) will make the scripts run as expected:  It seems to be caused by a fix in the C extension of wrapt which was not adopted yet by tensorflow: https://github.com/GrahamDumpleton/wrapt/issues/231,same issue for me... ,">  yep looks like Python 3.12 is required to reproduce this issue. Your collab is running Python 3.10. it also happens using newer versions of typing_extensions on <3.12. Happened for me on 3.9 with typing_extensions=4.12, even on tensorflow 2.15 and wrapt < 1.15. Is this issue being looked into? ","FYI, same issue here:  python==3.12.4, tf==2.16.2 gives this error, but python==3.11.9, tf==2.15.0 works fine.","Same issue with: TypeError: this __dict__ descriptor does not support '_DictWrapper' objects This happened when trying to follow the SAC agent tutorial on the tf agents website. Particularly, the ""Learners"" bit.  I use a custom environment, python == 3.10.12, tf == 2.14.0. wrapt ==1.14.1","Facing issue with tf=2.17 (installed from source) and python 3.12.6 in **tf.saved_model.save(model,path)** typingextensions:4.12.2, protobuf(python):4.21.9 I need to save model so that I can load(and use) it using cppapi. ",Do you know which is the minimum version of typingextensions that doesn't cause this?,"New pc with tf 2.18 + Python 3.12.7 + wrapt 1.14.1 + typingextensions 4.11.0 +protobuf  4.25.3 , have the same issue, but another pc with tf 2.17 + Python 3.12.4 + wrapt 1.14.1 ""saved_model.save"" works fine. I checked typingextensions 4.11.0, protobuf  3.20.3"
rag,copybara-service[bot],hlo_reachability: reserve indices' storage at Build() time,hlo_reachability: reserve indices' storage at Build() time name                             old time/op  new time/op   delta BM_HloReachabilityBuild/1         165ns ± 0%    167ns ± 0%   +0.88%  (p=0.016 n=5+4) BM_HloReachabilityBuild/64       6.27µs ± 1%   5.25µs ± 1%  16.18%  (p=0.008 n=5+5) BM_HloReachabilityBuild/128      12.9µs ± 3%   11.0µs ± 4%  15.11%  (p=0.008 n=5+5) BM_HloReachabilityBuild/256      27.3µs ± 2%   23.4µs ± 2%  14.06%  (p=0.008 n=5+5) BM_HloReachabilityBuild/512      60.0µs ± 2%   51.9µs ± 1%  13.55%  (p=0.008 n=5+5) BM_HloReachabilityBuild/4096      974µs ± 1%    901µs ± 1%   7.45%  (p=0.008 n=5+5) BM_HloReachabilityBuild/32768    42.5ms ± 1%   41.1ms ± 2%   3.17%  (p=0.008 n=5+5) BM_HloReachabilityBuild/262144    5.19s ±50%  11.10s ±159%     ~     (p=0.548 n=5+5),2024-03-12T17:33:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63524
yi,Rajcr2,I cannot understand this AttributeError : module 'tensorflow.python.ops.control_flow_ops' has no attribute 'case', Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.15.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? AttributeError: module 'tensorflow.python.ops.control_flow_ops' has no attribute 'case'. I clone this Git model  https://github.com/tensorflow/models.git and got this error anyone please tell solution. Thank You.  Standalone code to reproduce the issue   Relevant log output _No response_,2024-03-12T15:43:58Z,stat:awaiting response type:bug stale comp:ops TF 2.15,closed,0,9,https://github.com/tensorflow/tensorflow/issues/63515,"Can you post the output of `pip list` please? It is likely you have mismatched packages. The error occurs from `control_flow_ops.case` where `control_flow_ops` is the module `tensorflow.python.ops.control_flow_ops` from TensorFlow, as it is imported by `tf_slim`. It means that the version of TF you have does not have a `case` symbol defined in this module, but the code from the `tf_slim` version you have is expecting one.","> Can you post the output of `pip list` please? It is likely you have mismatched packages. >  > The error occurs from `control_flow_ops.case` where `control_flow_ops` is the module `tensorflow.python.ops.control_flow_ops` from TensorFlow, as it is imported by `tf_slim`. It means that the version of TF you have does not have a `case` symbol defined in this module, but the code from the `tf_slim` version you have is expecting one.  is This info fine ? ","> is This info fine ? Yes, I just edited it to be properly wrapped in formatted text (useful when reading command output). You are using `tfslim` 1.1.0, which has the following on their README > Note: Latest version of TFSlim, 1.1.0, was tested with TF 1.15.2 py2, TF 2.0.1, TF 2.1 and TF 2.2. However, you have `tensorflow2.15.0`. This is a difference of nearly 34 years between the latest tested version and the version you are using. TF code has changed during this time and the `case` symbol is no longer present","> > is This info fine ? >  > Yes, I just edited it to be properly wrapped in formatted text (useful when reading command output). >  > You are using `tfslim` 1.1.0, which has the following on their README >  > > Note: Latest version of TFSlim, 1.1.0, was tested with TF 1.15.2 py2, TF 2.0.1, TF 2.1 and TF 2.2. >  > However, you have `tensorflow2.15.0`. This is a difference of nearly 34 years between the latest tested version and the version you are using. TF code has changed during this time and the `case` symbol is no longer present Thanks. What should i do now to solve this attribute error. do i need to upgrade this tfslim package. using pip install upgrade tf_slim","There is no newer version of `tfslim`. So you either have to _down_grade TF (install TF 2.2 and `tfslim` in a new virtual environment) or find an alternate library. Note: There is a hacky approach where you'd hack into your local copy of `tfslim` to change the code around the error, but for this you need to be very sure you know what you are doing, this voids any guarantees, makes it impossible to debug future issues you might encounter. But if the proper approaches above don't work, this is something you could try. In any case, I suggest moving this to tf_slim if you want to keep using it, it is not relevant to TF.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"I am facing an error while training a TensorFlow custom object detection model. Initially, I started with TensorFlow 2.13.0, but the GPU was not detected. Therefore, I upgraded to TensorFlow 2.15.0. File ""/usr/local/lib/python3.10/distpackages/object_detection/data_decoders/tf_example_decoder.py"", line 556, in decode  *     tensors = decoder.decode(serialized_example, items=keys) File ""/usr/local/lib/python3.10/distpackages/tf_slim/data/tfexample_decoder.py"", line 722, in decode  *     outputs.append(handler.tensors_to_item(keys_to_tensors)) File ""/usr/local/lib/python3.10/distpackages/tf_slim/data/tfexample_decoder.py"", line 405, in tensors_to_item  *     return self._decode(image_buffer, image_format) File ""/usr/local/lib/python3.10/distpackages/tf_slim/data/tfexample_decoder.py"", line 453, in _decode  *     image = control_flow_ops.case( AttributeError: module 'tensorflow.python.ops.control_flow_ops' has no attribute 'case'` "
yi,alkaf-ahmed,Error : (GATHER) failed to prepare,"Hello,  I'm working on colab and I'm trying to adapt the following tutorial on TFLite : https://www.tensorflow.org/lite/performance/post_training_integer_quant  In the tutorial, the MNIST dataset is used, so when making a ""representative_dataset "" function, the following function is used :   I completly undersatnd why this is done (at least I hope I'm) so I tried to adapt the tutorial to another NN on regression which is vary basic but here the input to the NN so the ""representative_dataset "" function will be different as well.  I'm trying to do that just so I undersatnd well how to work with TFLite, hence the simple NN that doesn't necessarily serve at anything.   ML code Here's the code for the data and the learning :     Code causing the error  Here's where I have a problem :    Error logs Here's the error for **Approach 1** :   Here's the error for **Approach 2** :   In the doc's it's advised to use the second approach to make the function :  https://www.tensorflow.org/lite/performance/post_training_quantization  But when I did that I just got `KeyError: 'dense_74_input' `. So any idea where I missed up ?  Thanks for any help ",2024-03-12T15:42:35Z,stat:awaiting response stale comp:lite comp:ops TFLiteConverter,closed,0,3,https://github.com/tensorflow/tensorflow/issues/63514,"Hi ahmed, I have reproduced the issue with TF 2.15. It is observed that the issue is with the representative data. Make sure the key you're using to yield data in your representative_data_gen_reg function matches the actual input name in your model. You need to reshape the input. Please try this fix  Thank You",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,copybara-service[bot],[XLA:Python] Fix build of xla/python on Windows.,"[XLA:Python] Fix build of xla/python on Windows. * ssize_t isn't a Windows platform type. Use npy_intp or Py_hash_t, depending on the context. * nanobind needs to be built with DNB_BUILD=1 on Windows.",2024-03-12T13:40:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63508
rag,copybara-service[bot],PR #10097: [ROCm] Allow host buffer allocation in GpuExecutor::Allocate,PR CC(Windows: Make TensorFlow build with Bazel again): [ROCm] Allow host buffer allocation in GpuExecutor::Allocate Imported from GitHub PR https://github.com/openxla/xla/pull/10097 Copybara import of the project:  79a89bec3a27e37fdd483315c1ea7e855b988adb by Dragan Mladjenovic : [ROCm] Allow host buffer allocation in GpuExecutor::Allocate Merging this change closes CC(Windows: Make TensorFlow build with Bazel again) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10097 from ROCm:host_alloc 79a89bec3a27e37fdd483315c1ea7e855b988adb,2024-03-12T10:06:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63493
gemma,copybara-service[bot],PR #10011: [ROCM] cuSolver API fixes 27/02/24,"PR CC(Cherrypick RNN migration CLs into r1.2): [ROCM] cuSolver API fixes 27/02/24 Imported from GitHub PR https://github.com/openxla/xla/pull/10011 In this PR, we fix ROCM build after the recent cuSolver API changes from https://github.com/openxla/xla/commit/dad57bd0d943e08795b3cd55dd27fb4abe0cd9c4 Also a small rocblas fix after the recent refactoring. rotation: would you please have a look ? Copybara import of the project:  6d7265ba17be640da97c87d256919dadb4fe4d71 by Chao Chen : use potrfDSCZ  d965fd3a817838c4b7440868afb6884ea011b078 by Pavel Emeliyanenko : fixing use_hgemm_alt_impl_ flag Merging this change closes CC(Cherrypick RNN migration CLs into r1.2) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10011 from ROCm:ci_cusolver_fix_240227 d965fd3a817838c4b7440868afb6884ea011b078",2024-03-12T00:11:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63465
llm,copybara-service[bot],hlo_reachability: make operator|= friendly to auto-vectorization,"hlo_reachability: make operator|= friendly to autovectorization This vectorizes the OR loop, at least with clang trunk on AVXcapable x86 CPUs. name                                  old time/op  new time/op  delta BM_HloReachabilityBitSetUnion/1       1.50ns ± 0%  2.99ns ± 0%  +99.47%  (p=0.008 n=5+5) BM_HloReachabilityBitSetUnion/64      1.50ns ± 1%  2.99ns ± 0%  +99.51%  (p=0.008 n=5+5) BM_HloReachabilityBitSetUnion/128     2.62ns ± 0%  3.74ns ± 0%  +42.96%  (p=0.008 n=5+5) BM_HloReachabilityBitSetUnion/256     4.86ns ± 0%  5.23ns ± 0%   +7.54%  (p=0.008 n=5+5) BM_HloReachabilityBitSetUnion/512     9.35ns ± 0%  4.31ns ± 9%  53.92%  (p=0.008 n=5+5) BM_HloReachabilityBitSetUnion/4096    72.2ns ± 0%  19.8ns ± 0%  72.52%  (p=0.008 n=5+5) BM_HloReachabilityBitSetUnion/32768    589ns ± 0%   152ns ± 0%  74.22%  (p=0.016 n=4+5) BM_HloReachabilityBitSetUnion/262144  4.71µs ± 1%  1.84µs ± 0%  61.01%  (p=0.016 n=4+5) BM_HloReachabilityBuild/1              165ns ± 1%   164ns ± 0%     ~     (p=0.343 n=4+4) BM_HloReachabilityBuild/64            6.28µs ± 3%  6.37µs ± 4%     ~     (p=0.548 n=5+5) BM_HloReachabilityBuild/128           13.1µs ± 5%  13.0µs ± 1%     ~     (p=0.690 n=5+5) BM_HloReachabilityBuild/256       ",2024-03-11T23:31:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63463
rag,Tanishq-JM,Could not load library cudnn_cnn_infer64_8.dll. Error code 126," Issue type bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.10.0  Custom code Yes  OS platform and distribution windows 10  Mobile device Thinkpad p53  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 11.2 / 11.2(8.1 I think)  GPU model and memory Nvidia T2000 4GB  Current behavior?  Problem : I expect this problem to be resolved soon. I am unable to use my GPU for training, as it remains at 0% usage. When I try another version, it only uses my CPU, which takes days and causes my computer (laptop) to overheat. `python code ` import tensorflow as tf import subprocess physical_devices = tf.config.list_physical_devices('GPU') print(""Num GPUs Available:"", len(physical_devices),physical_devices) print(f""tensor version : {tf.__version__}"") def get_gpu_info():     try:         result = subprocess.run(['nvidiasmi'], stdout=subprocess.PIPE)         return result.stdout.decode('utf8')     except FileNotFoundError:         return ""nvidiasmi not found. Make sure NVIDIA drivers are installed."" gpu_info = get_gpu_info() print(gpu_info) if physical_devices:     for i, gpu in enumerate(physical_devices):         p",2024-03-11T22:08:44Z,stat:awaiting response type:support stale subtype:windows TF 2.10,closed,0,10,https://github.com/tensorflow/tensorflow/issues/63455,"Hi **JM** , Update GPU drivers:  Ensure that you have the latest NVIDIA drivers installed for your gpu(Nvidia Quadro T2000). You can download the latest drivers from the NVIDIA website and follow the installation instructions. Check CUDA and cuDNN installation: Make sure that CUDA and cuDNN are installed correctly and that the paths to their binaries are included in your system's PATH environment variable. Verify TensorFlow installation:  Reinstall TensorFlow using pip to ensure that it's correctly installed and compatible with your system configuration. You can do this by running:  Here i am providing gist for your reference, Once go through it. Thank you!"," Thank You  for your guidance i installed VS and Nvidia graphic drivers . But The problem is still there when i am using another version of tensorflow it uses CPU instead  but TF 2.1 needs some .dll file and its unable to access it . code  `import tensorflow as tf import subprocess from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) def get_gpu_info():     try:         result = subprocess.run(['nvidiasmi'], stdout=subprocess.PIPE)         return result.stdout.decode('utf8')     except FileNotFoundError:         return ""nvidiasmi not found. Make sure NVIDIA drivers are installed."" from keras import layers, models, datasets import time (train_images, train_labels), (_, _) = datasets.mnist.load_data() train_images = train_images.reshape(1, 28, 28, 1).astype('float32') / 255.0 model = models.Sequential([     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),     layers.MaxPooling2D((2, 2)),     layers.Conv2D(64, (3, 3), activation='relu'),     layers.MaxPooling2D((2, 2)),     layers.Conv2D(64, (3, 3), activation='relu'),     layers.Flatten(),     layers.Dense(64, activation='relu'),     layers.Dense(10, activation='softmax') ]) model.compile(optimizer='adam',               loss='sparse_categorical_crossentropy',               metrics=['accuracy']) start_time = time.time() end_time = start_time + 30 while time.time()  & d:/python/ML/Scripts/python.exe d:/python/test.py 20240314 11:50:53.845372: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 20240314 11:50:55.378833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 2128 MB memory:  > device: 0, name: Quadro T2000, pci bus id: 0000:01:00.0, compute capability: 7.5 [name: ""/device:CPU:0"" device_type: ""CPU"" memory_limit: 268435456 locality { } incarnation: 12036372220610172862 xla_global_id: 1 , name: ""/device:GPU:0"" device_type: ""GPU"" memory_limit: 2232156160 locality {   bus_id: 1   links {   } } incarnation: 1063671899388078998 physical_device_desc: ""device: 0, name: Quadro T2000, pci bus id: 0000:01:00.0, compute capability: 7.5"" xla_global_id: 416903419 ] 20240314 11:50:55.686954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2128 MB memory:  > device: 0, name: Quadro T2000,  pci bus id: 0000:01:00.0, compute capability: 7.5 Thu Mar 14 11:50:55 2024   !image 20240314 11:50:57.159136: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600 Could not load library cudnn_cnn_infer64_8.dll. Error code 126 Please make sure cudnn_cnn_infer64_8.dll is in your library path! (ML) PS D:\python> Here in the Photo is showing a process is present and there is some GPU getting used at start .  where i am doing mistake ? > ` Environment variables  ` C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\extras\CUPTI\lib64 C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\extras\CUPTI\include D:\Tanishq games\google install\New folder\cuda\bin D:\Tanishq games\google install\New folder\cuda\include >` Cuda == 11.2 and cudnn == 8.1 (11.2)`","Hi **JM** , Could you please confirm if this issue is resolved for you ? Please feel free to close the issue if it is resolved. Thank ypu!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"I am encountering the same error: 20240903 17:30:41.022935: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700.08x Could not load library cudnn_cnn_infer64_8.dll. Error code 126 using ""/tfjsnodegpu"": ""^4.20.0"" cuDNN 9.3.0 CUDA Toolkit 12.6 Python 3.11.9 NVIDIA driver 536.23 20240903 17:30:20.374506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1403 MB memory: > device: 0, name: GeForce GT 710, pci bus id: 0000:01:00.0, compute capability: 3.5",can u share more info?,"I receive an image in the buffer and want to decode the image and create predictions. If I use /tfjsnode, when the load goes to the CPU, everything works. But if I try to use /tfjsnodegpu, I get this error: [Nest] 10516   03.09.2024, 17:44:04     LOG [PredictionService] Captured frame buffer [Nest] 10516   03.09.2024, 17:44:04     LOG [PredictionService] 2146002 Loading model... 20240903 17:44:09.279364: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700.87x Could not load library cudnn_cnn_infer64_8.dll. Error code 126 Please make sure cudnn_cnn_infer64_8.dll is in your library path! Here is my code. import { parentPort } from 'worker_threads'; import * as cocoSsd from 'models/cocossd'; import * as tf from '/tfjsnodegpu'; let model; async function loadModel() {   model = await cocoSsd.load(); } async function decodeImageToTensor(frameBuffer) {   if (!Buffer.isBuffer(frameBuffer)) {     throw new Error('frameBuffer is not a Buffer');   }   try {     const imageTensor = tf.node.decodeImage(frameBuffer);     console.log('decodedImage', imageTensor);     return imageTensor;   } catch (error) {     console.error('Error decoding image:', error.message);     throw error;   } } parentPort.on('message', async (message) => {   const frameBuffer = Buffer.from(message.frameBuffer.buffer);   const cameraPort = message.cameraPort;   if (!Buffer.isBuffer(frameBuffer)) {     console.error('frameBuffer is not a Buffer:', frameBuffer);     parentPort.postMessage({ error: 'frameBuffer is not a Buffer' });     return;   }   try {     if (!model) {       console.log('Loading model...');       await loadModel();       console.log('Model loaded successfully');     }     console.log('Decoding image to tensor...');     const imageTensor = await decodeImageToTensor(frameBuffer);     console.log('Image tensor decoded successfully');     const predictions = await model.detect(imageTensor);     imageTensor.dispose();     // Формируем данные для отправки     const response = predictions.map((prediction) => ({       bbox: prediction.bbox,       class: prediction.class,       score: prediction.score,     }));     parentPort.postMessage({ [cameraPort]: response });   } catch (error) {     console.error('Error in worker:', error.message);     parentPort.postMessage({ error: error.message });   } });",Ok have you installed correct cudnn version and graphic drivers? `you need to make sure that the dependencies are compatible and then.` if you have done so then try with microsoft Visual studios packages which is missing in your system. it didn't work for me in old computer but installing everything from scratch does the job.
rag,copybara-service[bot],PR #10097: [ROCm] Allow host buffer allocation in GpuExecutor::Allocate,PR CC(Windows: Make TensorFlow build with Bazel again): [ROCm] Allow host buffer allocation in GpuExecutor::Allocate Imported from GitHub PR https://github.com/openxla/xla/pull/10097 Copybara import of the project:  79a89bec3a27e37fdd483315c1ea7e855b988adb by Dragan Mladjenovic : [ROCm] Allow host buffer allocation in GpuExecutor::Allocate Merging this change closes CC(Windows: Make TensorFlow build with Bazel again) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10097 from ROCm:host_alloc 79a89bec3a27e37fdd483315c1ea7e855b988adb,2024-03-11T20:40:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63442
yi,copybara-service[bot],Indexing analysis: move utility for epilogue indexing to main library.,"Indexing analysis: move utility for epilogue indexing to main library. This is needed for transpose, reduce, concat.",2024-03-11T12:51:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63406
rag,copybara-service[bot],PR #9873: [ROCm] Don't use CUDA PTX for ROCM in ComputationIdCmd,PR CC(Can't not find the tutorial code for Convolutional Neural network): [ROCm] Don't use CUDA PTX for ROCM in ComputationIdCmd Imported from GitHub PR https://github.com/openxla/xla/pull/9873 Copybara import of the project:  818077159230e06ce8e94b3c556d1d68fa125b09 by Dragan Mladjenovic : [ROCm] Don't use CUDA PTX for ROCM in ComputationIdCmd Merging this change closes CC(Can't not find the tutorial code for Convolutional Neural network) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/9873 from ROCm:comp_id 818077159230e06ce8e94b3c556d1d68fa125b09,2024-03-11T11:32:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63403
rag,copybara-service[bot],PR #10082: [ROCm] Enable GpuCommandBuffer::Trace,PR CC(Automatically convert inputs to tensors in Dataset.from_tensor_slices): [ROCm] Enable GpuCommandBuffer::Trace Imported from GitHub PR https://github.com/openxla/xla/pull/10082 Copybara import of the project:  e7c8de8fcdadd7bf9b02e4e79afe941afc35d190 by Dragan Mladjenovic : [ROCm] Enable GpuCommandBuffer::Trace  b66e16f53a9a6d6a44f51439a5b39e2670effaf7 by Dragan Mladjenovic : [ROCm] Allow capturing operations to command buffer that require tracing Merging this change closes CC(Automatically convert inputs to tensors in Dataset.from_tensor_slices) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10082 from ROCm:cmmand_trace b66e16f53a9a6d6a44f51439a5b39e2670effaf7,2024-03-11T11:19:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63402
rag,vwrewsge,Aborted (core dumped) with tf.raw_ops.AvgPoolGrad, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.15  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? core dumped error with specific input parameters.  Standalone code to reproduce the issue   Relevant log output ,2024-03-09T14:54:40Z,type:bug comp:ops TF 2.15,open,0,1,https://github.com/tensorflow/tensorflow/issues/63353, I was able to replicate the error reported here. Thank you!
yi,SuryanarayanaY,Fix Checkfail in MatrixDiagV3,The Op raw_ops.MatrixDiagV3 terminates due to checkfail when the input `diagonal` is of `rank=1` and `k[0] != k[1]`. This happens at this line below which is trying to access `diagonal_shape.dim_size(1)` when rank=1 . Currently API will not work with Rank=1 and k[0] != k[1] due to this check fail. https://github.com/tensorflow/tensorflow/blob/e193d8ea7776ef5c6f5d769b6fb9c070213e737a/tensorflow/core/kernels/linalg/matrix_diag_op.ccL241 Proposed a fix for same. Fixes CC(C++ API `MatrixDiagV3` aborts with tensor type of `k`) .,2024-03-09T10:08:01Z,ready to pull size:S prtype:bugfix comp:core,closed,0,2,https://github.com/tensorflow/tensorflow/issues/63344,This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,Done the suggested changes.
llm,copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10388 from Tixxx:tixxx/coll_matmul_fix d55af60f1335d82fc9c32d0dfb34708cb32174a3,2024-03-09T10:00:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63343
llm,copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10388 from Tixxx:tixxx/coll_matmul_fix d55af60f1335d82fc9c32d0dfb34708cb32174a3,2024-03-09T09:57:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63342
yi,hugolden,GPU not detected on WSL2," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.16.1  Custom code No  OS platform and distribution WSL2 Ubuntu 22.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi, I am trying to install run keras3 on wsl2.  But the output of device listing indicates that there is still some dynamic library missing. I followed the doc to finish the installation  I am using the latest NV driver, version of which can be find on nvidiasmi output. I have managed to install CUDA 11.8 and cudnn 8.6 on my device which are the exact the same version listed on doc. Asides from installing cudnn using Local Installer for Ubuntu22.04 x86_64 (Deb) on this page, I have also tried to manually copy the cudnn header files and lib files to cuda11.8/include and cuda11.8/lib and the  LD_LIBRARY_PATH is updated  according to this artical and this one Anything I missed in my steps? Regards Sichao Hu  Standalone code to reproduce the issue   Relevant log output ",2024-03-09T09:42:14Z,type:build/install,closed,12,22,https://github.com/tensorflow/tensorflow/issues/63341,"I've gotten the same problem but with python3.12. I found it was a problem with environment variables and finally fixed it. My solution is (if in pwsh):  Make sure cuda and cudnn are installed correctly.  Make sure `$Env:CUDA_PATH` is given correctly  Make sure  `$Env:LD_LIBRARY_PATH` is given correctly  Make sure `$Env:TF_CUDA_PATHS` is given correctly  Use `pip install tensorflow[andcuda]` instead of `pip install tensorflow`. Because the extensions, such as  `nvidiacuda*` and  `nvidiacudnn*` from pip(Pypi) are also needed.",  thank you for sharing. what path shoud I set for those environment variables ? Make sure $Env:CUDA_PATH is given correctly Make sure $Env:LD_LIBRARY_PATH is given correctly Make sure $Env:TF_CUDA_PATHS is given correctly,">  thank you for sharing. what path shoud I set for those environment variables ? Make sure $Env:CUDA_PATH is given correctly Make sure $Env:LD_LIBRARY_PATH is given correctly Make sure $Env:TF_CUDA_PATHS is given correctly That depends on your specific installation method of cuda and cudnn. For me, since I use miniconda and conda Installation, so my setting is as:  Actually, either package manager installation or conda Installation, the `$Env:CUDA_PATH` should be such a root path where you can see cuda's installed directories, such as `./nvvm`, `./nvvm`, `./extras/Debugger/lib64/libcudacore.a` and so on. And, `$Env:TF_CUDA_PATHS` should be equal to `$Env:CUDA_PATH`.","> >  thank you for sharing. what path shoud I set for those environment variables ? Make sure $Env:CUDA_PATH is given correctly Make sure $Env:LD_LIBRARY_PATH is given correctly Make sure $Env:TF_CUDA_PATHS is given correctly >  > That depends on your specific installation method of cuda and cudnn. For me, since I use miniconda and conda Installation, so my setting is as: >  >  >  > Actually, either package manager installation or conda Installation, the `$Env:CUDA_PATH` should be such a root path where you can see cuda's installed directories, such as `./nvvm`, `./nvvm`, `./extras/Debugger/lib64/libcudacore.a` and so on. And, `$Env:TF_CUDA_PATHS` should be equal to `$Env:CUDA_PATH`. Does your setup detect GPU with tensorflow 2.16.1? mine works with 2.15 but not this one.","shahrokhi Yes, since I use python3.12 and only tensorflow2.16.1 that can be installed by pip. Maybe you should check your cuda and cudnn version that satisfied tensorflow2.16.1's requirements. "," Thank you for your comment. Regarding the cudnn and cuda version, I searched around and finally found a matrix here.  I will update my result once I have installed newer cuda and cudnn.",">  Thank you for your comment. >  > Regarding the cudnn and cuda version, I searched around and finally found a matrix here. >  > I will update my result once I have installed newer cuda and cudnn. Thank you guys.  After I have installed cuda12.3+cudnn8.9 ( I am using TF 2.16 )and set those environment paths following  's guide, GPU can be successfully detected under wsl2(Ubuntun 22.04). Hope this ticket helps those who are looking for answers.",Are you satisfied with the resolution of your issue? Yes No,i install cuda 12.3.2 and cudnn .then when i install tensorflow via `pip install tensorflow[andcuda]` will auto install`tensorflow 2.16.1` and can not find my GPU. when i input `pip install tensorflow==2.15` . It will autoremove 2.16.1 and install 2.15.then I can find my GPU   bash output ,"I am stuck with the same issue while upgrading to Tensorflow 2.16 within an Anaconda environment, with an hope to get started with Keras 3.0 released by   I am using Ubuntu on WSL2 and have never set any path variables explicitly starting from TF 2.15 on different machine **Starting from TF 2.15, this process was automated, but again we are back (at least for the moment)** TensorFlow team might've missed that setting where they set path when the virtual environment is activated, original motivation was to have different version of CUDA and cuDNN installed in different virtual environments for different TF versions. I am hopeful this will be fixed soon gardener ","**Listing when just downgraded to TF 2.15 in same environment**  (py3) chaudhary:~$ python3 c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"" 20240314 17:54:21.028849: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20240314 17:54:21.028973: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20240314 17:54:21.029714: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20240314 17:54:21.034104: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20240314 17:54:21.620019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20240314 17:54:23.291920: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node Your kernel may have been built without NUMA support. 20240314 17:54:23.317718: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node Your kernel may have been built without NUMA support. 20240314 17:54:23.317830: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node Your kernel may have been built without NUMA support. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]  **The GPU device cannot be detected however for Tensorflow 2.16**  (py3) chaudhary:~$ python3 c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"" 20240314 17:49:42.014773: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20240314 17:49:42.753522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20240314 17:49:43.943419: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node Your kernel may have been built without NUMA support. 20240314 17:49:43.943633: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... [] ","hi guys, this is what works for me. Installation summary, following build configs https://www.tensorflow.org/install/sourcegpu Wsl distro : Ubuntu22.04 Miniconda with python 3.10 Cuda toolkit 12.3 cuda Cudnn 8.9 use older version, cudnn 8.9 archive follow commands replace with 8.9 version : cmds Added these to .bashrc, and source it  video reference : https://www.youtube.com/watch?v=VE5OiQSfPLg result:  If you think it too much of hassle, i would recommend to use tensorflow/tensorflow:latestgpu .  I tried both ways, realized container way train the same model faster(likely due hardware configs) and setup time is quick with no issue.",". This weekend, I have shared my steps in this post. Maybe you can check it and transfer it to your own environment.","Thanks  and  for providing with workarounds. I certainly agree that these are ways to mitigate this problem  However, while following the pipinstallation guide I can see that  pip install tensorflow[andcuda]  do download many cuda libraries including cuDNN as well, which was set to path variables as soon as you activate a conda environment which is a very nice functionality up to TF 2.15 I believe the developers might've missed the same while moving to TF 2.16 Do let me know if you two agree?"," I agree with you. Actually, this should not occur, we, users should not need to install cuda and cudnn libraries twice. So, there must be some potential mistakes on optional dependency `[andcuda]`. I have done some ablation tests before, and I find even though I use conda to install cuda and manually install cudnn to my conda env, the optional dependency `[andcuda]` is still needed when installing TensorFlowGPU. ","> . This weekend, I have shared my steps in this post. Maybe you can check it and transfer it to your own environment.   Wow, thank you so much for this!"," Almost final and automated fix below * Where I found the resolution     * TF 2.16.1 Fails to work with GPUs         * Solution proposed by ""shshahrokhi"", improved by ""ChristofKaufmann""         * See specially Comment by COntributor     * Related Issues         * GPU not detected on WSL2, where I have post some comments         * Tensorflow WSL GPU CUDA recognition issue RTX3090         * Once gain: tf.2.16.1 fails to recognize GPUs     * Other mention on social media         * Reddit issue * Exact solution     * Temporary fix (after activating environment in which Tensorflow 2.16.1 is installed)                  export NVIDIA_DIR=$(dirname $(dirname $(python c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)"")))         export LD_LIBRARY_PATH=$(echo ${NVIDIA_DIR}/*/lib/  sed r 's/\s+/:/g')${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}                          * anaconda3/envs//etc/conda/deactivate.d/env_vars.sh                          !/bin/sh             unset NVIDIA_DIR             unset LD_LIBRARY_PATH                      * Official documentation to do this via conda.io         * Stackoverflow question where I got this Set environment vars when activating conda env * What else helped me     * How to list all variables names and their current values?     * Conda documenation for ""env_vars.sh"" for activate/deactivate",> i install cuda 12.3.2 and cudnn .then when i install tensorflow via `pip install tensorflow[andcuda]` will auto install`tensorflow 2.16.1` and can not find my GPU. when i input `pip install tensorflow==2.15` . It will autoremove 2.16.1 and install 2.15.then I can find my GPU >  >  >  > bash output >  >  This worked for me. Thanks !,"> > i install cuda 12.3.2 and cudnn .then when i install tensorflow via `pip install tensorflow[andcuda]` will auto install`tensorflow 2.16.1` and can not find my GPU. when i input `pip install tensorflow==2.15` . It will autoremove 2.16.1 and install 2.15.then I can find my GPU > >  > >  > >  > >      > >        > >      > >  > >        > >      > >  > >      > >    > > bash output > >  >  > This worked for me. Thanks ! Can you run import tensorflow by using IDE that install in same environment? I can not i can only run in terminal by typing python3 c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"".","> > > i install cuda 12.3.2 and cudnn .then when i install tensorflow via `pip install tensorflow[andcuda]` will auto install`tensorflow 2.16.1` and can not find my GPU. when i input `pip install tensorflow==2.15` . It will autoremove 2.16.1 and install 2.15.then I can find my GPU > > >  > > >  > > >  > > >      > > >        > > >      > > >  > > >        > > >      > > >  > > >      > > >    > > > bash output > > >  > >  > >  > > This worked for me. Thanks ! >  > Can you run import tensorflow by using IDE that install in same environment? I can not i can only run in terminal by typing python3 c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"". Yes, I was able to run my python scripts. You need to make sure the versions align. TensorFlow versions So for the **latest(compatiable) version**.  you need tensorflow2.16.1 cuDNN 8.9  the nvdia website guides you to install this for WSL CUDA 12.3 If you run `pip install tensorflow[andcuda] ` this will install the incorrect version of tensorflow, that is not compatible with cuDNN 8.9. So after the pip installation finishes, be sure to run pip install tensorflow==2.16.1",The latest version of cuDNN does not work with tensorflow. You need cuDNN 8.9 for version 2.16.1 of tensorflow TensorFlow Versions cuDNN 8.9 can be found here https://developer.nvidia.com/rdp/cudnnarchive You might need an account to access the download once you download the artifact. you can copy it into your wsl home directory and run `sudo dpkg i cudnnlocalrepoubuntu22048.9.7.29_1.01_amd64.deb` `sudo cp /var/cudnnlocalrepoubuntu22048.9.7.29/cudnn*keyring.gpg /usr/share/keyrings/` `sudo aptget update` `sudo aptget y install cudnn`,"> >  thank you for sharing. what path shoud I set for those environment variables ? Make sure $Env:CUDA_PATH is given correctly Make sure $Env:LD_LIBRARY_PATH is given correctly Make sure $Env:TF_CUDA_PATHS is given correctly >  > That depends on your specific installation method of cuda and cudnn. For me, since I use miniconda and conda Installation, so my setting is as: >  >  >  > Actually, either package manager installation or conda Installation, the `$Env:CUDA_PATH` should be such a root path where you can see cuda's installed directories, such as `./nvvm`, `./nvvm`, `./extras/Debugger/lib64/libcudacore.a` and so on. And, `$Env:TF_CUDA_PATHS` should be equal to `$Env:CUDA_PATH`. I installed cudatoolkit via `conda install` but can't seem to find any of the filles you listed. Any idea where I can find them? I am using anaconda though"
llm,copybara-service[bot],PR #10388: Address lost changes for pr 7854,PR CC(Update random_poisson_test): Address lost changes for pr 7854 Imported from GitHub PR https://github.com/openxla/xla/pull/10388 Copybara import of the project:  d55af60f1335d82fc9c32d0dfb34708cb32174a3 by TJ : Address lost changes for pr https://github.com/openxla/xla/pull/7854 Merging this change closes CC(Update random_poisson_test) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10388 from Tixxx:tixxx/coll_matmul_fix d55af60f1335d82fc9c32d0dfb34708cb32174a3,2024-03-09T07:54:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63340
llm,copybara-service[bot],PR #10388: Address lost changes for pr 7854,PR CC(Update random_poisson_test): Address lost changes for pr 7854 Imported from GitHub PR https://github.com/openxla/xla/pull/10388 Copybara import of the project:  d55af60f1335d82fc9c32d0dfb34708cb32174a3 by TJ : Address lost changes for pr https://github.com/openxla/xla/pull/7854 Merging this change closes CC(Update random_poisson_test) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10388 from Tixxx:tixxx/coll_matmul_fix d55af60f1335d82fc9c32d0dfb34708cb32174a3,2024-03-09T01:10:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63335
rag,copybara-service[bot],PR #10082: [ROCm] Enable GpuCommandBuffer::Trace,PR CC(Automatically convert inputs to tensors in Dataset.from_tensor_slices): [ROCm] Enable GpuCommandBuffer::Trace Imported from GitHub PR https://github.com/openxla/xla/pull/10082 Copybara import of the project:  e7c8de8fcdadd7bf9b02e4e79afe941afc35d190 by Dragan Mladjenovic : [ROCm] Enable GpuCommandBuffer::Trace  b66e16f53a9a6d6a44f51439a5b39e2670effaf7 by Dragan Mladjenovic : [ROCm] Allow capturing operations to command buffer that require tracing Merging this change closes CC(Automatically convert inputs to tensors in Dataset.from_tensor_slices) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10082 from ROCm:cmmand_trace b66e16f53a9a6d6a44f51439a5b39e2670effaf7,2024-03-08T21:36:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63315
yi,sumanth1989,Unable to  build the pip package after building the package builder," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.15  Custom code No  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version 6.1.0  GCC/compiler version clang 16  CUDA/cuDNN version 8.9  GPU model and memory NVIDIA A100SXM480GB   Current behavior? After running the bazel package builder i am trying to build the pip package when i see the following error. Also important to note that i had to supply the outputname and projectname flags (since they are mandatory). This is not reflected in the guide to build tf from source. I also dont understand what headers need to be supplied. I see a command line option to supply headers though for the whl build.  Traceback (most recent call last):   File ""/home/administrator/tensorinstall/tensorflow/./bazelbin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/tools/pip_package/build_pip_package.py"", line 333, in      prepare_wheel_srcs(args.headers, args.srcs, args.xla_aot,   File ""/home/administrator/tensorinstall/tensorflow/./bazelbin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorf",2024-03-08T17:24:19Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.15,closed,0,9,https://github.com/tensorflow/tensorflow/issues/63302,"Please see https://github.com/tensorflow/tensorflow/issues/63298, particularly https://github.com/tensorflow/tensorflow/issues/63298issuecomment1986141742"," , i ran into that issue  mentioned in CC(Target 'build_pip_package' no longer existing?) previously. In order to fix it i editted the ""/home/administrator/tensorinstall/tensorflow/tensorflow/tools/pip_package/BUILD"" file.  i changed the following lines after which the package built succesfully py_binary(     name = ""build_pip_package"",  changed from ""build_pip_package_py""     srcs = [""build_pip_package.py""],     main = ""build_pip_package.py"",",">  , i ran into that issue mentioned in CC(未找到相关数据) previously. In order to fix it i editted the ""/home/administrator/tensorinstall/tensorflow/tensorflow/tools/pip_package/BUILD"" file. i changed the following lines after which the package built succesfully >  > py_binary( name = ""build_pip_package"",  changed from ""build_pip_package_py"" srcs = [""build_pip_package.py""], main = ""build_pip_package.py"", SAME error, thanks, package has built succesfully","I have the same issue, but this change does not solve my issue. Where does the script gets the ""headers"" and other arguments from?",">  , i ran into that issue mentioned in CC(未找到相关数据) previously. In order to fix it i editted the ""/home/administrator/tensorinstall/tensorflow/tensorflow/tools/pip_package/BUILD"" file. i changed the following lines after which the package built succesfully >  > py_binary( name = ""build_pip_package"",  changed from ""build_pip_package_py"" srcs = [""build_pip_package.py""], main = ""build_pip_package.py"", bazel build repo_env=TF_PYTHON_VERSION=3.10 //tensorflow/tools/pip_package:build_pip_package repo_env=WHEEL_NAME=tensorflowcudart . . . INFO: Build completed successfully, 4 total actions (cuda121) administradorpc:~/tensorflow$ ./bazelbin/tensorflow/tools/pip_package/build_pip_package outputname /SSD/tmp/.tmp/tensorflow_pkg_wheel usage: build_pip_package.py [h] outputname OUTPUT_NAME projectname PROJECT_NAME [headers HEADERS] [srcs SRCS] [xla_aot XLA_AOT] [version VERSION] [collab COLLAB] build_pip_package.py: error: the following arguments are required: projectname (cuda121) administradorpc:~/tensorflow$ ./bazelbin/tensorflow/tools/pip_package/build_pip_package outputname /SSD/tmp/.tmp/tensorflow_pkg_wheel projectname tensorflowcudart Traceback (most recent call last):   File ""/home/administrador/tensorflow/./bazelbin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/tools/pip_package/build_pip_package.py"", line 343, in      prepare_wheel_srcs(args.headers, args.srcs, args.xla_aot,   File ""/home/administrador/tensorflow/./bazelbin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/tools/pip_package/build_pip_package.py"", line 175, in prepare_wheel_srcs     prepare_headers(headers, os.path.join(srcs_dir, ""tensorflow/include""))   File ""/home/administrador/tensorflow/./bazelbin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/tools/pip_package/build_pip_package.py"", line 90, in prepare_headers     for file in headers: TypeError: 'NoneType' object is not iterable (cuda121) administradorpc:~/tensorflow$",", Apologies for the delay. Could you please try to install the latest tensorflow v2.17 and provide the update if you are facing the same issue in the current version.  https://www.tensorflow.org/install/pipstepbystep_instructions Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,acode-x,Help needed to load tflite model, 1. System information  OS Platform and Distribution: Debian GNU/Linux 12 (bookworm)  TensorFlow installation: pip  TensorFlow library: 2.14.1  tfliteruntime library: 2.14.0  2. Code   Option B: Paste your code here or provide a link to a custom endtoend colab Output when converting:   3. Failure after conversion Output when loading and trying to infer using tflite  I am not sure how to get this working on my edge device  Debian GNU/Linux 12 which has tfliteruntime. Requesting guidance on same. Thanks,2024-03-08T13:52:44Z,stat:awaiting response stale comp:lite TFLiteConverter TF 2.15,closed,0,3,https://github.com/tensorflow/tensorflow/issues/63293,"Hi x, I have reproduced the code on Debian GNU/Linux 12 (bookworm) with TF 2.14.1. The LSTM model is successfully converted to TFLite and inferencing on the same  platform also done and no errors found. Screenshots are attached below. Regarding the error which you encountered ....please confirm, on which platform you have taken the inference? !image !image Thank You",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,nomadeCodeur,prediction change for an image in a batch depending of the size of the batch.," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.10.1  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.8.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuda 11.2 cuDnn 8.1  GPU model and memory NVIDIA GeForce RTX 3070 Ti  Current behavior? After training my model I save it using model.save(path_saving_model). When I reload my model with model = keras.models.load_model(path_saved_model) I run some predictions  for x_batch, y_GT in batch_generator_i:     print(""start"");     print(""1 => ""); print(model.predict(x_batch, verbose=0));     print(""2 => ""); print(model.predict(x_batch[0:1], verbose=0));     print(""end""); Inside one iteration I get the results in the console start 1 =>  20240308 10:27:03.928564: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8101 20240308 10:27:05.143783: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat32 will be used for the matrix multiplication. This will only be logged once. [[8.15018594e01 6.95814118e02 1.15400046e01]  [9.66099262e01 3.29955555e02 9.05242225e04]  [5.1439",2024-03-08T10:33:50Z,stat:awaiting response stale comp:keras type:performance TF 2.10,closed,0,6,https://github.com/tensorflow/tensorflow/issues/63282,", I tried to execute the mentioned code with the sample images and observed that the code was executed without any issue/error & the output is also intended. Kindly find the gist of it here and also try to execute the code with keras3.0 version. Thank you!"," Thank you for your response. If I understand well, the output is as intended because it is really near to zeros (about 10**8) . It is possible that is a model is not well trained, it deviate a lot from zeros ? I am trying to understand the possible causes of this phenomenon. In my case the deviation from zero was about 0.1 which is significant.",", Could you please try to execute the code by passing a single image in multiple iterations as the input. Also **pred_batch** uses the entire batch, while **pred_batch2** uses only the first element. Depending on the model architecture, processing the entire batch might involve internal calculations that affect the final output for each element. This could lead to a slight difference even for the first element compared to using it alone. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"I tried to test the following. It gives very slight difference. I think there are internal calculations that are not significant for a well trained model. Thank you.  print(""start"") first_image = x_batch[0:1] two_images = tf.concat([first_image, first_image], axis=0) three_images = tf.concat([first_image, first_image, first_image], axis=0) print(""1 => ""); print(model.predict(first_image, verbose=0)) print(""2 => ""); print(model.predict(two_images, verbose=0)) print(""3 => ""); print(model.predict(three_images, verbose=0)) The output is:  start 1 =>  20240322 15:44:55.612396: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8101 20240322 15:44:56.980805: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat32 will be used for the matrix multiplication. This will only be logged once. [[0.39561632 0.52117676 0.08320692]] 2 =>  [[0.39560094 0.52118075 0.08321837]  [0.39560094 0.52118075 0.08321837]] 3 =>  [[0.3956172  0.52115935 0.08322345]  [0.3956172  0.52115935 0.08322345]  [0.3956172  0.52115935 0.08322345]]",Are you satisfied with the resolution of your issue? Yes No
transformer,copybara-service[bot],PR #10297: [NVIDIA GPU] Unroll all nested loops for double buffering transformer,PR CC(docker tensorflow python  FileNotFoundError: [WinError 3] The system cannot find the path specified: ''): [NVIDIA GPU] Unroll all nested loops for double buffering transformer Imported from GitHub PR https://github.com/openxla/xla/pull/10297 The loop_double_buffer_transformer pass currently only unrolls the loops in the main computation. There are some cases where the memcpies(which this pass is designed to eliminate) would exist in nested loops. This pr changes the pass to look at all loops in the module include the nested ones and unroll them. Copybara import of the project:  af8c7b1944d14f5268809c5eb0e71166411ddb19 by TJ : Unroll all nested loops for double buffering transformer Merging this change closes CC(docker tensorflow python  FileNotFoundError: [WinError 3] The system cannot find the path specified: '') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10297 from Tixxx:tixxx/enable_double_buffer_nested af8c7b1944d14f5268809c5eb0e71166411ddb19,2024-03-08T09:55:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63281
transformer,copybara-service[bot],PR #7854: [NVIDIA SPMD] Add HLO support to run windowed einsum in multiple streams,"PR CC(mnist.input_data.read_data_sets() cause Segmentation Fault upon exiting): [NVIDIA SPMD] Add HLO support to run windowed einsum in multiple streams Imported from GitHub PR https://github.com/openxla/xla/pull/7854 This PR contains the HLO changes to be able to run windowed einsum in multiple cuda streams. This adds a debug option to use multistreaming for einsum, it will unroll the loop when turned on and mark the corresponding dots with different stream ids. With the unrolled windowed einsum loop and overlapping multiple gemms, we get 23% speedup on the gpt3 175b models. There are some improvements to be made, major ones are: 1. redundant memcpy for each einsum loop, this can be mitigated by improving loop_double_buffer_transformer to unroll all nested while loops. 2. Some collectivepermutes are scheduled after the async gemm, they won't be able to obtain enough SMs to run and are therefore partially exposed, this can be fixed by forcing them to be delayed in the latency hiding scheduler pass. I'm working on both the optimizations above in separate PRs. Detailed discussion here Copybara import of the project:  9674dd75275376c41220da45b04cd9318453e501 by TJ Xu : Add option to run windowed einsum in ",2024-03-08T09:53:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63280
gpt,j-dsouza-19,Unable to build tensorflow-lite-select-ops.aar for tf 2.16, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version TF 2.16.1  Custom code No  OS platform and distribution Ubuntu 20.04  Mobile device Android  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Getting build errors when building tensorflow lite 2.16.   Standalone code to reproduce the issue  ```  Relevant log output _No response_,2024-03-08T09:52:22Z,stat:awaiting tensorflower type:build/install subtype: ubuntu/linux TF 2.16,open,0,22,https://github.com/tensorflow/tensorflow/issues/63279,Have you solved this problem?,"Hi  , The problem is not solved","Hi dsouza19, I am working on it. Please allow me some time to resolve. Thank You","> Hi dsouza19, >  > I am working on it. Please allow me some time to resolve. >  > Thank You Thank you  ","Hi , While reproducing the error, I have encountered another error   Please look into the user error. Thank You","Hi dsouza19, can you share the model.tflite file which produced this error? Feel free to make it smaller with only the ops which produced the error. Alternatively, one thing you can try is using the nightly branch from source instead of that zip file. That may resolve your issue. Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,I do have the same issue during building v2.15.0 with select ops. What could be the problem?,"Hi vorobiov, are you following the same steps as this issue? If not please let us know your reproducible steps. If your steps are different enough than that may be a separate issue. Thanks for your help.", yes I was trying to build it using docker and following the same steps as described in this issue. And also I encountered the same error when I was trying to build locally on my linux machine following the next steps: 1. Clone gtihub repo 2. run ./configure for Android 3. Run bash tensorflow/lite/tools/build_aar.sh input_models= target_archs=arm64v8a,"Hi vorobiov, can you please share your input models (perhaps a reduced one which replicates the issue still)? Thanks.","Hi , here is the model with which I'm trying to build (attached as a file transformer_large.tflite.zip ). It was created using this repo: https://github.com/PIC4SeR/AcT","Hi vorobiov, what error are you reaching? Judging by your replication steps, you are testing against master? Here are my results: ","Hi , yes I'm testing this on the current master branch. When reproducing my steps I also first encountered the same error as you (error: designated initializers are a C++20 extension). ` ERROR: /home/chao/Desktop/TFBuild/tensorflow/tensorflow/lite/delegates/nnapi/BUILD:14:11: Compiling tensorflow/lite/delegates/nnapi/nnapi_delegate.: (Exit 1): clang failed: error executing command (from target //tensorflow/lite/delegates/nnapi:nnapi_delegate_no_nnapi_implementation) external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/bin/clang nocanonicalprefixes 'target=aarch64linuxandroid21' fdiagnosticscolor Wa,noexecstack fnoexceptions 'std=gnu++17' ... (remaining 157 arguments skipped) tensorflow/lite/delegates/nnapi/nnapi_delegate.cc:522:7: error: designated initializers are a C++20 extension [Werror,Wc++20designator]       .type = nn_type,       ^ tensorflow/lite/delegates/nnapi/nnapi_delegate.cc:1529:45: error: designated initializers are a C++20 extension [Werror,Wc++20designator]     ANeuralNetworksOperandType operand_type{.type = nn_type};                                             ^ tensorflow/lite/delegates/nnapi/nnapi_delegate.cc:1682:45: error: designated initializers are a C++20 extension [Werror,Wc++20designator]     ANeuralNetworksOperandType operand_type{.type = nn_type};                                             ^ tensorflow/lite/delegates/nnapi/nnapi_delegate.cc:1701:45: error: designated initializers are a C++20 extension [Werror,Wc++20designator]     ANeuralNetworksOperandType operand_type{.type = nn_type,                                             ^ tensorflow/lite/delegates/nnapi/nnapi_delegate.cc:1743:9: error: designated initializers are a C++20 extension [Werror,Wc++20designator]         .type = nn_type,         ^ tensorflow/lite/delegates/nnapi/nnapi_delegate.cc:1844:17: error: designated initializers are a C++20 extension [Werror,Wc++20designator]                 .channelDim = static_cast(                 ^ tensorflow/lite/delegates/nnapi/nnapi_delegate.cc:6915:7: error: designated initializers are a C++20 extension [Werror,Wc++20designator]       .init = [](TfLiteContext* context, const char* buffer,       ^ 7 errors generated. Target //tmp:tensorflowlite failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 94.614s, Critical Path: 24.10s INFO: 540 processes: 21 internal, 519 local. FAILED: Build did NOT complete successfully ` I fixed this issue just by replacing the C++ initializers syntax as follows: old:  `ANeuralNetworksOperandType nn_operand_type{       .type = nn_type,       .dimensionCount = tensor_rank,       .dimensions = tensor_dims,       .scale = scale,       .zeroPoint = zero_point,   };` new: `ANeuralNetworksOperandType nn_operand_type{       nn_type,       tensor_rank,       tensor_dims,       scale,       zero_point,   };` For the ./configure action I only selected to configure ./WORKSPACE for Android builds. I've used NDK version 25.08775105, min Android NDK API level 21, Android SDK API level 34, and Android build tools version 34.0.0. After fixing the issue with C++20 initializers you will be able to reach this error with protobufs while trying to build selecttfops: `ERROR: /home/chao/Desktop/TFBuild/tensorflow/tmp/BUILD:19:28: Linking tmp/libtensorflowlite_flex_jni.so failed: (Exit 1): clang failed: error executing command (from target //tmp:libtensorflowlite_flex_jni.so) external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/bin/clang out/androidarm64v8aopt/bin/tmp/libtensorflowlite_flex_jni.so2.params ld.lld: error: undefined symbol: google::protobuf::Message::DebugString() const >>> referenced by function_ops.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/0/function_ops.pic.o:(std::__ndk1::__function::__func, absl::lts_20230802::Status (tensorflow::shape_inference::InferenceContext*)>::operator()(tensorflow::shape_inference::InferenceContext*&&)) >>> referenced by function_ops.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/0/function_ops.pic.o:(std::__ndk1::__function::__func, absl::lts_20230802::Status (tensorflow::shape_inference::InferenceContext*)>::operator()(tensorflow::shape_inference::InferenceContext*&&)) >>> referenced by function_ops.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/0/function_ops.pic.o:(std::__ndk1::__function::__func, absl::lts_20230802::Status (tensorflow::shape_inference::InferenceContext*)>::operator()(tensorflow::shape_inference::InferenceContext*&&)) >>> referenced 28 more times ld.lld: error: undefined symbol: google::protobuf::internal::kGlobalEmptyTable >>> referenced by partitioned_function_ops.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/partitioned_function_ops.pic.o:(google::protobuf::Map, std::__ndk1::allocator >, tensorflow::AttrValue>::Map(google::protobuf::Map, std::__ndk1::allocator >, tensorflow::AttrValue> const&)) >>> referenced by partitioned_function_ops.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/partitioned_function_ops.pic.o:(google::protobuf::Map, std::__ndk1::allocator >, tensorflow::AttrValue>::Map(google::protobuf::Map, std::__ndk1::allocator >, tensorflow::AttrValue> const&)) >>> referenced by optimized_function_graph_info.cc >>>               bazelout/androidarm64v8aopt/bin/tensorflow/core/_objs/portable_tensorflow_lib_lite/optimized_function_graph_info.pic.o:(google::protobuf::Map, std::__ndk1::allocator >, std::__ndk1::basic_string, std::__ndk1::allocator > >::Map, std::__ndk1::allocator >, std::__ndk1::basic_string, std::__ndk1::allocator > >, void*>*> > >(std::__ndk1::__hash_map_const_iterator, std::__ndk1::allocator >, std::__ndk1::basic_string, std::__ndk1::allocator > >, void*>*> > const&, std::__ndk1::__hash_map_const_iterator, std::__ndk1::allocator >, std::__ndk1::basic_string, std::__ndk1::allocator > >, void*>*> > const&)) >>> referenced 207 more times ld.lld: error: undefined symbol: google::protobuf::Arena::AllocateAlignedWithHookForArray(unsigned long, std::type_info const*) >>> referenced by partitioned_function_ops.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/partitioned_function_ops.pic.o:(google::protobuf::Arena::AllocateAlignedWithHookForArray(unsigned long, unsigned long, std::type_info const*)) >>> referenced by partitioned_function_ops.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/partitioned_function_ops.pic.o:(google::protobuf::Arena::AllocateAlignedWithHookForArray(unsigned long, unsigned long, std::type_info const*)) >>> referenced by kernel.cc >>>               bazelout/androidarm64v8aopt/bin/tensorflow/lite/delegates/flex/_objs/delegate_only_runtime/kernel.pic.o:(std::__ndk1::pair, std::__ndk1::allocator >, tensorflow::AttrValue>::InnerMap::iterator_base, std::__ndk1::allocator >, tensorflow::AttrValue> >, bool> google::protobuf::Map, std::__ndk1::allocator >, tensorflow::AttrValue>::InnerMap::TryEmplaceInternal(char const (&) [25])) ld.lld: error: undefined symbol: google::protobuf::internal::ThreadSafeArena::AddCleanup(void*, void (*)(void*)) >>> referenced by partitioned_function_ops.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/partitioned_function_ops.pic.o:(void google::protobuf::Arena::RegisterDestructorInternal, std::__ndk1::allocator > >(std::__ndk1::basic_string, std::__ndk1::allocator >*, google::protobuf::Arena*, std::__ndk1::integral_constant)) >>> referenced by kernel.cc >>>               bazelout/androidarm64v8aopt/bin/tensorflow/lite/delegates/flex/_objs/delegate_only_runtime/kernel.pic.o:(std::__ndk1::pair, std::__ndk1::allocator >, tensorflow::AttrValue>::InnerMap::iterator_base, std::__ndk1::allocator >, tensorflow::AttrValue> >, bool> google::protobuf::Map, std::__ndk1::allocator >, tensorflow::AttrValue>::InnerMap::TryEmplaceInternal(char const (&) [25])) >>> referenced by feature.pb.cc >>>               bazelout/androidarm64v8aopt/bin/tensorflow/core/example/_objs/example_protos_cc_impl/feature.pb.pic.o:(tensorflow::Features::Features(google::protobuf::Arena*, bool)) >>> referenced 71 more times ld.lld: error: undefined symbol: google::protobuf::Arena::AllocateAlignedWithCleanup(unsigned long, std::type_info const*) >>> referenced by partitioned_function_ops.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/partitioned_function_ops.pic.o:(google::protobuf::Map, std::__ndk1::allocator >, tensorflow::AttrValue>::InnerMap::TreeConvert(unsigned long)) >>> referenced by feature_util.cc >>>               bazelout/androidarm64v8aopt/bin/tensorflow/core/_objs/portable_tensorflow_lib_lite/feature_util.pic.o:(google::protobuf::Map, std::__ndk1::allocator >, tensorflow::FeatureList>::InnerMap::TreeConvert(unsigned long)) >>> referenced by function.cc >>>               bazelout/androidarm64v8aopt/bin/tensorflow/core/_objs/portable_tensorflow_lib_lite/0/function.pic.o:(google::protobuf::internal::StringTypeHandler::New(google::protobuf::Arena*, std::__ndk1::basic_string, std::__ndk1::allocator >&&)) >>> referenced 59 more times ld.lld: error: undefined symbol: google::protobuf::internal::RepeatedPtrFieldBase::AddOutOfLineHelper(void*) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(google::protobuf::RepeatedPtrField::TypeHandler::Type* google::protobuf::internal::RepeatedPtrFieldBase::Add::TypeHandler>(google::protobuf::RepeatedPtrField::TypeHandler::Type const*)) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(google::protobuf::RepeatedPtrField::TypeHandler::Type* google::protobuf::internal::RepeatedPtrFieldBase::Add::TypeHandler>(google::protobuf::RepeatedPtrField::TypeHandler::Type const*)) >>> referenced by compression_utils.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/compression_utils.pic.o:(google::protobuf::RepeatedPtrField::TypeHandler::Type* google::protobuf::internal::RepeatedPtrFieldBase::Add::TypeHandler>(google::protobuf::RepeatedPtrField::TypeHandler::Type const*)) >>> referenced 85 more times ld.lld: error: undefined symbol: google::protobuf::RepeatedField::Reserve(int) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(void google::protobuf::RepeatedField::Add(unsigned long const*, unsigned long const*)) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(google::protobuf::RepeatedField::FastAdderImpl::Add(unsigned long)) ld.lld: error: undefined symbol: google::protobuf::RepeatedField::Reserve(int) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(void google::protobuf::RepeatedField::Add(long const*, long const*)) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(google::protobuf::RepeatedField::FastAdderImpl::Add(long)) ld.lld: error: undefined symbol: google::protobuf::RepeatedField::Reserve(int) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(void google::protobuf::RepeatedField::Add(unsigned int const*, unsigned int const*)) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(google::protobuf::RepeatedField::FastAdderImpl::Add(unsigned int)) ld.lld: error: undefined symbol: google::protobuf::RepeatedField::Reserve(int) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(void google::protobuf::RepeatedField::Add(unsigned short const*, unsigned short const*)) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(google::protobuf::RepeatedField::FastAdderImpl::Add(int)) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(void google::protobuf::RepeatedField::Add(short const*, short const*)) >>> referenced 5 more times ld.lld: error: undefined symbol: google::protobuf::RepeatedField::Reserve(int) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(void google::protobuf::RepeatedField::Add(float const*, float const*)) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(google::protobuf::RepeatedField::FastAdderImpl::Add(float)) ld.lld: error: undefined symbol: google::protobuf::RepeatedField::Reserve(int) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(void google::protobuf::RepeatedField::Add(double const*, double const*)) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(google::protobuf::RepeatedField::FastAdderImpl::Add(double)) ld.lld: error: undefined symbol: google::protobuf::RepeatedField::Reserve(int) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(void google::protobuf::RepeatedField::Add(bool const*, bool const*)) >>> referenced by save_restore_tensor.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/save_restore_tensor.pic.o:(google::protobuf::RepeatedField::FastAdderImpl::Add(bool)) ld.lld: error: undefined symbol: google::protobuf::MessageLite::AppendToString(std::__ndk1::basic_string, std::__ndk1::allocator >*) const >>> referenced by tensor_list.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/tensor_list.pic.o:(tensorflow::TensorList::Encode(tensorflow::VariantTensorData*) const) >>> referenced by tensor_coding.cc >>>               bazelout/androidarm64v8aopt/bin/tensorflow/core/_objs/portable_tensorflow_lib_lite/tensor_coding.pic.o:(tensorflow::port::StringListEncoderImpl::Append(google::protobuf::MessageLite const&)) ld.lld: error: undefined symbol: google::protobuf::MessageLite::ParseFromString(std::__ndk1::basic_string, std::__ndk1::allocator > const&) >>> referenced by tensor_list.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/tensor_list.pic.o:(tensorflow::TensorList::Decode(tensorflow::VariantTensorData const&)) >>> referenced by compression_utils.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/compression_utils.pic.o:(tensorflow::data::UncompressElement(tensorflow::data::CompressedElement const&, std::__ndk1::vector >*)) >>> referenced by compression_utils.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/compression_utils.pic.o:(bool tensorflow::DecodeVariantImpl(tensorflow::VariantTensorData, tensorflow::TypeResolver, tensorflow::data::CompressedElement*)) >>> referenced 8 more times ld.lld: error: undefined symbol: google::protobuf::MessageLite::SerializeToArray(void*, int) const >>> referenced by compression_utils.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/compression_utils.pic.o:(tensorflow::data::CompressElement(std::__ndk1::vector > const&, tensorflow::data::CompressedElement*)) ld.lld: error: undefined symbol: google::protobuf::RepeatedField::Add(unsigned long const&) >>> referenced by compression_utils.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/compression_utils.pic.o:(tensorflow::data::CompressedComponentMetadata::_internal_add_uncompressed_bytes(unsigned long)) >>> referenced by tensor.pb_text.cc >>>               bazelout/androidarm64v8aopt/bin/tensorflow/core/_objs/portable_tensorflow_lib_lite/tensor.pb_text.pic.o:(tensorflow::TensorProto::_internal_add_uint64_val(unsigned long)) >>> referenced by graph_debug_info_builder.cc >>>               bazelout/androidarm64v8aopt/bin/tensorflow/core/_objs/portable_tensorflow_lib_lite/graph_debug_info_builder.pic.o:(tensorflow::GraphDebugInfo_StackTrace::_internal_add_frame_id(unsigned long)) >>> referenced 1 more times ld.lld: error: undefined symbol: google::protobuf::internal::ArenaStringPtr::Mutable(google::protobuf::Arena*) >>> referenced by compression_utils.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/compression_utils.pic.o:(tensorflow::data::CompressedElement::_internal_mutable_data()) >>> referenced by dataset_utils.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/dataset_utils.pic.o:(tensorflow::NodeDef::_internal_mutable_device()) >>> referenced by attr_value.pb_text.cc >>>               bazelout/androidarm64v8aopt/bin/tensorflow/core/_objs/portable_tensorflow_lib_lite/attr_value.pb_text.pic.o:(tensorflow::AttrValue::_internal_mutable_s()) >>> referenced 64 more times ld.lld: error: undefined symbol: google::protobuf::Message::GetTypeName() const >>> referenced by compression_utils.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/compression_utils.pic.o:(tensorflow::Variant::Value::TypeName() const) >>> referenced by compression_utils.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/compression_utils.pic.o:(void tensorflow::EncodeVariant(tensorflow::data::CompressedElement const&, tensorflow::VariantTensorData*)) >>> referenced by compression_utils.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/compression_utils.pic.o:(void tensorflow::EncodeVariant(tensorflow::data::CompressedElement const&, std::__ndk1::basic_string, std::__ndk1::allocator >*)) >>> referenced 287 more times ld.lld: error: undefined symbol: google::protobuf::MessageLite::SerializeToString(std::__ndk1::basic_string, std::__ndk1::allocator >*) const >>> referenced by compression_utils.cc >>>               bazelout/androidarm64v8aopt/bin/tmp/_objs/custom_tensorflowlite_flex_flex_delegate_tensorflow_lib/compression_utils.pic.o:(void tensorflow::EncodeVariantImpl(tensorflow::data::CompressedElement const&, tensorflow::TypeResolver, tensorflow::VariantTensorData*)) >>> referenced by variant.cc >>>               bazelout/androidarm64v8aopt/bin/tensorflow/core/_objs/portable_tensorflow_lib_lite/variant.pic.o:(void tensorflow::EncodeVariant(tensorflow::VariantTensorDataProto const&, std::__ndk1::basic_string, std::__ndk1::allocator >*)) >>> referenced by variant_tensor_data.cc >>>               bazelout/androidarm64v8aopt/bin/tensorflow/core/_objs/portable_tensorflow_lib_lite/variant_tensor_data.pic.o:(tensorflow::VariantTensorData::SerializeToString(std::__ndk1::basic_string, std::__ndk1::allocator >*)) >>> referenced 3 more times ld.lld: error: too many errors emitted, stopping now (use errorlimit=0 to see all errors) clang: error: linker command failed with exit code 1 (use v to see invocation) Target //tmp:tensorflowliteselecttfops failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 8650.876s, Critical Path: 460.98s INFO: 15523 processes: 346 internal, 15177 local. FAILED: Build did NOT complete successfully`","Hi vorobiov, we probably need to fix both but changing the source might make things unstable, we need to at least fix the original issue so I will say we will focus on that first (Though if you want to make a PR on your own we definitely won't stop you either) Summary of reproduce steps:  > For the ./configure action I only selected to configure ./WORKSPACE for Android builds. I've used NDK version 25.08775105, min Android NDK API level 21, Android SDK API level 34, and Android build tools version 34.0.0. input model: transformer_large.tflite.zip  produces the above issues Hi , can you please take a look? Thanks."," thanks for looking into the issue. By the way, do you know what stable version (branch, commit hash) I can use in order to build selecttfops for both Android and IOS without any errors or issues?","Hi vorobiov, I would try with r2.15 and go backwards until you have something stable... there may be some weird interactions with iOS and xcode if you go too far back though.","Hello guys, I have the same issue. Any updates about it? Thanks",We worked around this issue by making the following changes;  NOT A CONTRIBUTION,"Dear lawre, Thanks for your response. I tested the workaround and resolved the ""error: designated intializers are a C++20"". However, I got an error in the linking part: ERROR: /host_dir/tmp/BUILD:4:30: Linking tmp/libtensorflowlite_jni.so failed: (Exit 1): clang failed: error executing command (from target//tmp:libtensorflowlite_jni.so) external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/bin/clang out/androidarmeabiv7aopt/bin/tmp/libtensorflowlite_jni.so2.params ld.lld: error: cannot open crtbegin_so.o: No such file or directory ld.lld: error: unable to find library llog ld.lld: error: cannot open crtend_so.o: No such file or directory clang: error: linker command failed with exit code 1 (use v to see invocation) Target //tmp:tensorflowlite failed to build I am wondering which SDK, NDK version that you used. Thanks again! On Mon, Sep 23, 2024, 11:02 Matt Lawrence ***@***.***> wrote: > We worked around this issue by making the following changes; > > diff git .bazelrc .bazelrc > index 315669e7930..612f7007c86 100644 .bazelrc+++ .bazelrc@@ 323,6 +323,7 @@ build:linux copt=""Wnodeprecated"" >  build:linux copt=""Wnodeprecateddeclarations"" >  build:linux copt=""Wnoignoredattributes"" >  build:linux copt=""Wnoarraybounds""+build:linux copt=""Wnoerror"" > >   Add unusedresult as an error on Linux. >  build:linux copt=""Wunusedresult""diff git tensorflow/core/BUILD tensorflow/core/BUILD > index a3538f62e93..066f28f0b4a 100644 tensorflow/core/BUILD+++ tensorflow/core/BUILD@@ 1029,6 +1029,7 @@ cc_library( >  ""//tensorflow/core:mobile_additional_lib_deps"", >  ""//tensorflow/core/platform:resource"", >  ""//tensorflow/core/util:stats_calculator_portable"",+ ***@***.***_google_protobuf//:protobuf"", >  ] + tf_portable_proto_lib() + tf_portable_deps_no_runtime(), >  alwayslink = 1, >  ) > > NOT A CONTRIBUTION > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you commented.Message ID: > ***@***.***> >",I just used the versions as provided/specified in `tensorflow/lite/tools/tfliteandroid.Dockerfile`,"It worked. Thanks Mr. Lawrence. On Tue, Sep 24, 2024, 06:52 Matt Lawrence ***@***.***> wrote: > I just used the versions as provided/specified in > tensorflow/lite/tools/tfliteandroid.Dockerfile > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you commented.Message ID: > ***@***.***> >"
glm,zihaomu,2.16.1 Tflite CMake compiling error,**System information**  OS Platform and Distribution : MacOS 14.1.2  TensorFlow installed from (source or binary): source  TensorFlow version : 2.16.1  NDK version: 26.1.1090912 **Provide the text output from tflite_convert**  looks like the `tensorflow/tensorflow/lite/delegates/gpu/gl/metadata.fbs` is not be compiled to `.h` and `.cpp` file.,2024-03-08T02:58:33Z,stat:awaiting response type:build/install stale comp:lite TFLiteGpuDelegate,closed,0,4,https://github.com/tensorflow/tensorflow/issues/63258,"Hi , can you try following these instructions? https://www.tensorflow.org/lite/guide/build_ios, let us know if that works for you.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
llm,copybara-service[bot],Implement `GetQuantizationMethod` that retrieves `Method` from `XlaCallModuleOp`.,Implement `GetQuantizationMethod` that retrieves `Method` from `XlaCallModuleOp`. A utility function that will be useful when retrieving `Method` object from `XlaCallModuleOp`s.,2024-03-08T02:22:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63257
yi,copybara-service[bot],Move optimized function graph proto into graph construction to avoid copies.,"Move optimized function graph proto into graph construction to avoid copies. Previously we were attempting to `std::move()` a `const GraphDef&` which led to the copying overload of `ConvertGraphDefToGraph(..., const GraphDef&, ...)` being called. After this change we properly call the `ConvertGraphDefToGraph(..., GraphDef&&, ...)` overload, which avoids the memory overheads of copying each `NodeDef` in the graph.",2024-03-07T16:59:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63206
yi,copybara-service[bot],[XLA:GPU][MLIR-based emitters] Use rewriter to erase YieldOp after merging blocks.,[XLA:GPU][MLIRbased emitters] Use rewriter to erase YieldOp after merging blocks. yieldOp was erased using `yield_op>erase` instead of `rewriter.eraseOp(yield_op)`.,2024-03-07T10:59:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63183
transformer,copybara-service[bot],PR #10297: [NVIDIA GPU] Unroll all nested loops for double buffering transformer,PR CC(docker tensorflow python  FileNotFoundError: [WinError 3] The system cannot find the path specified: ''): [NVIDIA GPU] Unroll all nested loops for double buffering transformer Imported from GitHub PR https://github.com/openxla/xla/pull/10297 The loop_double_buffer_transformer pass currently only unrolls the loops in the main computation. There are some cases where the memcpies(which this pass is designed to eliminate) would exist in nested loops. This pr changes the pass to look at all loops in the module include the nested ones and unroll them. Copybara import of the project:  af8c7b1944d14f5268809c5eb0e71166411ddb19 by TJ : Unroll all nested loops for double buffering transformer Merging this change closes CC(docker tensorflow python  FileNotFoundError: [WinError 3] The system cannot find the path specified: '') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/10297 from Tixxx:tixxx/enable_double_buffer_nested af8c7b1944d14f5268809c5eb0e71166411ddb19,2024-03-07T10:34:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63181
llm,copybara-service[bot],PR #9319: Avoid rewriting tiny matrices with cublas or triton,"PR CC([feature] Mobile Integration with NNPACK): Avoid rewriting tiny matrices with cublas or triton Imported from GitHub PR https://github.com/openxla/xla/pull/9319 This disables triton/cublas rewriting for matrix multiplication instructions that are too small to benefit from cublas/triton. In particular, this will avoid triton/cublas if the sum of the sizes of the matrices is : Avoid rewriting tiny matrices with cublas or triton  024d7f9eef4fec9239a62f4bddad7bd5d0c69d29 by Jaroslav Sevcik : Fix copypasta bug  f54f9043a77da60e239614c866aca9a21b5aa65f by Jaroslav Sevcik : Add tests with testing flag  fb689cc164155fe36013a8f5739035b5f77f914c by Jaroslav Sevcik : Address reviewer comments  a777fab3cf1ce811222e4c4f95f3ee1f717fee4e by Jaroslav Sevcik : Test rename  bd64e289892b3115e01dffde9bcefee118421bb0 by Jaroslav Sevcik : Only allow float dots to reach the backend  07bd1ad038ab517fd75673da01b24dbc8773ed3e by Jaroslav Sevcik : Add background info into a comment  0d18f84f1c93dbc2de5614407b9b1092c3b36986 by Jaroslav Sevcik : clang format  23ccff6698efafe1fc384b99d706158ee68b075f by Jaroslav Sevcik : Also update gemm_broadcast_folding_rewrite test  690bb0e1d8b5894f64134d68a031d387399c638f by Jaroslav Sevcik",2024-03-07T10:06:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63176
yi,Aditya-Singh-3112,JIT compilation failed., Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.10.10  Custom code Yes  OS platform and distribution Windows 11  Mobile device No  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 11.8  GPU model and memory NVIDIA RTX 3060 12 GB  Current behavior? The code should have loaded the val data.  Standalone code to reproduce the issue   Relevant log output ,2024-03-07T07:37:25Z,type:bug subtype:windows TF 2.10,closed,0,7,https://github.com/tensorflow/tensorflow/issues/63167,"This is a fairly old version of TF by now, did you try with a newer one?","> This is a fairly old version of TF by now, did you try with a newer one? I am on windows native and TF 2.10 is the latest version on TensorFlow that support CUDA on windows. So, I am with it.","Hi **Singh3112** , Could you make sure the frames, mean, and std variables are of compatible types before the subtraction and division. You are already casting frames to tf.float32 during normalization, but ensure that mean and std are also cast to tf.float32 to avoid any type mismatch issues.  Make sure to adjust the axis parameter in reduce_mean and reduce_std if necessary, depending on the shape of your frames tensor. Thank you!","Hello  , I casted the mean, frames and std to tf.float32 before normalizing, as you advised and it worked perfectly. Thank you very much. ",I will close this issue now.,Are you satisfied with the resolution of your issue? Yes No,"Singh3112  hey, can you  send me the code "
llm,copybara-service[bot],Attach `Method` to matched `XlaCallModuleOp`.,"Attach `Method` to matched `XlaCallModuleOp`. With this change, matched `XlaCallModuleOp` will carry the corresponding `Method` as an attribute, set within `LiftQuantizableSpotsAsFunctionsPass`. This will help locally reason about the quantization method to apply to each `XlaCallModuleOp` in the downstream passes (e.g. `QuantizePass`).",2024-03-07T06:02:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63163
llm,copybara-service[bot],Add tests to evaluate XlaCallModule op.,Add tests to evaluate XlaCallModule op.,2024-03-07T00:31:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63145
yi,gmyrianthous,Could not find a version that satisfies the requirement tensorflow-macos==2.15.0," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15.0  Custom code No  OS platform and distribution macOS Sonoma 14.1.1  Mobile device _No response_  Python version 3.10.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory Apple M1 Pro  Current behavior? I am trying to install `tensorflow` on my macbook pro (Apple M1 pro) and in order to do so, I am trying to install `tensorflowmacos`.  For some reason, any version greater than `2.12.0` cannot be inferred.   Python Version:  I also made sure to upgrade `pip` which is currently set to the latest version:   Standalone code to reproduce the issue  shell ERROR: Could not find a version that satisfies the requirement tensorflowmacos==2.15.0 (from versions: 2.9.0, 2.9.1, 2.9.2, 2.10.0, 2.11.0, 2.12.0) ERROR: No matching distribution found for tensorflowmacos==2.15.0 ```",2024-03-06T16:16:50Z,stat:awaiting tensorflower type:build/install subtype:macOS TF 2.15,open,0,11,https://github.com/tensorflow/tensorflow/issues/63134,", I tried to install the tensorflow v2.15 on MacOS M1 pro with the same command **python m pip install tensorflowmacos==2.15.0** and it was installed without any error/issues. Kindly find the screenshot for the reference. Also please try to create the new virtual environment and install the tensorflow as per the official document instructions.  Installation Log Details   (tensorflowmac) rajasekharpmacbookpro:~ rajasekharp$ python m pip install tensorflowmacos==2.15.0 Collecting tensorflowmacos==2.15.0   Downloading tensorflow_macos2.15.0cp311cp311macosx_12_0_arm64.whl.metadata (4.2 kB) Collecting abslpy>=1.0.0 (from tensorflowmacos==2.15.0)   Using cached absl_py2.1.0py3noneany.whl.metadata (2.3 kB) Collecting astunparse>=1.6.0 (from tensorflowmacos==2.15.0)   Downloading astunparse1.6.3py2.py3noneany.whl.metadata (4.4 kB) Collecting flatbuffers>=23.5.26 (from tensorflowmacos==2.15.0)   Using cached flatbuffers23.5.26py2.py3noneany.whl.metadata (850 bytes) Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflowmacos==2.15.0)   Downloading gast0.5.4py3noneany.whl.metadata (1.3 kB) Collecting googlepasta>=0.1.1 (from tensorflowmacos==2.15.0)   Downloading google_pasta0.2.0py3noneany.whl.metadata (814 bytes) Collecting h5py>=2.9.0 (from tensorflowmacos==2.15.0)   Downloading h5py3.10.0cp311cp311macosx_11_0_arm64.whl.metadata (2.5 kB) Collecting libclang>=13.0.0 (from tensorflowmacos==2.15.0)   Downloading libclang16.0.6py2.py3nonemacosx_11_0_arm64.whl.metadata (5.2 kB) Collecting mldtypes~=0.2.0 (from tensorflowmacos==2.15.0)   Using cached ml_dtypes0.2.0cp311cp311macosx_10_9_universal2.whl.metadata (20 kB) Collecting numpy=1.23.5 (from tensorflowmacos==2.15.0)   Using cached numpy1.26.4cp311cp311macosx_11_0_arm64.whl.metadata (114 kB) Collecting opteinsum>=2.3.2 (from tensorflowmacos==2.15.0)   Using cached opt_einsum3.3.0py3noneany.whl.metadata (6.5 kB) Collecting packaging (from tensorflowmacos==2.15.0)   Using cached packaging23.2py3noneany.whl.metadata (3.2 kB) Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,=3.20.3 (from tensorflowmacos==2.15.0)   Using cached protobuf4.25.3cp37abi3macosx_10_9_universal2.whl.metadata (541 bytes) Requirement already satisfied: setuptools in ./tensorflowmac/lib/python3.11/sitepackages (from tensorflowmacos==2.15.0) (65.5.0) Collecting six>=1.12.0 (from tensorflowmacos==2.15.0)   Using cached six1.16.0py2.py3noneany.whl.metadata (1.8 kB) Collecting termcolor>=1.1.0 (from tensorflowmacos==2.15.0)   Downloading termcolor2.4.0py3noneany.whl.metadata (6.1 kB) Collecting typingextensions>=3.6.6 (from tensorflowmacos==2.15.0)   Using cached typing_extensions4.10.0py3noneany.whl.metadata (3.0 kB) Collecting wrapt=1.11.0 (from tensorflowmacos==2.15.0)   Downloading wrapt1.14.1cp311cp311macosx_11_0_arm64.whl.metadata (6.7 kB) Collecting tensorflowiogcsfilesystem>=0.23.1 (from tensorflowmacos==2.15.0)   Downloading tensorflow_io_gcs_filesystem0.36.0cp311cp311macosx_12_0_arm64.whl.metadata (14 kB) Collecting grpcio=1.24.3 (from tensorflowmacos==2.15.0)   Downloading grpcio1.62.0cp311cp311macosx_10_10_universal2.whl.metadata (4.0 kB) Collecting tensorboard=2.15 (from tensorflowmacos==2.15.0)   Downloading tensorboard2.15.2py3noneany.whl.metadata (1.7 kB) Collecting tensorflowestimator=2.15.0 (from tensorflowmacos==2.15.0)   Downloading tensorflow_estimator2.15.0py2.py3noneany.whl.metadata (1.3 kB) Collecting keras=2.15.0 (from tensorflowmacos==2.15.0)   Downloading keras2.15.0py3noneany.whl.metadata (2.4 kB) Collecting wheel=0.23.0 (from astunparse>=1.6.0>tensorflowmacos==2.15.0)   Using cached wheel0.42.0py3noneany.whl.metadata (2.2 kB) Collecting googleauth=1.6.3 (from tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading google_auth2.28.1py2.py3noneany.whl.metadata (4.7 kB) Collecting googleauthoauthlib=0.5 (from tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading google_auth_oauthlib1.2.0py2.py3noneany.whl.metadata (2.7 kB) Collecting markdown>=2.6.8 (from tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading Markdown3.5.2py3noneany.whl.metadata (7.0 kB) Collecting requests=2.21.0 (from tensorboard=2.15>tensorflowmacos==2.15.0)   Using cached requests2.31.0py3noneany.whl.metadata (4.6 kB) Collecting tensorboarddataserver=0.7.0 (from tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading tensorboard_data_server0.7.2py3noneany.whl.metadata (1.1 kB) Collecting werkzeug>=1.0.1 (from tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading werkzeug3.0.1py3noneany.whl.metadata (4.1 kB) Collecting cachetools=2.0.0 (from googleauth=1.6.3>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading cachetools5.3.3py3noneany.whl.metadata (5.3 kB) Collecting pyasn1modules>=0.2.1 (from googleauth=1.6.3>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading pyasn1_modules0.3.0py2.py3noneany.whl.metadata (3.6 kB) Collecting rsa=3.1.4 (from googleauth=1.6.3>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading rsa4.9py3noneany.whl.metadata (4.2 kB) Collecting requestsoauthlib>=0.7.0 (from googleauthoauthlib=0.5>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading requests_oauthlib1.3.1py2.py3noneany.whl.metadata (10 kB) Collecting charsetnormalizer=2 (from requests=2.21.0>tensorboard=2.15>tensorflowmacos==2.15.0)   Using cached charset_normalizer3.3.2cp311cp311macosx_11_0_arm64.whl.metadata (33 kB) Collecting idna=2.5 (from requests=2.21.0>tensorboard=2.15>tensorflowmacos==2.15.0)   Using cached idna3.6py3noneany.whl.metadata (9.9 kB) Collecting urllib3=1.21.1 (from requests=2.21.0>tensorboard=2.15>tensorflowmacos==2.15.0)   Using cached urllib32.2.1py3noneany.whl.metadata (6.4 kB) Collecting certifi>=2017.4.17 (from requests=2.21.0>tensorboard=2.15>tensorflowmacos==2.15.0)   Using cached certifi2024.2.2py3noneany.whl.metadata (2.2 kB) Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1>tensorboard=2.15>tensorflowmacos==2.15.0)   Using cached MarkupSafe2.1.5cp311cp311macosx_10_9_universal2.whl.metadata (3.0 kB) Collecting pyasn1=0.4.6 (from pyasn1modules>=0.2.1>googleauth=1.6.3>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading pyasn10.5.1py2.py3noneany.whl.metadata (8.6 kB) Collecting oauthlib>=3.0.0 (from requestsoauthlib>=0.7.0>googleauthoauthlib=0.5>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading oauthlib3.2.2py3noneany.whl.metadata (7.5 kB) Downloading tensorflow_macos2.15.0cp311cp311macosx_12_0_arm64.whl (208.8 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 208.8/208.8 MB 21.1 MB/s eta 0:00:00 Using cached absl_py2.1.0py3noneany.whl (133 kB) Downloading astunparse1.6.3py2.py3noneany.whl (12 kB) Using cached flatbuffers23.5.26py2.py3noneany.whl (26 kB) Downloading gast0.5.4py3noneany.whl (19 kB) Downloading google_pasta0.2.0py3noneany.whl (57 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 5.8 MB/s eta 0:00:00 Downloading grpcio1.62.0cp311cp311macosx_10_10_universal2.whl (10.0 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.0/10.0 MB 26.3 MB/s eta 0:00:00 Downloading h5py3.10.0cp311cp311macosx_11_0_arm64.whl (2.6 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 21.6 MB/s eta 0:00:00 Downloading keras2.15.0py3noneany.whl (1.7 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 21.8 MB/s eta 0:00:00 Downloading libclang16.0.6py2.py3nonemacosx_11_0_arm64.whl (20.6 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.6/20.6 MB 26.0 MB/s eta 0:00:00 Using cached ml_dtypes0.2.0cp311cp311macosx_10_9_universal2.whl (1.2 MB) Using cached numpy1.26.4cp311cp311macosx_11_0_arm64.whl (14.0 MB) Using cached opt_einsum3.3.0py3noneany.whl (65 kB) Using cached protobuf4.25.3cp37abi3macosx_10_9_universal2.whl (394 kB) Using cached six1.16.0py2.py3noneany.whl (11 kB) Downloading tensorboard2.15.2py3noneany.whl (5.5 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 21.7 MB/s eta 0:00:00 Downloading tensorflow_estimator2.15.0py2.py3noneany.whl (441 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 442.0/442.0 kB 22.8 MB/s eta 0:00:00 Downloading tensorflow_io_gcs_filesystem0.36.0cp311cp311macosx_12_0_arm64.whl (3.4 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 27.6 MB/s eta 0:00:00 Downloading termcolor2.4.0py3noneany.whl (7.7 kB) Using cached typing_extensions4.10.0py3noneany.whl (33 kB) Downloading wrapt1.14.1cp311cp311macosx_11_0_arm64.whl (36 kB) Using cached packaging23.2py3noneany.whl (53 kB) Downloading google_auth2.28.1py2.py3noneany.whl (186 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 186.9/186.9 kB 13.6 MB/s eta 0:00:00 Downloading google_auth_oauthlib1.2.0py2.py3noneany.whl (24 kB) Downloading Markdown3.5.2py3noneany.whl (103 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.9/103.9 kB 12.7 MB/s eta 0:00:00 Using cached requests2.31.0py3noneany.whl (62 kB) Downloading tensorboard_data_server0.7.2py3noneany.whl (2.4 kB) Downloading werkzeug3.0.1py3noneany.whl (226 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.7/226.7 kB 17.8 MB/s eta 0:00:00 Using cached wheel0.42.0py3noneany.whl (65 kB) Downloading cachetools5.3.3py3noneany.whl (9.3 kB) Using cached certifi2024.2.2py3noneany.whl (163 kB) Using cached charset_normalizer3.3.2cp311cp311macosx_11_0_arm64.whl (118 kB) Using cached idna3.6py3noneany.whl (61 kB) Using cached MarkupSafe2.1.5cp311cp311macosx_10_9_universal2.whl (18 kB) Downloading pyasn1_modules0.3.0py2.py3noneany.whl (181 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 17.9 MB/s eta 0:00:00 Downloading requests_oauthlib1.3.1py2.py3noneany.whl (23 kB) Downloading rsa4.9py3noneany.whl (34 kB) Using cached urllib32.2.1py3noneany.whl (121 kB) Downloading oauthlib3.2.2py3noneany.whl (151 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 10.8 MB/s eta 0:00:00 Downloading pyasn10.5.1py2.py3noneany.whl (84 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.9/84.9 kB 6.3 MB/s eta 0:00:00 Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, typingextensions, termcolor, tensorflowiogcsfilesystem, tensorflowestimator, tensorboarddataserver, six, pyasn1, protobuf, packaging, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charsetnormalizer, certifi, cachetools, abslpy, werkzeug, rsa, requests, pyasn1modules, opteinsum, mldtypes, h5py, googlepasta, astunparse, requestsoauthlib, googleauth, googleauthoauthlib, tensorboard, tensorflowmacos Successfully installed MarkupSafe2.1.5 abslpy2.1.0 astunparse1.6.3 cachetools5.3.3 certifi2024.2.2 charsetnormalizer3.3.2 flatbuffers23.5.26 gast0.5.4 googleauth2.28.1 googleauthoauthlib1.2.0 googlepasta0.2.0 grpcio1.62.0 h5py3.10.0 idna3.6 keras2.15.0 libclang16.0.6 markdown3.5.2 mldtypes0.2.0 numpy1.26.4 oauthlib3.2.2 opteinsum3.3.0 packaging23.2 protobuf4.25.3 pyasn10.5.1 pyasn1modules0.3.0 requests2.31.0 requestsoauthlib1.3.1 rsa4.9 six1.16.0 tensorboard2.15.2 tensorboarddataserver0.7.2 tensorflowestimator2.15.0 tensorflowiogcsfilesystem0.36.0 tensorflowmacos2.15.0 termcolor2.4.0 typingextensions4.10.0 urllib32.2.1 werkzeug3.0.1 wheel0.42.0 wrapt1.14.1 (tensorflowmac) rajasekharpmacbookpro:~ rajasekharp$ python Python 3.11.6 (v3.11.6:8b6ee5ba3b, Oct  2 2023, 11:18:21) [Clang 13.0.0 (clang1300.0.29.30)] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.  >>> import tensorflow as tf >>> tf.__version__ '2.15.0'","I have created a new virtual environment. The error persists.  Also, when you say `official document instructions` can you please clarify? It's just a pip install command 🤣 ","Also, FYI, I see the same error on a different machine (but still, Apple M1 Pro). So there's something else going wrong here.  I have even tried with Python 11, but still getting the same error:   Can you please try with Python 3.10 and let me know? ",", I tried python 3.11 which is compatible with 2.15. Could you please allow me to test with python 3.10 as well, so that I can provide more details/info on the same. Thank you!",", I tried installing tensorflow v2.15 with the python version3.10 on Apple M1 pro(OSmacOS Sonoma) and it was installed without any issue/error. Kindly find the screenshot and the log details for the reference.  Log Information! nallavemacbookpro:~ nallave$ pip3 version pip 24.0 from /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/sitepackages/pip (python 3.10) nallavemacbookpro:~ nallave$ python m pip install tensorflowmacos==2.15.0 bash: python: command not found nallavemacbookpro:~ nallave$ python3 m pip install tensorflowmacos==2.15.0 Collecting tensorflowmacos==2.15.0   Downloading tensorflow_macos2.15.0cp310cp310macosx_12_0_arm64.whl.metadata (4.2 kB) Collecting abslpy>=1.0.0 (from tensorflowmacos==2.15.0)   Downloading absl_py2.1.0py3noneany.whl.metadata (2.3 kB) Collecting astunparse>=1.6.0 (from tensorflowmacos==2.15.0)   Downloading astunparse1.6.3py2.py3noneany.whl.metadata (4.4 kB) Collecting flatbuffers>=23.5.26 (from tensorflowmacos==2.15.0)   Downloading flatbuffers24.3.7py2.py3noneany.whl.metadata (849 bytes) Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflowmacos==2.15.0)   Downloading gast0.5.4py3noneany.whl.metadata (1.3 kB) Collecting googlepasta>=0.1.1 (from tensorflowmacos==2.15.0)   Downloading google_pasta0.2.0py3noneany.whl.metadata (814 bytes) Collecting h5py>=2.9.0 (from tensorflowmacos==2.15.0)   Downloading h5py3.10.0cp310cp310macosx_11_0_arm64.whl.metadata (2.5 kB) Collecting libclang>=13.0.0 (from tensorflowmacos==2.15.0)   Downloading libclang16.0.6py2.py3nonemacosx_11_0_arm64.whl.metadata (5.2 kB) Collecting mldtypes~=0.2.0 (from tensorflowmacos==2.15.0)   Downloading ml_dtypes0.2.0cp310cp310macosx_10_9_universal2.whl.metadata (20 kB) Collecting numpy=1.23.5 (from tensorflowmacos==2.15.0)   Downloading numpy1.26.4cp310cp310macosx_11_0_arm64.whl.metadata (61 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.1/61.1 kB 999.7 kB/s eta 0:00:00 Collecting opteinsum>=2.3.2 (from tensorflowmacos==2.15.0)   Downloading opt_einsum3.3.0py3noneany.whl.metadata (6.5 kB) Collecting packaging (from tensorflowmacos==2.15.0)   Downloading packaging24.0py3noneany.whl.metadata (3.2 kB) Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,=3.20.3 (from tensorflowmacos==2.15.0)   Downloading protobuf4.25.3cp37abi3macosx_10_9_universal2.whl.metadata (541 bytes) Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/sitepackages (from tensorflowmacos==2.15.0) (57.4.0) Collecting six>=1.12.0 (from tensorflowmacos==2.15.0)   Downloading six1.16.0py2.py3noneany.whl.metadata (1.8 kB) Collecting termcolor>=1.1.0 (from tensorflowmacos==2.15.0)   Downloading termcolor2.4.0py3noneany.whl.metadata (6.1 kB) Collecting typingextensions>=3.6.6 (from tensorflowmacos==2.15.0)   Downloading typing_extensions4.10.0py3noneany.whl.metadata (3.0 kB) Collecting wrapt=1.11.0 (from tensorflowmacos==2.15.0)   Downloading wrapt1.14.1cp310cp310macosx_11_0_arm64.whl.metadata (6.7 kB) Collecting tensorflowiogcsfilesystem>=0.23.1 (from tensorflowmacos==2.15.0)   Downloading tensorflow_io_gcs_filesystem0.36.0cp310cp310macosx_12_0_arm64.whl.metadata (14 kB) Collecting grpcio=1.24.3 (from tensorflowmacos==2.15.0)   Downloading grpcio1.62.1cp310cp310macosx_12_0_universal2.whl.metadata (4.0 kB) Collecting tensorboard=2.15 (from tensorflowmacos==2.15.0)   Downloading tensorboard2.15.2py3noneany.whl.metadata (1.7 kB) Collecting tensorflowestimator=2.15.0 (from tensorflowmacos==2.15.0)   Downloading tensorflow_estimator2.15.0py2.py3noneany.whl.metadata (1.3 kB) Collecting keras=2.15.0 (from tensorflowmacos==2.15.0)   Downloading keras2.15.0py3noneany.whl.metadata (2.4 kB) Requirement already satisfied: wheel=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/sitepackages (from astunparse>=1.6.0>tensorflowmacos==2.15.0) (0.43.0) Collecting googleauth=1.6.3 (from tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading google_auth2.28.2py2.py3noneany.whl.metadata (4.7 kB) Collecting googleauthoauthlib=0.5 (from tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading google_auth_oauthlib1.2.0py2.py3noneany.whl.metadata (2.7 kB) Collecting markdown>=2.6.8 (from tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading Markdown3.5.2py3noneany.whl.metadata (7.0 kB) Collecting requests=2.21.0 (from tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading requests2.31.0py3noneany.whl.metadata (4.6 kB) Collecting tensorboarddataserver=0.7.0 (from tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading tensorboard_data_server0.7.2py3noneany.whl.metadata (1.1 kB) Collecting werkzeug>=1.0.1 (from tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading werkzeug3.0.1py3noneany.whl.metadata (4.1 kB) Collecting cachetools=2.0.0 (from googleauth=1.6.3>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading cachetools5.3.3py3noneany.whl.metadata (5.3 kB) Collecting pyasn1modules>=0.2.1 (from googleauth=1.6.3>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading pyasn1_modules0.3.0py2.py3noneany.whl.metadata (3.6 kB) Collecting rsa=3.1.4 (from googleauth=1.6.3>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading rsa4.9py3noneany.whl.metadata (4.2 kB) Collecting requestsoauthlib>=0.7.0 (from googleauthoauthlib=0.5>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading requests_oauthlib1.4.0py2.py3noneany.whl.metadata (11 kB) Collecting charsetnormalizer=2 (from requests=2.21.0>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading charset_normalizer3.3.2cp310cp310macosx_11_0_arm64.whl.metadata (33 kB) Collecting idna=2.5 (from requests=2.21.0>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading idna3.6py3noneany.whl.metadata (9.9 kB) Collecting urllib3=1.21.1 (from requests=2.21.0>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading urllib32.2.1py3noneany.whl.metadata (6.4 kB) Collecting certifi>=2017.4.17 (from requests=2.21.0>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading certifi2024.2.2py3noneany.whl.metadata (2.2 kB) Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading MarkupSafe2.1.5cp310cp310macosx_10_9_universal2.whl.metadata (3.0 kB) Collecting pyasn1=0.4.6 (from pyasn1modules>=0.2.1>googleauth=1.6.3>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading pyasn10.5.1py2.py3noneany.whl.metadata (8.6 kB) Collecting oauthlib>=3.0.0 (from requestsoauthlib>=0.7.0>googleauthoauthlib=0.5>tensorboard=2.15>tensorflowmacos==2.15.0)   Downloading oauthlib3.2.2py3noneany.whl.metadata (7.5 kB) Downloading tensorflow_macos2.15.0cp310cp310macosx_12_0_arm64.whl (208.8 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 208.8/208.8 MB 7.9 MB/s eta 0:00:00 Downloading absl_py2.1.0py3noneany.whl (133 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 9.8 MB/s eta 0:00:00 Downloading astunparse1.6.3py2.py3noneany.whl (12 kB) Downloading flatbuffers24.3.7py2.py3noneany.whl (26 kB) Downloading gast0.5.4py3noneany.whl (19 kB) Downloading google_pasta0.2.0py3noneany.whl (57 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 4.6 MB/s eta 0:00:00 Downloading grpcio1.62.1cp310cp310macosx_12_0_universal2.whl (10.0 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.0/10.0 MB 22.3 MB/s eta 0:00:00 Downloading h5py3.10.0cp310cp310macosx_11_0_arm64.whl (2.7 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 20.7 MB/s eta 0:00:00 Downloading keras2.15.0py3noneany.whl (1.7 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 21.3 MB/s eta 0:00:00 Downloading libclang16.0.6py2.py3nonemacosx_11_0_arm64.whl (20.6 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.6/20.6 MB 20.7 MB/s eta 0:00:00 Downloading ml_dtypes0.2.0cp310cp310macosx_10_9_universal2.whl (1.2 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 19.4 MB/s eta 0:00:00 Downloading numpy1.26.4cp310cp310macosx_11_0_arm64.whl (14.0 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.0/14.0 MB 22.2 MB/s eta 0:00:00 Downloading opt_einsum3.3.0py3noneany.whl (65 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 6.1 MB/s eta 0:00:00 Downloading protobuf4.25.3cp37abi3macosx_10_9_universal2.whl (394 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 394.2/394.2 kB 27.6 MB/s eta 0:00:00 Downloading six1.16.0py2.py3noneany.whl (11 kB) Downloading tensorboard2.15.2py3noneany.whl (5.5 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 20.3 MB/s eta 0:00:00 Downloading tensorflow_estimator2.15.0py2.py3noneany.whl (441 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 442.0/442.0 kB 17.6 MB/s eta 0:00:00 Downloading tensorflow_io_gcs_filesystem0.36.0cp310cp310macosx_12_0_arm64.whl (3.4 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 20.0 MB/s eta 0:00:00 Downloading termcolor2.4.0py3noneany.whl (7.7 kB) Downloading typing_extensions4.10.0py3noneany.whl (33 kB) Downloading wrapt1.14.1cp310cp310macosx_11_0_arm64.whl (35 kB) Downloading packaging24.0py3noneany.whl (53 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.5/53.5 kB 4.2 MB/s eta 0:00:00 Downloading google_auth2.28.2py2.py3noneany.whl (186 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 186.9/186.9 kB 14.4 MB/s eta 0:00:00 Downloading google_auth_oauthlib1.2.0py2.py3noneany.whl (24 kB) Downloading Markdown3.5.2py3noneany.whl (103 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.9/103.9 kB 10.0 MB/s eta 0:00:00 Downloading requests2.31.0py3noneany.whl (62 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.6/62.6 kB 5.1 MB/s eta 0:00:00 Downloading tensorboard_data_server0.7.2py3noneany.whl (2.4 kB) Downloading werkzeug3.0.1py3noneany.whl (226 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.7/226.7 kB 19.4 MB/s eta 0:00:00 Downloading cachetools5.3.3py3noneany.whl (9.3 kB) Downloading certifi2024.2.2py3noneany.whl (163 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.8/163.8 kB 14.2 MB/s eta 0:00:00 Downloading charset_normalizer3.3.2cp310cp310macosx_11_0_arm64.whl (120 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.4/120.4 kB 10.6 MB/s eta 0:00:00 Downloading idna3.6py3noneany.whl (61 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.6/61.6 kB 5.1 MB/s eta 0:00:00 Downloading MarkupSafe2.1.5cp310cp310macosx_10_9_universal2.whl (18 kB) Downloading pyasn1_modules0.3.0py2.py3noneany.whl (181 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 15.9 MB/s eta 0:00:00 Downloading requests_oauthlib1.4.0py2.py3noneany.whl (24 kB) Downloading rsa4.9py3noneany.whl (34 kB) Downloading urllib32.2.1py3noneany.whl (121 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.1/121.1 kB 10.5 MB/s eta 0:00:00 Downloading oauthlib3.2.2py3noneany.whl (151 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 15.4 MB/s eta 0:00:00 Downloading pyasn10.5.1py2.py3noneany.whl (84 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.9/84.9 kB 10.7 MB/s eta 0:00:00 Installing collected packages: libclang, flatbuffers, wrapt, urllib3, typingextensions, termcolor, tensorflowiogcsfilesystem, tensorflowestimator, tensorboarddataserver, six, pyasn1, protobuf, packaging, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charsetnormalizer, certifi, cachetools, abslpy, werkzeug, rsa, requests, pyasn1modules, opteinsum, mldtypes, h5py, googlepasta, astunparse, requestsoauthlib, googleauth, googleauthoauthlib, tensorboard, tensorflowmacos Successfully installed MarkupSafe2.1.5 abslpy2.1.0 astunparse1.6.3 cachetools5.3.3 certifi2024.2.2 charsetnormalizer3.3.2 flatbuffers24.3.7 gast0.5.4 googleauth2.28.2 googleauthoauthlib1.2.0 googlepasta0.2.0 grpcio1.62.1 h5py3.10.0 idna3.6 keras2.15.0 libclang16.0.6 markdown3.5.2 mldtypes0.2.0 numpy1.26.4 oauthlib3.2.2 opteinsum3.3.0 packaging24.0 protobuf4.25.3 pyasn10.5.1 pyasn1modules0.3.0 requests2.31.0 requestsoauthlib1.4.0 rsa4.9 six1.16.0 tensorboard2.15.2 tensorboarddataserver0.7.2 tensorflowestimator2.15.0 tensorflowiogcsfilesystem0.36.0 tensorflowmacos2.15.0 termcolor2.4.0 typingextensions4.10.0 urllib32.2.1 werkzeug3.0.1 wrapt1.14.1 nallavemacbookpro:~ nallave$ python3 Python 3.10.0 (v3.10.0:b494f5935c, Oct  4 2021, 14:59:19) [Clang 12.0.5 (clang1205.0.22.11)] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf >>> tf.__version__ '2.15.0' >>> "," Thanks for your reply. Do you have any clue why this is happening then? As I said, I am pretty sure I am using the latest `pip` version so I don't understand why these versions are not available. ",", From another open issue https://github.com/tensorflow/tensorflow/issues/63495, I can sense that you are facing an issue with the tensorflow v2.16.1 where the similar issue was raised and still it is in open state. Kindly follow the same issue for the updates. Thank you!", I think all these issues are unrelated to each other to be honest.,Any thoughts on this dependency hell? ,"cc:  , toplay","If you are trying to install TF 2.15, look at my working venv https://github.com/msusol/datasciencepro/blob/main/notebooks/TensorFlow215.ipynb if you are trying to install TF 2.16.1, look at my working venv https://github.com/msusol/datasciencepro/blob/main/notebooks/TensorFlow216.ipynb"
yi,yspyhp,"image similarity model，The input sequence is inconsistent, resulting in inconsistent results", Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.11.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? 3 files test.py ==》model definition image_same_train.py ==> model train image_same.py ==> model predict 归档.zip  Standalone code to reproduce the issue   Relevant log output ,2024-03-06T12:34:05Z,stat:awaiting response type:support stale comp:ops TF 2.11,closed,0,4,https://github.com/tensorflow/tensorflow/issues/63133,"Hi **** , Sorry for the delay,  Image Preprocessing: Ensure that the images you are using for prediction are preprocessed in the same way as during training. This includes resizing to (224, 224) and applying the same preprocessing steps using preprocess input function. Model Loading:  Make sure that the model is loaded properly and the weights are correctly loaded. From the log output, it seems the model is loaded successfully. Normalization:  Ensure that the normalization applied during preprocessing is consistent between training and prediction. This includes mean subtraction and scaling. Data Ordering:  Check if the data ordering (channelslast or channelsfirst) is consistent between your training and prediction datasets. ResNet50 usually expects 'channelslast' ordering. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,yspyhp,"image similarity model，The input sequence is inconsistent, resulting in inconsistent results"," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.11.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior?  model definition import tensorflow as tf from keras import backend as K from keras.layers import Lambda from tensorflow.keras import layers, models from tensorflow.keras.applications import ResNet50 tf.random.set_seed(2) def euclidean_distance(vects):     x, y = vects     euclidean_dist = K.sqrt(K.sum(K.square(x  y), axis=1, keepdims=True))     return 1 / (1 + euclidean_dist) def build_similarity_model(base_model):     base_model.trainable = False      input1 = layers.Input(shape=(224, 224, 3))     input2 = layers.Input(shape=(224, 224, 3))     x1 = base_model(input1)     f1 = layers.Flatten()(x1)     v1 = layers.Dense(256, activation='relu')(f1)     d1 = layers.Dropout(0.5)(v1)     x2 = base_model(input2)     f2 = layers.Flatten()(x2)     v2 = layers.Dense(256, activation='relu')(f2)     d2 = layers.Dropout(0.5)(v2",2024-03-06T12:23:54Z,type:support,closed,0,3,https://github.com/tensorflow/tensorflow/issues/63132,Are you satisfied with the resolution of your issue? Yes No,new issue,Are you satisfied with the resolution of your issue? Yes No
yi,Jovian-Dsouza,[BUG] Tensorflow lite select ops gives runtime error ," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version TF 2.15  Custom code No  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? While trying to use tensorflowliteselecttfops.aar in android studio results in the following runtime error  Some fixes suggest `config=monolithic` to be added to .bazelrc file, however with this build error is noticed for Tensorflow 2.15   Standalone code to reproduce the issue   Relevant log output _No response_",2024-03-06T03:32:48Z,stat:awaiting response type:bug stale comp:lite Android TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/63130,"Hi Dsouza, thanks for the information, currently this is a duplicate of https://github.com/tensorflow/tensorflow/issues/60831, please follow there or comment why you think this is not a duplicate, thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,libofei2004,"Error occurred when initializing ObjectDetector: Mobile SSD models are expected to have exactly 4 outputs, found 3"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf2.8  Custom code Yes  OS platform and distribution android (MIUI 13)  Mobile device MI 10 pro  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I used custom vision(https://www.customvision.ai/) to train my object detction model, and exported an tflite model , but my exported tensorflow tflite model cannot loaded rightly in tensorflow example（https://github.com/tensorflow/examples/tree/cbe96424b6b930fd74dbd4ef2b1e826a031a3554/lite/examples/object_detection）. It seems lack of metadata. If I use metadata_writers py script to add metadata to tflite model and load model in the object_detection example, an error like ""Mobile SSD models are expected to have exactly 4 outputs, found 3"" will occured. I have read https://github.com/tensorflow/tensorflow/issues/47595, but still not find how to solve my problem, it seems customize inputs and outputs for TFLiteObjectDetectionAPIModel cannot solve the problem.  Standalone code to reproduce the issue   Relevant log output ",2024-03-04T15:48:34Z,stat:awaiting response type:support stale comp:lite TF 2.8,closed,0,26,https://github.com/tensorflow/tensorflow/issues/63115,"Hi，  , could you please  take a look at this issue?","Hi , your link isn't working ... is there any reason you are using an older version? Can you please try with nightly and let us know the results. Thanks."," Thank you, I have read issue CC(Tensorflow Lite Android Object Detection — Mobile SSD models are expected to have exactly 4 outputs, found 8) and try to use the code from the link ,it can support me to customize inputs and outputs, the right link is  https://github.com/tensorflow/examples/blob/cbe96424b6b930fd74dbd4ef2b1e826a031a3554/lite/examples/object_detection/android/lib_interpreter/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.javaL56 The lastest version I also tried, but met the same error. https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android I think my exported tflite model may be not suit for the tensorflow example.","The model readme.md is as follows: !image As mentioned by readme, the outputs num is indeed 3, but tensorflow example needs 4, how to solve it?","Hi , there are multiple things that need updating with this project so I will fix this, then we can address your issue. Thanks.","Thank you  ,I will wait.","Hi , I was able to compile a newer version of the project by updating the dependencies: object_detection/android/build.gradle.kts:  additionally a namespace needs to be specified, in object_detection/android/app/build.gradle.kts:  Before I commit this change, I will need to see if there are any other changes needed to support your workflow. It seems the issue is specific to your model ... can you share/upload your model or explain how I can recreate it? Thanks."," , can you visit my model https://github.com/libofei2004/customvisionmodel, it includes tflite model, tflite float16 model, saved model and  tflite float16 with metadata.","Hi , this example uses quantized models which is described in the README: https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android . There's assumptions built it which make it not work with nonquantized models... are any of your models quantized? if not, can you try with a quantized model?","Hi  , I have run the example yet before,I can run it successfully by using the models mentioned in the README,but I can't run my model successfully.I also tied to quantize my model yet.  I think the problem is my model's  output number is not fit for the example .",", can you please share the exact model you ""can't run successfully""? I tried with model.tflite and when I run it effectively states the model isn't quantized, do you have a quantized model that you are attempting to run it on?"," ,My models are all in  https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android, they can run successfully in the https://github.com/AzureSamples/cognitiveservicesandroidcustomvisionsample but not in tensorflow sample. I have tried to quantized the model by the script https://github.com/libofei2004/customvisionmodel/blob/main/quantization.py but failed,I don't exactly konw how to make a right quantization, maybe the model format exported from the custom vision platform is not standard.","Hi , I reread your original description.. so the current problem is that your models have `kTfLiteFloat32` input tensors i.e. nonquantized, we can get around this with writing the metadata which is what you described here: > I use metadata_writers py script to add metadata to tflite model and load model in the object_detection example can you please elaborate on how you wrote the meta data with the script? i.e. did you import the tfllitesupport library and ran a custom script or did you run a script we already provided? If so, what arguments did you pass? Thanks for your help.", ，My metadata script is https://github.com/libofei2004/customvisionmodel/blob/main/meta.py. Thank you.,"Hi , please review this gist. I uploaded all the models that you provided and an original mobilenetv1 model. As you can see your models have 3 outputs instead of the expected 4. This is prior to the metadata issue so the problem is in order to ""drop"" in this model you have to conform to the expected API: https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/task/vision/detector/ObjectDetector Please review the API documentation. Of course you can use a custom ObjectDetector but you will have to code a bit to make it work with our example. I don't know the details of your model but it might be worth it to see if you can fit the outputs (by modifying the original model) to the expected API (perhaps if an output is unnecessary for your use case you can modify your model to output a dummy value). As I currently see it, it is working as expected. Let me know if you have any additional questions. "," ,I can't visit the gist. I think either modify the tensorflow sample source code to fit the model or modify the model structure to fit the source code. Both are not easy. From the error message, it seems that the error lies in the JNI of the code, so changing the code might not be straightforward.","My apologies, updated the wrong link, here's the right one (also edited on the original): gist.   Your understanding is correct.... the example code is designed to get you started, unfortunately we can not maintain example code for all potential use cases out there. Since your own model doesn't seem to fit well with the API, perhaps you can try the Interpreter API? which is more meant for a lower level of abstraction (and thus can be more customized to your use case): https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/InterpreterApi","，thank you, but the  Interpreter API is hard to use, is there any examples with the API ?","Hi , Does this help you? https://www.tensorflow.org/lite/inference_with_metadata/lite_support, the support library will give you higher level tools for input/output processing."," , with the error message: at org.tensorflow.lite.task.vision.detector.ObjectDetector.initJniWithByteBuffer(Native Method)..., I think the error  occurred at the low level （C++），Can I use Java API to modify the number of output parameters?","Hi , you would have to modify the API which is more or less located here: https://github.com/tensorflow/tflitesupport/blob/master/tensorflow_lite_support/java/src/java/org/tensorflow/lite/task/vision/detector/ObjectDetector.java This would be nontrivial and unless you make a feature request in that repo that improves the optionality of the outputs for the API you would have to maintain that change itself. So... there probably is a way, if you dig down far enough."," ， I don't think I can get the sample running through modifying the ObjectDetector.java, the error must be in ""modelBuffer""  and the real code parses it is in C++.","Hi , apologies, I am less familiar with that repo overall, you are correct, I guess this was covered under ""if you dig down far enough"" but yeah I did a little more and it seems like here is probably where you would have to change: https://github.com/tensorflow/tflitesupport/blob/master/tensorflow_lite_support/java/src/native/task/vision/detector/object_detector_jni.ccL115",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,zihaomu,Tflite: android benchmark test get differen result on GPU delegate,"Hi, When I use the benchmark script and benchmark apk to test my model performance, I get the same performance on CPU of XNNPACK delegate, but different performance on GPU of OpenCL delegate. And the result  The benchmark apk is almost twice as fast as the benchmark script on GPU.  Q:What's the different between the benchmark script and the apk?  Running on benchmark script Log of benchmark script:   Running on benchmark apk  Log on logcat:  **System information**  Android Device information: xiaomi12",2024-03-04T08:58:55Z,stat:awaiting tensorflower type:bug comp:lite TFLiteGpuDelegate Android,closed,0,7,https://github.com/tensorflow/tensorflow/issues/63111,Testing model:mobilenetv3_large.tflite.zip,"I feel an emulator will not be a reasonable test case for this, , can you please take a look? Thanks.","Uh, I don't own the benchmarks, so it's hard for me to tell what the differences are.   do you happen to know who owns the benchmarks?","Hi , can you please take a look? Thanks.","I encountered the same problem when using GPU inference, but I don’t know how to solve it. I hope to get a reply.","Hi,   Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/75 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
yi,vatsalraicha,WSL2 - TensorFlow Install Issue Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered,"Facing this error message when trying to use tensorflow on WSL2 Ubuntu  Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered. TFTRT Warning: Could not find TensorRT could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node Your kernel may have been built without NUMA support. Environment TensorRT Version: 8.6.1.6 GPU Type: 4080 Laptop GPU Nvidia Driver Version: NVIDIASMI 546.17 Driver Version: 546.17 CUDA Version: 12.3, CUDA Toolkit Version  11.8 CUDNN Version: v8.6 Operating System + Version: Windows 11 Python Version (if applicable): Python 3.10.13 TensorFlow Version (if applicable): 2.15.0 PyTorch Version (if applicable): N/A Baremetal or Container (if container which image + tag): N/A pyenv Version  2.3.3511g9908daf8 WSL version: 2.0.14.0 Kernel version: 5.15.133.11 tensorFlowErrors.txt Steps to reproduce  Followed the exact steps as mentioned in https://www.tensorflow.org/install/pip?hl=pt",2024-03-04T01:26:49Z,stat:awaiting response stale comp:gpu subtype:windows TF 2.15,closed,6,45,https://github.com/tensorflow/tensorflow/issues/63109,"Hi **** , Ensure Correct Installation of CUDA, cuDNN, and TensorRT: CUDA and cuDNN: Make sure that CUDA and cuDNN are correctly installed and that TensorFlow can detect them. You have mentioned using CUDA 12.3 and cuDNN v8.6, which should be compatible with TensorFlow 2.15.0. Ensure that the CUDA and cuDNN paths are correctly added to your `PATH` and `LD_LIBRARY_PATH` environment variables.  Reinstall TensorFlow: Consider creating a fresh virtual environment and reinstalling TensorFlow within it. This can help resolve any conflicts or issues with previous installations:  Test Your Setup: After ensuring all configurations and installations are correct, test your TensorFlow setup to see if it can access the GPU:  Thank you!","Same error.  WSL version: 2.0.14.0 Kernel version: 5.15.133.11 WSLg version: 1.0.59 MSRDC version: 1.2.4677 Direct3D version: 1.611.181528511 DXCore version: 10.0.25131.10022205311700.rsonecorebase2hyp Windows version: 10.0.22631.3235 NVIDIA GeForce RTX 4050 Driver Version: 551.61         CUDA Version: 12.4 CUDA sample functions run without errors python 3.10.12 cuda_12.3.2_545.23.08 tensorrt 8.6.1 Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorrt >>> print(tensorrt.__version__) 8.6.1 >>> import tensorflow as tf 20240305 16:13:59.938649: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20240305 16:13:59.960210: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20240305 16:13:59.960255: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20240305 16:13:59.960774: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20240305 16:13:59.964109: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20240305 16:14:00.340405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT >>> print(tf.config.list_physical_devices('GPU')) 20240305 16:14:19.162569: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node Your kernel may have been built without NUMA support. 20240305 16:14:19.166217: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node Your kernel may have been built without NUMA support. 20240305 16:14:19.166261: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node Your kernel may have been built without NUMA support. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] >>> print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) Num GPUs Available:  1 >>>"," On WSL2, I dont have CUDA 12.3 director in /usr/local. So this path is invalid for me  /usr/local/cuda12.3/ Now as per NVIDIA documentation, I don't need to install any drivers on WSL2 and it should automatically pick from native Windows. What could be the next step for me here?",Same problem,"Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered google colab error ,how to resolve it",same  20240426 16:37:38.371348: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20240426 16:37:38.371404: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20240426 16:37:38.371421: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered,I have the same problem. I am using WSL2. 20240425 16:44:46.357075: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20240425 16:44:46.357110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20240425 16:44:46.357608: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered,"After several days struggle,  I am able to get rid of this errors.  It's a version compatibility issue. You need to install the package versions according to the webpage.  I was struggled on installing specific CUDA toolkit versions through deb packages, because the version was always updated to the latest version 12.4. In the end, I used the runfiles to install the toolkit. The other problem I had was that I started with tensorflow 2.15.1, which I could not get it to work.  The versions that work for me are: CUDA toolkit:  12.2.  cudnn:             8.9.7.29 Tensorflow:      2.16.1", Did you achieve this on WSL2 or on Native Linux? What was your CUDA version? Would you be able to share the Version info that you have setup that works? TensorRT Version:  Nvidia Driver Version:  NVIDIASMI Driver Version: 546.17 CUDA Version:  CUDNN Version:  Python Version (if applicable): WSL version: ," I am using WSL2.  $nvcc version  has output: nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052023 NVIDIA Corporation Built on Tue_Jun_13_19:16:58_PDT_2023 Cuda compilation tools, release 12.2, V12.2.91 Build cuda_12.2.r12.2/compiler.32965470_0 $nvidiasmi has output ++ +++ notice that the CUDA version is different from using command $nvcc version. cudnn version: 8.9.7.29 Python version: Python 3.10.12 WSL version: 5.15.146.1microsoftstandardWSL2 I am not sure about the TensorRT version.",> same 20240426 16:37:38.371348: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20240426 16:37:38.371404: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20240426 16:37:38.371421: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered how did you solve it? I have the same problem in Colab,", for me, I just tried difference version combinations of the tools/packages. I am not not familiar with Colab. You probably can't have the software versions of your choice. ","for me work tesla p100, x9dri, ubuntu 22.04 conda python 3.10 sudo mkdir p /usr/lib/xorg/modules sudo aptget update sudo aptget install pkgconfig xorgdev sudo apt install libvulkan1 sudo apt install dkms nvidia driver 535.183.06 fo cuda 12.2 CUDA Toolkit 12.3 runfile local cudnnv8.9.7.29 **tar file** (instruction) tensorflow 2.16.1 for gpu for python 3.10 whl (https://storage.googleapis.com/tensorflow/versions/2.16.1/tensorflow2.16.1cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl) echo '/usr/local/cuda12.3/lib64' | sudo tee a /etc/ld.so.conf.d/cuda.conf sudo ldconfig .bashrc export PATH=/usr/local/cuda12.3/bin:$PATH export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda12.3/lib64:/usr/local/cuda12.3/extras/CUPTI/lib64 export CUDA_HOME=/usr/local/cuda12.3 sudo reboot",My algorithm uses Pytorch and gets the same error. Is there anybody to solve that?,"I have the same error messages with Docker Hub's Tensorflow image. Specifically, I issued the following command to start a Jupyter notebook with the latest Tensorflow: docker run it rm p 8888:8888 gpus all tensorflow/tensorflow:latestgpujupyter In the Jupyter notebook, I just did ""import tensorflow as tf"". Then I see these error messages. So apparently, it is not my local configuration issue as a Docker image should be self contained. ","Same issue found for me. CUDA12.3, CUDNN 8.9.  change to tensorflow 2.16.1, seems working","> After several days struggle, I am able to get rid of this errors. It's a version compatibility issue. You need to install the package versions according to the webpage. I was struggled on installing specific CUDA toolkit versions through deb packages, because the version was always updated to the latest version 12.4. In the end, I used the runfiles to install the toolkit. The other problem I had was that I started with tensorflow 2.15.1, which I could not get it to work. The versions that work for me are: CUDA toolkit: 12.2. cudnn: 8.9.7.29 Tensorflow: 2.16.1 Could you please do me a favour and convert me a model ? i need it urgently if possible please.","> Same issue found for me. CUDA12.3, CUDNN 8.9. change to tensorflow 2.16.1, seems working Could you please do me a favour and convert me a model ? i need it urgently if possible please.","> Same issue found for me. CUDA12.3, CUDNN 8.9. change to tensorflow 2.16.1, seems working Could you please do me a favour and convert me a model ? i need it urgently if possible please.",">  Hello, what do you mean by converting you a model? Do you just need a simple example with a model? I have one in  CC(Tensorflow version 2.16.1 has retracing problem for keras.model.train_on_batch(). ) . First thanks for your reply. I mean you fixed this error, you are able to use tensorflow with TensorRT correctly. I have python tensorflow model and want to convert it to JavaScript model. Could you please help me? I need it urgently for the university tonight . Thank you!",">  Sorry, I don't know JavaScript coding. No, it's not coding at all, only python script run it to convert the model and that's all.",">  Sorry, I don't know JavaScript coding. One line of python code and that's set", You may want to try google's colab to run it.,>  You may want to try google's colab to run it. I tried but same error 😔 and today is the deadline 😢. I have been trying colab for few days but doesn't work.,">  You may want to try google's colab to run it. If you could do me the favour, i would never forget it :)",No one  Whether from NVIDIA or TensorRT team really cares about this issue. Their documentation is terrible. Leaving Nonenterprise developers in lurch.,Same here  Windows 11 Pro with wsl2 and tensorflow 1.17.0 ,same problem how can you find out what CUDNN version is?,> how can you find out what CUDNN version is? Do you mean https://stackoverflow.com/questions/31326015/howtoverifycudnninstallation ? `cat /usr/include/x86_64linuxgnu/cudnn_v*.h | grep CUDNN_MAJOR A 2`,"Hi **** , Apologies for the delay, Please check your code with the recent TensorFlow versions (2.17.0 and 2.18.0) and let us know if the issue persists. And starting from TF2.14 tensorflow provides CUDA package which can install all the cuDNN,cuFFT and cubLas libraries. You can use  command for that. Thank you!"
yi,SuryanarayanaY,Fix checkfail with tf.raw_ops.Conv2DBackpropInput with MKL enabled.,The API tf.raw_ops.Conv2DBackpropInput with oneDNN enabled causing checkfail if strides<4. This is due to trying to access channel dimension 'C' with NHWC format without validating the strides dimension. Fixes CC(`tf.raw_ops.Conv2DBackpropInput` aborts due to lack of input check) .,2024-03-03T17:03:16Z,stale comp:mkl size:XS prtype:bugfix,closed,0,6,https://github.com/tensorflow/tensorflow/issues/63106,"Hi  Can you please take a look on the above comments from , . Thank you!",cc: MKL ,Done the changes as reviewed and suggested by MKL team.,Hi  Can you please check 's comments. Thank you!,This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,This PR was closed because it has been inactive for 14 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,SuryanarayanaY,Fix checkfail in tf.raw_ops.DrawBoundingBoxesV2,The API `tf.raw_ops.DrawBoundingBoxesV2` trying to access the `dim_size(3)` of image tensor before checking its rank. This is causing checkfail if image is not of rank 4. Fixes CC(`tf.raw_ops.DrawBoundingBoxesV2` aborts with inappropriate input),2024-03-02T17:06:06Z,ready to pull size:XS comp:core,closed,0,0,https://github.com/tensorflow/tensorflow/issues/63105
yi,drus-13,TFLite FlexDelegate Android the library is not linked error,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS  TensorFlow installed from (source or binary): https://github.com/tensorflow/tensorflow  TensorFlow version (or github SHA if from source): r2.14 Hi everyone! I have a problem with Tensorflow flex delegate on Android. I’m trying to run simple model with some functions that require Flex Delegate to be imported. I built `libModelTrainer.so` and `libtensorflowlite_flex.so` from tensorflowr2.14 for Android. I’m using CMake to build my own .so lib with included tf’s lib. So the first CMakeLists.txt looks like this  Than I link my own .so lib with main.cpp like this:  Than I get executable file successfully. I push these files (my own lib, tf’s libs, executable file and model) to my device with adb. Run program and still get error: > ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding “org.tensorflow:tensorflowliteselecttfops” dependency. See instructions: Selecione operadores do TensorFlow  |  TensorFlow Lite >  > ERROR: Node number 5 (FlexSave) failed to prepare. I ",2024-03-02T12:57:37Z,stat:awaiting response type:build/install type:support stale comp:lite Android TF2.14,closed,0,28,https://github.com/tensorflow/tensorflow/issues/63104,"    Hello guys, any suggestion? Maybe there is a more stable version where the delegate is guaranteed to work? Or what could be the problem? I will be glad of any help!","Hi , Please look into the issue Thank you","Hi 13, can you tell me the exact commands you used to both build the required files, pushed to the phone, and then run the execution on the phone? From reading the above it does not sound like you run any cmake or make commands on the phone. Feel free to exclude any PII, include any relevant information like which directory you are in when running the commands  it seems you have a separate project with your code  I understand that may not always be shareable but usually the more details you provide the less guesswork I will have to do. Thanks for your help. One possible alternative is trying to build libtensorflowlite_flex.so with bazel: https://www.tensorflow.org/lite/guide/ops_select and then linking this file with your executable.","Hi , thanks for your help. So, as I wrote above, I built libtensorflowlite_flex.so using bazel and linked the library to my code. The command I used is like `bazel build c opt config=android_arm64 config=monolithic tensorflow/lite/delegates/flex:tensorflowlite_flex` Alternatively, you can try using my sources https://github.com/drus13/mnist_tf Essentially, every piece of the application (lib and app folders) is built using the build_android.sh script",H  will there be an opportunity to take a look at what I have attached above. Or do you need any additional information from me?,"Hi 13, I'm looking into this... I am running into issues building the .so files ... if you can help me understand how you did it. i.e. what NDK and SDK did you use? How did you answer the ./configure script in the tensorflow source root folder? What command did you use to build libtensorflowlite.so? Something like this?  Thanks for your help","Hi , so for `libtensorflowlite_flex.so` i use configuration like this:  for `libtensorflowlite.so` same thing:  I hope this helps you, Thanks for your help","Hi 13, I'm further along .. I modified app/CMakeLists.txt to use relative paths:  When I run build_android.sh:  I don't have lib/build_android_arm64/libModelTrainer.so How did you build this? Thanks.","Hi , could you please run similar script build_android.sh from `mnist_tf/lib` folder? this should work Thanks.","Hi 13, I tried that and ran into various include issues, is your environment set up in such a way to pull in additional include files from somewhere else (not in the repo you shared)? That's what it looks like to me. I tried to also copy all headers from the tensorflow source (r2.14 branch) as well as absl (HEAD) library into the include folder, my last attempt looked like this:  We have a couple of options...  1. Try to mirror your environment more exactly. 2. Modifying mnist_tf/lib/CMakeLists.txt to look more like this: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/CMakeLists.txt 3. Adjust your project to use Bazel ""tensorflow/core/framework/function.pb.h"" is a generated file, so this is likely a problem with project setup.","Hi , sorry, there was an unused include in ModelTrainer.h. I updated repo with `libModelTrainer.so` and script build_android.sh from mnist_tf/lib also should work locally.  I hope this will help you, Thanks!","Hi 13, let's take a step back, why are you trying to run a C++ executable on your device? We can certainly make it happen .. but if you're trying to build an app the preferred way is with the Android NDK and JNI if you want to code in C++: https://developer.android.com/ndk/samples/sample_hellojni Reason I ask is, if we deviate from well defined paths, these type of issues tend to pop up a lot.","Hi,  good question, I plan to integrate this solution with another thirdparty library, and everything is very much tied to cmake and pure C++. As a result, I plan to get a computing application that will be launched(from background) on Android from under another application. In fact, I already tested this solution with tensorflowlite, everything was successfully assembled and worked in the end, the problem is that I need the flex delegate functionality (saving and loading weights, for example), so there is a need for tensorflowliteflex","Hi 13, thank you for the additional context, it is certainly always helpful, can you provide the exact adb commands you used to push to the device? Did you just put it all in one directory or is there any structure at the destination/device? Also the command used to actually execute the program.","Hi  well, I push all files model.tflite, UseModelTrainerApp, libModelTrainer.so, libtensorflowlite_flex.so, libtensorflowlite.so, test_set.txt, train_set.txt (from pushToDevice folder for example) adb push ""file name"" /data/local/tmp adb shell cd /data/local/tmp export LD_LIBRARY_PATH=/data/local/tmp:${LD_LIBRARY_PATH} ./UseModelTrainerApp","Hi 13, I was able to replicate your issue (Though I have not verified if everything we are doing is correct)... It seems linking to a .so file within Android may not be so trivial/well supported, I suspect us using it like normal Linux is causing these things to not link properly, can you try a simple integration of this code/app within your larger Android project and then add ""org.tensorflow:tensorflowliteselecttfops"" to your dependencies? Can you see if you can make a simple Native C++ project in Android studio and see if your code more or less works that way? Thanks for your help.", Hi! Ok I'll try to make a simple project in android studio and let you know. Maybe there is already source code in examples somewhere?,"Hi 13, sure thing... here's example code for hellojni: https://github.com/android/ndksamples/tree/androidmk/hellojni, this documentation is probably useful to guide you through it: https://developer.android.com/ndk/samples/sample_hellojni. You can also try asking gemini: https://gemini.google.com/, example prompt: ""Hi Gemini, can you make me a tutorial for creating a simple JNI/NDK hello world project in Android Studio?"" I would say get that working on an emulator or your own device then try to integrate your code (it should be mostly copy paste and some mangling of the interfaces from there ... but let me know if you get stuck).","Hi  Okay Thanks,  I'll try to create a project, but I meant, maybe there is an example project using tensorflowliteflex as you suggested?","Hi 13, I can't find an existing one but if you follow these directions: https://www.tensorflow.org/lite/guide/ops_selectandroid_aar, it should integrate flex ops to your project, please try it out and let  me know if it doesn't work.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Can i extend the life of the thread so that the bot doesn’t close it?) I'll try the suggested version soon and post the results,"13 yeah you can just do what you just did, just comment with ""bump"" or something, we still have to be fair to all issues so we will keep the usual policy, but also it does help remind users :).",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,bump,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,cupidp9,Issues in Tensorflow model training," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.13  Custom code No  OS platform and distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? 0 I am trying to run a pretrained tensorflow object detection model with my own datasets in google collab. But getting this unknown errors while running the training. I tried changing the batch size=2, and steps=20000, but still the same errors. Its just stopping the training with ^C. I noticed my system memory went its peak just before stopping. is it something due to this?  Standalone code to reproduce the issue   Relevant log output  !Screenshot 20240301 132219",2024-03-01T12:09:24Z,type:bug comp:model TF 2.13,closed,0,4,https://github.com/tensorflow/tensorflow/issues/63103,"Hi **** , When your system memory peaks, it can lead to the training process being killed to prevent the system from crashing. This is what the ""^C"" in your log indicates—it's the interrupt signal, typically generated by pressing Ctrl+C, but in this context, it might also indicate that the Colab environment killed your process due to excessive resource usage. Reduce Memory Usage: Reduce Batch Size: You have already tried reducing the batch size to 2, which is quite small, but if you are still hitting memory limits, consider if there is any other part of your pipeline that can be optimized for memory usage. Simplify Your Model: If possible, use a simpler or smaller model architecture that requires less memory during training. Optimize TensorFlow and GPU Usage: Since the logs mention TensorFlow trying to find TensorRT and CUDA devices and failing, it indicates that the training is attempting to run on a GPU but cannot find one. Google Colab offers free access to GPUs and TPUs, but you need to make sure you have requested access to these resources in your notebook   Thank you!","> Hi **** , >  > When your system memory peaks, it can lead to the training process being killed to prevent the system from crashing. This is what the ""^C"" in your log indicates—it's the interrupt signal, typically generated by pressing Ctrl+C, but in this context, it might also indicate that the Colab environment killed your process due to excessive resource usage. >  > Reduce Memory Usage: >  > Reduce Batch Size: You have already tried reducing the batch size to 2, which is quite small, but if you are still hitting memory limits, consider if there is any other part of your pipeline that can be optimized for memory usage. Simplify Your Model: If possible, use a simpler or smaller model architecture that requires less memory during training. >  > Optimize TensorFlow and GPU Usage: >  > Since the logs mention TensorFlow trying to find TensorRT and CUDA devices and failing, it indicates that the training is attempting to run on a GPU but cannot find one. Google Colab offers free access to GPUs and TPUs, but you need to make sure you have requested access to these resources in your notebook `(Runtime > Change runtime type > Hardware accelerator).` >  > Thank you! Thanks for your reply! Well, the size of my input images was the problem. They were too large and due to that reason the system memory was going at it's peak and stopping. ",Can anyone guide me how to contribute to this open source project i am novice please provide guidance.,Are you satisfied with the resolution of your issue? Yes No
rag,kjack45,TFC," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. L",2024-03-01T07:10:07Z,stat:awaiting response invalid TFLiteConverter,closed,0,2,https://github.com/tensorflow/tensorflow/issues/63100,"HI , This issue is exactly duplicate of  63081, please track there and you can close this issue. Thank You","This is just the template, nothing filled."
yi,jaslip,gcc 7.5 build fails on Tensorflow 2.12," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.12  Custom code No  OS platform and distribution Ubuntu 18.04  Mobile device _No response_  Python version _No response_  Bazel version bazel 5.3.0  GCC/compiler version 7.50  CUDA/cuDNN version 11.8/8.8  GPU model and memory _No response_  Current behavior? I need to build Tensorflow 2.12 with D_GLIBCXX_USE_CXX11_ABI=0, just building the C++ library.  Standalone code to reproduce the issue   Relevant log output ",2024-03-01T06:41:11Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.12,closed,0,4,https://github.com/tensorflow/tensorflow/issues/63099,", Tensorflow v2.12 is not compatible with compiler gcc 7.5 for the build. As per the official documentation Tensorflow **v2.12** is compatible with **GCC 9.3.1, Bazel 5.3.0, cuDNN  8.6 and CUDA  11.8** which are tested build configurations.  https://www.tensorflow.org/install/sourcegpu Could you try **bazel clean expunge** followed by bazel sync. Also kindly try to install the tensorflow v2.12 with GCC 9.3.1 compiler which is compatible. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
gemma,ahmednassar, Running gemma model in Mac m2, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.15.0  Custom code Yes  OS platform and distribution Macos 14.3.1  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In order to run the following gemma code I need keras 3.0.5 and `tensorflowtext` which is not compatible with Keras 3. `tensorflowtext` needs `tensorflowmacos=2.16` but there is no `tensorflowmacos>=2.16` as the following.     Standalone code to reproduce the issue   Relevant log output _No response_,2024-02-29T21:43:37Z,stat:awaiting response type:bug type:build/install subtype:macOS TF 2.15,closed,0,15,https://github.com/tensorflow/tensorflow/issues/63096,", Looks like this issue is not related to Gemma Model. Instead of using **pip install tensorflowmacos==2.16.0**, could you please try using **pip install tensorflow** as mentioned in the document.  https://github.com/tensorflow/tensorflow/releases Thank you!","Hi  , The venv has `tensorflow 2.16.0rc0` as shown in the below pip command. I think the problem in `tensorflowtext`, why it ignored `tensorflow` and ask for `tensorflowmacos`. Thanks in advance ","From another perspective, why tensorflowmacos has no upgrade support keras 3.0.5?  ","I have just tested `v2.16.0rc0` on my Macbook (Apple M1 Pro) and apparently the statement that `tensorflowmacos` is no longer required, is not true. I observe the following error:  which normally would get fixed after installing `tensorflowmacos` explicitly.  I'm wondering if these releases are tested after all.  PS: Tested with Python 3.11 on macOS Sonoma 14.3.1","My Apple is M2, I'm not able to install `tensorflowtext` using pip nor local build as the following  ========================= ","> My Apple is M2, I'm not able to install `tensorflowtext` using pip nor local build as the following >  >  >  > ========================= >  >  Same issue here with Mac M2 chip. It says  However, when I tried to install `tensorflowtext`, it says  Also, I am curious whether Gemma requires `tensorflow`. I have changed the backend to `torch` using `os.environ[""KERAS_BACKEND""] = ""torch""`. Why does it still require `tensorflowtext`?","While we wait for TensorFlowtext to work with TF 2.16, you could try a workaround to use Gemma with TF 2.15.  Can you try these steps and see if that resolves the issue of using Gemma model from KerasNLP?","`tensorflowtext` is still having dependency on `tensorflowmacos `, this dependency will be changed to tensorflow and the new release of `tensorflowtext 2.16.2` will be made available with this patch. Till then, continue using the workaround as suggested in the above comment.","That doesn’t work, tensorflow 2.15 downgrade Keras (uninstall and reinstall Keres 2.15). Then I can’t install Keres 3.0.5",`tensorflowtext` 2.16.2 will most likely be released today and that should fix this issue.,"Actually, `tensorflowtext` is not available for Mac M1/M2. You could build your own using this blog  https://forums.developer.apple.com/forums/thread/700906 But at this time, we don't have it. Since its a dependency for KerasNLP, we will not be able to use Gemma in mac M1/M2 unless you can build from source for `tensorflowtext`.","> `tensorflowtext` 2.16.2 will most likely be released today and that should fix this issue. Thanks  that fixed the issue, one small notice, the fix committed to master not 2.16.2. Thanks again Please advice if we should close the close the issue?"," , Please close the issue if you're issue is resolved.",The problem have been fixed (here)[https://github.com/tensorflow/tensorflow/issues/63096issuecomment1994918684] and I'm able to run the following code in my M2 chip Apple ,Are you satisfied with the resolution of your issue? Yes No
yi,SuryanarayanaY,Fix checkfail in tf.raw_ops.Substr,The API `tf.raw_ops.Substr`  currently validates whether the input args `pos` and `len` are of same shape or not.Its not checking whether these tensors are empty or not and trying to access the Tensor values directly without validating.If a user passes empty tensors it will lead to` assertion failure` causing core dumped error. May fixes CC(`tf.raw_ops.Substr` can lead to eigen assertion failure),2024-02-29T15:08:23Z,stat:awaiting response ready to pull size:XS comp:core,closed,0,2,https://github.com/tensorflow/tensorflow/issues/63091,This is being rolled back. We need a different fix,Let's try again with `>= 0` instead of `> 0`
yi,zzj0402,ValueError: Only instances of `keras.Layer` can be added to a Sequential mode," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version v2.15.0rc18g6887368d6d4 2.15.0  Custom code No  OS platform and distribution Kaggle  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When trying to build a sequential model with TFhub model USE in Kaggle, the notebook throws an error complaining that the hub model is not a valid class instance. At the same time, the same code works fine locally.  Standalone code to reproduce the issue  shell ValueError                                Traceback (most recent call last) Cell In[9], line 1 > 1 model=build_model(embed)       2 model.fit(descriptions, labels, epochs=4)       3 model.save('quality/use.keras') Cell In[8], line 12, in build_model(embed)      10 def build_model(embed): > 12     model = Sequential([      13         Input(shape=[], dtype=tf.string),      14         embed,      15         Dense(1, activation='sigmoid')      16     ])      17     model.compile(loss='mean_squared_error',      18               optimizer=tf.keras.optimiz",2024-02-29T03:27:03Z,stat:awaiting response type:bug stale comp:keras TF 2.15,closed,0,26,https://github.com/tensorflow/tensorflow/issues/63085,"Hi  , The code executes fine on Colab as per gist. This moght be related to Kaggle environment which might have incompatible TF and tensorflow_hub version. You may check the Tensorflow and tensorflow_hub in kaggle environment and try to install those works in locally. Neverthless it's not problem with Tensorflow or Keras. Thanks!","i got the same issue also with Kaggle's environment, with  conclusion i got the solution by just  `pip install tensorflow`  again and found out that the keras library reinstalls to latest version ", fixes the problem in Kaggle.,Are you satisfied with the resolution of your issue? Yes No,"i had the same issue, tried all the things explained here but still having the same issue. i am not using kaggle i am coding on jupyter notebook. please help"," I am also having same issue. I am using jupyter notebook as well with PyCharm. I have tried many things but cannot fix this error. I believe it might be a package incompatibility error. However, I am using most up to date versions. ","im also having this using conda, tried conda install again tensorflow and it no compatible with tensorflow_hub that i installed before, please help",Has someone resolved this for jupyter notebook / conda enviroment? I am also having a similar problem (although with tensorflow probability),"I got the same error when running the official notebook, https://www.tensorflow.org/tutorials/interpretability/integrated_gradients, the environment I am using is tensorflow==2.16 and tensorflowhub==0.16. But if I roll back to tensorflow 2.14 and tensorflowhub 0.15, there is no such error.","I've tried to use some previous versions of tensorflow, but on MacOS there was another bug that was't allowing me to use GPU acceleration. So I've solved the problem using the Model Subclassing API, here the example from my code:     class MyModel(keras.Model):       def __init__(self):         super(MyModel, self).__init__()         self.embedding_layer = embed         self.dense_layer1 = layers.Dense(256, activation='relu')         self.dense_layer2 = layers.Dense(128, activation='relu')         self.dense_layer3 = layers.Dense(64, activation='relu')         self.output_layer = layers.Dense(13, activation='softmax')       def call(self, inputs):         x = self.embedding_layer(inputs)         x = self.dense_layer1(x)         x = self.dense_layer2(x)         x = self.dense_layer3(x)         return self.output_layer(x)","do anyone know how to resolve this error, i have been trying and reinstalling the tensorflow to its latest but still its gets the error ",i cant even install older version of tensorflow the oldest i can install in 2.16,"Consegui añadir el modelo con una lambda import tensorflow_hub as hub  Cargar el modelo base preentrenado base_model = hub.KerasLayer(""https://www.kaggle.com/models/google/mobilenetv3/TensorFlow2/large075224classification/1"",                             trainable=True, arguments=dict(batch_norm_momentum=0.997))  Construir el modelo model = tf.keras.Sequential([      Capa de entrada     tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),      Envolver la capa del modelo base en una capa Lambda     tf.keras.layers.Lambda(lambda x: base_model(x)),     tf.keras.layers.Dropout(0.2),      Capa densa final para clasificación     tf.keras.layers.Dense(3, activation='softmax') ])  Compilar el modelo model.compile(     optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),     loss='sparse_categorical_crossentropy',     metrics=['accuracy'] )  Callbacks early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True) reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5, min_lr=1e6) model.summary()",For colab users: by downgrading your Tensorflow and TensorHub `!pip install tensforflow  == 2.8.0 and tensorflow_hub == 0.12.0`,"For Jupyter Notebook users: I have installed these version of Tensorflow and tersorflow_hub: TensorFlow version: 2.14.0 TensorFlow Hub version: 0.15.0 and changes input_shape from [None,224, 224, 3] to [224, 224, 3] (removed batch value) and these changes resolved the issue for me.",>  >  > fixes the problem in Kaggle. The error pops up again. Seems like version issue.,fixed with:  This is definitely a Tensorflow issue where the package management fails to check backward capability.,"Hi  , you forgot to run `build_model(embed)` on your gist, when run, it will raise exactly the same error.",**you have to wrap the hub layer in a Lambda layer like below:**   I fix my code by using lambda layer I hope it is useful for you,"> i cant even install older version of tensorflow the oldest i can install in 2.16 Same, I can only install the version starting from 2.16.1 ","Tensorflow 2.17 contains Keras3.0 which might be the reason for the error. Could you please try to install tfkeras i.e., keras2.0 and try to  test the code and the if the issue is present, please raise the issue in the kerasteam/keras repo as it is more related to Keras.  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"> fixed with: >  >  >  > This is definitely a Tensorflow issue where the package management fails to check backward capability. Thanks, this worked for me","> **you have to wrap the hub layer in a Lambda layer like below:** >  >  >  > I fix my code by using lambda layer I hope it is useful for you This worked. Although model.summary() not showing anything for layers. So, I believe this fix is just temporary and might get fixed on future versions."
yi,Icacoding1,DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed., Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.10.0  Custom code Yes  OS platform and distribution Windows 11  Mobile device Android  Python version 3.9.18  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? the tensorflow is already installed but when I am going to import it there is an error  Standalone code to reproduce the issue   Relevant log output ,2024-02-28T16:41:49Z,stat:awaiting response type:build/install stale subtype:windows TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/63082," The error originates from tensorflow.python.pywrap_tensorflow.py, indicating an issue loading the native TensorFlow runtime, which is a Dynamic Link Library (DLL) on Windows. Please download and install the appropriate Microsoft Visual C++ Redistributable from https://learn.microsoft.com/enus/cpp/windows/latestsupportedvcredist?view=msvc170 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,Milehigh-wrld,AIML.MHW," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. L",2024-02-28T15:24:29Z,stat:awaiting response comp:lite invalid TFLiteConverter,closed,0,2,https://github.com/tensorflow/tensorflow/issues/63081,"Hi wrld, As per your query even though the conversion is successful, but model produces wrong results and less due to insufficient data, architectural flaws, in correct hyperparameter tuning.  The model produces correct results and is slower than expected: This is right, even though the model produced correct results, based on the target hardware the speed might differ. Based on the hardware different optimization techniques help to improve speed. In your use case, the tflite model conversion is successful and accuracy also as expected. Refer the gist for accuracy and size.  Still if any specific query regarding the model, please feel to elaborate. The issue is the same as  62833).  Thank You",Closing as it is only the template
yi,ACE07-Sev,ModuleNotFoundError: No module named 'tensorflow.python', Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.8  Custom code No  OS platform and distribution Windows 10  Mobile device _No response_  Python version 3.11.7  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to import tensorflow but get the error I mentioned.  Standalone code to reproduce the issue   Relevant log output ,2024-02-28T10:10:12Z,stat:awaiting response type:build/install stale subtype:windows TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/63078,"Sev Please ensure that you're using the Python environment where TensorFlow is installed.  Kindly run `python version ` to check. If you haven't installed TensorFlow yet, use the following command in your terminal:  Could you try to use the latest TF version, please use the following;  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,Sehun0819,Conv2D backprop APIs can lead to assertion failure at graph optimization step," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04 LTS  Mobile device _No response_  Python version 3.11.7  Bazel version 6.5.0  GCC/compiler version clang 16  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Conv2D backprop APIs can lead to assertion failure at graph optimization step. Error location:  Here It tries to get strides from input. But because there is no guard that checks size of `strides`, it ends up with assertion failure at `GetStrides`:  Debug build: ",2024-02-28T05:19:29Z,stat:awaiting response type:bug stale comp:ops TF 2.15,closed,0,6,https://github.com/tensorflow/tensorflow/issues/63076,"Hi  , I can see missing validation check for strides in the API Conv2DBackpropFilter. For other APIs DepthwiseConv2DNativeBackpropFilter & DepthwiseConv2DNativeBackpropInput there is check for `strides.size() `and it should raise error if `strides.size() !=4` . https://github.com/tensorflow/tensorflow/blob/6fd130f8ad839257a274212fbd935b35f2625584/tensorflow/core/kernels/depthwise_conv_grad_op.ccL559 https://github.com/tensorflow/tensorflow/blob/6fd130f8ad839257a274212fbd935b35f2625584/tensorflow/core/kernels/depthwise_conv_grad_op.ccL1070 Could you please confirm whether the behaviour happens with same code above where `strides.size!=4`",For Conv2DBackpropInput it should check strides.size here. https://github.com/tensorflow/tensorflow/blob/6fd130f8ad839257a274212fbd935b35f2625584/tensorflow/core/kernels/conv_grad_input_ops.hL294,"Hi  , The proposed PR got merged.You can you test with nightly and confirm if this is still an issue?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,Sehun0819,`tf.raw_ops.ArgMax`: Heap buffer overflow, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04 LTS  Mobile device _No response_  Python version 3.11.7  Bazel version 6.5.0  GCC/compiler version clang 16  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? `tf.raw_ops.ArgMax` can lead to heap buffer overflow. Error location:  It copies scalar value of `dimension` without checking exact type. Therefore when `dimension` is an `int16` it reads over the bound. Note that `int16` is an allowed type for `dimension` according to opdef:   Standalone code to reproduce the issue   Relevant log output The below log needs ASAN build. ,2024-02-27T13:06:46Z,awaiting review type:bug comp:ops TF 2.15,open,0,4,https://github.com/tensorflow/tensorflow/issues/63070, Could you please have a look at the gist and confirm the issue? Thank you!,"  Even it does not print out ASAN log(as it wasn't built with ASAN flags), the error message seems to be an evidence. `Expected dimension in the range [4, 4), but got 1945239553` `1945239553` is `0x73F20001` in hexadecimal. I guess the right part, `0x0001` came from 2byte input argument `tf.constant(1,shape=[],dtype=tf.int16)` and the left part `0x73F2` is the 2bytes that was not meant to be read."," , Thanks for reporting. Proposed a fix which might fix this issue. "," Thank you! There is one more code we should handle, here:  This leads another heap buffer overflow, because it just assumes int32 and int64. Reproduction:  Log: "
yi,Doomski99,Support for Quantized ELU is missing in TFLite MLIR converter,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WSL2 Ubuntu  TensorFlow installed from (source or binary): pip  TensorFlow version (or github SHA if from source): Latest Unlike ReLU for example, currently ELU isn't supported for 8 bit quantization in TFLite: !image I think we can confirm this by looking at ELU's definition in tfl_ops.td , it doesn't have the quantizable trait. While researching upon this issue, I stumbled upon an old stackoverflow issue  implying that quantized ELU was once supported. Maybe it was skipped after the move to MLIR? Are there plans to support it? If not, will it be straightforward to implement it by my own?",2024-02-27T10:53:54Z,type:feature comp:lite TFLiteConverter awaiting PR merge TF 2.16,open,0,5,https://github.com/tensorflow/tensorflow/issues/63065,"Hi , I tried `elu` activation on a sample model and converted to tflite with different quantization schemes and it is working fine. Please refer to the gist. If your use case is different from this, please elaborate it w.r.t ` def TFL_EluOp: TFL_Op<""elu"" ` in tfl_ops.td and  share a reproducible code for further steps. Thank You ","Hello ,  Thanks for taking the time to look at this issue. I was looking at the graphs produced by your gist and they were all the same because I think you were mistakenly saving the original nonquantized model each time: You've written:   Instead of:  After that modification, you can look at the graph (in this case, converted_model4) and see: !image Which is odd, we would expect the converter to output an error since you've added to the code this line:  and clearly the ELU isn't an INT8 operation in this case.  Concerning tfl_ops.td, we can confirm that ELU doesn't support quantized inputs from it's definition and by comparing it to LeakyReLU for example: ELU definition:  LeakyReLU definition:  Note how ELU is missing the ""QuantizableResult"" trait and its arguments are missing the quantized data types ""QUI8"" and ""QI8"". ","Hi  , Appreciable your efforts in bringing the feature request to our attention. Yes, as you mentioned `elu`  definition doesn't support  quantizable trait. The observation is clearly presented in the below screenshot. !image  Our team will work on it and will update you. Thank You","Hi , Please track the PR  CC(Needs support for the data types : QUI8, QI8, TFL_Quint8, QI16 in  ELU's definition at tfl_ops.td ), for feature request. Thank You","Hi,   Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/78 Let us know if you have any questions. Thanks."
yi,priyakansal,Getting wrong output after tflite conversion," 1. System information  OS Platform and Distribution = **Ubuntu 20.04**  TensorFlow installation (pip package or built from source): **pip installation**  TensorFlow library (version, if pip package or github SHA, if built from source):**tf2.12.1**  2. Code Model is converted using the following code:   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong: Following Code is used for the prediction:   **Model produces wrong results:** The attached images shows the results before and after conversion for the same input image: Before Conversion/Keras Model Results: !keras_model_output After converting into tflite/tflite model results: !tflite_output  5. (optional) Any other info / logs Following are the logs while converting: ",2024-02-27T08:24:46Z,stat:awaiting response type:support stale TFLiteConverter TF 2.12,closed,0,8,https://github.com/tensorflow/tensorflow/issues/63061,"  Could you please suggest, what may be the reason for the wrong results?",  Is there any update?," Sorry for the late response! The model was saved and loaded successfully with some warnings about untraced functions. Could you consider disabling debuglevel logging if it clutters the output. If you're encountering issues after loading the model, please investigate the missing placeholder values (inputs and Placeholder/_0). Thank you!","Hello  ,  > If you're encountering issues after loading the model, please investigate the missing placeholder values (inputs and Placeholder/_0). what should I do for the above?"," Here is  an example;  Please remember to replace the placeholder names and data preparation with your specific model and code. By using either print statements or the session approach, you can identify if the inputs and Placeholder/_0 placeholders are not receiving the necessary data during inference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
llm,claCase,Custom Keras RNN with constants changes constants shape when saving , Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15  Custom code Yes  OS platform and distribution Windows WSL  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Constants shape changes from rank 2 during inference to rank 3 tensor during model saving  Standalone code to reproduce the issue   Relevant log output ,2024-02-25T06:56:30Z,type:bug,closed,0,2,https://github.com/tensorflow/tensorflow/issues/63047,I've opened the issue in the keras repo ,Are you satisfied with the resolution of your issue? Yes No
yi,ek-ex,Overlapping window with tf.data.experimental.make_csv_dataset," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.8  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Window dataset coming from the  tf.data.experimental.make_csv_dataset is not working as expected  Standalone code to reproduce the issue  This produces the following structure:  This is not allowing me to transform in a simple way because OrderedDict has not batch method and I cannot flatten following the documentation.  Gives the following error:  If I try to batch the datasets of the OrderedDict, I get the following error   This is becoming extremely confusing. What would be a the right way to transform this structure so that I can later apply better transformations to build a timeseries dataset. ```  Relevant log output _No response_",2024-02-24T20:39:20Z,stat:awaiting response type:bug stale comp:data TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/63044,"Hi ex , Instead of applying flat_map directly on a WindowDataset could you please try batch seprately. ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Hi  thank you for your response.  Applying `.batch(5)` to the dataset generates a NestedVariant dataset and I havent found a way to extract the contents of the dataset or iterate over it.  The structure that this generates is:   prints the following:  ,"Hi ex , It would help us to debug if we can get a reproducible code snippet along with the dependent resources. Thanks!","Hi ex , This still missing the dataset CSV file. Please refer gist and provide the details.  Please also note that this repo is for reporting bugs and performance related issues. If it is pure support you can reachout TFforum also with all the details.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
gemma,lgemc,Running gemma model in Mac m1 gets xla_compile_on_demand_op error," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.15.0  Custom code Yes  OS platform and distribution macOS Sonoma 14.1.2  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory Macbook pro m1 16G  Current behavior? I expect get the model working, if I can help adding support to missing features I probably need some guidance but I can help 😸 Keras version: 3.0.5 I try same TensorFlow and Keras version using cuda on linux and I get the model working 🙈  Standalone code to reproduce the issue   Relevant log output  ```",2024-02-24T02:45:55Z,stat:awaiting response type:bug stale comp:ops TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/63043," Please ensure that you're using TensorFlow version 2.16.0 or higher, as it includes better M1 compatibility and might resolve the XLA compilation error. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,matthew-olson-intel,[oneDNN] Add oneDNN version of SparseMatrixMatMul (v2),"This is version 2 of CC([oneDNN] Add oneDNN version of SparseMatrixMatMul), which should fix the build on ARM64. Adds `_MklNativeSparseMatrixMatMul` and its accompanying kernel, which uses oneDNN to multiply a CSR sparse matrix by a dense tensor. The op is enabled with an environment variable (`TF_ENABLE_ONEDNN_SPMM`), so is entirely optin. It also includes tests and a benchmark, which we've used below to measure its performance against the existing kernel. The performance looks promising particularly for larger shapes, and is optimized to use the AVX2 and AVX512 ISAs. These results were collected using the new benchmark in `tensorflow/core/kernels/mkl/mkl_sparse_matrix_matmul_op_benchmark.cc` on an Intel Xeon Platinum 8480 with hyperthreading enabled. To minimize NUMA effects, we bound it to the first socket. Configuration (NNZ_M_K_N)  1.82",2024-02-22T20:34:18Z,awaiting review ready to pull size:XL comp:core,closed,0,6,https://github.com/tensorflow/tensorflow/issues/63030," , JFYI, this PR also enables an optin CSR matmul for ARM. Currently, a reference implementation will be used when running on ARM.", This is version 2 of the PR; should have fixed the ARM64 build issue that arose in overnight testing from the last version.,"Great  hopefully this time, we've fixed the nightly tests!","Thanks for the heads up and for fixing the failure! I should say that if this flag goes from optin to optout before there’s an optimized implementation of SPMM in oneDNN for AArch64, then it will need a platform specific guard to stick with Eigen."," No problem! Yes, definitely: that's exactly one of the reasons that it's optin.","So all of the internal overnight checks are successful, now?"
yi,RageshAntonyHM,Failure in convert Gemma 2B models to TfLite,"I tried converting Google Gemma 2B models to TfLite. Found it ending in failure   1. System information   Ubuntu 22.04  TensorFlow installation (installed with kerasnlp) :  TensorFlow library (installed with kerasnlp):  2. Code   3. Failure after conversion I am getting this error: tensorflow/core.py"":65:1))))))))))))))))))))))))))]): error: missing attribute 'value' LLVM ERROR: Failed to infer result type(s). Aborted (core dumped)  5. (optional) Any other info / logs ",2024-02-22T07:33:01Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter,closed,0,63,https://github.com/tensorflow/tensorflow/issues/63025,"Hi , I am trying to reproduce the issue while I had another error `ModuleNotFoundError: No module named 'keras_nlp.backend`, could you please confirm the version of it. Thank You",  it is keras 3.0.5 and installed kerasnlp via pip install git+https://github.com/kerasteam/kerasnlp (0.8.1),  first install ` pip install git+https://github.com/kerasteam/kerasnlp` and then update Keras (`pip install U keras`),Then install tensorflowdatasets also  ,Also crashing in Colab with or without quantization.,  This conversion pipeline needs lot of Vram. At Least 24 GB.   any updates on this please?,  I got same crash in colab A100(40GB GPU RAM).,"  Yeah. Actually, till it is crashing for me in 48 GB RTX 6000.  (What I told to   was,  it will crash prematurally if VRAM is low. But also crashes in final step even if you have enough VRAM) ",I saw that training is working OK having installed first TensorFlow nightly version (2.17.0dev20240223).  can you try with nightly version and check again the conversion?,"  How to install  TensorFlow nightly version? I tried pip install tfnightly, but I am getting error    File ""/usr/local/lib/python3.10/distpackages/keras/src/backend/tensorflow/core.py"", line 5, in      from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice ModuleNotFoundError: No module named 'tensorflow.compiler.tf2xla' Name: tfnightly Version: 2.17.0.dev20240223",I work with Colab. So it is !pip install tfnightly !pip install q upgrade kerasnlp !pip install q U keras>=3,"  Now, again I am getting that first mentioned error  could you please share your notebook link ?",The colab is from this example https://ai.google.dev/gemma/docs/lora_tuning I have changed nothing. So the idea is if you install tfnightly the error for conversion disappears? I don't understand from your previous answer if the error is during tfnightly installation or during conversion.,"  I hope some package conflicts ,like some packages reinstall 'stable' version of tensorflow. Let me check","  I able to ran inference already. my problem is, i need to create a TFlite model for Gemma 2B. I think there is some problem still in conversion  i am very new to AI and even python. ",">  >  > I able to ran inference already. my problem is, i need to create a TFlite model for Gemma 2B. I think there is some problem still in conversion >  > i am very new to AI and even python. Then we have to wait a little bit so the TF team solves this and provide us the tfnightly version we can use to convert it.","  `import keras_nlp.backend import ops  ` is not needed. Sorry  But when using all nightly versions, I got some ""GraphDef"" issue ","a minimal script to reproduce the issue  I tested with tf2.15, 2.16, and 2.17 nightly and their corresponding packages. None of them works.","Hi , I have reproduced the issue in Colab with TF 2.15, the session crashed at the step `generator = keras_nlp.models.GemmaCausalLM.from_preset(""gemma_2b_en"")` . Please take a look. Thank You",Adding  and  for visibility.,"I believe colab is running out of memory for  's case, In attempting to replicate the below, I am running into tensorflowtext installation issues (apparently the Gemma tokenizer uses it for the tokenizer), this may be because of the new 2.16 release.  my error: ",I think there is an answer here that it is working: https://github.com/kerasteam/keras/issues/19108 It is based on this comment: https://github.com/kerasteam/keras/issues/19108issuecomment1913421572 So my code now is:  With the above the conversion finishes and the .tflite model is running into android. I have not used quantization since it is failing into android.,"  I ran like this:  Iit fails at ""converting"" with this error ""GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity] name:     "" :  Am I doing something wrong ?",You can skip the Kaggle_key...😀 I think it's a memory error,"  I rented 48 GB GPU, now got another error  :   The process terminates with ""killed"" message. Didn't enter ""writing"" ! "," this looks like the OS terminated the process, maybe due to memory consumption/cpu time limitation?", looks like it is still memory and compute issue,> I think there is an answer here that it is working: kerasteam/keras CC(Set the weights in tf.layers with other variables but not as initializers.) It is based on this comment: kerasteam/keras CC(Set the weights in tf.layers with other variables but not as initializers.) (comment) >  > So my code now is: >  >  >  > With the above the conversion finishes and the .tflite model is running into android. I have not used quantization since it is failing into android. I can cofirm that I could get tflite by using:  instead of   Note that the converter seems not memory efficient; I observed more than 90 GiB virtual memory was needed on my desktop machine., can you share your working colab here ,">  can you share your working colab here nope, because I tested it with a simple script on my local machine; didn't try to deal with memory issues in Colab  :)  For test, I modified keras_nlp to have fixed tensor dimensions. That's it."
yi,eneskelestemur,load_model() cannot load model from previous version," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.14 and tf 2.15  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hello,  I was trying to load a model I previously saved with tf 2.14, and got the displayed error. I realized that the model was saved with tf 2.14, and the environment I was trying to load the model in was tf 2.15. I'm not sure if this is an expected behavior, but it seemed odd since the version difference wasn't that big.  Standalone code to reproduce the issue   Relevant log output ",2024-02-21T23:21:33Z,stat:awaiting response type:bug stale comp:model TF2.14,closed,0,5,https://github.com/tensorflow/tensorflow/issues/63021,"Hi **** , Sorry for late reply, I tried to run your code on Colab using TF v2.14 and 2.15 faced the same issue. Please find the gist here for reference.  Thanks you!","Hi  , I tried to run your code on colab using TF v2.16.1 with nightly now it is working fine. Could you please check with recent version. Here i providing gist for your reference. Thank you",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,vanskarner,Dataset is never fully read when caching to disk," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.13.1  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version Python 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When displaying the dataset elements for viewing with either `train_data.take(1)` or `train_data.take(1).cache()` it always shows the message ""The calling iterator did not fully read the dataset being cached..."". The following has already been tried, but the same message always appears:  There is a similar problem but the bot closed it due to inactivity: CC(Dataset is never fully read when caching to disk)   Standalone code to reproduce the issue   Relevant log output ",2024-02-21T19:14:40Z,stat:awaiting response type:bug stale comp:apis TF 2.13,closed,0,6,https://github.com/tensorflow/tensorflow/issues/63019,", I tried to execute the mentioned code with **train_data.take(1) or train_data.take(1).cache()** on both tensorflow v2.15 and v2.13, and observed that the output is intended and also the image is also visible. Kindly find the gist of it here and let me know. Thank you!","> , I tried to execute the mentioned code with **train_data.take(1) or train_data.take(1).cache()** on both tensorflow v2.15 and v2.13, and observed that the output is intended and also the image is also visible. Kindly find the gist of it here and let me know. Thank you! Hi, when the code is executed in the google colab there are no resulting messages, however based on the template I filled in(OS platform and distribution) for this issue I still get the same message even if I upgrade to tensorflow v2.15, either using train_data.take(1) or train_data.take(1).cache() generate the same message described in Relevant log output.",", I suspect this is happening only with the Windows OS. When I tried to execute the same code on the other environments like Linux, MacOS and Colab, I don't face any issue/warning with the tensorflow 2.15. Also the message is the Warning(W) which might not affect the execution of the code. Thank you! ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,lbertho-gpsw,Compilation error on macos with GPU delegate," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15.0  Custom code No  OS platform and distribution Macos 14.3.1  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version Apple clang 15.0.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm trying to build tensorflowlite with the GPU delegates with CMake. It works well with Linux and Android (gcc), but for Macos (apple clang compiler) , I have a compilation error: `tensorflowlite/tensorflow/lite/delegates/gpu/api.cc:72:10: error: no matching function for call to 'visit'   return std::visit(ObjectTypeGetter{}, object);` It complains about using std::visit on a absl::variant. My guess is that std::visit with gcc is more permissive than the standard about std::visit and allow to use it with not only std::variant but also with other variantlike. On the other side, Clang is not that permissive. I tried with the source from the 2.15.0 and also with the sources of the latest commit.  Standalone code to reproduce the issue   Relevant log output ",2024-02-21T14:45:55Z,stat:awaiting tensorflower type:build/install comp:gpu subtype:macOS TF 2.15,open,0,1,https://github.com/tensorflow/tensorflow/issues/63014,"I followed these directions on MacOS: https://www.tensorflow.org/lite/guide/build_cmakeopencl_gpu_delegate on r2.15 and nightly. I'm running into a different issue actually: This part works fine:  but  fails here:  , can you please take a look? Thanks."
llm,elfringham,Unit test failures with Python 3.12 and gcc, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version git HEAD  Custom code No  OS platform and distribution Ubuntu 20.04  Mobile device n/a  Python version 3.12.1  Bazel version 6.5.0  GCC/compiler version 10.2.1  CUDA/cuDNN version n/a  GPU model and memory n/a  Current behavior? The following tests fail //tensorflow/python/eager:backprop_test_cpu //tensorflow/python/eager/polymorphic_function:tracing_compilation_test //tensorflow/python/eager:forwardprop_test_cpu //tensorflow/python/saved_model:load_test_cpu //tensorflow/python/eager/polymorphic_function:polymorphic_function_test_cpu  Standalone code to reproduce the issue   Relevant log output ,2024-02-21T13:02:23Z,stat:awaiting tensorflower type:bug type:build/install subtype: ubuntu/linux,closed,0,2,https://github.com/tensorflow/tensorflow/issues/63012,Building with debug enabled makes the tests pass. This is an indication that there is a programming error causing undefined behaviour.,Are you satisfied with the resolution of your issue? Yes No
yi,igormintz,tf.data.Dataset.from_tensor_slices name argument is not properley saved nor shown," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17.0dev20240220  Custom code Yes  OS platform and distribution colab, mac  Mobile device _No response_  Python version 3.10.5  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? when trying to name a datset using `tf.data.Dataset.from_tensor_slices`, the name is saved under ._name and not name. When printing the ds, the name is None  Standalone code to reproduce the issue   Relevant log output ",2024-02-20T18:48:52Z,stat:awaiting response type:bug stale comp:data,closed,0,7,https://github.com/tensorflow/tensorflow/issues/63001," when you assign a name to a dataset using tf.data.Dataset.from_tensor_slices, the actual name gets stored in an internal attribute ._name instead of the public name attribute. This can be confusing, as printing the dataset won't show the assigned name. Could you use the `._name `attribute:  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,why not change the method form ._name to .name? how to make the name stick to the tensor so it will show up when printed?," TensorFlow maintains a consistent API for interacting with tensor attributes. Using a public property like .name ensures uniformity across different operations and functionalities. You can make the name stick to the tensor so it shows up when printed by assigning a name during its creation. You can assign names to any type of tensor created using TensorFlow functions like tf.constant, tf.zeros, tf.random.normal, etc. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,2JS,Fix TensorFlowLiteSwift `Interpreter` use-after-free bug,"TensorFlowLiteSwift `Interpreter` has useafterfree bug, releasing underlying `Model` after `init(modelData: Data)`, causing `EXC_BAD_ACCESS`.",2024-02-20T14:11:02Z,awaiting review comp:lite ready to pull size:XS,closed,0,10,https://github.com/tensorflow/tensorflow/issues/62998,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",Hi  Can you please review this PR ? Thank you!,Are there any internal updates for this issue?,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,"Hi  , Can you please review this PR ? Thank you!","Hi  , Can you please review this PR ? Thank you!","Hi  , Can you please review this PR ? Thank you!","Hi  , Can you please review this PR ? Thank you!"
yi,lxzheng,Full Integer Quantization Issue with Multiple Signatures in TensorFlow Lite," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 22.04.3 LTS  TensorFlow installation (pip package or built from source):pip  TensorFlow library (version, if pip package or github SHA, if built from source):2.15.0  2. Code To help reproduce this issue, I am providing a link to a custom Colab notebook: Full Integer Quantization Issue with Multiple Signatures in TensorFlow Lite  3. Failure after conversion In the dynamic range quantization process of TensorFlow Lite, it appears that for models with multiple signatures (including aliased ones), the quantization treats references to the same computational graph as a single entity. This is evidenced by the  TFLite ModelAnalyzer report showing two subgraphs with identical sizes, yet the overall model size corresponds roughly to the size of a single subgraph. Specifically:      TFLite ModelAnalyzer Output of  Dynamic Range Quantization:          The total model size is not the sum of the two subgraphs, suggesting that the same subgraph is counted twice but only stored once. However, the situation is markedly different in the full integer quantization process. Here, the quantization leads to two subgraphs with significantly",2024-02-20T12:58:00Z,stat:awaiting tensorflower comp:lite type:performance TFLiteConverter ModelOptimizationToolkit TF 2.15,closed,0,5,https://github.com/tensorflow/tensorflow/issues/62996,"Hi , I have reproduced the issue without signatures and with signatures. Without signatures the model is behaving as intended but the full integer model is not quantized properly with signatures. Here is the gist for reference. Tried with the 'tf nightly' version but the session is getting crashed. Here is the gist with nightly version. Thank You","I was able to replicate with the first gist,  can you please take a look? Thanks.","   Regarding the issue encountered with the 'tf nightly' version, it's important to note that this version utilizes Keras 3, which has known compatibility issues when using the `tf.lite.TFLiteConverter.from_saved_model` or `tf.lite.TFLiteConverter.from_keras_model` method, as reported in the Keras GitHub issue  CC(Set the weights in tf.layers with other variables but not as initializers.). As a workaround for this compatibility issue, the `model.export` or `keras.export.ExportArchive` methods can be used for exporting models for TFLite conversion. Here is a gist using Keras 3 to reproduce the issue with that method: gist.","Hi,  Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/80 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
rag,Dibyajyoti227,"AssertionError: Found 312 Python objects that were not bound to checkpointed values, likely due to changes in the Python program. Showing 10 of 312 unmatched objects","Tensorflow = 2.13.1 OS: Ubuntu 20.04 Python: 3.8 Code used to convert to tflite framework before converting to tflite file: `python3 object_detection/export_tflite_graph_tf2.py pipeline_config_path /home/export/best_model_d0_198_0.807/ssd_efficientdet_d0_512x512_coco17_mod.config trained_checkpoint_dir /home/checkpoints/ output_directory /home/export/best_model_d0_198_0.807/ssd_tfexport/` The content of the .config file is   Error I got was, ",2024-02-19T16:45:41Z,stat:awaiting response stale comp:lite TFLiteConverter TF 2.13,closed,0,3,https://github.com/tensorflow/tensorflow/issues/62992,"Hi , I am unsure how to use/ingest your config file, do you have a program/tool/documentation on how to use your config file? If so please share so that we may help you. Thanks for your help.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,Dibyajyoti227,ValueError: Only fixed_shape_resizeris supported with tflite. Found keep_aspect_ratio_resi,"I am using the latest TensorFlow Model Garden release and TensorFlow 2. I checked to make sure that this issue has not already been filed. 1. The entire URL of the file you are using https://github.com/tensorflow/models/tree/master/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu8.config 2. Describe the bug While trying to create tflite graph using `python3 object_detection/export_tflite_graph_tf2.py pipeline_config_path ssd_efficientdet_d0_512x512_coco17_tpu8.config trained_checkpoint_dir /checkpoints output_directory /ssd_tfexport` I am getting error, ` File "".local/lib/python3.8/sitepackages/object_detection/export_tflite_graph_lib_tf2.py"", line 100, in _process_config raise ValueError( ValueError: Only fixed_shape_resizeris supported with tflite. Found keep_aspect_ratio_resizer` I checked the .config file and looks like the keep_aspect_ratio_resizer is mentioned there. How do I get rid of this error and successfully create tflite graph for tflite conversion? 6. System information     OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04     TensorFlow installed from (source or binary):pip install     TensorFlow version (use command below):2.13.1     Python",2024-02-19T15:07:26Z,stat:awaiting response type:support stale TFLiteConverter TF 2.13,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62991, Could you try to replace `keep_aspect_ratio_resizer` with `fixed_shape_resizer` and use latest TF version. Kindly let us know if it helps? Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,tymorrow,Understanding `Could not find TensorRT`," Issue type Others  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.12, 2.15, nightly  Custom code No  OS platform and distribution Linux Ubuntu 22.04, MacOS 12, Windows 2022  Mobile device _No response_  Python version 3.8  3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? A warning ""Could not find TensorRT"" is produced when importing TensorFlow on Ubuntu 22.04. However, this warning does **not** appear on Windows 2022 and MacOS 12.  I would expect the behavior to be consistent across platforms unless their is an Ubuntuspecific reason. This issue is intended to help me understand: 1. Why is there this inconsistency? 2. Can it either be made consistent or can the warning be updated to explain the platformspecific benefits behind why TensorRT should be installed? Additional remarks:  * I am not trying to utilize TensorRT, so installing an additional package to satisfy this message is not a desired fix, but it may be for others. * I am also not trying to suppress logging warnings, just trying to understand the purpose of it only appearing on Ubuntu. ",2024-02-16T01:50:04Z,type:others comp:gpu:tensorrt TF 2.15,open,1,3,https://github.com/tensorflow/tensorflow/issues/62976," TensorRT installation might require additional steps or dependencies, particularly on specific platforms like Ubuntu. TensorRT is an NVIDIAspecific library optimized for NVIDIA GPUs. It might not be available on nonNVIDIA hardware or specific Linux distributions like Ubuntu due to package management differences. Thank you!","Hi , could you elaborate on why the warning does not appear on Windows or MacOS? I'm also wondering why the warning appears at all; is it specifically to tell me what you just said? If so, could it instead say what you just said to be more informative? Thank you for your time.",Any update? Been getting the same issue for about a month now. This what i am getting:  I have manually installed TensorRT and still have the same error Here is my Dockerfile for reference: 
yi,Mathanraj-Sharma,Tensorflow build fails with ModuleNotFoundError: No module named 'six.moves'," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.4.4  Custom code No  OS platform and distribution Linux x86_64 CentOS7  Mobile device _No response_  Python version 3.12  Bazel version 3.1.0  GCC/compiler version 7.3.1  CUDA/cuDNN version   GPU model and memory   Current behavior? I am trying to custombuild tensorflow `2.4.4` for Python 3.12 for a specific use case. I can build it up to Python 3.11 without any issues, but it fails with `ModuleNotFoundError: No module named 'six.moves' ` for 3.12. I ensured `six` is installed in my python environment, appreciate any help with this   Standalone code to reproduce the issue   Relevant log output ",2024-02-15T07:55:35Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.4,closed,0,6,https://github.com/tensorflow/tensorflow/issues/62970,Sharma You are using an older version of TF which is not actively supported. Could you please check this doc for reference? Thank you!," I need that specific version of tf needed to be built, I would appreciate any input/suggestion to overcome this error ",Sharma You may use the latest TF version 2.15 and let us know? !68Mw8bzkKTZ4owJ Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,andvsilva,Setting nvidia-driver to work with tensorflow 2.15 [GPU] ," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15  Custom code Yes  OS platform and distribution ""Ubuntu 23.04  Mobile device ""Ubuntu 23.04  Python version Python 3.11.4  Bazel version bazel 6.1.0  GCC/compiler version gcc (Ubuntu 12.3.01ubuntu1~23.04) 12.3.0  CUDA/cuDNN version CUDA Version: 12.3  GPU model and memory NVIDIA GeForce GTX 750 Ti  Current behavior? I trying to use GPU with tensorflow in jupyter notebook. prereq: https://www.tensorflow.org/install/sourcegpu  from terminal:    Standalone code to reproduce the issue   Relevant log output ",2024-02-14T21:23:20Z,stat:awaiting response type:bug stale comp:gpu TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62962," These errors suggest conflicts between different implementations of cuDNN, cuFFT, and cuBLAS libraries.  Please verify that CUDA environment variables like CUDA_HOME and CUDA_VISIBLE_DEVICES are set correctly. Also please do ensure you have the latest NVIDIA drivers installed for your GPU.  I tried to replicate this on colab, please find the gist here. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,schm0,Convert tf.variable into tf. constant ," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version v1.15.81g3800a8e1cd 1.15.8  Custom code Yes  OS platform and distribution Windows 10  Mobile device _No response_  Python version 3.7.17  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hello! I'm trying to create ""snapshot"" (constant) of an iterations counter (tf.variable) like this:  `new_var = tf.constant(someTFvariable)` But it results: `TypeError: Failed to convert object of type  to Tensor. Contents: . Consider casting elements to a supported type. ` The variable got declared like this:  `new_var = tf.constant(tf.cast(someTFvariable, dtype=tf.int64))` Results in: `Error: List of Tensors when single Tensor expected` I guess there is an easy solution to this but I can't figure out. Thanks.  Standalone code to reproduce the issue   Relevant log output _No response_",2024-02-14T08:55:10Z,stat:awaiting response type:support stale comp:ops TF 1.15 subtype:direct-ml-plugin,closed,0,10,https://github.com/tensorflow/tensorflow/issues/62958, Directly converting a tf.Variable to a tf.constant in TensorFlow is not possible as tf.Variable is designed to be mutable and track its value through iterations. There are a few ways to achieve this such as below; 1. Access the variable's current value  2. Create a constant tensor from the value  Thank you!,Thanks for your answer but this results in: `TypeError: List of Tensors when single Tensor expected.` Does it matter that the variable gets assigned with state_ops.assign_add? I can't find much info about state_ops.," Could you please let us know the exact TF version you are using?  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thank you!",">  Could you please let us know the exact TF version you are using? In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thank you! I use TensorFlowDirectML 1.15.8. It's optimizer class. The variable gets declared as follows:  and in the actual ""main"" optimizer function:  So this is a global iteration counter. I need to calculate an offset from this value which I then can subtract from it. But because the iterations are counted upwards by 1 the offset changes too. So I need to create a static/constant value.", TF v1.15 is an older version which is not actively supported. We request you to kindly upgrade to the latest TF version. Thank you!,Well... Isn't this the lastest version that's supports directml? So it is not possible to create a constant value from this state_op variable?," it is not possible to directly create a constant value from a state_op variable like self.iterations in TensorFlowDirectML 1.15.8. This is because state_ops are designed to modify existing state (tf.Variable) rather than create new constants. You could potentially manipulate the selfiterations tensor using TensorFlow operations, such as slicing or masking, to extract a constant portion as an offset. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,Moddingear,TFLite CMake Task Library build," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version master  Custom code No  OS platform and distribution Debian 12  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detectorrun_inference_in_c No includes are given. Class does not exist in Tensorflow Lite. Same for other classes in the task library. I'm just trying to use the Coral TPU to accelerate DNN execution wiht OpenCV, the fact that it's this hard to even get working in infuriating.  Standalone code to reproduce the issue   Relevant log output _No response_",2024-02-13T13:16:16Z,stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux,closed,0,6,https://github.com/tensorflow/tensorflow/issues/62954, Please ensure you are using a TensorFlow Lite version compatible with your Coral TPU model. Your OpenCV version should support Coral TPU acceleration as well. Thank you!,"Hello, thanks for the response. In OpenCV (4.9.0), I didn't see any backends that would correspond with the Coral TPU. Calling dnn::getAvailableBackends() only lists DNN_BACKEND_OPENCV with DNN_TARGET_CPU (I have libusbdev installed) As for The TFLite version, I don't know what I should be downloading...","Hi , can you let me know what setup/installation you have performed as well as the commands which are running into issues? i.e. Have you tried compiling? Are you only using C++? Which task library are you trying to use? If you can share with us your steps & files, we can probably help you better. Thanks for your help. Does this minimal example work for you? https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal Please also review this https://coral.ai/docs/edgetpu/tflitecpp/runaninferencewiththelibcoralapi and see if that works for you or not.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,jyochichili,Checksum Error during TensorFlow Lite Build for kissfft v130," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.6.2  Custom code No  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? While building the application with Tensorflow v2.6.2 downloaded from source https://github.com/tensorflow/tensorflow/archive/v2.6.2.zip and with Tensorflow PYPI package(2.6.2) installed, I am getting checksum error while downloading kissfft library. This issue is seen from past 8 days.  Standalone code to reproduce the issue   Relevant log output ",2024-02-12T09:31:29Z,stat:awaiting response type:build/install comp:lite subtype: ubuntu/linux 2.6.0,closed,0,8,https://github.com/tensorflow/tensorflow/issues/62944, Please doublecheck that the downloaded KissFFTv130RELEASEX86_64.zip file is complete and hasn't been corrupted during download. Some filehosting services might introduce errors. Kindly ensure your internet connection is stable and that no interruptions occurred during the download. We recommend you to kindly upgrade to the latest TF version as you are using an older version which is not actively supported. Thank you!,"Hi ,  Thank you for the response. you are right that the kissfft lib is not properly downloaded or corrupted, but I doubt if network issue is the culprit here, as the other libs (tf_lite_micro_person_data_grayscale_2020_05_27, ruy, gemmlowp) has no issue in downloading. I have also noticed that the in the case of failure, the incorrect checksum is always same, which I assume won't be the case when the file is corrupted. Also switching to the latest version won't help in my case as my models are only compatible to v2.6.2. Can you please help through this issue? Thanks. Please note the disk size of downloaded Kissftt lib(tensorflow2.6.2/tensorflow/lite/micro/tools/make/downloads/kissfft) in the failure scenario is only 4K (original size 248K), so I can confirm that the file is not properly downloaded."," Sometimes, downloads can fail due to temporary network hiccups. Please retry downloading the kissfft library a few times. If possible, kindly change the download directory to a location with sufficient space.  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"I have found that within tflite(v2.6.2), the URL for downloading the KISSFFT library doesn’t involve secure download(https), which seems to be causing an incomplete download of KISSFFT library. The issue doesn’t persist in the later versions, as the URL has been updated to use a secure https connection. Thanks.", Thank you for the response! Could you please move this issue to closed status if it has been resolved? Thank you!,This issue got fixed by upgrading to the later versions of tflite. Closing this issue.,Are you satisfied with the resolution of your issue? Yes No
yi,CaptainDario,[Bug] compiling the tf lite benchmark tool fails on macos (CMake), Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15  Custom code No  OS platform and distribution MacOS 13.5.2  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version Apple clang version 15.0.0 (clang1500.1.0.2.5) Target: arm64appledarwin22.6.0 Thread model: posix InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to compile the tf lite benchmark tool on MacOS. For this I ran the following commands on tf 2.15 and master  This crashes with this error  I would expect the benchmark to be compiled successfully.  Standalone code to reproduce the issue   Relevant log output ,2024-02-11T21:24:15Z,stat:awaiting tensorflower type:build/install comp:lite subtype:macOS TF 2.15,closed,0,7,https://github.com/tensorflow/tensorflow/issues/62941,Building just the tf lite binary works , also works,"Hi , I was able to replicate with your exact steps and cmake. I also tried with bazel and that seemed to work fine:  Hi , can you please take a look for the cmake flow? Thanks."," yes, the bazel build works.","Hi,  Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/81 Let us know if you have any questions. Thanks.", thank you!,Are you satisfied with the resolution of your issue? Yes No
yi,aaronsuydam,Question: Why no GPU support on Windows Native?," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.15  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory GTX 1660Ti  Current behavior? Hey there! This is just me trying to ask a question, I did some googling to try and figure it out, but I was just wondering why support for CUDA acceleration was dropped for TF > 2.10 on windows native installations? I'd be curious just learning what would be needed to manually work around that (I'm a CpE major, would be a fun activity i think)? I may be way out of my league, but I am curious!  Standalone code to reproduce the issue   Relevant log output ",2024-02-11T00:03:56Z,stat:awaiting response type:build/install subtype:windows TF 2.15,closed,0,8,https://github.com/tensorflow/tensorflow/issues/62938," The introduction of Windows Subsystem for Linux 2 (WSL 2) provided a more efficient and consistent platform for GPU acceleration on Windows, leveraging established Linux builds. This delivers full access to GPUenabled builds and superior performance. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.," Hey! Sorry for the late response here. I am trying to develop a Windows app that leverages some functionality that has been implemented using tensor flow. Will the lack of GPU support impact that? As in, can a windows app that has some backend functionality that uses TF gpu stuff work on a system without wsl installed? I don't want users to have to install wsl to get that benefit, maybe i can find a workaround if that's the case? This is also just me being curious, I'm a college student, so this is really just a hobby for me. No stress either way."," If you don't want to use WSL2 then there are a few alternatives. If your app's functionality can be achieved with TensorFlow 2.10 or older, you can utilize the native GPU support offered by those versions. However, this might limit access to newer features and optimizations.  Microsoft released the DirectML Plugin for TensorFlow, enabling limited GPU acceleration via Microsoft's DirectML hardware acceleration library.  Thank you!"," thanks for the quick reply. I looked into the DirectML plugin, and it looks like they paused development in favor of stuff with the ONNX Runtime. From their repo: ""⚠️ Development of TensorFlowDirectMLPlugin has been paused until further notice. To take advantage of the latest DirectML features and performance improvements for inference scenarios, we recommend taking a look at ONNX Runtime. ⚠️"" Are there plans to support feature integration with this new direction they are taking?"," You are right! Microsoft paused development of the TensorFlowDirectMLPlugin in October 2023 in favor of focusing on ONNX Runtime for inference scenarios with DirectML. Currently, there's no concrete information about potential feature integration between these two directions. Please stay informed about future developments.  Thank you!",Will do! Thank you for the information!,Are you satisfied with the resolution of your issue? Yes No
rag,basnetr,Error with Custom Keras Model, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version v2.13.017gf841394b1b7 2.13.1  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04.6 LTS  Mobile device _No response_  Python version 3.9.18  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Expected output (ran using v2.8.390g1b8f5c396f0 2.8.4):   Standalone code to reproduce the issue   Relevant log output ,2024-02-09T05:34:22Z,type:bug comp:keras TF 2.13,open,1,5,https://github.com/tensorflow/tensorflow/issues/62927, Could you try to modify your CustomModel class to incorporate the inputs and output attributes into the configuration returned by get_config(). It's recommended to use model.save('my_model.keras') for the native Keras format. Thank you!," Incorporating inputs and outputs doesn't work (these are tensors that give errors: `TypeError: Cannot serialize object KerasTensor`), and I don't think it's the main issue, attributes such as `layers`, `input_layers` and `output_layers` are still missing in the inherited class that are present in the parent class. Also no difference using `.keras` vs. `.h5` in this case. I think the issue might be related to keras and tensorflow incompatibility  at least for tensorflow 2.12, 2.13 and 2.15.  A quick fix for the issue was to force install keras 2.11 (downgrading from 2.13.1) for tensorflow 2.13.1. This creates dependency conflicts but works in my case.  Another fix for just the custom model was to inherit CustomModel from `tensorflow.python.keras.Model` instead of `tensorflow.keras.Model` but this will further cause other issues moving forward, example: `tensorflow.keras.optimizers` are not supported for training `tensorflow.python.keras.Model`."," I was able to replicate the issue reported here.  This issue might be occurring due to Keras and TensorFlow incompatibility in versions between 2.12 and 2.15. While downgrading Keras can be a temporary fix, it's generally not recommended due to potential dependency conflicts and lack of bug fixes/security updates. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you., Awaiting a permanent fix.
rag,AshebirGetuBelete,Machine learning from Edx," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info/logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Lar",2024-02-08T20:15:03Z,stat:awaiting response stale TFLiteConverter,closed,0,3,https://github.com/tensorflow/tensorflow/issues/62926,  Could you please provide more context on the issue reported here?  Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,michaelpoluektov,TFL StridedSlice not lowered to TFL Slice if masks are set and strides are 1,"The following code:  Will produce a TFLite model with the following MLIR:  Despite begin and end being constants, and the strides being all set to 1, `tfl.strided_slice` is not lowered to `tfl.slice`, because the TensorFlow code generates a strided slice with masks. A pass was added in 6db21275a7579e0772f134c029b737a8c6407e01 to lower strided slices with masks set to zero, but this case is not covered. I would like to add a pass that lowers such cases:   if begin_mask is set, and begin is constant, set begin_mask to zero and set the corresponding element in begin to zero  same for end_mask, except a check has to be added to make sure the input shape is static It seems like `ellipsis_mask` is being lowered already, and `new_axis_mask` inserts a `tfl.reshape` after the strided slice (in the TF to TFLite conversion), however `shrink_axis_mask` is not lowered: I would like to lower it to include a reshape as well. After all of those transformations, the existing lowering pass should be able to convert any `strided_slice` with strides set to 1 to a regular `tfl.slice`.",2024-02-07T14:41:54Z,stat:awaiting response stale comp:lite TFLiteConverter,closed,0,5,https://github.com/tensorflow/tensorflow/issues/62916," Could you please avoid using masks with StridedSlice when strides are all 1. Instead, explicitly use the Slice operator for better performance.  Thank you!"," Thanks for your reply! I'm working on a TFLite backend, and unfortunately I don't get to pick which operators our clients use. I hacked together a pass that does this, but I don't see a reason some version of this shouldn't be included upstream. I don't think this is best practice though: I believe the MLIR way of doing this is to define a canonicalization rule for strided slice? I am willing to submit a PR but I'm not particularly experienced with the TF codebase: where do you have canonicalizations defined? Are you running an MLIR `CanonicalizerPass` anywhere? ","Hi , https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir/lite this readme is a good high level overview of the passes TF > TFL goes through, I would say at this point it is outdated but it's a good start to understand the original intention. Also as noted in the README: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/tf_tfl_passes., there are multiple Canonicalization passes, but it's best to review the code to determine where your logic may fit. If you are able to create a PR we will greatly appreciate the help. Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,JoshPPrieto,TFLite selective builds using TF ops (flex delegate) for embedded linux on aarch64," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.10 ... 2.15  Custom code No  OS platform and distribution Linux Ubuntu 22.04  Mobile device Yocto based Linux running kernel 6.1.x  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi, From what it is described in Select TensorFlow operators and Reduce TensorFlow Lite binary size, it is possible to generate reduced size binaries (minimal TFLite runtime + specific Flex ops) for Android, and it is also described how to build custom C/C++ shared libraries containing the Flex ops that are part of the given models during the build process. When building the shared libs with models containing flex ops and `elinux_aarch64` as config, the expected artifacts are built with the full size, including all the TF ops rather than selecting the ones from the model. The behavior should be reproduced when trying to build benchmarkmodel with flex (or even the standalone shared lib): tmp/BUILD (change `init_tensorflow` visibility if needed, `custommodel.tflite` should be ",2024-02-07T09:53:13Z,stat:awaiting tensorflower type:feature comp:lite,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62914,"Hi , I wouldn't call it expected, we always appreciate it when the users share any research they have already done as it usually does help us. Feel free to make a PR that helps resolve the issue... if it affects other builds we'll have to analyze whether the effect is a regression or not but it'll be easier for us to examine when the PR is created. , can you please take a look? Thanks.","Hi again, sorry for the delay. As the changes contain some issues with latest master (remaining double registrations), rather than raising a PR (I don't think it complies with contribution guidelines), I'm sharing the commit (https://github.com/nxpimx/tensorflow/commit/bd29abc2798e1bb28ac571bbb403d980eac52df0) to continue the discussion. Please,  , let me know if the PR is still preferred or something else is needed for reproduction/analysis.","Hi,   Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/82 Let us know if you have any questions. Thanks.","> Hi,  >  > Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: googleaiedge/LiteRT CC(bazel compile error) >  > Let us know if you have any questions. Thanks. Any alternative solution to reduce the binary size?"
yi,johntharian28,How to create libtensorflow.a file for static linking during go build ?," System information  OS Platform and Distribution: Linux Ubuntu 22.04  TensorFlow installation: Built from source  TensorFlow library: tf 2.15  Python version 3.10.12  Issue I am trying to use tfgo for some ml applications. I had created a go executbale using go build, but the executable requires tensorflow c api to be installed. I am trying to package libtensorflow  so that the executable does not require any external packages. I saw many github issues on creating libtensrflow.a file for tf lite but not for this. Is there some way to do this ? Thanks",2024-02-07T00:47:45Z,stat:awaiting response type:build/install type:support stale TF 2.15,closed,0,6,https://github.com/tensorflow/tensorflow/issues/62910, Could you please try creating a libtensorflow.a file for static linking with Go's `go build` command is not officially supported by TensorFlow and is generally considered quite challenging. Using dynamic linking is the recommended and officially supported approach by TensorFlow. You can install the shared libraries (`libtensorflow.so or .dll`) and link them dynamically with your Go code using `ldflags `options with `go build`. Thank you!,"Thanks  . So to install the `libtensorflow.so`  file , do I just do  `bazel test config opt //tensorflow/tools/lib_package:libtensorflow_test bazel build config opt //tensorflow/tools/lib_package:libtensorflow`"," Please proceed with caution, as it's not officially supported and might not work reliably. The commands you provided (bazel test and bazel build) wouldn't directly generate a usable libtensorflow.a for Go.  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,ellie-jan,Manylinux wheel issue using clang," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.16  Custom code No  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version 6.1.0  GCC/compiler version Clang 17  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Case 1  We can successfully build tensorflow with clang17; however, we faced the error below when trying to repair the wheel using auditwheel package to make it manylinux2014 compatible.  Case 2  We have tried to reference this previous issue CC([MKL] Manylinux wheel issue on clang) and add this flag crosstool_top=""r2.16clang_config_cuda//crosstool:toolchain"" as a build option. However, this addition causes an error during the tensorflow build.   Standalone code to reproduce the issue   Relevant log output ",2024-02-06T21:51:35Z,stat:awaiting tensorflower type:build/install subtype: ubuntu/linux,closed,0,8,https://github.com/tensorflow/tensorflow/issues/62909,jan Please refer to the official TensorFlow Manylinux wheel building guide (https://github.com/pypa/manylinux: https://github.com/pypa/manylinux) Provide the complete auditwheel error message. Could you consider a clean virtual environment and let us know ? Thank you!,We are building tensorflow and running auditwheel repair within the docker container tensorflow/build:2.16python3.11. I have attached the full auditwheel repair verbose output. Thank you! auditwheel_repair_verbose.txt," Hi, I was wondering if there are any updates regarding this issue. Thank you!",A temporary fix for case 2 is to add `copt=Wnoerror=unusedcommandlineargument` to the bazel command. It will suppress the command not used error. ,"I have tried to add copt=Wnoerror=unusedcommandlineargument as well as crosstool_top=""r2.16clang_config_cuda//crosstool:toolchain"" to the bazel command to build with clang17, python3.11 and 2.17 version of GLIBC. However, after tensorflow is built successfully, I ran ldd version && getconf GNU_LIBC_VERSION and saw that the version of glibc used is still 2.31.  I have attached the output of the bazel command below.  build copt=O3 copt=Wnognuoffsetofextensions crosstool_top=""r2.16clang_config_cuda//crosstool:toolchain"" copt=Wnoerror=unusedcommandlineargument copt=Wformat copt=Wformatsecurity copt=fstackprotector copt=fPIC copt=fpic linkopt=Wl,z,noexecstack linkopt=Wl,z,relro linkopt=Wl,z,now linkopt=fstackprotector verbose_failures copt=march=sandybridge  Are there additional steps needed in order to downgrade glibc to be compatible with manylinux2014?  build_tf_output.txt",I'm not able to reproduce the error that you are seeing. Can you check if the following steps work for you? , I was able to build the manylinux wheel after following the steps you gave above. I will close this issue. Thank you!,Are you satisfied with the resolution of your issue? Yes No
yi,Varfalamei,Padding type for the pooling layer 'PoolingLayerBuilder (MEAN)_57' is not set," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.13.0  Custom code Yes  OS platform and distribution 20.04  Mobile device Iphone 15  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I’m trying to convert the tf efficientnetv2s model to tflite format so that I can then use it on iOS with the CoreML delegate, but I encountered the problem: `Error compiling model compiler error: Error reading protobuf spec. validator error: Padding type for the pooling layer 'PoolingLayerBuilder (MEAN)_57' is not set.` Is there a normal way to solve this problem? So far, only translating directly into CoreML has helped, but I still want to understand the problem and how to solve it. It is also worth clarifying that the model runs on Android with different delegates and on iOS with the Metal delegate  Standalone code to reproduce the issue   Relevant log output ",2024-02-06T12:43:51Z,stat:awaiting response type:bug stale comp:lite TF 2.13,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62902," The error message ""Error reading protobuf spec. validator error: Padding type for the pooling layer 'PoolingLayerBuilder (MEAN)_57' is not set."" indicates that you haven't specified the padding behavior for the pooling layer with index 57 in your TensorFlow model. Please refer to the compatibility guide for supported operators and layers: https://www.tensorflow.org/lite/guide/ops_compatibility. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,yide1235,TFlite 2.4.2 build flex delegate error,"Hi, deso anyone know when i try to rebuild the tflite 2.4.2 with the flex delegate open for support the hitnet model, i have met this error: this is my code:  include  include  include  include ""tensorflow/lite/interpreter.h"" include ""tensorflow/lite/kernels/register.h"" include ""tensorflow/lite/string_util.h"" include ""tensorflow/lite/examples/label_image/get_top_n.h"" include ""tensorflow/lite/model.h"" // // //add the delegate include ""tensorflow/lite/delegates/flex/delegate.h"" // // include ""tensorflow/lite/delegates/flex/converter.h"" define H 256 define W 256 define THREADS 4 int main() {     const char* MODEL = ""eth3d"";     const int CHANNEL = 2;     // const char* model_path = ""eth3d/saved_model_720x1280/model_float32.tflite"";  // Replace with the actual path to your TFLite model     std::unique_ptr model =tflite::FlatBufferModel::BuildFromFile(""./eth3d/saved_model_720x1280/model_float32.tflite"");     // std::unique_ptr model =tflite::FlatBufferModel::BuildFromFile(""./yolov8s_integer_quant.tflite"");//this one works, means the issue is the flex delegate     // std::unique_ptr flex_delegate = tflite::FlexDelegate::Create();     tflite::ops::builtin::BuiltinOpResolver resolver;     std::unique_ptr interp",2024-02-06T09:16:15Z,stat:awaiting response type:support comp:lite TF 2.4,closed,0,2,https://github.com/tensorflow/tensorflow/issues/62900,", TensorFlow 2.4 is pretty older version. Could you please update TensorFlow to the latest stable version v2.15 and check if you are facing the same issue. Also I suspect you have missed to submit the CMakeLists Info:  and try to add a declare `class tensorflow::error::Code` Thank you!",Are you satisfied with the resolution of your issue? Yes No
yi,RRiva,TensorFlow 2.15 cannot be imported after Poetry install on Windows," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.15.0  Custom code No  OS platform and distribution Windows  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I expect to be able to install and import TensorFlow on the `python:3.11windowsservercore` Docker container, using Poetry. I made a new project with the following pyproject.toml, copied from Poetry issue 8271  Then, I generate the lock file and on the Docker container `python:3.11windowsservercore`, and install my package with  Finally, `pytest` fails as soon as it tries to import TensorFlow. The reason for this behavior is described at Poetry issue 8271.  Standalone code to reproduce the issue   Relevant log output _No response_",2024-02-06T07:52:58Z,stat:awaiting tensorflower type:build/install subtype:windows TF 2.15,open,2,18,https://github.com/tensorflow/tensorflow/issues/62899,cc:  ,"Hi , the issue has been addressed in https://github.com/tensorflow/tensorflow/issues/58674issuecomment1593891706 I just successfully imported TF 2.15 on the Windows using the Poetry tool. Before running the command poetry add tensorflow==2.15.0 please run the command poetry add tensorflowiogcsfilesystem==0.31.0 TF 2.15 supports Python 3.93.11, please add it in the pyproject.toml !image !image","Hi , thanks for trying, but it doesn't work yet. To demonstrate it, I made a new clean repo, with the pyproject.toml that you have written. It's available at https://gitlab.windenergy.dtu.dk/surrogatemodels/testpoetryandtensorflow As you can see, the last pipeline still fails with `ModuleNotFoundError: No module named 'tensorflow'`. https://gitlab.windenergy.dtu.dk/surrogatemodels/testpoetryandtensorflow//jobs/238494","Hi , please run the command below to fix your issue.  poetry add tensorflowintel  !image","Hi , we are working to fix the issues of separate poetry installation of dependent packages","Hi , thanks a lot! It is now possible to import TensorFlow on both Windows and Linux 🙂 The only difference is that I had to specify that `tensorflowintel` is only available for Windows, since Poetry cannot infer it from the metadata. The pyproject.toml now looks like  It should be noted though that `tensorflowiogcsfilesystem` 0.31.0  is not compatible with `tensorflow` 2.15.","Thank you , yes Intel releases tensorflowintel for the Windows platform. In pip installation, TensorFlowintel is automatically installed while installing TensorFlow on the Windows platform. We are working to fix it for Poetry Installation as well which should be available in the TF 2.16 release","> Thank you , yes Intel releases tensorflowintel for the Windows platform. In pip installation, TensorFlowintel is automatically installed while installing TensorFlow on the Windows platform. We are working to fix it for Poetry Installation as well which should be available in the TF 2.16 release Will this also fix all the other metadata issues? Can't install tensorflowiogcsfilesystem on alpine linux (I know not ideal but it's a specific environment requirement) Sorry if this is unrelated, wondering if the metadata will fix all","Hi , thank you for letting us know about the issue with Alpine Linux. Request you to raise a separate Github issue to TensorFlow. We will work to fix it.","Hi, I just checked if the installation improved with TF 2.16, but unfortunately nothing has changed for Windows:  `tensorflowintel` still needs to be specified as a requirement, since it's not found automatically.  `tensorflowiogcsfilesystem` does not provide recent Windows wheels, and therefore does not support python 3.12. I hope that the fix that you mentioned will be available with the next release 🙂 ","Hi , yes the fix for automatic installation of tensorflowintel using poetry, will be available with the next release. I will contact tensorflow/io team to check the latest release of tensorflowiogcsfilesystem on the Windows platform","Hey  do you have any idea on the status of tensorflowiogcsfilesystem on windows? Can't seem to locate an answer in any of the issues, or at least find the correct one.",Hi  I have raised an issue https://github.com/tensorflow/io/issues/1966. I haven't heard anything yet. If you are getting any errors please let me know.,I've fixed it using this comment: manually specifying the versions in the pyproject.toml.,"Hi , I saw that you have release 2.16.2 with a very promising headline 🙂 Unfortunately, I tested 2.17.0, and nothing has changed with respect to my comment above. , I have developed a leaner pyproject here.","Hi , I’m sorry to hear that the issue persists in version 2.17.0. Unfortunately, the changes are beyond my control","Hi , I understand, and thank you for your effort 🙂 Maybe an idea is to introduce Poetry into the test suite. You would only have to copy my project here, and strive to simplify the pyproject.","Hi, running into the same issue, is it's possible to make ""tensorflowiogcsfilesystem"" an optional dependency? in most use cases where you install it on windows, you're probably won't going to need those extra file systems, and if you do, you can always use WSL like the tensorflowio suggests  this will still allow the usage of tensorflowcpu on windows without pinning a version of an external dependency while allow those who needs this dependency to continue to get it."
llm,giuliocn,Solved issue #62645 CI build fails on install pip packages ,"Here is a technical summary CC(CI build gives a command not found error on /install/install_pip_packages.sh) : 1. Use a script to build and install python from source  2. Edit `Dockerfile.cpu` to execute `/install/build_and_install_python.sh 3.9.18`  3. Edit `install_deb_packages.sh` to update and install packages      clang       gcc      libffidev liblapackdev libblasdev      make      python3numpy 4. Edit `install_pip_packages.sh` to install pip      21, use python3.9       25, upgrade pip with python3.9 5. Edit `install_pip_packages.sh` to upgrade pip packages       setuptools      future      numpy scipy joblib threadpoolctl scikitlearn pandas IMPORTANT NOTES: Running all tests with `bazel test` fails on **win_1803/py38** because of its `interpreter_path`  Running 'bazel test //tensorflow/...' inside tf_ci.cpu... Reading package lists... Building dependency tree... Reading state information... sudo is already the newest version (1.8.160ubuntu1.10). 0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded. Adding group `codespace' (GID 1000) ... Done. /workspace /workspace You have bazel 6.1.0 installed. Found possible Python library paths:  /usr/lib/python2.7/distpackages  /usr/local/lib/pytho",2024-02-05T18:19:19Z,size:M,closed,0,5,https://github.com/tensorflow/tensorflow/issues/62898,"  No tests are found inside `tensorflow/tools/ci_build` but BUILD was successful.  tensorflow/tools/ci_build:* WORKSPACE: /workspaces/tensorflow CI_DOCKER_BUILD_EXTRA_PARAMS:  CI_DOCKER_EXTRA_PARAMS:  COMMAND: bazel test //tensorflow/tools/ci_build:* CI_COMMAND_PREFIX: ./tensorflow/tools/ci_build/builds/with_the_same_user ./tensorflow/tools/ci_build/builds/configured cpu CONTAINER_TYPE: cpu BUILD_TAG: tf_ci  (docker container name will be tf_ci.cpu) Building container (tf_ci.cpu)... [+] Building 1.0s (18/18) FINISHED                                       docker:default  => [internal] load .dockerignore                                             0.1s  => => transferring context: 2B                                              0.0s  => [internal] load build definition from Dockerfile.cpu                                 0.1s  => => transferring dockerfile: 702B                                           0.0s  => [internal] load metadata for docker.io/library/ubuntu:16.04                              0.5s  => [ 1/13] FROM docker.io/library/ubuntu:16.04:1f1a2d56de1d604801a9671f301190704c25d604a416f59e03c04f5c6ffee0d6  0.0s  => [internal] load build context                                             0.1s  => => transferring context: 1.73kB                                            0.0s  => CACHED [ 2/13] COPY install/*.sh /install/                                      0.0s  => CACHED [ 3/13] RUN /install/install_bootstrap_deb_packages.sh                             0.0s  => CACHED [ 4/13] RUN addaptrepository y ppa:openjdkr/ppa &&   addaptrepository y ppa:georgeedison55/cmake3. 0.0s  => CACHED [ 5/13] RUN /install/install_deb_packages.sh                                  0.0s  => CACHED [ 6/13] RUN /install/build_and_install_python.sh 3.9.18                            0.0s  => CACHED [ 7/13] RUN /install/install_pip_packages.sh                                  0.0s  => CACHED [ 8/13] RUN /install/install_bazel.sh                                     0.0s  => CACHED [ 9/13] RUN /install/install_proto3.sh                                     0.0s  => CACHED [10/13] RUN /install/install_buildifier.sh                                   0.0s  => CACHED [11/13] RUN /install/install_auditwheel.sh                                   0.0s  => CACHED [12/13] RUN /install/install_golang.sh                                     0.0s  => CACHED [13/13] COPY install/.bazelrc /etc/bazel.bazelrc                                0.0s  => exporting to image                                                  0.0s  => => exporting layers                                                  0.0s  => => writing image sha256:7c5a8e70f3dbc138e6dc4b80287af162145e156a8606179861da4692a1063196               0.0s  => => naming to docker.io/library/tf_ci.cpu                                       0.0s Running 'bazel test //tensorflow/tools/ci_build:*' inside tf_ci.cpu... Reading package lists... Building dependency tree... Reading state information... sudo is already the newest version (1.8.160ubuntu1.10). 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. Adding group `codespace' (GID 1000) ... Done. /workspace /workspace You have bazel 6.5.0 installed. Found possible Python library paths:  /usr/lib/python2.7/distpackages  /usr/local/lib/python2.7/distpackages Please input the desired Python library path to use. Default is [/usr/lib/python2.7/distpackages] Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you want to use Clang to build TensorFlow? [Y/n]: Clang will be used to compile TensorFlow. Please specify the path to clang executable. [Default is /usr/bin/clang]:  You have Clang 3.8.02ubuntu4 installed. Please specify optimization flags to use during compilation when bazel option ""config=opt"" is specified [Default is Wnosigncompare]:  Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""config="" to your build command. See .bazelrc for more details.     config=mkl       Build with MKL support.     config=mkl_aarch64   Build with oneDNN and Compute Library for the Arm Architecture (ACL).     config=monolithic    Config for mostly static monolithic build.     config=numa       Build with NUMA support.     config=dynamic_kernels     (Experimental) Build kernels into separate shared objects.     config=v1        Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:     config=nogcp      Disable GCP support.     config=nonccl      Disable NVIDIA NCCL support. /workspace TF_BUILD_INFO = {container_type: ""cpu"", command: ""bazel test //tensorflow/tools/ci_build:*"", source_HEAD: ""258780cb34bfa9fc03cffe2371eeebdb974d8df4"", source_remote_origin: ""https://github.com/giuliocn/tensorflow"", OS: ""Linux"", kernel: ""6.2.01019azure"", architecture: ""x86_64"", processor: ""AMD EPYC 7763 64Core Processor"", processor_count: ""2"", memory_total: ""8120292 kB"", swap_total: ""0 kB"", Bazel_version: ""Build label: 6.5.0"", Java_version: ""1.8.0_292"", Python_version: ""2.7.12"", gpp_version: ""g++ (Ubuntu 5.4.06ubuntu1~16.04.12) 5.4.0 20160609"", swig_version: """", NVIDIA_driver_version: """", CUDA_device_count: ""0"", CUDA_device_names: """", CUDA_toolkit_version: """"} INFO: Options provided by the client:  Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'test' from /etc/bazel.bazelrc:  Inherited 'common' options: color=yes INFO: Reading rc options for 'test' from /workspace/.bazelrc:  Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'test' from /etc/bazel.bazelrc:  Inherited 'build' options: verbose_failures spawn_strategy=standalone strategy=Genrule=standalone INFO: Reading rc options for 'test' from /workspace/.bazelrc:  Inherited 'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Reading rc options for 'test' from /workspace/.tf_configure.bazelrc:  Inherited 'build' options: host_force_python=PY2 action_env PYTHON_BIN_PATH=/usr/bin/python action_env PYTHON_LIB_PATH=/usr/lib/python2.7/distpackages python_path=/usr/bin/python action_env CLANG_COMPILER_PATH=/usr/lib/llvm3.8/bin/clang repo_env=CC=/usr/lib/llvm3.8/bin/clang repo_env=BAZEL_COMPILER=/usr/lib/llvm3.8/bin/clang INFO: Reading rc options for 'test' from /etc/bazel.bazelrc:  'test' options: spawn_strategy=standalone verbose_failures test_output=errors test_verbose_timeout_warnings INFO: Reading rc options for 'test' from /workspace/.tf_configure.bazelrc:  'test' options: test_size_filters=small,medium INFO: Found applicable config definition build:short_logs in file /workspace/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /workspace/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition test:v2 in file /workspace/.tf_configure.bazelrc: test_tag_filters=benchmarktest,no_oss,oss_excluded,gpu,oss_serial,v1only build_tag_filters=benchmarktest,no_oss,oss_excluded,gpu,v1only INFO: Found applicable config definition build:linux in file /workspace/.bazelrc: host_copt=w copt=Wnoall copt=Wnoextra copt=Wnodeprecated copt=Wnodeprecateddeclarations copt=Wnoignoredattributes copt=Wnoarraybounds copt=Wunusedresult copt=Werror=unusedresult copt=Wswitch copt=Werror=switch copt=Wnoerror=unusedbutsetvariable define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=dynamic_kernels experimental_guard_against_concurrent_changes INFO: Found applicable config definition build:dynamic_kernels in file /workspace/.bazelrc: define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS Loading:  Loading:  Loading:  Loading: 0 packages loaded Analyzing: 35 targets (1 packages loaded, 0 targets configured) INFO: Analyzed 35 targets (1 packages loaded, 35 targets configured). INFO: Found 35 targets and 0 test targets... [0 / 1] [Prepa] BazelWorkspaceStatusAction stablestatus.txt INFO: Elapsed time: 4.206s, Critical Path: 0.09s INFO: 1 process: 1 internal. INFO: Build completed successfully, 1 total action ERROR: No test targets were found, yet testing was requested ","Hi  This PR is in draft, any update on this? Please. Thank you!","> Hi  This PR is in draft, any update on this? Please. Thank you!  Apparently, CPU build runs successfully. Then, Bazel fails raising the error reported above. Since I could not test my PR, I believe that further work is needed to merge it safely. Would you like to take care of it ? Otherwise I need to learn how Bazel works...     ",Hi  Can you please assist on above comments from . Thank you!,"> Hi  Can you please assist on above comments from . Thank you! The build failure is related to an invalid path configuration for the Python interpreter on the Windows 1803 system with Python 3.8 (denoted as `win_1803/py38`).  Here's a breakdown of the error message generated by **Gemini**: * **Error Message:**       *  Link to BUILD file referenced above on TF master branch * **Explanation:** The error points to a specific line (line 9, column 11) in a build file (`BUILD`). This line defines a rule named `py_runtime` which specifies the path to the Python interpreter used during the build process. The error message states that the provided path in the `interpreter_path` attribute is not a valid absolute path. An absolute path starts with the drive letter (e.g., `C:`) or a network location and specifies the complete directory structure to reach the desired file. **Possible Causes:** * **Incorrect Path:** The path configured in the `interpreter_path` attribute might be incorrect or relative. It should be the complete path to the Python 3.8 executable on the Windows system (e.g., `C:\Python38\python.exe`). * **Missing Configuration:** The build configuration might not be set up to automatically locate the Python interpreter. You might need to explicitly specify the path during the build process. **Resolving the Issue:** 1. **Locate Python Executable:** Find the absolute path to the Python 3.8 executable on your Windows 1803 system. It's typically installed in a directory like `C:\Python38` or `C:\Users\\AppData\Local\Programs\Python\Python38`. 2. **Update Build Configuration:** Modify the `BUILD` file (`/workspace/tensorflow/tools/toolchains/win_1803/py38/BUILD`) and update the `interpreter_path` attribute with the absolute path you located in step 1. 3. **Rerun Build:** After updating the path, rerun the build command (`bazel test //tensorflow/...`) to attempt building TensorFlow again. By providing a valid absolute path to the Python interpreter, the build process should be able to locate the correct executable and proceed successfully."
yi,Caarvaa,Failed to load the native TensorFlow runtime when loading DeepLabCut," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.10.0  Custom code No  OS platform and distribution Windows 10  Mobile device _No response_  Python version 3.8.18  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Attempting to load DeepLabCut (version 2.2.2) via anaconda with python version 3.8.18 but i get a response saying it 'Failed to load the native TensorFlow runtime'. (full output below) I've tried to uninstall and reinstall tensorflow but that doesn't resolve the issue (instead get other issues, the output code from which i can provide if its helpful) I have 0 programming experience so i apologise in advance for that Thank you for any response!  Standalone code to reproduce the issue   Relevant log output ",2024-02-05T17:04:44Z,stat:awaiting response type:build/install stale subtype:windows TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62896,"  Verify the exact TensorFlow version required for DeepLabCut 2.2.2. If necessary, install the compatible version using conda install tensorflow== or pip install tensorflow==. Kindly consider using virtual environments that help to isolate projectspecific dependencies and avoid conflicts. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,lrohlfs,CUPTI_ERROR_INVALID_DEVICE when trying to profile model performance," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15.0  Custom code No  OS platform and distribution WSL 2 Ubuntu 22_04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.2/8.9  GPU model and memory RTX 4060TI 16GB  Current behavior? When trying to profile model performance on my fresh WSL2 install of tensorflow (using the pip [andcuda] approach), I am encountering a CUPTI error.  The installation works perfectly fine for just training models on the GPU (RTX 4060Ti, 16gb), but whenever I enable the profiler, the log states the following error: ""external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:194] cuptiSubscribe: error 2: CUPTI_ERROR_INVALID_DEVICE"" I reproduced the error on the same machine with tf 2.14 as well as a custom install of cuda using the recommended versions from the install guide.  Is there something I am doing obviously wrong or is there possibly an issue with my GPU?  Standalone code to reproduce the issue   Relevant log output ",2024-02-05T14:34:46Z,stat:awaiting response type:build/install wsl2 TF 2.15,closed,0,3,https://github.com/tensorflow/tensorflow/issues/62895, Please ensure the environment variable CUDA_VISIBLE_DEVICES is not set to exclude your RTX 4060Ti. You can check its value using the following:  Please doublecheck if you have installed the appropriate NVIDIA drivers for your WSL2 instance. You can find instructions for different distributions on the NVIDIA website: https://developer.nvidia.com/embedded/linuxtegra. And also make sure the drivers are compatible with your WSL2 version and CUDA Toolkit version.,"I have found a working solution. The issue can be closed, but here are my findings for future reference/others: Profiling on WSL2 was added with CUDA 12, so everything before tf 2.15 does not work.  For tf 2.15 the packaged CUDNN Version (from [andcuda] does not work with profiling, but a custom install of the Toolkit (12.2) and CUDNN (8.9) finally works. Here it is important to correctly set the LD_LIBRARY_PATH and XLA_FLAGS environmental variable so that the tensorflow installation finds the correct CUDA libraries. So if someone wants to investigate this further, I would start by comparing the libraries installed by [andcuda] with the custom install and check if something is missing/different here.",Are you satisfied with the resolution of your issue? Yes No
yi,SteefR,Building from Source java.io.IOException PKIX path building failed," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16  Custom code No  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.11  Bazel version 6.5.0  GCC/compiler version Clang 14.0  CUDA/cuDNN version   GPU model and memory   Current behavior? I am trying to install Tensorflow from source on Linux Ubuntu 22.04 following the guide, and I managed to get to configure the bazel build just fine (I am building without gpu support). When trying to build it can download all packages except when trying to download from [https://golang.org/dl/?mode=json&include=all, https://golang.google.cn/dl/?mode=json&include=all] i get the error java.io.IOException: Error downloading [https://golang.org/dl/?mode=json&include=all, https://golang.google.cn/dl/?mode=json&include=all] to /home/name/.cache/bazel/_bazel_name/53019e683ff9cb9992223629eccf493b/external/go_sdk/versions.json: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target I tried to install the certification manually and found some places to do it, but still n",2024-02-05T08:56:49Z,stat:awaiting response type:build/install subtype: ubuntu/linux subtype:bazel,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62894,", Have you tried the above query after trying the **bazel clean expunge** command. Could you try once and try repeating the above steps. And also make sure you follow the steps mentioned here. Thank you for the details and usually users get this error `PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested` target due to the system firewall. The system firewall restricts the application to connect to external systems. The firewall requires a valid certificate to allow access to the external systems. Could you please confirm, whether you're installing Bazel normally or via Bazelisk because Bazelisk is an easy way to install Bazel and automatically downloads the correct Bazel version for TensorFlow and if you're using Bazelisk then please download manually and please follow below steps : wget https://github.com/bazelbuild/bazelisk/releases/download/v1.16.0/bazeliskdarwinarm64 (If you get any error with respect to certificate then you can use this command wget nocheckcertificate https://github.com/bazelbuild/bazelisk/releases/download/v1.16.0/bazeliskdarwinarm64  If you have already done the above steps and installed Bazel correctly then please execute below steps to see the Bazel or Bazelisk location and if possible please help me with those details  Also Could you please have a look at  https://bazel.build/install/compilesourcebootstrapunix.  and comment out 'for' statement and rebuild a binary version of bazel: https://github.com/bazelbuild/bazel/blob/master/src/main/cpp/blaze.ccL1015 Then use this to build tensorflow. Thank you!","The way I fixed it was removing my bazel installation, and installing using apt, and adding startup host_jvm_args=Djavax.net.ssl.trustStore=/etc/ssl/certs/java/cacerts \             host_jvm_args=Djavax.net.ssl.trustStorePassword=changeit to ~/.bazelrc",", Glad the issue was fixed. Could you please feel free to move this issue to closed status. Thank you!",Are you satisfied with the resolution of your issue? Yes No
transformer,SchweitzerGAO,Model inference crashed," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  TensorFlow installation (pip package or built from source): pip  TensorFlow library (version, if pip package or github SHA, if built from source): 2.15.0  2. Code Provide code to help us reproduce your issues using one of the following options:  Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong: Inference crashed with the following error: ",2024-02-04T02:18:05Z,stat:awaiting response TFLiteConverter TF 2.15,closed,0,7,https://github.com/tensorflow/tensorflow/issues/62891,I think this may related to CC(Model conversion crashed when feed data during quantization), This error likely indicates an issue with a Mean operation within the TFLite model. It might be due to Incompatibility between the Mean operation in the original model and its implementation in TFLite also has incorrect input or output shapes for the Mean operation. It can be because of the issues with quantization if the model is quantized. Thank you!,Thank you!  I downgraded to Python 3.8 and tensorflow 2.13.0 and the error disappeared,"It works with version 2.14.1 on Python too. However, I'm encountering the same issue when trying to use it on Android. If anyone has successfully converted Whisper to tflite for Android, could you please share the versions of the libraries you used, or any modifications you made? In my experiments, I observed that adding multiple tf.print statements to monitor intermediate results leads to the expected outcomes. What could be the reason for this?","> It works with version 2.14.1 on Python too. However, I'm encountering the same issue when trying to use it on Android. If anyone has successfully converted Whisper to tflite for Android, could you please share the versions of the libraries you used, or any modifications you made? I needed to remove the app to change the assets of the app.","This still happens on Android, but only with the base.en model.","Has anyone got any solution for the Android problem? I was trying back and forth with some pip packages (transformers, keras, tensorflow) but so far no luck :( FYI: https://github.com/nyadlasys/whisper.tflite/issues/35issuecomment2157391280 Some users applied this patch and got it working(?)"
yi,timotheeMM,Fix a typo in tensorflow/tools/pip_package/setup.py,I fixed a typo in the setup.py file: accomodate > accommodate,2024-02-03T17:50:09Z,size:XS,closed,0,2,https://github.com/tensorflow/tensorflow/issues/62890,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , Please submit multiple typo fixes in a single PR as the CPU/GPU hours are wasted on CI. Hence, we do not encourage one liner grammatical changes as it is an expensive process. Thank you for your contribution!"
yi,claCase,Batch Shape inside customized train_step of keras subclassed model is None," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.15  Custom code Yes  OS platform and distribution Windows WSL  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm trying to get the full input shape inside the train_step function of a subclassed keras model. When using tf.shape(inputs) will output the rank of the tensor, when using inputs.shape will output (None, d1, d2, ...).   This issue is similar to CC(How to get integer batch size in Keras model.fit()) and CC(tf.keras.Model subclassing: when check inputs size inside the model, the size display ""batch number"" as None) .  Standalone code to reproduce the issue   Relevant log output ",2024-02-03T10:50:34Z,stat:awaiting response type:bug comp:keras TF 2.15,closed,0,8,https://github.com/tensorflow/tensorflow/issues/62889,"Hi, for me this is not a bug at all here my explanation: During the training process in TensorFlow, the batch dimension is usually automatically removed before passing the data to the train_step method. In my opinion and this is part of the internal functionality of the TensorFlow facilitate the deployment of custom models, this make models more flexible and can be used with different amounts of data at each training step. I have assumed this behavior from the tensor docu and the following argument: * dynamic_batch: Whether to set the batch sizes of all the returned tf.TensorSpec to None. (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X), the batch size will always be preserved). Defaults to True. If I'm wrong please make me know since I would also like to learn.","Hi , you are right, this is the intentional API behaviour.  I've better identified the issue. I'm trying to sample a random vector from a tensorflow_probability distribution. If we follow the example at the  docs for a custom train step tf.shape(inputs)[0] is used to get the batch dimension and sample a random vector from keras.backend.random_normal, and in the example below it outputs the right dimensions (None, dim1).  However if we use a Normal distribution from tensorflow_probability we: 1) Cannot use tf.shape(inputs)[0] as it is a 'tensorflow.python.framework.ops.SymbolicTensor' class, and therfore we must use inputs.shape[0] 2) Cannot sample from a (None, 2) input shape, since we get  the following: ValueError: Attempt to convert a value (None) with an unsupported type () to a Tensor. I've updated the code example to better show case the issue:  which outputs:   I think this issue may best be opened on the tfp repo. ","Hi , thanks for the answer and for your effort! I think it's impressive how you found this! best regards, Reyes","Hi  , thanks for the kind words. I think I may have found the culprit in the Tensorflow Probability  Distribution Class. Tensors must be statically defined, and a shape of None type can't be automatically inferred. I don't think there's a solution to this issue, the only solution would be to create a custom fit function... I'm still opening a new issue to get more insights in the tfp repo. ",", As mentioned above, this looks like the intended behaviour. The batch dimension is automatically removed before passing the data to the **train_step** method. Also it is mentioned in the tf.keras.Sequential official document. Reference Also as you mentioned above, the issue related to tensorflow probabilty will discuss and resolved in this respective repo. Thank you!","Hi   I've opened the issue under the tfp repo, this issue can be closed,  Thank you",Are you satisfied with the resolution of your issue? Yes No,I solved the issue by passing the batch_size parameter to the fit function 
yi,jamwar01,PReLU Op Builtin Kernel gives NaN output," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.14  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04.6 LTS  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Some output values in a PReLU output tensor are NaN when using TFLite Interpreter with BUILTIN kernels. No NaNs are seen when using BUILTIN_REF (reference) kernels, so this appears to only be an issue with builtin. I would expect to see _similar_ values when using both builtin and reference kernels; and not see any NaNs.  Standalone code to reproduce the issue   Relevant log output ",2024-02-02T18:03:25Z,stat:awaiting response type:bug stale comp:lite TF2.14,closed,0,9,https://github.com/tensorflow/tensorflow/issues/62888," The simplest solution is to use the BUILTIN_REF kernels instead of the BUILTIN kernels. BUILTIN_REF kernels are reference implementations and often slower, but they shouldn't produce NaN outputs in this case. Here's how to switch:  Thank you!","Thank you for your reply! 😄  Yes, the workaround for now is to use the reference kernels like you say.  My aim was to just report that the builtin kernels appear to be broken so that this is highlighted to the relevant team. The decrease in performance from using the reference kernels is likely a deterrent in many cases, however, so I believe it would be useful to have this addressed.","Hi , I have tested the given code with BUILTIN kernels in TF 2.15 version.  It is working fine and the output tensor is not giving any Nan values.  Here is the screenshot. !image and the output tensor values are  Please refer the gist. Thank You",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,I still have nan output when using my specific numpy input. Could you provide a method whereby I can transfer the faulty_input.npy to you? Its compressed size is 28MB which exceeds the 25MB limit set in this page. Thank you.,"Hi  , Please share your `faulty_input.npy` through the google drive link. Thank You",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
gpt,juancresc,tfgo Library not loaded: @rpath/libtensorflow.2.dylib in mac M1 (no LC_RPATH's found), Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version v2.14.1  Custom code Yes  OS platform and distribution Apple M1 Pro 14.3  Mobile device _No response_  Python version 3.7  Bazel version 6.1.0  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? when I run my program that imports tensorflowgo   Standalone code to reproduce the issue   Relevant log output _No response_,2024-02-02T13:08:02Z,stat:awaiting response type:build/install subtype:macOS TF2.14,closed,0,7,https://github.com/tensorflow/tensorflow/issues/62886,adding more details ," If you are building tfgo from source, make sure you've configured it for the ARM64 architecture (Apple M1) using the `config=macos_arm64 `flag. If you are using a prebuilt binary, please ensure it's specifically for Apple M1. Thank you!","I am having the same issue with `tf v2.15.0` on `macos_arm64`. A simple C code  to display tf version builds and runs fine, however, I get following error when running with Go bindings: ",It seems the issue is only when running `go test`. I was able to build an executable using Go bindings to TF and that runs fine on `macos_arm64` for TF v2.15.0,This is what I did to make it work.  , Could you please confirm if the issue has been resolved? Thank you!,Are you satisfied with the resolution of your issue? Yes No
yi,matthew-olson-intel,[oneDNN] Add oneDNN version of SparseMatrixMatMul,"Adds `_MklNativeSparseMatrixMatMul` and its accompanying kernel, which uses oneDNN to multiply a CSR sparse matrix by a dense tensor. The op is enabled with an environment variable (`TF_ENABLE_ONEDNN_SPMM`), so is entirely optin. It also includes tests and a benchmark, which we've used below to measure its performance against the existing kernel. The performance looks promising particularly for larger shapes, and is optimized to use the AVX2 and AVX512 ISAs. These results were collected using the new benchmark in `tensorflow/core/kernels/mkl/mkl_sparse_matrix_matmul_op_benchmark.cc` on an Intel Xeon Platinum 8480 with hyperthreading enabled. To minimize NUMA effects, we bound it to the first socket. Configuration (NNZ_M_K_N)  1.82",2024-02-01T19:53:00Z,ready to pull size:XL comp:core,closed,1,17,https://github.com/tensorflow/tensorflow/issues/62883,Nice ,cool,"> Could you please help make more changes? There are some more errors. Sure. Sorry, didn't see those errors in my local tests, and I didn't see which target in the CI threw those errors.",I think I've made all of the appropriate changes;   does this look OK to you?," Made that change, and I think the current ""Presubmit"" test failure is unrelated to this PR."," I can see that some internal checks failed, but can't see what they are.",">  I can see that some internal checks failed, but can't see what they are. ","> >  I can see that some internal checks failed, but can't see what they are. >  >  Is it possible to get a commandline reproducer for this error? Can't reproduce locally, so fixing it might be trialanderror that spams this PR with commits and spams you with emails!","> > >  I can see that some internal checks failed, but can't see what they are. > >  > >  > >  >  > Is it possible to get a commandline reproducer for this error? Can't reproduce locally, so fixing it might be trialanderror that spams this PR with commits and spams you with emails! I'm not sure... I'm surprised bazel doesn't find the dependency issue.  Maybe Google's internal tooling is more strict?","> > > >  I can see that some internal checks failed, but can't see what they are. > > >  > > >  > > >  > >  > >  > > Is it possible to get a commandline reproducer for this error? Can't reproduce locally, so fixing it might be trialanderror that spams this PR with commits and spams you with emails! >  > I'm not sure... I'm surprised bazel doesn't find the dependency issue. Maybe Google's internal tooling is more strict? All right, trying some things. Thank you for your reviews, by the way!", Can you get the internal checks running to test the latest version out?,"> Thank you! Sorry for so many change requests! (It's from our internal checks.) Hey, I get it  totally fine! Thanks for bearing with me while I made them.",Any update on this? Is the next step in the process for  to review? Looks like the Copybara checks are passing now.,> Any update on this? Is the next step in the process for  to review? Looks like the Copybara checks are passing now. It's going through the internal battery of tests now.,"> > Any update on this? Is the next step in the process for  to review? Looks like the Copybara checks are passing now. >  > It's going through the internal battery of tests now. Oh, nice, sounds good.","Unfortunately I had to rollback this PR since it broke a oneDNN build on arm64. The error was:  olsonintel, can you create a new PR with this issue fixed?", Sure. The new PR is in CC([oneDNN] Add oneDNN version of SparseMatrixMatMul (v2)).
llm,elfringham,remapper_test fails on AARCH64 with --config=mkl_aarch64_threadpool, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version git HEAD  Custom code No  OS platform and distribution Ubuntu 20.04  Mobile device n/a  Python version 3.9.17  Bazel version 6.5.0  GCC/compiler version 17.0.0  CUDA/cuDNN version n/a  GPU model and memory n/a  Current behavior? The tests //tensorflow/core/grappler/optimizers:remapper_test and //tensorflow/core/grappler/optimizers:mkl_remapper_test will fail on AARCH64 when built with config=mkl_aarch64_threadpool and run with TF_ENABLE_ONEDNN_OPTS=1. But also remapper_test will fail those tests that are not skipped even with TF_ENABLE_ONEDNN_OPTS=0. All tests in mkl_remapper_test are skipped in that case.  Standalone code to reproduce the issue   Relevant log output ,2024-02-01T17:04:48Z,type:bug comp:mkl,closed,0,5,https://github.com/tensorflow/tensorflow/issues/62882,MKL     ,It seems that certain parts of tensorflow/core/grappler/optimizers/remapper.., Do these features still need to be disabled for the best performance with ACL?,Are you satisfied with the resolution of your issue? Yes No,", thanks for your fix. >  Do these features still need to be disabled for the best performance with ACL? Yes, they are still needed.  Unfortunately, we should have updated the tests initially when the changes were made in this PR: https://github.com/tensorflow/tensorflow/pull/60723, but I think back then the `remapper_test` was disabled and we didn't see it failing. Probably the right fix for AArch64 would be to still execute the tests, but check the name of the operators and arguments matches the operators and arguments without the optimisation. In this way we will have testing of `remapper` phase too for occasions when we do not lower to oneDNN on AArch64 and if there are any changes of future we will be able to catch them. We will look into making those changes."
gpt,j-dsouza-19,Unable to build tensorflow lite select ops from source using docker image, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version r2.14  Custom code No  OS platform and distribution Docker image   tensorflow/build:latestpython3.11  Mobile device tensorflow/build:latestpython3.11  Python version 3.11  Bazel version 6.5.0  GCC/compiler version 9.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Unable to build tensorflow lite select ops from source using docker image  facing error due to protobuf mismatch (I think)  Standalone code to reproduce the issue ,2024-02-01T15:44:21Z,stat:awaiting response type:build/install stale comp:lite TF2.14,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62881,"dsouza19, Looks like there is incompatible with the Bazel version 6.5.0 and GCC/compiler version 9.4.0 with tensorflow v2.14. The tensorflow version2.14 is compatible with Clang 16.0.0 and Bazel 6.1.0. https://www.tensorflow.org/install/sourcelinux Could you please try to use Bazel 6.1.0 in order to compile the master  https://www.tensorflow.org/install/sourcelinux Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,akhilgoe,[oneDNN] Fix macro titles in TF CPU feature guard,This PR fixes the titles of some of the macros used by the TF CPU feature guard and prevents displaying some erroneous messages while importing TensorFlow.,2024-01-31T22:57:51Z,awaiting review ready to pull size:S comp:core,closed,0,0,https://github.com/tensorflow/tensorflow/issues/62879
yi,MoritzMSP,Trying to reshape a tensor after applying a boolean_mask with XLA context will result in an error or wrong values," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15.post1  Custom code Yes  OS platform and distribution WSL Ubuntu 22.04  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Trying to reshape a tensor after applying a boolean_mask with XLA context will result in an error. The masked Tensor is not recognized as being smaller and can therefor not be reshaped into the correct size.  In my particular case, working with the flattened tensor after applying the mask is not possible. A reshape into the right dimension is therefor necessary. Also applying the mask outside of the XLA context won't work in my context. I've also tried using tf.ragged.boolean_mask, however the error still persists and my further computations won't work with ragged Tensors.  Standalone code to reproduce the issue   Relevant log output ",2024-01-31T18:19:23Z,stat:awaiting response type:bug comp:xla TF 2.15,closed,0,3,https://github.com/tensorflow/tensorflow/issues/62878,"Hi **** , It throws an error as it should, given that you're attempting to reshape the tensor to [3, 2], a shape that does not match the number of elements passed through the mask (which should be 6 for your example). This is the correct behavior because XLA cannot validate the shape you're trying to reshape into due to the dynamic nature of the boolean mask operation. Here I attached gist for your reference. Thank you!",  thank you for clarification,Are you satisfied with the resolution of your issue? Yes No
yi,N1k0l1n,tf.matMul," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 3.9.0  Custom code Yes  OS platform and distribution Windows 10  Mobile device _No response_  Python version Python 3.12.1  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Im new to tensorflow js, so please excuse me if the issue is resolved prior. I have a problem iwth tf.matMul, when trying to do this operation with  a 2x2 matrix, and the first value is wrong.  Standalone code to reproduce the issue   Relevant log output _No response_",2024-01-31T14:17:11Z,type:bug comp:ops,closed,0,3,https://github.com/tensorflow/tensorflow/issues/62875," in the given case the output [[17, 23], [39, 53]] is correct. You're doing a matrix multiplication with the following matricies:  so the first element will be  `1*5 + 2*6 =17` Perhaps you've reversed 6 and 7.","Yeah, I switched the possition and i didnt evene noticed. Im terrebly sorry. Thank you for your time",Are you satisfied with the resolution of your issue? Yes No
yi,Truongdhvnu,Details of the calculation when referencing a quantized TFLite CNN model," Issue type Documentation Feature Request  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.14.0  Custom code Yes  OS platform and distribution Window10  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? for the purpose of research and learning, I am trying to understand the series of mathematical operations needed to operate when using a CNN quantized model. I started with quantized conv2D layers. I studied a paper about the quantization method and inference method. As I understand, when operating conv2D, an affine function is applied to input data and kernels; then, we add bias to the result. Finally, we quantize the result to uint8 (also using an affine function), so the uint8 quantized result can be used for the next layer. I created a very simple model that contains only one conv2D layer, with 1 input channel and 1 output channel. So, the only operation this layer performs is the convolution between the input and the simple kernel that I created.  Then, i quantized the simple model  I succesfully",2024-01-30T18:12:54Z,stat:awaiting response type:support stale comp:lite TF2.14,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62872,"Hi , It's appreciable for your research interest on TFLite quantization.  >How my data calculate thought these tensors with specific input value. And why is the last tensor look so strange (no name and all paramers are equal 0) In the TFLite quantization process, the input values, weights and bias data types(float) at each layer are converted to  either to int8/int16/float16/...  during conversion depending on target device. The quantization might be symmetric(127to +128) or affine(Unsymmetric 0255) .  To know how  tensors are quantized, please refer  link1. The last unnamed tensor might be an unused tensor created during the conversion process.  >I tried to quantization with all parameters is uint8, but i can't find supported parameter to configure for converter For more clear interpretation about tensors and their quantized values add the following command for tflite file.  and  use link2 for better visualization of the quantized model .  Thank You",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,Doomski99,"Building stuck at ""Linking tensorflow/libtensorflow_cc.so.2.16.0"" with ""--compilation_mode=dbg"""," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.16  Custom code No  OS platform and distribution Linux Ubuntu 22.04.1 LTS  Mobile device _No response_  Python version 3.11  Bazel version 6.5.0  GCC/compiler version 11.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm trying to build tensorflow/lite/python:tflite_convert with compilation_mode=dbg to debug it later with gdb. I've tried with two very different systems: one with an intel cpu on WSL2 win11 and one with a ryzen cpu on native ubuntu. Both are getting stuck with linking ""tensorflow/libtensorflow_cc.so.2.16.0"". Is it supposed to take many hours? I've built the tflite_convert many times without that compilation mode and it always builds successfully. Full output (after restarting the command):   Standalone code to reproduce the issue   Relevant log output _No response_",2024-01-30T15:40:11Z,type:build/install,closed,0,3,https://github.com/tensorflow/tensorflow/issues/62871,Build stopped with an error after several hours:  The file stderr2 is very large (at 600MB+) and contains lines such as: ,"I'm not sure if changing compilation_mode to config fixed it or it was the per_file_copt, nonetheless I ran this command and it worked: `bazel build config=dbg verbose_failures jobs=16 per_file_copt=+tensorflow/compiler/mlir/lite/*@g //tensorflow/lite/python:tflite_convert`. This helped: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md",Are you satisfied with the resolution of your issue? Yes No
yi,Frinklin-Wang,"error: ‘tensorflow::error’ has not been declared  Status(tensorflow::error::Code code, tensorflow::StringPiece msg);",**System information**  I am a newer to c++. I finish tensorflow compiling and generate the `libtensorflowcore.a` file. Now I want to test using cmake 2.8+.  I create a `CMakeLists.txt` such as:  The ` load_model.cpp` such as:  Can anyone give me some help? Thank you very much!,2024-01-30T02:50:20Z,type:support comp:core TF 1.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62868,"Wang, TensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.15 and check if you are facing the same issue. Also I suspect you have missed to submit the CMakeLists Info:  and try to add a declare `class tensorflow::error::Code` Thank you!"," , I add a declare `class tensorflow::error::Code` in `.../core/lib/core/status.h`. But it doesn't work. And I don't know how to compile the tensorflow2.x c++ static library. I only try tensorflow1.x c++ static library. I  add `include` in `tensorflow/core/lib/core/status.h `, it does work. But, there are some other errors:  My test.cpp is :  How to solve the problems?",I solved these errors when I modified the contents of CMakeLists.txt. ,Are you satisfied with the resolution of your issue? Yes No
yi,suyash-narain,TFLITE: Execution on GPU delegate gives runtime error with no CPU fallback," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.14  Custom code Yes  OS platform and distribution aarch64 linux  Mobile device _No response_  Python version python 3.10.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am using an aarch64 device similar to raspberry pi running tf 2.14.  I installed the latest version of tflite_runtime using pip3 install tflite_runtime which installed v2.14 I have a tflite model sourced from here: https://github.com/usefulsensors/openaiwhisper/blob/main/models/whisper.tflite  which works well on CPU but when I try to execute it on GPU or NNAPI tflite delegate, I get runtime error and no other error log accompanying it.  The error snippet is below:  the code I am using is similar to the one mentioned in this comment: https://github.com/tensorflow/tensorflow/issues/59273issuecomment1397704596 I checked the model support using model Analyzer  and i get the output:   the entire log is attached: model_analyzer_log.txt Not all ops in this model are supported in GPU but other ops are supported. My understan",2024-01-30T02:18:21Z,stat:awaiting tensorflower type:bug comp:lite TFLiteGpuDelegate TF2.14,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62867,"I don't have a VM which matches this architecture with GPU closely enough, Hi , can you please take a look? Thanks.","Hi  , I get the same error when i use any whisper based tflite models. On digging a bit deeper I found out that the delegate is giving runtime errors because the model contains an op which has dynamic sized tensors whereas the delegate can support only static sized tensors.  My question now is, why are these ops not falling back onto CPU instead and giving a runtime error on GPU? Is there a way i can convert dynamic tensors to static while converting the model? I use the below script to generate my whisper tflite model  the generated model has an OP named 'WHILE' which is INT32, and is the second last op, having multiple inputs. How can i give it static inputs instead or ensure this op fallsback onto CPU instead of the delegate? thanks","Hi, narain  Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/83 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
yi,MoritzMSP,tf.concat (and tf.transpose) inside a for loop with tf.range in the context of a GradientTape while using XLA dosn't work," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.15.0.post1, 2.15.0  Custom code Yes  OS platform and distribution WSL Ubuntu 22.04  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.2  GPU model and memory RTX 3080 Ti  Current behavior?  Current behavior Even tho the shapes of the elements which are concatenated are well defined to compile time, when running the code provided, the execution fails with the error log provided. This is also the case, if you enforce the shapes with using tf.reshape().  If you remove either   the GradientTape context  or the for loop with tf.range  or the XLA compilation  the code will work as expected. Unrolling the for loop with range() is not desired since the number of iterations will be >50000 in the final project. Also converting the for loop in a tf.while loop with maximum_iterations specified will result in the same error. Specifying the batchsize to a constant value (also in the input_signature) won't resolve the issue either. Also changing the tf.device between GPU / CPU won't resolve the issue. The same error arise",2024-01-29T09:22:38Z,stat:awaiting tensorflower type:bug comp:xla TF 2.15,open,0,8,https://github.com/tensorflow/tensorflow/issues/62861, Could you consider to calculate shapes and perform tf.concat and tf.transpose outside the loop. Precompute Shapes and Operations could help in this case. Thank you!,Are you satisfied with the resolution of your issue? Yes No,Are you satisfied with the resolution of your issue? Yes No," This could indeed work in the given simplified example, however in other models I'm developing this is not possible since the values which are concatenated are dependent on the value of yi which is in turn dependent on the prior iteration of the loop. e.g. Part of the compute function from another model:  (The Modeling.skewSym(X) function just rearranges the elements in X with the shape [None, 3, 1] into a Matrix with a shape of [None, 3, 3]) > In the tf,concat / tf.transpose Function A_LR and A_RL are used, which are depended on eta_LR and eta_LR is part of yi. Thats the reason why it can't be Precomputed outside of the loop.", Thank you for the update!  Please find the replicated gist of the error reported. Thank you!,"I am not familiar with tf.function, assigning to  who is familiar with this and might know who can help.","I'm not sure this is expected behavior or not. I was able to simplify the program a bit:  Fails due to `ConcatOffset` not having constant time inputs. The error message is a weird red herring since the shapes are known but the actual arguments to `ConcatOffset` need to be constant. Also the Model.compute(..) has jit_compile=False but since it's called by Estimator.estimate(...) all the callees also have to be compiled.  > If you remove either > the GradientTape context > or the for loop with tf.range > or the XLA compilation > the code will work as expected. Removing these makes this totally skip compilation and so that's why the error doesn't show up. I'm not sure this is expected behavior. Looping in swachhandl@ who might know more. ================ EXTRA DEBUG DUMP=============== Failed MLIR dump from simple version:  I'm not sure why `tf.ConcatOffset` is being created here upstream, but the op definition says the input for the dimension should be constant. Here it's the output of `tf.FLoorMod` and not const and hence the error. ",Unfortunately I'm not very familiar with the behavior here. The documentation for ConcatOffset seems to mention that it is used for gradient computations: https://www.tensorflow.org/mlir/tf_opstfconcatoffset_tfconcatoffsetop  Tagging  and  to see if they know more about the gradient computation / concat op. 
yi,SVyatoslavG,Tensorflow distributes training throws exception on mac m2," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15  Custom code Yes  OS platform and distribution Mac OS 14.2.1  Mobile device _No response_  Python version 3.11.6  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to run distributed training using `tf.distribute.experimental.MultiWorkerMirroredStrategy()` on two Mac M2 machines. However, training does not start on the GPU, and the code throws the attached exception. The distributed training works fine if I use CPU only. I have installed the latest `tensorflowmetal 1.1.0`. Is `MultiWorkerMirroredStrategy` supported on Mac M2?  Standalone code to reproduce the issue   Relevant log output ",2024-01-28T23:20:51Z,stat:awaiting tensorflower type:bug comp:dist-strat TF 2.15,open,0,0,https://github.com/tensorflow/tensorflow/issues/62859
yi,chrisyeh96,RAM memory leak with tf.function when training multiple models in a loop," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.15  Custom code Yes  OS platform and distribution Ubuntu 22.04.3 LTS  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version n/a  GPU model and memory n/a  Current behavior? When I train multiple models in a loop, if I decorate the `train()` function with `.function`, then the memory usage keeps on increasing after each loop iteration, even when I delete the model at the end of each loop and clear the TensorFlow graph/session. The memory leak does not occur when `.function` is removed. However, model training performance is significantly slower. Colab notebook to reproduce issue: https://colab.research.google.com/drive/1sJsGmcFeZVx6ImNbqnBsgF_LzIyPxXPW?usp=sharing  Standalone code to reproduce the issue   Relevant log output ",2024-01-27T05:06:09Z,stat:awaiting tensorflower type:bug comp:tf.function TF 2.15,open,0,1,https://github.com/tensorflow/tensorflow/issues/62854, I was able to replicate the issue on colab. Please find the gist here for reference. Thank you!
yi,suyash-narain,TfLite: undefined symbol TfLiteGpuDelegateV2Create in android ," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15  Custom code Yes  OS platform and distribution Mac Big Sur  Mobile device aarch64 device  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi, I have a whispertflite model and android app as well sourced from https://github.com/nyadlasys/whisper.tflite/tree/main/whisper_android which works well on CPU of an aarch64 target similar to raspberry pi. I wanted to add support for GPU delegate and NNAPI delegate in the app source code in android studio.  To note, the app is unsupported on GPU/NNAPI (or some ops are unsupported on GPU/NNAPI). And according to my understanding, if a model op is unsupported on GPU/NNAPI, it will fallback onto CPU. So general understanding is the model should fallback and execute on CPU When I add support for GPU delegate using the below code snippet sourced from https://www.tensorflow.org/lite/android/delegates/gpu_nativeenable_gpu_acceleration,   I get android build error saying  **ld: error: undefined symbol: TfLiteGpuDelegateV2Cre",2024-01-26T22:22:05Z,stat:awaiting tensorflower type:bug comp:lite TFLiteGpuDelegate TF 2.15,closed,0,11,https://github.com/tensorflow/tensorflow/issues/62853,"Hi narain, I'm trying to understand how you integrated TFLite into your application. Did you build a shared library from source and included in your project? If so do you happen to have the build command you used? I'm assuming you're using JNI. Alternatively if you can export your project, I believe that will answer my questions as well. Thanks for your help.","Hi , how can i export my project onto github issues? or should i point you to the github link from where the project is sourced? its sourced from  https://github.com/nyadlasys/whisper.tflite/tree/main/whisper_android","Hi narain, you should be able to export it in Android Studio, then upload the zip file:  !image If not you can upload it to an external source such as gdrive and ensure I can download it (i.e make it public).","i think i figured out the reason behind the error. the model is not compatible with the delegate and hence it fails to even create the same on android studio. the thing i cannot understand is, if a model is CPU compatible but not compatible with the delegate, why can't the model be simply fallback on CPU instead of giving 'error invoking delegate' error?","narain https://www.tensorflow.org/lite/performance/gputroubleshooting_gpu_support seems to state that it should just fallback to running parts of the model on CPU (despite it being slower). So this should be a bug... do you mind uploading your project so that we may reproduce? (Often times people change more than they think, that affects our current understanding of the issue.)",https://drive.google.com/file/d/1EQ4_ieaV_7SFZBLR9aM9yDvBY_pUXN_3/view?usp=sharing here is the zip file for the project,"I tried updating the tflite versions to see if that helped, it did not, I was able to reproduce with the uploaded zip file:  Hi, , can you please take a look? Thanks.","narain  I did resolve this error by linking tflite for gpu in cmake file. You can download this from https://github.com/ValYouW/tflitedist/releases //tensorflow/lite:libtensorflowlite.so //this is already available in your tree tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so // this needs to be pushed at cpp/tfliteapi/generatedlibs/armeabiv7a Upodate the whisper_android/app/src/main/cpp/CMakeLists.txt as below if (ANDROID)     add_library(audioEngine SHARED TFLiteEngine.cpp TFLiteEngineJNI.cpp)     target_include_directories(audioEngine PRIVATE ${INCLUDE_DIRS})      Add 'tflite' library (imported)     message(""new"")     add_library(tflite SHARED IMPORTED)     set_target_properties(tflite PROPERTIES IMPORTED_LOCATION             ${CMAKE_CURRENT_LIST_DIR}/tfliteapi/generatedlibs/${ANDROID_ABI}/libtensorflowlite.so)     **add_library(tflite_gpu_delegate  SHARED IMPORTED)     set_target_properties(tflite_gpu_delegate  PROPERTIES IMPORTED_LOCATION             ${CMAKE_CURRENT_LIST_DIR}/tfliteapi/generatedlibs/${ANDROID_ABI}/libtensorflowlite_gpu_delegate.so)     target_link_libraries(audioEngine tflite tflite_gpu_delegate)** endif () I can successfully build for gpu delegate. However, it still breaks when doing inference.","Hi synaptics  thank you for your reply. Using the prebuilt libtensorflowlite_gpu_delegate.so file, the app does get compile, but it fails on the android aarch64 target i am trying to execute it on.  somehow it doesn't build for arm64v8a but only builds for armeabiv7a. do you have any suggestions on how to build it for arm64v8a? thanks","Hi, narain  Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/85 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
yi,mahrukhS,Build Error when installing dm-tree on Docker," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.15.0  Custom code No  OS platform and distribution Windows 11 Pro (Host), Docker container based on ubuntu:16.04  Mobile device _No response_  Python version 3.11.6  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? **Steps to reproduce the error:** 1. Forked the TensorFlow master branch from the TensorFlow GitHub repository. 2. Created a new branch within a GitHub workspace for making changes and running tests. 3. Attempted to run the TensorFlow Continuous Integration (CI) tests locally on a Windows machine using Docker, from the `/workspaces/tensorflow/` directory. 4. Executed the command `./tensorflow/tools/ci_build/ci_build.sh CPU ./tensorflow/tools/ci_build/ci_sanity.sh` to start the sanity check process. 5. Modified `Dockerfile.cpu` to ensure all script files use LF line endings and to set up the environment correctly. 6. Encountered errors when reaching the `RUN pip install packages` step within the `install_pip_packages.sh` script, specifically on the line `pip3 install dmtree",2024-01-24T17:32:58Z,stat:awaiting tensorflower type:build/install TF 2.15,open,0,3,https://github.com/tensorflow/tensorflow/issues/62841,"~~dmtree has no 3.12 wheels right now, but~~ as the error message says, installing CMake should allow for it to be built during installation.","I removed **pip3 install dmtree** from `install_pip_packages.sh` since `dockerfile.cpu` has cmake command. The build is completed, as I can see in the docker desktop build ""a completed build"". However, will removing **pip3 install dmtree** will result in accurate sanity check?","Downloading manually `dmtree==0.1.7` before tensorflow resolved my issue. Got same problem when I was trying to download tensorflow (2.2.0) on Ubuntu 18.04 bionic with a python3.6 and CMake 3.12.1. For some reason pip considered that dmtree==0.1.8 supports python3.6, and attempted unsuccessfully to download it, failing with similar error:  As far as I understand, dmtree==0.1.8 do not support python3.6, therefore downgrading to dmtree==0.1.7 makes sense. But probably that won't help you that much, considering you are running on python3.11.6. "
large language model,YuriSizuku,"tflite RNN model invoke failed with ""num_input_elements != num_output_elements (4288 != 64)Node number 18 (RESHAPE) failed to prepare.Node number 5 (WHILE) failed to invoke."""," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10  TensorFlow installation (pip package or built from source): pip install tensorflow==2.10.1  TensorFlow library (version, if pip package or github SHA, if built from source):   2. Code  tf model: simplernn1d.tf.zip tflite model: simplernn1d.zip  3. Failure after conversion The tf graph runs successfuly but tflite model can not invoke   4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. ",2024-01-24T16:40:59Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.10 TF 2.15,closed,0,7,https://github.com/tensorflow/tensorflow/issues/62840,"Hi , I am trying to reproduce the issue, meanwhile i got a similar Runtime error related to Batch MATMUL. Here is the screenshot. Please take a look. !image Thank You","I was able to replicate both (the difference is the versions), I was also able to replicate the BATCH_MATMUL issue on tfnightly. yang, can you please take a look? Thanks.",I have encountered a similar problem. Does anyone know the reason and how to solve it? thank you.,"Hi , we now have a PyTorch conversion library, AIEdgeTorch, you can find more information here: googleblog. I have actually created a simple script for converting an rnn model here:  example output:  If you want to, you can actually try visualizing the result in modelexplorer as well. Please try them out and let us know if this resolves your issue. If you still need further help, feel free to open a new issue at the respective repo.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,bergentruckung,Significant difference in RSS memory usage between TF1 and TF2," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version TF 2.13.1  Custom code Yes  OS platform and distribution Redhat Enterprise Linux 8.9  Mobile device _No response_  Python version 3.11.4  Bazel version 5.4.0  GCC/compiler version 10.4  CUDA/cuDNN version CUDA 12.2, cuDNN 8.9.5  GPU model and memory A100 80GB  Current behavior? When running the same keras workload on TF1 vs. TF2, I'm seeing a significant increase in memory utilization per epoch. This happens when using both CPUs/GPUs. The utilization for an epoch climbs up significantly after every epoch. See below for TF1 vs. TF2:   Standalone code to reproduce the issue   Relevant log output _No response_",2024-01-24T10:19:10Z,stat:awaiting response type:bug stale type:performance TF 2.13,closed,0,12,https://github.com/tensorflow/tensorflow/issues/62839,", please take a look at this ASAP. ","!image If it helps, running it through `memray` and generating a flamegraph with `leaks`, we see the above as having ~18 GB memory allocated from 27k allocations. So, likely that something is up with those.","Hi  , You need to call `tf.keras.backend.clear_session()` in the loop to remove the global states generated by keras. Could you please add this at the beginning of epoch and let us know. Thanks!","Hi , I tried that out, but it doesn't seem to work.","Hi  , I did replicated the behaviour with Keras3nightly also.Attached gist for reference. Not sure whether this needs to be addressed in Keras repo or here itself. Could you please open an issue at Keras repo also.","Hi , I think your suggested workaround may work for newer TF that has changes from https://github.com/tensorflow/tensorflow/pull/62154 pulled in. This wasn't cherrypicked onto 2.13 branch. Could you backport that PR to 2.13?", Could you please upgrade to the latest if possible as backported changes in 2.13 may introduce new bugs or regressions. It's unlikely for older versions to receive any bug fixes except when we have security patches.  Thank you!,"We're building from source. We were able to build 2.14, but that has cuBLAS open issue that's problematic for us (the fix was to go back to 2.13 and that's what we did). We couldn't find a proper closure of dependencies for 2.15, and that's why we were trying previous versions.", Could you please let us know if you have created another issue in Keras repo as suggested here? Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
transformer,h4ck4l1,Custom Loss Function can't access multiple y_true inputs. ," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15.0  Custom code Yes  OS platform and distribution Ubuntu 22.04 LTS  Mobile device Google Colab  Python version 3.10.12  Bazel version _No response_  GCC/compiler version 11.4.0  CUDA/cuDNN version 12.1  GPU model and memory _No response_  Current behavior? I am using .tfrecord files as DataInput, I made a custom function that gives 4 inputs in X and 2 inputs in y.  I am unable to access the 2nd input in y in a custom loss function.  Data Input code   Tried looking what was actually being accessed in custom loss apparently only reactivity (reac) is being accessed as print(y_true.shape) gave me output of (32,208,2) so only true reac values are being accessed. I want to also have access to masked input values.   Standalone code to reproduce the issue   Relevant log output ",2024-01-24T08:24:36Z,stat:awaiting tensorflower type:support comp:dist-strat TF 2.15,open,0,3,https://github.com/tensorflow/tensorflow/issues/62838,"I want to add some additional Info that I will be running it on TPU so I am testing it on CPU using onedevicestrategy and when I ran the above code on TPU despite of giving me an error on CPU, It gave the same error. ","I am currently not using masks and it gives very similar result. I want to know if there are any ways to implement multi inputs into custom loss functions. Any resources,tutorials will be helpful as I didn't come across any. ",Also have a similar issue.
gpt,tayyibgondal,JIT compilation failed with tf.math.sqrt on GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 15.0  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 11.7  GPU model and memory NVIDIA RTX A5000 (24 GB)  Current behavior? Here is my code for position embedding of gpt model:  This particular line causes the error:  `x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))` Here is the error: !Screenshot from 20240124 113851  Standalone code to reproduce the issue   Relevant log output ",2024-01-24T07:05:56Z,stat:awaiting response type:bug comp:ops comp:gpu TF 2.15,closed,0,5,https://github.com/tensorflow/tensorflow/issues/62837,"Hi **** , Could you please try again? I tried with TF2.15 and I cannot reproduce the error. Please check the gist here. Thank you!",I think there was a cuda version mismatch. I updated my cuda drivers to version 12 using the line: `!pip install tensorflow[andcuda]` Now it's working fine. Thanks!,"Hi **** , Could you please confirm if this issue is resolved for you ? Please feel free to close the issue if it is resolved. Thank you!","Yes, it is resolved!",Are you satisfied with the resolution of your issue? Yes No
yi,reuvenperetz,Can't load model with tf-nightly if the model was saved with tf 2.15," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.15  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hello, I have a model that I saved using tf 2.15 version. When trying to load it using tfnightly I'm getting the following error: ImportError: cannot import name 'is_tensor_or_tensor_list' from 'keras.src.utils.tf_utils' (/usr/local/lib/python3.10/distpackages/keras/src/utils/tf_utils.py)  Standalone code to reproduce the issue Save model with **TF2.15**:  Load model with **tfnightly**:   Relevant log output ",2024-01-23T17:56:48Z,type:bug,closed,0,2,https://github.com/tensorflow/tensorflow/issues/62836,Moved to keras github issues. ,Are you satisfied with the resolution of your issue? Yes No
yi,elfringham,mkl_remapper_test fails with TF_ENABLE_ONEDNN_OPTS=1, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version git HEAD  Custom code No  OS platform and distribution Ubuntu 20.04  Mobile device n/a  Python version 3.9.17  Bazel version 6.1.0  GCC/compiler version 17.0.0  CUDA/cuDNN version n/a  GPU model and memory n/a  Current behavior? The unit test //tensorflow/core/grappler/optimizers:mkl_remapper_test fails when oneDNN is enabled with TF_ENABLE_ONEDNN_OPTS=1  Standalone code to reproduce the issue   Relevant log output ,2024-01-23T16:21:02Z,stat:awaiting tensorflower type:bug type:build/install comp:mkl subtype: ubuntu/linux,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62835,MKL , , ,Are you satisfied with the resolution of your issue? Yes No
rag,wbjnpu,"When convert from keras to tflite, the output is different from what it shoud be."," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab  https://github.com/Zyq/TensorflowASR/blob/v2/asr/models/chunk_conformer_blocks.py this file CTCdecoder is not correct,but converted tflite of conformerblock work well, and I convert it using https://github.com/TensorSpeech/TensorFlowASR . And I have control input shape when load weight. It complex me a long time. maybe because its dynamic train?  3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produ",2024-01-23T08:00:58Z,stat:awaiting response stale comp:lite TFLiteConverter,closed,0,6,https://github.com/tensorflow/tensorflow/issues/62833,"my problem is that ""Model produces wrong results and/or has lesser accuracy""",And I find that .h5 infer is correct， but tflite is wrong. they are the same size and without any quant.,"Hi , I reproduced the code and noticed that each tflite quantized file has the size 105528 bytes like the model. Please look into the issue. Thank You","Hi , can you please upload your .h5 and .tflite file and let us know the inference value you expected vs what you saw? and how you know that the .tflite version is incorrect? Thanks for your help! Generally the more information you provide the faster we can help you.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,ABtwo,"When I install tensorflow using conda, I have  the problem saying that AttributeError: module 'numpy' has no attribute 'object'. `np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:     https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tensorflowGPU 2.6  Custom code Yes  OS platform and distribution Windows10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.3 cudnn version :8.9.6  GPU model and memory _No response_  Current behavior? When I Install tensorflowgpu by conda ,  I created the tfgpu environment, but when I  run this instraction""import tensorflow as tf"" in python I met this problem  AttributeError: module 'numpy' has no attribute 'object'. `np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:     https://numpy.org/devdocs/release/1.20.0notes.htmldeprecations Then I tried to correct the problem by uninstall my numpy which version is 1.26.3  and install the numpy=1.19 , there was another problem like this  ""  error: subprocessexitedwitherror   × Preparing metadata (pyproject.toml) did not run succes",2024-01-23T04:15:09Z,stat:awaiting response type:bug stale comp:gpu 2.6.0,closed,0,8,https://github.com/tensorflow/tensorflow/issues/62832,", The error you are facing **module 'numpy' has no attribute 'object'** is due to numpy version. Could you please try to install `pip install numpy==1.23.4`. Also you are trying to install tensorflow v2.6 which is older and try to follow the compatible test build configurations from the official document. Please try to update to the latest stable version. Thank you!", I have the same issue on Windows. According to the compatible versions table cuda 12.3 isn’t supported in the latest windows release. Is there an issue tracking the update to 12.3 cuda for windows? ,", AFAIK there is no such request for the 12.3 CUDA for Windows environment. There might be a chance that the CUDA version 12.3 will be compatible in the latest tensorflow versions. Could you please feel free to raise the new feature request from here for the same.  Also GPU support on nativeWindows is only available for 2.10 or earlier versions, starting in TF 2.11, CUDA build is not supported for Windows. For using TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2 or use tensorflowcpu with TensorFlowDirectMLPlugin Thank you!"," , Thank you so much for answering my question,  Are you the official technician? I have tried this ""pip install numpy==1.23.4"" command.  It prompted me ""Successfully installed numpy1.23.4"" but there occurred incompatible errors  like that   ""ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow 2.6.0 requires clang~=5.0, which is not installed. tensorflow 2.6.0 requires keras>=2.4.0, which is not installed. tensorboard 2.6.0 requires googleauth=1.6.3, but you have googleauth 2.22.0 which is incompatible. tensorflow 2.6.0 requires abslpy~=0.10, but you have abslpy 1.4.0 which is incompatible. tensorflow 2.6.0 requires flatbuffers~=1.12, but you have flatbuffers 20210226132247 which is incompatible."" So I tried ""conda install numpy==1.23.4""  It may installed the compatible packages automatically and there were  no incompatible errors. then I tried to import tensorflow,It worked correctly Before I saw your answer, I have tried to correct the deprecated errors by changing the ""np.object"" to ""object ,""np.bool"" to""bool"" as it prompted me in the corresponding files. It may be worked, I have imported tensorflow correctly, and run the command ""print(tf.__version__)"" and ""print(tf.config.list_physical_devices('GPU'))"". they all made the correct output. What's more , I want to say when I try to install tensorflow 2.10  using ""conda  create n tfgpu tensorflowgpu==2.10"",It prompts me that  ""Channels:   defaults Platform: win64 Collecting package metadata (repodata.json): done Solving environment: failed PackagesNotFoundError: The following packages are not available from current channels:    tensorflowgpu==2.10 Current channels:    defaults To search for alternate channels that may provide the conda package you're looking for, navigate to     https://anaconda.org and use the search bar at the top of the page."" Should I try ""pip install tensorflowgpu==2.10"" command in the virtual environment?  Please give me some advice. Thanks again for answering me",", Yes, you can try to install the required tensorflow version using `pip install tensorflowgpu==2.10` in the new virtual environment. GPU support on nativeWindows is only available for 2.10 or earlier versions, starting in TF 2.11, CUDA build is not supported for Windows. For using TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2 or use tensorflowcpu with TensorFlowDirectMLPlugin. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
rag,SamKnightGit,Mirrored strategy model.load_weights() failure," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  appears to be fixed in Nightly  Source binary  TensorFlow version tf 2.15  Custom code Yes  OS platform and distribution Linux Ubuntu 23.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.2  GPU model and memory _No response_  Current behavior? Expected: `load_weights` succeeds when using `MirroredStrategy` Actual: `load_weights` raises an obscure error: ""TypeError: unsupported operand type(s) for /: 'Dataset' and 'int'"" Note that this error does **not** occur when using `OneDeviceStrategy` and appears to have been fixed in `nightly` (example code runs successfully).  Standalone code to reproduce the issue  shell TypeError                                 Traceback (most recent call last) Cell In[8], line 25      19 model.fit(      20   train_dataset,      21   epochs=10,      22   validation_data=val_dataset,      23 )      24 model.save_weights(MODEL_WEIGHTS_PATH) > 25 model.load_weights(MODEL_WEIGHTS_PATH)      26 print(""Weights loaded successfully!"")      27 model.summary() File ~/miniconda3/envs/tf214/lib/python3.11/sitepackages/keras/src",2024-01-23T00:41:46Z,stat:awaiting response type:bug stale comp:keras TF 2.15,closed,0,10,https://github.com/tensorflow/tensorflow/issues/62831,"Hi  , This is fixed in Keras3. Please verify the attached gist.",Verified that works after upgrading to Keras3. Thanks for the help!,Are you satisfied with the resolution of your issue? Yes No,"To confirm, will the fix for this come in a patch release for tf 2.15? Or will an upgrade to tf 2.16 be necessary when it is released?","Hi  , This seems fixed with tfnightly as Keras3 will be imported.I have tested with tfnightly and used keras as tf.keras and it works fine. Please refer attached gist.","Thanks , do you know what the timeline is for a fix for this issue? Is waiting for the tf 2.16 release (with keras 3) the only option or will there be a patch for this released to tf 2.15. Perhaps I should create a tfkeras issue detailing the bug to make a patch release for tf 2.15 possible?","Hi  , As this got fixed in master,I doubt to cherry pick it as it will work with Keras3 and TF2.15v as backend also. However if you want this to cherry pick it for tfkeras=2.15v, you may raise a request at tfkeras repo and let's hear from there.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,MintyMH,Jax grad doesn't work with tfp jax module's Gram-Schmidt function because of a lax for loop, Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 0.24.0.dev20240121  Custom code Yes  OS platform and distribution Windows 10 Home 10.0.19045 Build 19045  Mobile device _No response_  Python version 3.11.4  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The gramschmidt function seems to not work with jax's grad function because of a problematic lax for loop.  Standalone code to reproduce the issue   Relevant log output ,2024-01-22T10:18:23Z,stat:awaiting response type:support stale,closed,2,5,https://github.com/tensorflow/tensorflow/issues/62828, One workaround could be to rewrite the GramSchmidt function using vectorized operations and functional programming constructs like jax.map and jax.lax.cond. This avoids control flow and ensures compatibility with grad. Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No," This is probably too late to be of use to you but here is a fix:  I adapted it from the tfp source code. Bizarrely, the tfp source code uses a while loop to perform the GS iterations, despite the number of iterations being known and fixed (it uses the while loop like a for loop). Jax doesn't like while loops because the number of iterations can vary which interferes with its ability to precompile code for a fixed number of iterations. In my version, I use jax.lax.scan instead, which has a fixed number of iterations.  I've written the code to orthonormalize a single set of vectors. If you have multiple sets you can use vmap."
llm,rcostu,Panic when loading a keras LSTM model," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version v2.15.0rc18g6887368d6d4 2.15.0  Custom code Yes  OS platform and distribution Mac OS Sonoma 14.2.1  Mobile device _No response_  Python version Python 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi, Redirected this ticket from tfgo repository Issue CC(Unable to run tensorboard) as the problem seems to be in tensorflow library. The problem documented here happens exactly the same using either tfgo LoadModel function or tf LoadSavedModel function. I have developed a model that makes a time series forecast receiving data from the last 60 days as floats and returns one float. I am training the model in Python and saving it with the export function as stated in the tf documentation for keras models. I have tried anything that I have come up to but I didn't found any solution. There seems to be a reproduction of the very same scenario found on  CC(Loading LSTM ""SavedModel"" from golang results in crash) from 2017 that was solved. Any help is more than welcomed. Thanks in advance!  Standalon",2024-01-20T18:18:48Z,stat:awaiting response type:bug comp:keras TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62824,", Thank you for raising the issue. As this issue is related to keras Could you please post this issue on kerasteam/keras repo. Thank you!",Thank you  for your prompt response. Leaving the link to the opened issue here as a reference.,", Could you please feel free to move this issue to closed status, as it has been tracking in the Keras repo. Thank you!",Are you satisfied with the resolution of your issue? Yes No
yi,w3sip,Link error when using both TensorFlowLiteSelectTfOps.framework and TensorFlowLiteC.framework on iOS," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.14.1  Custom code Yes  OS platform and distribution iOS  Mobile device _No response_  Python version _No response_  Bazel version 6.1.10  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? TFLite is built using bazel commands: `bazel build config=ios_fat c opt copt Os cpu=ios_arm64 apple_platform_type=ios cxxopt=std=c++17 host_cxxopt=std=c++17 //tensorflow/lite/ios:TensorFlowLiteSelectTfOps_framework` `bazel build config=ios_fat  c opt cxxopt=std=c++17 host_cxxopt=std=c++17 //tensorflow/lite/ios:TensorFlowLiteC_framework` When trying to build a framework linking to both resulting frameworks above, seeing the following errors:  (same errors repeat for pretty much every XNNPack method).  Standalone code to reproduce the issue   Relevant log output _No response_",2024-01-19T01:18:56Z,stat:awaiting response type:bug stale comp:lite TF2.14,closed,0,9,https://github.com/tensorflow/tensorflow/issues/62814,"Oh, and I've also attempted to build the Select framework with `bazel build config=ios_fat c opt copt Os cpu=ios_arm64 apple_platform_type=ios cxxopt=std=c++17 host_cxxopt=std=c++17 //tensorflow/lite/ios:TensorFlowLiteSelectTfOps_framework define tflite_with_xnnpack_enabled=false` ... didn't make a difference.", Could you try to temporarily disable the XNNPack delegate during the build of one of the frameworks. This can be done by modifying the build settings or configuration options. Also experiment with linker flags like          `force_load` or `undefined dynamic_lookup` to control symbol visibility. Please use the latest TF version and let us know? Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"  thanks, yes disabling XNNPack is the route I went and it worked, once I've fixed the incorrect way to set the parameter above. Another question I have  is there a way to only use/build a subset of `TensorFlowLiteSelectTfOps_framework` ? It's a very large binary by default; prohibitive for an iOS app."," While there's no direct way to build a subset of TensorFlowLiteSelectTfOps_framework, here are effective approaches to reduce its size for iOS apps: such as follows; 1. Identify Essential Ops: Analyze your model to determine the specific ops it utilizes. 2. Register Only Needed Ops: Employ tflite::MutableOpResolver to register only those essential ops, excluding unnecessary ones. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,">  While there's no direct way to build a subset of TensorFlowLiteSelectTfOps_framework, here are effective approaches to reduce its size for iOS apps: such as follows; >  >     1. Identify Essential Ops: Analyze your model to determine the specific ops it utilizes. >  >     2. Register Only Needed Ops: Employ tflite::MutableOpResolver to register only those essential ops, excluding unnecessary ones. >  >  > Thank you!  Do you mind if you could list down the detailed steps to achieve the above suggestion you made?"
rag,flexorcist,Neural network always outputs the average of target values no matter the input," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.10  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I have a linear regression neural network made with Keras with a numerical input and a text input converted to encoded integers. It consistently outputs the same value no matter the input. The reason I think it outputs average of the entire Ytrain array, which consists of normalized values mostly between 0.40.6, is that my model's loss is decreasing during training to really small values like 0.02 though the output is always the same. Training accuracy always stays the same and model is able to accurately predict exactly one entry. I checked which value does it predict correctly every epoch and it is always quite similar to average. Also the output of a trained model is almost identical to the average. Firstly I tried training the model with 3 different optimizers and 2 loss functions (6 models in total), each model produced ",2024-01-16T16:20:54Z,stat:awaiting response type:support stale TF 2.10,closed,0,5,https://github.com/tensorflow/tensorflow/issues/62804,"Also, I checked the weights and biases of a recently trained model. Almost in all layers weights look like this:  And biases look like this:  ","Hi  , This seems to be a support issue not a bug. FOr support issues please post the issue at tensorflow forum for help. As you confirmed most of your Y_train data having range of 0.40.6 which means your data seems biased.Data Augmentation might help for your case. Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,yasiribmcon,Encode data for bitcast in LE for BE systems,"After introducing commit to add builtin op bitcast for TFLite, below test cases started failing for s390x(BE systems):  Reason for the failure is zip_test_bitcast is comparing Bitcast operation output from TF with builtin Bitcast tflite operation output. As per tf.bitcast documentation, bitcast output should be encoded in LE for BE machines. Since the builtin Bitcast op is providing native output(BE for BE machines), it is not consistent w.r.t tf.bitcast behaviour. After this PR, bitcast operation will be consistent across TF and TFLite, rectifying above mentioned test cases for s390x. These changes will not cause any regression on LE/BE platforms.",2024-01-16T11:50:08Z,stale comp:lite size:S,closed,0,11,https://github.com/tensorflow/tensorflow/issues/62802,Hi   Can you please review this PR ? Thank you very much!  ,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,"Hi  , Can you please review this PR ? Thank you!","Hi  , Can you please review this PR ? Thank you!","Hi  , Can you please review this PR ? Thank you!","Hi  , Can you please review this PR ? Thank you!","Hi  , Can you please review this PR ? Thank you!","Hi  , Can you please review this PR ? Thank you!","Hi  , Can you please review this PR ? Thank you!"
yi,yasiribmcon,Rectify in-memory buffer model loading for s390x,"After introducing commit to add inmemory model support for minibenchmark, below test cases started to fail on s390x(BE machines) :  Reason for the failures is because TFLite buffers in buffer model are being loaded in LE format. After this PR, buffer model will be loading TFLite buffers in BE format for BE plaforms, rectifying above mentioned test cases for s390x.  These changes will not cause any regression on LE/BE platforms.",2024-01-16T10:38:04Z,stale comp:lite size:XS,closed,0,11,https://github.com/tensorflow/tensorflow/issues/62801,Hi   Can you please review this PR ? Thank you very much!  ,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,"Hi  , Can you please review this PR ? Thank you!","Hi  , Can you please review this PR ? Thank you!","Hi  , Can you please review this PR ? Thank you!","Hi  , Can you please review this PR ? Thank you!","Hi  , Can you please review this PR ? Thank you!","Hi  , Can you please review this PR ? Thank you!"
yi,04samuel,Grad Cam Returning None," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.8  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am using Gradient Tape to perform Grad Cam analysis on my CNN however grads returns None  Please see my code below, I have looked through pervious issues but nothing seems to work    Standalone code to reproduce the issue   Relevant log output _No response_",2024-01-15T15:32:47Z,stat:awaiting response type:bug stale comp:ops TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/62797,"Hi  , Could you please submit a minimal reproducible code snippet. Attached code has some dependencies missing and not able to reproduce reported behaviour due to missing dependencies.Attached gist.",,"Hi , The code `grads = tape.gradient(preds[:, top_class_index], last_conv_output)` returns `None` since the two arguments are not connected and there is no differentiable path connecting these args.Please refer this source for more details. If you change the above line of code by adding the parameter `unconnected_gradients=tf.UnconnectedGradients.ZERO` it will return ZERO tensor instead of None. Please refer gist.  This is just to prove that the `target` and `sources` passed to gradient function is not connected and hence the problem. Please rewrite the code by referring this tutorial. Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
agent,cyanic-selkie,Multi-GPU training with gradient propagation in FP16 with XLA fails.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.14  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 11.8/8.7  GPU model and memory A100 40GB (20GB MiG)  Current behavior? Following advice given here, I implemented gradient propagation in FP16 and concurrent backpropagation with gradient propagation. I am using mixed precision with XLA enabled. The custom training step works fine when XLA is not enabled. However, when I enable it, I get the error that you can see in the log. This is run on 2 GPUs using MultiWorkerMirroredStrategy.  Standalone code to reproduce the issue   Relevant log output ",2024-01-15T14:56:06Z,stat:awaiting tensorflower type:bug comp:dist-strat TF2.14,open,0,6,https://github.com/tensorflow/tensorflow/issues/62796,selkie The error in the outcome would be because of the following reasons; a. Incorrect loss scaling factor leading to overflows or underflow. b. Incompatible data types between layers or operations. c. Casting issues between FP16 and FP32 during gradient propagation. Please check if XLA not enabled or not being used for all relevant operations? Thank you!,"Hi, . How do I check that?","selkie To check if XLA is enabled or not please use , `tf.config.list_logical_devices()'  If XLA is disabled, you won't see any XLA devices listed in the output.   Please have a look at this issue. Thank you!","Sorry for the delayed response, here is the result : ",Hi. I have the same problem when using Mirrored strategy for ditributed training and jit compiling some functions called inside the training loop step. Any news regarding this selkie  ?," Unfortunately, no. In fact, all of my opened issues ultimately went unanswered. It's no secret that TF is internally deprecated, and I would suggest switching to PyTorch or JAX/Flax, perhaps using Keras 3 as a stepping stone if legacy code is a big issue for you."
yi,john-james-ai,Automating Transfer Learning: Gradual fine-tuning of a TensorFlow model produces: “ValueError: Unknown metric function: val_loss.” exception during the fit method of the fine-tuning stage.," Current behavior? I’m attempting to automate transfer learning finetuning using iterative, gradual thawing of an underlying pretrained and frozen base network. A compiled model containing the underlying, pretrained, and frozen base network architecture is fed into an X4Learner class. The class exposes two methods: feature_extraction, and fine_tuning. The feature extraction method fits the model for a designated number of epochs and stores the last epoch as an instance variable. The fine_tuning method operates on the ‘feature_extracted’ model and performs an iterative finetuning process in which each iteration thaws some number of layers in the underlying base model, then recompiles it. During finetuning, the fit method produces the following exception. ValueError: Unknown metric function: val_loss. Please ensure this object is passed to the `custom_objects` argument. A reproducible test case can be found here I've included the code below for convenience.  Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  TensorFlow version v2.8.00g3f878cff5b6 2.8.0  Custom code Yes  OS platform and distribution Windows Subsystem for Linux: Ubuntu 22.04.2 LTS  Python version 3.10..12  Standalone c",2024-01-15T12:32:34Z,type:bug TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62795,"Hi **jamesai**, I was able to reproduce the issue on Colab using TF v2.15, Please find the gist here for reference. Thank you!","Hi   Thanks so much for the quick response. I saw the gist and it appears to give a different error in Tf v2.15. In either event, I'm running TF 2.8 because my computer was manufactured in the Mesozoic before AVX was invented. What are your thoughts on the next steps or recommendations? p.s. I'm using the DenseNet201","  I think I owe you a coffee! It turns out that val_loss actually isn't an acceptable metric in the compile method; although, it can be used as a monitor metric in the EarlyStopping callback.  I've tested it with other metrics and it seems to be working fine. I'll go ahead and close this. Thanks again mate!  j2 ",Are you satisfied with the resolution of your issue? Yes No
yi,joseangel-mm,Load models through bytes (by memory),"In my team, we wonder if it is possible to load the models directly by bytes. In the following example, I explain better: Right now, we have in Golang the following to load the model: `tf.LoadSavedModel(ModelPath, tags, nil)` To do the previous step we use this library. What we want is to load the model directly from memory, something like the following: `tf.LoadSavedModelFromMemory(ModelInbytes, tags, nil)` We know we are using a binding for Golang which is the one that creates the API functions, but researching the TensorFlow code we discovered there is no alternative to it, so we wonder if there is any fork that does it, if so we could create or change the binding. Thanks in advance.",2024-01-15T10:49:46Z,stat:awaiting response type:support stale,closed,0,6,https://github.com/tensorflow/tensorflow/issues/62794,"mm, TensorFlow doesn't have direct **load from bytes** api, but there is an alternative approach to achieve loading from bytelike objects Could you please try saving and loading weights and then use the loaded weights from bytes like **model.load_weights(weights_bytes)** or saving and loading the Model (SavedModel format) and load the model from bytes as below  Also try to load the savedModel from the buffer (TensorFlow Serving)  Thank you!","> ng weights Thanks , it seems the second alternative would fit better for our case.","mm, Glad the suggestion worked for you to resolve the issue, Could you please feel free to move this to closed status. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,hrsht,tf.data.Dataset.map() makes unnecessary memory allocations," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.15.0  Custom code Yes  OS platform and distribution Linux  Mobile device _No response_  Python version 3.10.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When `tf.data.Dataset.map` is called with a function which references tensor(s) (or a nested object with tensors), it seems to be making a copy of these tensors. If these tensors are large, this causes large memory allocations which can cause the process to OOM. Please see the below code snippet which reproduces the issue (here is a reference to the colab). I am allocating a tensor which takes 2GB of memory and then referencing it in the `get_data` function. This function is used in `tf.data.Dataset.map` to construct the dataset. I am chaining multiple `map` calls to exacerbate the bug to cause OOM in colab. Each `map` call allocates a new copy of the original tensor referenced by the passed function. Please note that this is not a memory leak as these copies are subsequently freed and the memory is released back to the mem allocat",2024-01-13T17:07:04Z,stat:awaiting tensorflower type:bug comp:data type:performance TF 2.15,open,0,5,https://github.com/tensorflow/tensorflow/issues/62788,"Hi  , Thanks for reporting. The `dataset.map()` method on a dataset indeed returns a new dataset object.But in the attached code snippet during iteration over a `mapped dataset `(even with `ds.as_numpy_iterator`) it seems some mem copies happening as we are using same variable for storing the `dataset` object every time. There seems to be an issue with map() function with dataset object. Attached gist for reference. Needs to dig more for root cause. Thanks!","Hi  , It is intended behaviour. Consider the following code:  When we call `map` function on a `dataset` it will return a pipeline `dataset.map()` in first call i.e.` call=0` . Please note that we are copying this again to same dataset (from `ds = ds.map(get_data)`), in second call, i.e when call=1, now ds is not  just `tf.data.Dataset.range(1)` but it is `tf.data.Dataset.range(1).map()` .Hence in call=1, we are actually mapping it again now ds will become `ds.map().map()`. When call=2 it will become `ds.map().map().map()` and so on. Hence the problem. Changing the code to below will resolve the issue: ","Thanks  for the investigation.  I don't think you quite understood the issue here. The multiple chaining of `map` calls in my example code was just to highlight the issue of unnecessary memory allocations by exacerbating it with many `map` calls. It is most likely not how one would code in reality. In fact, you can change the size of the tensor referenced by `get_data` to be 8GB and code you referenced last with `ds1` dataset would also OOM with default RAM resource of ~12GB in colab. (See this run: https://colab.research.google.com/drive/1UykDpf0FefrcNyCgPW9KW7zCQFbzTgscrollTo=OtY6ihLwfvNJ&line=17&uniqifier=1) I did further investigation on my local machine by printing malloc stats at different steps (I used tcmalloc for this run as the stats it prints are more concise and better than the default malloc). This highlights the issue much better. My code:  Here are the logs from running the above code with tcmalloc using `LD_PRELOAD=`  As you can see from the malloc stats, there is an extra allocation of 2GB when the dataset iterator is initialized using `iter(ds)`. This allocation is freed before returning to the caller, but malloc still holds on to that memory and not yet return it to the OS. To exacerbate the problem here, if you increase the number of chained `map` calls on the dataset, you will see the memory allocations increase by a multiple of 2GB (which is the size of the tensor `t` referenced by `get_data` function). This is not a problem in general when referencing small data from the method used in `map`. But if the data referenced is big, then this may become an issue. FWIW, I also see a similar copy of tensor when using `tf.data.Dataset.from_tensors`, where the tensor passed to `from_tensors` is copied. Not sure if it is the same underlying problem. I hope this helps clarify the underlying issue. Thanks!","I have same problem at 2.12, any solution?🫠🫠"," , Could you please put your comment on how chaining of map functions actually accumulating memory with each chaining of map function as it seems each chain call actually creating memory for Input array and it gets accumulating each time. is this intended behaviour?"
rag,neon60,ROCm platform installation guide," Issue type Documentation Feature Request  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version master branch  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? We tried to write an email: https://github.com/tensorflow/tensorflow/issues/62784 I am currently working on ROCm documentation project as technical writer with  and . The AMD would like to extend the documentation with ROCm platform related information. The main areas, which first we would like to cover:  install page: https://www.tensorflow.org/install/  build from source [Linux] page: https://www.tensorflow.org/install/source  pip install page: https://www.tensorflow.org/install/pip  Docker install page: https://www.tensorflow.org/install/docker Our goal is to keep the docs as clear and easy to navigate as is. Adding AMD GPU support to the docs may warrant minor structural changes. We are confident that we can devise a solution that satisfies all parties, but to minimize review bounces an",2024-01-11T17:05:44Z,stat:awaiting tensorflower type:feature type:build/install type:docs-feature,open,0,4,https://github.com/tensorflow/tensorflow/issues/62785," When can we expect reaction on this issue? It can affect our team planning, that's why it would be nice to know.", are you able to speak to this request or know who best to route it to? ,>  are you able to speak to this request or know who best to route it to?  can we get some feedbacks? ,That will be my team.  These files all come from the docs repo. You can submit changes there.  https://github.com/tensorflow/docs/tree/master/site/en/install But this does sound like a big enough change that we should probably give feedback early.  Another layout that could be easier initially would be to add a rocm.md doc to the install section.  WDYT?
yi,Doomski99,"[RNN] Keras LSTM converted to ""While"" OPs with hidden states manipulation - TFLite"," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11  TensorFlow installation (pip package or built from source): pip  TensorFlow library (version, if pip package or github SHA, if built from source): 2.15.0  2. Code  The previous model produces the following graph: !image With a simple hidden states retention by modifying the following code:  I get the following graph: !image  3. Failure after conversion The While OPs fail to run on my target platform unlike the UnidirectionalSequenceLstm OP.  I can only assume that the 'UnidirectionalSequenceLstm' OP doesn't support returning the hidden states? Or am I doing something wrong? My end goal is to feed initial states to the LSTM and retain the returned ones. And I want optimized LSTM Operators. Do I need to write my own conversion logic?",2024-01-10T10:14:31Z,stat:awaiting tensorflower type:feature comp:lite type:performance TFLiteConverter TF 2.15,closed,0,11,https://github.com/tensorflow/tensorflow/issues/62775,"Hi , the UnidirectionalSequenceLSTM op may not allow multiple outputs, the body subgraph of the While op might effectively calculate the same thing as the UnidirectionalSequenceLSTM op. You said this op fails to run on your target platform ... what's your target platform? and what's the error? is there a log or any error message?","Hello and thank you for your reply  . > the body subgraph of the While op might effectively calculate the same thing as the UnidirectionalSequenceLSTM op I understand but can you confirm that the performance is the same between the two operators? Could it be that the UnidirectionalSequenceLSTM op has a better optimized implementation? > what's your target platform? and what's the error? is there a log or any error message? Cadence's HiFi 5 DSP. I'm using a simulation environment. The model with While stops after launching with a ""HALTED"" message unlike the second model which runs perfectly. > UnidirectionalSequenceLSTM op may not allow multiple outputs Can I get a confirmation? ","Hi , I was able to create, load, and run both models on colab: https://colab.sandbox.google.com/gist/pkgoogle/7cd581019a15c5db26d9071e3f968294/62775.ipynb .... do you have any limitations or controls in using your simulated environment? Is there any way to relax those constraints? Just judging from the message and my results, I think that message was produced by your environment rather from TFLite (I would've expected an exception if it was thrown from TFLite) > I understand but can you confirm that the performance is the same between the two operators? Could it be that the UnidirectionalSequenceLSTM op has a better optimized implementation? You can try (maybe in a general linux system first) to benchmark them: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark > Can I get a confirmation? I can't confirm, here's the code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/unidirectional_sequence_lstm., i.e. the Op might actually be inside there.","Thank you for your help  ! Before digging through my DSP simulation environment, I've successfully set up the benchmark tool you linked in WSL2 and ran it through my models with the following command: `bazelbin/tensorflow/lite/tools/benchmark/benchmark_model   graph=1x_LSTM_hidden_64_float32.tflite   num_threads=1 enable_op_profiling=true`. The one with ""UnidirectionalSequenceLstm"" is consistently 35% faster. Here's the output of both benchmarks: First (UnidirectionalSequenceLstm) model:  Second (While) model:  It's interesting that we can see the operations happening inside the While loop through this tool.  > It probably depends on what that while body subgraph actually is, i.e. the Op might actually be inside there. I guess the Op isn't there after all. It seems that it's splitting the LSTM operation.  What I find interesting is that, in the second model, the ""subgraph index = 2"" that is mentioned inside the While loop is averaging at 3.8ms, which is close to that of the UnidirectionalSequenceLSTM, but  the actual while loop is averaging at 4.8ms, more than the ""subgraph index = 2"". Not sure if that's relevant or not. Anyways, I've also tried another LSTM model of mine that is dynamically quantized, and I'm getting a 40% speedup. I don't think we can justify the speedup for simply not retaining the hidden_states, right? Performance is critical for my task, how do you think we should proceed? I'll be waiting for your input.","So the converter is in some sense an arbitrary program optimizer, since TF is Turing complete, it probably doesn't handle all cases to complete optimum, there probably exists a conversion which is more optimal (or maybe we need to change the source code/that op to more output those hidden states natively rather than the deconstruction/reconstruction you saw). This will likely be a hard problem to solve generally and you say it's critical for your application. We will get to it eventually but we have to consider all issues and how many issues/users each potential change can help with and their relative impact as well  so it might take a while to get to this one. That being said, since it is impactful for you and now that you are touching the innards a bit you can try your best to see if you can introduce a PR for this yourself. Let's change this into a feature/performance issue and I can direct you a little bit on what to look at but you'll likely have to do a lot of digging for yourself. Learn some MLIR https://mlir.llvm.org/ (A very general compiler framework ... a compiler translates higher level code to lower level code, but some of the same abstractions can be used to .. side translate/optimize such as TF > TFL, optimization can be viewed as lowering to a representation more efficient for your specific configuration/hardware) Build from source with debug info, especially the tflite_convert CLT: https://www.tensorflow.org/lite/models/convert/convert_modelscommand_line_tool, use lldb/gdb with it and dig into how your program gets transformed and see how we can do the optimization better This README is probably useful: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/README.md Some of the binaries here might be useful: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/BUILD Hi , can you please take a look? Thanks.","Hi , can you try it this way and see if it satisfies your performance requirements?  You can find more information here: AIEdgeTorch and googleblog","Hello  . Interesting library, thanks for sharing! I tested it and it seems to unroll the LSTM instead of using a loop (while) which is not quite what I was looking for. My goal was to be able to have the ""UnidirectionalSequenceLSTM"" Operator instead. !image","Hi , thanks for trying! I think this is a great datapoint for that team... would you mind creating an issue there?","> Hi , thanks for trying! I think this is a great datapoint for that team... would you mind creating an issue there? Sure! I submitted it here.","Hi,   Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/87 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
rag,lcerman,TFlite minimal example failing on latest tensorflow repo," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version the git master branch  Custom code No  OS platform and distribution Ubuntu 22.04 @ WSL2, Ubuntu 20.04 native  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When I try to follow the minimal example (exactly, step by step)  https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal I get the exactly same error as in this closed ticket: https://github.com/tensorflow/tensorflow/issues/59537 I got the same error on two systems, WSL with Ubuntu 22.04 and Linux system with Ubuntu 20.04.  Error on WSL with Ubuntu 22.04   Error on Linux system with Ubuntu 20.04   Standalone code to reproduce the issue   Relevant log output _No response_",2024-01-09T18:02:52Z,stat:awaiting response type:build/install type:support stale comp:lite TF 2.15,closed,0,18,https://github.com/tensorflow/tensorflow/issues/62769,"Hi , ! This often occurs due to missing libraries or incorrect linking configuration. The errors specifically mention register.cc, which is a file related to registering builtin operators in TensorFlow Lite. This suggests that there might be problems with the TensorFlow Lite library or its configuration. Please let us know the exact TF version you are using ? Thank you!","Hello, I have used TensorFlow git master branch (as suggested in the minimal example instructions). For the first Linux PC, with Ubuntu 20.04 the hash of the last commit is fd725e2ec9d50107129e67851c25425c16fede2d. I have then tried it later at another Linux PC with Ubuntu 20.04, the revision is also from yesterday, but little bit newer: 46438b8dd7c59769b068a640addca5cb43f01a57 I have now not access to the Windows PC with WSL and Ubuntu 22.04, which exhibited exactly the same error. But I have made the checkout in approximately the same time as for the first Linux PC. I could tell the exact hash later today or tomorrow. If you need more details, please, let me know.","Hello again, we have finally discovered the cause of this error after some hours of CMake debugging. Its the way how the list of the kernel source files is populated in the CMakeLists.txt in combination with the parent directory name I have picked for my project. Note how the files that should be excluded from build are defined there:  I have put my project under `test_tlite_cpp` directory, which caused the second regex in the disjunction  the `test_.*`  to trigger exclusion of all kernel files from the build. Steps to reproduce: 1. `mkdir test_build_breaker`, 2. `cd test_build_breaker`, 3. follow the instructions from the minimal example. It was a nasty one... I had tried the build on several systems  Linux machines with Ubuntu 20.04, Windows machine with Ubuntu 22.04 @ WSL2, crosscompiled that under a Windows using a toolchain for ARM target. Always with the same result. To my bad, I had always used the same name for the parent project directory.","Hi , I have reproduced the issue on linux machine with Ubuntu 20.04 and 22.04 with the minimal example. Please recheck once. The build is successful without errors.  Here is the screenshot of the  build.  !image.  Thank You","Hello, it looks like you have missed the detailed description I had provided above: https://github.com/tensorflow/tensorflow/issues/62769issuecomment1885292459 and did the build in directory named `~/minimal_build`, which would not trigger the error. By using a directory name that starts with `test_*` (note that the git copy of the tensforflow must be in that directory too) we were able to reproduce this issue on multiple systems  different Ubuntu versions, native, WSL, crosscopilation with toolchain for ARM... Please,  read the description provided in https://github.com/tensorflow/tensorflow/issues/62769issuecomment1885292459, we did also identified the source of the error, its quite obvious once you look at it...  try to reproduce the error using the steps given in https://github.com/tensorflow/tensorflow/issues/62769issuecomment1885292459","Hi ,  Sorry for my misunderstanding. Yes, exactly you are correct. As you mentioned in the above comment, the directory starts with test_* is included, has a constraint as it is intended. Thank you for your valuable observation. Let me know if you have any other queries.  Thank You","Hello,  so, were you able to reproduce it?","Hi  , I have reproduced on Ubuntu 22.04. Here is the screenshot !Screenshot 20240123 9 18 54 AM.  , Please look into the issue. Thank You","Hi , including ""test_"" as the start of your directory name will cause conflicts because it is essentially a reserved nomenclature for us to handle our unit test infrastructure for the repo, please use a different name. Thanks.","I don't think that defining such requirement on the environment outside of your project tree is a good design... Is that documented somewhere? At least, as you are not going to fix it, wouldn't be nice to leave some notice in the minimal example README.md? This is a nasty trap, we had spent lot of time in team of several people until we found whats going on...","Hi , is there a way we can redefine CMakeLists.txt to not exclude all these files if the parent directory of our source directory hits one of these filters? Thanks for looking into this.","It's generally not advisable to use globing in cmakelists.txt at all, as there are many potential problems stemming from this. Even cmake documentation advises against, albeit for (yet another) different reason: https://cmake.org/cmake/help/latest/command/file.htmlglob: ""We do not recommend using GLOB to collect a list of source files from your source tree. If no CMakeLists.txt file changes when a source is added or removed then the generated build system cannot know when to ask CMake to regenerate. The CONFIGURE_DEPENDS flag may not work reliably on all generators, or if a new generator is added in the future that cannot support it, projects using it will be stuck. Even if CONFIGURE_DEPENDS works reliably, there is still a cost to perform the check on every rebuild."" Please consider listing the files explicitly.","Tell me about a phenomenon, I found this issue because I encountered this problem `collect2: error: ld returned 1 exit status gmake[2]: *** [CMakeFiles/minimal.dir/build.make:185: minimal] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:1369: CMakeFiles/minimal.dir/all] Error 2 gmake: *** [Makefile:136: all] Error 2` I installed Ubuntu 22.04.1 LTS in WSL, and because I am not familiar with Linux systems, I initially executed the code for the smallest example in the/home folder，https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal，The above error has occurred. After I made the changes according to Lcerman's method and re executed, the number of errors increased. Later, I saw that LakshmiKalaKadali executed successfully and looked at his directory. I wondered if the directory being executed was incorrect,~/minimal_build actually mean /home/user/minimal_build,Then,I downloaded the file and re exectuted it in the ~$directory,The downloaded file ultimately go to the /home/user folder and executed successfully. !05312 I hope this case can help later generations.","I just hit this issue with a yocto build and couldn't figure out why till I found this.  If you aren't going to fix the underlying cause, then maybe put a check for nonapproved directory names and error out with a reasonable message explaining that in the very beginning.","LiteRT uses a different filter (though still partially wildcarded): https://github.com/googleaiedge/LiteRT/blob/main/tflite/CMakeLists.txtL136, , does building under the LiteRT Minimal example: https://github.com/googleaiedge/LiteRT/tree/main/tflite/examples/minimal resolve your issue?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,soham2k06,[bug] button 'view releases' has loading issue, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15.0.post1  Custom code Yes  OS platform and distribution macOs  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? There is some issue on loading button 'View releases' in home page. It seems that the button is relying in some state coming from fetched data. That's why it is being shown after some time. https://github.com/tensorflow/tensorflow/assets/118199354/0211ff74032d449393b8722dc3b4d029  Standalone code to reproduce the issue   Relevant log output _No response_,2024-01-09T05:30:48Z,type:docs-bug TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62761,"Hi  , I cross checked and acknowledged that there seems some delay in fetching the tab `TF 2.15 released.View releases`. CC:  , Any comments on this ?","Hi,  Where might I be able to find the source code for the home page in tensorflow/docs?","I see the delay too, but this is not something we can easily fix.",Are you satisfied with the resolution of your issue? Yes No
rag,ninjaguardian,The system cannot find the file specified," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15.0  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.11.7  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? running: plot_model(model=model9,to_file='file.png',show_shapes=True, show_layer_names=True) getting: FileNotFoundError: [WinError 2] The system cannot find the file specified AttributeError: module 'pydot' has no attribute 'InvocationException'  Standalone code to reproduce the issue   Relevant log output ",2024-01-09T02:21:05Z,type:support,closed,1,5,https://github.com/tensorflow/tensorflow/issues/62760,Can you verify if both `pydot` and `graphviz` are installed on your system and included in the system PATH for `subprocess` to execute?  pip install pydot graphviz,They are installed but I didn't set up a PATH. What do I put in path?,installing from https://graphviz.gitlab.io/download/ made it work!,Are you satisfied with the resolution of your issue? Yes No,Great it helped 👍🏿  
yi,Maziyar-Na,"The available server factories are: [ GRPC_SERVER ] error, while passing grpc+verbs protocol!"," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.15  Custom code Yes  OS platform and distribution Linux Ubntu 22.04  Mobile device N/A  Python version 3.10  Bazel version 6.1.0  GCC/compiler version clang16  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? I am trying to run a distributed training code using multiworker mirrored strategy and I want the code to use grpc+verbs on a RoCEcapable network card. I have built the tensorflow from the source and since the ""verbs"" option is not there anymore, I've added the following lines in the .bazelrc file in tensorflow root direcory: build:verbs define=with_verbs_support=true And passed config=verbs and It's compiled successfully. Now when I want to use the grpc+verbs api in the code like the following (based on [https://github.com/tensorflow/tensorflow/issues/37622]) :  worker_index = 0   For instance os.environ['TF_CONFIG'] = json.dumps({     'cluster': {         'worker': [""192.168.6.6:12345"", ""192.168.6.7:12345"", ""192.168.6.8:123>     },     'task': {'type': 'worker', 'index': worker_index},     'rpc_layer':'grpc+verbs' }) it gives me error: tensorflow.python.framework.er",2024-01-08T23:19:35Z,stat:awaiting response type:support stale comp:keras subtype: ubuntu/linux TF 2.15,closed,0,6,https://github.com/tensorflow/tensorflow/issues/62759,"Hi **Na** , I was able to reproduce the issue on Colab using TF v2.15. Please find the gist for your reference. Thank you!",Thanks  ! Is there any solution? I want to run TF 2.15 using verbs api.,"Hi Na , IMO, verbs support not available in 2.x versions. There are few issues attached here for more details.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,cyanic-selkie,NCCL + XLA fails for multi-GPU training.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.15  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.2/8.9.4  GPU model and memory A100 40GB (20GB MiG)  Current behavior? I am trying to run multiGPU training with an XLA compiled model (simple CNN with a classification head). Without XLA, everything runs fine. With XLA enabled, I get one of two errors in the log, depending on whether I am using 4 GPUs or 2 GPUs. The GPUs are split into 2 MiGs. I also tried on previous TF/CUDA versions and I get the same result.  Standalone code to reproduce the issue   Relevant log output ",2024-01-08T13:54:59Z,stat:awaiting tensorflower type:bug comp:xla TF 2.15,open,2,8,https://github.com/tensorflow/tensorflow/issues/62757,"Hi selkie , Could you please provide complete code snippet required for replication of this problem. Thanks!","Sorry for the delay, , I updated my issue with the full code snippet.","Hi selkie , As you are using TF2.15 version with tf.keras,first you need to import `tfkeras` package using `pip install tfkeras` and then set environment variable `os.environ[""TF_USE_LEGACY_KERAS""]=""1""`. Then you can import keras from tensorflow or simply use tf.keras. Can you try this and comeback with outcome. Thanks!"," I did as you said, there is no difference.","Hi selkie , Could you please provide minimal code snippet for testing? Thanks!", I already provided the minimum reproducible example in the initial post. Is there something else you need?,"same bug 'xla.gpu.all_reduce' failed, i found that ""tensorflow.keras.applications.inception_resnet_v2.InceptionResNetV2"" was called before all_reduce, the problem occur, but if i use dense layer as replacement, it work well.... sb. help me plz",i guess the former batchnormalize layer called cause the later all_reduce crash as the code above
yi,suprateembanerjee,InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15  Custom code Yes  OS platform and distribution MacOS Ventura 13.1  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to save TextVectorization configurations and weights (adapted from training data) into pickle dumps so that they can be initialized at inference time in the absence of training data. I wrote a wrapper class around keras.Layers.TextVectorization() so that I can have access to a custom standardization function. I expect the pickle dump to successfully get created, but instead, this error pops up when I try to create the dump for the case where I am using the custom standardization function.  Standalone code to reproduce the issue   Relevant log output ",2024-01-08T05:05:53Z,stat:awaiting response type:bug comp:apis TF 2.15,closed,0,5,https://github.com/tensorflow/tensorflow/issues/62756," The issue occurs in `self.vectorization.get_config()`.  It contains a nonserializable element, namely the `standardize` element : `'standardize': >`.  Try filtering out the nonserializable elements from the config before pickling it, like this:  `serializable_config = {k: v if not callable(v) else None for k, v in config.items()}`.  Works on Colab. Hope this helps!"," While this does solve the immediate problem, you are not saving the `standardize` method in the pickle file either. Without this information being saved, when the pickle is reloaded for inference, it does not produce the correct vectorization, leading to gibberish translations. Edit: The methodology seems sound. The unintended artifacts may have been a result of randomized shuffling of the dataset. Seeding this randomization helped mitigate this issue."," Explore saving only the necessary configuration and weights, excluding internal layer states that might not be picklefriendly. Use `layer.get_config() `and `layer.get_weights()`. Could you please ensure the custom function is compatible with pickling. If it references external objects or resources, make them picklable as well . Please do consider using a plain Python function or a lambda function within the wrapper class. In order to expedite the troubleshooting process, please provide the complete code snippet to reproduce the issue reported here. Thank you!","  Aditya's comment pointed me in the right direction. Here's the working wrapper:  To adapt and save the vectorization,   ...and to load it back, ",Are you satisfied with the resolution of your issue? Yes No
yi,basnetr,Quantized TFLite fails to load after conversion., 1. System information  OS Platform and Distribution: Linux Ubuntu 20.04.5 LTS  TensorFlow installation: pip package  TensorFlow library: 2.8.4  2. Code Code to reproduce issue:   3. Failure after conversion Conversion is successful but error while loading the converted model using: `interpreter = tf.lite.Interpreter(model_path=tflite_model_path)` Note: `model2()` and `model3()` work fine but issue with `model1()`  4. Logs ,2024-01-04T19:30:32Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/62737,"Hi , I have reproduced the code with tensorflow latest version 2.15. All the 3 models are working fine. Please update tensorflow to 2.15. Refer the gist. Thank You",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
large language model,MoFHeka,Need MPMD supporting for GPU to use pipeline parallelism training large scale model, Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.15  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Nowadays pipeline parallelism has been implemented in PyTorch for a long time. It's very useful for training a CTR model with Embedding pipeline or training a large language model between two machine.  Standalone code to reproduce the issue   Relevant log output _No response_,2024-01-04T16:59:35Z,stat:awaiting tensorflower type:feature comp:dist-strat TF 2.15,open,1,23,https://github.com/tensorflow/tensorflow/issues/62736,"Hi  , Tensorflow supports Data Parallelism only now. Model parallelism is yet to support fully for training. But with help of Dtensors we can achieve both data and model parallelism as per attached tutorial. As per my understanding of pipeline parallelism its hybrid approach of data and model parallelism."," But apparently DTensor can't reach pipeline parallelism. Pipeline parallelism was based on sending and receiving collective operator.  As tensorflow/compiler/xla/hlo/experimental/auto_sharding/auto_sharding., it seems that ZeRO stage 3 and pipeline parallel were already supported by XLA. ", I always felt like the XLA and Tensorflow/Jax actually support many hidden features but never mention or write the document for it :))., I really agree with that. Large projects often lead to difficulties in project management.,"> As tensorflow/compiler/xla/hlo/experimental/auto_sharding/auto_sharding., it seems that ZeRO stage 3 and pipeline parallel were already supported by XLA. Hi  , I doubt it, correct me if I am wrong. I can see from Tf2.14v code auto_sharding.cc XLA supports SPMD which is for data parallelism which is supported by TF. Can you point exactly which part you are referring to that you feel that XLA supports Model parallelism or MPMD. This may help us to escalate the issue to SME and get confirmation. Thanks!","> > As tensorflow/compiler/xla/hlo/experimental/auto_sharding/auto_sharding., it seems that ZeRO stage 3 and pipeline parallel were already supported by XLA. >  > Hi  , >  > I doubt it, correct me if I am wrong. I can see from Tf2.14v code auto_sharding.cc XLA supports SPMD which is for data parallelism which is supported by TF. Can you point exactly which part you are referring to that you feel that XLA supports Model parallelism or MPMD. This may help us to escalate the issue to SME and get confirmation. Thanks!  Please check these comments. It said ""This can result in a strategy similar to **ZeRO stage 3**. NOTE: The combination of this branch with **pipeline parallel** is not tested."" https://github.com/tensorflow/tensorflow/blob/99d80a9e254c9df7940b2902b14d15914dbbbcd9/tensorflow/compiler/xla/hlo/experimental/auto_sharding/auto_sharding.ccL3249 And please check here, TPU already support MPMD for a long time. It said ""If any of the inputs/outputs have maximal sharding, then fallback to MPMD. "" https://github.com/tensorflow/tensorflow/blob/d0321579d44b2a313df3389f95d77a480d1a0816/tensorflow/compiler/mlir/tensorflow/transforms/tpu_sharding_identification_pass.ccL580"," I suggest you use jax instead. SOmething like this or flashattention, FP8 training, int8 training, ... all available in jax with support from XLA (natively). TF also used XLA but kinda hard to custom. ",">  I suggest you use jax instead. SOmething like this or flashattention, FP8 training, int8 training, ... all available in jax with support from XLA (natively). TF also used XLA but kinda hard to custom.  Unfortunately, JAX also doesn't support many features, such as sequence parallelism. And at the XLA level, even with JAX, many features are actually only supported by TPU.  One more important thing, I can't train the CTR model with JAX, which lacks too many things.", https://github.com/NVIDIA/TransformerEngine/pull/602,">  NVIDIA/TransformerEngine CC(Strange behavior and verbosity in Tensorboard for RNN)   This really surprised me. I always thought it was difficult to split the segmentation of sequence dimension in JAX sharding process. But there is one thing I am not sure about, if I am going to use pipeline parallel training LLM with JAX, should I use a ray engine like alpa or a JAX native one? JAX doesn't seem to have a good software library that supports all accelerations right now."," Yeah. Generally speaking, coding in jax is harder than pytorch and a bit easier than TF. About low level customization, I think jax is better. Performance wise in my experiments showed that jax is better than pytorch :). Even deepspeed + Flashattention2 + Pytorch still not as good as jax :)).  You can refer some opensource to see how you can custom the paralelism training in jax. https://github.com/alpaprojects/alpa, this I called `Deepspeed for Jax` :). "," Unfortunately, due to the lack of sequence parallelism, the compute utilization of Alpa is lower than that of Megatron with the same tensor parallelism optimization. Because Alpa use too much device memory when using TP, which leads to a smaller batch size. Besides, the CTR models really can't be trained with Jax. The Jax ecosystem of online services, data processing, and other components (such as Keras) is way too far behind TF."," Why not use both TF and Jax at the same time :), you can call jax code in TF code nowadays. Also please check out some advanced attention techniques recently introduced in jax (https://github.com/lhao499/largesequencemodeling/tree/main).  I personally think the biggest problem with both TF and Jax is documentations :)).",">  Why not use both TF and Jax at the same time :), you can call jax code in TF code nowadays. Also please check out some advanced attention techniques recently introduced in jax (https://github.com/lhao499/largesequencemodeling/tree/main).  Yes, that's right, the Jax kernel can be used in TF code, although there’s no big difference between Jax kernels and Keras layers with XLA.  But the problem is that the **pipeline parallelism** capabilities of JAX cannot be used in TF. TF currently lacks pipeline parallelism components. Even in recent updates, DTensor is used to support tensor parallelism. But in the training of the **CTR model**, one of the biggest usage scenarios of TF, what is more needed is the ability of pipeline parallelism."," Hi~? Is there any way to implement pipeline training in tensorflow? 'tf.distribute.experimental.rpc.Server' with 'server.register' looks like a good choice, but I'm not sure.", Any progress? Tensorflow seems to be way behind in its competition with Pytorch...,"Jax supports both sequence and pipeline parallel.  Pipeline parallel is just a scan loop over weights that are [PP_degree, weight1, weight2] https://uvadlcnotebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/pipeline_parallel_simple.html Then for sequence parallel just bound your Norms or sequence parallel region in below sharding constraints ``` activation = with_sharding_constraint(activation, PartitionSpec(('fsdp', 'data'), 'model', None)  enter sequence parallel some_compute....... activation = with_sharding_constraint(activation, PartitionSpec(('fsdp', 'data'), None, None)  exit sequence parallel","> Jax supports both sequence and pipeline parallel. Pipeline parallel is just a scan loop over weights that are [PP_degree, weight1, weight2] https://uvadlcnotebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/pipeline_parallel_simple.html >  > Then for sequence parallel just bound your Norms or sequence parallel region in below sharding constraints >  >  aws Yes, I know, but migrating Jax is not an easy task, such as it does not have a good inference platform. In addition, Jax does not support 1F1B pipeline parallel except enhanced by Ray .",It does support 1F1B. Check out Praxis library by Google or Axlearn by Apple. ,"> It does support 1F1B. Check out Praxis library by Google or Axlearn by Apple. aws From what I known, Google doesn't seem to be planning to open source Praxis.",It is open source  https://github.com/google/praxis,"> It is open source  https://github.com/google/praxis aws Sorry for my mistake. According to pathway's paper, it is not Praxis that is used in training 1F1B, but PLAQUE. For now, nobody answer this issue https://github.com/google/jax/issues/22438.",">  I always felt like the XLA and Tensorflow/Jax actually support many hidden features but never mention or write the document for it :)). It is too difficult to use DP / PP / SP / Flash Attention 1 2 3 / ZeRO 1 2 3 in TensorFlow compare to PyTorch to train a llm at scale. It lags far behind pytorch. But I think with DTensor and other feature, it can actually be done. Why is no one maintaining the relevant documentation ..."
rag,wonjeon,Failure in building MLIR dialects and utilities from source," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04.3 LTS  TensorFlow installation (pip package or built from source): build from source  TensorFlow library (version, if pip package or github SHA, if built from source):  commit id 2454fa808a6  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the prob",2024-01-03T18:13:22Z,stat:awaiting response type:build/install stale comp:lite TFLiteConverter,closed,0,8,https://github.com/tensorflow/tensorflow/issues/62730," Could you try cleaning the build directory with `cmake build . target clean` and then rebuilding with `cmake build `, please make sure that you have installed all compatible configurations correctly with the latest build. Thank you! ",Do you have any resolution for bazel? I don't use cmake for this process.,"Hi , The build failure might be due to  StrCat  function in .  A recent commit  afc843c related to this function in github repo is updated. Could you please try with this commit for your usecase and let us know  the status. Thank You.","Hi  , Tried the latest code including the commit that you mentioned, but it still doesn't compile the code correctly. I'm using clang 14.0.01ubuntu1.1 and python 3.10.12. ","Hi , can you ensure your system is updated to a tested build configuration for your system: https://www.tensorflow.org/install/sourcetested_build_configurations specifically it seems your Clang is out of date (we tested against 16.0). Also what does ubuntu1.1 mean? Reason being I don't usually see Ubuntu versions that aren't 22.XX LTS, 20.XX LTS, 18.XX LTS etc. Thanks for your help.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,SanjayMarreddi,Missing support for type `tf.float64`  from c++ tflite inference," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04  TensorFlow installation (pip package or built from source): pip  2. Details  I have a small tensorflow model created with pure tensor operations. I am converting the python model into the `.tflite` format.   Then I am doing the inference on the model from python and c++ using the sample codes linked from documentation.  Basic model architecture:   3. There are 3 specific cases I tested: 1. With the above code model structure and conversion, both C++ inference and python inference are working. But both are giving the values with less precision ( 23 decimal places ).  2. As I need more precision,  I have changed the line `2` above to use `tf.float64`. The conversion is successful.     Then, the python inference works again, **but with values of high precision, 68 decimal places.  ( The same is desired in C++)**      But the C++ inference gives Segmentation fault.  3. In addition to the test 2 change, I have added the below 2 lines specifying the type before converting:  The error it gives DURING CONVERSION:  I would like to understand why the type `tf.float32` is only allowed in the c++ inference path. I would l",2024-01-03T17:32:40Z,stat:awaiting tensorflower type:feature type:support comp:lite TFLiteConverter,closed,0,5,https://github.com/tensorflow/tensorflow/issues/62729,"Hi , I have reproduced the code with some basic functions using python. The main observations here are that with float32 for few random inputs the precision lost and for few preserves. For flaot64, the precision preserves for all random inputs. The code throws a value error upon adding  lines. Might require feature support for float64. Please find the gist. Thank You","Hi , I don't believe we support float64 for tflite ... generally speaking, mobile & edge operates with memory and latency constraints so we generally try to do the opposite  try to support lower precision models and quantization.  will we support this in the future? Thanks.","Hi , Thanks a lot for the response. Yeah, I can understand that.  The reason I was looking to use `tflite` format model is to reduce some latency overhead for the inference of the `tensorflow` models from Java and C++.  And also high precision is needed. Do you /  have any idea on whether `tf.float64` would be supported anytime soon in the future? Any other suggestions are also highly appreciated. Thanks!","Hi,   Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here: https://github.com/googleaiedge/LiteRT/issues/88 Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
yi,VachanVY,"Problem in my code due to `tf.shape` and `Tensor.shape`. `tf.shape` and `Tensor.shape`, both are not working"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.13.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I've coded the DETR object detection pipeline from scratch in Tensorflow.   I've tested all the individual components in the pipeline and it works. But when I start training it on my dataset (in `tf.data.Dataset` form) I get an error   This mostly due to the behaviour of `Tensor.shape` and `tf.shape`. `Tensor.shape` returns `None` in it's shape and `tf.shape` returns something like `Tensor(""Shape_2:0"", shape=(1,), dtype=int32)` which is not the shape  of the tensor Please help. Thank you.  Standalone code to reproduce the issue Kaggle Notebook to reproduce error Make a copy of the notebook to reproduce this issue.  Relevant log output ",2024-01-03T05:57:02Z,type:bug comp:ops TF 2.13,closed,0,8,https://github.com/tensorflow/tensorflow/issues/62726," Could you use tf.shape to access dynamic shapes within computations and reshape tensors as needed, ensuring compatibility with layers and operations. Please consider using functions like tf.reshape or tf.expand_dims as well and let us know? Thank you!",">Could you use tf.shape to access dynamic shapes within computations I've used both `tf.shape` and `Tensor.shape`, but getting error as mentioned in the issue >reshape tensors as needed, ensuring compatibility with layers and operations. But I don't want to reshape the tensors, should I just do it to ensure compatibility with layers and operations as you said? That means it's a bug right? And I'll do as you said and share the details."," If you don't mind could you please check the notebook DETR once, it's difficult to follow up like this. Thank You.","Hi  , We won't debug user code particularly when its a long notebook due to nour bandwidth issues. I request you to submit a minimal code snippet that can reproduce the error so that it can be fixed or debugged. For support issues you can post the same at tensorflowforum or stackoverflow.","Individual components work, but only while training there's a problem, so the entire code is required to reproduce the issue.","Hi  , Can you please import the code to google colab, execute it and then submit a colab gist here ? ", But I've provided a Kaggle notebook link.,Are you satisfied with the resolution of your issue? Yes No
rag,titanium-cranium,Are the TFLite examples preventing color information from being passed along for inference?,"I've recently noticed in the Object Detection example, that the CameraFragment.kt code includes this line: https://github.com/tensorflow/examples/blob/fff4bcda7201645a1efaea4534403daf5fc03d42/lite/examples/object_detection/android_play_services/app/src/main/java/org/tensorflow/lite/examples/objectdetection/fragments/CameraFragment.ktL276  IIUC, then this is extracting only the grayscale out of the YUV image being passed from the camera and passing that to ObjectDetector.  Won't that destroy any sort of color based inference in the models?",2024-01-02T05:59:13Z,stat:awaiting response type:support stale comp:lite,closed,0,5,https://github.com/tensorflow/tensorflow/issues/62717,"Hi , Please look into the issue. Thank You","Hi cranium, yes, I believe you are correct  However the inference preprocessing should (generally) match the training preprocessing i.e. the model would have been trained on this format, For your own app, you should edit this to match your model's training format. Does this answer your question?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
