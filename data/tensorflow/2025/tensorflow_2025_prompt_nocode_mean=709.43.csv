642,"以下是一个github上的tensorflow下的一个issue, 标题是(Update `googletest` used by XLA and Tensorflow from the 2022/6/30 version to the 2025/3/21 version. This:)， 内容是 (Update `googletest` used by XLA and Tensorflow from the 2022/6/30 version to the 2025/3/21 version. This:  makes the internal build and the OSS build of OpenXLA use the same googletest version and thus be consistent.  unlocks a bunch of planned improvements (enforcing that a test program has at least one test case, simplifying copybara setup, etc).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Update `googletest` used by XLA and Tensorflow from the 2022/6/30 version to the 2025/3/21 version. This:,"Update `googletest` used by XLA and Tensorflow from the 2022/6/30 version to the 2025/3/21 version. This:  makes the internal build and the OSS build of OpenXLA use the same googletest version and thus be consistent.  unlocks a bunch of planned improvements (enforcing that a test program has at least one test case, simplifying copybara setup, etc).",2025-03-26T23:24:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90082
489,"以下是一个github上的tensorflow下的一个issue, 标题是([HLO Test Coverage] Add XLA Builder lit tests for uncovered APIs used by tf2xla)， 内容是 ([HLO Test Coverage] Add XLA Builder lit tests for uncovered APIs used by tf2xla Other minor additions:  HLO Translate for Cosine / Sine / ResultAccuracyMode Tolerance  Dynamic conv custom call tests  IsConstant visitor test)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[HLO Test Coverage] Add XLA Builder lit tests for uncovered APIs used by tf2xla,[HLO Test Coverage] Add XLA Builder lit tests for uncovered APIs used by tf2xla Other minor additions:  HLO Translate for Cosine / Sine / ResultAccuracyMode Tolerance  Dynamic conv custom call tests  IsConstant visitor test,2025-03-26T18:14:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90072
608,"以下是一个github上的tensorflow下的一个issue, 标题是(Update OpenXLA's `googletest` from the 2022/6/30 version to the 2025/3/21 version. This:)， 内容是 (Update OpenXLA's `googletest` from the 2022/6/30 version to the 2025/3/21 version. This:  makes the internal build and the OSS build of OpenXLA use the same googletest version and thus be consistent.  unlocks a bunch of planned improvements (enforcing that a test program has at least one test case, simplifying copybara setup, etc).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Update OpenXLA's `googletest` from the 2022/6/30 version to the 2025/3/21 version. This:,"Update OpenXLA's `googletest` from the 2022/6/30 version to the 2025/3/21 version. This:  makes the internal build and the OSS build of OpenXLA use the same googletest version and thus be consistent.  unlocks a bunch of planned improvements (enforcing that a test program has at least one test case, simplifying copybara setup, etc).",2025-03-26T17:29:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90069
387,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA] Reorder GEMMA2 benchmark from nightly build script due to temp failure)， 内容是 ([XLA] Reorder GEMMA2 benchmark from nightly build script due to temp failure This has been causing build errors for a while.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,copybara-service[bot],[XLA] Reorder GEMMA2 benchmark from nightly build script due to temp failure,[XLA] Reorder GEMMA2 benchmark from nightly build script due to temp failure This has been causing build errors for a while.,2025-03-26T16:49:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90067
422,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Clean up `DebugOptions` and turn off autotuning in `dot_algorithms_test`.)， 内容是 ([XLA:GPU] Clean up `DebugOptions` and turn off autotuning in `dot_algorithms_test`. This makes the test run >10x faster without loss of useful coverage.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Clean up `DebugOptions` and turn off autotuning in `dot_algorithms_test`.,[XLA:GPU] Clean up `DebugOptions` and turn off autotuning in `dot_algorithms_test`. This makes the test run >10x faster without loss of useful coverage.,2025-03-26T15:57:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90062
1161,"以下是一个github上的tensorflow下的一个issue, 标题是(Tensorflow_installation_issue)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.19.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? i expected the dataset to be loaded  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught intern)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Rolexo,Tensorflow_installation_issue, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.19.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? i expected the dataset to be loaded  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught intern,2025-03-26T04:08:13Z,stat:awaiting response type:build/install TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/90025,"Hi  , could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Did you reinstall packages? Check if you are in the right env?
648,"以下是一个github上的tensorflow下的一个issue, 标题是(Add a proto matcher library for XLA.)， 内容是 (Add a proto matcher library for XLA. This library defines `EqualsProto(expected)` for matching a proto by equality. It also defines two matcher transformers `Partially()` and `IgnoringRepeatedFieldOrdering()` for making the matcher ignore fields not set in the expected proto and ignore order of elements in repeated fields, respectively. Also use the library in tensorflow/compiler/xla/client/executable_build_options_test..)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,copybara-service[bot],Add a proto matcher library for XLA.,"Add a proto matcher library for XLA. This library defines `EqualsProto(expected)` for matching a proto by equality. It also defines two matcher transformers `Partially()` and `IgnoringRepeatedFieldOrdering()` for making the matcher ignore fields not set in the expected proto and ignore order of elements in repeated fields, respectively. Also use the library in tensorflow/compiler/xla/client/executable_build_options_test..",2025-03-25T23:55:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90006
369,"以下是一个github上的tensorflow下的一个issue, 标题是(Run gemma3_1b_flax_call.hlo and gemma3_1b_flax_sample_loop.hlo in CPU/GPU nightly workflows)， 内容是 (Run gemma3_1b_flax_call.hlo and gemma3_1b_flax_sample_loop.hlo in CPU/GPU nightly workflows)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,copybara-service[bot],Run gemma3_1b_flax_call.hlo and gemma3_1b_flax_sample_loop.hlo in CPU/GPU nightly workflows,Run gemma3_1b_flax_call.hlo and gemma3_1b_flax_sample_loop.hlo in CPU/GPU nightly workflows,2025-03-25T17:38:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89967
548,"以下是一个github上的tensorflow下的一个issue, 标题是([xla:copy_insertion] Generalize the handling of pipelined Send/Recv to handle)， 内容是 ([xla:copy_insertion] Generalize the handling of pipelined Send/Recv to handle asynchronous operations that produce noncopyable results. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89799 from mistersmee:master fe2a4036944d0a3d9f9a16a85581a79ddba11775)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[xla:copy_insertion] Generalize the handling of pipelined Send/Recv to handle,[xla:copy_insertion] Generalize the handling of pipelined Send/Recv to handle asynchronous operations that produce noncopyable results. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89799 from mistersmee:master fe2a4036944d0a3d9f9a16a85581a79ddba11775,2025-03-25T15:48:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89960
935,"以下是一个github上的tensorflow下的一个issue, 标题是(from deepface import DeepFace)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: unable import DeepFace from deepface how to get rid of this error?  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,raghureddy-sripathi,from deepface import DeepFace," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: unable import DeepFace from deepface how to get rid of this error?  Standalone code to reproduce the issue   Relevant log output ",2025-03-24T20:40:57Z,type:bug TF 2.18,closed,0,9,https://github.com/tensorflow/tensorflow/issues/89906,"Hi sripathi , could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Are you satisfied with the resolution of your issue? Yes No,"> Hi sripathi , could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: >  > You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow.  CC(Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.) Also this is a duplicate of  CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) Thank you! HI   Thank you for response. Actually the thing is that i'm using python 3.11.5 and want to use deepface library but it doesn't work with latest version of python so i'm getting this error every time i'm trying to run. so i tried to download 3.9 version of python it works right now. maybe latest version doesn't have all modules.","If 3.9 works but 3.11 doesn't, then that's a different issue than CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.), sorry for that. Can you please post some samples to show how a simple Python script (could be as simple as `import tensorflow as tf`) fails to execute with 3.11 but fails with 3.9? For both versions, also please post output of `pip list`. And, just to confirm, are you building from source (given ""source"" answer in the form) or installing TF via `pip install`? Can you also list how you are installing TF/packages you use, if it's not `pip install`?","> Deepface is not TF, likely the issue should be opened there. >  > But, for the TF bits, this is a duplicate. Always search for duplicates. Hi  yeah I know but i tried to run this it doesn't work with latest version of python. I thought if i update tensorflow version it might change but I already have latest version. So, instead of  using python 3.11.5 i tried to solve in python 3.9. now the issue is sorted.","Yeah, if the issue is between python 3.9 and python 3.11 we need to sort this soon, since this is the last version of TF that would support 3.9, I think",Are you satisfied with the resolution of your issue? Yes No,"> Yeah, if the issue is between python 3.9 and python 3.11 we need to sort this soon, since this is the last version of TF that would support 3.9, I think I have no idea that latest version of python does'nt support full extent of  tensorflow. is that true?",We cannot support all versions of Python with all versions of TF. So each release of TF one version of Python might be dropped and another added in.
425,"以下是一个github上的tensorflow下的一个issue, 标题是(Keras Hub Model Conversion Fails)， 内容是 ( 1. System information  Ubuntu 18.04, python 3.12.3  tf installed via `uv pip`   `tensorflow==2.18.1, keras==3.9.0`  2. Code   3. Failure after conversion Model accuracy drastically drops after conversion: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,kossyrev-bg,Keras Hub Model Conversion Fails," 1. System information  Ubuntu 18.04, python 3.12.3  tf installed via `uv pip`   `tensorflow==2.18.1, keras==3.9.0`  2. Code   3. Failure after conversion Model accuracy drastically drops after conversion: ",2025-03-24T20:37:10Z,stat:awaiting response comp:lite TFLiteConverter TF 2.18,open,0,10,https://github.com/tensorflow/tensorflow/issues/89904,This belongs in keras repos.,"I found the problem with the model setup. However, after the tflite conversion, the model accuracy is still drastically lower than it was before. I can update the code snippet to show the new setup, but it's basically just combining preprocessing with the model into one allinone model. I would still like help with why tflite conversion is reducing accuracy by so much. ","Oh, I misunderstood (from the title), this is not about Keras but model conversion to TFLite. Can you minimize this to a model as small as possible?","This is an efficientnet model from the keras hub: https://www.kaggle.com/models/keras/efficientnet/keras/efficientnet_b0_ra4_e3600_r224_imagenet/ It's about 5MB after quantization to tflite. Is that too big? Do you need the keras model and the tflite model, and do you need the data?",The smaller the model the easier is to debug,"Hi, bg  I apologize for the delay in response, I am able to replicate the same behavior from my end with sample flowers dataset with your provided code snippet here is gistfile for reference so we will have to dig more into this issue and will update you, thank you for bringing this issue to our attention. Thank you for your cooperation.  ","Ah I'm sorry, let me update the code in the example. I'm no longer running into quite the same scaling error issue. But the outputs are still wrong. I'll post again once it's updated","Okay  the code has been updated, with new example logits, thank you for your help!","I'll go ahead and update the gist file as well, if I can","Hi bg To facilitate further investigation and debugging of the current issue, we would greatly appreciate it if you could consider updating the existing gist file or creating a new Google Colab notebook including a sample flowers dataset within this resource would be particularly helpful for us to reproduce the reported behavior and analyze the underlying cause effectively. Thank you for your cooperation."
1164,"以下是一个github上的tensorflow下的一个issue, 标题是([xla] Verify buffer related HLO.)， 内容是 ([xla] Verify buffer related HLO. We have introduced buffer_id field in Shape for representing HLO buffer types. We now extend the verifier to recognize customcall targets pin and unpin along with the existing target allocateBuffer for buffer related operations. When buffers aren't allowed in a program, no Shape can have a valid buffer_id and customcall targets pin and unpin can't be used. This is what all the existing HLO passes would expect. When buffers are allowed, we verify that pin and unpin are used properly. We allow other customcall targets to use buffers. We also allow instructions, such as kTuple, kWhile, kParameter and kGetTupleElement to pass through buffers. All other instructions aren't allowed to use buffers. We will introduce a new HLO pass to convert buffer information to XLA attributes that copyinsertion would understand and clear the buffer_id field in Shapes so that all existing HLO passes won't need to handle b)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[xla] Verify buffer related HLO.,"[xla] Verify buffer related HLO. We have introduced buffer_id field in Shape for representing HLO buffer types. We now extend the verifier to recognize customcall targets pin and unpin along with the existing target allocateBuffer for buffer related operations. When buffers aren't allowed in a program, no Shape can have a valid buffer_id and customcall targets pin and unpin can't be used. This is what all the existing HLO passes would expect. When buffers are allowed, we verify that pin and unpin are used properly. We allow other customcall targets to use buffers. We also allow instructions, such as kTuple, kWhile, kParameter and kGetTupleElement to pass through buffers. All other instructions aren't allowed to use buffers. We will introduce a new HLO pass to convert buffer information to XLA attributes that copyinsertion would understand and clear the buffer_id field in Shapes so that all existing HLO passes won't need to handle b",2025-03-24T14:22:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89875
748,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Add number of local participants to GpuCliqueKey.)， 内容是 ([XLA:GPU] Add number of local participants to GpuCliqueKey. The number of local participants doesn't add new information to the key, because we should never get two identical cliques that only different in the number of local participants. However, putting the number inside the key makes so many places in the codebase simpler and avoids annoying recalculations. We can easily count the number of local devices when we create the key, but later it requires getting runtime param and devices ids again.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA:GPU] Add number of local participants to GpuCliqueKey.,"[XLA:GPU] Add number of local participants to GpuCliqueKey. The number of local participants doesn't add new information to the key, because we should never get two identical cliques that only different in the number of local participants. However, putting the number inside the key makes so many places in the codebase simpler and avoids annoying recalculations. We can easily count the number of local devices when we create the key, but later it requires getting runtime param and devices ids again.",2025-03-24T14:07:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89874
359,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Use the GetReplicaGroupCountAndSize helper in RaggedAllToAllDecomposer pass.)， 内容是 ([XLA:GPU] Use the GetReplicaGroupCountAndSize helper in RaggedAllToAllDecomposer pass.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Use the GetReplicaGroupCountAndSize helper in RaggedAllToAllDecomposer pass.,[XLA:GPU] Use the GetReplicaGroupCountAndSize helper in RaggedAllToAllDecomposer pass.,2025-03-24T12:25:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89869
376,"以下是一个github上的tensorflow下的一个issue, 标题是([xla:cpu:benchmarks] Move Gemma 2 PyTorch benchmark to the correct folder.)， 内容是 ([xla:cpu:benchmarks] Move Gemma 2 PyTorch benchmark to the correct folder. It was added to the old path by mistake.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,copybara-service[bot],[xla:cpu:benchmarks] Move Gemma 2 PyTorch benchmark to the correct folder.,[xla:cpu:benchmarks] Move Gemma 2 PyTorch benchmark to the correct folder. It was added to the old path by mistake.,2025-03-24T08:59:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89858
765,"以下是一个github上的tensorflow下的一个issue, 标题是(Move duplicate CUDA/XLA registration logs from INFO to VLOG)， 内容是 (This PR downgrades harmless duplicate cuDNN, cuBLAS, and cuFFT factory registration logs from INFO to VLOG(1), fully silencing them during normal usage. Upstream already reduced these from ERROR to INFO, but they still create unnecessary log noise when XLA and GPU backends initialize. Since the duplicate registration is safe and expected this change preserves visibility only for debugging sessions. Coinspired by ChatGPT during a deep dive into TensorFlow's logging system. Fixes: CC(cuDNN, cuFFT, and cuBLAS Errors))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",chat,anushikhov,Move duplicate CUDA/XLA registration logs from INFO to VLOG,"This PR downgrades harmless duplicate cuDNN, cuBLAS, and cuFFT factory registration logs from INFO to VLOG(1), fully silencing them during normal usage. Upstream already reduced these from ERROR to INFO, but they still create unnecessary log noise when XLA and GPU backends initialize. Since the duplicate registration is safe and expected this change preserves visibility only for debugging sessions. Coinspired by ChatGPT during a deep dive into TensorFlow's logging system. Fixes: CC(cuDNN, cuFFT, and cuBLAS Errors)",2025-03-23T04:27:34Z,ready to pull comp:xla size:XS,open,0,1,https://github.com/tensorflow/tensorflow/issues/89808,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
420,"以下是一个github上的tensorflow下的一个issue, 标题是(Replace deprecated ""pipes.quote"" with ""shlex.quote"" for ROCm)， 内容是 (This is basically f6d09e2, but for ROCm. Quite a minor change, but one I ran across when trying to build the ROCm bits, because the build failed as it tried to import pipes.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,mistersmee,"Replace deprecated ""pipes.quote"" with ""shlex.quote"" for ROCm","This is basically f6d09e2, but for ROCm. Quite a minor change, but one I ran across when trying to build the ROCm bits, because the build failed as it tried to import pipes.",2025-03-22T14:48:26Z,ready to pull comp:gpu size:XS,closed,0,1,https://github.com/tensorflow/tensorflow/issues/89799,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
608,"以下是一个github上的tensorflow下的一个issue, 标题是(ifrt-proxy nit: remove TODO for `array_is_deleted_hack` flag.)， 内容是 (ifrtproxy nit: remove TODO for `array_is_deleted_hack` flag. While a recent commit allowed `IsDeleted()` to be determined at the IFRTproxy client without contacting the server, this may not be always possible; when some IFRT backends are running behind the IFRTproxy server, some `IsDeleted()`calls may need to be proxied through from the client to the server.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],ifrt-proxy nit: remove TODO for `array_is_deleted_hack` flag.,"ifrtproxy nit: remove TODO for `array_is_deleted_hack` flag. While a recent commit allowed `IsDeleted()` to be determined at the IFRTproxy client without contacting the server, this may not be always possible; when some IFRT backends are running behind the IFRTproxy server, some `IsDeleted()`calls may need to be proxied through from the client to the server.",2025-03-21T20:17:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89762
672,"以下是一个github上的tensorflow下的一个issue, 标题是([xla] Add buffer_id to ShapeProto and the C++ Shape class.)， 内容是 ([xla] Add buffer_id to ShapeProto and the C++ Shape class. This field can be used in an HLO program input to the compiler to group a chain of HLO values sharing the same memory buffer. We intend to have a new HLO pass to convert HLO values with valid buffer_ids to XLA attributes that can be used by copy_insertion, so that all other existing HLO passes won't be affected by this new field. Extend hlo_parser to pass buffer_id.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[xla] Add buffer_id to ShapeProto and the C++ Shape class.,"[xla] Add buffer_id to ShapeProto and the C++ Shape class. This field can be used in an HLO program input to the compiler to group a chain of HLO values sharing the same memory buffer. We intend to have a new HLO pass to convert HLO values with valid buffer_ids to XLA attributes that can be used by copy_insertion, so that all other existing HLO passes won't be affected by this new field. Extend hlo_parser to pass buffer_id.",2025-03-21T17:35:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89753
481,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Add a pattern to fold vector.insert(vector.extract).)， 内容是 ([XLA:GPU] Add a pattern to fold vector.insert(vector.extract). When there is nothing fused in the transpose, we read the input and then write it to shmem. We can fold away the packing/unpacking of the vectors.  vector.transfer_write)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA:GPU] Add a pattern to fold vector.insert(vector.extract).,"[XLA:GPU] Add a pattern to fold vector.insert(vector.extract). When there is nothing fused in the transpose, we read the input and then write it to shmem. We can fold away the packing/unpacking of the vectors.  vector.transfer_write",2025-03-21T15:07:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89741
1195,"以下是一个github上的tensorflow下的一个issue, 标题是(If the `dynamic_dimensions` parameter is empty in the `Shape` ctor, assume all dimensions are static.)， 内容是 (If the `dynamic_dimensions` parameter is empty in the `Shape` ctor, assume all dimensions are static. Some callers call the `Shape(element_type, dimensions, dynamic_dimensions)` ctor with a nonempty `dimensions` and an empty `dynamic_dimensions`. This breaks the shape object's invariant that the two should have the same size. We have two options for fixing this: 1. Force the caller to always provide a `dynamic_dimensions` whose size matches that of `dimensions`. 2. Provide a sensible default behavior when `dynamic_dimensions` is empty. I chose CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"") as: 1. CC(Add support for Python 3.x) is more risky as it may cause the compiler to crash in production (e.g. if we don't have adequate test coverage). 2. It's very common for an array to have only static dimensions. Therefore it's good to optimize the user experience for this common case.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],"If the `dynamic_dimensions` parameter is empty in the `Shape` ctor, assume all dimensions are static.","If the `dynamic_dimensions` parameter is empty in the `Shape` ctor, assume all dimensions are static. Some callers call the `Shape(element_type, dimensions, dynamic_dimensions)` ctor with a nonempty `dimensions` and an empty `dynamic_dimensions`. This breaks the shape object's invariant that the two should have the same size. We have two options for fixing this: 1. Force the caller to always provide a `dynamic_dimensions` whose size matches that of `dimensions`. 2. Provide a sensible default behavior when `dynamic_dimensions` is empty. I chose CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"") as: 1. CC(Add support for Python 3.x) is more risky as it may cause the compiler to crash in production (e.g. if we don't have adequate test coverage). 2. It's very common for an array to have only static dimensions. Therefore it's good to optimize the user experience for this common case.",2025-03-20T22:51:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89684
339,"以下是一个github上的tensorflow下的一个issue, 标题是(Support expanding ragged all-to-all dims similar to all-to-alls is not needed.)， 内容是 (Support expanding ragged alltoall dims similar to alltoalls is not needed.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Support expanding ragged all-to-all dims similar to all-to-alls is not needed.,Support expanding ragged alltoall dims similar to alltoalls is not needed.,2025-03-20T21:45:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89680
845,"以下是一个github上的tensorflow下的一个issue, 标题是(TensorFlow Lite 2.19 ARM compilation failed: TfLiteQuantizationType : int expected identifier or ...)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19  OS platform and distribution Raspberry Pi OS Bookworm  GCC/compiler version gcc (Debian 12.2.014) 12.2.0  Current behavior? Getting error when trying to compile simple program using TensorFlow Lite 2.19 library on Raspberry Pi. main.c:  Compilation error:  Compilation command:  No problems using TensorFlow Lite 2.18. This commit https://github.com/tensorflow/tensorflow/commit/977257e45ea83169c2dd6ff24a403cdd05b138bd caused issue.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,distlibs,TensorFlow Lite 2.19 ARM compilation failed: TfLiteQuantizationType : int expected identifier or ..., Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19  OS platform and distribution Raspberry Pi OS Bookworm  GCC/compiler version gcc (Debian 12.2.014) 12.2.0  Current behavior? Getting error when trying to compile simple program using TensorFlow Lite 2.19 library on Raspberry Pi. main.c:  Compilation error:  Compilation command:  No problems using TensorFlow Lite 2.18. This commit https://github.com/tensorflow/tensorflow/commit/977257e45ea83169c2dd6ff24a403cdd05b138bd caused issue.,2025-03-20T17:57:36Z,type:build/install comp:lite subtype: raspberry pi TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/89666
1073,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #89210: Qualcomm AI Engine Direct - Provide op optimization)， 内容是 (PR CC(Qualcomm AI Engine Direct  Provide op optimization): Qualcomm AI Engine Direct  Provide op optimization Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/89210 Summary:  Support FC>CONV2D optimization  Add CONV2D op builder with CPU transpose Test:  [x] a8w8 Gemma3 compile and execution successfully  [x] qnn_compiler_plugin_test all pass Copybara import of the project:  8ff43628b495ce8ae730a5ed42fa6bea1ef15844 by jiunkaiy : Qualcomm AI Engine Direct  Op optimization Summary:  Support FC>CONV2D optimization  Add CONV2D op builder with CPU transpose Merging this change closes CC(Qualcomm AI Engine Direct  Provide op optimization) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89210 from jiunkaiy:dev/jiunkaiy/gemma2_fc 8ff43628b495ce8ae730a5ed42fa6bea1ef15844)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,copybara-service[bot],PR #89210: Qualcomm AI Engine Direct - Provide op optimization,PR CC(Qualcomm AI Engine Direct  Provide op optimization): Qualcomm AI Engine Direct  Provide op optimization Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/89210 Summary:  Support FC>CONV2D optimization  Add CONV2D op builder with CPU transpose Test:  [x] a8w8 Gemma3 compile and execution successfully  [x] qnn_compiler_plugin_test all pass Copybara import of the project:  8ff43628b495ce8ae730a5ed42fa6bea1ef15844 by jiunkaiy : Qualcomm AI Engine Direct  Op optimization Summary:  Support FC>CONV2D optimization  Add CONV2D op builder with CPU transpose Merging this change closes CC(Qualcomm AI Engine Direct  Provide op optimization) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89210 from jiunkaiy:dev/jiunkaiy/gemma2_fc 8ff43628b495ce8ae730a5ed42fa6bea1ef15844,2025-03-20T17:36:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89662
567,"以下是一个github上的tensorflow下的一个issue, 标题是(`ArrayImpl` is now a proper subclass of `jax.Array`)， 内容是 (`ArrayImpl` is now a proper subclass of `jax.Array` This allows to make `jax.Array` a ""strict"" ABC which doesn't support virtual subclasses and is thus can do faster isinstance/issubclass checks. Note that I had to move `StrictABC` from `util` into a separate submodule to avoid an import cycle with `basearray` and `xla_client`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],`ArrayImpl` is now a proper subclass of `jax.Array`,"`ArrayImpl` is now a proper subclass of `jax.Array` This allows to make `jax.Array` a ""strict"" ABC which doesn't support virtual subclasses and is thus can do faster isinstance/issubclass checks. Note that I had to move `StrictABC` from `util` into a separate submodule to avoid an import cycle with `basearray` and `xla_client`.",2025-03-20T14:16:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89651
1200,"以下是一个github上的tensorflow下的一个issue, 标题是(Add support for SeparableConv2DTranspose (Depthwise Conv2DTranspose))， 内容是 ( Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.16.1  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Feature Request TensorFlow currently lacks a SeparableConv2DTranspose operation, which is essential for efficient depthwise transposed convolutions. PyTorch already supports this easily via the groups parameter in ConvTranspose2d. However, TensorFlow’s Conv2DTranspose does not have a groups argument, making it impossible to achieve the same functionality. Workarounds Currently, the only way to approximate this functionality in TensorFlow is: 	•	Manually splitting input channels and applying Conv2DTranspose separately (inefficient and slow)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,eewindfly,Add support for SeparableConv2DTranspose (Depthwise Conv2DTranspose)," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.16.1  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Feature Request TensorFlow currently lacks a SeparableConv2DTranspose operation, which is essential for efficient depthwise transposed convolutions. PyTorch already supports this easily via the groups parameter in ConvTranspose2d. However, TensorFlow’s Conv2DTranspose does not have a groups argument, making it impossible to achieve the same functionality. Workarounds Currently, the only way to approximate this functionality in TensorFlow is: 	•	Manually splitting input channels and applying Conv2DTranspose separately (inefficient and slow",2025-03-20T09:27:50Z,type:feature,open,0,0,https://github.com/tensorflow/tensorflow/issues/89628
1190,"以下是一个github上的tensorflow下的一个issue, 标题是(NotImplementedError: StreamingModel.call() not implemented)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.18.0  Custom code Yes  OS platform and distribution windows 11  Mobile device _No response_  Python version 3.12.6  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When I try to compile and run my custom Keras model (StreamingModel), I encounter the following error: `NotImplementedError: Exception encountered when calling StreamingModel.call(). Could not automatically infer the output shape / dtype of 'streaming_model' (of type StreamingModel). Either the `StreamingModel.call()` method is incorrect, or you need to implement the `StreamingModel.compute_output_spec() / compute_output_shape()` method. Error encountered: Model StreamingModel does not have a `call()` method implemented.` Expected Behavior I expect the Stre)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,zhouxiaoyaozzz,NotImplementedError: StreamingModel.call() not implemented," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.18.0  Custom code Yes  OS platform and distribution windows 11  Mobile device _No response_  Python version 3.12.6  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When I try to compile and run my custom Keras model (StreamingModel), I encounter the following error: `NotImplementedError: Exception encountered when calling StreamingModel.call(). Could not automatically infer the output shape / dtype of 'streaming_model' (of type StreamingModel). Either the `StreamingModel.call()` method is incorrect, or you need to implement the `StreamingModel.compute_output_spec() / compute_output_shape()` method. Error encountered: Model StreamingModel does not have a `call()` method implemented.` Expected Behavior I expect the Stre",2025-03-20T02:55:01Z,type:bug TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/89600
917,"以下是一个github上的tensorflow下的一个issue, 标题是(Adjust sharding rules for ragged_dot operations.)， 内容是 (Adjust sharding rules for ragged_dot operations.  Mode 3 (the ragged dimension is a batch dimension) Do not propagate shardings to/from the group_sizes (the 3rd operand) since it is redundant. We will discard this tensor and convert ragged_dot into a standard dot in the partitioner.  Mode 1 (the ragged dimension is a lhs noncontracting dimension) The ragged dimension and the group_size dimension needs full replication in the partitioner.  Mode 2 (the ragged dimension is a contracting dimension) The ragged dimension and the group_size dimension needs full replication in the partitioner. We do not mark the ragged dimension as replicated although it is a contracting dimension.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Adjust sharding rules for ragged_dot operations.,Adjust sharding rules for ragged_dot operations.  Mode 3 (the ragged dimension is a batch dimension) Do not propagate shardings to/from the group_sizes (the 3rd operand) since it is redundant. We will discard this tensor and convert ragged_dot into a standard dot in the partitioner.  Mode 1 (the ragged dimension is a lhs noncontracting dimension) The ragged dimension and the group_size dimension needs full replication in the partitioner.  Mode 2 (the ragged dimension is a contracting dimension) The ragged dimension and the group_size dimension needs full replication in the partitioner. We do not mark the ragged dimension as replicated although it is a contracting dimension.,2025-03-20T00:11:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89592
630,"以下是一个github上的tensorflow下的一个issue, 标题是(IFRT: Introduce `LoadedExecutable::GetDonatableInputIndices()`.)， 内容是 (IFRT: Introduce `LoadedExecutable::GetDonatableInputIndices()`. The function returns indices of parameters that will be donated whenever `Execute` gets called, provided they are not present in `execute_options.non_donatable_input_indices`. This change will be used in an upcoming IFRTproxy commit that will allow make incurring RPC roundtrips for `Array::IsDeleted()` unnecessary.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],IFRT: Introduce `LoadedExecutable::GetDonatableInputIndices()`.,"IFRT: Introduce `LoadedExecutable::GetDonatableInputIndices()`. The function returns indices of parameters that will be donated whenever `Execute` gets called, provided they are not present in `execute_options.non_donatable_input_indices`. This change will be used in an upcoming IFRTproxy commit that will allow make incurring RPC roundtrips for `Array::IsDeleted()` unnecessary.",2025-03-19T17:59:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89569
247,"以下是一个github上的tensorflow下的一个issue, 标题是(internal BUILD rule visibility)， 内容是 (internal BUILD rule visibility)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],internal BUILD rule visibility,internal BUILD rule visibility,2025-03-19T17:52:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89568
469,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] new fusion kind for nested dot fusions)， 内容是 ([XLA:GPU] new fusion kind for nested dot fusions To prevent priority fusion from modifying the construct produced by nest_gemm_fusion pass.  For example priority fusion may insert back bitcasts that we have carefully extracted before.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA:GPU] new fusion kind for nested dot fusions,[XLA:GPU] new fusion kind for nested dot fusions To prevent priority fusion from modifying the construct produced by nest_gemm_fusion pass.  For example priority fusion may insert back bitcasts that we have carefully extracted before.,2025-03-19T16:48:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89562
930,"以下是一个github上的tensorflow下的一个issue, 标题是(Compilation error（The call method of the StreamingModel class is not implemented correctly）)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Is this a new compilation error？The call method of the StreamingModel class is not implemented correctly, causing TensorFlow/Karas to be unable to infer the output shape and data type of the model.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,zhouxiaoyaozzz,Compilation error（The call method of the StreamingModel class is not implemented correctly）," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Is this a new compilation error？The call method of the StreamingModel class is not implemented correctly, causing TensorFlow/Karas to be unable to infer the output shape and data type of the model.  Standalone code to reproduce the issue   Relevant log output ",2025-03-19T08:11:56Z,type:bug TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/89540
297,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA] Adds a helper function for copying original value)， 内容是 ([XLA] Adds a helper function for copying original value)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA] Adds a helper function for copying original value,[XLA] Adds a helper function for copying original value,2025-03-18T21:01:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89489
1217,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #23320: [NVIDIA GPU] Add utility functions for multi-host fast-interconnect domain)， 内容是 (PR CC([Feature Request] Addition of new operation to Tensorflow Lite for ""ENet""): [NVIDIA GPU] Add utility functions for multihost fastinterconnect domain Imported from GitHub PR https://github.com/openxla/xla/pull/23320 On Blackwell and onwards, multiple hosts may be connected by NVLink thus creating a fastinterconnect domain beyond a single node. This PR leverages NVML APIs to detect the actual fastinterconnect domain. Followup PRs will update the logic in PjRt client on top of these utility functions. Copybara import of the project:  ee084714876336243fa3ba52a857d2d41124b730 by Terry Sun : pipe nvml to detect nvl  36e1f71dfabb75dbd0853355a7be8c0d6d103903 by Terry Sun : convert clusterUUID  917eb219a1bdebfc7a12f5849545615c83092c73 by Terry Sun : bake boot id into key  cbd909ab66ccf47f6e140632645fbb4d6c61c7c3 by Terry Sun : use global id and add getter  e487c4718eefe72e77708ffaf2c8cbbf6c979e8e by Terry Sun : use device ordinal and )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],PR #23320: [NVIDIA GPU] Add utility functions for multi-host fast-interconnect domain,"PR CC([Feature Request] Addition of new operation to Tensorflow Lite for ""ENet""): [NVIDIA GPU] Add utility functions for multihost fastinterconnect domain Imported from GitHub PR https://github.com/openxla/xla/pull/23320 On Blackwell and onwards, multiple hosts may be connected by NVLink thus creating a fastinterconnect domain beyond a single node. This PR leverages NVML APIs to detect the actual fastinterconnect domain. Followup PRs will update the logic in PjRt client on top of these utility functions. Copybara import of the project:  ee084714876336243fa3ba52a857d2d41124b730 by Terry Sun : pipe nvml to detect nvl  36e1f71dfabb75dbd0853355a7be8c0d6d103903 by Terry Sun : convert clusterUUID  917eb219a1bdebfc7a12f5849545615c83092c73 by Terry Sun : bake boot id into key  cbd909ab66ccf47f6e140632645fbb4d6c61c7c3 by Terry Sun : use global id and add getter  e487c4718eefe72e77708ffaf2c8cbbf6c979e8e by Terry Sun : use device ordinal and ",2025-03-18T20:59:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89488
1120,"以下是一个github上的tensorflow下的一个issue, 标题是(Fix doc bug in tf.keras.losses.SparseCategoricalCrossentropy)， 内容是 (Fixes CC(tf.keras.losses.SparseCategoricalCrossentropy has logical/Doc bug) This PR enhances the error handling for invalid values of the reduction parameter. Previously, if reduction was None or an incorrect value (including variations like ""None"", ""Auto"", ""Sum"", etc.), the error message was not specific enough, making it difficult for users to understand what went wrong. If the value does not match exactly (casesensitive), a clear error message is raised, specifying the invalid value and listing the expected options. This improves usability by guiding the user toward the correct values. Impact: 1. Improves error clarity for developers using the reduction parameter. 2. Ensures better debugging by explicitly pointing out incorrect values. 3. Reduces confusion caused by incorrect casing or unexpected inputs.  For more details, open the Copilot Workspace session.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,AYUHSPATIL,Fix doc bug in tf.keras.losses.SparseCategoricalCrossentropy,"Fixes CC(tf.keras.losses.SparseCategoricalCrossentropy has logical/Doc bug) This PR enhances the error handling for invalid values of the reduction parameter. Previously, if reduction was None or an incorrect value (including variations like ""None"", ""Auto"", ""Sum"", etc.), the error message was not specific enough, making it difficult for users to understand what went wrong. If the value does not match exactly (casesensitive), a clear error message is raised, specifying the invalid value and listing the expected options. This improves usability by guiding the user toward the correct values. Impact: 1. Improves error clarity for developers using the reduction parameter. 2. Ensures better debugging by explicitly pointing out incorrect values. 3. Reduces confusion caused by incorrect casing or unexpected inputs.  For more details, open the Copilot Workspace session.",2025-03-18T18:47:30Z,size:XL invalid python,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89480,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , can you please sign the CLA? Many thanks!",This seems like AI generated content which goes against contributors guidelines and could be considered spam.
427,"以下是一个github上的tensorflow下的一个issue, 标题是(Unify implicit dependencies of OSS and internal xla_test macro)， 内容是 (Unify implicit dependencies of OSS and internal xla_test macro Both macro implementations used to add different implicit runtime dependency targets. This change is unifying that.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Unify implicit dependencies of OSS and internal xla_test macro,Unify implicit dependencies of OSS and internal xla_test macro Both macro implementations used to add different implicit runtime dependency targets. This change is unifying that.,2025-03-18T08:08:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89436
397,"以下是一个github上的tensorflow下的一个issue, 标题是(Move `value_inference_test.py` next to file to improve code coverage and remove unnecessary dependencies.)， 内容是 (Move `value_inference_test.py` next to file to improve code coverage and remove unnecessary dependencies.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Move `value_inference_test.py` next to file to improve code coverage and remove unnecessary dependencies.,Move `value_inference_test.py` next to file to improve code coverage and remove unnecessary dependencies.,2025-03-17T22:29:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89403
411,"以下是一个github上的tensorflow下的一个issue, 标题是(Support ragged_dot in Shardy.)， 内容是 (Support ragged_dot in Shardy. 1. Allow_xla_features (mhlo.ragged_dot) in MhloToStablehlo translation. 2. Add mhlo_extensions for ShardyXLA, and define the sharding rule for ragged_dot operations.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Support ragged_dot in Shardy.,"Support ragged_dot in Shardy. 1. Allow_xla_features (mhlo.ragged_dot) in MhloToStablehlo translation. 2. Add mhlo_extensions for ShardyXLA, and define the sharding rule for ragged_dot operations.",2025-03-17T18:43:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89388
1216,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA] Adjust the `CallInliner` pass to allow uniquifying channel ids after inlining.)， 内容是 ([XLA] Adjust the `CallInliner` pass to allow uniquifying channel ids after inlining. This is an option that can be enabled specifically on the pass, and will now enabled in the XLA:GPU compilation pipeline if `xla_ignore_channel_id` is `true`. This works around an issue whereby the `CallInliner` might inline several computations involving collectives with the same `channel_id` into the same computation. This is actually not wellbehaved and can result in the creation of cyclic HLO graphs. This change ensures that inlining does not create cyclic graphs. Note that the transformation is actually not semanticspreserving, in that a computation involving several `call` instructions wrapping collectives that use the same `channel_id` is already illformed. Nevertheless, `channel_id`s are only used as a ""hack"" for pipeline parallelism in XLA:GPU currently, and are mostly vestigial at this point. The flagdriven enablement can be deleted once )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA] Adjust the `CallInliner` pass to allow uniquifying channel ids after inlining.,"[XLA] Adjust the `CallInliner` pass to allow uniquifying channel ids after inlining. This is an option that can be enabled specifically on the pass, and will now enabled in the XLA:GPU compilation pipeline if `xla_ignore_channel_id` is `true`. This works around an issue whereby the `CallInliner` might inline several computations involving collectives with the same `channel_id` into the same computation. This is actually not wellbehaved and can result in the creation of cyclic HLO graphs. This change ensures that inlining does not create cyclic graphs. Note that the transformation is actually not semanticspreserving, in that a computation involving several `call` instructions wrapping collectives that use the same `channel_id` is already illformed. Nevertheless, `channel_id`s are only used as a ""hack"" for pipeline parallelism in XLA:GPU currently, and are mostly vestigial at this point. The flagdriven enablement can be deleted once ",2025-03-17T18:14:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89382
759,"以下是一个github上的tensorflow下的一个issue, 标题是(Align accelerator application during compiled model creation to the expected behaviour.)， 内容是 (Align accelerator application during compiled model creation to the expected behaviour. This adds a new function field in the accelerator implementation structure and associated functions to set/call that function. The function queries the accelerator to know if the underlying TFLite delegate does JIT compilation. When that's the case, even if the accelerator is registered, it won't be used to create and apply a delegate unless explicitly requested through the compilation options.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Align accelerator application during compiled model creation to the expected behaviour.,"Align accelerator application during compiled model creation to the expected behaviour. This adds a new function field in the accelerator implementation structure and associated functions to set/call that function. The function queries the accelerator to know if the underlying TFLite delegate does JIT compilation. When that's the case, even if the accelerator is registered, it won't be used to create and apply a delegate unless explicitly requested through the compilation options.",2025-03-17T12:31:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89364
343,"以下是一个github上的tensorflow下的一个issue, 标题是(Use `UniquifyName(HloModule*)` and `UniquifyId(HloModule*)` whenever possible.)， 内容是 (Use `UniquifyName(HloModule*)` and `UniquifyId(HloModule*)` whenever possible.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Use `UniquifyName(HloModule*)` and `UniquifyId(HloModule*)` whenever possible.,Use `UniquifyName(HloModule*)` and `UniquifyId(HloModule*)` whenever possible.,2025-03-17T10:53:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89360
1206,"以下是一个github上的tensorflow下的一个issue, 标题是(Conv1D Layer with kernel size 1 does not match with iteratively processing)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.14.1  Custom code Yes  OS platform and distribution Linux Ubuntu  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I was trying to set up a realtime/causal Conv2D layers, by multiple Conv1D layers. I noticed an error in the order of 1e7, so i started to narrow down the issue. This is why i ended up with the following code. I essentially create 2 identical conv1D layers where i then load the weights from one into the other. When iterating over the time dimension (axis 1; as one would to in an realtime/causal implementation) this error occurs. Note:  this issue only happens for ""large"" filters (> 10)  this issue only happens for ""large"" features (> 30)  when setting the wei)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,woegerbauerJ,Conv1D Layer with kernel size 1 does not match with iteratively processing," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.14.1  Custom code Yes  OS platform and distribution Linux Ubuntu  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I was trying to set up a realtime/causal Conv2D layers, by multiple Conv1D layers. I noticed an error in the order of 1e7, so i started to narrow down the issue. This is why i ended up with the following code. I essentially create 2 identical conv1D layers where i then load the weights from one into the other. When iterating over the time dimension (axis 1; as one would to in an realtime/causal implementation) this error occurs. Note:  this issue only happens for ""large"" filters (> 10)  this issue only happens for ""large"" features (> 30)  when setting the wei",2025-03-17T10:20:20Z,stat:awaiting response type:bug comp:ops TF2.14,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89353,"addon: this issue also happens for `    x1 = np.ones([1,11,10,10])      init the layers     conv1 = tf.keras.layers.Conv1D(filters = 30,kernel_size=1, use_bias =False)     output1 = conv1(x1.copy())     output2 = np.zeros_like(output1)     for i in range(output1.shape[1]): calculated iteratively over time steps         output2[:,i,:] = conv1(x1[:,i,:])     print(""max error"", np.max(abs(output1output2)))` and also leads to the same error!","Hi  , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. However, I encountered a different issue. After making some modifications, the code worked for me. Additionally, I noticed that you are using an older version of TensorFlow. I recommend upgrading to the latest version for better results. I have attached a gist for your reference. Thank you!","Hi, thanks for your response! Due to our hardware we somewhat bounded to tf 2.14 (we are working with an NPU).  Thanks for finding out the issue with the dimension. I still assume this is somewhat of an issue, since mathematically it should be identical right? Even for 4 dimensions. However i am happy with the solution. Thank you very much!",Are you satisfied with the resolution of your issue? Yes No
1177,"以下是一个github上的tensorflow下的一个issue, 标题是(Failed to load the native TensorFlow runtime.)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tensorflow2.19.0   Custom code Yes  OS platform and distribution windows  Mobile device _No response_  Python version 3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the targ)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,devnas-2004,Failed to load the native TensorFlow runtime., Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tensorflow2.19.0   Custom code Yes  OS platform and distribution windows  Mobile device _No response_  Python version 3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the targ,2025-03-15T10:58:30Z,stat:awaiting response type:build/install subtype:windows TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89296,"Hi 2004 , Apologies for the delay, could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:  You need to install the MSVC 2019 redistributable  Your CPU does not support AVX2 instructions  Your CPU/Python is on 32 bits  There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Please search for duplicates before opening a new issue.,Are you satisfied with the resolution of your issue? Yes No
1154,"以下是一个github上的tensorflow下的一个issue, 标题是(TensorFlow on RTX 5090)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.20.0.dev20250314  Custom code No  OS platform and distribution Windows 11  WSL2  Ubuntu 22.04.5 LTS  Mobile device _No response_  Python version 3.10.12  Bazel version 7.4.1  GCC/compiler version gcc (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0  CUDA/cuDNN version CUDA Version: 12.8  GPU model and memory RTX 5090 32GB  Current behavior? I had hoped that tensorflow would work on the RTX 5090 at all. It does not, sadly. I tried building from source but that didn't work either. I tried running the environment script but that didn't work either. At least bash is my primary programming language, so I was able to tidy that one up here: https://github.com/tensorflow/tensorflow/pull/89271 But I wasn't able to get tensorflow running. I had a similar issue with PyTorch, which needed to use CUDA 12.8.* to work on the Blackwell cards, but no )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,maludwig,TensorFlow on RTX 5090," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.20.0.dev20250314  Custom code No  OS platform and distribution Windows 11  WSL2  Ubuntu 22.04.5 LTS  Mobile device _No response_  Python version 3.10.12  Bazel version 7.4.1  GCC/compiler version gcc (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0  CUDA/cuDNN version CUDA Version: 12.8  GPU model and memory RTX 5090 32GB  Current behavior? I had hoped that tensorflow would work on the RTX 5090 at all. It does not, sadly. I tried building from source but that didn't work either. I tried running the environment script but that didn't work either. At least bash is my primary programming language, so I was able to tidy that one up here: https://github.com/tensorflow/tensorflow/pull/89271 But I wasn't able to get tensorflow running. I had a similar issue with PyTorch, which needed to use CUDA 12.8.* to work on the Blackwell cards, but no ",2025-03-15T03:40:16Z,stat:awaiting tensorflower type:bug wsl2 TF 2.18,open,0,29,https://github.com/tensorflow/tensorflow/issues/89272,same problem,"I should mention that I'm a Senior AI Developer by trade and I'm more than happy to invest my personal time in helping to fix this, I'm just not sure where to start.",I should also mention that the latest clang release here supports building for compute_100/sm_100+ https://github.com/llvm/llvmproject/releases/tag/llvmorg20.1.0 It's not supported in LLVM 18. But it compiles this on my GPU just fine (extra logs attached just in case they help someone else). ,"I'm going to keep writing my attempts to get things working here. I've cut a branch on my fork, still no luck, but here's some halfdiscoveries. More and more of the project is building as I continue, zero idea how far away I am from victory. He's the branch I'm on, compared with the base: https://github.com/maludwig/tensorflow/compare/ml/fixing_tf_env...maludwig:tensorflow:ml/attempting_build_rtx5090?expand=1 A few findings:  CUDA 12.8.1 adds support for the RTX 5090 (and other Blackwells), so we need that  There's a bug in cutlass, which was forked for tensorflow for a reason I don't know, the bug was fixed here: https://github.com/NVIDIA/cutlass/pull/1784/files  The old fork, done by  was certainly done for a reason, no idea what I'm breaking by going back to the NVIDIA main branch here. Not sure how to message people on GitHub, but maybe they'll get notified on this?  I updated NCCL to the latest 2.26.2 wheel  Build is still failing, but it's taking WAY longer to fail now. This is possibly a good sign.","Yep, I'm stopping for the night, it's currently stuck on what seems to be duplicate logging macros, looks like maybe two different logging libraries are somehow being included at the same time. Two very very similar logging libraries. But instead of taking 30 seconds before it fails, now it takes 17 minutes to fail, which I define as progress! "," VICTORY Ok I didn't stop for the night. Instead, I just ignored all manner of warnings that shouldn't be ignored:  And bam!  No idea if it'll work, but it **did build**! I've pushed the latest code changes to my branch. https://github.com/maludwig/tensorflow/compare/ml/fixing_tf_env...maludwig:tensorflow:ml/attempting_build_rtx5090?expand=1","It passed one test!  I also installed the wheel generated in the last step to a new python venv, and it worked!  I...am...going...to...run all the tests overnight? My build process is complete trash and I have no idea what I'm doing, but I COULD also PR this code, but like, that's slightly terrifying. I've ignoring probably thousands of warnings that a competent C++ developer could probably actually solve, rather than just ignore...","Tests didn't pass, but it did build! And it could do basic matrix addition and multiplication in Python! NOW I'm definitely going to bed though.","It also is able to do the classic ""hello world"" ML task of learning digits on MNIST, but the warnings are PLENTIFUL and cryptic. I don't know what they mean, but the final model happens to work great! ", you should upgrade commit hash256 XLA on bazel file and it should work,"Sorry that's a bit cryptic for me. I'm normally a Python dev, apologies. Did you mean in my commits on my branch above? https://github.com/maludwig/tensorflow/compare/ml/fixing_tf_env...maludwig:tensorflow:ml/attempting_build_rtx5090?expand=1",+1," Steps to get it running on your RTX 5000 series card  Guide for all platforms  Install llvm 20.1.0 LLVM 20.1.0 is required to compile code for compute capability 10.0 and 12.0 (RTX 5000 series). All platforms here: https://github.com/llvm/llvmproject/releases/tag/llvmorg20.1.0  Install CUDA 12.8.1 CUDA 12.8.1 is required to compile code for compute capability 10.0 and 12.0 (RTX 5000 series). Also install cuDNN 9.8.0 and NCCL 2, for CUDA 12.  Install Python 3.10.12 This just happens to be the version I'm using and may be completely unnecessary. I personally love pyenv because it installs it to your local user, so you don't need to fret about admin/root permissions.  Make a Python venv for tensorflow This will prevent your system from being polluted by tensorflow dependencies, and will make it much much much easier to clean up if you want to start over.  Install Bazelisk Bazelisk is a wrapper for Bazel that downloads the correct version of Bazel for the project.  Clone tensorflow   Configure bazel   Build tensorflow   Script for WSL Ubuntu 22.04 This script should let you compile for RTX 5000 series on WSL Ubuntu 22.04. Before running this script, be sure to install the latest drivers for your RTX 5000 series card on the Windows side, install WSL2, and use Ubuntu 22.04. Then reboot your PC, that way, WSL2 will be able to see your GPU. It probably also works on nonWSL Ubuntu 22.04. It might maybe work on other Ubuntu versions. It's not going to work for Windows except in WSL. It may not work at all. Consider copying it line by line and handle errors manually.  Add these lines to your `~/.bashrc` or `~/.zshrc` file:  Restart your terminal. Test that the LLVM installation worked: Make this file in `~/rtx5000/card_details.cu`:   NOTE You mayyyybe need to get the very latest cuDNN with this, but I don't think so.  NOTE: If this doesn't work for you, let me know which error you got, and maybe I missed something in my environment. Since this was already my dev box, I'm not sure if this is a complete guide, but it's what I did to get it working.","Hey  , just seeing your tags you added. To be clear, this is on tf_nightly, not tf 2.18, and I have no idea really what I'm doing, so I'm not gonna PR my extremely busted and testsfailing branch, even though it does build. I put it here so that someone who knows what they're doing could fold in the new stuff more easily, or so that other normal humans like me could run tensorflow on an RTX 5000, instead of just being unable to run it. An actual human who knows what they're doing should look this over and figure it out.",cd ~/rtx5000 nvcc o card_details_nvcc card_details.cu bash: cd: /home/nicolai/rtx5000: No such file or directory cc1plus: fatal error: card_details.cu: No such file or directory compilation terminated.,"I tryed to let it run on my wsl. Build dosen't work, how can I use a prebuiled nightly build?   Configuration: 8850a00e136a9e8be32c557a177e77f38f3c27b70c44518acb5ba0af47f7836b  Execution platform: @//:platform In file included from external/local_xla/xla/stream_executor/cuda/cuda_status.cc:16: external/local_xla/xla/stream_executor/cuda/cuda_status.h:22:10: fatal error: 'third_party/gpus/cuda/include/cuda.h' file not found    22           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 1 error generated. Target //tensorflow/tools/pip_package:wheel failed to build ERROR: /mnt/c/Projekte/tmp/tensorflow/tensorflow/tools/pip_package/BUILD:293:9 Action tensorflow/tools/pip_package/wheel_house/tensorflow2.20.0.dev0+selfbuiltcp312cp312linux_x86_64.whl failed: (Exit 1): clang20 failed: error executing CppCompile command (from target @//xla/stream_executor/cuda:cuda_status)   (cd /root/.cache/bazel/_bazel_root/509ab554767d44265e0030c4731aba07/execroot/org_tensorflow && \   exec env  \     CLANG_CUDA_COMPILER_PATH=/usr/local/bin/clang20 \     PATH=/root/.cache/bazelisk/downloads/sha256/c97f02133adce63f0c28678ac1f21d65fa8255c80429b588aeeba8a1fac6202b/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \     PWD=/proc/self/cwd \     PYTHON_BIN_PATH=/mnt/c/Projekte/env/bin/python3 \     PYTHON_LIB_PATH=/mnt/c/Projekte/env/lib/python3.12/sitepackages \     TF2_BEHAVIOR=1 \   /usr/local/bin/clang20 MD MF bazelout/k8opt/bin/external/local_xla/xla/stream_executor/cuda/_objs/cuda_status/cuda_status.pic.d 'frandomseed=bazelout/k8opt/bin/external/local_xla/xla/stream_executor/cuda/_objs/cuda_status/cuda_status.pic.o' iquote external/local_xla iquote bazelout/k8opt/bin/external/local_xla iquote external/com_google_absl iquote bazelout/k8opt/bin/external/com_google_absl iquote external/local_config_cuda iquote bazelout/k8opt/bin/external/local_config_cuda iquote external/cuda_cudart iquote bazelout/k8opt/bin/external/cuda_cudart iquote external/cuda_cublas iquote bazelout/k8opt/bin/external/cuda_cublas iquote external/cuda_cccl iquote bazelout/k8opt/bin/external/cuda_cccl iquote external/cuda_nvtx iquote bazelout/k8opt/bin/external/cuda_nvtx iquote external/cuda_nvcc iquote bazelout/k8opt/bin/external/cuda_nvcc iquote external/cuda_cusolver iquote bazelout/k8opt/bin/external/cuda_cusolver iquote external/cuda_cufft iquote bazelout/k8opt/bin/external/cuda_cufft iquote external/cuda_cusparse iquote bazelout/k8opt/bin/external/cuda_cusparse iquote external/cuda_curand iquote bazelout/k8opt/bin/external/cuda_curand iquote external/cuda_cupti iquote bazelout/k8opt/bin/external/cuda_cupti iquote external/cuda_nvml iquote bazelout/k8opt/bin/external/cuda_nvml iquote external/cuda_nvjitlink iquote bazelout/k8opt/bin/external/cuda_nvjitlink iquote external/local_tsl iquote bazelout/k8opt/bin/external/local_tsl Ibazelout/k8opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers Ibazelout/k8opt/bin/external/cuda_cudart/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cublas/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cccl/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_nvtx/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_nvcc/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cusolver/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cufft/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cusparse/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_curand/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cupti/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_nvml/_virtual_includes/headers Ibaroot199root199P461:/mroot199P461:/mroot199P461:/mroot199P461:/mroot199P461:/mroot199P461:/mnt/c/Projekte/tmp/tensorflow    ", and  is there a build that I can use? (like nightly build),"Hey  , scroll up until you see ""Script for WSL Ubuntu 22.04"" in the comments.  The issue I raised is that there is no build, nightly or otherwise, that supports the latest Blackwell GPUs. I arguably managed to build one myself. You also could. But read through the script I put up above slowly. It looks like you missed some steps. HOPEFULLY the script I wrote will work for someone else, but since I got it working on an old dev box, rather than a brand fresh new black docker container or something, it's likely that I missed a dependency or two."," dosen't work for me. echo ""This should be Bazel v8.8.1"" bazel version here I get only 8.1.1 and I get some Errors for the build. is there any chance when tensorflow will support the 5090 on its own and I can simply use the next version of tensorflow? If so, please give me a date when.","Yes, I feel problem with NVIDIA RTX 5090  32GB Blackwell (not nightly version of PyTorch). I cannot see GPU with TensorFlow success. Can you take a look at https://gist.github.com/donhuvy/6cd637a09b034168d01181d5ce98a5fe  . I catch `Num GPUs Available:  0` . My environment: Windows 11 pro, JupyterLab latest version, Python 3.11.x .",wait so you got it to work 100%. it sucks my system has 2 5090's and i'm using a cpu for training.,"Whenever I could not get drivers to work, it usually resolved after installing, reinstalling and changing versions of different packages, since the shortage I doubt there is overwhelming support for the 5090, I remember all launches to have crashing and minimal error bugs that disappear over a relatively short period of time. I got stuck a while ago similarly on different cards and in general, it might be a tiny thing somewhere with your paths and env. Try on Linux and see if that works, I don't know why you are using Windows as a senior Dev. My speeds on the 4090 doubled on render times for anything AI/ML related and loading times of nearly everything python vanished.",">  dosen't work for me. > echo ""This should be Bazel v8.8.1"" > bazel version here I get only 8.1.1 and I get some Errors for the build. >  >  > is there any chance when tensorflow will support the 5090 on its own and I can simply use the next version of tensorflow? >  > If so, please give me a date when. >  Sorry  , I'm not a tensorflow employee. I'm just some dude. Can't guess when it will be fixed. I just got my build to work and my personal projects running fine. My tests are failing and I assume that needs resolving. If your Bazel version is wrong, try installing Bazelisk. See above for instructions. ","> wait so you got it to work 100%. it sucks my system has 2 5090's and i'm using a cpu for training. >  Yep. For my workflow (training StableDiffusion LoRAs) it works fine. The tests are failing locally, but they must be testing tensorflow components that I am not using. You could presumably try following in my footsteps and use your 5090s. ","> Try on Linux and see if that works, I don't know why you are using Windows as a senior Dev. My speeds on the 4090 doubled on render times for anything AI/ML related and loading times of nearly everything python vanished. I'm also a senior dev, and while I agree in general that Linux is better and faster, Windows is still a perfectly legit OS. In fact, Apple Silicon is quite nice for training too. There's no distinction between RAM and VRAM in arm64a. Huge models run on consumer hardware. Not near as fast as on nvidia, but OSX is a legit OS too. "," I tried following the WSL script and got this error in the final step: ""external/local_tsl/tsl/profiler/lib/nvtx_utils.cc:32:10: fatal error: 'third_party/gpus/cuda/include/cuda.h' file not found"". All previous steps were OK, such as the one building the .cu file using clang. ","  What's your HERMETIC_CUDA_VERSION? It should be 12.8.1 Apart from that, maybe try cleaning the Bazel cache? "," It is 12.8.1 correctly. I also tried cleaning the Bazel cache, the error was same: header files in 'third_party/gpus/cuda/include' cannot be found. "," Some additional info: among the error verbose text, it displayed some environmental variables such as ""LD_LIBRARY_PATH"" ""PATH"", etc, but no ""CPATH"" can be seen. Could this be related to the issue?"
679,"以下是一个github上的tensorflow下的一个issue, 标题是(Fixing bugs and old code in the tf_env script)， 内容是 (I was trying to make a bug report, and the Github Issues thing told me to run this script, which failed to run and produced a lot of buggy output. I didn't try to make any changes to the script, since I'm not strictly sure what all of these help with, I just tried to stabilize the script, make the code cleaner, and account for common bugs in bash scripts (like a space in the path to python). I'll make comments on the PR to explain each change.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,maludwig,Fixing bugs and old code in the tf_env script,"I was trying to make a bug report, and the Github Issues thing told me to run this script, which failed to run and produced a lot of buggy output. I didn't try to make any changes to the script, since I'm not strictly sure what all of these help with, I just tried to stabilize the script, make the code cleaner, and account for common bugs in bash scripts (like a space in the path to python). I'll make comments on the PR to explain each change.",2025-03-15T03:11:04Z,ready to pull size:M,closed,0,7,https://github.com/tensorflow/tensorflow/issues/89271,"Sorry, maybe I should have merged right away rather than making the changes. Not sure what the process is for letting you know, but I've made extremely minor modifications as suggested to the top of the file.","Sorry if I'm doing it wrong, new to contributing to this project, but I added the comment lines you suggested, can you rereview,  ?","Sorry for the delay, I was not able to do much TF reviews over the past few days.","So, now what happens. I can't seem to merge this. Who does the merging?","If you look at https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.mdcontributorlicenseagreements, there's a graph just above the section I just linked to. The PR needs to get imported internally and there are more checks and reviews there. You only see what is above the topmost line.","Oh, ok, I expected it to be like an automated thing, where once it gets approved then it rather quickly gets merged, if tests pass. So now I just wait then I guess? I just found a different PR (unrelated) that you approved and then it got merged like 2 weeks later. Is this normal? https://github.com/tensorflow/tensorflow/pull/73692","Yeah, sadly we only have one single engineer that is pinged when the internal change gets imported, plus the people that review the PR. But if I review the PR I cannot also approve the internal change, so either I need to add someone else or the pinged engineer has to review. The nice thing is that here everything seems to pass, so once we get the internal approval this should be ready to merge."
1191,"以下是一个github上的tensorflow下的一个issue, 标题是(A dynamic link library (DLL) initialization routine failed.)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.8  Custom code Yes  OS platform and distribution windows 11  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,kaushikn02,A dynamic link library (DLL) initialization routine failed., Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.8  Custom code Yes  OS platform and distribution windows 11  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap,2025-03-15T01:59:40Z,type:build/install,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89269,please update the MSVC 2019 redistributable，download links: https://download.visualstudio.microsoft.com/download/pr/285b28c73cf947fb9be801cf5323a8df/8F9FB1B3CFE6E5092CF1225ECD6659DAB7CE50B8BF935CB79BFEDE1F3C895240/VC_redist.x64.exe,Please search for duplicates before opening a new issue,Are you satisfied with the resolution of your issue? Yes No
496,"以下是一个github上的tensorflow下的一个issue, 标题是(Fix parsing for `RaggedDotDimensionNumbersAttr` to correctly parse nested attributes.)， 内容是 (Fix parsing for `RaggedDotDimensionNumbersAttr` to correctly parse nested attributes. The issue was that the `dot_dimension_numbers` attribute was parsed like raw structlike element instead of being parsed as an `Attribute`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Fix parsing for `RaggedDotDimensionNumbersAttr` to correctly parse nested attributes.,Fix parsing for `RaggedDotDimensionNumbersAttr` to correctly parse nested attributes. The issue was that the `dot_dimension_numbers` attribute was parsed like raw structlike element instead of being parsed as an `Attribute`.,2025-03-15T00:13:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89266
325,"以下是一个github上的tensorflow下的一个issue, 标题是(Lower lax.ragged_dot_general to chlo.ragged_dot in some cases on tpu.)， 内容是 (Lower lax.ragged_dot_general to chlo.ragged_dot in some cases on tpu.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Lower lax.ragged_dot_general to chlo.ragged_dot in some cases on tpu.,Lower lax.ragged_dot_general to chlo.ragged_dot in some cases on tpu.,2025-03-14T22:54:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89262
737,"以下是一个github上的tensorflow下的一个issue, 标题是(Inconsistent results when running tf.keras.layers.InputLayer)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? The expectation is that the API should raise a ValueError, but it does not pass the test.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gpt,sjh0849,Inconsistent results when running tf.keras.layers.InputLayer," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? The expectation is that the API should raise a ValueError, but it does not pass the test.  Standalone code to reproduce the issue   Relevant log output ",2025-03-14T19:41:05Z,stat:awaiting response type:bug stale comp:keras TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89252,"Hi  , Apologies for the delay, and thank you for your patience. I tried running your code on Colab using TensorFlow version 2.19.0 and nightly, but I did not encounter the error you are facing. I am attaching a gist for your reference. Based on my findings, this issue seems to be more related to Keras. I recommend posting this issue on the kerasteam/keras repository for better support. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.
674,"以下是一个github上的tensorflow下的一个issue, 标题是(Getting AttributeError when running tf.ones_like)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux 22.04  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? Getting AttributeError when running tf.ones_like.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gpt,sjh0849,Getting AttributeError when running tf.ones_like, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux 22.04  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? Getting AttributeError when running tf.ones_like.  Standalone code to reproduce the issue   Relevant log output ,2025-03-14T19:34:37Z,type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89250,"Hi  , Apologies for the delay, and thanks for raising your concern here. Could you please provide the full code snippet? This would help debug your issue more accurately. I tried running your code on Colab using TensorFlow versions 2.18.0 and 2.19.0, but I did not encounter any issues. Please find the gist here for your reference. Let me know if you need further assistance. Thank you!",sorry for that again. here is the complete class!  many thanks!
1229,"以下是一个github上的tensorflow下的一个issue, 标题是(The API documentation of tf.no_op specifies that tf.no_op() should return a TensorFlow Operation.)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution N\A  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? The API documentation of tf.no_op specifies that tf.no_op() should return a TensorFlow Operation.  The tests expect that calling tf.no_op() returns an instance of tf.Operation (and that its name property can be set by providing a name, e.g., ""my_no_op""). The error message indicates that tf.no_op() is returning None rather than an Operation (None has no attribute ""name""), causing the tests to fail. Given that the test suite is properly written according to the API's specification and the expected behavior, the issue stems from the source code implementation.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gpt,sjh0849,The API documentation of tf.no_op specifies that tf.no_op() should return a TensorFlow Operation.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution N\A  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? The API documentation of tf.no_op specifies that tf.no_op() should return a TensorFlow Operation.  The tests expect that calling tf.no_op() returns an instance of tf.Operation (and that its name property can be set by providing a name, e.g., ""my_no_op""). The error message indicates that tf.no_op() is returning None rather than an Operation (None has no attribute ""name""), causing the tests to fail. Given that the test suite is properly written according to the API's specification and the expected behavior, the issue stems from the source code implementation.  Standalone code to reproduce the issue   Relevant log output ",2025-03-14T19:01:19Z,type:bug comp:apis comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89248,"Hi  , Apologies for the delay, and thanks for raising your concern here. Could you please provide the full code snippet? This would help debug your issue more accurately. I tried running your code on Colab using TensorFlow versions 2.18.0 and 2.19.0, but I did not encounter any issues. Please find the gist here for your reference. Let me know if you need further assistance. Thank you!",sorry for that. here is the complete class!  many thanks!
899,"以下是一个github上的tensorflow下的一个issue, 标题是(tf.keras.losses.SparseCategoricalCrossentropy has logical/Doc bug)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? API documentation of tf.keras.losses.SparseCategoricalCrossentropy mentions that one of the parameters can be None, but the implementation does not check None, it checks 'none' which is a string.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gpt,sjh0849,tf.keras.losses.SparseCategoricalCrossentropy has logical/Doc bug," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? API documentation of tf.keras.losses.SparseCategoricalCrossentropy mentions that one of the parameters can be None, but the implementation does not check None, it checks 'none' which is a string.  Standalone code to reproduce the issue   Relevant log output ",2025-03-14T18:50:50Z,type:bug comp:keras TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89246,"This should be opened against the keras repository (tf_keras or keras directly). From the TF side, the only change that could be done is to update the documentation of the function, make sure the docstring uses the correct phrasing. Looking at https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy I see an example where `None` is being used instead of the string `""none""`, so if the example was a doctest then some test would have failed. We should convert the examples to be doctests. Probably though all of this would happen on keras side, not TF.", I see! Thanks for the clarification. I will report this to the keras repository. Thanks!
695,"以下是一个github上的tensorflow下的一个issue, 标题是(I get an invalid shape error when running tf.keras.losses.BinaryCrossentropy)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux 22.04  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? I get an invalid shape error when running   Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,sjh0849,I get an invalid shape error when running tf.keras.losses.BinaryCrossentropy, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux 22.04  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? I get an invalid shape error when running   Standalone code to reproduce the issue   Relevant log output ,2025-03-14T18:42:02Z,type:bug comp:keras TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89245,"Hi  , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18, 2.19, and the nightly, but I did not encounter any issues. Please find the gist attached for your reference. Thank you!","Hi  , Thanks for getting back to me! I noticed, like the other issue, that the replicable code wasn't sufficient, leading to the failure to call and execute the function. However, I reran the test, but it's not giving me the errors it used to give me. I think it's being flaky."
716,"以下是一个github上的tensorflow下的一个issue, 标题是(AssertionError when calling tf.keras.layers.Conv2DTranspose)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux 22.4  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? When running , I get AssertionError. The minimum reproducing example is attached.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gpt,sjh0849,AssertionError when calling tf.keras.layers.Conv2DTranspose," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux 22.4  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? When running , I get AssertionError. The minimum reproducing example is attached.  Standalone code to reproduce the issue   Relevant log output ",2025-03-14T18:37:22Z,type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89244,"Hi  , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18, 2.19, and the nightly, but I did not encounter any issues. Please find the gist attached for your reference. Thank you!","> Hi [](https://github.com/sjh0849) , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18, 2.19, and the nightly, but I did not encounter any issues. Please find the gist attached for your reference. >  > Thank you! Hi  , Sorry, I think my replication code earlier was not sufficient. (The colab seems like it's not executing the test.) Here is the updated replicable code.  > Hi [](https://github.com/sjh0849) , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18, 2.19, and the nightly, but I did not encounter any issues. Please find the gist attached for your reference. >  > Thank you!"
370,"以下是一个github上的tensorflow下的一个issue, 标题是(Reland: [XLA:GPU] Enable RaggedAllToAll one shot kernel by default.)， 内容是 (Reland: [XLA:GPU] Enable RaggedAllToAll one shot kernel by default. Reverts 2931aad06d9d11a6c028e89a5a061d60e3808193)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Reland: [XLA:GPU] Enable RaggedAllToAll one shot kernel by default.,Reland: [XLA:GPU] Enable RaggedAllToAll one shot kernel by default. Reverts 2931aad06d9d11a6c028e89a5a061d60e3808193,2025-03-14T18:26:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89243
572,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Enable peer access for all collective cliques.)， 内容是 ([XLA:GPU] Enable peer access for all collective cliques. To check for peer access, we need to call CUDA driver for all pairs of devices, so we want to do it only once per clique, if possible. The peer access is currently needed only for oneshot raggedalltoall kernel, but we might want to experiment with other collectives later.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Enable peer access for all collective cliques.,"[XLA:GPU] Enable peer access for all collective cliques. To check for peer access, we need to call CUDA driver for all pairs of devices, so we want to do it only once per clique, if possible. The peer access is currently needed only for oneshot raggedalltoall kernel, but we might want to experiment with other collectives later.",2025-03-14T16:29:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89237
555,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:CPU] Make the small while loop hoisting threshold configurable & increase default to 1024)， 内容是 ([XLA:CPU] Make the small while loop hoisting threshold configurable & increase default to 1024 This change also made an underlying bug apparent where dot ops would call into Eigen runtime and crash, this resolves that by not hoisting while loops which transitively call dot.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA:CPU] Make the small while loop hoisting threshold configurable & increase default to 1024,"[XLA:CPU] Make the small while loop hoisting threshold configurable & increase default to 1024 This change also made an underlying bug apparent where dot ops would call into Eigen runtime and crash, this resolves that by not hoisting while loops which transitively call dot.",2025-03-14T10:01:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89223
905,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #23688: [ROCm] Triton performance fixes)， 内容是 (PR CC(remove reduce_ from established functions, make it closer to numpy): [ROCm] Triton performance fixes Imported from GitHub PR https://github.com/openxla/xla/pull/23688 Copybara import of the project:  f6998514cd08d018a313294f6974ccab674525bb by Dragan Mladjenovic : [ROCm] Apply precise block size metadata  bdcba45f58f7ad40e6ee8e8d2afd0c04956b34d5 by Dragan Mladjenovic : [ROCm] Pass correct warp size to Triton pipeline Merging this change closes CC(remove reduce_ from established functions, make it closer to numpy) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23688 from ROCm:ci_rocm_triton_perf_fixes bdcba45f58f7ad40e6ee8e8d2afd0c04956b34d5)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],PR #23688: [ROCm] Triton performance fixes,"PR CC(remove reduce_ from established functions, make it closer to numpy): [ROCm] Triton performance fixes Imported from GitHub PR https://github.com/openxla/xla/pull/23688 Copybara import of the project:  f6998514cd08d018a313294f6974ccab674525bb by Dragan Mladjenovic : [ROCm] Apply precise block size metadata  bdcba45f58f7ad40e6ee8e8d2afd0c04956b34d5 by Dragan Mladjenovic : [ROCm] Pass correct warp size to Triton pipeline Merging this change closes CC(remove reduce_ from established functions, make it closer to numpy) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23688 from ROCm:ci_rocm_triton_perf_fixes bdcba45f58f7ad40e6ee8e8d2afd0c04956b34d5",2025-03-14T09:00:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89222
417,"以下是一个github上的tensorflow下的一个issue, 标题是(Qualcomm AI Engine Direct - Provide op optimization)， 内容是 (Summary:  Support FC>CONV2D optimization  Add CONV2D op builder with CPU transpose Test:  [x] a8w8 Gemma3 compile and execution successfully  [x] qnn_compiler_plugin_test all pass)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,jiunkaiy,Qualcomm AI Engine Direct - Provide op optimization,Summary:  Support FC>CONV2D optimization  Add CONV2D op builder with CPU transpose Test:  [x] a8w8 Gemma3 compile and execution successfully  [x] qnn_compiler_plugin_test all pass,2025-03-14T07:08:28Z,awaiting review ready to pull size:L,closed,0,2,https://github.com/tensorflow/tensorflow/issues/89210,Any test results?,> Any test results? I have left some comments in the first conversation block. Thanks!
333,"以下是一个github上的tensorflow下的一个issue, 标题是(Preserve `mhlo.frontend_attributes` when legalizing to `mhlo.ragged_dot`.)， 内容是 (Preserve `mhlo.frontend_attributes` when legalizing to `mhlo.ragged_dot`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Preserve `mhlo.frontend_attributes` when legalizing to `mhlo.ragged_dot`.,Preserve `mhlo.frontend_attributes` when legalizing to `mhlo.ragged_dot`.,2025-03-13T22:20:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89192
513,"以下是一个github上的tensorflow下的一个issue, 标题是(Update PyArray::BatchedDevicePut to support zero buffers and destination devices. Use batched_device_put instead of ArrayImpl to build arrays with no local shards.)， 内容是 (Update PyArray::BatchedDevicePut to support zero buffers and destination devices. Use batched_device_put instead of ArrayImpl to build arrays with no local shards.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Update PyArray::BatchedDevicePut to support zero buffers and destination devices. Use batched_device_put instead of ArrayImpl to build arrays with no local shards.,Update PyArray::BatchedDevicePut to support zero buffers and destination devices. Use batched_device_put instead of ArrayImpl to build arrays with no local shards.,2025-03-13T19:16:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89180
616,"以下是一个github上的tensorflow下的一个issue, 标题是([xla:copy_insertion] Fix a problem in handling Send feeding into a while-loop.)， 内容是 ([xla:copy_insertion] Fix a problem in handling Send feeding into a whileloop. Previously, we only make a copy of the operand for rotated Send/SendDone inside a whileloop. We now also make a copy of the operand for the Send feeding into a whileloop so that the buffer for the operand does not have to hold different values with live range interference.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[xla:copy_insertion] Fix a problem in handling Send feeding into a while-loop.,"[xla:copy_insertion] Fix a problem in handling Send feeding into a whileloop. Previously, we only make a copy of the operand for rotated Send/SendDone inside a whileloop. We now also make a copy of the operand for the Send feeding into a whileloop so that the buffer for the operand does not have to hold different values with live range interference.",2025-03-13T18:19:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89174
303,"以下是一个github上的tensorflow下的一个issue, 标题是([xla:gpu] Remove mentions of NCCL from RaggedAllToAllThunk)， 内容是 ([xla:gpu] Remove mentions of NCCL from RaggedAllToAllThunk)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[xla:gpu] Remove mentions of NCCL from RaggedAllToAllThunk,[xla:gpu] Remove mentions of NCCL from RaggedAllToAllThunk,2025-03-13T15:36:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89155
393,"以下是一个github上的tensorflow下的一个issue, 标题是(Revert: [XLA:GPU] Enable RaggedAllToAll one shot kernel by default.)， 内容是 (Revert: [XLA:GPU] Enable RaggedAllToAll one shot kernel by default. Breaks internal tests. Reverts a92cc3d7cff250c40fce23067b0a2e32f96a39f7)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Revert: [XLA:GPU] Enable RaggedAllToAll one shot kernel by default.,Revert: [XLA:GPU] Enable RaggedAllToAll one shot kernel by default. Breaks internal tests. Reverts a92cc3d7cff250c40fce23067b0a2e32f96a39f7,2025-03-13T14:35:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89149
1217,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #23320: [NVIDIA GPU] Add utility functions for multi-host fast-interconnect domain)， 内容是 (PR CC([Feature Request] Addition of new operation to Tensorflow Lite for ""ENet""): [NVIDIA GPU] Add utility functions for multihost fastinterconnect domain Imported from GitHub PR https://github.com/openxla/xla/pull/23320 On Blackwell and onwards, multiple hosts may be connected by NVLink thus creating a fastinterconnect domain beyond a single node. This PR leverages NVML APIs to detect the actual fastinterconnect domain. Followup PRs will update the logic in PjRt client on top of these utility functions. Copybara import of the project:  ee084714876336243fa3ba52a857d2d41124b730 by Terry Sun : pipe nvml to detect nvl  36e1f71dfabb75dbd0853355a7be8c0d6d103903 by Terry Sun : convert clusterUUID  917eb219a1bdebfc7a12f5849545615c83092c73 by Terry Sun : bake boot id into key  cbd909ab66ccf47f6e140632645fbb4d6c61c7c3 by Terry Sun : use global id and add getter  e487c4718eefe72e77708ffaf2c8cbbf6c979e8e by Terry Sun : use device ordinal and )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],PR #23320: [NVIDIA GPU] Add utility functions for multi-host fast-interconnect domain,"PR CC([Feature Request] Addition of new operation to Tensorflow Lite for ""ENet""): [NVIDIA GPU] Add utility functions for multihost fastinterconnect domain Imported from GitHub PR https://github.com/openxla/xla/pull/23320 On Blackwell and onwards, multiple hosts may be connected by NVLink thus creating a fastinterconnect domain beyond a single node. This PR leverages NVML APIs to detect the actual fastinterconnect domain. Followup PRs will update the logic in PjRt client on top of these utility functions. Copybara import of the project:  ee084714876336243fa3ba52a857d2d41124b730 by Terry Sun : pipe nvml to detect nvl  36e1f71dfabb75dbd0853355a7be8c0d6d103903 by Terry Sun : convert clusterUUID  917eb219a1bdebfc7a12f5849545615c83092c73 by Terry Sun : bake boot id into key  cbd909ab66ccf47f6e140632645fbb4d6c61c7c3 by Terry Sun : use global id and add getter  e487c4718eefe72e77708ffaf2c8cbbf6c979e8e by Terry Sun : use device ordinal and ",2025-03-13T11:13:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89140
1171,"以下是一个github上的tensorflow下的一个issue, 标题是([xla:cpu] enable scatter fusion emitter)， 内容是 ([xla:cpu] enable scatter fusion emitter Known issues:  bf16 performance is poor. This is because in the emitters   we are missing an optimization that we have in XLA thunks.   We will fix this soon.  No parallel scatter. We are leaving this as future work   since the singlethreaded implementation is already bringing   significant performance improvements. Scatter microbenchmarks:  The gap from a few days ago was wider (geomean improvement of 76%), but the recent work on improving performance of small while loops (https://github.com/openxla/xla/commit/db734148ec74) narrowed that to the numbers above. Legacy emitters (""nothunks"") compile all while loops, and are therefore a tougher baseline to compare against. Still, scatter fusion emitters are faster (all singlethreaded):  For the rest of the microbenchmarks, the scatter emitter does not affect performance, except when bf16 is involved due to the known issue mentioned above.  Microb)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,copybara-service[bot],[xla:cpu] enable scatter fusion emitter,"[xla:cpu] enable scatter fusion emitter Known issues:  bf16 performance is poor. This is because in the emitters   we are missing an optimization that we have in XLA thunks.   We will fix this soon.  No parallel scatter. We are leaving this as future work   since the singlethreaded implementation is already bringing   significant performance improvements. Scatter microbenchmarks:  The gap from a few days ago was wider (geomean improvement of 76%), but the recent work on improving performance of small while loops (https://github.com/openxla/xla/commit/db734148ec74) narrowed that to the numbers above. Legacy emitters (""nothunks"") compile all while loops, and are therefore a tougher baseline to compare against. Still, scatter fusion emitters are faster (all singlethreaded):  For the rest of the microbenchmarks, the scatter emitter does not affect performance, except when bf16 is involved due to the known issue mentioned above.  Microb",2025-03-13T02:19:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89124
593,"以下是一个github上的tensorflow下的一个issue, 标题是([xla:cpu] flip xla_cpu_use_fusion_emitters to true)， 内容是 ([xla:cpu] flip xla_cpu_use_fusion_emitters to true Note, however, that no fusion emitter is enabled yet. Flipping this flag does change the HLO pipeline, and is therefore worth submitting as its own CL to get test coverage and measure performance differences. The largest perf difference can be seen in Gather:  Microbenchmarks thunks vs. fusion emitters  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[xla:cpu] flip xla_cpu_use_fusion_emitters to true,"[xla:cpu] flip xla_cpu_use_fusion_emitters to true Note, however, that no fusion emitter is enabled yet. Flipping this flag does change the HLO pipeline, and is therefore worth submitting as its own CL to get test coverage and measure performance differences. The largest perf difference can be seen in Gather:  Microbenchmarks thunks vs. fusion emitters  ",2025-03-13T02:17:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89123
245,"以下是一个github上的tensorflow下的一个issue, 标题是(Extending op coverage for TFL)， 内容是 (Extending op coverage for TFL)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Extending op coverage for TFL,Extending op coverage for TFL,2025-03-12T23:34:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89113
269,"以下是一个github上的tensorflow下的一个issue, 标题是(Fix grammar mistake and rewrite paragraph)， 内容是 (Fix grammar mistake and rewrite paragraph)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Fix grammar mistake and rewrite paragraph,Fix grammar mistake and rewrite paragraph,2025-03-12T23:07:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89112
1232,"以下是一个github上的tensorflow下的一个issue, 标题是(ImportError: DLL load failed while importing _pywrap_tensorflow_internal on Windows (TensorFlow CPU))， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.9  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am facing an issue while trying to import TensorFlow on a CPU machine. The error traceback suggests that a DLL load failed while importing _pywrap_tensorflow_internal, which prevents TensorFlow from initializing properly. Traceback (most recent call last):   File ""C:\Users\gagan\Desktop\EGU_replication\venv\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization rout)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,GaganaMD,ImportError: DLL load failed while importing _pywrap_tensorflow_internal on Windows (TensorFlow CPU)," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.9  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am facing an issue while trying to import TensorFlow on a CPU machine. The error traceback suggests that a DLL load failed while importing _pywrap_tensorflow_internal, which prevents TensorFlow from initializing properly. Traceback (most recent call last):   File ""C:\Users\gagan\Desktop\EGU_replication\venv\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization rout",2025-03-12T10:23:53Z,type:bug,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89072,TensorFlow version: 2.18.0,Please search for duplicates before opening a new issue,Are you satisfied with the resolution of your issue? Yes No,Update the MSVC like this download link ：https://download.visualstudio.microsoft.com/download/pr/285b28c73cf947fb9be801cf5323a8df/8F9FB1B3CFE6E5092CF1225ECD6659DAB7CE50B8BF935CB79BFEDE1F3C895240/VC_redist.x64.exe
607,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Enable RaggedAllToAll one-shot kernel by default.)， 内容是 ([XLA:GPU] Enable RaggedAllToAll oneshot kernel by default. Oneshot kernel is the most performant and stable implementation of RaggedAllToAll that we have right now. NCCL version is currently unstable and sometimes causes deadlocks. Enabling the flag will make singlehost usage of RaggedAllToAll stable by default. Multihost case is currently not well supported.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Enable RaggedAllToAll one-shot kernel by default.,[XLA:GPU] Enable RaggedAllToAll oneshot kernel by default. Oneshot kernel is the most performant and stable implementation of RaggedAllToAll that we have right now. NCCL version is currently unstable and sometimes causes deadlocks. Enabling the flag will make singlehost usage of RaggedAllToAll stable by default. Multihost case is currently not well supported.,2025-03-12T09:59:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89069
799,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #23613: [ROCm] Pass correct warp size to Triton pipeline)， 内容是 (PR CC(ERROR: Config value opt is not defined in any .rc file): [ROCm] Pass correct warp size to Triton pipeline Imported from GitHub PR https://github.com/openxla/xla/pull/23613 Copybara import of the project:  dc43a7518d690038398ae3cf301de477d1ca715f by Dragan Mladjenovic : [ROCm] Pass correct warp size to Triton pipeline Merging this change closes CC(ERROR: Config value opt is not defined in any .rc file) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23613 from ROCm:triton_wrap_size dc43a7518d690038398ae3cf301de477d1ca715f)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],PR #23613: [ROCm] Pass correct warp size to Triton pipeline,PR CC(ERROR: Config value opt is not defined in any .rc file): [ROCm] Pass correct warp size to Triton pipeline Imported from GitHub PR https://github.com/openxla/xla/pull/23613 Copybara import of the project:  dc43a7518d690038398ae3cf301de477d1ca715f by Dragan Mladjenovic : [ROCm] Pass correct warp size to Triton pipeline Merging this change closes CC(ERROR: Config value opt is not defined in any .rc file) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23613 from ROCm:triton_wrap_size dc43a7518d690038398ae3cf301de477d1ca715f,2025-03-12T09:48:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89067
227,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal change only)， 内容是 (Internal change only)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal change only,Internal change only,2025-03-12T01:45:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89052
1203,"以下是一个github上的tensorflow下的一个issue, 标题是(Allow appending a custom error message when using error checking macros & other improvements.)， 内容是 (Allow appending a custom error message when using error checking macros & other improvements.  Rename and move `ErrorStatusReturnHelper` to `litert::ErrorStatusBuilder`.  Add extra logging capabilities to `litert::ErrorStatusBuilder`.   It is now possible to stream data to the builder. This creates an extra   message that is appended to the original error message.     Refactor `LITERT_RETURN_IF_ERROR` so that the default `ErrorStatusBuilder`   can be used (see example above).  Refactor `LITERT_ASSIGN_OR_RETURN` so that the return expression can   reference a variable called `_` that holds an `ErrorStatusBuilder` built with   the result of the expression.     In functions returning a `LiteRtStatus`, this logs the message automatically   upon conversion.   This makes it possible to easily log messages.     The log severity upon conversion can be adjusted with the `Log*()` functions   and silenced with `NoLog()`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Allow appending a custom error message when using error checking macros & other improvements.,"Allow appending a custom error message when using error checking macros & other improvements.  Rename and move `ErrorStatusReturnHelper` to `litert::ErrorStatusBuilder`.  Add extra logging capabilities to `litert::ErrorStatusBuilder`.   It is now possible to stream data to the builder. This creates an extra   message that is appended to the original error message.     Refactor `LITERT_RETURN_IF_ERROR` so that the default `ErrorStatusBuilder`   can be used (see example above).  Refactor `LITERT_ASSIGN_OR_RETURN` so that the return expression can   reference a variable called `_` that holds an `ErrorStatusBuilder` built with   the result of the expression.     In functions returning a `LiteRtStatus`, this logs the message automatically   upon conversion.   This makes it possible to easily log messages.     The log severity upon conversion can be adjusted with the `Log*()` functions   and silenced with `NoLog()`.",2025-03-11T15:45:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89023
1173,"以下是一个github上的tensorflow下的一个issue, 标题是(gemma2-2b SavedModel to tflite conversion)， 内容是 (Hello,  I am trying to convert a Gemma Saved Model in tflite format to run on an edge device.  1. System information I am running my code on a Jupyter Notebook. My tensorflow version is 2.18.0  2. Code  Code Here is the code I used once I have my Saved Model :  Since it is too large to compress and I can't share my Saved Model folder, here is how I got it:  I downloaded the Keras model from Kaggle  Here is the code I used to get de Saved Model Colab link  I tried different solutions:   with the converter.target_spec.supported_ops  without the converter.target_spec.supported_ops   only with TFLITE_BUILTINS and only with SELECT_TF_OPS  with an optimization like this converter.optimizations = [tf.lite.Optimize.DEFAULT] And I also tried this code with the method I saw on another resolved issue:   3. Failure after conversion After running it, the code produces a .tflite file but the StridedSlice is not supported, making the tflite model)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,18marie05,gemma2-2b SavedModel to tflite conversion,"Hello,  I am trying to convert a Gemma Saved Model in tflite format to run on an edge device.  1. System information I am running my code on a Jupyter Notebook. My tensorflow version is 2.18.0  2. Code  Code Here is the code I used once I have my Saved Model :  Since it is too large to compress and I can't share my Saved Model folder, here is how I got it:  I downloaded the Keras model from Kaggle  Here is the code I used to get de Saved Model Colab link  I tried different solutions:   with the converter.target_spec.supported_ops  without the converter.target_spec.supported_ops   only with TFLITE_BUILTINS and only with SELECT_TF_OPS  with an optimization like this converter.optimizations = [tf.lite.Optimize.DEFAULT] And I also tried this code with the method I saw on another resolved issue:   3. Failure after conversion After running it, the code produces a .tflite file but the StridedSlice is not supported, making the tflite model",2025-03-11T13:38:13Z,comp:lite TFLiteConverter TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/89018,"Hi, Is there a solution to fix this conversion issue  ? Thank you for your answer"
612,"以下是一个github上的tensorflow下的一个issue, 标题是(Remove TF_CUDNN_USE_FRONTEND env var.)， 内容是 (Remove TF_CUDNN_USE_FRONTEND env var. This environment variable has defaulted to true for over three years, and no known users set it to false. There is a lot of code maintaining the nonfrontend legacy cuDNN API which we'd like to delete. Removing the env var is the first step. I don't remove any code relying on the env var yet, in order to make this CL easier to rollback if necessary.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Remove TF_CUDNN_USE_FRONTEND env var.,"Remove TF_CUDNN_USE_FRONTEND env var. This environment variable has defaulted to true for over three years, and no known users set it to false. There is a lot of code maintaining the nonfrontend legacy cuDNN API which we'd like to delete. Removing the env var is the first step. I don't remove any code relying on the env var yet, in order to make this CL easier to rollback if necessary.",2025-03-11T02:42:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89000
602,"以下是一个github上的tensorflow下的一个issue, 标题是(Remove flag --xla_gpu_enable_cudnn_frontend.)， 内容是 (Remove flag xla_gpu_enable_cudnn_frontend. This flag has defaulted to true for over three years, and no known users set it to false. There is a lot of code maintaining the nonfrontend legacy cuDNN API which we'd like to delete. Removing the flag is the first step. I don't remove any code relying on the flag yet, in order to make this CL easier to rollback if necessary.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Remove flag --xla_gpu_enable_cudnn_frontend.,"Remove flag xla_gpu_enable_cudnn_frontend. This flag has defaulted to true for over three years, and no known users set it to false. There is a lot of code maintaining the nonfrontend legacy cuDNN API which we'd like to delete. Removing the flag is the first step. I don't remove any code relying on the flag yet, in order to make this CL easier to rollback if necessary.",2025-03-11T00:46:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88996
323,"以下是一个github上的tensorflow下的一个issue, 标题是(Delete unused IFRT proxy backend factory in `ifrt_proxy_internal.py`)， 内容是 (Delete unused IFRT proxy backend factory in `ifrt_proxy_internal.py`)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Delete unused IFRT proxy backend factory in `ifrt_proxy_internal.py`,Delete unused IFRT proxy backend factory in `ifrt_proxy_internal.py`,2025-03-10T22:17:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88984
281,"以下是一个github上的tensorflow下的一个issue, 标题是(Define CAPI and Python API for chlo.ragged_dot.)， 内容是 (Define CAPI and Python API for chlo.ragged_dot.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Define CAPI and Python API for chlo.ragged_dot.,Define CAPI and Python API for chlo.ragged_dot.,2025-03-10T21:00:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88976
931,"以下是一个github上的tensorflow下的一个issue, 标题是(could not find registered transfer manager for platform Host -- check target linkage 	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_33145])， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.8  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi since I added some conv2D to my model I am getting this error on the Colab TPU  :    Standalone code to reproduce the issue Here's the Conv2D which I added before getting this Error :  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,nassimus26,could not find registered transfer manager for platform Host -- check target linkage 	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_33145], Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.8  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi since I added some conv2D to my model I am getting this error on the Colab TPU  :    Standalone code to reproduce the issue Here's the Conv2D which I added before getting this Error :  ,2025-03-10T16:27:36Z,stat:awaiting response type:bug stale comp:dist-strat TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88960,"Hi **** , Apologies for the delay, and thank you for raising your concern here. Could you please share the complete code snippet? This would help us investigate the issue more effectively. Additionally, I noticed that you are using an older version of TensorFlow (2.8). I recommend upgrading to the latest version and checking if the issue persists. If the problem still occurs, please let us know, and we will be happy to assist further. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
876,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #23542: [ROCm] Support clang19 as host compiler)， 内容是 (PR CC(No C++ symbols exported after built libtensorflow_cc with bazel on windows): [ROCm] Support clang19 as host compiler Imported from GitHub PR https://github.com/openxla/xla/pull/23542 Pass nocanonicalprefixes to clang19 to get old InstalledDir behavior. Copybara import of the project:  03a6958a7ef6fde43ec6c20c8eb984b4afa181ff by Dragan Mladjenovic : [ROCm] Support clang19 as host compiler Merging this change closes CC(No C++ symbols exported after built libtensorflow_cc with bazel on windows) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23542 from ROCm:ci_clang19 03a6958a7ef6fde43ec6c20c8eb984b4afa181ff)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],PR #23542: [ROCm] Support clang19 as host compiler,PR CC(No C++ symbols exported after built libtensorflow_cc with bazel on windows): [ROCm] Support clang19 as host compiler Imported from GitHub PR https://github.com/openxla/xla/pull/23542 Pass nocanonicalprefixes to clang19 to get old InstalledDir behavior. Copybara import of the project:  03a6958a7ef6fde43ec6c20c8eb984b4afa181ff by Dragan Mladjenovic : [ROCm] Support clang19 as host compiler Merging this change closes CC(No C++ symbols exported after built libtensorflow_cc with bazel on windows) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23542 from ROCm:ci_clang19 03a6958a7ef6fde43ec6c20c8eb984b4afa181ff,2025-03-10T14:05:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88952
1072,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #23552: [ROCm] Add rocm deps for ragged_all_to_all_kernel)， 内容是 (PR CC(if i set the inputshape with (None, None, featdim),how can I split the input by second dimention(axis=1)?): [ROCm] Add rocm deps for ragged_all_to_all_kernel Imported from GitHub PR https://github.com/openxla/xla/pull/23552 Kernel added in https://github.com/openxla/xla/commit/be68e80894862fe97757ea2b6110958ef4244c21. For ROCm build is currently failing with:  This PR just adds necessary deps for rocm  Copybara import of the project:  80f96ff15de134bf14ff00a4483f7b6707744445 by Milica Makevic : Add rocm deps for ragged_all_to_all_kernel Merging this change closes CC(if i set the inputshape with (None, None, featdim),how can I split the input by second dimention(axis=1)?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23552 from ROCm:hotfix_250310 80f96ff15de134bf14ff00a4483f7b6707744445)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],PR #23552: [ROCm] Add rocm deps for ragged_all_to_all_kernel,"PR CC(if i set the inputshape with (None, None, featdim),how can I split the input by second dimention(axis=1)?): [ROCm] Add rocm deps for ragged_all_to_all_kernel Imported from GitHub PR https://github.com/openxla/xla/pull/23552 Kernel added in https://github.com/openxla/xla/commit/be68e80894862fe97757ea2b6110958ef4244c21. For ROCm build is currently failing with:  This PR just adds necessary deps for rocm  Copybara import of the project:  80f96ff15de134bf14ff00a4483f7b6707744445 by Milica Makevic : Add rocm deps for ragged_all_to_all_kernel Merging this change closes CC(if i set the inputshape with (None, None, featdim),how can I split the input by second dimention(axis=1)?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23552 from ROCm:hotfix_250310 80f96ff15de134bf14ff00a4483f7b6707744445",2025-03-10T14:00:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88949
1192,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #23468: Ensure PTX version compatibility w/ Clang & ptxas)， 内容是 (PR CC(Add option for inferring op attributes from inputs): Ensure PTX version compatibility w/ Clang & ptxas Imported from GitHub PR https://github.com/openxla/xla/pull/23468 Using the flag `cudafeature=+ptx`, Clang can be instructed to emit a specific PTX version from the NVPTX backend. If this flag is omitted, then Clang might emit a newer version of PTX than what ptxas from Hermetic CUDA can recognize which can lead to compilation errors. This commit adds a mapping from Clang & CUDA version to PTX version in `third_party/gpus/cuda/hermetic/cuda_redist_versions.bzl` which will need to be updated over time. If either the version for Clang or CUDA cannot be mapped to a PTX version, then configuration will fail. Resolves openxla/xla CC(Dataset shard index automatic change in Estimator DistributionStrategy) Copybara import of the project:  49c5940498f608b82539243b286431a74cdfc0dd by Jack Wolfard : Ensure PTX version compatibility w/ )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gpt,copybara-service[bot],PR #23468: Ensure PTX version compatibility w/ Clang & ptxas,"PR CC(Add option for inferring op attributes from inputs): Ensure PTX version compatibility w/ Clang & ptxas Imported from GitHub PR https://github.com/openxla/xla/pull/23468 Using the flag `cudafeature=+ptx`, Clang can be instructed to emit a specific PTX version from the NVPTX backend. If this flag is omitted, then Clang might emit a newer version of PTX than what ptxas from Hermetic CUDA can recognize which can lead to compilation errors. This commit adds a mapping from Clang & CUDA version to PTX version in `third_party/gpus/cuda/hermetic/cuda_redist_versions.bzl` which will need to be updated over time. If either the version for Clang or CUDA cannot be mapped to a PTX version, then configuration will fail. Resolves openxla/xla CC(Dataset shard index automatic change in Estimator DistributionStrategy) Copybara import of the project:  49c5940498f608b82539243b286431a74cdfc0dd by Jack Wolfard : Ensure PTX version compatibility w/ ",2025-03-10T07:38:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88939
229,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal change only.)， 内容是 (Internal change only.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal change only.,Internal change only.,2025-03-10T06:42:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88937
1190,"以下是一个github上的tensorflow下的一个issue, 标题是(Fixed-point Softmax() calls exp_on_negative_values() twice)， 内容是 (Fixedpoint `Softmax()` calls `exp_on_negative_values()` twice per element: once to calculate the value of the exp of each element https://github.com/tensorflow/tensorflow/blob/4266e9ab53bb3f7fca0dd816b965b82862b69b4e/tensorflow/lite/kernels/internal/reference/softmax.hL131 and another one a few lines earlier to calculate the sum of all exps https://github.com/tensorflow/tensorflow/blob/4266e9ab53bb3f7fca0dd816b965b82862b69b4e/tensorflow/lite/kernels/internal/reference/softmax.hL109L110 The `exp_on_negative_values()` function can be rather slow, taking most of the time of `Softmax()` (which can be especially concerning in embedded implementations), so calling it twice per element makes the whole `Softmax()` function be about twice as slow as necessary. Could this be optimized so that it is only called once per element?  Store the exp results in a temporary array (for example, reusing `output_data`), computing the sum as they're stor)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,cousteaulecommandant,Fixed-point Softmax() calls exp_on_negative_values() twice,"Fixedpoint `Softmax()` calls `exp_on_negative_values()` twice per element: once to calculate the value of the exp of each element https://github.com/tensorflow/tensorflow/blob/4266e9ab53bb3f7fca0dd816b965b82862b69b4e/tensorflow/lite/kernels/internal/reference/softmax.hL131 and another one a few lines earlier to calculate the sum of all exps https://github.com/tensorflow/tensorflow/blob/4266e9ab53bb3f7fca0dd816b965b82862b69b4e/tensorflow/lite/kernels/internal/reference/softmax.hL109L110 The `exp_on_negative_values()` function can be rather slow, taking most of the time of `Softmax()` (which can be especially concerning in embedded implementations), so calling it twice per element makes the whole `Softmax()` function be about twice as slow as necessary. Could this be optimized so that it is only called once per element?  Store the exp results in a temporary array (for example, reusing `output_data`), computing the sum as they're stor",2025-03-07T21:48:15Z,comp:lite,open,0,0,https://github.com/tensorflow/tensorflow/issues/88861
643,"以下是一个github上的tensorflow下的一个issue, 标题是(Deprecate `HloTestBase`.)， 内容是 (Deprecate `HloTestBase`. New tests should utilize `HloPjRtTestBase` where possible. If `RunAndCompare` functionality is required, tests can instantiate `HloPjRtInterpreterReferenceMixin`. Please see `HloRunnerAgnosticTestBase` and `HloRunnerAgnosticReferenceMixin` for generic implementations that can be subclassed for different use cases, if your `HloPjRtTestBase` and/or `HloPjRtInterpreterReferenceMixin` do not meet your needs.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Deprecate `HloTestBase`.,"Deprecate `HloTestBase`. New tests should utilize `HloPjRtTestBase` where possible. If `RunAndCompare` functionality is required, tests can instantiate `HloPjRtInterpreterReferenceMixin`. Please see `HloRunnerAgnosticTestBase` and `HloRunnerAgnosticReferenceMixin` for generic implementations that can be subclassed for different use cases, if your `HloPjRtTestBase` and/or `HloPjRtInterpreterReferenceMixin` do not meet your needs.",2025-03-07T19:59:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88851
689,"以下是一个github上的tensorflow下的一个issue, 标题是(#litert Detect memory sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`.)， 内容是 (litert Detect memory sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`. Trying to load a library using `RTLD_DEEPBIND` is not supported by memory sanitizers. In an effort to enable testing we strip the flag. If this leads to unintended behaviour, either remove the `RTLD_DEEPBIND` flag or run without a memory sanitizer. See https://github.com/google/sanitizers/issues/611 for more information.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],#litert Detect memory sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`.,"litert Detect memory sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`. Trying to load a library using `RTLD_DEEPBIND` is not supported by memory sanitizers. In an effort to enable testing we strip the flag. If this leads to unintended behaviour, either remove the `RTLD_DEEPBIND` flag or run without a memory sanitizer. See https://github.com/google/sanitizers/issues/611 for more information.",2025-03-07T17:47:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88843
448,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Add one-shot kernel implementation to RaggedAllToAll.)， 内容是 ([XLA:GPU] Add oneshot kernel implementation to RaggedAllToAll. The kernel uses a CUDA kernel for an efficient implementation of ra2a on single host when direct peer access between GPUs is available.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Add one-shot kernel implementation to RaggedAllToAll.,[XLA:GPU] Add oneshot kernel implementation to RaggedAllToAll. The kernel uses a CUDA kernel for an efficient implementation of ra2a on single host when direct peer access between GPUs is available.,2025-03-07T17:37:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88841
1193,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #23477: [Hermetic CUDA] Skip arch if not in redistrib json)， 内容是 (PR CC(Tensorflow's Estimator stops training): [Hermetic CUDA] Skip arch if not in redistrib json Imported from GitHub PR https://github.com/openxla/xla/pull/23477 Given a custom CUDA redistribution which only specifies a distribution for `linuxx86_64` and not `linuxsbsa` nor `linuxaarch64`, a key error occurs due to `_get_redistribution_urls` expecting all architectures to be present for each subproject.  This can be resolved by adding a key for each arch mapping to an empty dict if there is no artifact for that arch. However, this solution is awkward and adds clutter to the custom CUDA redistribution.  Instead, allow for `_get_redistribution_urls` to skip the arch if it is not present in the redistribution. Copybara import of the project:  2a00755a0aba650588b2ba22340990980f18b472 by Jack Wolfard : [Hermetic CUDA] Skip arch if not in redistrib json Given a custom CUDA redistribution which only specifies a distribution for `linuxx86)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],PR #23477: [Hermetic CUDA] Skip arch if not in redistrib json,"PR CC(Tensorflow's Estimator stops training): [Hermetic CUDA] Skip arch if not in redistrib json Imported from GitHub PR https://github.com/openxla/xla/pull/23477 Given a custom CUDA redistribution which only specifies a distribution for `linuxx86_64` and not `linuxsbsa` nor `linuxaarch64`, a key error occurs due to `_get_redistribution_urls` expecting all architectures to be present for each subproject.  This can be resolved by adding a key for each arch mapping to an empty dict if there is no artifact for that arch. However, this solution is awkward and adds clutter to the custom CUDA redistribution.  Instead, allow for `_get_redistribution_urls` to skip the arch if it is not present in the redistribution. Copybara import of the project:  2a00755a0aba650588b2ba22340990980f18b472 by Jack Wolfard : [Hermetic CUDA] Skip arch if not in redistrib json Given a custom CUDA redistribution which only specifies a distribution for `linuxx86",2025-03-07T16:38:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88836
1109,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #22541: [ROCm] Cleanup atomics support)， 内容是 (PR CC(Java process crashes during model loading): [ROCm] Cleanup atomics support Imported from GitHub PR https://github.com/openxla/xla/pull/22541 Weaken the ordering barriers to match what atomicAdd does on rocm. Emulate fp16 atomic on top of packed fp16 atomic where possible. Also for bfloat16 atomics, albeit those don't get matched right now due to FloatNormalization. Left in support for fp16 and bfloat16 vector atomics. We might enable the vectorization for them in the future if we can prove the access satisfies 4byte aligment. Copybara import of the project:  06907ef930c76c824788e86db8d3b30eeb141175 by Dragan Mladjenovic : [ROCm] Cleanup atomics support Merging this change closes CC(Java process crashes during model loading) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22541 from ROCm:atomics_cleanup 06907ef930c76c824788e86db8d3b30eeb141175)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],PR #22541: [ROCm] Cleanup atomics support,"PR CC(Java process crashes during model loading): [ROCm] Cleanup atomics support Imported from GitHub PR https://github.com/openxla/xla/pull/22541 Weaken the ordering barriers to match what atomicAdd does on rocm. Emulate fp16 atomic on top of packed fp16 atomic where possible. Also for bfloat16 atomics, albeit those don't get matched right now due to FloatNormalization. Left in support for fp16 and bfloat16 vector atomics. We might enable the vectorization for them in the future if we can prove the access satisfies 4byte aligment. Copybara import of the project:  06907ef930c76c824788e86db8d3b30eeb141175 by Dragan Mladjenovic : [ROCm] Cleanup atomics support Merging this change closes CC(Java process crashes during model loading) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22541 from ROCm:atomics_cleanup 06907ef930c76c824788e86db8d3b30eeb141175",2025-03-07T14:45:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88828
1168,"以下是一个github上的tensorflow下的一个issue, 标题是(cuSteamSynchronize take tons of time)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.14  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.6  GPU model and memory 4070S  Current behavior? I am trying to benchmark the inference.  A simple code as   The inference is run by  The profile is collected by `nsys profile w true t cuda,nvtx,cudnn,cublas f true x true o profile_c /opt/tensorflow/tensorflowsource/bazelbin/tensorflow/examples/image_classification/MiniBatch/mini_tftrt model_path=""./resnet50_saved_model_RT"" batch_size=64 output_to_host=False` And I found that cuSteamSynchronize takes most of time, as shown below: profile_c.zip !Image I think the real computation is done and the GPU is wasting its time. Is that right? I don't see any other kernel worki)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,xxHn-pro,cuSteamSynchronize take tons of time," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.14  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.6  GPU model and memory 4070S  Current behavior? I am trying to benchmark the inference.  A simple code as   The inference is run by  The profile is collected by `nsys profile w true t cuda,nvtx,cudnn,cublas f true x true o profile_c /opt/tensorflow/tensorflowsource/bazelbin/tensorflow/examples/image_classification/MiniBatch/mini_tftrt model_path=""./resnet50_saved_model_RT"" batch_size=64 output_to_host=False` And I found that cuSteamSynchronize takes most of time, as shown below: profile_c.zip !Image I think the real computation is done and the GPU is wasting its time. Is that right? I don't see any other kernel worki",2025-03-07T04:27:50Z,type:bug TF2.14,open,0,2,https://github.com/tensorflow/tensorflow/issues/88796,"Hi **pro** , Apologies for the delay, and thanks for raising your concern here.I noticed a version compatibility issue. I am attaching the official documentation for your reference, please verify all compatibility requirements. Additionally, I see that you are using an older version of TensorFlow (2.14). Could you please try updating to the latest version for better results? Thank you!","Hi  , Thanks for your reply. Actually I try the same code at other machine. And there are more information from the new test, which indicate that CUDA kernel is running all the time during the waiting of tensorRT kernel. I think the tensorflow is running asynchronously. It turns out that is not a problem of cuSteamSynchronize !Image PS, the new machine with better GPU uses singularity to run the docker. It runs real CentOS Linux instead of WSL. It is still not clear why I can not get all information at my first try. Anyway, Thanks you. If there is any misunderstand above, pls correct it."
1144,"以下是一个github上的tensorflow下的一个issue, 标题是(import error)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.8  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior?  ImportError                               Traceback (most recent call last) File c:\Users\SUJITH\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jsuj1th,import error, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.8  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior?  ImportError                               Traceback (most recent call last) File c:\Users\SUJITH\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the,2025-03-06T23:45:28Z,stat:awaiting response type:build/install stale TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88780,"Hi ****, Apologies for the delay, and thanks for raising your concern here. I noticed that you are using an older version of TensorFlow (2.8). Could you please check with the latest version for better results? Also, please verify all compatibility requirements. I am attaching the official documentation for your reference. Additionally, a similar issue is currently being discussed, please follow that thread for further updates. CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Please always search for duplicate issues. Please do a minimum of effort for that and for properly formatting the issue.,Are you satisfied with the resolution of your issue? Yes No
229,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal change only.)， 内容是 (Internal change only.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal change only.,Internal change only.,2025-03-06T23:30:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88777
399,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal change only.)， 内容是 (Internal change only. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88221 from jiunkaiy:dev/weilhuan/more_op_builders 542a108226dcb1c31abc30578cba2b74b20e8e0b)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal change only.,Internal change only. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88221 from jiunkaiy:dev/weilhuan/more_op_builders 542a108226dcb1c31abc30578cba2b74b20e8e0b,2025-03-06T22:44:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88774
1212,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #21638: Add the hlo verifier before host offloader to check host memory space)， 内容是 (PR CC([XLA] ResourceExhaustedError when trying to define a Sequential model in Keras under jit_scope context manager): Add the hlo verifier before host offloader to check host memory space Imported from GitHub PR https://github.com/openxla/xla/pull/21638 Ensure No Instructions Have Host Memory Space S(5) Before Host Offloader This change verifies that no instruction possesses host memory space S(5) prior to the host offloader pass. It addresses an issue where the HLO passes before the host offloader could inadvertently leak memory space annotations from the entry computation layout to the graph. In PR https://github.com/openxla/xla/pull/20426, the layout assignment pass was corrected to prevent instructions from inheriting memory space S(5) from the entry computation layout. This commit further ensures that such annotations are not propagated, keeping host memory space not changed until the host offloader pass. Copybara import of t)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #21638: Add the hlo verifier before host offloader to check host memory space,"PR CC([XLA] ResourceExhaustedError when trying to define a Sequential model in Keras under jit_scope context manager): Add the hlo verifier before host offloader to check host memory space Imported from GitHub PR https://github.com/openxla/xla/pull/21638 Ensure No Instructions Have Host Memory Space S(5) Before Host Offloader This change verifies that no instruction possesses host memory space S(5) prior to the host offloader pass. It addresses an issue where the HLO passes before the host offloader could inadvertently leak memory space annotations from the entry computation layout to the graph. In PR https://github.com/openxla/xla/pull/20426, the layout assignment pass was corrected to prevent instructions from inheriting memory space S(5) from the entry computation layout. This commit further ensures that such annotations are not propagated, keeping host memory space not changed until the host offloader pass. Copybara import of t",2025-03-06T20:33:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88768
702,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #23395: [ROCm] gfx950 support)， 内容是 (PR CC(deploying the Tensorflow model in Python): [ROCm] gfx950 support Imported from GitHub PR https://github.com/openxla/xla/pull/23395 rotation please have a look Thanks Copybara import of the project:  6c6bfad5a896154c1a21c263cda433253e9f8597 by Chao Chen : gfx950 support Merging this change closes CC(deploying the Tensorflow model in Python) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23395 from ROCm:ci_gfx950 6c6bfad5a896154c1a21c263cda433253e9f8597)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #23395: [ROCm] gfx950 support,PR CC(deploying the Tensorflow model in Python): [ROCm] gfx950 support Imported from GitHub PR https://github.com/openxla/xla/pull/23395 rotation please have a look Thanks Copybara import of the project:  6c6bfad5a896154c1a21c263cda433253e9f8597 by Chao Chen : gfx950 support Merging this change closes CC(deploying the Tensorflow model in Python) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23395 from ROCm:ci_gfx950 6c6bfad5a896154c1a21c263cda433253e9f8597,2025-03-06T19:48:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88765
331,"以下是一个github上的tensorflow下的一个issue, 标题是(Define lax.ragged_dot_general and express lax.ragged_dot in terms of it.)， 内容是 (Define lax.ragged_dot_general and express lax.ragged_dot in terms of it.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Define lax.ragged_dot_general and express lax.ragged_dot in terms of it.,Define lax.ragged_dot_general and express lax.ragged_dot in terms of it.,2025-03-06T19:18:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88760
467,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Add RaggedAllToAll CUDA kernel.)， 内容是 ([XLA:GPU] Add RaggedAllToAll CUDA kernel. The kernel will be used in RaggedAllToAll thunk for singlehost collectives. Runtime is responsible for communication, synchronization and exchange of pointer. The kernel only need to move the data.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Add RaggedAllToAll CUDA kernel.,"[XLA:GPU] Add RaggedAllToAll CUDA kernel. The kernel will be used in RaggedAllToAll thunk for singlehost collectives. Runtime is responsible for communication, synchronization and exchange of pointer. The kernel only need to move the data.",2025-03-06T19:02:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88758
694,"以下是一个github上的tensorflow下的一个issue, 标题是(#litert Detect address sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`.)， 内容是 (litert Detect address sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`. Trying to load a library using `RTLD_DEEPBIND` is not supported by address sanitizers. In an effort to enable testing we strip the flag. If this leads to unintended behaviour, either remove the `RTLD_DEEPBIND` flag or run without an address sanitizer. See https://github.com/google/sanitizers/issues/611 for more information.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],#litert Detect address sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`.,"litert Detect address sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`. Trying to load a library using `RTLD_DEEPBIND` is not supported by address sanitizers. In an effort to enable testing we strip the flag. If this leads to unintended behaviour, either remove the `RTLD_DEEPBIND` flag or run without an address sanitizer. See https://github.com/google/sanitizers/issues/611 for more information.",2025-03-06T15:46:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88743
733,"以下是一个github上的tensorflow下的一个issue, 标题是(Graph Execution Error)， 内容是 ( Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0dev20240917  Custom code Yes  OS platform and distribution Rocky Linux  Mobile device Rocky Linux  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 9.3.0.  GPU model and memory NIVIDIA SMI 560.28.03  Current behavior? The code is showing a Graph execution error while training a model  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,priyanshujiiii,Graph Execution Error, Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0dev20240917  Custom code Yes  OS platform and distribution Rocky Linux  Mobile device Rocky Linux  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 9.3.0.  GPU model and memory NIVIDIA SMI 560.28.03  Current behavior? The code is showing a Graph execution error while training a model  Standalone code to reproduce the issue   Relevant log output ,2025-03-06T06:42:15Z,stat:awaiting response stale type:performance TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88708,"Hi **** , Apologies for the delay, and thanks for raising your concern here. Could you please provide a Colab gist for troubleshooting this issue more accurately? Alternatively, you can share the specific code where you are facing the issue. I attempted to replicate the provided code but encountered a different issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
337,"以下是一个github上的tensorflow下的一个issue, 标题是(Delete `PjRtClient.Defragment`.)， 内容是 (Delete `PjRtClient.Defragment`. The `Defragment` implementation for GPU is in `py_client.cc`, so this should be a noop.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Delete `PjRtClient.Defragment`.,"Delete `PjRtClient.Defragment`. The `Defragment` implementation for GPU is in `py_client.cc`, so this should be a noop.",2025-03-05T19:08:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88642
1196,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #88221: Qualcomm AI Engine Direct - Op Builders for 1P Models)， 内容是 (PR CC(Qualcomm AI Engine Direct  Op Builders for 1P Models): Qualcomm AI Engine Direct  Op Builders for 1P Models Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/88221  WHAT 1. Conv2d 2. DepthwiseConv2d 3. AveragePool 4. MaxPool 5. DepthToSpace 6. SpaceToDepth 7. HardSwish 8. LeakyRelu 9. ResizeBilinear 10. Litert options for these op builders, unit test 11. update qnn_compiler_plugin_test with these op builders  TEST  qnn_compiler_plugin_test   litert_options_test  Copybara import of the project:  542a108226dcb1c31abc30578cba2b74b20e8e0b by weilhuanquic : Qualcomm AI Engine Direct  Op Builders for 1P Models 1. Conv2d 2. DepthwiseConv2d 3. AveragePool 4. MaxPool 5. DepthToSpace 6. SpaceToDepth 7. HardSwish 8. LeakyRelu 9. ResizeBilinear 10. Litert options for these op builders, unit test 11. update qnn_compiler_plugin_test with these op builders Merging this change closes CC(Qualcomm AI Engine Direct  Op Build)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],PR #88221: Qualcomm AI Engine Direct - Op Builders for 1P Models,"PR CC(Qualcomm AI Engine Direct  Op Builders for 1P Models): Qualcomm AI Engine Direct  Op Builders for 1P Models Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/88221  WHAT 1. Conv2d 2. DepthwiseConv2d 3. AveragePool 4. MaxPool 5. DepthToSpace 6. SpaceToDepth 7. HardSwish 8. LeakyRelu 9. ResizeBilinear 10. Litert options for these op builders, unit test 11. update qnn_compiler_plugin_test with these op builders  TEST  qnn_compiler_plugin_test   litert_options_test  Copybara import of the project:  542a108226dcb1c31abc30578cba2b74b20e8e0b by weilhuanquic : Qualcomm AI Engine Direct  Op Builders for 1P Models 1. Conv2d 2. DepthwiseConv2d 3. AveragePool 4. MaxPool 5. DepthToSpace 6. SpaceToDepth 7. HardSwish 8. LeakyRelu 9. ResizeBilinear 10. Litert options for these op builders, unit test 11. update qnn_compiler_plugin_test with these op builders Merging this change closes CC(Qualcomm AI Engine Direct  Op Build",2025-03-05T00:33:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88592
351,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Fix incorrect buffer aliasing in ir_emitter_unnested for RaggedAllToAll.)， 内容是 ([XLA:GPU] Fix incorrect buffer aliasing in ir_emitter_unnested for RaggedAllToAll.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Fix incorrect buffer aliasing in ir_emitter_unnested for RaggedAllToAll.,[XLA:GPU] Fix incorrect buffer aliasing in ir_emitter_unnested for RaggedAllToAll.,2025-03-04T21:45:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88579
727,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Add RaggedAllToAllCanonicalizer pass.)， 内容是 ([XLA:GPU] Add RaggedAllToAllCanonicalizer pass. In too many places we need to assume or work around the element type of offset and size operands of raggedalltoall. It's much easier to do an HLO rewrite. The added converts are tiny and will likely be fused with other operation. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88412 from tensorflow:dependabot/docker/tensorflow/tools/tf_sig_build_dockerfiles/ubuntued1544e 4b6ba37c2667bdfded35b1774ed8ee438bc5fb25)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Add RaggedAllToAllCanonicalizer pass.,[XLA:GPU] Add RaggedAllToAllCanonicalizer pass. In too many places we need to assume or work around the element type of offset and size operands of raggedalltoall. It's much easier to do an HLO rewrite. The added converts are tiny and will likely be fused with other operation. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88412 from tensorflow:dependabot/docker/tensorflow/tools/tf_sig_build_dockerfiles/ubuntued1544e 4b6ba37c2667bdfded35b1774ed8ee438bc5fb25,2025-03-04T13:31:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88553
1269,"以下是一个github上的tensorflow下的一个issue, 标题是([RNN] tensorflow/lite/kernels/transpose.cc:62 op_context->perm->dims->data[0] != dims (3 != 2)Node number 3 (TRANSPOSE) failed to prepare)， 内容是 ( 1. System information  Window 10  PyCharm 2024.2.3  TensorFlow 2.13  Python 3.8  2. Code Keras model convert to TFLite model fail, please help find the debug method.  TensorFlow Model Colab (Train a TensorFlow Keras LSTM Model for SIN funtion regression using random generated dataset) ([TensorFlow Model Colab]: (https://colab.research.google.com/gist/LiuJitai/08faad02c37315eb09576e85f6df44eb/tensorflowdatasets.ipynb)  Keras model convert to TFLite model Code [Keras model convert to TFLite model]  Fail at tflite_model_quant = converter.convert()  Import libraries import logging import tensorflow as tf from tensorflow import keras import numpy as np from tensorflow.keras.models import load_model from sklearn.preprocessing import MinMaxScaler  Helper functions def generate_data(seq_length, sequences_num):     x = []     y = []     for _ in range(sequences_num):         start = np.random.rand() * 2 * np.pi         sequence = np.sin(np)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Liu-Jitai,[RNN] tensorflow/lite/kernels/transpose.cc:62 op_context->perm->dims->data[0] != dims (3 != 2)Node number 3 (TRANSPOSE) failed to prepare," 1. System information  Window 10  PyCharm 2024.2.3  TensorFlow 2.13  Python 3.8  2. Code Keras model convert to TFLite model fail, please help find the debug method.  TensorFlow Model Colab (Train a TensorFlow Keras LSTM Model for SIN funtion regression using random generated dataset) ([TensorFlow Model Colab]: (https://colab.research.google.com/gist/LiuJitai/08faad02c37315eb09576e85f6df44eb/tensorflowdatasets.ipynb)  Keras model convert to TFLite model Code [Keras model convert to TFLite model]  Fail at tflite_model_quant = converter.convert()  Import libraries import logging import tensorflow as tf from tensorflow import keras import numpy as np from tensorflow.keras.models import load_model from sklearn.preprocessing import MinMaxScaler  Helper functions def generate_data(seq_length, sequences_num):     x = []     y = []     for _ in range(sequences_num):         start = np.random.rand() * 2 * np.pi         sequence = np.sin(np",2025-03-04T12:10:24Z,comp:lite TFLiteConverter TF 2.13,closed,0,3,https://github.com/tensorflow/tensorflow/issues/88549,"Hi, Jitai  Thank you for bringing this issue to our attention, I have been able to replicate the same behavior from my end with your provided code snippet but I think there was issue with directly yields samples without a batch dimension: `yield [k]` This produces data with shape (50, 1) causing a dimension mismatch. The model expects 3D inputs but receives 2D data leading to the TRANSPOSE node error so I've modified your provided code for missing batch dimension in the representative dataset and it seems like working as expected please refer this gistfile. Please give it try from your end and see is it working as expected or not ? If I have missed something here please let me know. Thank you for your cooperation and understanding. ",您好，您所发的邮件我已经收到，我会尽快阅读，祝您工作愉快！,The error was solved. Thank you so much for your help! 
424,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Add ragged-all-to-all tests with multiple replica groups.)， 内容是 ([XLA:GPU] Add raggedalltoall tests with multiple replica groups. As a side effect, we can also support tests where devices in a replica group are not in increasing order.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Add ragged-all-to-all tests with multiple replica groups.,"[XLA:GPU] Add raggedalltoall tests with multiple replica groups. As a side effect, we can also support tests where devices in a replica group are not in increasing order.",2025-03-04T10:20:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88547
1209,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #23271: [XLA] Improve GPU memory limit handling and shape size calculation)， 内容是 (PR CC(Improve shape function of tf.sparse_reduce_sum): [XLA] Improve GPU memory limit handling and shape size calculation Imported from GitHub PR https://github.com/openxla/xla/pull/23271 When running MaxText Llama27b with FP16 optimizer state offloading, an out of memory failure occurred. The root cause was the int64_t memory limit being incorrectly calculated and interpreted as a large uint64_t memory limit close to UINT64_MAX, leading to overly aggressive buffer allocation by the LHS. This changelist addresses the out of memory failure by correctly handling the uint64_t memory limit and accurately calculating the device memory limit, excluding host memory space.  Change memory limit type from int64_t to uint64_t to prevent negative values and better represent memory sizes.  Add memory space filtering to exclude host memory when calculating input/output sizes that impact GPU memory usage. This ensures we only count buffers that a)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #23271: [XLA] Improve GPU memory limit handling and shape size calculation,"PR CC(Improve shape function of tf.sparse_reduce_sum): [XLA] Improve GPU memory limit handling and shape size calculation Imported from GitHub PR https://github.com/openxla/xla/pull/23271 When running MaxText Llama27b with FP16 optimizer state offloading, an out of memory failure occurred. The root cause was the int64_t memory limit being incorrectly calculated and interpreted as a large uint64_t memory limit close to UINT64_MAX, leading to overly aggressive buffer allocation by the LHS. This changelist addresses the out of memory failure by correctly handling the uint64_t memory limit and accurately calculating the device memory limit, excluding host memory space.  Change memory limit type from int64_t to uint64_t to prevent negative values and better represent memory sizes.  Add memory space filtering to exclude host memory when calculating input/output sizes that impact GPU memory usage. This ensures we only count buffers that a",2025-03-04T03:53:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88523
1209,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #23271: [XLA] Improve GPU memory limit handling and shape size calculation)， 内容是 (PR CC(Improve shape function of tf.sparse_reduce_sum): [XLA] Improve GPU memory limit handling and shape size calculation Imported from GitHub PR https://github.com/openxla/xla/pull/23271 When running MaxText Llama27b with FP16 optimizer state offloading, an out of memory failure occurred. The root cause was the int64_t memory limit being incorrectly calculated and interpreted as a large uint64_t memory limit close to UINT64_MAX, leading to overly aggressive buffer allocation by the LHS. This changelist addresses the out of memory failure by correctly handling the uint64_t memory limit and accurately calculating the device memory limit, excluding host memory space.  Change memory limit type from int64_t to uint64_t to prevent negative values and better represent memory sizes.  Add memory space filtering to exclude host memory when calculating input/output sizes that impact GPU memory usage. This ensures we only count buffers that a)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #23271: [XLA] Improve GPU memory limit handling and shape size calculation,"PR CC(Improve shape function of tf.sparse_reduce_sum): [XLA] Improve GPU memory limit handling and shape size calculation Imported from GitHub PR https://github.com/openxla/xla/pull/23271 When running MaxText Llama27b with FP16 optimizer state offloading, an out of memory failure occurred. The root cause was the int64_t memory limit being incorrectly calculated and interpreted as a large uint64_t memory limit close to UINT64_MAX, leading to overly aggressive buffer allocation by the LHS. This changelist addresses the out of memory failure by correctly handling the uint64_t memory limit and accurately calculating the device memory limit, excluding host memory space.  Change memory limit type from int64_t to uint64_t to prevent negative values and better represent memory sizes.  Add memory space filtering to exclude host memory when calculating input/output sizes that impact GPU memory usage. This ensures we only count buffers that a",2025-03-03T23:28:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88510
291,"以下是一个github上的tensorflow下的一个issue, 标题是(Added `WatchJobState` to coordination service agent.)， 内容是 (Added `WatchJobState` to coordination service agent.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,copybara-service[bot],Added `WatchJobState` to coordination service agent.,Added `WatchJobState` to coordination service agent.,2025-03-03T22:09:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88500
824,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Use rendezvous to exchange pointers and CUDA events in memcpy ragged-all-to-all.)， 内容是 ([XLA:GPU] Use rendezvous to exchange pointers and CUDA events in memcpy raggedalltoall. The previous implementation used NCCL AllGather to exchange target device pointers and synchronize streams at the start of the kernel, but it didn't synchronize streams at the end of the kernel. Since we use push model for memcpy, it could be that one stream progresses before all updates have arrived. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23351 from ROCm:ci_fix_stringop_trunc_20250304 d039829ba6a45807a13a2230cfb35e17590cd497)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Use rendezvous to exchange pointers and CUDA events in memcpy ragged-all-to-all.,"[XLA:GPU] Use rendezvous to exchange pointers and CUDA events in memcpy raggedalltoall. The previous implementation used NCCL AllGather to exchange target device pointers and synchronize streams at the start of the kernel, but it didn't synchronize streams at the end of the kernel. Since we use push model for memcpy, it could be that one stream progresses before all updates have arrived. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23351 from ROCm:ci_fix_stringop_trunc_20250304 d039829ba6a45807a13a2230cfb35e17590cd497",2025-03-03T17:12:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88487
1162,"以下是一个github上的tensorflow下的一个issue, 标题是(Tensorflow with C++ Builder 12)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version libtensorflowcpuwindowsx86_64.zip  Custom code Yes  OS platform and distribution windows 10  Mobile device N/A  Python version N/A  Bazel version N/A  GCC/compiler version N/A  CUDA/cuDNN version _No response_  GPU model and memory N/A  Current behavior? this simple c file should compile: bcc64 test6.c D__NO_INLINE DWIN64 I ""N:\DOWNLOADS\FASTEST\26WORKS\TOTEST\include"" L ""N:\DOWNLOADS\FASTEST\26WORKS\TOTEST\lib"" l""tensorflow"" v  but it does not: Embarcadero C++ 7.70 for Win64 Copyright (c) 20122024 Embarcadero Technologies, Inc. test6.c: Turbo Incremental Link64 6.99 Copyright (c) 19972024 Embarcadero Technologies, Inc. Error: Unresolved external 'TF_NewGraph' referenced from C:\USERS\USER\APPDATA\LOCAL\TEMP\TEST6AF147F.O if I add: pragma comment(lib, ""tensorflow.lib"")  I got invalid object file tensorflow.dll  Standalone co)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,dicotom,Tensorflow with C++ Builder 12," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version libtensorflowcpuwindowsx86_64.zip  Custom code Yes  OS platform and distribution windows 10  Mobile device N/A  Python version N/A  Bazel version N/A  GCC/compiler version N/A  CUDA/cuDNN version _No response_  GPU model and memory N/A  Current behavior? this simple c file should compile: bcc64 test6.c D__NO_INLINE DWIN64 I ""N:\DOWNLOADS\FASTEST\26WORKS\TOTEST\include"" L ""N:\DOWNLOADS\FASTEST\26WORKS\TOTEST\lib"" l""tensorflow"" v  but it does not: Embarcadero C++ 7.70 for Win64 Copyright (c) 20122024 Embarcadero Technologies, Inc. test6.c: Turbo Incremental Link64 6.99 Copyright (c) 19972024 Embarcadero Technologies, Inc. Error: Unresolved external 'TF_NewGraph' referenced from C:\USERS\USER\APPDATA\LOCAL\TEMP\TEST6AF147F.O if I add: pragma comment(lib, ""tensorflow.lib"")  I got invalid object file tensorflow.dll  Standalone co",2025-03-02T16:05:15Z,stat:awaiting tensorflower type:bug subtype:windows comp:core,open,0,0,https://github.com/tensorflow/tensorflow/issues/88451
1157,"以下是一个github上的tensorflow下的一个issue, 标题是(Failed to parse TfLiteSettingsJsonParser on TensorFlow Lite C++)， 内容是 ( Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.1  Custom code No  OS platform and distribution Debian aarch64   Mobile device VIA VAB5000  Python version Only C++  Bazel version 6.5.0  GCC/compiler version gcc version 12.2.0 (Debian 12.2.014) / Debian clang version 14.0.6  CUDA/cuDNN version None  GPU model and memory MediaTek Genio 700 MDLA  Current behavior? I am trying to use Delegate on VIA VAB5000 (aarch64) by referring to the following site. However, an error occurs when loading the json file to be used for Delegate, and I cannot use Delegate. Do you have any good ideas? I apologize for bothering you during your busy schedule. Thank you in advance. https://mediatek.gitlab.io/genio/doc/tao/npu_acceleration.html Below is the json file I am trying to parse.   Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,xtrizeShino,Failed to parse TfLiteSettingsJsonParser on TensorFlow Lite C++," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.1  Custom code No  OS platform and distribution Debian aarch64   Mobile device VIA VAB5000  Python version Only C++  Bazel version 6.5.0  GCC/compiler version gcc version 12.2.0 (Debian 12.2.014) / Debian clang version 14.0.6  CUDA/cuDNN version None  GPU model and memory MediaTek Genio 700 MDLA  Current behavior? I am trying to use Delegate on VIA VAB5000 (aarch64) by referring to the following site. However, an error occurs when loading the json file to be used for Delegate, and I cannot use Delegate. Do you have any good ideas? I apologize for bothering you during your busy schedule. Thank you in advance. https://mediatek.gitlab.io/genio/doc/tao/npu_acceleration.html Below is the json file I am trying to parse.   Standalone code to reproduce the issue   Relevant log output ",2025-03-02T05:56:37Z,type:support comp:lite TF 2.16,open,0,3,https://github.com/tensorflow/tensorflow/issues/88435,"The TensorFlow Lite ""*.so"" files were crosscompiled using the following steps.   And I am building it with cmake using the following CMakeLists.txt: ","Hi,   I apologize for the delay in my response, I see after analyzing the error you're encountering with the `TfLiteSettingsJsonParser` maybe the values `NEURON_PRIORITY_HIGH, NEURON_OPTIMIZATION_NONE` and `NEURON_FAST_SINGLE_ANSWER` are not valid JSON as they're not enclosed in quotes. In proper JSON, string values must be enclosed in double quotes. If these are meant to be string constants:  JSON parsing errors can also occur due to file encoding problems. The parser might be failing due to hidden characters or incorrect encoding so create a new file with a text editor (like Notepad++ if available) save it with `UTF8` encoding (not UTF8BOM) and use this new file and see is it resolving your error or not ? Thank you for your cooperation and patience.","Thank you for your reply!! However, I tried the method you taught me, but unfortunately the result did not change. The output is as follows.  When I open the file in Notepad++, it shows as UTF8.  Since the character code contains only alphabets, the nkf command displayed it as ""ASCII"".  https://github.com/xtrizeShino/peoplenet_onnx_to_tflite/blob/main/cpp_infer_vab5000/peoplenet_main.cppL232 "
911,"以下是一个github上的tensorflow下的一个issue, 标题是(`tf.compat.v1.linalg.set_diag` aborts with ""Check failed: d < dims() (2 vs. 2)"")， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.20.0dev20250225  Custom code Yes  OS platform and distribution Ubuntu 20.04 LTS   Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an aborted issue in TensorFlow when I used API tf.compat.v1.linalg.set_diag . I have confirmed that below code would crash on tfnightly 2.20.0dev20250225 (nightlybuild).  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,cybersupersoap,"`tf.compat.v1.linalg.set_diag` aborts with ""Check failed: d < dims() (2 vs. 2)""", Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.20.0dev20250225  Custom code Yes  OS platform and distribution Ubuntu 20.04 LTS   Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an aborted issue in TensorFlow when I used API tf.compat.v1.linalg.set_diag . I have confirmed that below code would crash on tfnightly 2.20.0dev20250225 (nightlybuild).  Standalone code to reproduce the issue   Relevant log output ,2025-03-02T02:21:06Z,type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/88426,I was able to reproduce this issue on Colab using TensorFlow 2.18 and the nightly version. Please find the gist attached for your reference. Thank you!
1176,"以下是一个github上的tensorflow下的一个issue, 标题是(Bump the github-actions group with 6 updates)， 内容是 (Bumps the githubactions group with 6 updates:  Updates `peterevans/createpullrequest` from 7.0.6 to 7.0.7  Release notes Sourced from peterevans/createpullrequest's releases.  Create Pull Request v7.0.7 ⚙️ Fixes an issue with commit signing where modifications to the same file in multiple commits squash into the first commit. What's Changed  build(deps): bump @​octokit/core from 6.1.2 to 6.1.3 by @​dependabot in peterevans/createpullrequest CC(How to run custom encoderdecoder in Tensorflow using available APIs?) build(depsdev): bump @​types/node from 18.19.68 to 18.19.70 by @​dependabot in peterevans/createpullrequest CC(Moved eightbit graph trimming to before output_nodes definition) Update distribution by @​actionsbot in peterevans/createpullrequest CC(RC 0.10 3X Slower than 0.9 and Error Compiling From Source Under Certain Conditions) build(depsdev): bump typescript from 5.7.2 to 5.7.3 by @​dependabot in peterevans/createpullreq)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,dependabot[bot],Bump the github-actions group with 6 updates,Bumps the githubactions group with 6 updates:  Updates `peterevans/createpullrequest` from 7.0.6 to 7.0.7  Release notes Sourced from peterevans/createpullrequest's releases.  Create Pull Request v7.0.7 ⚙️ Fixes an issue with commit signing where modifications to the same file in multiple commits squash into the first commit. What's Changed  build(deps): bump @​octokit/core from 6.1.2 to 6.1.3 by @​dependabot in peterevans/createpullrequest CC(How to run custom encoderdecoder in Tensorflow using available APIs?) build(depsdev): bump @​types/node from 18.19.68 to 18.19.70 by @​dependabot in peterevans/createpullrequest CC(Moved eightbit graph trimming to before output_nodes definition) Update distribution by @​actionsbot in peterevans/createpullrequest CC(RC 0.10 3X Slower than 0.9 and Error Compiling From Source Under Certain Conditions) build(depsdev): bump typescript from 5.7.2 to 5.7.3 by @​dependabot in peterevans/createpullreq,2025-03-01T08:27:20Z,ready to pull size:S dependencies github_actions,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88411
605,"以下是一个github上的tensorflow下的一个issue, 标题是(litert: LiteRT changes for GPU support)， 内容是 (litert: LiteRT changes for GPU support  Update CheckCpuTensors() to check nodes with execution_plan() instead of   checking all nodes_and_registration(). This aligns with SubGraph::Invoke()   and works well after applying a Delegate.  Added ExternalLiteRtBufferContext::RegisterLiteRtBufferRequirement()   to register with C type LiteRtTensorBufferRequirements.  Update visibility)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],litert: LiteRT changes for GPU support,litert: LiteRT changes for GPU support  Update CheckCpuTensors() to check nodes with execution_plan() instead of   checking all nodes_and_registration(). This aligns with SubGraph::Invoke()   and works well after applying a Delegate.  Added ExternalLiteRtBufferContext::RegisterLiteRtBufferRequirement()   to register with C type LiteRtTensorBufferRequirements.  Update visibility,2025-03-01T01:30:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88396
415,"以下是一个github上的tensorflow下的一个issue, 标题是(Part2: Remove moved code createLegalizeTFXlaCallModuleToStablehloPass from tensorflow/compiler/mlir/lite/stablehlo)， 内容是 (Part2: Remove moved code createLegalizeTFXlaCallModuleToStablehloPass from tensorflow/compiler/mlir/lite/stablehlo)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,copybara-service[bot],Part2: Remove moved code createLegalizeTFXlaCallModuleToStablehloPass from tensorflow/compiler/mlir/lite/stablehlo,Part2: Remove moved code createLegalizeTFXlaCallModuleToStablehloPass from tensorflow/compiler/mlir/lite/stablehlo,2025-02-28T23:45:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88390
281,"以下是一个github上的tensorflow下的一个issue, 标题是(Define CAPI and Python API for chlo.ragged_dot.)， 内容是 (Define CAPI and Python API for chlo.ragged_dot.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Define CAPI and Python API for chlo.ragged_dot.,Define CAPI and Python API for chlo.ragged_dot.,2025-02-28T22:44:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88383
275,"以下是一个github上的tensorflow下的一个issue, 标题是(Do not infer return type of chlo.ragged_dot.)， 内容是 (Do not infer return type of chlo.ragged_dot.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Do not infer return type of chlo.ragged_dot.,Do not infer return type of chlo.ragged_dot.,2025-02-28T22:35:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88382
381,"以下是一个github上的tensorflow下的一个issue, 标题是(Part1: Migrate createLegalizeTFXlaCallModuleToStablehloPass to tensorflow/compiler/mlir/stablehlo)， 内容是 (Part1: Migrate createLegalizeTFXlaCallModuleToStablehloPass to tensorflow/compiler/mlir/stablehlo)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,copybara-service[bot],Part1: Migrate createLegalizeTFXlaCallModuleToStablehloPass to tensorflow/compiler/mlir/stablehlo,Part1: Migrate createLegalizeTFXlaCallModuleToStablehloPass to tensorflow/compiler/mlir/stablehlo,2025-02-28T21:37:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88378
1225,"以下是一个github上的tensorflow下的一个issue, 标题是(ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors)， 内容是 (for this code  (on colab https://colab.research.google.com/drive/1WKSgxQUSZp4Q5dHeq2HgJvjjVzxJUoqA?usp=sharing) :   And then  `!edgetpu_compiler s cnn.tflite` I am getting :   I am fully aware that the Coral Dev Board team release the EdgeTPU Model version (efficientnetedgetpuL_quant_edgetpu.tflite) But I am not interested in the default generated EdgeTPU file, indeed I want to do something like this :   And then build the TFLite and the EdgeTPU. Unfortunately it doesn't seems possible to do this directly on a TFLite or TFliteEdgeTPU model :        For example : how to apply the code above with this : EfficientB3 EDGE_TPU version ? And it seems the Coral Dev Board Team has rewrite the EfficientNet model from scratch, but they don't explain why or how to import their model implemented here TPU repo  By import I mean how to replace the pretrainded **tf.keras.applications.EfficientNetV2B3** with their model ?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,nassimus26,ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors,"for this code  (on colab https://colab.research.google.com/drive/1WKSgxQUSZp4Q5dHeq2HgJvjjVzxJUoqA?usp=sharing) :   And then  `!edgetpu_compiler s cnn.tflite` I am getting :   I am fully aware that the Coral Dev Board team release the EdgeTPU Model version (efficientnetedgetpuL_quant_edgetpu.tflite) But I am not interested in the default generated EdgeTPU file, indeed I want to do something like this :   And then build the TFLite and the EdgeTPU. Unfortunately it doesn't seems possible to do this directly on a TFLite or TFliteEdgeTPU model :        For example : how to apply the code above with this : EfficientB3 EDGE_TPU version ? And it seems the Coral Dev Board Team has rewrite the EfficientNet model from scratch, but they don't explain why or how to import their model implemented here TPU repo  By import I mean how to replace the pretrainded **tf.keras.applications.EfficientNetV2B3** with their model ?",2025-02-28T20:35:22Z,stat:awaiting response stale comp:lite TFLiteConverter,closed,0,5,https://github.com/tensorflow/tensorflow/issues/88361,"Hi,   I apologize for the delay in my response, I have been able to replicate the similar issue with your provided code and I'm also getting same error message please refer this gistfile so we will have to dig more into this issue and will update you. **Here is error log output for reference :**  Thank you for your cooperation and patience.","Hi  , thank you, what do you mean by : please refer this [gistfile] ?  What do you want me to do about it ?, I already write you a test case and publish it on this page. And I hope that the Edge compiler project is not dead, most of their converted models has been written with TF 1.x, and the compiler has not been updated since many years, and it seems not Open source.","Hi,  I apologize for the delayed response, sorry for the confusion I mean to say I am able to replicate the same behavior which you reported from my end so for further investigation from our end as reference I added gistfile As per the official documentation of TensorFlow models on the Edge TPU specifically Model requirements section the model must meet these basic requirements:  Tensor parameters are quantized (8bit fixedpoint numbers; int8 or uint8).  Tensor sizes are constant at compiletime (no dynamic sizes).  Model parameters (such as bias tensors) are constant at compiletime.  Tensors are either 1, 2, or 3dimensional. If a tensor has more than 3 dimensions, then only the 3 innermost dimensions may have a size greater than 1.  The model uses only the operations supported by the Edge TPU (see table 1 below). Please use tools like Netron to visualize your model and check for operations that produce dynamicshaped tensors. The Edge TPU requires models to be quantized to (8bit fixedpoint numbers; int8 or uint8). Make sure that the representative_dataset is correctly set and that the model is fully quantized. some layers or operations might not be supported by the Edge TPU. Please make sure that all layers in your model are compatible with Edge TPU. You can refer to the coral documentation for a list of supported operations. In some cases there are no dynamic size tensors in the graph but it has a control flow op, While op which classifies graph as dynamic graph that might be the cause for this. Thank you for your cooperation and understanding.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
614,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Fix RaggedAllToAllDecomposer when input and output buffers have different sizes.)， 内容是 ([XLA:GPU] Fix RaggedAllToAllDecomposer when input and output buffers have different sizes. We were doubling the size of the buffer to be able to use dynamicupdateslice, because by HLO semantics, if the update goes out of bound of the result, the update is not applied at all. The correct solution is to pad to `input_size + output_size`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Fix RaggedAllToAllDecomposer when input and output buffers have different sizes.,"[XLA:GPU] Fix RaggedAllToAllDecomposer when input and output buffers have different sizes. We were doubling the size of the buffer to be able to use dynamicupdateslice, because by HLO semantics, if the update goes out of bound of the result, the update is not applied at all. The correct solution is to pad to `input_size + output_size`.",2025-02-28T18:21:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88350
448,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Constrain ragged-all-to-all layout.)， 内容是 ([XLA:GPU] Constrain raggedalltoall layout. We can only support cases when ragged dimension (dim 0) is the most major dimension. This was all update are contiguous in memory. Layout of other dimensions doesn't matter.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Constrain ragged-all-to-all layout.,[XLA:GPU] Constrain raggedalltoall layout. We can only support cases when ragged dimension (dim 0) is the most major dimension. This was all update are contiguous in memory. Layout of other dimensions doesn't matter.,2025-02-28T15:53:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88340
317,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Cover RaggedAllToAllDecomposer in collective e2e tests.)， 内容是 ([XLA:GPU] Cover RaggedAllToAllDecomposer in collective e2e tests.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Cover RaggedAllToAllDecomposer in collective e2e tests.,[XLA:GPU] Cover RaggedAllToAllDecomposer in collective e2e tests.,2025-02-28T13:49:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88333
289,"以下是一个github上的tensorflow下的一个issue, 标题是(Make Pathways IFRT client get GPU topology as well.)， 内容是 (Make Pathways IFRT client get GPU topology as well.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,copybara-service[bot],Make Pathways IFRT client get GPU topology as well.,Make Pathways IFRT client get GPU topology as well.,2025-02-27T23:29:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88275
415,"以下是一个github上的tensorflow下的一个issue, 标题是(Disable `//xla/python/ifrt_proxy/integration_tests:executable_impl_test_tfrt_cpu` on ARM64)， 内容是 (Disable `//xla/python/ifrt_proxy/integration_tests:executable_impl_test_tfrt_cpu` on ARM64 This test occasionally times out on ARM builds.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Disable `//xla/python/ifrt_proxy/integration_tests:executable_impl_test_tfrt_cpu` on ARM64,Disable `//xla/python/ifrt_proxy/integration_tests:executable_impl_test_tfrt_cpu` on ARM64 This test occasionally times out on ARM builds.,2025-02-27T21:30:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88268
229,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal change only.)， 内容是 (Internal change only.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal change only.,Internal change only.,2025-02-27T20:44:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88262
321,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA] Remove extraneous `gemma2_2b_keras_jax.hlo` argument in wget.)， 内容是 ([XLA] Remove extraneous `gemma2_2b_keras_jax.hlo` argument in wget.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,copybara-service[bot],[XLA] Remove extraneous `gemma2_2b_keras_jax.hlo` argument in wget.,[XLA] Remove extraneous `gemma2_2b_keras_jax.hlo` argument in wget.,2025-02-27T19:18:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88249
1162,"以下是一个github上的tensorflow下的一个issue, 标题是(Tensorflow Website Out-of-date)， 内容是 ( Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2  Custom code Yes  OS platform and distribution Windows 11 24H2  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In tensorflow.org/install, everything is just so out of date. It started when i'm reading the chinese version of the install page (https://www.tensorflow.org/install?hl=zhcn): which is:   我们在以下 64 位系统上测试过 TensorFlow 并且这些系统支持 TensorFlow： Python 3.6–3.9     See the problem? it said: TensorFlow is tested and supported on the following 64bit systems: Python 3.63.9 But 3.6 is EOS like millions of years ago, so i scrolled down and saw the page last updated date: 20210825. But this may be that the people are lazy to translate them to chinese, so)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Unknownuserfrommars,Tensorflow Website Out-of-date," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2  Custom code Yes  OS platform and distribution Windows 11 24H2  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In tensorflow.org/install, everything is just so out of date. It started when i'm reading the chinese version of the install page (https://www.tensorflow.org/install?hl=zhcn): which is:   我们在以下 64 位系统上测试过 TensorFlow 并且这些系统支持 TensorFlow： Python 3.6–3.9     See the problem? it said: TensorFlow is tested and supported on the following 64bit systems: Python 3.63.9 But 3.6 is EOS like millions of years ago, so i scrolled down and saw the page last updated date: 20210825. But this may be that the people are lazy to translate them to chinese, so",2025-02-27T13:07:47Z,type:docs-bug type:bug,open,0,0,https://github.com/tensorflow/tensorflow/issues/88226
542,"以下是一个github上的tensorflow下的一个issue, 标题是(Qualcomm AI Engine Direct - Op Builders for 1P Models)， 内容是 ( WHAT 1. Conv2d 2. DepthwiseConv2d 3. AveragePool 4. MaxPool 5. DepthToSpace 6. SpaceToDepth 7. HardSwish 8. LeakyRelu 9. ResizeBilinear 10. Litert options for these op builders, unit test 11. update qnn_compiler_plugin_test with these op builders  TEST  qnn_compiler_plugin_test   litert_options_test )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,weilhuan-quic,Qualcomm AI Engine Direct - Op Builders for 1P Models," WHAT 1. Conv2d 2. DepthwiseConv2d 3. AveragePool 4. MaxPool 5. DepthToSpace 6. SpaceToDepth 7. HardSwish 8. LeakyRelu 9. ResizeBilinear 10. Litert options for these op builders, unit test 11. update qnn_compiler_plugin_test with these op builders  TEST  qnn_compiler_plugin_test   litert_options_test ",2025-02-27T10:49:15Z,awaiting review comp:lite ready to pull size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88221
866,"以下是一个github上的tensorflow下的一个issue, 标题是(Fix TOSA bytecode conversion and improve code readability)， 内容是 ( Bug Fix This PR fixes a critical bug in the TOSA conversion pipeline where `experimental_tflite_to_tosa_bytecode()` was trying to call `ExperimentalTFLiteToTosaBytecode` directly instead of through the `_pywrap_mlir` module.  Code Improvements Additionally, this PR improves code readability and reduces complexity by:  Adding a helper function for string encoding  Simplifying parameter defaults  Making code more consistent across functions  Testing Verified the fix resolves the original NameError by importing and using the function. Fixes CC([TOSA] NameError: name 'ExperimentalTFLiteToTosaBytecode' is not defined) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,clickbaron,Fix TOSA bytecode conversion and improve code readability," Bug Fix This PR fixes a critical bug in the TOSA conversion pipeline where `experimental_tflite_to_tosa_bytecode()` was trying to call `ExperimentalTFLiteToTosaBytecode` directly instead of through the `_pywrap_mlir` module.  Code Improvements Additionally, this PR improves code readability and reduces complexity by:  Adding a helper function for string encoding  Simplifying parameter defaults  Making code more consistent across functions  Testing Verified the fix resolves the original NameError by importing and using the function. Fixes CC([TOSA] NameError: name 'ExperimentalTFLiteToTosaBytecode' is not defined) ",2025-02-27T02:38:09Z,awaiting review size:M comp:lite-tosa,open,0,3,https://github.com/tensorflow/tensorflow/issues/88193,> [!IMPORTANT] > The terms of service for this installation has not been accepted. Please ask the Organization owners to visit the Gemini Code Assist Admin Console to sign it.,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , Can you please sign CLA, Thank you !"
414,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Fix post-optimization pipeline parallelism tests)， 内容是 ([XLA:GPU] Fix postoptimization pipeline parallelism tests These tests passed earlier (by chance). The underlying issue is the same as for the test fixed in cl/730568729.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA:GPU] Fix post-optimization pipeline parallelism tests,[XLA:GPU] Fix postoptimization pipeline parallelism tests These tests passed earlier (by chance). The underlying issue is the same as for the test fixed in cl/730568729.,2025-02-27T01:14:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88191
1191,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #23020: Bump github/codeql-action from 3.24.9 to 3.28.10)， 内容是 (PR CC(Error in C++ Tensorflow to load model trained by python): Bump github/codeqlaction from 3.24.9 to 3.28.10 Imported from GitHub PR https://github.com/openxla/xla/pull/23020 Bumps github/codeqlaction from 3.24.9 to 3.28.10.  Release notes Sourced from github/codeqlaction's releases.  v3.28.10 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.10  21 Feb 2025  Update default CodeQL bundle version to 2.20.5.  CC(Please consider adding flatten) Address an issue where the CodeQL Bundle would occasionally fail to decompress on macOS.  CC(Checkpoint Restore blocked by changed default bias variable name)  See the full CHANGELOG.md for more information. v3.28.9 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.9  07 Feb 2025  Update default CodeQL bundle version to 2.20.4.  CC(C++ compilation of rule '//:sip_ha)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #23020: Bump github/codeql-action from 3.24.9 to 3.28.10,PR CC(Error in C++ Tensorflow to load model trained by python): Bump github/codeqlaction from 3.24.9 to 3.28.10 Imported from GitHub PR https://github.com/openxla/xla/pull/23020 Bumps github/codeqlaction from 3.24.9 to 3.28.10.  Release notes Sourced from github/codeqlaction's releases.  v3.28.10 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.10  21 Feb 2025  Update default CodeQL bundle version to 2.20.5.  CC(Please consider adding flatten) Address an issue where the CodeQL Bundle would occasionally fail to decompress on macOS.  CC(Checkpoint Restore blocked by changed default bias variable name)  See the full CHANGELOG.md for more information. v3.28.9 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.9  07 Feb 2025  Update default CodeQL bundle version to 2.20.4.  CC(C++ compilation of rule '//:sip_ha,2025-02-26T23:40:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88180
1189,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #23018: Bump ossf/scorecard-action from 2.3.1 to 2.4.1)， 内容是 (PR CC(.): Bump ossf/scorecardaction from 2.3.1 to 2.4.1 Imported from GitHub PR https://github.com/openxla/xla/pull/23018 Bumps ossf/scorecardaction from 2.3.1 to 2.4.1.  Release notes Sourced from ossf/scorecardaction's releases.  v2.4.1 What's Changed  This update bumps the Scorecard version to the v5.1.1 release. For a complete list of changes, please refer to the v5.1.0 and v5.1.1 release notes. Publishing results now uses half the API quota as before. The exact savings depends on the repository in question.  use Scorecard library entrypoint instead of Cobra hooking by @​spencerschrock in ossf/scorecardaction CC(Need force_gpu_if_available for tests)   Some errors were made into annotations to make them more visible  Make default branch error more prominent by @​jsoref in ossf/scorecardaction CC(partial_run segfault)   There is now an optional file_mode input which controls how repository files are fetched from GitHub. The defa)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #23018: Bump ossf/scorecard-action from 2.3.1 to 2.4.1,"PR CC(.): Bump ossf/scorecardaction from 2.3.1 to 2.4.1 Imported from GitHub PR https://github.com/openxla/xla/pull/23018 Bumps ossf/scorecardaction from 2.3.1 to 2.4.1.  Release notes Sourced from ossf/scorecardaction's releases.  v2.4.1 What's Changed  This update bumps the Scorecard version to the v5.1.1 release. For a complete list of changes, please refer to the v5.1.0 and v5.1.1 release notes. Publishing results now uses half the API quota as before. The exact savings depends on the repository in question.  use Scorecard library entrypoint instead of Cobra hooking by @​spencerschrock in ossf/scorecardaction CC(Need force_gpu_if_available for tests)   Some errors were made into annotations to make them more visible  Make default branch error more prominent by @​jsoref in ossf/scorecardaction CC(partial_run segfault)   There is now an optional file_mode input which controls how repository files are fetched from GitHub. The defa",2025-02-26T23:37:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88178
479,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Fix RaggedAllToAllDecomposer pass.)， 内容是 ([XLA:GPU] Fix RaggedAllToAllDecomposer pass. There were two issues that are happening: 1. Update slice logic was not correct and would overwrite values outside of the update. 2. RaggedAllToAll API was extended to allow multiple updates per replica.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Fix RaggedAllToAllDecomposer pass.,[XLA:GPU] Fix RaggedAllToAllDecomposer pass. There were two issues that are happening: 1. Update slice logic was not correct and would overwrite values outside of the update. 2. RaggedAllToAll API was extended to allow multiple updates per replica.,2025-02-26T22:46:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88170
334,"以下是一个github上的tensorflow下的一个issue, 标题是(Update cpu_benchmarks.yml and gpu_benchmarks.yml to run the Gemma2-2B HLO.)， 内容是 (Update cpu_benchmarks.yml and gpu_benchmarks.yml to run the Gemma22B HLO.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,copybara-service[bot],Update cpu_benchmarks.yml and gpu_benchmarks.yml to run the Gemma2-2B HLO.,Update cpu_benchmarks.yml and gpu_benchmarks.yml to run the Gemma22B HLO.,2025-02-26T21:40:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88164
349,"以下是一个github上的tensorflow下的一个issue, 标题是([XlaCallModule] Add better error message to computation deserialization failures.)， 内容是 ([XlaCallModule] Add better error message to computation deserialization failures.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,copybara-service[bot],[XlaCallModule] Add better error message to computation deserialization failures.,[XlaCallModule] Add better error message to computation deserialization failures.,2025-02-26T20:11:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88158
736,"以下是一个github上的tensorflow下的一个issue, 标题是([IFRT] Add short form for specifying platform_names for IFRT IR passes.)， 内容是 ([IFRT] Add short form for specifying platform_names for IFRT IR passes. Some IFRT IR passes require a list of platform names to be given as an option. Currently, a platform names list requires an entry for each device, which makes  manually running the passes on modules with many devices tedious. This change introduces a short form for specifying platform names with the format  platform_name:number_of_occurrences. For example, tpu:2,cpu:3 is expanded to tpu,tpu,cpu,cpu,cpu.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[IFRT] Add short form for specifying platform_names for IFRT IR passes.,"[IFRT] Add short form for specifying platform_names for IFRT IR passes. Some IFRT IR passes require a list of platform names to be given as an option. Currently, a platform names list requires an entry for each device, which makes  manually running the passes on modules with many devices tedious. This change introduces a short form for specifying platform names with the format  platform_name:number_of_occurrences. For example, tpu:2,cpu:3 is expanded to tpu,tpu,cpu,cpu,cpu.",2025-02-25T19:48:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88071
629,"以下是一个github上的tensorflow下的一个issue, 标题是(Introduce `TPUDummyInput` as a specialization of `Fill` for ICI weight distribution.)， 内容是 (Introduce `TPUDummyInput` as a specialization of `Fill` for ICI weight distribution. The new op has a few benefits over the previous version: * We can generate a single op instead of three ops for each dummy input. * The new op is marked as `DoNotOptimize` and `TF_NoConstantFold`, so it will never be accidentally constantfolded to a large memory footprint.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Introduce `TPUDummyInput` as a specialization of `Fill` for ICI weight distribution.,"Introduce `TPUDummyInput` as a specialization of `Fill` for ICI weight distribution. The new op has a few benefits over the previous version: * We can generate a single op instead of three ops for each dummy input. * The new op is marked as `DoNotOptimize` and `TF_NoConstantFold`, so it will never be accidentally constantfolded to a large memory footprint.",2025-02-25T18:07:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88059
304,"以下是一个github上的tensorflow下的一个issue, 标题是(Bump the priority of CHLO->MHLO ragged dot pass to highest.)， 内容是 (Bump the priority of CHLO>MHLO ragged dot pass to highest.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Bump the priority of CHLO->MHLO ragged dot pass to highest.,Bump the priority of CHLO>MHLO ragged dot pass to highest.,2025-02-25T04:28:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87997
1043,"以下是一个github上的tensorflow下的一个issue, 标题是(Help with reference)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 13.3.06ubuntu2~24.04) 13.3.0  CUDA/cuDNN version CUDA 12.0  GPU model and memory NVIDIA GeForce RTX 4050  Current behavior? I am expecting my model to run without giving any type of warnings. I know for certain that my data does not have any NaN values in it, so I am not sure why am I getting the warning I am receiving. My output should look something like the below: 2138/14403 ━━━━━━━━━━━━━━━━━━━━ 2:45 14ms/step  accuracy: 0.7590  loss: 1.1440E0000 00:00:1740370043.647457  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,diddlywob,Help with reference," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 13.3.06ubuntu2~24.04) 13.3.0  CUDA/cuDNN version CUDA 12.0  GPU model and memory NVIDIA GeForce RTX 4050  Current behavior? I am expecting my model to run without giving any type of warnings. I know for certain that my data does not have any NaN values in it, so I am not sure why am I getting the warning I am receiving. My output should look something like the below: 2138/14403 ━━━━━━━━━━━━━━━━━━━━ 2:45 14ms/step  accuracy: 0.7590  loss: 1.1440E0000 00:00:1740370043.647457  Standalone code to reproduce the issue   Relevant log output ",2025-02-24T05:00:27Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87925,Are you satisfied with the resolution of your issue? Yes No
247,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal change for visibility)， 内容是 (Internal change for visibility)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal change for visibility,Internal change for visibility,2025-02-24T04:42:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87920
1054,"以下是一个github上的tensorflow下的一个issue, 标题是(Results Do Not Match Reference)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 13.3.06ubuntu2~24.04) 13.3.0  CUDA/cuDNN version CUDA 12.0  GPU model and memory NVIDIA GeForce RTX 4050  Current behavior? I am expecting my model to run without giving any type of warnings. I know for certain that my data does not have any NaN values in it, so I am not sure why am I getting the warning I am receiving. My output should look something like the below: 2138/14403 ━━━━━━━━━━━━━━━━━━━━ 2:45 14ms/step  accuracy: 0.7590  loss: 1.1440E0000 00:00:1740370043.647457  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,diddlywob,Results Do Not Match Reference," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 13.3.06ubuntu2~24.04) 13.3.0  CUDA/cuDNN version CUDA 12.0  GPU model and memory NVIDIA GeForce RTX 4050  Current behavior? I am expecting my model to run without giving any type of warnings. I know for certain that my data does not have any NaN values in it, so I am not sure why am I getting the warning I am receiving. My output should look something like the below: 2138/14403 ━━━━━━━━━━━━━━━━━━━━ 2:45 14ms/step  accuracy: 0.7590  loss: 1.1440E0000 00:00:1740370043.647457  Standalone code to reproduce the issue   Relevant log output ",2025-02-24T04:41:34Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87919,Are you satisfied with the resolution of your issue? Yes No
1120,"以下是一个github上的tensorflow下的一个issue, 标题是(Results do not match the reference. This is likely a bug/unexpected loss of precision.)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 13.3.06ubuntu2~24.04) 13.3.0  CUDA/cuDNN version CUDA 12.0  GPU model and memory NVIDIA GeForce RTX 4050  Current behavior? I am simply expecting my model to run without giving any type of warnings. I know for certain that my data does not have any NaN values in it, so I am not sure why am I getting the warning I am receiving. My output should look something like the below:  2138/14403 ━━━━━━━━━━━━━━━━━━━━ 2:45 14ms/step  accuracy: 0.7590  loss: 1.1440E0000 00:00:1740370043.647457    Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,diddlywob,Results do not match the reference. This is likely a bug/unexpected loss of precision.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 13.3.06ubuntu2~24.04) 13.3.0  CUDA/cuDNN version CUDA 12.0  GPU model and memory NVIDIA GeForce RTX 4050  Current behavior? I am simply expecting my model to run without giving any type of warnings. I know for certain that my data does not have any NaN values in it, so I am not sure why am I getting the warning I am receiving. My output should look something like the below:  2138/14403 ━━━━━━━━━━━━━━━━━━━━ 2:45 14ms/step  accuracy: 0.7590  loss: 1.1440E0000 00:00:1740370043.647457    Standalone code to reproduce the issue   Relevant log output ",2025-02-24T04:27:53Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87918,Are you satisfied with the resolution of your issue? Yes No
1026,"以下是一个github上的tensorflow下的一个issue, 标题是(Clear out XLA computations from compilation caches after finalizing the TensorFlow session.)， 内容是 (Clear out XLA computations from compilation caches after finalizing the TensorFlow session. After compiling a TF Graph into an XLA HLO program and after compiling the HLO into an executable, we keep around a `std::shared_ptrcomputation`. When the compiled HLO contains many constants, its heap memory consumption is significant and otherwise unreferenced after initialization. This CL adds an entrypoint `DeviceCompilationCache::Finalize`, which is exposed as `DeviceCompiler::Finalize`, which is an implementation of the virtual function `ResourceBase::Finalize`. `ResourceBase::Finalize` returns `absl::AnyInvocable` so that we can defer destruction of finalized objects owned by `ResourceBase` until after we release the lock `ResourceMgr::mu_`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Clear out XLA computations from compilation caches after finalizing the TensorFlow session.,"Clear out XLA computations from compilation caches after finalizing the TensorFlow session. After compiling a TF Graph into an XLA HLO program and after compiling the HLO into an executable, we keep around a `std::shared_ptrcomputation`. When the compiled HLO contains many constants, its heap memory consumption is significant and otherwise unreferenced after initialization. This CL adds an entrypoint `DeviceCompilationCache::Finalize`, which is exposed as `DeviceCompiler::Finalize`, which is an implementation of the virtual function `ResourceBase::Finalize`. `ResourceBase::Finalize` returns `absl::AnyInvocable` so that we can defer destruction of finalized objects owned by `ResourceBase` until after we release the lock `ResourceMgr::mu_`.",2025-02-24T00:17:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87909
741,"以下是一个github上的tensorflow下的一个issue, 标题是(Inconsistent Behavior When Using tf.keras.metrics.Accuracy with F1-Score and Precision)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.0  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? !Image !Image !Image  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,JacksonDivakar,Inconsistent Behavior When Using tf.keras.metrics.Accuracy with F1-Score and Precision, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.0  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? !Image !Image !Image  Standalone code to reproduce the issue   Relevant log output ,2025-02-23T06:34:05Z,stat:awaiting response type:bug stale type:others comp:keras TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/87893,"Hi **** , Please post this issue on kerasteam/keras repo. as this issue is more related to keras Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
245,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal change of visibility)， 内容是 (Internal change of visibility)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal change of visibility,Internal change of visibility,2025-02-22T20:22:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87879
843,"以下是一个github上的tensorflow下的一个issue, 标题是(Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `all-gather` with an `all-to-all`.)， 内容是 (Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `allgather` with an `alltoall`. Given the following input  Previously, we (1) replicate the input along the concat dimension, (2) apply concat, (3) partition the result with dynamicslice. With this change, we (1) use alltoall to move sharding axis from the concat dim to batch dim for operands, (2) apply concat, and then (3) use alltoall to reshard the result. Reverts 81b0a48fcf8618fdab0a03907b05a65413399585)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,copybara-service[bot],"Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `all-gather` with an `all-to-all`.","Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `allgather` with an `alltoall`. Given the following input  Previously, we (1) replicate the input along the concat dimension, (2) apply concat, (3) partition the result with dynamicslice. With this change, we (1) use alltoall to move sharding axis from the concat dim to batch dim for operands, (2) apply concat, and then (3) use alltoall to reshard the result. Reverts 81b0a48fcf8618fdab0a03907b05a65413399585",2025-02-22T01:34:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87841
1136,"以下是一个github上的tensorflow下的一个issue, 标题是(spam)， 内容是 ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,LaithMustafa,spam," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN",2025-02-21T18:02:20Z,invalid TFLiteConverter,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87796,Please don't spam.
247,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal change for visibility)， 内容是 (Internal change for visibility)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal change for visibility,Internal change for visibility,2025-02-21T05:49:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87734
670,"以下是一个github上的tensorflow下的一个issue, 标题是(Adds visibility restriction to some XLA bzl files to prevent them from being used outside of XLA, as they are internal implementation details.)， 内容是 (Adds visibility restriction to some XLA bzl files to prevent them from being used outside of XLA, as they are internal implementation details. This CL is not complete. It's the first step that establishes the mechanism. Once I get buyin on the approach, I'll follow up with more CLs to add visibility restriction to the other XLA bazl files.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],"Adds visibility restriction to some XLA bzl files to prevent them from being used outside of XLA, as they are internal implementation details.","Adds visibility restriction to some XLA bzl files to prevent them from being used outside of XLA, as they are internal implementation details. This CL is not complete. It's the first step that establishes the mechanism. Once I get buyin on the approach, I'll follow up with more CLs to add visibility restriction to the other XLA bazl files.",2025-02-20T23:21:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87718
818,"以下是一个github上的tensorflow下的一个issue, 标题是([tsl:concurrency] Micro optimizations for AsyncValue::AndThen)， 内容是 ([tsl:concurrency] Micro optimizations for AsyncValue::AndThen Instead of relying on absl::AnyInvocable keep Waiter directly in the linked list node, this improves performance by: 1. Avoiding one extra heap allocation for the absl::AnyInvocable 2. Remove one pointer indirection in RunWaiters BEFORE:  Benchmark                      Time             CPU   Iterations  BM_AddAndThenCallback       20.3 ns         20.3 ns     34354892 AFTER:  Benchmark                      Time             CPU   Iterations  BM_AddAndThenCallback       12.2 ns         12.2 ns     57932249)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[tsl:concurrency] Micro optimizations for AsyncValue::AndThen,"[tsl:concurrency] Micro optimizations for AsyncValue::AndThen Instead of relying on absl::AnyInvocable keep Waiter directly in the linked list node, this improves performance by: 1. Avoiding one extra heap allocation for the absl::AnyInvocable 2. Remove one pointer indirection in RunWaiters BEFORE:  Benchmark                      Time             CPU   Iterations  BM_AddAndThenCallback       20.3 ns         20.3 ns     34354892 AFTER:  Benchmark                      Time             CPU   Iterations  BM_AddAndThenCallback       12.2 ns         12.2 ns     57932249",2025-02-20T22:58:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87714
1276,"以下是一个github上的tensorflow下的一个issue, 标题是(Direction Problem after creating model / So after sucsessfully creating my Model and saving it automatic in the folder i need i get this message)， 内容是 ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64Bit  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source): abslpy==1.0.0 appdirs==1.4.4 astunparse==1.6.3 attrs==22.1.0 audioread==3.0.0 cachetools==5.2.0 certifi==2022.9.24 cffi==1.15.1 chardet==5.0.0 charsetnormalizer==2.1.1 colorama==0.4.5 contourpy==1.0.5 cycler==0.11.0 Cython==0.29.32 dataclasses==0.6 decorator==5.1.1 dill==0.3.5.1 dmtree==0.1.7 etils==0.8.0 fire==0.4.0 flatbuffers==1.12 fonttools==4.37.4 gast==0.4.0 ginconfig==0.5.0 googleapicore==2.8.2 googleapipythonclient==2.64.0 googleauth==2.12.0 googleauthhttplib2==0.1.0 googleauthoauthlib==0.4.6 googlecloudbigquery==3.3.3 googlecloudbigquerystorage==2.16.0 googlecloudcore==2.3.2 googlecrc32c==1.5.0 googlepasta==0.2.0 googleresumablemedia==2.4.0 googleapiscommonprotos==1.56.4 grpcio==1.48.2)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Schnipper24,Direction Problem after creating model / So after sucsessfully creating my Model and saving it automatic in the folder i need i get this message," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64Bit  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source): abslpy==1.0.0 appdirs==1.4.4 astunparse==1.6.3 attrs==22.1.0 audioread==3.0.0 cachetools==5.2.0 certifi==2022.9.24 cffi==1.15.1 chardet==5.0.0 charsetnormalizer==2.1.1 colorama==0.4.5 contourpy==1.0.5 cycler==0.11.0 Cython==0.29.32 dataclasses==0.6 decorator==5.1.1 dill==0.3.5.1 dmtree==0.1.7 etils==0.8.0 fire==0.4.0 flatbuffers==1.12 fonttools==4.37.4 gast==0.4.0 ginconfig==0.5.0 googleapicore==2.8.2 googleapipythonclient==2.64.0 googleauth==2.12.0 googleauthhttplib2==0.1.0 googleauthoauthlib==0.4.6 googlecloudbigquery==3.3.3 googlecloudbigquerystorage==2.16.0 googlecloudcore==2.3.2 googlecrc32c==1.5.0 googlepasta==0.2.0 googleresumablemedia==2.4.0 googleapiscommonprotos==1.56.4 grpcio==1.48.2",2025-02-20T22:29:17Z,TFLiteConverter,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87708,I Guess i found the Answer by myself and changed the folder from Klötze to Kloetze and it was done .
466,"以下是一个github上的tensorflow下的一个issue, 标题是(#litert Add a automatically added accelerator compilation structure.)， 内容是 (litert Add a automatically added accelerator compilation structure. This structure allows passing metadata that is generated during the model compilation onto accelerators when they alter the underlying runtime.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],#litert Add a automatically added accelerator compilation structure.,litert Add a automatically added accelerator compilation structure. This structure allows passing metadata that is generated during the model compilation onto accelerators when they alter the underlying runtime.,2025-02-20T21:54:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87705
933,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA] Partial rollback to the previous implementation so that hlo casting utils does not use tsl::down_cast and we can insert better debug information regarding which HLO and what subclasses are involved. Performance is identical to using tsl::down_cast since both implementations only call dynamic_cast under debug build. Updated comments and added test coverage are kept.)， 内容是 ([XLA] Partial rollback to the previous implementation so that hlo casting utils does not use tsl::down_cast and we can insert better debug information regarding which HLO and what subclasses are involved. Performance is identical to using tsl::down_cast since both implementations only call dynamic_cast under debug build. Updated comments and added test coverage are kept.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA] Partial rollback to the previous implementation so that hlo casting utils does not use tsl::down_cast and we can insert better debug information regarding which HLO and what subclasses are involved. Performance is identical to using tsl::down_cast since both implementations only call dynamic_cast under debug build. Updated comments and added test coverage are kept.,[XLA] Partial rollback to the previous implementation so that hlo casting utils does not use tsl::down_cast and we can insert better debug information regarding which HLO and what subclasses are involved. Performance is identical to using tsl::down_cast since both implementations only call dynamic_cast under debug build. Updated comments and added test coverage are kept.,2025-02-20T07:22:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87648
950,"以下是一个github上的tensorflow下的一个issue, 标题是(xla::ShardingPropagation. Avoid applying sharding constraints if the operand is used by multiple sharding constraints with different shardings.)， 内容是 (xla::ShardingPropagation. Avoid applying sharding constraints if the operand is used by multiple sharding constraints with different shardings. With `B = customcall(A), custom_call_target=""Sharding""`, we can set the sharding for A when all the three conditions are true. 1. Unspecified dims are empty. Otherwise, the sharding is open and can be further modified. 2. A has no sharding. We cannot overwrite the existing one. 3. A does not have other sharding constraints. A can have multiple sharding constraints with the same sharding. The first two conditions are checked before this cl. This cl add the third condition.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],xla::ShardingPropagation. Avoid applying sharding constraints if the operand is used by multiple sharding constraints with different shardings.,"xla::ShardingPropagation. Avoid applying sharding constraints if the operand is used by multiple sharding constraints with different shardings. With `B = customcall(A), custom_call_target=""Sharding""`, we can set the sharding for A when all the three conditions are true. 1. Unspecified dims are empty. Otherwise, the sharding is open and can be further modified. 2. A has no sharding. We cannot overwrite the existing one. 3. A does not have other sharding constraints. A can have multiple sharding constraints with the same sharding. The first two conditions are checked before this cl. This cl add the third condition.",2025-02-20T00:20:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87611
402,"以下是一个github上的tensorflow下的一个issue, 标题是(Add a test for the `rng-bit-generator-expander` HLO optimization pass.)， 内容是 (Add a test for the `rngbitgeneratorexpander` HLO optimization pass. This increases `rngbitgeneratorexpander` coverage from about 4% to about 89%.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Add a test for the `rng-bit-generator-expander` HLO optimization pass.,Add a test for the `rngbitgeneratorexpander` HLO optimization pass. This increases `rngbitgeneratorexpander` coverage from about 4% to about 89%.,2025-02-19T23:59:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87609
741,"以下是一个github上的tensorflow下的一个issue, 标题是(Importerror keras)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When importing from keras I get an importerror. Reinstalling tensorflow does not work?  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Teun2305,Importerror keras, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When importing from keras I get an importerror. Reinstalling tensorflow does not work?  Standalone code to reproduce the issue   Relevant log output ,2025-02-19T12:21:05Z,type:build/install,closed,0,2,https://github.com/tensorflow/tensorflow/issues/87565,Please search for similar issues before opening duplicates,Are you satisfied with the resolution of your issue? Yes No
644,"以下是一个github上的tensorflow下的一个issue, 标题是(Use ""common"" instead of ""build"" for some flags in .bazelrc)， 内容是 (Use ""common"" instead of ""build"" for some flags in .bazelrc Setting ""build"" options in the RC file prevents applying the flags to the query command. ""common"" works for both build and query commands. Flags like `experimental_cc_shared_library` changes the starlark semantics which forces refetching all repo rules when switching between commands. Ideally, more flags should be common instead of build.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],"Use ""common"" instead of ""build"" for some flags in .bazelrc","Use ""common"" instead of ""build"" for some flags in .bazelrc Setting ""build"" options in the RC file prevents applying the flags to the query command. ""common"" works for both build and query commands. Flags like `experimental_cc_shared_library` changes the starlark semantics which forces refetching all repo rules when switching between commands. Ideally, more flags should be common instead of build.",2025-02-19T11:28:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87564
615,"以下是一个github上的tensorflow下的一个issue, 标题是(Update libtpu installation index path)， 内容是 (Update libtpu installation index path Update the libtpu installation index to https://storage.googleapis.com/libtpuwheels/index.html, which includes both stable and nightly libtpu versions, as per the cloud libtpu team's guidance. Also update the libtpu version in the setup.py. It starts to differ from the TF version to support JAX, and it requires manual updates for new releases for now.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Update libtpu installation index path,"Update libtpu installation index path Update the libtpu installation index to https://storage.googleapis.com/libtpuwheels/index.html, which includes both stable and nightly libtpu versions, as per the cloud libtpu team's guidance. Also update the libtpu version in the setup.py. It starts to differ from the TF version to support JAX, and it requires manual updates for new releases for now.",2025-02-19T04:47:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87524
1224,"以下是一个github上的tensorflow下的一个issue, 标题是([RNN] Conversion of model containing GRU layer to quantized TFLite causes Segmentation Fault)， 内容是 ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04  TensorFlow installation (pip package or built from source): both pip package and built from source  TensorFlow library (version, if pip package or github SHA, if built from source): 2.18  2. Code import os os.environ[""TF_USE_LEGACY_KERAS""] = ""1"" import tensorflow as tf import numpy as np import tf_keras as keras def gru_model():     """"""Factory method for gru model.""""""     gru_input = keras.layers.Input(batch_input_shape=(1, 1, 64),                                     name=""gru_input"")     gru_state_in = keras.layers.Input(batch_input_shape=(1, 128),                                        name=""gru_state_in"")     gru_output, gru_state_out = keras.layers.GRU(128,                                                  activation=""tanh"",                                                  recurrent_activation=""sigmoid"",                          )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,knebojsa11,[RNN] Conversion of model containing GRU layer to quantized TFLite causes Segmentation Fault," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04  TensorFlow installation (pip package or built from source): both pip package and built from source  TensorFlow library (version, if pip package or github SHA, if built from source): 2.18  2. Code import os os.environ[""TF_USE_LEGACY_KERAS""] = ""1"" import tensorflow as tf import numpy as np import tf_keras as keras def gru_model():     """"""Factory method for gru model.""""""     gru_input = keras.layers.Input(batch_input_shape=(1, 1, 64),                                     name=""gru_input"")     gru_state_in = keras.layers.Input(batch_input_shape=(1, 128),                                        name=""gru_state_in"")     gru_output, gru_state_out = keras.layers.GRU(128,                                                  activation=""tanh"",                                                  recurrent_activation=""sigmoid"",                          ",2025-02-18T11:08:09Z,type:support comp:lite TFLiteConverter TF 2.18,open,0,3,https://github.com/tensorflow/tensorflow/issues/87468,"Hi,   I apologize for the delay in my response, I have been able to replicate the similar behavior from my end for reference here is gistfile so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention. Thank you for your cooperation and patience.","Hi,  I apologize for the delay in my response, The error occurs because GRU layers have complex state management that conflicts with full integer quantization so if full integer quantization is not compulsary for your use case/ project you can go with either float16 Quantization or dynamic range quantization which is more compatible with RNN layers.  This is a limitation in the TensorFlow Lite quantization system when dealing with complex recurrent structures. GRU and LSTM layers involve internal states and complex cell implementations that the quantization process sometimes cannot properly handle resulting in segmentation faults during model calibration. I have tried from my end with float16 Quantization and dynamic range quantization it's working as expected for your reference here is gistfile Thank you for your cooperation and patience.",Dear   I really need the model to be fully quantized so I am interested if there are plans to fix the broken feature in the converter. Best Regards
735,"以下是一个github上的tensorflow下的一个issue, 标题是([CUDA] illegal memory read in ShuffleInTensor3Simple)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuDNN 90300  GPU model and memory H100 80G  Current behavior? computesanitizer detects invalid memory read.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,jwnhy,[CUDA] illegal memory read in ShuffleInTensor3Simple, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuDNN 90300  GPU model and memory H100 80G  Current behavior? computesanitizer detects invalid memory read.  Standalone code to reproduce the issue   Relevant log output ,2025-02-18T05:14:25Z,type:bug comp:gpu TF 2.8,open,0,3,https://github.com/tensorflow/tensorflow/issues/87454,"Hi **** , Apologies for the delay, and thanks for raising your concern here. I observed that you are using an older version TensorFlow 2.8 which might be causing the issue. Could you please try using the latest version for better results? I ran your code on Colab using TensorFlow 2.18.0 with GPU, and it produced the following proper error message: ` ResourceExhaustedError: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[1,79768,65470] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator gpu_async_0 [Op:StatelessRandomUniformV2] name:  ` This error indicates that the GPU is running out of memory (OOM) due to large tensor allocations. To resolve this, I reduced the memory requirements based on the model, and it worked fine for me. I am providing a gist here for your reference. Thank you!",Sorry I mistyped the version number... I found this issue on the latest tensorflow 2.18. I am using an H100 with 80GiB ram to find this issue. Can you try to reproduce this on a card with more ram?,The following is the environment I am using. 
762,"以下是一个github上的tensorflow下的一个issue, 标题是([CUDA] illegal memory write in ShuffleInTensor3Simple)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.8.1  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuDNN: 90300  GPU model and memory H100 80G  Current behavior? computesanitizer reports Illegal Memory Access. Expect: no IMA should occur.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,jwnhy,[CUDA] illegal memory write in ShuffleInTensor3Simple, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.8.1  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuDNN: 90300  GPU model and memory H100 80G  Current behavior? computesanitizer reports Illegal Memory Access. Expect: no IMA should occur.  Standalone code to reproduce the issue   Relevant log output ,2025-02-18T04:22:19Z,type:bug comp:gpu TF 2.8,open,0,2,https://github.com/tensorflow/tensorflow/issues/87438,"Hi **** , Apologies for the delay, and thanks for your patience. I ran your code on Colab using TensorFlow 2.18.0, and it threw the following error:  This error occurs because the program is trying to allocate more GPU memory than is available. The issue seems to be caused by the large number of filters used in your model. To resolve this, I reduced the number of filters, and it worked fine for me. I am attaching a gist here for your reference. Thank you!",Sorry I mistyped the version number... I found this issue on the latest tensorflow 2.18. I am using an H100 with 80GiB ram to find this issue. Can you try to reproduce this on a card with more ram?
1162,"以下是一个github上的tensorflow下的一个issue, 标题是(NaN loss on multi-gpu training)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version v1.12.1122039g784fed5357b 2.20.0dev20250211  Custom code Yes  OS platform and distribution Amazon Linux 2  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 12.3 (Build V12.3.107)  GPU model and memory GPU Model: NVIDIA A10G Memory per GPU: 24 GB  Current behavior? I find that training a distributed model  with either MirroredStrategy or MultiWorkerMirroredStrategy  that the loss jumps to NaN on the 61st batch. The problem vanishes if I train on a single GPU (no distribute strategy), or if I use tf 2.13.1. The problem also vanishes if I set the number of steps per epoch to be less than 61. I've attached some reproducible code but note that I find the exact same finding (the role of batch 61, single GPU, v.2.13) on other datasets, othe)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,datasciantle,NaN loss on multi-gpu training," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version v1.12.1122039g784fed5357b 2.20.0dev20250211  Custom code Yes  OS platform and distribution Amazon Linux 2  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 12.3 (Build V12.3.107)  GPU model and memory GPU Model: NVIDIA A10G Memory per GPU: 24 GB  Current behavior? I find that training a distributed model  with either MirroredStrategy or MultiWorkerMirroredStrategy  that the loss jumps to NaN on the 61st batch. The problem vanishes if I train on a single GPU (no distribute strategy), or if I use tf 2.13.1. The problem also vanishes if I set the number of steps per epoch to be less than 61. I've attached some reproducible code but note that I find the exact same finding (the role of batch 61, single GPU, v.2.13) on other datasets, othe",2025-02-17T21:04:11Z,stat:awaiting response type:bug stale comp:gpu TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/87432,"Hi **** , Apologies for the delay, and thanks for raising your concern here. I tried running your code using TensorFlow 2.18.0 (stable) on VM instances, but I did not encounter any issues. I am attaching the output below for your reference. And I recommend using stable versions for better results.  ``` (tf_env) maayaragpu1:~$ python3 test.py steps_per_epoch 61 strategy MultiWorkerMirroredStrategy 20250305 11:01:28.725763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741172488.746097   33345 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741172488.752394   33345 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250305 11:01:28.774422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. I0000 00:00:1741172491.290716   33345 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  > device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5 I0000 00:00:1741172491.293431   33345 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13764 MB memory:  > device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5 Using MultiWorkerMirroredStrategy. Model: ""functional"" ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ Layer (type)                  ┃ Output Shape              ┃         Param  ┃ Connected to               ┃ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩ │ input_features (InputLayer)   │ (None, 256)               │               0 │                           │ ├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤ │ embedding (Embedding)         │ (None, 256, 64)           │          16,384 │ input_features[0][0]       │ ├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤ │ mha_0 (MultiHeadAttention)    │ (None, 256, 64)           │          16,640 │ embedding[0][0],           │ │                               │                           │                 │ embedding[0][0]            │ ├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤ │ mha_1 (MultiHeadAttention)    │ (None, 256, 64)           │          16,640 │ mha_0[0][0], mha_0[0][0]   │ ├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤ │ the_label (Dense)             │ (None, 256, 256)          │          16,640 │ mha_1[0][0]                │ └───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘  Total params: 66,304 (259.00 KB)  Trainable params: 66,304 (259.00 KB)  Nontrainable params: 0 (0.00 B) Training with batch size 64 and steps per epoch 61 with 2 replicas. Epoch 1/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 10s 48ms/step  loss: 5.5452  val_loss: 5.5454 Epoch 2/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step  loss: 5.5448  val_loss: 5.5454 . . . . Epoch 97/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step  loss: 1.3885  val_loss: 1.3389 Epoch 98/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step  loss: 1.4202  val_loss: 1.3394 Epoch 99/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 3s 44ms/step  loss: 1.3504  val_loss: 1.3385 Epoch 100/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 3s 44ms/step  loss: 1.4391  val_loss: 1.3389 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1223,"以下是一个github上的tensorflow下的一个issue, 标题是(Memory leak when compiling tfp.util.TransformedVariable since TF 2.14 (worked fine before!))， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.14+ (present in 2.14, 2.16, 2.18; not an issue in 2.11, 2.12, 2.13)  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Upgrading a GPflowbased workflow to use TF 2.14 is showing memory leaks where none previously occurred. Preliminary investigations connected this to the compilation of GPflow models: in the code snippet below the memory usage increases over time (by around 1MB per iteration for a total of ~200MB) when run with TF 2.14, 2.16 or 2.18, but not when run with TF 2.11, 2.12 or 2.13. Raising as a TF bug as code that was previously executing fine is now appearing to leak memory, though there is al)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,uri-granta,Memory leak when compiling tfp.util.TransformedVariable since TF 2.14 (worked fine before!)," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.14+ (present in 2.14, 2.16, 2.18; not an issue in 2.11, 2.12, 2.13)  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Upgrading a GPflowbased workflow to use TF 2.14 is showing memory leaks where none previously occurred. Preliminary investigations connected this to the compilation of GPflow models: in the code snippet below the memory usage increases over time (by around 1MB per iteration for a total of ~200MB) when run with TF 2.14, 2.16 or 2.18, but not when run with TF 2.11, 2.12 or 2.13. Raising as a TF bug as code that was previously executing fine is now appearing to leak memory, though there is al",2025-02-17T11:48:58Z,type:bug type:performance TF 2.18,open,0,5,https://github.com/tensorflow/tensorflow/issues/87414,"Hi **granta** , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow 2.13.0 it is working as expected and 2.18.0 faced the same issue like you. The main cause might be version compatibility. When using GPflow, you need to install the GPflow package along with its dependencies TensorFlow and TensorFlow Probability ensuring all versions are compatible. According to the GPflow documentation, compatibility is mentioned only up to TensorFlow 2.12. Therefore, the exact TensorFlow Probability version required for the latest TensorFlow versions is unknown. I recommend first confirming the compatibility versions in the GPflow documentation. It would be best to check with GPflow for the correct compatibility versions for the latest TensorFlow releases. Thank you!","Hi , Thanks for looking into this! **I've since managed to reproduce the issue without using `gpflow` at all, just with a `TransformedVariable` from `tensorflowprobability`.** (Also FYI `gpflow` does actually support tensorflow up to 2.16, though it looks like the docs aren't up to date!) As before, the following script shows continually increasing memory usage when run with TF 2.1418 but not when run with TF 2.1113, though the memory increase is smaller than before (only around 0.1MB per iteration) and takes a few iterations to get going. ","FYI here's an even simpler class that can be used in the previous example:  As before, setting the bijector to `tfp.bijectors.Identity()` removes the leak. However, changing this bijector's `_forward(self, x)` method to return `x + 0` rather than `x` reintroduces it. AFAICT the bijector is never evaluated in the test, so this is presumably all to do with compilation.","So it looks like the issue in this particular example is that in every compilation, a pair of weak references to the input and output `SymbolicTensor`'s aren't being garbage collected, which in turn means that their cleanup callbacks aren't being called to remove them from the global `BijectorCache` (`tfp.bijectors.bijector._cache`). By contrast, both in TF=2.14 both references *are* garbage collected and the callbacks *are* called. Note that the `BijectorCache` code in tfp hasn't changed in the last 4 years, and the `gpflow` example from the beginning continues to show issues even after manually disabling the cache, so I suspect the cache is just a symptom and the underlying cause is somehow connected to why the compiled graphs aren't being garbage collected (or at least why their cleanup callbacks aren't being called).","So I'm pretty sure the `BijectoCache` issue is due to https://github.com/tensorflow/tensorflow/commit/333ee99ecb9bd8f122c683defa74281bb3bd1664. Before this change, when initialising a `Function`, TF used to call `TracingCompiler._get_concrete_function_internal_garbage_collected`, which ended up creating and deleting a `ConcreteFunctionGarbageCollector`, dismantling the function graph at the end. After this change, when initialising a `Function`, TF now calls `tracing_compilation.trace_function`, which similarly creates a `ConcreteFunctionGarbageCollector`. However, because `tracing_options.bind_graph_to_function` is `False` (the default value, unchanged by `Function._generate_tracing_options`) it also calls `release` on the garbage collector before deleting it, preventing it from dismantling the function graph (and therefore from cleaning up the bijector cache). Note though that this probably isn't the whole story. Changing `Function._initialize` to use `dataclasses.replace(self._variable_creation_config, bind_graph_to_function=True)` instead does fix the memory leak in the two bijector examples above. However, it doesn't fix the original leaks discovered in `gpflow` (which based on profiling may now be connected to the `gradient_registry` somehow)."
1129,"以下是一个github上的tensorflow下的一个issue, 标题是(Warning in XNNPACK: unable to enable JIT: not compiled with JIT enabled)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.1  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version GCC 9 aarch cross compiler  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Upon updating our tflite service to 2.17.1, XNNPACK fails to apply with this cryptic error  `Warning in XNNPACK: unable to enable JIT: not compiled with JIT enabled` I don't understand this error at all, there is really only way to build XNNPACK while building TFLite as described in CMake, so I have no idea what this is referring to. We didn't face this issue with earlier versions (2.7.0)  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,AD-lite24,Warning in XNNPACK: unable to enable JIT: not compiled with JIT enabled," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.1  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version GCC 9 aarch cross compiler  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Upon updating our tflite service to 2.17.1, XNNPACK fails to apply with this cryptic error  `Warning in XNNPACK: unable to enable JIT: not compiled with JIT enabled` I don't understand this error at all, there is really only way to build XNNPACK while building TFLite as described in CMake, so I have no idea what this is referring to. We didn't face this issue with earlier versions (2.7.0)  Standalone code to reproduce the issue   Relevant log output ",2025-02-16T14:38:54Z,stat:awaiting response type:support stale comp:lite comp:lite-xnnpack 2.17,open,0,9,https://github.com/tensorflow/tensorflow/issues/87385,"Hi, lite24  Thank you for bringing this issue to our attention, I believe you followed this official documentation Build LiteRT with CMake, if possible could you please help me with exact steps before encountering mentioned warning to replicate the same behavior from my end ? Thank you for your cooperation and understanding.","Hi   Please find the CMakeLists.txt we use to build the project (tflite is built alongside it)  and our build script  with patches  Once the project is built, the entry point simply invokes the interpreter in a standard manner and tries to apply the delegate. There is a an issue in the cross compilation process with respect to protobuf where the protobuf library built is built for the target but the host tries to use it. We resolved it by manually building protobuf for the host architecture and setting the appropriate system paths.", Was there any update on this? ,"Hi, lite24  If possible could you please add this `set(XNNPACK_ENABLE_JIT ON CACHE BOOL ""Enable JIT in XNNPACK"")` in CMakeLists.txt and see is it resolving your issue or not ? after updating the CMake configuration clean your build directory and recompile to apply the changes. Thank you for your cooperation and patience.",Hi   Thank you for the reply I was not aware that such a flag existed. Yes that error was resolved but a new error has popped up  This error also did not used to occur earlier and has only come up once we upgraded tflite. Could you please tell us why this might be occuring,"Hi, lite24 Good to hear that initial reported issue got resolved, Troubleshooting TFLite delegate application failures can be challenging.  I recommend the following approaches: Increase logging verbosity to obtain more detailed information about the delegate application process and also verify the model compatibility with the chosen delegate. You can run the model without any delegates to ensure that the model itself is functioning correctly. This helps isolate whether the issue is with the model or the delegate.  Please check if the tensors are already owned by another delegate. The error message `tensor>delegate == nullptr  tensor>delegate == delegate was not true` indicates a conflict please ensure that tensors are not being shared between delegates.You can do this by inspecting the tensor's delegate before applying a new one.  Please check for unsupported operations some operations in your model may not be supported by the delegate. For XNNPACK, you can find the list of supported operations in the XNNPACK documentation and for GPU delegates  Thank you for your cooperation and understanding."," As you said it likely fails due to tensors being owned by two delegates. The issue occurs due to a failure to apply GPU delegate (which is a separate issue), and it then falls back to XNNPACK to run on CPU. If I don't apply GPU delegates XNNPACK works fine.  This I believe is a bug that did not occur in older versions, the fallback to XNNPACK is likely not changing ownership of the tensors.","Hi, lite24  I apologize for the delayed response, if possible could you please give it try with latest stable version of `TensorFlow 2.19.0` and `tfnightly` and see is it resolving your issue or not ? Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.
748,"以下是一个github上的tensorflow下的一个issue, 标题是(Colab TPU crash on transformer import)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.18  Custom code No  OS platform and distribution Google colab default  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Import transformer module get crash on a colab notebook with TPU   Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,khangtruong2252314,Colab TPU crash on transformer import, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.18  Custom code No  OS platform and distribution Google colab default  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Import transformer module get crash on a colab notebook with TPU   Standalone code to reproduce the issue   Relevant log output ,2025-02-16T13:13:48Z,type:bug,closed,0,5,https://github.com/tensorflow/tensorflow/issues/87383,Are there any error messages during the installation process? It could help identify what is causing the issue,"No, sorry. I just open the colab and import, then crash. Perhaps you now can recreate it quickly as it is the default colab session. There are some session log that might be useful.","Is there something in the error message that points to Tensorflow? Or, in other words, should this be opened on Colab repo or on transformers repo?","Ah, sorry, I was working with tensorflow that I didn’t realize it was colab’s fault. ",Are you satisfied with the resolution of your issue? Yes No
1150,"以下是一个github上的tensorflow下的一个issue, 标题是(GPU and CPU utilization dropping to 0% during long training runs)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.1 LTS  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 2.14  GPU model and memory _No response_  Current behavior? When running the attached test script for longer, at some point during the training GPU and CPU utilization will both fall to 0%, although neither VRAM nor RAM are exhausted. The training on my NVIDIA L40S vGPU slows from ~35ms/step with batch size 256 to minutes per step. Training speed only recovers on reboot. Larger batch sizes will make the error occurr later during training. I am unsure if this is an issue of my machine/the vGPU configuration.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,MoritzKronberger,GPU and CPU utilization dropping to 0% during long training runs," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.1 LTS  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 2.14  GPU model and memory _No response_  Current behavior? When running the attached test script for longer, at some point during the training GPU and CPU utilization will both fall to 0%, although neither VRAM nor RAM are exhausted. The training on my NVIDIA L40S vGPU slows from ~35ms/step with batch size 256 to minutes per step. Training speed only recovers on reboot. Larger batch sizes will make the error occurr later during training. I am unsure if this is an issue of my machine/the vGPU configuration.  Standalone code to reproduce the issue   Relevant log output ",2025-02-15T21:12:30Z,type:bug TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/87366,"Hi **** , Thanks for raising your concern here. Could you please provide information about the cuDNN version you are using? There might be a mismatch in version compatibility. Please check all the compatible versions to avoid issues. I am providing the documentation for your reference. I also tried with reduced sizes, and it worked fine for me. Please find gist1 and gist2 here for your reference. Thank you!","I forgot to add `tf.config.experimental.set_device_policy(""warn"")` to the standalone code, which produces the log output on my local machine. Trying this in Colab did not produce the same logs, but I am usure if Colab is filtering logs. The crash happens after ~200 epochs on my machine (without reduced sizes), 10 epochs are not an issue. Regarding cuDNN: `nvidiacudnncu12=9.3.0.75` and `nvidiacudnncu11=8.5.0.96` are installed, according to TF logs 9.3.0.75 is used."
499,"以下是一个github上的tensorflow下的一个issue, 标题是(Regenerate tf_generated_ops.td after adding a new attribute to the WriteTrainingPredictions custom op to support writing vector predictions to file storage.)， 内容是 (Regenerate tf_generated_ops.td after adding a new attribute to the WriteTrainingPredictions custom op to support writing vector predictions to file storage.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Regenerate tf_generated_ops.td after adding a new attribute to the WriteTrainingPredictions custom op to support writing vector predictions to file storage.,Regenerate tf_generated_ops.td after adding a new attribute to the WriteTrainingPredictions custom op to support writing vector predictions to file storage.,2025-02-15T01:13:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87343
451,"以下是一个github上的tensorflow下的一个issue, 标题是(Use seprate collective resource when scheduling p2p communication)， 内容是 (Use seprate collective resource when scheduling p2p communication This is in preparation of removing all the 4 existing p2p resources. We are simplifying the pipeline parallelism implementation here.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Use seprate collective resource when scheduling p2p communication,Use seprate collective resource when scheduling p2p communication This is in preparation of removing all the 4 existing p2p resources. We are simplifying the pipeline parallelism implementation here.,2025-02-13T23:09:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87272
333,"以下是一个github上的tensorflow下的一个issue, 标题是([xla:python] Remove unused _single_device_array_to_np_array on ArrayImpl.)， 内容是 ([xla:python] Remove unused _single_device_array_to_np_array on ArrayImpl.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[xla:python] Remove unused _single_device_array_to_np_array on ArrayImpl.,[xla:python] Remove unused _single_device_array_to_np_array on ArrayImpl.,2025-02-13T19:18:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87256
1165,"以下是一个github上的tensorflow下的一个issue, 标题是(Tensorflow in pycharm setup issue)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.12.0  Custom code Yes  OS platform and distribution windows 11  Mobile device windows11  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? error while running the file predeiction.py                                                                                                                                                      (c) Microsoft Corporation. All rights reserved. (venv_new) C:\analytics_driven_enginemaster\analytics_driven_enginemaster>python m analysis.prediction [20250213 15:38:26,485] INFO  Starting prediction pipeline... ['Number' 'Opened' 'State' 'Symptom category' 'Short description' 'State'] Traceback (most recent call last):   File ""C:\analytics_driven_enginemaster\an)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,KeskoPreeti,Tensorflow in pycharm setup issue," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.12.0  Custom code Yes  OS platform and distribution windows 11  Mobile device windows11  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? error while running the file predeiction.py                                                                                                                                                      (c) Microsoft Corporation. All rights reserved. (venv_new) C:\analytics_driven_enginemaster\analytics_driven_enginemaster>python m analysis.prediction [20250213 15:38:26,485] INFO  Starting prediction pipeline... ['Number' 'Opened' 'State' 'Symptom category' 'Short description' 'State'] Traceback (most recent call last):   File ""C:\analytics_driven_enginemaster\an",2025-02-13T10:24:36Z,stat:awaiting response type:build/install subtype:windows TF 2.12,closed,0,3,https://github.com/tensorflow/tensorflow/issues/87226,", Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:  Also kindly provide the environment details and the steps followed to install the tensorflow. CC(Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.) Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Please always search for similar issues before opening duplicates,Are you satisfied with the resolution of your issue? Yes No
549,"以下是一个github上的tensorflow下的一个issue, 标题是([Function runtime] Avoid copying reachable function definitions when graph collection is disabled.)， 内容是 ([Function runtime] Avoid copying reachable function definitions when graph collection is disabled. Additionally, avoid copying each function during the `UpdateTPUEmbeddingModePass` for the common case where the function does not include any TPUEmbedding layer ops.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[Function runtime] Avoid copying reachable function definitions when graph collection is disabled.,"[Function runtime] Avoid copying reachable function definitions when graph collection is disabled. Additionally, avoid copying each function during the `UpdateTPUEmbeddingModePass` for the common case where the function does not include any TPUEmbedding layer ops.",2025-02-13T07:32:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87219
247,"以下是一个github上的tensorflow下的一个issue, 标题是(internal BUILD rule visibility)， 内容是 (internal BUILD rule visibility)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],internal BUILD rule visibility,internal BUILD rule visibility,2025-02-13T06:28:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87217
533,"以下是一个github上的tensorflow下的一个issue, 标题是(Enable `bazel build --nobuild` to prevent network flakes for TensorFlow builds)， 内容是 (Enable `bazel build nobuild` to prevent network flakes for TensorFlow builds Removes the usage of their `py_cpp_test_filters` config which is incompatible with `bazel build nobuild` and instead replicate the effect of the config by specifying bazel options explicitly.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Enable `bazel build --nobuild` to prevent network flakes for TensorFlow builds,Enable `bazel build nobuild` to prevent network flakes for TensorFlow builds Removes the usage of their `py_cpp_test_filters` config which is incompatible with `bazel build nobuild` and instead replicate the effect of the config by specifying bazel options explicitly.,2025-02-13T00:19:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87200
291,"以下是一个github上的tensorflow下的一个issue, 标题是(Split `CompileOnlyIfRtClient` into its own directory)， 内容是 (Split `CompileOnlyIfRtClient` into its own directory)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Split `CompileOnlyIfRtClient` into its own directory,Split `CompileOnlyIfRtClient` into its own directory,2025-02-12T23:29:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87196
447,"以下是一个github上的tensorflow下的一个issue, 标题是(Ensure InterpreterFactoryImpl's constructor is kept)， 内容是 (Ensure InterpreterFactoryImpl's constructor is kept Add  annotation to the constructor, ensuring it's kept even if `keep class X` rule semantics change as it relates to keeping the default (empty) constructor.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Ensure InterpreterFactoryImpl's constructor is kept,"Ensure InterpreterFactoryImpl's constructor is kept Add  annotation to the constructor, ensuring it's kept even if `keep class X` rule semantics change as it relates to keeping the default (empty) constructor.",2025-02-12T19:36:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87180
641,"以下是一个github上的tensorflow下的一个issue, 标题是(Add subgraph_input_names(), subgraph_output_names() to the SignatureRunner)， 内容是 (Add subgraph_input_names(), subgraph_output_names() to the SignatureRunner In LiteRT, it assumes that the order of names are aligned with Subgraph. But the existing input_names(), output_names() API doesn't follow it, also there are customers who rely on the legacy order. This cl creates these new methods which returns the names which reflects the underlying Subgraph I/O order.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],"Add subgraph_input_names(), subgraph_output_names() to the SignatureRunner","Add subgraph_input_names(), subgraph_output_names() to the SignatureRunner In LiteRT, it assumes that the order of names are aligned with Subgraph. But the existing input_names(), output_names() API doesn't follow it, also there are customers who rely on the legacy order. This cl creates these new methods which returns the names which reflects the underlying Subgraph I/O order.",2025-02-12T19:06:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87175
1290,"以下是一个github上的tensorflow下的一个issue, 标题是(CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""pthreadpool"" that is not in any export set.)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.20.0  Custom code No  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following the indications found here: https://ai.google.dev/edge/litert/build/cmakebuild_installable_package I'm trying to build tensorflowlite as installable package:     (base) raphy:~/Grasp/tflite_build$ cmake ../../tensorflow_src/tensorflow/lite/ \     > DTFLITE_ENABLE_INSTALL=ON \     > DCMAKE_FIND_PACKAGE_PREFER_CONFIG=ON \     > DSYSTEM_FARMHASH=ON \     > DSYSTEM_PTHREADPOOL=ON \     > DEigen3_DIR=/home/raphy/Grasp/eigen/share/eigen3/cmake \     > DFlatBuffers_DIR=/home/raphy/Grasp/flatbuffers/lib/cmake/flatbuffers \     > Dcpuinfo_DIR)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,raphael10-collab,"CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""pthreadpool"" that is not in any export set.", Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.20.0  Custom code No  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following the indications found here: https://ai.google.dev/edge/litert/build/cmakebuild_installable_package I'm trying to build tensorflowlite as installable package:     (base) raphy:~/Grasp/tflite_build$ cmake ../../tensorflow_src/tensorflow/lite/ \     > DTFLITE_ENABLE_INSTALL=ON \     > DCMAKE_FIND_PACKAGE_PREFER_CONFIG=ON \     > DSYSTEM_FARMHASH=ON \     > DSYSTEM_PTHREADPOOL=ON \     > DEigen3_DIR=/home/raphy/Grasp/eigen/share/eigen3/cmake \     > DFlatBuffers_DIR=/home/raphy/Grasp/flatbuffers/lib/cmake/flatbuffers \     > Dcpuinfo_DIR,2025-02-12T18:05:05Z,type:build/install comp:lite subtype: ubuntu/linux,open,0,3,https://github.com/tensorflow/tensorflow/issues/87172,"Hi, collab I apologize for the delayed response, I see a similar issue please refer this workaround mentioned in this comment and see is it resolving your issue or not ? If issue still persists please let us know with updated error log for further investigation from our end.  Thank you for your cooperation and patience.","Hi   I did everything from scratch      (base) raphy:~$ mkdir NEW     (base) raphy:~$ cd NEW     (base) raphy:~/NEW$ git clone recursesubmodules https://github.com/tensorflow/tensorflow.git     Cloning into 'tensorflow'...     remote: Enumerating objects: 1977738, done.     remote: Counting objects: 100% (1682/1682), done.     remote: Compressing objects: 100% (758/758), done.     remote: Total 1977738 (delta 1298), reused 936 (delta 924), packreused 1976056 (from 3)     Receiving objects: 100% (1977738/1977738), 1.08 GiB | 31.86 MiB/s, done.     Resolving deltas: 100% (1627307/1627307), done.     Updating files: 100% (34863/34863), done.     (base) raphy:~/NEW$ mkdir tflite_build According to the comment https://github.com/tensorflow/tensorflow/issues/57658issuecomment1534245153 I should modify the XNNPACK's CMakeLists.txt and the ptreadpool's CMakeLists.txt files :       (base) raphy:~/NEW/tensorflow$ find name  ""CMakeLists.txt""     ./tensorflow/lite/kernels/CMakeLists.txt     ./tensorflow/lite/CMakeLists.txt     ./tensorflow/lite/examples/minimal/CMakeLists.txt     ./tensorflow/lite/examples/label_image/CMakeLists.txt     ./tensorflow/lite/profiling/proto/CMakeLists.txt     ./tensorflow/lite/tools/benchmark/CMakeLists.txt     ./tensorflow/lite/tools/cmake/modules/ml_dtypes/CMakeLists.txt     ./tensorflow/lite/tools/cmake/modules/farmhash/CMakeLists.txt     ./tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt     ./tensorflow/lite/tools/cmake/modules/xnnpack/CMakeLists.txt     ./tensorflow/lite/tools/cmake/native_tools/flatbuffers/CMakeLists.txt     ./tensorflow/lite/c/CMakeLists.txt     ./tensorflow/tools/pip_package/xla_build/CMakeLists.txt     ./tensorflow/tools/pip_package/xla_build/pip_test/CMakeLists.txt     ./tensorflow/core/example/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/cmake/modules/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/utils/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/IR/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/analysis/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/utils/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/tools/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/tools/mlirhloopt/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/transforms/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/deallocation/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/deallocation/utils/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/deallocation/transforms/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/stablehlo_ext/IR/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/stablehlo_ext/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/bindings/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/bindings/c/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/bindings/python/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/tests/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/tests/python/CMakeLists.txt But the XNNPACK's CMakeLists.txt file contains just these lines :      (base) raphy:~/NEW/tensorflow$ cat ./tensorflow/lite/tools/cmake/modules/xnnpack/CMakeLists.txt           Copyright 2022 The TensorFlow Authors. All Rights Reserved.           Licensed under the Apache License, Version 2.0 (the ""License"");      you may not use this file except in compliance with the License.      You may obtain a copy of the License at                https://www.apache.org/licenses/LICENSE2.0           Unless required by applicable law or agreed to in writing, software      distributed under the License is distributed on an ""AS IS"" BASIS,      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.      See the License for the specific language governing permissions and      limitations under the License.      The ""mcpu="" switch might be introduced externaly into CMake: either in _FLAGS or      as part of CC, CXX, ASM environmental variables (to be stored in CMAKE__COMPILER_ARG1).      This switch is not compatible with XNNPACK build mechanism and causes the XNNPACK compilation      break due to ""unsupported instructions"". This switch needs to be removed for XNNPACK      In order to isolate the changes only for XNNPACK and its depencencies, a subfolder is      introduced.     foreach(FLAG IN ITEMS CMAKE_ASM_FLAGS CMAKE_ASM_COMPILER_ARG1 CMAKE_C_FLAGS CMAKE_C_COMPILER_ARG1 CMAKE_CXX_FLAGS CMAKE_CXX_COMPILER_ARG1)       if(${FLAG})         string(REGEX REPLACE ""mcpu=[azAZ09_.^$*+?]*"" """" _tmp ${${FLAG}})         set(${FLAG} ${_tmp})       endif()     endforeach()     add_subdirectory(       ""${xnnpack_SOURCE_DIR}""       ""${xnnpack_BINARY_DIR}"" and not the lines that, according to the comment, should be modified And, as you can see from the list of CMakeLists.txt files, the CMakeLists.txt file for pthreadpool is not present What am I missing and/or doing wrong? How to make it work?",after i finished cross compiling it seems like there is a symbol missing 
1109,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #22541: [ROCm] Cleanup atomics support)， 内容是 (PR CC(Java process crashes during model loading): [ROCm] Cleanup atomics support Imported from GitHub PR https://github.com/openxla/xla/pull/22541 Weaken the ordering barriers to match what atomicAdd does on rocm. Emulate fp16 atomic on top of packed fp16 atomic where possible. Also for bfloat16 atomics, albeit those don't get matched right now due to FloatNormalization. Left in support for fp16 and bfloat16 vector atomics. We might enable the vectorization for them in the future if we can prove the access satisfies 4byte aligment. Copybara import of the project:  b4ac2bc984b40bb33f287a4ed351b6c1560e6895 by Dragan Mladjenovic : [ROCm] Cleanup atomics support Merging this change closes CC(Java process crashes during model loading) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22541 from ROCm:atomics_cleanup b4ac2bc984b40bb33f287a4ed351b6c1560e6895)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],PR #22541: [ROCm] Cleanup atomics support,"PR CC(Java process crashes during model loading): [ROCm] Cleanup atomics support Imported from GitHub PR https://github.com/openxla/xla/pull/22541 Weaken the ordering barriers to match what atomicAdd does on rocm. Emulate fp16 atomic on top of packed fp16 atomic where possible. Also for bfloat16 atomics, albeit those don't get matched right now due to FloatNormalization. Left in support for fp16 and bfloat16 vector atomics. We might enable the vectorization for them in the future if we can prove the access satisfies 4byte aligment. Copybara import of the project:  b4ac2bc984b40bb33f287a4ed351b6c1560e6895 by Dragan Mladjenovic : [ROCm] Cleanup atomics support Merging this change closes CC(Java process crashes during model loading) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22541 from ROCm:atomics_cleanup b4ac2bc984b40bb33f287a4ed351b6c1560e6895",2025-02-12T14:48:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87158
504,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use)， 内容是 ([XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use in tests. This is to work around issues of test parametrization while `xla_gpu_enable_triton_gemm_any` needs to be worked around in the main compiler path for A100.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,copybara-service[bot],[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use,[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use in tests. This is to work around issues of test parametrization while `xla_gpu_enable_triton_gemm_any` needs to be worked around in the main compiler path for A100.,2025-02-12T09:32:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87148
504,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use)， 内容是 ([XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use in tests. This is to work around issues of test parametrization while `xla_gpu_enable_triton_gemm_any` needs to be worked around in the main compiler path for A100.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,copybara-service[bot],[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use,[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use in tests. This is to work around issues of test parametrization while `xla_gpu_enable_triton_gemm_any` needs to be worked around in the main compiler path for A100.,2025-02-12T09:23:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87143
504,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use)， 内容是 ([XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use in tests. This is to work around issues of test parametrization while `xla_gpu_enable_triton_gemm_any` needs to be worked around in the main compiler path for A100.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,copybara-service[bot],[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use,[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use in tests. This is to work around issues of test parametrization while `xla_gpu_enable_triton_gemm_any` needs to be worked around in the main compiler path for A100.,2025-02-12T07:51:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87109
229,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal change only.)， 内容是 (Internal change only.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal change only.,Internal change only.,2025-02-11T23:41:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87090
247,"以下是一个github上的tensorflow下的一个issue, 标题是(internal BUILD rule visibility)， 内容是 (internal BUILD rule visibility)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],internal BUILD rule visibility,internal BUILD rule visibility,2025-02-11T23:29:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87088
489,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Disable `--xla_gpu_triton_gemm_any` on Ampere.)， 内容是 ([XLA:GPU] Disable `xla_gpu_triton_gemm_any` on Ampere. Triton's conversion logic from `f16` to `f8e5m2` is wrong preHopper. Disabling this wholesale is a bit overkill, but easiestsince this flag flip is what surfaced the issue in the first place.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,copybara-service[bot],[XLA:GPU] Disable `--xla_gpu_triton_gemm_any` on Ampere.,"[XLA:GPU] Disable `xla_gpu_triton_gemm_any` on Ampere. Triton's conversion logic from `f16` to `f8e5m2` is wrong preHopper. Disabling this wholesale is a bit overkill, but easiestsince this flag flip is what surfaced the issue in the first place.",2025-02-11T19:30:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87077
809,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #22509: [ROCm] Avoid hardcoding hipcc compiler includes)， 内容是 (PR CC(Make sure broken tests are filtered out in XLA tests suites.): [ROCm] Avoid hardcoding hipcc compiler includes Imported from GitHub PR https://github.com/openxla/xla/pull/22509 Copybara import of the project:  f4e7d6d91fa349eab54478a9f03875159378f237 by Dragan Mladjenovic : [ROCm] Avoid hardcoding hipcc compiler includes Merging this change closes CC(Make sure broken tests are filtered out in XLA tests suites.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22509 from ROCm:automatic_include f4e7d6d91fa349eab54478a9f03875159378f237)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],PR #22509: [ROCm] Avoid hardcoding hipcc compiler includes,PR CC(Make sure broken tests are filtered out in XLA tests suites.): [ROCm] Avoid hardcoding hipcc compiler includes Imported from GitHub PR https://github.com/openxla/xla/pull/22509 Copybara import of the project:  f4e7d6d91fa349eab54478a9f03875159378f237 by Dragan Mladjenovic : [ROCm] Avoid hardcoding hipcc compiler includes Merging this change closes CC(Make sure broken tests are filtered out in XLA tests suites.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22509 from ROCm:automatic_include f4e7d6d91fa349eab54478a9f03875159378f237,2025-02-11T14:02:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87055
846,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #22452: [ROCm] Fix missing header in hipblaslt_wrapper.h)， 内容是 (PR CC(Documentation for tf.train.init_from_checkpoint doesn't say what it does when): [ROCm] Fix missing header in hipblaslt_wrapper.h Imported from GitHub PR https://github.com/openxla/xla/pull/22452 Copybara import of the project:  225a63f3d9596e8827bf04904d9b8691de2b0bc7 by Dragan Mladjenovic : [ROCm] Fix missing header in hipblaslt_wrapper.h Merging this change closes CC(Documentation for tf.train.init_from_checkpoint doesn't say what it does when) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22452 from ROCm:hipblaslt_wrapper 225a63f3d9596e8827bf04904d9b8691de2b0bc7)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],PR #22452: [ROCm] Fix missing header in hipblaslt_wrapper.h,PR CC(Documentation for tf.train.init_from_checkpoint doesn't say what it does when): [ROCm] Fix missing header in hipblaslt_wrapper.h Imported from GitHub PR https://github.com/openxla/xla/pull/22452 Copybara import of the project:  225a63f3d9596e8827bf04904d9b8691de2b0bc7 by Dragan Mladjenovic : [ROCm] Fix missing header in hipblaslt_wrapper.h Merging this change closes CC(Documentation for tf.train.init_from_checkpoint doesn't say what it does when) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22452 from ROCm:hipblaslt_wrapper 225a63f3d9596e8827bf04904d9b8691de2b0bc7,2025-02-11T11:05:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87045
363,"以下是一个github上的tensorflow下的一个issue, 标题是([IFRT] Fully support the dtype arg to AssembleArrayFromSingleDeviceArrays in IFRT Proxy.)， 内容是 ([IFRT] Fully support the dtype arg to AssembleArrayFromSingleDeviceArrays in IFRT Proxy.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[IFRT] Fully support the dtype arg to AssembleArrayFromSingleDeviceArrays in IFRT Proxy.,[IFRT] Fully support the dtype arg to AssembleArrayFromSingleDeviceArrays in IFRT Proxy.,2025-02-11T05:26:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87006
1188,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #85477: Integrate Op Builder with LiteRT Compile Part)， 内容是 (PR CC(Integrate Op Builder with LiteRT Compile Part): Integrate Op Builder with LiteRT Compile Part Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/85477  WHAT We replace the compiler part with Qualcomm implementations. This PR include commits in 3 PRs below. Follow these PRs or commit should help you review. You can get more details in the PR descriptions. 1. https://github.com/jiunkaiy/tensorflow/pull/1 2. https://github.com/jiunkaiy/tensorflow/pull/3 3. https://github.com/jiunkaiy/tensorflow/pull/5  TEST You can checkout this branch and run this test:  I disable these models because I don't have them.  kFeedForwardModel,  kKeyEinsumModel,  kQueryEinsumModel,  kValueEinsumModel,  kAttnVecEinsumModel,  kROPEModel,  kLookUpROPEModel,  kRMSNormModel,  kSDPAModel,  kAttentionModel,  kTransformerBlockModel,  kQSimpleMul16x16Model,  kQMulAdd16x16Model,  kQQueryEinsum16x8Model,  kQKeyEinsum16x8Model,  kQVauleEinsum)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,copybara-service[bot],PR #85477: Integrate Op Builder with LiteRT Compile Part,"PR CC(Integrate Op Builder with LiteRT Compile Part): Integrate Op Builder with LiteRT Compile Part Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/85477  WHAT We replace the compiler part with Qualcomm implementations. This PR include commits in 3 PRs below. Follow these PRs or commit should help you review. You can get more details in the PR descriptions. 1. https://github.com/jiunkaiy/tensorflow/pull/1 2. https://github.com/jiunkaiy/tensorflow/pull/3 3. https://github.com/jiunkaiy/tensorflow/pull/5  TEST You can checkout this branch and run this test:  I disable these models because I don't have them.  kFeedForwardModel,  kKeyEinsumModel,  kQueryEinsumModel,  kValueEinsumModel,  kAttnVecEinsumModel,  kROPEModel,  kLookUpROPEModel,  kRMSNormModel,  kSDPAModel,  kAttentionModel,  kTransformerBlockModel,  kQSimpleMul16x16Model,  kQMulAdd16x16Model,  kQQueryEinsum16x8Model,  kQKeyEinsum16x8Model,  kQVauleEinsum",2025-02-10T19:51:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86971
385,"以下是一个github上的tensorflow下的一个issue, 标题是(This change:)， 内容是 (This change:   creates an empty class HloCustomCallInstruction::PerInstructionStorage, with a getter and a locked setter on the parent class that will act exactly once.   tests for above)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],This change:,"This change:   creates an empty class HloCustomCallInstruction::PerInstructionStorage, with a getter and a locked setter on the parent class that will act exactly once.   tests for above",2025-02-10T15:03:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86952
299,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Turn `--xla_gpu_triton_gemm_any` on by default.)， 内容是 ([XLA:GPU] Turn `xla_gpu_triton_gemm_any` on by default.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,copybara-service[bot],[XLA:GPU] Turn `--xla_gpu_triton_gemm_any` on by default.,[XLA:GPU] Turn `xla_gpu_triton_gemm_any` on by default.,2025-02-10T10:16:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86946
1086,"以下是一个github上的tensorflow下的一个issue, 标题是(Bug sur TensorFlow 2.13 multi-GPU : freeze lors du fitting)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.13.0  Custom code Yes  OS platform and distribution Macos 15.3 (worker 0) Macos 12.7.6(worker 1)  Mobile device _No response_  Python version 3.8.20  Bazel version ...  GCC/compiler version 16.0.0 (apple M3) 14.0.0 (intel iris)  CUDA/cuDNN version _No response_  GPU model and memory Apple M3 and Intel Iris Graphics 6100  Current behavior? When I train a TensorFlow model on several GPUs (with tf.distribute.MultiWorkerMirroredStrategy), the execution is blocked at  Build the model under the strategy No error message is displayed, but the process no longer progresses after •	 I followed the recommendations of the official documentation, but the problem persists.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,epokrso,Bug sur TensorFlow 2.13 multi-GPU : freeze lors du fitting," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.13.0  Custom code Yes  OS platform and distribution Macos 15.3 (worker 0) Macos 12.7.6(worker 1)  Mobile device _No response_  Python version 3.8.20  Bazel version ...  GCC/compiler version 16.0.0 (apple M3) 14.0.0 (intel iris)  CUDA/cuDNN version _No response_  GPU model and memory Apple M3 and Intel Iris Graphics 6100  Current behavior? When I train a TensorFlow model on several GPUs (with tf.distribute.MultiWorkerMirroredStrategy), the execution is blocked at  Build the model under the strategy No error message is displayed, but the process no longer progresses after •	 I followed the recommendations of the official documentation, but the problem persists.  Standalone code to reproduce the issue   Relevant log output ",2025-02-08T11:40:33Z,stat:awaiting tensorflower type:bug comp:gpu TF 2.13,open,0,1,https://github.com/tensorflow/tensorflow/issues/86897,"I was able to reproduce the same issue with a single GPU using TensorFlow 2.13. Below, I am attaching the output for your reference.  Thank you!"
592,"以下是一个github上的tensorflow下的一个issue, 标题是(Fixes a build failure in macos_arm64 environment that disallows `static_cast`ing 'const npy_intp *' (aka 'const long *') to 'const int64_t *' (aka 'const long long *').)， 内容是 (Fixes a build failure in macos_arm64 environment that disallows `static_cast`ing 'const npy_intp *' (aka 'const long *') to 'const int64_t *' (aka 'const long long *'). This is fixed by explicitly iterating and building the int64_t Span.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Fixes a build failure in macos_arm64 environment that disallows `static_cast`ing 'const npy_intp *' (aka 'const long *') to 'const int64_t *' (aka 'const long long *').,Fixes a build failure in macos_arm64 environment that disallows `static_cast`ing 'const npy_intp *' (aka 'const long *') to 'const int64_t *' (aka 'const long long *'). This is fixed by explicitly iterating and building the int64_t Span.,2025-02-07T20:19:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86854
331,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Add triton support test for ragged-all-to-all, rng-x and complex)， 内容是 ([XLA:GPU] Add triton support test for raggedalltoall, rngx and complex)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],"[XLA:GPU] Add triton support test for ragged-all-to-all, rng-x and complex","[XLA:GPU] Add triton support test for raggedalltoall, rngx and complex",2025-02-07T10:57:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86830
303,"以下是一个github上的tensorflow下的一个issue, 标题是([xla:cpu] Add run and setup scripts for e2e gemma2 pyTorch)， 内容是 ([xla:cpu] Add run and setup scripts for e2e gemma2 pyTorch)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,copybara-service[bot],[xla:cpu] Add run and setup scripts for e2e gemma2 pyTorch,[xla:cpu] Add run and setup scripts for e2e gemma2 pyTorch,2025-02-06T21:27:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86785
627,"以下是一个github上的tensorflow下的一个issue, 标题是(Remove Thunk::Cleanup method.)， 内容是 (Remove Thunk::Cleanup method. The method was effectively unused, since it wasn't overridden by SequentialThunk, and so SequentialThunk wouldn't call Cleanup on its subthunks. NcclRaggedAllToAllStartThunk overrode Cleanup to free some device buffers, but these were never freed since Cleanup was not called. The memory is now stored in DeviceMemoryHandles, which automatically free the buffers in the destructor.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Remove Thunk::Cleanup method.,"Remove Thunk::Cleanup method. The method was effectively unused, since it wasn't overridden by SequentialThunk, and so SequentialThunk wouldn't call Cleanup on its subthunks. NcclRaggedAllToAllStartThunk overrode Cleanup to free some device buffers, but these were never freed since Cleanup was not called. The memory is now stored in DeviceMemoryHandles, which automatically free the buffers in the destructor.",2025-02-06T21:14:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86783
229,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal change only.)， 内容是 (Internal change only.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal change only.,Internal change only.,2025-02-06T21:08:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86782
547,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Do not regard the 'fusion' op as part of the `HloComputationFusion`.)， 内容是 ([XLA:GPU] Do not regard the 'fusion' op as part of the `HloComputationFusion`. Instead, check for this case in `ResolveUsers` and `ResolveOperand`, by querying whether the `fused_expression_root` is part of the `HloFusionAdaptor`. This prevents us from stepping into nested fusions.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA:GPU] Do not regard the 'fusion' op as part of the `HloComputationFusion`.,"[XLA:GPU] Do not regard the 'fusion' op as part of the `HloComputationFusion`. Instead, check for this case in `ResolveUsers` and `ResolveOperand`, by querying whether the `fused_expression_root` is part of the `HloFusionAdaptor`. This prevents us from stepping into nested fusions.",2025-02-06T16:46:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86762
803,"以下是一个github上的tensorflow下的一个issue, 标题是(Fix race condition in FenceInsertionPass)， 内容是 (Fix race condition in FenceInsertionPass It looks like the idea here was to add some basic memoization so we don't end up with a ton of recursive calls, and potentially deadlock. However, doing that through a static variable is problematic both because it's not threadsafe, and because it's a silent memory leak, since we never free up the set (so a longrunning program would just continue adding stuff to it as we compile new kernels indefinitely). I'm still trying to get a good upstreamable reproducer, but this should fix the issue for now so we don't crash in production.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Fix race condition in FenceInsertionPass,"Fix race condition in FenceInsertionPass It looks like the idea here was to add some basic memoization so we don't end up with a ton of recursive calls, and potentially deadlock. However, doing that through a static variable is problematic both because it's not threadsafe, and because it's a silent memory leak, since we never free up the set (so a longrunning program would just continue adding stuff to it as we compile new kernels indefinitely). I'm still trying to get a good upstreamable reproducer, but this should fix the issue for now so we don't crash in production.",2025-02-06T16:15:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86760
381,"以下是一个github上的tensorflow下的一个issue, 标题是([xla:emitters] fix type mismatch for several passes)， 内容是 ([xla:emitters] fix type mismatch for several passes I've seen Windows failures because of these after applying some upcoming changes. Fix them.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[xla:emitters] fix type mismatch for several passes,[xla:emitters] fix type mismatch for several passes I've seen Windows failures because of these after applying some upcoming changes. Fix them.,2025-02-06T15:36:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86757
1189,"以下是一个github上的tensorflow下的一个issue, 标题是(unexpected import during stub creation from mypy-protobuf)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version master  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.12.8  Bazel version 6.5.0  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm doing a stub distribution for tensorflow, and when I used mypyprotobuf to create stubs for .proto files, it resulted in an import that I didn't expect The format it should create is:  but it does  I don't really understand how they structure the configurations to compile from Bazel Is there a setting I'm missing, or should I make the change manually? Is there a way for tensorflow to automatically create the stubs files from the compiled proto files? Here I leave you the code to speed up your analysis. third_party\xla\xla\tsl\protobuf\histogram.proto  ten)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,AlanBogarin,unexpected import during stub creation from mypy-protobuf," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version master  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.12.8  Bazel version 6.5.0  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm doing a stub distribution for tensorflow, and when I used mypyprotobuf to create stubs for .proto files, it resulted in an import that I didn't expect The format it should create is:  but it does  I don't really understand how they structure the configurations to compile from Bazel Is there a setting I'm missing, or should I make the change manually? Is there a way for tensorflow to automatically create the stubs files from the compiled proto files? Here I leave you the code to speed up your analysis. third_party\xla\xla\tsl\protobuf\histogram.proto  ten",2025-02-06T14:03:17Z,type:build/install type:support,open,0,3,https://github.com/tensorflow/tensorflow/issues/86752,", Could you please provide the error log which you are facing the issue. It helps to analyse the issue in an effective way. Thank you!","> [](https://github.com/AlanBogarin), Could you please provide the error log which you are facing the issue. It helps to analyse the issue in an effective way. Thank you! ","When I install tensorflow with pip, tsl is inside tensorflow "
706,"以下是一个github上的tensorflow下的一个issue, 标题是(Fix GatherClientLibraryTest under PjRt.)， 内容是 (Fix GatherClientLibraryTest under PjRt. It was unclear to me what this test was actually trying to achieve because it is not clearly documented. If it is trying to exercise a specific behavior with the old nonPjRt `Client`, then that should probably live elsewhere. I've converted it to something that can just run on top of `HloPjRtTestBase` directly. I added some boilerplate to allow the `XlaBuilder` code to remain, though it might be better to just use a HLO string directly.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Fix GatherClientLibraryTest under PjRt.,"Fix GatherClientLibraryTest under PjRt. It was unclear to me what this test was actually trying to achieve because it is not clearly documented. If it is trying to exercise a specific behavior with the old nonPjRt `Client`, then that should probably live elsewhere. I've converted it to something that can just run on top of `HloPjRtTestBase` directly. I added some boilerplate to allow the `XlaBuilder` code to remain, though it might be better to just use a HLO string directly.",2025-02-05T23:49:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86697
503,"以下是一个github上的tensorflow下的一个issue, 标题是(Removing some patches by:)， 内容是 (Removing some patches by:  Upstreaming internal testto remove file entirely. Also removing patch with redundant (already upstream) tests.  Verifying an issue is already fixed.  Attempting to upstream 2 changes. Added comments to remove 2 more patches in a followup if they land successfully.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Removing some patches by:,Removing some patches by:  Upstreaming internal testto remove file entirely. Also removing patch with redundant (already upstream) tests.  Verifying an issue is already fixed.  Attempting to upstream 2 changes. Added comments to remove 2 more patches in a followup if they land successfully.,2025-02-05T13:29:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86642
247,"以下是一个github上的tensorflow下的一个issue, 标题是(internal change for visibilily)， 内容是 (internal change for visibilily)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],internal change for visibilily,internal change for visibilily,2025-02-05T07:06:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86609
1187,"以下是一个github上的tensorflow下的一个issue, 标题是(In literal_util.cc, use absl::uniform_int_distribution.)， 内容是 (In literal_util.cc, use absl::uniform_int_distribution. absl::uniform_int_distribution is faster than std::uniform_int_distribution. This makes initializing literals in run_hlo_module faster. In particular, I tested the following HLO:     ENTRY f {       arg = s8[2000000000] parameter(0)       ROOT add_result = s8[2000000000] add(arg, arg)     } It takes 7.8 seconds to initialize the input literal with the absl function, and 18.2 with the std function. I had to change several tests, which were sensitive to the exact values randomlyinitialized Literals were initialized to. Literals are initialized to a fixed seed, but this change causes such literals to be initialized to different values than before. Unfortunately the absl version of uniform_real_distribution is not faster. It takes 25.5 seconds with absl and 8.3 with std on the HLO when s8 is replaced with f16. The function does become faster if we use an absl::BitGen instead of an)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],"In literal_util.cc, use absl::uniform_int_distribution.","In literal_util.cc, use absl::uniform_int_distribution. absl::uniform_int_distribution is faster than std::uniform_int_distribution. This makes initializing literals in run_hlo_module faster. In particular, I tested the following HLO:     ENTRY f {       arg = s8[2000000000] parameter(0)       ROOT add_result = s8[2000000000] add(arg, arg)     } It takes 7.8 seconds to initialize the input literal with the absl function, and 18.2 with the std function. I had to change several tests, which were sensitive to the exact values randomlyinitialized Literals were initialized to. Literals are initialized to a fixed seed, but this change causes such literals to be initialized to different values than before. Unfortunately the absl version of uniform_real_distribution is not faster. It takes 25.5 seconds with absl and 8.3 with std on the HLO when s8 is replaced with f16. The function does become faster if we use an absl::BitGen instead of an",2025-02-05T03:37:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86599
523,"以下是一个github上的tensorflow下的一个issue, 标题是(Return arrays from `ArrayImpl._check_and_rearrange`.)， 内容是 (Return arrays from `ArrayImpl._check_and_rearrange`. This is in preparation for a larger change, so that input buffers can be checked before Array creation in XLA and the user gets more helpful JAX error messages instead of XLA errors. Reverts 135a67d02fc6282a323fc4ad42ef7d8a687995e6)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Return arrays from `ArrayImpl._check_and_rearrange`.,"Return arrays from `ArrayImpl._check_and_rearrange`. This is in preparation for a larger change, so that input buffers can be checked before Array creation in XLA and the user gets more helpful JAX error messages instead of XLA errors. Reverts 135a67d02fc6282a323fc4ad42ef7d8a687995e6",2025-02-04T21:54:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86583
614,"以下是一个github上的tensorflow下的一个issue, 标题是(Stop modifying the TraceEventsContainer in DoStoreAsLevelDbTable. This behavior)， 内容是 (Stop modifying the TraceEventsContainer in DoStoreAsLevelDbTable. This behavior is not intuitive (modifying a const value that was passed in) and unnecessary. Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Stop modifying the TraceEventsContainer in DoStoreAsLevelDbTable. This behavior,Stop modifying the TraceEventsContainer in DoStoreAsLevelDbTable. This behavior is not intuitive (modifying a const value that was passed in) and unnecessary. Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-04T19:55:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86577
631,"以下是一个github上的tensorflow下的一个issue, 标题是([xla:copy_insertion] Avoid adding a redundant control dependence from a)， 内容是 ([xla:copy_insertion] Avoid adding a redundant control dependence from a pipelined RecvDone to its previous Recv in a whileloop. When we add a copy of the RecvDone, we also add a control dependence from the copy to the Recv. If the copy is later on remove, the control dependence from the RecvDone to the Recv becomes the only side effect of the pass, which is not intended.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[xla:copy_insertion] Avoid adding a redundant control dependence from a,"[xla:copy_insertion] Avoid adding a redundant control dependence from a pipelined RecvDone to its previous Recv in a whileloop. When we add a copy of the RecvDone, we also add a control dependence from the copy to the Recv. If the copy is later on remove, the control dependence from the RecvDone to the Recv becomes the only side effect of the pass, which is not intended.",2025-02-04T19:13:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86576
511,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Add support for multiple updates per replica in RaggedAllToAll.)， 内容是 ([XLA:GPU] Add support for multiple updates per replica in RaggedAllToAll. A recent proposal suggested to extend the API of ra2a to allow to send multiple updates in one op. Before we would need to emit multiple chained ra2a to achieve the same effect.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Add support for multiple updates per replica in RaggedAllToAll.,[XLA:GPU] Add support for multiple updates per replica in RaggedAllToAll. A recent proposal suggested to extend the API of ra2a to allow to send multiple updates in one op. Before we would need to emit multiple chained ra2a to achieve the same effect.,2025-02-04T18:57:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86573
350,"以下是一个github上的tensorflow下的一个issue, 标题是(Add memcpy implementation of ragged-all-to-all.)， 内容是 (Add memcpy implementation of raggedalltoall. The implementation is similar to the memcpy implementation of alltoall.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Add memcpy implementation of ragged-all-to-all.,Add memcpy implementation of raggedalltoall. The implementation is similar to the memcpy implementation of alltoall.,2025-02-04T03:48:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86507
1190,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] move DotDecompose out of simplification pipeline)， 内容是 ([XLA:GPU] move DotDecompose out of simplification pipeline That seems to be a better approach then moving TransposeFold to simplification2 in 961e5c25fbd4082a1ac4f2e0865ad28163d12f7d: 1. There is a report that previous change has resulted in perf degradation https://github.com/openxla/xla/pull/22081 2. I have found another case when DotDecompose is competing with algsimp. Added a test for that. Overall, having an pass that expands operation together with passes that are trying to do the simplification asks for such infinite loops.  For archeologists:   passes DotDimensionSorter and DotDecomposer were added along with GpuAlgebraicSimplifier as it previously could have added multiple contracting dimensions to dot. But cudnn does not support dots with 2+ dimensions, forcing us to use a less efficient loop emitter.  That what ""// AlgebraicSimplifier may add contracting dimensions to a dot."" comment was about. After a while simplifier s)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA:GPU] move DotDecompose out of simplification pipeline,"[XLA:GPU] move DotDecompose out of simplification pipeline That seems to be a better approach then moving TransposeFold to simplification2 in 961e5c25fbd4082a1ac4f2e0865ad28163d12f7d: 1. There is a report that previous change has resulted in perf degradation https://github.com/openxla/xla/pull/22081 2. I have found another case when DotDecompose is competing with algsimp. Added a test for that. Overall, having an pass that expands operation together with passes that are trying to do the simplification asks for such infinite loops.  For archeologists:   passes DotDimensionSorter and DotDecomposer were added along with GpuAlgebraicSimplifier as it previously could have added multiple contracting dimensions to dot. But cudnn does not support dots with 2+ dimensions, forcing us to use a less efficient loop emitter.  That what ""// AlgebraicSimplifier may add contracting dimensions to a dot."" comment was about. After a while simplifier s",2025-02-03T09:16:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86443
992,"以下是一个github上的tensorflow下的一个issue, 标题是(User Guide: Deprecated Nvidia Docker Link)， 内容是 ( Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version any  Custom code No  OS platform and distribution Linux GPU  Mobile device _No response_  Python version any  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The link to the Nvidia Docker github https://github.com/NVIDIA/nvidiadocker?tab=readmeovfile Reports: This repository has been archived by the owner on Jan 22, 2024. It is now readonly.  and provides a link to: https://github.com/NVIDIA/nvidiacontainertoolkit Titled: Build and run containers leveraging NVIDIA GPUs   Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Cloudberrydotdev,User Guide: Deprecated Nvidia Docker Link," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version any  Custom code No  OS platform and distribution Linux GPU  Mobile device _No response_  Python version any  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The link to the Nvidia Docker github https://github.com/NVIDIA/nvidiadocker?tab=readmeovfile Reports: This repository has been archived by the owner on Jan 22, 2024. It is now readonly.  and provides a link to: https://github.com/NVIDIA/nvidiacontainertoolkit Titled: Build and run containers leveraging NVIDIA GPUs   Standalone code to reproduce the issue   Relevant log output ",2025-02-02T09:16:37Z,type:docs-bug stat:awaiting response type:build/install stale,closed,0,9,https://github.com/tensorflow/tensorflow/issues/86407,https://www.tensorflow.org/install/docker Sorry I meant to include this url for the documentation page that holds the error link.,The relevant text is: Docker is the easiest way to enable TensorFlow GPU support on Linux since only the NVIDIA® GPU driver is required on the host machine (the NVIDIA® CUDA® Toolkit does not need to be installed). The link is in the text: NVIDIA® GPU driver,  您好，邮件已经收到，我会尽快处理的。谢谢,"Sam, Thanks for picking this up. I'm not sure what happens now. Do I wait for a reply from the Tensorflow documentation team before closing this issue? Regards Ian Berry On Sun, 2 Feb 2025, 16:35 Sam Fletcher, ***@***.***> wrote: > The issue is that the TensorFlow documentation still links to the > nowarchived NVIDIA Docker repository. Instead, it should link to the new, > active repository: > > Solution: > >    1. The TensorFlow documentation team needs to update the incorrect >    link. >    2. The old link: > >    https://github.com/NVIDIA/nvidiadocker?tab=readmeovfile > >    Should be replaced with: > >    https://github.com/NVIDIA/nvidiacontainertoolkit > >    3. If you're relying on the old NVIDIA Docker setup, transition to >    using the *NVIDIA Container Toolkit* as per the new repository. You >    can follow their official setup guide: > > For now, you can manually install TensorFlow with GPU support using the > NVIDIA Container Toolkit instead of the deprecated NVIDIA Docker. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",", Thank you for reporting the issue. I have raised the internal request for the mentioned change and will be updated once it gets submitted. Thank you!",", The raised request was submitted and also the changes are reflected in the official document. https://www.tensorflow.org/install/dockertensorflow_docker_requirements reflecting to https://github.com/NVIDIA/nvidiacontainertoolkit !Image Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1176,"以下是一个github上的tensorflow下的一个issue, 标题是(Bump the github-actions group with 8 updates)， 内容是 (Bumps the githubactions group with 8 updates:  Updates `google/osvscanneraction` from 1.9.0 to 1.9.2  Release notes Sourced from google/osvscanneraction's releases.  v1.9.2 What's Changed  Update to v1.9.2 by @​hogo6002 in google/osvscanneraction CC(Can't install on ubuntu 12.04.5 LTS)  Full Changelog: https://github.com/google/osvscanneraction/compare/v1.9.1...v1.9.2 v1.9.1 What's Changed  Update to use osvscanner v1.9.1 chore(deps): update workflows by @​renovatebot in google/osvscanneraction CC(Integration with blaze ecosystem numba python to llvm compiler?) Update to v1.9.1 by @​anotherrex in google/osvscanneraction CC(error __init__() got an unexpected keyword argument 'syntax') chore(deps): update workflows by @​renovatebot in google/osvscanneraction CC(Object Detection)  Full Changelog: https://github.com/google/osvscanneraction/compare/v1.9.0...v1.9.1    Commits  764c918 Merge pull request  CC(Can't install on ubuntu 12.04.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",text generation,dependabot[bot],Bump the github-actions group with 8 updates,Bumps the githubactions group with 8 updates:  Updates `google/osvscanneraction` from 1.9.0 to 1.9.2  Release notes Sourced from google/osvscanneraction's releases.  v1.9.2 What's Changed  Update to v1.9.2 by @​hogo6002 in google/osvscanneraction CC(Can't install on ubuntu 12.04.5 LTS)  Full Changelog: https://github.com/google/osvscanneraction/compare/v1.9.1...v1.9.2 v1.9.1 What's Changed  Update to use osvscanner v1.9.1 chore(deps): update workflows by @​renovatebot in google/osvscanneraction CC(Integration with blaze ecosystem numba python to llvm compiler?) Update to v1.9.1 by @​anotherrex in google/osvscanneraction CC(error __init__() got an unexpected keyword argument 'syntax') chore(deps): update workflows by @​renovatebot in google/osvscanneraction CC(Object Detection)  Full Changelog: https://github.com/google/osvscanneraction/compare/v1.9.0...v1.9.1    Commits  764c918 Merge pull request  CC(Can't install on ubuntu 12.04.,2025-02-01T08:42:19Z,ready to pull size:S dependencies github_actions,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86354
636,"以下是一个github上的tensorflow下的一个issue, 标题是(fix(kernels): Handle empty values with non-empty row splits in RaggedTensorToTensor)， 内容是 (This commit addresses a segmentation fault in the `RaggedTensorToTensor` op when processing empty values with nonempty row splits:  Checking for empty values before processing.  Ensuring consistent handling of dimension sizes.  Providing clear error messages for invalid input configurations. Might fix CC(Segmentation fault (core dumped) in `RaggedTensorToTensor`).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,harshaljanjani,fix(kernels): Handle empty values with non-empty row splits in RaggedTensorToTensor,This commit addresses a segmentation fault in the `RaggedTensorToTensor` op when processing empty values with nonempty row splits:  Checking for empty values before processing.  Ensuring consistent handling of dimension sizes.  Providing clear error messages for invalid input configurations. Might fix CC(Segmentation fault (core dumped) in `RaggedTensorToTensor`).,2025-02-01T06:51:34Z,ready to pull size:M comp:core,open,0,2,https://github.com/tensorflow/tensorflow/issues/86349,"> Please make sure to not include irrelevant spacing changes. Understood, it's my first time contributing here; thanks for the information! Will take care of the linting next time around.","> Can you please make sure to run all tests and make sure they pass? Hello , thanks for the reply. Actually, I'm not quite able to figure out why my local setup's failing with these `bash r not found` errors. Besides, I tried converting the CRLF endings to LF endings (given that I'm running the bazel tests in WSL). I wished to ask if there's a better way to set up and run tests locally that you'd recommend with WSL, as I've read the CONTRIBUTING.md file and set it up to the tee but am still facing these issues that impede my progress; thanks!"
580,"以下是一个github上的tensorflow下的一个issue, 标题是(Return arrays from `ArrayImpl._check_and_rearrange`. Build IFRT shardings with both addressable and non-addressable devices, instead of only addressable devices.)， 内容是 (Return arrays from `ArrayImpl._check_and_rearrange`. Build IFRT shardings with both addressable and nonaddressable devices, instead of only addressable devices. This is a rollforward of two previous rollbacks after fixing breakages.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],"Return arrays from `ArrayImpl._check_and_rearrange`. Build IFRT shardings with both addressable and non-addressable devices, instead of only addressable devices.","Return arrays from `ArrayImpl._check_and_rearrange`. Build IFRT shardings with both addressable and nonaddressable devices, instead of only addressable devices. This is a rollforward of two previous rollbacks after fixing breakages.",2025-01-31T23:55:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86334
349,"以下是一个github上的tensorflow下的一个issue, 标题是(Update RaggedAllToAll API to clarify supported shapes for offsets/sizes operands.)， 内容是 (Update RaggedAllToAll API to clarify supported shapes for offsets/sizes operands.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Update RaggedAllToAll API to clarify supported shapes for offsets/sizes operands.,Update RaggedAllToAll API to clarify supported shapes for offsets/sizes operands.,2025-01-31T23:38:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86332
389,"以下是一个github上的tensorflow下的一个issue, 标题是([HLO] Use llvm::StringRef when building MHLO string attributes instead of relying on implicit casting)， 内容是 ([HLO] Use llvm::StringRef when building MHLO string attributes instead of relying on implicit casting)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[HLO] Use llvm::StringRef when building MHLO string attributes instead of relying on implicit casting,[HLO] Use llvm::StringRef when building MHLO string attributes instead of relying on implicit casting,2025-01-31T19:44:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86319
1125,"以下是一个github上的tensorflow下的一个issue, 标题是(Stateful LSTM bug with batch size)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.18  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When running a stateful LSTM one needs to fix a batch size. Previously this was achieved with the `batch_input_shape=(batch_size, sequence_length, num_features)` argument, but with the most recent version of TF batch_input_shape is not recognised as valid.  Basically the error is the same as this https://github.com/tensorflow/tensorflow/issues/64061 Below is some LLM generated code to reproduce the behaviour. Also, the `model.reset_states()` bit appears broken.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,ceschi,Stateful LSTM bug with batch size," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.18  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When running a stateful LSTM one needs to fix a batch size. Previously this was achieved with the `batch_input_shape=(batch_size, sequence_length, num_features)` argument, but with the most recent version of TF batch_input_shape is not recognised as valid.  Basically the error is the same as this https://github.com/tensorflow/tensorflow/issues/64061 Below is some LLM generated code to reproduce the behaviour. Also, the `model.reset_states()` bit appears broken.  Standalone code to reproduce the issue   Relevant log output ",2025-01-31T18:07:07Z,stat:awaiting response type:bug stale comp:keras TF 2.18,closed,0,7,https://github.com/tensorflow/tensorflow/issues/86310,"Hello Jordan, thanks for the reply. I am indeed using TF 2.18 (and Python 3.11.0 on Win11), though if I run your code I get precisely the error I referred to in the first place: `ValueError: Unrecognized keyword arguments passed to LSTM: {'batch_input_shape': (32, 5, 1)}`",", Hi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands.  Also I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it here. Take a look at this issue for reference. https://github.com/kerasteam/keras/issues/20106 Thank you!","  Hello, thanks for the pointers. I am prototyping on TF 2.18 and Keras 3.8, to then do the training on TF 2.13. If I understand correctly this post, an Input layer with `batch_shape` does the trick. Would this work in both versions of TF? Thanks a ton for the help, the documentation is quite confusing currently.",", As per above comments, I can sense that you tried the code in tensorflow 2.18, keras 3.8 and then training in TF 2.13. In such a scenario, the code might be having compatible issues with both 2.18 and 2.13 which wouldn't be suggestible.  Tensorflow v2.18 contains Keras3.0 version and tensorflow v2.13 contains keras2.0. where both versions are different. https://keras.io/keras_3/ And also the code is working in tf_keras(keras2.0), and provided the error in keras3.0. So, please feel free to raise the issue in Kerasteam/keras repo for the further inputs. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1205,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #21886: [ROCM][NFC] BlasLt interface refactoring & simplifying: part I)， 内容是 (PR CC(Tensorflow 1.10.0 MKL build (win64) with bazel throws linker error): [ROCM][NFC] BlasLt interface refactoring & simplifying: part I Imported from GitHub PR https://github.com/openxla/xla/pull/21886 After this PR https://github.com/tensorflow/tensorflow/pull/73926 is merged, we can remove unnecessary lowlevel DoMatmul functions from GpuBlasLt interface (which otherwise looks scary and unnecessarily complicated). Furthermore, we can also remove **ValidateInputs** function from the interface and derived classes since a highlevel **ExecuteOnStream** function already handles datatypes correctly. This also greatly simplifies the code. Also, I have packed the input arguments of ExecuteOnStream calls to a struct **MemoryArgs** to simplify arguments passing in derived classes and improve code readability. Finally, in the original GpuBlasLt PR: https://github.com/openxla/xla/pull/5911, I made a sort of mistake by adding a reference to )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #21886: [ROCM][NFC] BlasLt interface refactoring & simplifying: part I,"PR CC(Tensorflow 1.10.0 MKL build (win64) with bazel throws linker error): [ROCM][NFC] BlasLt interface refactoring & simplifying: part I Imported from GitHub PR https://github.com/openxla/xla/pull/21886 After this PR https://github.com/tensorflow/tensorflow/pull/73926 is merged, we can remove unnecessary lowlevel DoMatmul functions from GpuBlasLt interface (which otherwise looks scary and unnecessarily complicated). Furthermore, we can also remove **ValidateInputs** function from the interface and derived classes since a highlevel **ExecuteOnStream** function already handles datatypes correctly. This also greatly simplifies the code. Also, I have packed the input arguments of ExecuteOnStream calls to a struct **MemoryArgs** to simplify arguments passing in derived classes and improve code readability. Finally, in the original GpuBlasLt PR: https://github.com/openxla/xla/pull/5911, I made a sort of mistake by adding a reference to ",2025-01-31T17:49:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86308
709,"以下是一个github上的tensorflow下的一个issue, 标题是(DRQ  (Dynamic Range Quantization) - which ops are affected?)， 内容是 (Hi,  I am performing DRQ (Dynamic Range Quantization) using https://ai.google.dev/edge/litert/models/post_training_quant  how to get more details on which ops will be affected(and what exactly will happen with these ops)? My understanding that in case of transformers only fully connected layers will be affected, is this correct? What would be the impact on op computations  would computations be happening with int8 (ie both weights and activations)? Thank you.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Alexey234432,DRQ  (Dynamic Range Quantization) - which ops are affected?,"Hi,  I am performing DRQ (Dynamic Range Quantization) using https://ai.google.dev/edge/litert/models/post_training_quant  how to get more details on which ops will be affected(and what exactly will happen with these ops)? My understanding that in case of transformers only fully connected layers will be affected, is this correct? What would be the impact on op computations  would computations be happening with int8 (ie both weights and activations)? Thank you.",2025-01-31T14:46:34Z,stat:awaiting response comp:lite TFLiteConverter,closed,0,7,https://github.com/tensorflow/tensorflow/issues/86293,"Hi,   I apologize for the delayed response, As far I know during DRQ the weights are quantized to `int8` but the activations remain in `float32`. This means that the multiplication is int8 * float32. You're correct in your understanding that fully connected layers are major focus in transformers. Transformers heavily rely on fully connected layers (in the Feed Forward Network(FFN) and in the attention mechanism). The query, key and value transformations within the attention mechanism often use fully connected layers. The FFN which is typically a multilayer perceptron (MLP) consists of multiple fully connected layers.Therefore DRQ will primarily affect the fully connected operations in these parts of the transformer architecture. I would suggest you to please use these tools modelexplorer and Netron to visualize the architecture of your TensorFlow Lite (TFLite) model including the changes made by Dynamic Range Quantization (DRQ) Thank you for your cooperation and patience.","Thank you for your reply   Yes, thanks for suggestion  I use these tools and they are extremely useful but to be honest I am still confused (let's concentrate on fully connected ops behaviour for the sake of simplicity) whether actual computations (mat muls) are happening in int8 or fp32. Looking into docs from https://ai.google.dev/edge/litert/models/post_training_quant  this per my understanding implies that compute is happening in int8. Also inference time of DRQ quantized llama3 model vs Float TFLite llama3 model was ~2 times faster (on CPU using TFlite interpreter with 1 cpu only)  this is also a weak evidence of compute using different approach under the hood. Any chance you could please help me understand what's happening on lower level? Thank you.","Hi,  You're correct in your understanding, Dynamic Range Quantization (DRQ) in TensorFlow Lite stores activations in `float32` for range and precision. However, for operations with quantized kernels (like matrix multiplications) activations are dynamically quantized to `int8` immediately before computation.  The actual computation (e.g. matrix multiplication) happens in `int8` precision using both the `int8` quantized weights and the dynamically `int8` quantized activations.  Results are then dequantized back to `float32`.  Thus, the core computations occur in `int8` providing performance benefits despite `float32` activation storage. Thank you for your understanding and cooperation.",Thank you  this is really helpful and detailed answer. Do you know how could I come to this conclusion on my own? ie any links to the relevant inference or quantization code (which I assume will be somewhere inside TFLite source code?) Thanks!,"Hi,   Unfortunately, the exact source code for Dynamic Range Quantization (DRQ) within TensorFlow Lite is not readily available in a single, easily isolated file. This is because DRQ is implemented across several components of the TensorFlow Lite framework. However, I can point you to the key areas and files where the relevant logic resides  1. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.h 2. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util., including dynamic quantization. They handle the scaling and conversion between `float32` and `int8` representations. 3. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.h 4. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.. The actual DRQ logic is implemented within the individual kernel implementations for the operations that support quantization (e.g. fully connected layers, convolutions). You can find these kernels in the tensorflow/lite/kernels directory. Within these kernel files you'll find code that uses the quantization utilities mentioned above to dynamically quantize the activations before performing the computation. If I have missed something here please let me know. If you notice any omissions or discrepancies between the official documentation and the source code implementation,  we welcome a pull request (PR).  Our team will review your submission and facilitate its integration provided the changes align with our contribution guidelines. Thank you for your understand and cooperation.",Thank you for your help!,"Hi,   You're welcome, Could you please confirm if this issue is resolved for you now? Please feel free to close the issue if it is resolved ? If need any further help in future w.r.t TFLite now renamed to LiteRT please feel free to post your issue in dedicated repo for LiteRT  Thank you for your cooperation and understanding."
326,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Extract more fragments into the smaller functions.)， 内容是 ([XLA:GPU] Extract more fragments into the smaller functions. It is noop change.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA:GPU] Extract more fragments into the smaller functions.,[XLA:GPU] Extract more fragments into the smaller functions. It is noop change.,2025-01-31T10:58:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86280
1055,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:LatencyHidingScheduler] Let `GetResourcesFromInstruction` return a complete list of resources used by instructions in a while loop. This will make async `done` and while ops have similar priority (in terms of occupying resource types) and avoid delaying the while loops only because they cross the overlap limit (even though they have a higher async depth).)， 内容是 ([XLA:LatencyHidingScheduler] Let `GetResourcesFromInstruction` return a complete list of resources used by instructions in a while loop. This will make async `done` and while ops have similar priority (in terms of occupying resource types) and avoid delaying the while loops only because they cross the overlap limit (even though they have a higher async depth). This CL also fixes the double counting of a resource in `GetNumResourcesPerInstruction` because of multiple async `done` ops in the while body.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA:LatencyHidingScheduler] Let `GetResourcesFromInstruction` return a complete list of resources used by instructions in a while loop. This will make async `done` and while ops have similar priority (in terms of occupying resource types) and avoid delaying the while loops only because they cross the overlap limit (even though they have a higher async depth).,[XLA:LatencyHidingScheduler] Let `GetResourcesFromInstruction` return a complete list of resources used by instructions in a while loop. This will make async `done` and while ops have similar priority (in terms of occupying resource types) and avoid delaying the while loops only because they cross the overlap limit (even though they have a higher async depth). This CL also fixes the double counting of a resource in `GetNumResourcesPerInstruction` because of multiple async `done` ops in the while body.,2025-01-30T22:59:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86242
438,"以下是一个github上的tensorflow下的一个issue, 标题是(litert: Use BuiltinOpResolver to enable lazy applying Xnnpack delegate)， 内容是 (litert: Use BuiltinOpResolver to enable lazy applying Xnnpack delegate Now, getting a signature runner before applying delegate isn't needed. So we can use BuiltinOpResolver safely.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],litert: Use BuiltinOpResolver to enable lazy applying Xnnpack delegate,"litert: Use BuiltinOpResolver to enable lazy applying Xnnpack delegate Now, getting a signature runner before applying delegate isn't needed. So we can use BuiltinOpResolver safely.",2025-01-30T18:50:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86226
394,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:CPU][roll forward] Underlying ObjectLoader dylibs are using DefinitionGenerator now.)， 内容是 ([XLA:CPU][roll forward] Underlying ObjectLoader dylibs are using DefinitionGenerator now. Reverts changelist 721389214)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA:CPU][roll forward] Underlying ObjectLoader dylibs are using DefinitionGenerator now.,[XLA:CPU][roll forward] Underlying ObjectLoader dylibs are using DefinitionGenerator now. Reverts changelist 721389214,2025-01-30T17:23:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86218
935,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA/Triton] Don't restrict contracting dimension tiling for predicate inputs in a GEMM during autotuning. Historically, the restriction was acceptable until a change to FMA landed from Triton upstream that started spilling registers for such configurations. The more correct way to handle this is to lift the restriction on predicates rather than applying it to small dots.)， 内容是 ([XLA/Triton] Don't restrict contracting dimension tiling for predicate inputs in a GEMM during autotuning. Historically, the restriction was acceptable until a change to FMA landed from Triton upstream that started spilling registers for such configurations. The more correct way to handle this is to lift the restriction on predicates rather than applying it to small dots.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],"[XLA/Triton] Don't restrict contracting dimension tiling for predicate inputs in a GEMM during autotuning. Historically, the restriction was acceptable until a change to FMA landed from Triton upstream that started spilling registers for such configurations. The more correct way to handle this is to lift the restriction on predicates rather than applying it to small dots.","[XLA/Triton] Don't restrict contracting dimension tiling for predicate inputs in a GEMM during autotuning. Historically, the restriction was acceptable until a change to FMA landed from Triton upstream that started spilling registers for such configurations. The more correct way to handle this is to lift the restriction on predicates rather than applying it to small dots.",2025-01-30T17:06:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86217
1197,"以下是一个github上的tensorflow下的一个issue, 标题是(Tensorflow_datasets - image_classification - cats_vs_dogs.py file)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tensorflow2.18.0  Custom code No  OS platform and distribution Edition: Windows 10 Pro, Version: 22H2, OS Build: 19045.5371  Mobile device _No response_  Python version 3.11.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I used a dataset named cats_vs_dogs for studying and training a model, MobileNet v2. Upon loading this dataset by using tensorflow_datasets.load() function which does the fetching too, the first image from arhive that should have been extracted throws an error that it cannot be found. I solved easily `_generate_examples()` from cats_vs_dogs.py by eliminating the context manager with ZipFile object and the one after. I suppose that attaching one ZipFile object to a BytesIO object does work in the contex)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",text generation,bog739,Tensorflow_datasets - image_classification - cats_vs_dogs.py file," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tensorflow2.18.0  Custom code No  OS platform and distribution Edition: Windows 10 Pro, Version: 22H2, OS Build: 19045.5371  Mobile device _No response_  Python version 3.11.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I used a dataset named cats_vs_dogs for studying and training a model, MobileNet v2. Upon loading this dataset by using tensorflow_datasets.load() function which does the fetching too, the first image from arhive that should have been extracted throws an error that it cannot be found. I solved easily `_generate_examples()` from cats_vs_dogs.py by eliminating the context manager with ZipFile object and the one after. I suppose that attaching one ZipFile object to a BytesIO object does work in the contex",2025-01-30T01:53:39Z,type:bug awaiting PR merge TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/86177,"Hi **** , Welcome to TensorFlow. This is a known issue, and a fix has already been merged. Once a new release is available, the problem should be resolved. I am providing a link to a similar issue here—please follow it for further updates: CC(KeyError: ""There is no item named 'PetImages\\Cat\\0.jpg' in the archive"" When Running TensorFlow Locally(CPU) on Anaconda in VS Code.) Thank you!","Hi , Thank you for suggestion, I will keep an eye on newer releases!",Are you satisfied with the resolution of your issue? Yes No
263,"以下是一个github上的tensorflow下的一个issue, 标题是(Add an ICYU pragma to silence linters.)， 内容是 (Add an ICYU pragma to silence linters.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Add an ICYU pragma to silence linters.,Add an ICYU pragma to silence linters.,2025-01-29T22:07:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86159
1061,"以下是一个github上的tensorflow下的一个issue, 标题是(ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.)， 内容是 ( Issue type Others  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version  2.18.0  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version Python version: 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an error when trying to import TensorFlow. The error message is: ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Multiple times reinstalled it Downgraded my Python, still no sucess.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,zainZayam,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.," Issue type Others  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version  2.18.0  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version Python version: 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an error when trying to import TensorFlow. The error message is: ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Multiple times reinstalled it Downgraded my Python, still no sucess.  Standalone code to reproduce the issue   Relevant log output ",2025-01-29T19:28:01Z,stat:awaiting response type:build/install type:others TF 2.18,closed,0,2,https://github.com/tensorflow/tensorflow/issues/86111,", Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:  Also kindly provide the environment details and the steps followed to install the tensorflow. CC(Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.) Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Are you satisfied with the resolution of your issue? Yes No
1204,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #21825: Exclude the usage of CPU memory from the GPU memory scheduler)， 内容是 (PR CC(TensorFlow lite android example simply does not sync or build.): Exclude the usage of CPU memory from the GPU memory scheduler Imported from GitHub PR https://github.com/openxla/xla/pull/21825 In the MaxText optimizer state offloading, we observed no memory savings when switching from f16 to f32. The root cause is that the GPU memory scheduler does not distinguish between CPU memory and GPU memory. This commit modifies the scheduler to exclude CPU memory. Copybara import of the project:  c77eefa1b4e31724dbfa40f4ab2aa7aff16e0840 by Jane Liu : Exclude the usage of CPU memory from the GPU memory scheduler  1c5720711c6a7d8173e132922b67eee6e2e8b9dd by Jane Liu : Add the explicit return type for the closure Merging this change closes CC(TensorFlow lite android example simply does not sync or build.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21825 from zhenyingliu:scheduler 1c5720711c6a7d8173e132922b67eee6)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #21825: Exclude the usage of CPU memory from the GPU memory scheduler,"PR CC(TensorFlow lite android example simply does not sync or build.): Exclude the usage of CPU memory from the GPU memory scheduler Imported from GitHub PR https://github.com/openxla/xla/pull/21825 In the MaxText optimizer state offloading, we observed no memory savings when switching from f16 to f32. The root cause is that the GPU memory scheduler does not distinguish between CPU memory and GPU memory. This commit modifies the scheduler to exclude CPU memory. Copybara import of the project:  c77eefa1b4e31724dbfa40f4ab2aa7aff16e0840 by Jane Liu : Exclude the usage of CPU memory from the GPU memory scheduler  1c5720711c6a7d8173e132922b67eee6e2e8b9dd by Jane Liu : Add the explicit return type for the closure Merging this change closes CC(TensorFlow lite android example simply does not sync or build.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21825 from zhenyingliu:scheduler 1c5720711c6a7d8173e132922b67eee6",2025-01-29T18:09:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86070
263,"以下是一个github上的tensorflow下的一个issue, 标题是(Add an ICYU pragma to silence linters.)， 内容是 (Add an ICYU pragma to silence linters.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Add an ICYU pragma to silence linters.,Add an ICYU pragma to silence linters.,2025-01-29T17:52:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86068
422,"以下是一个github上的tensorflow下的一个issue, 标题是([xla:gpu] [cleanup] Pull out some logic into IterableInput)， 内容是 ([xla:gpu] [cleanup] Pull out some logic into IterableInput This both simplifies the giant EmitMatmul function & makes it more generic, simplifying the TMA change (see CL chain).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[xla:gpu] [cleanup] Pull out some logic into IterableInput,"[xla:gpu] [cleanup] Pull out some logic into IterableInput This both simplifies the giant EmitMatmul function & makes it more generic, simplifying the TMA change (see CL chain).",2025-01-29T13:47:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86053
1184,"以下是一个github上的tensorflow下的一个issue, 标题是(Tensorflow lite cross compilation to aarch64 failing)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.1  Custom code No  OS platform and distribution Host: x86 Ubuntu 20.04  Mobile device target: aarch64  Python version N/A  Bazel version N/A  GCC/compiler version gcc 9  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? Build fails due to an exec format error with `protoc`  The only possible explanation for this seems to be that that the cmake build process is attempting to use `protoc` for this step   But since `protoc` was built using the cross compiler tool chain it is meant for aarch64 while my host machine is trying to run it. This is likely a bug with the cross compilation process and in that case, please suggest a fix. I am not sure to what extent `protoc` is used in the build so any fix I would make cannot be completely correct.  Edit: I installed the x86 version for `protoc` separatel)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,AD-lite24,Tensorflow lite cross compilation to aarch64 failing," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.1  Custom code No  OS platform and distribution Host: x86 Ubuntu 20.04  Mobile device target: aarch64  Python version N/A  Bazel version N/A  GCC/compiler version gcc 9  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? Build fails due to an exec format error with `protoc`  The only possible explanation for this seems to be that that the cmake build process is attempting to use `protoc` for this step   But since `protoc` was built using the cross compiler tool chain it is meant for aarch64 while my host machine is trying to run it. This is likely a bug with the cross compilation process and in that case, please suggest a fix. I am not sure to what extent `protoc` is used in the build so any fix I would make cannot be completely correct.  Edit: I installed the x86 version for `protoc` separatel",2025-01-29T11:42:26Z,type:build/install comp:lite subtype: ubuntu/linux 2.17,closed,0,2,https://github.com/tensorflow/tensorflow/issues/86044,"Figured it out. Apparently protobuf 3.21.12 is significantly different from 3.21.9 and there is no release for 3.21.9 so need to build it from source. Still there is a bug with with the cross compilation process that should be resolved. I will try to create a PR if I end up writing a seamless solution, but for now the workaround of manually building protobuf works. Closing the issue for now but the issue has not been resolved.",Are you satisfied with the resolution of your issue? Yes No
1214,"以下是一个github上的tensorflow下的一个issue, 标题是(Input pipeline with RaggedTensor no longer working in +2.16 - No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.1 LTS  Mobile device _No response_  Python version 3.11 & 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In version 2.15 and earlier i was able to have a batched `tf.data.Dataset` with `tf.RaggedTensor` as input for `model.fit` and `model.predict`, with `tf.keras.layers.Resizing` as the first layer of the model. This is no longer works in +2.16 and Keras 3 (Edit: not Keras 2). The error log contains `RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible`, which implies a missing implementation.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,DamarXCV,Input pipeline with RaggedTensor no longer working in +2.16 - No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.1 LTS  Mobile device _No response_  Python version 3.11 & 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In version 2.15 and earlier i was able to have a batched `tf.data.Dataset` with `tf.RaggedTensor` as input for `model.fit` and `model.predict`, with `tf.keras.layers.Resizing` as the first layer of the model. This is no longer works in +2.16 and Keras 3 (Edit: not Keras 2). The error log contains `RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible`, which implies a missing implementation.  Standalone code to reproduce the issue   Relevant log output ",2025-01-29T11:40:28Z,type:bug comp:keras TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/86043,"Hi **** , Apologies for the delay, and thank you for raising your concern here. The main cause of your issue is the Keras installation. Starting from TensorFlow version 2.16.0, it defaults to Keras 3. If you want to use Keras 2, you need to install it manually. This is also mentioned in the documentation. I installed everything as required, and it is working fine for me. Here, I am providing a gist for your reference. Thank you!"," Thank you for your reply. Sorry, i meant Keras 3 not Keras 2, i corrected it in my original post. If i console log the version it prints `3.8.0` for Keras with the following code  I know i should use the standalone keras import, as documented in the migration guide, but even i do so it still crashes with the error from my initial post  The printed versions are in both code snipets `2.18.0` and `3.8.0`. If i install tfkeras (aka Keras 2) with `pip install tfkeras` the following code works  But i would like to use Keras 3 and not the outdated Keras 2, which seems to not support `tf.RaggedTensor` as input for `keras.layers.Resizing`",I found kerasteam/keras CC(Add missing semicolon) which mentions  > No RaggedTensor support. We may add it back later. I guess that means that my input pipeline is not supported in Keras 3 for now.,Are you satisfied with the resolution of your issue? Yes No
440,"以下是一个github上的tensorflow下的一个issue, 标题是(Refactor XLA's common.bara.sky to make copying of top level files and dirs more terse)， 内容是 (Refactor XLA's common.bara.sky to make copying of top level files and dirs more terse This is in preparation for introducing the concept of a ""moveonly"" file explicitly)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Refactor XLA's common.bara.sky to make copying of top level files and dirs more terse,"Refactor XLA's common.bara.sky to make copying of top level files and dirs more terse This is in preparation for introducing the concept of a ""moveonly"" file explicitly",2025-01-29T00:58:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85996
323,"以下是一个github上的tensorflow下的一个issue, 标题是(Use efficient packed flatbuffer api to handle underlying tfl models.)， 内容是 (Use efficient packed flatbuffer api to handle underlying tfl models.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Use efficient packed flatbuffer api to handle underlying tfl models.,Use efficient packed flatbuffer api to handle underlying tfl models.,2025-01-28T23:43:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85991
651,"以下是一个github上的tensorflow下的一个issue, 标题是(Return arrays from `ArrayImpl._check_and_rearrange`.)， 内容是 (Return arrays from `ArrayImpl._check_and_rearrange`. This is in preparation for a larger change, so that `_check_arrays` can be called before Array creation in XLA and the user gets more helpful JAX error messages instead of XLA errors. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Return arrays from `ArrayImpl._check_and_rearrange`.,"Return arrays from `ArrayImpl._check_and_rearrange`. This is in preparation for a larger change, so that `_check_arrays` can be called before Array creation in XLA and the user gets more helpful JAX error messages instead of XLA errors. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e",2025-01-28T22:27:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85985
376,"以下是一个github上的tensorflow下的一个issue, 标题是(Remove `UpdateEntryComputationLayout` from `HloRunnerAgnosticTestBase`)， 内容是 (Remove `UpdateEntryComputationLayout` from `HloRunnerAgnosticTestBase` Reverts a47a28e840cf97148669ba3483cd72e87f0efa5b)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Remove `UpdateEntryComputationLayout` from `HloRunnerAgnosticTestBase`,Remove `UpdateEntryComputationLayout` from `HloRunnerAgnosticTestBase` Reverts a47a28e840cf97148669ba3483cd72e87f0efa5b,2025-01-28T19:55:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85969
1205,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #21886: [ROCM][NFC] BlasLt interface refactoring & simplifying: part I)， 内容是 (PR CC(Tensorflow 1.10.0 MKL build (win64) with bazel throws linker error): [ROCM][NFC] BlasLt interface refactoring & simplifying: part I Imported from GitHub PR https://github.com/openxla/xla/pull/21886 After this PR https://github.com/tensorflow/tensorflow/pull/73926 is merged, we can remove unnecessary lowlevel DoMatmul functions from GpuBlasLt interface (which otherwise looks scary and unnecessarily complicated). Furthermore, we can also remove **ValidateInputs** function from the interface and derived classes since a highlevel **ExecuteOnStream** function already handles datatypes correctly. This also greatly simplifies the code. Also, I have packed the input arguments of ExecuteOnStream calls to a struct **MemoryArgs** to simplify arguments passing in derived classes and improve code readability. Finally, in the original GpuBlasLt PR: https://github.com/openxla/xla/pull/5911, I made a sort of mistake by adding a reference to )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #21886: [ROCM][NFC] BlasLt interface refactoring & simplifying: part I,"PR CC(Tensorflow 1.10.0 MKL build (win64) with bazel throws linker error): [ROCM][NFC] BlasLt interface refactoring & simplifying: part I Imported from GitHub PR https://github.com/openxla/xla/pull/21886 After this PR https://github.com/tensorflow/tensorflow/pull/73926 is merged, we can remove unnecessary lowlevel DoMatmul functions from GpuBlasLt interface (which otherwise looks scary and unnecessarily complicated). Furthermore, we can also remove **ValidateInputs** function from the interface and derived classes since a highlevel **ExecuteOnStream** function already handles datatypes correctly. This also greatly simplifies the code. Also, I have packed the input arguments of ExecuteOnStream calls to a struct **MemoryArgs** to simplify arguments passing in derived classes and improve code readability. Finally, in the original GpuBlasLt PR: https://github.com/openxla/xla/pull/5911, I made a sort of mistake by adding a reference to ",2025-01-28T14:45:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85948
401,"以下是一个github上的tensorflow下的一个issue, 标题是(Replace ""external_buffer"" nomenclature with ""op_asset"" and leverage the unified buffer management approach.)， 内容是 (Replace ""external_buffer"" nomenclature with ""op_asset"" and leverage the unified buffer management approach.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],"Replace ""external_buffer"" nomenclature with ""op_asset"" and leverage the unified buffer management approach.","Replace ""external_buffer"" nomenclature with ""op_asset"" and leverage the unified buffer management approach.",2025-01-28T00:13:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85901
295,"以下是一个github上的tensorflow下的一个issue, 标题是(Unify metadata storage with unified buffer management.)， 内容是 (Unify metadata storage with unified buffer management.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Unify metadata storage with unified buffer management.,Unify metadata storage with unified buffer management.,2025-01-27T21:11:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85886
987,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #21845: [ROCM] Add missing triton MLIR int4 -> int8 rewrite pass for ROCM)， 内容是 (PR CC(未找到相关数据): [ROCM] Add missing triton MLIR int4 > int8 rewrite pass for ROCM Imported from GitHub PR https://github.com/openxla/xla/pull/21845  Tests above are failing on ROCm side after int4 rewriting was moved from legacy matmul emitter to MLIR pass. This MLIR pass is now missing in ROCm triton pipeline and I'm adding it in the place. rotation: would you please take a look?  Copybara import of the project:  75e78ad365a9d55f6e299c7b64400447ceebb26d by Jian Li : [ROCM] Add missing triton MLIR int4 > int8 rewrite pass for ROCM Merging this change closes CC(未找到相关数据) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21845 from ROCm:ci_fix_rocm_triton_test 75e78ad365a9d55f6e299c7b64400447ceebb26d)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #21845: [ROCM] Add missing triton MLIR int4 -> int8 rewrite pass for ROCM,PR CC(未找到相关数据): [ROCM] Add missing triton MLIR int4 > int8 rewrite pass for ROCM Imported from GitHub PR https://github.com/openxla/xla/pull/21845  Tests above are failing on ROCm side after int4 rewriting was moved from legacy matmul emitter to MLIR pass. This MLIR pass is now missing in ROCm triton pipeline and I'm adding it in the place. rotation: would you please take a look?  Copybara import of the project:  75e78ad365a9d55f6e299c7b64400447ceebb26d by Jian Li : [ROCM] Add missing triton MLIR int4 > int8 rewrite pass for ROCM Merging this change closes CC(未找到相关数据) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21845 from ROCm:ci_fix_rocm_triton_test 75e78ad365a9d55f6e299c7b64400447ceebb26d,2025-01-27T18:14:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85867
507,"以下是一个github上的tensorflow下的一个issue, 标题是([xla:cpu] Remove code for computing optimal number of workers at run time)， 内容是 ([xla:cpu] Remove code for computing optimal number of workers at run time Instead of trying to figure out optimal number of workers at run time, we'd better have a cost model that can make this decision at compile time based on the XNNPACK fusion.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[xla:cpu] Remove code for computing optimal number of workers at run time,"[xla:cpu] Remove code for computing optimal number of workers at run time Instead of trying to figure out optimal number of workers at run time, we'd better have a cost model that can make this decision at compile time based on the XNNPACK fusion.",2025-01-26T18:21:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85772
333,"以下是一个github上的tensorflow下的一个issue, 标题是(Support `mhlo.sharding` attr inside `backend_config` of ragged_all_to_all)， 内容是 (Support `mhlo.sharding` attr inside `backend_config` of ragged_all_to_all)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Support `mhlo.sharding` attr inside `backend_config` of ragged_all_to_all,Support `mhlo.sharding` attr inside `backend_config` of ragged_all_to_all,2025-01-24T21:11:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85692
1119,"以下是一个github上的tensorflow下的一个issue, 标题是(TF 2.18 with GPU does not detect GPU, Cannot dlopen some GPU libraries, in a container)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code Yes  OS platform and distribution Linux Centos 7.9, RHEL 8, RHEL 9  Mobile device _No response_  Python version 3.11.0rc1  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 550.90.07  GPU model and memory _No response_  Current behavior? After discussing this on the Apptainer Git we determined the latest TFGPU running 2.18.0 does not register any GPUs. Older versions like 2.7.1gpu work just fine. `apptainer run nv  /apps/Miniforge/lib/python3.12/sitepackages/containers/tensorflow/tensorflow/latestgpu/tensorflowtensorflowlatestgpusha256\:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif python`     Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,LinuxPersonEC,"TF 2.18 with GPU does not detect GPU, Cannot dlopen some GPU libraries, in a container"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code Yes  OS platform and distribution Linux Centos 7.9, RHEL 8, RHEL 9  Mobile device _No response_  Python version 3.11.0rc1  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 550.90.07  GPU model and memory _No response_  Current behavior? After discussing this on the Apptainer Git we determined the latest TFGPU running 2.18.0 does not register any GPUs. Older versions like 2.7.1gpu work just fine. `apptainer run nv  /apps/Miniforge/lib/python3.12/sitepackages/containers/tensorflow/tensorflow/latestgpu/tensorflowtensorflowlatestgpusha256\:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif python`     Standalone code to reproduce the issue   Relevant log output ",2025-01-24T20:16:31Z,stat:awaiting tensorflower type:build/install comp:gpu TF 2.18,open,0,3,https://github.com/tensorflow/tensorflow/issues/85689,The docker  TF container tries to load libcudnn.so.9 However the container has only been built with libcudnn.so.8 More detail here: tensorflow 2.18 requires libcudnn.so.9,"> The docker TF container tries to load libcudnn.so.9 However the container has only been built with libcudnn.so.8 >  > More detail here: tensorflow 2.18 requires libcudnn.so.9 Thanks, how do we get the maintained to flx if?",", I request you to take a look at this issue where a similar feature has been proposed and it is still open. Also I request to follow the similar feature which has been proposed to have the updates on the similar issue. Thank you!"
1102,"以下是一个github上的tensorflow下的一个issue, 标题是(tensorflow takes a long time to prepare before the first iteration)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version TF 2.10.0  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 11.4/8.9.1  GPU model and memory Nvidia Tesla K20m  Current behavior? tensorflow takes a long time to prepare before the first iteration.I used my custom model for training, but it took 4060 minutes from the time the data was ready to the first iteration. This was true even for a very small dataset. And my model only had 835,620 parameters. This model is used to pick up the phase of seismic data. If an experiment is conducted, the data can be fabricated by itself.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Chuan1937,tensorflow takes a long time to prepare before the first iteration," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version TF 2.10.0  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 11.4/8.9.1  GPU model and memory Nvidia Tesla K20m  Current behavior? tensorflow takes a long time to prepare before the first iteration.I used my custom model for training, but it took 4060 minutes from the time the data was ready to the first iteration. This was true even for a very small dataset. And my model only had 835,620 parameters. This model is used to pick up the phase of seismic data. If an experiment is conducted, the data can be fabricated by itself.  Standalone code to reproduce the issue   Relevant log output ",2025-01-24T13:47:24Z,stat:awaiting response stale type:performance TF 2.10,closed,4,7,https://github.com/tensorflow/tensorflow/issues/85667,This is likely the time spent JITing the Python imperative code to the graph representation that TF uses.,"How can I speed it up? Otherwise, I have to wait for nearly 60 minutes every time.","I think that that's too much. You could try writing TF code in graph mode directly (or switching to JAX for even more performance gains  since you use Keras, use Keras 3 and switch to Jax backend).","It is normal for the first epoch to take longer to train than subsequent epochs, as TensorFlow needs to compile the model and allocate resources. However, an hour does seem like a long time.  **Here are a few things that could be causing the slow training time:** * Larger models with more parameters will take longer to train. * Training on a larger dataset will take longer. * The speed of your CPU, GPU, and available RAM will all affect training speed. * If you are using custom code in your training loop, it could be slowing things down. **Here are some tips to improve training speed:** * If you are just starting out, try using a smaller model with fewer parameters. * If you have a large dataset, try using a smaller subset for training. * If you have a GPU, you can use it to accelerate training. * Use a profiler to identify bottlenecks in your training code. * A smaller batch size can sometimes improve training speed.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1164,"以下是一个github上的tensorflow下的一个issue, 标题是(LiteRT build for Android failing)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version v2.17.0  Custom code No  OS platform and distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version bazel 6.5.0  GCC/compiler version NDK26,NDK28  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following this link https://ai.google.dev/edge/litert/build/android for building libtensorflowlite.so for my android JNI project, but its failing all  the time with below error snapshot **Initial Steps** git clone https://github.com/tensorflow/tensorflow.git cd tensorflow git checkout v2.17.0 ./configure ( For Android Environment) **Below is the content of .tf_configure.bazelrc in my build environment** build action_env PYTHON_BIN_PATH=""/usr/bin/python3"" build action_env PYTHON_LIB_PATH=""/usr/lib/python3.10/distpackages"" build python_path=""/usr/bin/pytho)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,intelav,LiteRT build for Android failing," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version v2.17.0  Custom code No  OS platform and distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version bazel 6.5.0  GCC/compiler version NDK26,NDK28  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following this link https://ai.google.dev/edge/litert/build/android for building libtensorflowlite.so for my android JNI project, but its failing all  the time with below error snapshot **Initial Steps** git clone https://github.com/tensorflow/tensorflow.git cd tensorflow git checkout v2.17.0 ./configure ( For Android Environment) **Below is the content of .tf_configure.bazelrc in my build environment** build action_env PYTHON_BIN_PATH=""/usr/bin/python3"" build action_env PYTHON_LIB_PATH=""/usr/lib/python3.10/distpackages"" build python_path=""/usr/bin/pytho",2025-01-24T05:07:38Z,stat:awaiting response type:build/install stale comp:lite 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85631,"If I run the same build command again , similiar errors apperas from compiling other files ERROR: /home/avaish/.cache/bazel/_bazel_avaish/d98cf14fd195122b7f9fe191efe765ef/external/ruy/ruy/BUILD:423:11: Compiling ruy/denormal.: undeclared inclusion(s) in rule '//ruy:denormal': this rule is missing dependency declarations for the following files included by 'ruy/denormal.cc':   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stdint.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stddef.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/__stddef_max_align_t.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/xmmintrin.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/mmintrin.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/mm_malloc.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stdarg.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/emmintrin.h' Target //tensorflow/lite/java:tensorflowlite failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 0.903s, Critical Path: 0.12s INFO: 12 processes: 12 internal. FAILED: Build did NOT complete successfully","Hi,  I apologize for the delay in my response, I was trying to replicate the same behavior from my end but I'm getting different error and Build did NOT complete successfully so I have added error log below for reference please let me know if Am I missing something here to replicate same behavior which you reported here ?  Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
549,"以下是一个github上的tensorflow下的一个issue, 标题是(Make the TF shape inference update the XlaCallModule's StableHLO module when the shapes are refined, if enabled by `enable_stablehlo_propagation`. It is disabled by default for now.)， 内容是 (Make the TF shape inference update the XlaCallModule's StableHLO module when the shapes are refined, if enabled by `enable_stablehlo_propagation`. It is disabled by default for now.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,copybara-service[bot],"Make the TF shape inference update the XlaCallModule's StableHLO module when the shapes are refined, if enabled by `enable_stablehlo_propagation`. It is disabled by default for now.","Make the TF shape inference update the XlaCallModule's StableHLO module when the shapes are refined, if enabled by `enable_stablehlo_propagation`. It is disabled by default for now.",2025-01-24T02:42:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85630
784,"以下是一个github上的tensorflow下的一个issue, 标题是(Can not import tensorflow as tf)， 内容是 ( Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11.5  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,ghinaaraf,Can not import tensorflow as tf, Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11.5  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense  Standalone code to reproduce the issue   Relevant log output ,2025-01-24T02:39:30Z,type:performance,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85629,Please search for duplicates before opening a new issue,Are you satisfied with the resolution of your issue? Yes No
300,"以下是一个github上的tensorflow下的一个issue, 标题是(#sdy add sharding rules for ragged_all_to_all custom call)， 内容是 (sdy add sharding rules for ragged_all_to_all custom call)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],#sdy add sharding rules for ragged_all_to_all custom call,sdy add sharding rules for ragged_all_to_all custom call,2025-01-23T23:56:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85623
229,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal change only.)， 内容是 (Internal change only.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal change only.,Internal change only.,2025-01-23T23:46:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85622
443,"以下是一个github上的tensorflow下的一个issue, 标题是([ODML] Pass expand-tuple : Migrate from MHLO to StableHLO)， 内容是 ([ODML] Pass expandtuple : Migrate from MHLO to StableHLO FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21825 from zhenyingliu:scheduler 1c5720711c6a7d8173e132922b67eee6e2e8b9dd)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[ODML] Pass expand-tuple : Migrate from MHLO to StableHLO,[ODML] Pass expandtuple : Migrate from MHLO to StableHLO FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21825 from zhenyingliu:scheduler 1c5720711c6a7d8173e132922b67eee6e2e8b9dd,2025-01-23T22:38:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85606
332,"以下是一个github上的tensorflow下的一个issue, 标题是([cpp23] Remove std::aligned_storage<> in tensorflow)， 内容是 ([cpp23] Remove std::aligned_storage in tensorflow std::aligned_storage is deprecated in cpp23.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[cpp23] Remove std::aligned_storage<> in tensorflow,[cpp23] Remove std::aligned_storage in tensorflow std::aligned_storage is deprecated in cpp23.,2025-01-23T17:21:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85586
1082,"以下是一个github上的tensorflow下的一个issue, 标题是(Compatibility table for TensorFlow 2.18 with CUDA and cuDNN is missing on the official website)， 内容是 (In previous versions of TensorFlow, the official documentation included a clear compatibility table specifying which versions of TensorFlow worked with specific versions of CUDA and cuDNN. However, upon reviewing the documentation for TensorFlow 2.18, I noticed this information is no longer available. These tables were  helpful for users, as they prevented installation and compatibility issues when setting up the development environment. It would be great if this compatibility table could be brought back to the official documentation. If it was removed due to any updates or errors that have been identified, it would also be helpful to notify users of such changes. If you're reading this issue, I kindly ask that you notify users if any updates or changes regarding this topic are implemented.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Redoxon33,Compatibility table for TensorFlow 2.18 with CUDA and cuDNN is missing on the official website,"In previous versions of TensorFlow, the official documentation included a clear compatibility table specifying which versions of TensorFlow worked with specific versions of CUDA and cuDNN. However, upon reviewing the documentation for TensorFlow 2.18, I noticed this information is no longer available. These tables were  helpful for users, as they prevented installation and compatibility issues when setting up the development environment. It would be great if this compatibility table could be brought back to the official documentation. If it was removed due to any updates or errors that have been identified, it would also be helpful to notify users of such changes. If you're reading this issue, I kindly ask that you notify users if any updates or changes regarding this topic are implemented.",2025-01-23T10:09:06Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/85560,"After further investigation, I discovered that the issue is specific to the Spanish versions of the documentation (both Latin American and European Spanish), which appear to be outdated compared to the English version. The compatibility table is available in the English documentation, but it is not present in the Spanish translations. It would be helpful if an indicator could be added to show that the Spanish documentation is outdated or missing specific information, to prevent confusion for users relying on these versions."
1171,"以下是一个github上的tensorflow下的一个issue, 标题是([RFC] rocprof insights for rocprof data)， 内容是 ( RFC for rocprof insights  Introduction `rocprof`, `rocprofv2`, and `rocprofv3` (rocprofilersdk) are the profiling tools that can be used to collect AMD hardware performance data when running applications with ROCm/HIP. The collected timeline trace data, which are JSON format for `rocprof` and `rocprofv2` and `pftrace` format for `rocprofv3`, can be visualized via `https://ui.perfetto.dev/` to guide the loop of profiling, analysis and optimization. To gain deep insights into specific running kernels, API launch and memory copy, it is necessary to obtain more statistics about them. rocprof insights, which is developed as a Python package, aims to provide the following functionalities: 1. Data loading, including loading CSV and JSON files saved from `rocprof v1/v2/v3` 2. Data analysis, providing:     Total running time     Number of calls (instances)     Average time     Median time     Min/max time     StdDev time    For each:     K)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,cj401-amd,[RFC] rocprof insights for rocprof data," RFC for rocprof insights  Introduction `rocprof`, `rocprofv2`, and `rocprofv3` (rocprofilersdk) are the profiling tools that can be used to collect AMD hardware performance data when running applications with ROCm/HIP. The collected timeline trace data, which are JSON format for `rocprof` and `rocprofv2` and `pftrace` format for `rocprofv3`, can be visualized via `https://ui.perfetto.dev/` to guide the loop of profiling, analysis and optimization. To gain deep insights into specific running kernels, API launch and memory copy, it is necessary to obtain more statistics about them. rocprof insights, which is developed as a Python package, aims to provide the following functionalities: 1. Data loading, including loading CSV and JSON files saved from `rocprof v1/v2/v3` 2. Data analysis, providing:     Total running time     Number of calls (instances)     Average time     Median time     Min/max time     StdDev time    For each:     K",2025-01-23T10:00:14Z,size:XL,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85559
547,"以下是一个github上的tensorflow下的一个issue, 标题是(Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `all-gather` with an `all-to-all`.)， 内容是 (Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `allgather` with an `alltoall`. Given the following input  The partitioner generates allgather before this change )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,copybara-service[bot],"Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `all-gather` with an `all-to-all`.","Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `allgather` with an `alltoall`. Given the following input  The partitioner generates allgather before this change ",2025-01-22T23:48:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85519
1131,"以下是一个github上的tensorflow下的一个issue, 标题是(Integrate Op Builder with LiteRT Compile Part)， 内容是 ( WHAT We replace the compiler part with Qualcomm implementations. This PR include commits in 3 PRs below. Follow these PRs or commit should help you review. You can get more details in the PR descriptions. 1. https://github.com/jiunkaiy/tensorflow/pull/1 2. https://github.com/jiunkaiy/tensorflow/pull/3 3. https://github.com/jiunkaiy/tensorflow/pull/5  TEST You can checkout this branch and run this test:  I disable these models because I don't have them.  kFeedForwardModel,  kKeyEinsumModel,  kQueryEinsumModel,  kValueEinsumModel,  kAttnVecEinsumModel,  kROPEModel,  kLookUpROPEModel,  kRMSNormModel,  kSDPAModel,  kAttentionModel,  kTransformerBlockModel,  kQSimpleMul16x16Model,  kQMulAdd16x16Model,  kQQueryEinsum16x8Model,  kQKeyEinsum16x8Model,  kQVauleEinsum16x8Model,  kQAttnVecEinsum16x8Model And you will see  There are some bugs in simple_slice_op.mlir so the op validation will fail.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,weilhuan-quic,Integrate Op Builder with LiteRT Compile Part," WHAT We replace the compiler part with Qualcomm implementations. This PR include commits in 3 PRs below. Follow these PRs or commit should help you review. You can get more details in the PR descriptions. 1. https://github.com/jiunkaiy/tensorflow/pull/1 2. https://github.com/jiunkaiy/tensorflow/pull/3 3. https://github.com/jiunkaiy/tensorflow/pull/5  TEST You can checkout this branch and run this test:  I disable these models because I don't have them.  kFeedForwardModel,  kKeyEinsumModel,  kQueryEinsumModel,  kValueEinsumModel,  kAttnVecEinsumModel,  kROPEModel,  kLookUpROPEModel,  kRMSNormModel,  kSDPAModel,  kAttentionModel,  kTransformerBlockModel,  kQSimpleMul16x16Model,  kQMulAdd16x16Model,  kQQueryEinsum16x8Model,  kQKeyEinsum16x8Model,  kQVauleEinsum16x8Model,  kQAttnVecEinsum16x8Model And you will see  There are some bugs in simple_slice_op.mlir so the op validation will fail.",2025-01-22T10:19:36Z,comp:lite size:XL,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85477
311,"以下是一个github上的tensorflow下的一个issue, 标题是(Update the external buffer storage to use an ID based approach)， 内容是 (Update the external buffer storage to use an ID based approach)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Update the external buffer storage to use an ID based approach,Update the external buffer storage to use an ID based approach,2025-01-22T07:40:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85448
622,"以下是一个github上的tensorflow下的一个issue, 标题是([MHLO] Handle dynamic dimensions in HLO<->MHLO)， 内容是 ([MHLO] Handle dynamic dimensions in HLOMHLO  Fix creating constant zero for ConvertOp HLO>MHLO translation  Fix broadcast in dim bounded lowering from MHLO>HLO  Don't use StableHLO verification methods on MHLO ReshapeOp with bounded dynamic outputs  Currently this fails when trying to create the `mhlo.constant dense` that gets fed into compare since constants cannot have a bounded size.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[MHLO] Handle dynamic dimensions in HLO<->MHLO,[MHLO] Handle dynamic dimensions in HLOMHLO  Fix creating constant zero for ConvertOp HLO>MHLO translation  Fix broadcast in dim bounded lowering from MHLO>HLO  Don't use StableHLO verification methods on MHLO ReshapeOp with bounded dynamic outputs  Currently this fails when trying to create the `mhlo.constant dense` that gets fed into compare since constants cannot have a bounded size.,2025-01-22T03:54:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85441
312,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA] Add ragged-all-to-all support to latency hiding scheduler.)， 内容是 ([XLA] Add raggedalltoall support to latency hiding scheduler.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[XLA] Add ragged-all-to-all support to latency hiding scheduler.,[XLA] Add raggedalltoall support to latency hiding scheduler.,2025-01-21T23:08:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85434
555,"以下是一个github上的tensorflow下的一个issue, 标题是(Add HLO `RaggedAllToAll` --> `mhlo.custom_call @ragged_all_to_all` translation)， 内容是 (Add HLO `RaggedAllToAll` > `mhlo.custom_call ` translation Since `channel_handle` is used by select few ops, it's recommended to not import this attribute generally. Instead, just extract `channel_id` (`channel_type` is not used in this op) and set it as an `IntegerAttr` for roundtripping.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Add HLO `RaggedAllToAll` --> `mhlo.custom_call @ragged_all_to_all` translation,"Add HLO `RaggedAllToAll` > `mhlo.custom_call ` translation Since `channel_handle` is used by select few ops, it's recommended to not import this attribute generally. Instead, just extract `channel_id` (`channel_type` is not used in this op) and set it as an `IntegerAttr` for roundtripping.",2025-01-21T20:18:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85420
288,"以下是一个github上的tensorflow下的一个issue, 标题是(How do you install this using poetry on macos ?)， 内容是 (This still doesn't work     So how do I install this ?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,primski,How do you install this using poetry on macos ?,This still doesn't work     So how do I install this ?,2025-01-21T20:05:28Z,stat:awaiting response type:support stale,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85418,"I removed it from pyproject.toml and used pip, it works with that. ","TF is developed with pip in mind, no one has tested if it can be installed with other systems (although likely it should, to the extent that they are compatible)",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
345,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Remove simplify_int4_dots pass from gpu_compiler passes.)， 内容是 ([XLA:GPU] Remove simplify_int4_dots pass from gpu_compiler passes. It is not in use anymore.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA:GPU] Remove simplify_int4_dots pass from gpu_compiler passes.,[XLA:GPU] Remove simplify_int4_dots pass from gpu_compiler passes. It is not in use anymore.,2025-01-20T14:40:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85354
1193,"以下是一个github上的tensorflow下的一个issue, 标题是(Tutorial ""Multi-worker training with Keras"" fails to complete)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version v1.12.1120353gc5bd67bc56f 2.19.0dev20250107  Custom code No  OS platform and distribution Debian 6.1.1231 (20250102) x86_64 GNU/Linux  Mobile device _No response_  Python version Python 3.12.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following the tutorial everything goes well until you start the second worker. Then the below failure occures. 20250120 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250120 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467])请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,Chadster766,"Tutorial ""Multi-worker training with Keras"" fails to complete"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version v1.12.1120353gc5bd67bc56f 2.19.0dev20250107  Custom code No  OS platform and distribution Debian 6.1.1231 (20250102) x86_64 GNU/Linux  Mobile device _No response_  Python version Python 3.12.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following the tutorial everything goes well until you start the second worker. Then the below failure occures. 20250120 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250120 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467]",2025-01-20T14:03:18Z,type:bug TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/85351,"Hi **** , Apologies for the delay, and thank you for raising your concern here. Could you please provide more details, such as the versions of TensorFlow and any other relevant libraries you are using? Additionally, sharing your code would make it easier for us to troubleshoot the issue effectively. In the meantime, please ensure that all compatibility requirements are met. For your reference, here is the relevant documentation. Thank you!","Hi  , I think I've provided the info regarding versions of TensorFlow and any other relevant libraries in the issue creation. I'm not running any of my code I'm just using the jupyter notebook of the tutorial."
1234,"以下是一个github上的tensorflow下的一个issue, 标题是(Cannot use GpuDelegate - java.lang.IllegalArgumentException: Internal error: Cannot create interpreter)， 内容是 (Hi, I get ""Cannot use GpuDelegate  java.lang.IllegalArgumentException: Internal error: Cannot create interpreter"" when attempting to use GpuDelegate I have seen a couple of issue related to this but all seems to be abandoned. I have created a repo replicating the issue.  You can see the config at https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.javaL114 **System information**  Android Device information: Samsung S23  TensorFlow Lite in Play Services SDK version : 16.4.0  Google Play Services version: 24.50.34 **Standalone code to reproduce the issue** Clone and run project from https://github.com/NLLAPPS/WhisperOffline/ **Any other info / logs** `Created TensorFlow Lite delegate for GPU. Created interpreter. Created interpreter. java.lang.IllegalArgumentException: Internal error: Cannot create interpreter:  at com.google.android.gms.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,NLLAPPS,Cannot use GpuDelegate - java.lang.IllegalArgumentException: Internal error: Cannot create interpreter,"Hi, I get ""Cannot use GpuDelegate  java.lang.IllegalArgumentException: Internal error: Cannot create interpreter"" when attempting to use GpuDelegate I have seen a couple of issue related to this but all seems to be abandoned. I have created a repo replicating the issue.  You can see the config at https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.javaL114 **System information**  Android Device information: Samsung S23  TensorFlow Lite in Play Services SDK version : 16.4.0  Google Play Services version: 24.50.34 **Standalone code to reproduce the issue** Clone and run project from https://github.com/NLLAPPS/WhisperOffline/ **Any other info / logs** `Created TensorFlow Lite delegate for GPU. Created interpreter. Created interpreter. java.lang.IllegalArgumentException: Internal error: Cannot create interpreter:  at com.google.android.gms.",2025-01-19T22:01:13Z,comp:lite TFLiteGpuDelegate,open,0,12,https://github.com/tensorflow/tensorflow/issues/85313,"there is an issue with the configuration of the GpuDelegate or its compatibility with your environment, Test with CPUonly execution to confirm whether the issue is specific to the GPU Delegate","Hi and thank you. I have tested CPU only it works fine. What is the ""configuration of the GpuDelegate""? My configuration can be seen at https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.javaL114","'GpuDelegateFactory.Options' in your code uses generic settings. For better control, you can configure options like precision or inference preference. try this or Run a compatibility check  GpuDelegateFactory.Options gpuOptions = new GpuDelegateFactory.Options(); gpuOptions.setInferencePreference(GpuDelegateFactory.Options.INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER); gpuOptions.setPrecisionLossAllowed(true); ","Thanks, regarding ""compatibility check"". Is it possible to provide link to documentation for compatibility checking?",check out these links : https://stackoverflow.com/questions/50622525/whichtensorflowandcudaversioncombinationsarecompatible https://docs.nvidia.com/cuda/cudatoolkitreleasenotes/index.html https://docs.nvidia.com/deeplearning/cudnn/latest/reference/supportmatrix.html,"Your links seem to be related to PCs. Have I misunderstood what GpuDelegateFactory does. I thought it would be for using GPU on the phone since the artifact is ""playservicestflitegpu""",I also just noticed you are not related to this project. Do you have experience on implementing tflite on Android?,"you are correct in assuming that GpuDelegateFactory is intended for mobile devices to leverage the GPU for TensorFlow Lite inference, especially in Android using the playservicestflitegpu artifact. If you're targeting mobile platforms, this is the correct path to enable GPU acceleration dependencies {     implementation 'org.tensorflow:tensorflowlite:2.x.x'     implementation 'org.tensorflow:tensorflowlitegpu:2.x.x' } GpuDelegate delegate = new GpuDelegate(); Interpreter.Options options = new Interpreter.Options().addDelegate(delegate); Interpreter interpreter = new Interpreter(modelFile, options);","yes i am not a part of this project yet , im trying to contribute as much possible to be recognized by the organization before gsoc 2025","> yes i am not a part of this project yet , i'm trying to contribute as much possible to be recognized by the organization before gsoc 2025 I don't think you will be able to help me in this case. Issue seems to be related to actual SDK/API. Hopefully  will have a look at it.","Hi,   I apologize for the delayed response, I was trying to reproduce the similar issue from my end after cloning your provided repo but I'm getting below error message, if possible could you please help me to replicate the same issue from my end which you reported in the issue template to investigate this issue further from our end ? **Here is error log for reference :**  Thank you for your cooperation and patience.","Hi, project is using com.android.tools.build:gradle:8.8.0 there is no com.android.tools.build:gradle:8.10.2. 8.10.2 is a Gradle version, not Android build tools version. Have you changed anything? This stack overflow post seems to suggest it may be related to Android Studio Gradle Settings.  Here is mine attached for the project !Image"
1169,"以下是一个github上的tensorflow下的一个issue, 标题是(Force TF to log GPU memory allocation)， 内容是 ( Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.18.0  Custom code No  OS platform and distribution Debian 12  Mobile device _No response_  Python version 3.11.2  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.2  GPU model and memory RTX 3060 12Gb  Current behavior? If I ran out of GPU memory I get error saying `Allocator (GPU_0_bfc) ran out of memory trying to allocate 9.78GiB` But if allocation succeeded there is no real way to know what the allocator did/doing.. I know that you can use nvidiasmi etc but this is an indirect way to estimate very crudely whats going on. What I believe would be very beneficial (because I dont know about you guys but I run out of memory so very often and I cant affort 128Gb card)  is to be able to force TF to log every and single one GPU memory allocation.  So I can see whats going on my )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,maxima120,Force TF to log GPU memory allocation, Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.18.0  Custom code No  OS platform and distribution Debian 12  Mobile device _No response_  Python version 3.11.2  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.2  GPU model and memory RTX 3060 12Gb  Current behavior? If I ran out of GPU memory I get error saying `Allocator (GPU_0_bfc) ran out of memory trying to allocate 9.78GiB` But if allocation succeeded there is no real way to know what the allocator did/doing.. I know that you can use nvidiasmi etc but this is an indirect way to estimate very crudely whats going on. What I believe would be very beneficial (because I dont know about you guys but I run out of memory so very often and I cant affort 128Gb card)  is to be able to force TF to log every and single one GPU memory allocation.  So I can see whats going on my ,2025-01-19T13:50:42Z,stat:awaiting tensorflower type:feature comp:gpu,open,0,0,https://github.com/tensorflow/tensorflow/issues/85303
729,"以下是一个github上的tensorflow下的一个issue, 标题是(Aborted  in `tf.raw_ops.RaggedGather`)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tf.raw_ops.RaggedGather` triggers crash.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,LongZE666,Aborted  in `tf.raw_ops.RaggedGather`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tf.raw_ops.RaggedGather` triggers crash.  Standalone code to reproduce the issue   Relevant log output ",2025-01-18T09:32:16Z,type:bug comp:ops 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/85242,I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TFnightly. Please find the gist here for your reference. Thank you!
758,"以下是一个github上的tensorflow下的一个issue, 标题是(Segmentation fault (core dumped) in `RaggedTensorToTensor`)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tf.raw_ops.RaggedTensorToTensor` triggers crash.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,LongZE666,Segmentation fault (core dumped) in `RaggedTensorToTensor`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tf.raw_ops.RaggedTensorToTensor` triggers crash.  Standalone code to reproduce the issue   Relevant log output ",2025-01-18T09:27:19Z,type:bug comp:ops 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/85240,I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TFnightly. Please find the gist here for your reference. Thank you!
227,"以下是一个github上的tensorflow下的一个issue, 标题是(internal change only)， 内容是 (internal change only)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],internal change only,internal change only,2025-01-17T17:32:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85178
227,"以下是一个github上的tensorflow下的一个issue, 标题是(internal change only)， 内容是 (internal change only)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],internal change only,internal change only,2025-01-17T15:18:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85175
865,"以下是一个github上的tensorflow下的一个issue, 标题是(Could not get sample weight from customized loss)， 内容是 ( Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.13.1  Custom code Yes  OS platform and distribution CentOS 7.9  Mobile device _No response_  Python version 3.8.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? We used customized loss for the model training, and would like to get sample weight to calculate the loss. However, sample weight does not pass to loss function as expected.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,henghamao,Could not get sample weight from customized loss," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.13.1  Custom code Yes  OS platform and distribution CentOS 7.9  Mobile device _No response_  Python version 3.8.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? We used customized loss for the model training, and would like to get sample weight to calculate the loss. However, sample weight does not pass to loss function as expected.  Standalone code to reproduce the issue   Relevant log output ",2025-01-17T15:16:39Z,type:feature comp:keras TF 2.13,closed,0,9,https://github.com/tensorflow/tensorflow/issues/85174,"I was able to reproduce the issue on Colab using TensorFlow v2.13 and TFnightly. Please find the gist1, gist2 here for your reference. Thank you!","Hi   When you are building a custom loss function, the loss function should have a signature of **** (source). When doing this, you are essentially trying to override the `call()` function in `class Loss` in keras/src/losses/loss/py.  You can still pass in a `sample_weight` parameter to the `model.fit()` call with the custom loss function, but sample_weight will be multiplied elementwise with the loss terms in the reduce_weighted_values() call. So it won't work the way you want it to for the r2 loss. One observation about your code is that sample_weight only depends on y_true, so you could move the sample_weight computation inside your function.  The following code would work:  If you want to use a custom logic for applying sample_weights, there's another way to do it by subclassing the `keras.losses.Loss` class. You would have to override the `__init__` and `call` functions. You can pass in a custom function during `__init__` and use it during `call()`. For example:  Note that you would probably need to use tensorflow operations and not python operations, since the loss function gets converted to a tf.function during the graph execution. You can initialize the model with the loss class above as follows:  I hope this explanation helps. Please let me know if you have any questions. ","Hi  , Thanks for the reply. The code is to reproduce the issue. And in real scenario, the sample weight could not refer by y_ture value.  The problme is the regression for multiple categories of data. The sample weight is to apply for different categories. y_ture is the data point at time stamp t. If it is a classify problem, we could refer sample weight by category label of y_ture value. However, for regression problems, we could not do that. In torch, we could easily use customize loss to calcualte weighted_r2_loss. Example code as below  Hope tf could provide similar solution to solve the problem.",  Thank you for going over your use case in detail. You can do this in Tensorflow by subclassing `keras.Model` and overriding the `compute_loss()` function. ,"  Great thanks for providing the solution. It works for our problems. BTW, there is another issue about class weight and sample weight with the similar code to reproduce the issue. https://github.com/tensorflow/tensorflow/issues/77958 We submited the issue a few months ago, and it did not get any further updates.",Happy to help!  Let me take a look at https://github.com/tensorflow/tensorflow/issues/77958.,"  Hi SanjaySG, We found a problem with the code. By using comput_loss(), the model tend to maximize the loss, rather than minimize the loss. Here is the code to repoduce the issue:  We observed the metric 'mae' keep growing, and the loss grows as well.","  This is down to the objective chosen for optimization. The model.fit() call tries to **minimize** the loss. r2 on the other hand should increase when the model performs better. A naive way to do this is by providing the negative of r2.  More importantly, r2 is not differentiable. So ideally, it shouldn't be used as a loss function for gradient descent, but it can be used as a metric. Something like MSE or MAE would be a better loss function. ",Great thanks for the explanations.
227,"以下是一个github上的tensorflow下的一个issue, 标题是(internal change only)， 内容是 (internal change only)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],internal change only,internal change only,2025-01-17T15:11:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85173
277,"以下是一个github上的tensorflow下的一个issue, 标题是(Add some clarifying comments for Dockerfiles.)， 内容是 (Add some clarifying comments for Dockerfiles.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Add some clarifying comments for Dockerfiles.,Add some clarifying comments for Dockerfiles.,2025-01-17T14:59:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85172
229,"以下是一个github上的tensorflow下的一个issue, 标题是(internal changes only)， 内容是 (internal changes only)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],internal changes only,internal changes only,2025-01-17T13:07:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85161
537,"以下是一个github上的tensorflow下的一个issue, 标题是(Add LLM inference engine based on CompiledModel APIs)， 内容是 (Add LLM inference engine based on CompiledModel APIs The new pipeline is only enabled with `use_compiled_model` flag to the script. It will define `USE_LITERT_COMPILED_MODEL` for the executor builds. The KV Cache management logic is implemented in LlmLiteRtCompiledModelExecutor with TensorBuffers.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,copybara-service[bot],Add LLM inference engine based on CompiledModel APIs,Add LLM inference engine based on CompiledModel APIs The new pipeline is only enabled with `use_compiled_model` flag to the script. It will define `USE_LITERT_COMPILED_MODEL` for the executor builds. The KV Cache management logic is implemented in LlmLiteRtCompiledModelExecutor with TensorBuffers.,2025-01-16T21:22:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85088
419,"以下是一个github上的tensorflow下的一个issue, 标题是(Add a test for the `cholesky_expander` pass.)， 内容是 (Add a test for the `cholesky_expander` pass. There was some test coverage for `cholesky_expander`, but it wasn't located within the `hlo/` component and wasn't exactly a unit test as such.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Add a test for the `cholesky_expander` pass.,"Add a test for the `cholesky_expander` pass. There was some test coverage for `cholesky_expander`, but it wasn't located within the `hlo/` component and wasn't exactly a unit test as such.",2025-01-16T19:26:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85081
1156,"以下是一个github上的tensorflow下的一个issue, 标题是(Tensorflow related issue)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Collecting tensorflowNote: you may need to restart the kernel to use updated packages.   Using cached tensorflow2.18.0cp310cp310win_amd64.whl.metadata (3.3 kB) Requirement already satisfied: tensorflowintel==2.18.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflow) (2.18.0) Requirement already satisfied: abslpy>=1.0.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (2.1.0) Requirement already satisfied: astunparse>=1.6.0 in c:\users\avs mani\desktop\project\venv\lib\sitep)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,SankuriJeyaSanjana,Tensorflow related issue, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Collecting tensorflowNote: you may need to restart the kernel to use updated packages.   Using cached tensorflow2.18.0cp310cp310win_amd64.whl.metadata (3.3 kB) Requirement already satisfied: tensorflowintel==2.18.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflow) (2.18.0) Requirement already satisfied: abslpy>=1.0.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (2.1.0) Requirement already satisfied: astunparse>=1.6.0 in c:\users\avs mani\desktop\project\venv\lib\sitep,2025-01-16T15:24:33Z,type:build/install,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85064,"Please use \`\`\` to quote error messages to make them more readable. Also, please search previous issues, this issue has been discussed multiple times.",Are you satisfied with the resolution of your issue? Yes No
965,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #20274: [ROCm] Emit allocas on function entry in lower_tensors.cc)， 内容是 (PR CC(Error:Execution failed for task ':buildNativeBazel'. > A problem occurred starting process 'command '/usr/local/bin/bazel''): [ROCm] Emit allocas on function entry in lower_tensors.://github.com/openxla/xla/pull/20274 This fixes //tensorflow/compiler/tests:segment_reduction_ops_test_gpu Copybara import of the project:  6553059e8d5bf039ef526b2b904808b55051cab9 by Dragan Mladjenovic : [ROCm] Emit allocas on function entry in lower_tensors.(Error:Execution failed for task ':buildNativeBazel'. > A problem occurred starting process 'command '/usr/local/bin/bazel'') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20274 from ROCm:c64_atomics 6553059e8d5bf039ef526b2b904808b55051cab9)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],PR #20274: [ROCm] Emit allocas on function entry in lower_tensors.cc,PR CC(Error:Execution failed for task ':buildNativeBazel'. > A problem occurred starting process 'command '/usr/local/bin/bazel''): [ROCm] Emit allocas on function entry in lower_tensors.://github.com/openxla/xla/pull/20274 This fixes //tensorflow/compiler/tests:segment_reduction_ops_test_gpu Copybara import of the project:  6553059e8d5bf039ef526b2b904808b55051cab9 by Dragan Mladjenovic : [ROCm] Emit allocas on function entry in lower_tensors.(Error:Execution failed for task ':buildNativeBazel'. > A problem occurred starting process 'command '/usr/local/bin/bazel'') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20274 from ROCm:c64_atomics 6553059e8d5bf039ef526b2b904808b55051cab9,2025-01-16T15:23:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85063
1210,"以下是一个github上的tensorflow下的一个issue, 标题是(Issues on trying to compile TensorFlow C API for JETSON AGX Xavier using Bazel)， 内容是 (On my JETSON AGX Xavier, with: cuda: 11.4.315 cuDNN: 8.6.0 tensorrt: 8.5.2.2 jetpack: 5.1.3 python3 c “import tensorflow as tf; print(‘TensorFlow version:’, tf.version)” TensorFlow version: 2.11.0 I can’t compile tf with bazel ( bazel version: bazel 5.3.0 ) , error: ~/tensorflow$ bazel build config=opt config=cuda //tensorflow:libtensorflow.so Starting local Bazel server and connecting to it… WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Reading ‘startup’ options from /home/redans/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client: Inherited ‘common’ options: isatty=1 terminal_columns=237 INFO: Reading rc options for ‘build’ from /home/redans/tensorflow/.bazelrc: Inherited ‘common’ options: experimental_repo_remote_exec INFO: Reading rc options for ‘build’ from /home/redans/tensorflow/.baz)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,user-redans,Issues on trying to compile TensorFlow C API for JETSON AGX Xavier using Bazel,"On my JETSON AGX Xavier, with: cuda: 11.4.315 cuDNN: 8.6.0 tensorrt: 8.5.2.2 jetpack: 5.1.3 python3 c “import tensorflow as tf; print(‘TensorFlow version:’, tf.version)” TensorFlow version: 2.11.0 I can’t compile tf with bazel ( bazel version: bazel 5.3.0 ) , error: ~/tensorflow$ bazel build config=opt config=cuda //tensorflow:libtensorflow.so Starting local Bazel server and connecting to it… WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Reading ‘startup’ options from /home/redans/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client: Inherited ‘common’ options: isatty=1 terminal_columns=237 INFO: Reading rc options for ‘build’ from /home/redans/tensorflow/.bazelrc: Inherited ‘common’ options: experimental_repo_remote_exec INFO: Reading rc options for ‘build’ from /home/redans/tensorflow/.baz",2025-01-16T10:26:30Z,stat:awaiting response type:build/install stale subtype:bazel TF 2.11,closed,0,3,https://github.com/tensorflow/tensorflow/issues/85034,"redans, Tensorflow v2.11 is a pretty older version which is not actively supported. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.In this case, can you please try installing TensorFlow v2.11 which the respective configurations. https://www.tensorflow.org/install/source Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Are you satisfied with the resolution of your issue? Yes No
1078,"以下是一个github上的tensorflow下的一个issue, 标题是(tf.config.LogicalDeviceConfiguration() not able to set the memory limit but tf.config.experimental.VirtualDeviceConfiguration() is able to)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 22.04.4 LTS  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.2  GPU model and memory RTX A5000 24Gb  Current behavior? I was trying to set the memory limit of 10Gb on the virtual device using tf.config.LogicalDeviceConfiguration(), but when I trained the model it was taking way more than 10Gb of memory. Eventually I was able to set the memory limit using tf.config.experimental.VirtualDeviceConfiguration() but I'm not sure why  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,rogerci91,tf.config.LogicalDeviceConfiguration() not able to set the memory limit but tf.config.experimental.VirtualDeviceConfiguration() is able to," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 22.04.4 LTS  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.2  GPU model and memory RTX A5000 24Gb  Current behavior? I was trying to set the memory limit of 10Gb on the virtual device using tf.config.LogicalDeviceConfiguration(), but when I trained the model it was taking way more than 10Gb of memory. Eventually I was able to set the memory limit using tf.config.experimental.VirtualDeviceConfiguration() but I'm not sure why  Standalone code to reproduce the issue   Relevant log output ",2025-01-16T07:37:49Z,stat:awaiting response type:bug stale 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85026,"TensorFlow's tf.config.experimental.set_virtual_device_configuration is more focused on managing device resources at a lower level, such as limiting memory and managing the allocation between multiple virtual devices. This is why it worked for you while the tf.config.LogicalDeviceConfiguration did not.If you need to limit memory usage, you should use tf.config.experimental.set_virtual_device_configuration() to ensure the desired memory cap is respected.","Hi **** , Hi **** Thank you for your pointers. Apologies for the delay, and thank you for raising your concern here. As  mentioned, `tf.config.experimental.set_virtual_device_configuration()` works by managing resource allocation at a lower level, including limiting memory usage and handling allocation across multiple virtual devices. Therefore, `tf.config.LogicalDeviceConfiguration()` will not achieve the desired results in this scenario. It is recommended to use `tf.config.experimental.set_virtual_device_configuration()` for better outcomes. Here is the relevant TensorFlow documentation for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
569,"以下是一个github上的tensorflow下的一个issue, 标题是(Failed to load native TensorFlow Lite methods)， 内容是 (Hi, I'm trying to use tensorflow lite version 2.15.0 to run my tflite model and I'm getting an error when initializing the interpreter. I'm adding the tensor flow libraries in the gradle file  `implementation('org.tensorflow:tensorflowlite') { version { strictly(""2.15.0"") } }` Code:   TFLite version: 2.15.0 Device: Samsung S20 Error:  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,chandu464,Failed to load native TensorFlow Lite methods,"Hi, I'm trying to use tensorflow lite version 2.15.0 to run my tflite model and I'm getting an error when initializing the interpreter. I'm adding the tensor flow libraries in the gradle file  `implementation('org.tensorflow:tensorflowlite') { version { strictly(""2.15.0"") } }` Code:   TFLite version: 2.15.0 Device: Samsung S20 Error:  ",2025-01-15T17:27:48Z,stat:awaiting response type:support stale comp:lite TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84976,"Hi,   I apologize for the delayed response, if possible could you please help us with your Github repo along with TFLite model and complete steps to replicate the same behavior from our end to investigate this issue further from our end ? Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
375,"以下是一个github上的tensorflow下的一个issue, 标题是(Pass in pointer instead of Shape object to InstructionValueSet constructor)， 内容是 (Pass in pointer instead of Shape object to InstructionValueSet constructor Avoids the copying of the shape object.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Pass in pointer instead of Shape object to InstructionValueSet constructor,Pass in pointer instead of Shape object to InstructionValueSet constructor Avoids the copying of the shape object.,2025-01-15T15:24:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84972
1200,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #21437: [ds-fusion] Fix algebraic simplifier error in debug mode.)， 内容是 (PR CC(Resnet50 applying last pooling layer regardless pooling parameter): [dsfusion] Fix algebraic simplifier error in debug mode. Imported from GitHub PR https://github.com/openxla/xla/pull/21437 This error was observed while trying to land CC(Raspberry Pi install command not properly formatted.) (which is needed for the dsfusion work). This error occurs when there is a constant operation that can be converted into a scalar broadcast, but some other operation is a successor for the constant operation (via control dependency). Such a dependency is not relayed and so the operation is not converted even after the `ReplaceWithNewInstruction` function call. This causes a runtime error in debug mode testing. Fixing this by relaying this control dependency. Copybara import of the project:  9601c96d468a0d56d9f3ed0a925186ab49a4341b by Shraiysh Vaishay : [dsfusion] Fix algebraic simplifier error in debug mode. This error was observed while )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #21437: [ds-fusion] Fix algebraic simplifier error in debug mode.,"PR CC(Resnet50 applying last pooling layer regardless pooling parameter): [dsfusion] Fix algebraic simplifier error in debug mode. Imported from GitHub PR https://github.com/openxla/xla/pull/21437 This error was observed while trying to land CC(Raspberry Pi install command not properly formatted.) (which is needed for the dsfusion work). This error occurs when there is a constant operation that can be converted into a scalar broadcast, but some other operation is a successor for the constant operation (via control dependency). Such a dependency is not relayed and so the operation is not converted even after the `ReplaceWithNewInstruction` function call. This causes a runtime error in debug mode testing. Fixing this by relaying this control dependency. Copybara import of the project:  9601c96d468a0d56d9f3ed0a925186ab49a4341b by Shraiysh Vaishay : [dsfusion] Fix algebraic simplifier error in debug mode. This error was observed while ",2025-01-15T07:03:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84926
906,"以下是一个github上的tensorflow下的一个issue, 标题是(Add HasProperty to HloRunnerInterface and implementations.)， 内容是 (Add HasProperty to HloRunnerInterface and implementations. `HasProperty` allows us to opaquely communicate the presence of arbitrary facts that may or may not be backenddependent. The runner (or underlying client  at the runner's discretion) returns `true` when called with an appropriate property tag if that predicate is true. One key usecase for this feature is to decouple our tests from specific backends/runner implementations. Some of our test cases only work on specific configurations and have predicates to skip execution when not supported. Property tags provide a way for that predicate to be implemented at the runner level and outside of the test.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Add HasProperty to HloRunnerInterface and implementations.,Add HasProperty to HloRunnerInterface and implementations. `HasProperty` allows us to opaquely communicate the presence of arbitrary facts that may or may not be backenddependent. The runner (or underlying client  at the runner's discretion) returns `true` when called with an appropriate property tag if that predicate is true. One key usecase for this feature is to decouple our tests from specific backends/runner implementations. Some of our test cases only work on specific configurations and have predicates to skip execution when not supported. Property tags provide a way for that predicate to be implemented at the runner level and outside of the test.,2025-01-15T01:34:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84912
311,"以下是一个github上的tensorflow下的一个issue, 标题是(Support expanding ragged all-to-all dims similar to all-to-alls.)， 内容是 (Support expanding ragged alltoall dims similar to alltoalls.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Support expanding ragged all-to-all dims similar to all-to-alls.,Support expanding ragged alltoall dims similar to alltoalls.,2025-01-14T22:27:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84899
731,"以下是一个github上的tensorflow下的一个issue, 标题是(Seg Fault when iterate dataset created from data service)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Segfault when trying to iterate dataset get from data service.  Standalone code to reproduce the issue )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Yadan-Wei,Seg Fault when iterate dataset created from data service, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Segfault when trying to iterate dataset get from data service.  Standalone code to reproduce the issue ,2025-01-14T21:57:20Z,type:bug comp:ops TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/84897
846,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU] Add `RewritePattern`s for binary elementwise ops in `SimplifyAffinePass`.)， 内容是 ([XLA:GPU] Add `RewritePattern`s for binary elementwise ops in `SimplifyAffinePass`. The rewrites allow us to concretize `index` computations into computations on integers with fixed widths. Triton forces `index`es to concretize to 32bitwide integers, forcing us to concretize early in order to work around an integer overflow when we use `ApplyIndexingOp`s to compute a linear offset into an array with more than `2^32` elements. Eventually, the concretization should be made into a proper passbut we start with a set of `RewritePattern`s to fix the existing integer overflow.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA:GPU] Add `RewritePattern`s for binary elementwise ops in `SimplifyAffinePass`.,"[XLA:GPU] Add `RewritePattern`s for binary elementwise ops in `SimplifyAffinePass`. The rewrites allow us to concretize `index` computations into computations on integers with fixed widths. Triton forces `index`es to concretize to 32bitwide integers, forcing us to concretize early in order to work around an integer overflow when we use `ApplyIndexingOp`s to compute a linear offset into an array with more than `2^32` elements. Eventually, the concretization should be made into a proper passbut we start with a set of `RewritePattern`s to fix the existing integer overflow.",2025-01-14T19:52:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84891
435,"以下是一个github上的tensorflow下的一个issue, 标题是([HLO Componentization] Add deprecation timeline to aliased build targets.)， 内容是 ([HLO Componentization] Add deprecation timeline to aliased build targets. This step towards encouraging extrenal projects to migrate to the already migrated hlo subcomponents.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],[HLO Componentization] Add deprecation timeline to aliased build targets.,[HLO Componentization] Add deprecation timeline to aliased build targets. This step towards encouraging extrenal projects to migrate to the already migrated hlo subcomponents.,2025-01-14T18:40:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84878
1202,"以下是一个github上的tensorflow下的一个issue, 标题是(Yolov8-seg.pt segmentation model is deployed on Android after training)， 内容是 (**System information**  Android Device information (use `adb shell getprop ro.build.fingerprint`   if possible): ”“” vivo/PD2020/PD2020:10/QP1A.190711.020/compiler10141555:user/releasekeys “”“  TensorFlow Lite in Play Services SDK version (found in `build.gradle`): ”“”     implementation 'org.tensorflow:tensorflowlitetaskvision:0.4.0'     implementation 'org.tensorflow:tensorflowlitegpudelegateplugin:0.4.0'     implementation 'org.tensorflow:tensorflowlitegpu:2.9.0' “”“  Google Play Services version   (`Settings` > `Apps` > `Google Play Services` > `App details`): **Standalone code to reproduce the issue** ”“Here is the full code of the program”“ """""" public class MainActivity extends AppCompatActivity {     private String MODEL = ""best_float32_metadata.tflite"";          protected void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);         setContentView(R.layout.activity_main);         Bitmap bitm)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,fsamekl,Yolov8-seg.pt segmentation model is deployed on Android after training,"**System information**  Android Device information (use `adb shell getprop ro.build.fingerprint`   if possible): ”“” vivo/PD2020/PD2020:10/QP1A.190711.020/compiler10141555:user/releasekeys “”“  TensorFlow Lite in Play Services SDK version (found in `build.gradle`): ”“”     implementation 'org.tensorflow:tensorflowlitetaskvision:0.4.0'     implementation 'org.tensorflow:tensorflowlitegpudelegateplugin:0.4.0'     implementation 'org.tensorflow:tensorflowlitegpu:2.9.0' “”“  Google Play Services version   (`Settings` > `Apps` > `Google Play Services` > `App details`): **Standalone code to reproduce the issue** ”“Here is the full code of the program”“ """""" public class MainActivity extends AppCompatActivity {     private String MODEL = ""best_float32_metadata.tflite"";          protected void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);         setContentView(R.layout.activity_main);         Bitmap bitm",2025-01-14T08:50:42Z,type:support comp:lite Android,open,0,3,https://github.com/tensorflow/tensorflow/issues/84829,I uploaded the project to GitHub. Thank you for your help“https://github.com/fsamekl/Yolov8segAndroidtflite/tree/master”,"Hi,   I apologize for the delayed response, The first issue, `Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images` it requires specifying `NormalizationOptions` metadata to preprocess input images."", was due to lack of metadata. To be more specific, floating models require metadata information of NormalizationOptions. LiteRT Metadata Writer API provides an easytouse API to create Model Metadata for popular ML tasks supported by the TFLite Task Library. You can add metadata using Image segmenters, Please refer TensorFlow Lite Image Segmentation Demo example which may help you to solve your issue. Thank you for your understanding and patience.","> Hi, [](https://github.com/fsamekl) I apologize for the delayed response, The first issue, `Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images` it requires specifying `NormalizationOptions` metadata to preprocess input images."", was due to lack of metadata. To be more specific, floating models require metadata information of NormalizationOptions. >  > LiteRT Metadata Writer API provides an easytouse API to create Model Metadata for popular ML tasks supported by the TFLite Task Library. You can add metadata using Image segmenters, Please refer TensorFlow Lite Image Segmentation Demo example which may help you to solve your issue. >  > Thank you for your understanding and patience. The second question you didn't answer me. I used the segmentation model trained by yolov8. It has two outputs, but imagesegment=imagesegment.createfromfileandoptions (this, model, options); This can only accept one output. What should I do? I uploaded the code to GitHub. I hope you can help solve it"
1105,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #21104: [NVIDIA GPU] Preserve backend config when folding transpose)， 内容是 (PR CC(Feature Request: 5D rot90 (for voxel grid rotations)): [NVIDIA GPU] Preserve backend config when folding transpose Imported from GitHub PR https://github.com/openxla/xla/pull/21104 Transpose folding pass doesn't preserve backend config when creating the new dot with transpose folded. Changing the behavior to copy the old dot's config to the new dot. Copybara import of the project:  d2d6b628af1cab777a210e4ac62184e52fe9f4a9 by TJ Xu : Preserve backend config when folding transpose  6b5fa3a1cb70a790803e3ac57ff8329690e88e5e by TJ Xu : use SetupDerivedInstruction instead of just copying the backend config Merging this change closes CC(Feature Request: 5D rot90 (for voxel grid rotations)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21104 from Tixxx:tixxx/transpose_folding 6b5fa3a1cb70a790803e3ac57ff8329690e88e5e)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #21104: [NVIDIA GPU] Preserve backend config when folding transpose,PR CC(Feature Request: 5D rot90 (for voxel grid rotations)): [NVIDIA GPU] Preserve backend config when folding transpose Imported from GitHub PR https://github.com/openxla/xla/pull/21104 Transpose folding pass doesn't preserve backend config when creating the new dot with transpose folded. Changing the behavior to copy the old dot's config to the new dot. Copybara import of the project:  d2d6b628af1cab777a210e4ac62184e52fe9f4a9 by TJ Xu : Preserve backend config when folding transpose  6b5fa3a1cb70a790803e3ac57ff8329690e88e5e by TJ Xu : use SetupDerivedInstruction instead of just copying the backend config Merging this change closes CC(Feature Request: 5D rot90 (for voxel grid rotations)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21104 from Tixxx:tixxx/transpose_folding 6b5fa3a1cb70a790803e3ac57ff8329690e88e5e,2025-01-14T02:30:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84811
1175,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #21380: Add F4E2M1FN and F8E8M0FNU types)， 内容是 (PR CC(tf.GradientTape.gradient raise error with tf.nn.relu6): Add F4E2M1FN and F8E8M0FNU types Imported from GitHub PR https://github.com/openxla/xla/pull/21380 Previous PR https://github.com/openxla/xla/pull/19096 was rolled back, retrying. This PR adds F4E2M1FN primitive type (4bit float with 2 bits exponent and 1 bit mantissa), F8E8M0FNU primitive type (8bit float with 8 bits exponent, no mantissa and no sign) and enables loads/stores in the same way S4/U4 type is implemented. This will enable using microscaling (MX) formats (RFC), such as MXFP4.  Related PRs:  https://github.com/openxla/stablehlo/pull/2582  https://github.com/jaxml/ml_dtypes/pull/181  https://github.com/llvm/llvmproject/pull/95392  https://github.com/llvm/llvmproject/pull/108877  https://github.com/jaxml/ml_dtypes/pull/166  https://github.com/llvm/llvmproject/pull/107127  https://github.com/llvm/llvmproject/pull/111028 Copybara import of the project:  d7e00c49a)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #21380: Add F4E2M1FN and F8E8M0FNU types,"PR CC(tf.GradientTape.gradient raise error with tf.nn.relu6): Add F4E2M1FN and F8E8M0FNU types Imported from GitHub PR https://github.com/openxla/xla/pull/21380 Previous PR https://github.com/openxla/xla/pull/19096 was rolled back, retrying. This PR adds F4E2M1FN primitive type (4bit float with 2 bits exponent and 1 bit mantissa), F8E8M0FNU primitive type (8bit float with 8 bits exponent, no mantissa and no sign) and enables loads/stores in the same way S4/U4 type is implemented. This will enable using microscaling (MX) formats (RFC), such as MXFP4.  Related PRs:  https://github.com/openxla/stablehlo/pull/2582  https://github.com/jaxml/ml_dtypes/pull/181  https://github.com/llvm/llvmproject/pull/95392  https://github.com/llvm/llvmproject/pull/108877  https://github.com/jaxml/ml_dtypes/pull/166  https://github.com/llvm/llvmproject/pull/107127  https://github.com/llvm/llvmproject/pull/111028 Copybara import of the project:  d7e00c49a",2025-01-14T01:51:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84809
799,"以下是一个github上的tensorflow下的一个issue, 标题是(Create copy if the operands of gather/scatter instructions overlap.)， 内容是 (Create copy if the operands of gather/scatter instructions overlap. A gather has two operands, input and indices. If they point to the same instruction, create a copy for indices. A scatter has n inputs, 1 indices, and n updates (2n+1 operands in total). We allow overlap between n inputs. We also allow overlap between n updates. We need to create a copy if * indices overlap with any input or update * update overlap with any input The added copy will be removed if it is redundant in the following memory related passes (e.g., CopyInsertion).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Create copy if the operands of gather/scatter instructions overlap.,"Create copy if the operands of gather/scatter instructions overlap. A gather has two operands, input and indices. If they point to the same instruction, create a copy for indices. A scatter has n inputs, 1 indices, and n updates (2n+1 operands in total). We allow overlap between n inputs. We also allow overlap between n updates. We need to create a copy if * indices overlap with any input or update * update overlap with any input The added copy will be removed if it is redundant in the following memory related passes (e.g., CopyInsertion).",2025-01-13T23:26:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84805
319,"以下是一个github上的tensorflow下的一个issue, 标题是(Generate a build identifier for external tensor rearragement files)， 内容是 (Generate a build identifier for external tensor rearragement files)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Generate a build identifier for external tensor rearragement files,Generate a build identifier for external tensor rearragement files,2025-01-13T20:05:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84789
293,"以下是一个github上的tensorflow下的一个issue, 标题是(Regenerate pyi stubs with absl::Span imports included)， 内容是 (Regenerate pyi stubs with absl::Span imports included)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Regenerate pyi stubs with absl::Span imports included,Regenerate pyi stubs with absl::Span imports included,2025-01-13T18:24:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84782
302,"以下是一个github上的tensorflow下的一个issue, 标题是(Implement CHLO->StableHLO ragged_dot mode 1 decomposition.)， 内容是 (Implement CHLO>StableHLO ragged_dot mode 1 decomposition.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Implement CHLO->StableHLO ragged_dot mode 1 decomposition.,Implement CHLO>StableHLO ragged_dot mode 1 decomposition.,2025-01-13T17:34:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84776
611,"以下是一个github上的tensorflow下的一个issue, 标题是(Perform Set key operation first in the Exchange Topology to keep existing behavior unchanged.)， 内容是 (Perform Set key operation first in the Exchange Topology to keep existing behavior unchanged. Only when coordination_agent_recoverable is set, it tries to reconnect to the cluster and would lead to AlreadyExists error. In this case the already_existing error can be handled by checking the existing topology is same as the new one.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,copybara-service[bot],Perform Set key operation first in the Exchange Topology to keep existing behavior unchanged.,"Perform Set key operation first in the Exchange Topology to keep existing behavior unchanged. Only when coordination_agent_recoverable is set, it tries to reconnect to the cluster and would lead to AlreadyExists error. In this case the already_existing error can be handled by checking the existing topology is same as the new one.",2025-01-13T16:22:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84769
1120,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #20633: Improve the error message of the host out-of-memory)， 内容是 (PR CC(Delete 3_datasets.ipynb): Improve the error message of the host outofmemory Imported from GitHub PR https://github.com/openxla/xla/pull/20633 When working on weight offloading and activation offloading for MaxText Llama27B on a GH200, a host memory Out of Memory (OOM) error occurred as a large amount of memory was offloaded from the device to host memory. This CL clarifies that it was a host OOM, not a device OOM, and suggests using the environment variable XLA_PJRT_GPU_HOST_MEMORY_LIMIT_GB to increase the host memory limit. Copybara import of the project:  e38fac1d00c13185cbb96972814ddc45b0508cd8 by Jane Liu : Improve the error message of the host outofmemory. Merging this change closes CC(Delete 3_datasets.ipynb) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20633 from zhenyingliu:hostOOM e38fac1d00c13185cbb96972814ddc45b0508cd8)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #20633: Improve the error message of the host out-of-memory,"PR CC(Delete 3_datasets.ipynb): Improve the error message of the host outofmemory Imported from GitHub PR https://github.com/openxla/xla/pull/20633 When working on weight offloading and activation offloading for MaxText Llama27B on a GH200, a host memory Out of Memory (OOM) error occurred as a large amount of memory was offloaded from the device to host memory. This CL clarifies that it was a host OOM, not a device OOM, and suggests using the environment variable XLA_PJRT_GPU_HOST_MEMORY_LIMIT_GB to increase the host memory limit. Copybara import of the project:  e38fac1d00c13185cbb96972814ddc45b0508cd8 by Jane Liu : Improve the error message of the host outofmemory. Merging this change closes CC(Delete 3_datasets.ipynb) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20633 from zhenyingliu:hostOOM e38fac1d00c13185cbb96972814ddc45b0508cd8",2025-01-13T13:56:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84764
1120,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #20633: Improve the error message of the host out-of-memory)， 内容是 (PR CC(Delete 3_datasets.ipynb): Improve the error message of the host outofmemory Imported from GitHub PR https://github.com/openxla/xla/pull/20633 When working on weight offloading and activation offloading for MaxText Llama27B on a GH200, a host memory Out of Memory (OOM) error occurred as a large amount of memory was offloaded from the device to host memory. This CL clarifies that it was a host OOM, not a device OOM, and suggests using the environment variable XLA_PJRT_GPU_HOST_MEMORY_LIMIT_GB to increase the host memory limit. Copybara import of the project:  e38fac1d00c13185cbb96972814ddc45b0508cd8 by Jane Liu : Improve the error message of the host outofmemory. Merging this change closes CC(Delete 3_datasets.ipynb) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20633 from zhenyingliu:hostOOM e38fac1d00c13185cbb96972814ddc45b0508cd8)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #20633: Improve the error message of the host out-of-memory,"PR CC(Delete 3_datasets.ipynb): Improve the error message of the host outofmemory Imported from GitHub PR https://github.com/openxla/xla/pull/20633 When working on weight offloading and activation offloading for MaxText Llama27B on a GH200, a host memory Out of Memory (OOM) error occurred as a large amount of memory was offloaded from the device to host memory. This CL clarifies that it was a host OOM, not a device OOM, and suggests using the environment variable XLA_PJRT_GPU_HOST_MEMORY_LIMIT_GB to increase the host memory limit. Copybara import of the project:  e38fac1d00c13185cbb96972814ddc45b0508cd8 by Jane Liu : Improve the error message of the host outofmemory. Merging this change closes CC(Delete 3_datasets.ipynb) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20633 from zhenyingliu:hostOOM e38fac1d00c13185cbb96972814ddc45b0508cd8",2025-01-13T10:53:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84751
1208,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #19451: Setting xla_gpu_multi_streamed_windowed_einsum to true by default)， 内容是 (PR CC(Feature Request: evaluate both train_data and test_data using tf.contrib.learn.Experiment?): Setting xla_gpu_multi_streamed_windowed_einsum to true by default Imported from GitHub PR https://github.com/openxla/xla/pull/19451 We are trying to deprecate xla_gpu_multi_streamed_windowed_einsum  since we always have better perf with it enabled. This is the first pr to enable it by default to test for stability. Copybara import of the project:  808a9cc0af8901d36a3c219bdf19f38323d01bf3 by Tj Xu : Turn xla_gpu_multi_streamed_windowed_einsum on by default  8221fc4481773f457f5e0235625be22f255fe75b by TJ Xu : Add an option to StreamAttributeAnnotator to skip annotating copystart and async DUS Don't annotate copystart and async DUS when the pass is run before remat  352c1c593b9dcd895f123dea4f7c38e44a787ae6 by TJ Xu : Remove the option to skip annotating copy start and inpect if the module has schedule  257ff6768b59fc7c47c04fa5faa524399f7)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #19451: Setting xla_gpu_multi_streamed_windowed_einsum to true by default,PR CC(Feature Request: evaluate both train_data and test_data using tf.contrib.learn.Experiment?): Setting xla_gpu_multi_streamed_windowed_einsum to true by default Imported from GitHub PR https://github.com/openxla/xla/pull/19451 We are trying to deprecate xla_gpu_multi_streamed_windowed_einsum  since we always have better perf with it enabled. This is the first pr to enable it by default to test for stability. Copybara import of the project:  808a9cc0af8901d36a3c219bdf19f38323d01bf3 by Tj Xu : Turn xla_gpu_multi_streamed_windowed_einsum on by default  8221fc4481773f457f5e0235625be22f255fe75b by TJ Xu : Add an option to StreamAttributeAnnotator to skip annotating copystart and async DUS Don't annotate copystart and async DUS when the pass is run before remat  352c1c593b9dcd895f123dea4f7c38e44a787ae6 by TJ Xu : Remove the option to skip annotating copy start and inpect if the module has schedule  257ff6768b59fc7c47c04fa5faa524399f7,2025-01-13T09:13:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84738
825,"以下是一个github上的tensorflow下的一个issue, 标题是(enhancement: add sonargit pr metrics)， 内容是 (This PR introduces a GitHub workflow that leverages the SonarGit Action to collect pull request metrics such as open times, merge rates, and change failure rates. These metrics provide actionable insights to help improve the repository's development workflow. Currently, the workflow logs PR information to the console. Optionally, SonarGit offers a dashboard to visualize the data for deeper analysis—and it’s completely free for life! I’m doing this to help the developer community and to get my name out there. If you’d like more information or assistance in setting this up, feel free to reach out.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,axlrommel,enhancement: add sonargit pr metrics,"This PR introduces a GitHub workflow that leverages the SonarGit Action to collect pull request metrics such as open times, merge rates, and change failure rates. These metrics provide actionable insights to help improve the repository's development workflow. Currently, the workflow logs PR information to the console. Optionally, SonarGit offers a dashboard to visualize the data for deeper analysis—and it’s completely free for life! I’m doing this to help the developer community and to get my name out there. If you’d like more information or assistance in setting this up, feel free to reach out.",2025-01-12T22:41:52Z,size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84696
668,"以下是一个github上的tensorflow下的一个issue, 标题是(tensorflow)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.8  Custom code Yes  OS platform and distribution windows  Mobile device windows  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? unable to load tensorflow as tf  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cyprian-igban,tensorflow, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.8  Custom code Yes  OS platform and distribution windows  Mobile device windows  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? unable to load tensorflow as tf  Standalone code to reproduce the issue   Relevant log output ,2025-01-12T10:37:11Z,type:build/install,closed,0,3,https://github.com/tensorflow/tensorflow/issues/84692,hey igban check out CC(ImportError: DLL load failed while importing _pywrap_tensorflow_internal:),Duplicate of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.),Are you satisfied with the resolution of your issue? Yes No
836,"以下是一个github上的tensorflow下的一个issue, 标题是(Makes keyword arguments of functions loaded from TF1 SavedModels be treated as `POSITIONAL_OR_KEYWROD` (instead of `KEYWORD_ONLY`) arguments by `FunctionType`, so that `FunctionType` won't mistakenly change their order (which can lead to an order mismatch with the underlying TF Graph when calling or re-saving the function).)， 内容是 (Makes keyword arguments of functions loaded from TF1 SavedModels be treated as `POSITIONAL_OR_KEYWROD` (instead of `KEYWORD_ONLY`) arguments by `FunctionType`, so that `FunctionType` won't mistakenly change their order (which can lead to an order mismatch with the underlying TF Graph when calling or resaving the function).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],"Makes keyword arguments of functions loaded from TF1 SavedModels be treated as `POSITIONAL_OR_KEYWROD` (instead of `KEYWORD_ONLY`) arguments by `FunctionType`, so that `FunctionType` won't mistakenly change their order (which can lead to an order mismatch with the underlying TF Graph when calling or re-saving the function).","Makes keyword arguments of functions loaded from TF1 SavedModels be treated as `POSITIONAL_OR_KEYWROD` (instead of `KEYWORD_ONLY`) arguments by `FunctionType`, so that `FunctionType` won't mistakenly change their order (which can lead to an order mismatch with the underlying TF Graph when calling or resaving the function).",2025-01-11T20:44:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84685
247,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal relative changes only)， 内容是 (Internal relative changes only)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal relative changes only,Internal relative changes only,2025-01-11T04:54:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84649
456,"以下是一个github上的tensorflow下的一个issue, 标题是(internal change only to update dependency visibility)， 内容是 (internal change only to update dependency visibility FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19066 from Inteltensorflow:mabuzain/handleonednnscalar 576e244530ce0698de0b7137d8e93965fef9d528)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],internal change only to update dependency visibility,internal change only to update dependency visibility FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19066 from Inteltensorflow:mabuzain/handleonednnscalar 576e244530ce0698de0b7137d8e93965fef9d528,2025-01-10T22:45:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84613
475,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:Python] Make sure we hold the lock on cache_ when destroying executables_ in PjitFunction.)， 内容是 ([XLA:Python] Make sure we hold the lock on cache_ when destroying executables_ in PjitFunction. cache_'s object lock protects executables_ under freethreading mode, so we have to hold the lock.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA:Python] Make sure we hold the lock on cache_ when destroying executables_ in PjitFunction.,"[XLA:Python] Make sure we hold the lock on cache_ when destroying executables_ in PjitFunction. cache_'s object lock protects executables_ under freethreading mode, so we have to hold the lock.",2025-01-10T22:14:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84611
376,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:GPU][Emitters] Allow unrolling loops that yield values defined above.)， 内容是 ([XLA:GPU][Emitters] Allow unrolling loops that yield values defined above. The change upstream has been integrated.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA:GPU][Emitters] Allow unrolling loops that yield values defined above.,[XLA:GPU][Emitters] Allow unrolling loops that yield values defined above. The change upstream has been integrated.,2025-01-10T17:32:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84599
629,"以下是一个github上的tensorflow下的一个issue, 标题是(#sdy fix bug due to tensor dialect being introduced)， 内容是 (sdy fix bug due to tensor dialect being introduced When investigating a bug, I discovered this fails in JAX:  with the error  This was because the sdyroundtripimport introduces the tensor dialect. I'm unsure which pass adds it, but overall what I see is it is actually undone. The details shouldn't matter as long as the pass doesn't crash and the dialect doesn't show up during propagation.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],#sdy fix bug due to tensor dialect being introduced,"sdy fix bug due to tensor dialect being introduced When investigating a bug, I discovered this fails in JAX:  with the error  This was because the sdyroundtripimport introduces the tensor dialect. I'm unsure which pass adds it, but overall what I see is it is actually undone. The details shouldn't matter as long as the pass doesn't crash and the dialect doesn't show up during propagation.",2025-01-10T15:51:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84593
1200,"以下是一个github上的tensorflow下的一个issue, 标题是(gen_quantized_function_library: clang-cl compilation file path error)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19 nightly  Custom code Yes  OS platform and distribution Windows 11 24H2  Mobile device _No response_  Python version Anaconda 2024.101  Bazel version 6.5.0  GCC/compiler version Visual Studio 2022 (build tools 14.42) + LLVM 19.1.6 + msys2x86_6420241208  CUDA/cuDNN version CUDA 12.6.3 + CUDNN 9.6.0  GPU model and memory GTX 1050 Ti 4GB  Current behavior? `gen_quantized_function_library` is trying to read `'C:\\msys64\\home\\*\\_bazel_*\\*\\execroot\\org_tensorflow\\bazelout\\x64_windowsoptexec*\\bin\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.exe.runfiles\\org_tensorflow\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.py'` on Windows. However, `os.path.exists()` cannot resolve `\\` symbol. Correct path is just like: `'C:/msys64/home/)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,johnnkp,gen_quantized_function_library: clang-cl compilation file path error," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19 nightly  Custom code Yes  OS platform and distribution Windows 11 24H2  Mobile device _No response_  Python version Anaconda 2024.101  Bazel version 6.5.0  GCC/compiler version Visual Studio 2022 (build tools 14.42) + LLVM 19.1.6 + msys2x86_6420241208  CUDA/cuDNN version CUDA 12.6.3 + CUDNN 9.6.0  GPU model and memory GTX 1050 Ti 4GB  Current behavior? `gen_quantized_function_library` is trying to read `'C:\\msys64\\home\\*\\_bazel_*\\*\\execroot\\org_tensorflow\\bazelout\\x64_windowsoptexec*\\bin\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.exe.runfiles\\org_tensorflow\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.py'` on Windows. However, `os.path.exists()` cannot resolve `\\` symbol. Correct path is just like: `'C:/msys64/home/",2025-01-10T07:01:31Z,stat:awaiting response type:build/install subtype:windows TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84558,"I found out `rules_python` provide the python file template, but modify `python/private/python_bootstrap_template.txt` will not affect the generated `.py`.","Hi **** , Apologies for the delay, and thank you for raising your issue here. The main cause appears to be related to your file path. On Windows, path handling is different, and the error suggests that os.path.exists() cannot correctly resolve the path due to the use of backslashes (`\`). These are standard in Windows paths but need to be properly managed in Python. To resolve this, configure your setup to use forward slashes (`/`) instead of backslashes (`\`) for Windows paths. Python, especially when running in environments like MSYS2 or Git Bash, often handles forward slashes more consistently. If you have already tried this, please rebuild the Bazel target that generates the `.py` file. If the issue persists, let us know so we can further assist you. Thank you!","Although I already found out a fix for this issue, I am not going to create a pull request because my compilation failed with nvcc error in windows. And my fix seems unnecessary for other builds.",Are you satisfied with the resolution of your issue? Yes No
227,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal change only)， 内容是 (Internal change only)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal change only,Internal change only,2025-01-10T00:59:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84525
939,"以下是一个github上的tensorflow下的一个issue, 标题是(Memory Allocation Issues)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.1 LTS on WSL2  Mobile device _No response_  Python version 3.12.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 90300  GPU model and memory RTX 4070 12GB  Current behavior? I create a virtual gpu with a hard limit of 10GB. I start training the network and it works for bit but then says out of memory and tries to allocate more than the set limit. What I expect to happen is that is stays within the 10GB limit and can train the network successfully.  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Catakang,Memory Allocation Issues, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.1 LTS on WSL2  Mobile device _No response_  Python version 3.12.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 90300  GPU model and memory RTX 4070 12GB  Current behavior? I create a virtual gpu with a hard limit of 10GB. I start training the network and it works for bit but then says out of memory and tries to allocate more than the set limit. What I expect to happen is that is stays within the 10GB limit and can train the network successfully.  Standalone code to reproduce the issue   Relevant log output ,2025-01-10T00:49:17Z,stat:awaiting response type:bug stale TF 2.18,closed,1,7,https://github.com/tensorflow/tensorflow/issues/84523,"I am facing exactly the same issue for a while now on two slightly different systems:  It even shows the warnings when setting `os.environ['TF_CPP_MIN_LOG_LEVEL'] = ""3""`. In my case, my code runs through despite these error messages, but on the one hand they are annoying and also a bit worrying and on the other hand I have the feeling that they affect the execution time, because I often observe strange behaviour regarding the execution times.","It does affect execution time and my code does not run to completion with the errors as I let it run for an hour to see what happened and it finished with a message saying '0 successful operations'. I don't know why I am running into this error as I am just trying to follow a tutorial on the keras website. I may not have a multi GPU setup to train a bunch of networks in an optimized fashion but surely 10gb on my 4070 should be plenty to run 2 epochs with a batch size of 10. This is ridiculous and from what I am reading from other GitHub issues, this has been an issue for years on certain systems that they have simply not fixed(if I understand everything right) and I really just want to figure out tensorflow for my science fair project.","I will mention I have tried growing the memory, I tried the malloc cuda async flag, I tried slowly reducing the batch size smaller and smaller, this really shouldn't be that complicated.","Hi **** , Apologies for the delay, and thank you for raising your concern here. I attempted to run your code on Colab using the TensorFlow nightly version but encountered a different issue. I have attached a gist for your review—could you please check and let me know if I made any mistakes while executing your code? Additionally, in your setup, consider disabling XLA to potentially reduce memory usage. Let us know if you are still encountering the same issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
624,"以下是一个github上的tensorflow下的一个issue, 标题是(This is the first step in unifying the various StreamExecutor::Allocate and ::Deallocate methods.  (e.g. HostMemoryAllocate & HostMemoryDeallocate))， 内容是 (This is the first step in unifying the various StreamExecutor::Allocate and ::Deallocate methods.  (e.g. HostMemoryAllocate & HostMemoryDeallocate) Future CLs will make use of these new classes to eliminate the need for adding bespoke Allocate/Deallocate pairs as new MemoryTypes are added.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],This is the first step in unifying the various StreamExecutor::Allocate and ::Deallocate methods.  (e.g. HostMemoryAllocate & HostMemoryDeallocate),This is the first step in unifying the various StreamExecutor::Allocate and ::Deallocate methods.  (e.g. HostMemoryAllocate & HostMemoryDeallocate) Future CLs will make use of these new classes to eliminate the need for adding bespoke Allocate/Deallocate pairs as new MemoryTypes are added.,2025-01-09T22:42:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84508
305,"以下是一个github上的tensorflow下的一个issue, 标题是([xla:cpu:benchmarks] Add scripts to run Gemma2 Keras model.)， 内容是 ([xla:cpu:benchmarks] Add scripts to run Gemma2 Keras model.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,copybara-service[bot],[xla:cpu:benchmarks] Add scripts to run Gemma2 Keras model.,[xla:cpu:benchmarks] Add scripts to run Gemma2 Keras model.,2025-01-09T10:29:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84465
1124,"以下是一个github上的tensorflow下的一个issue, 标题是(GPU Profiling: MemoryProfile do not contain memory events when profile remote worker.)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version nightly  Custom code No  OS platform and distribution Ubuntu 22.04  Python version Python 3.12  CUDA/cuDNN version CUDA 12.4  GPU model and memory A100 80GB  Current behavior? Start a simple any collective training with Tensorflow cluster config. And then use RPC client capture_profile in Tensorboard or tf.profiler.experimental.client.trace.  No any memory profile events or OP profiler, but only trace view.  Standalone code to reproduce the issue **tf_allreduce.py**  Run script to start server.   use capture_profile in Tensorboard or tf.profiler.experimental.client.trace.  Try to convert xplane.pb to memory_profile, nothing show.  **Relevant log output**  Relative issue: CC(GPU Profiling: MemoryProfile do not contain memory events.) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,MoFHeka,GPU Profiling: MemoryProfile do not contain memory events when profile remote worker.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version nightly  Custom code No  OS platform and distribution Ubuntu 22.04  Python version Python 3.12  CUDA/cuDNN version CUDA 12.4  GPU model and memory A100 80GB  Current behavior? Start a simple any collective training with Tensorflow cluster config. And then use RPC client capture_profile in Tensorboard or tf.profiler.experimental.client.trace.  No any memory profile events or OP profiler, but only trace view.  Standalone code to reproduce the issue **tf_allreduce.py**  Run script to start server.   use capture_profile in Tensorboard or tf.profiler.experimental.client.trace.  Try to convert xplane.pb to memory_profile, nothing show.  **Relevant log output**  Relative issue: CC(GPU Profiling: MemoryProfile do not contain memory events.) ",2025-01-09T09:26:20Z,stat:awaiting tensorflower type:bug comp:core TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/84460
227,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal change only)， 内容是 (Internal change only)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal change only,Internal change only,2025-01-09T03:02:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84428
327,"以下是一个github上的tensorflow下的一个issue, 标题是(Internal change. Need to write more words to keep the presubmit happy.)， 内容是 (Internal change. Need to write more words to keep the presubmit happy.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Internal change. Need to write more words to keep the presubmit happy.,Internal change. Need to write more words to keep the presubmit happy.,2025-01-09T01:11:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84423
998,"以下是一个github上的tensorflow下的一个issue, 标题是(Add `ClientLibraryTestRunnerMixin`.)， 内容是 (Add `ClientLibraryTestRunnerMixin`. `ClientLibraryTestRunnerMixin` is a sortof replacement for `ClientLibraryTestBase` to run tests on top of `HloTestBase` and friends (e.g. `HloRunnerAgnosticTestBase`).  This is to enable a future migration to PjRt and TFRT. Due to `ClientLibraryTestBase` containing many clientspecific calls, moving tests is not as trivial as simply dropping in a new base class. The idea with this class is just to make that migration simpler and to reduce (but not eliminate) the amount of code changes required in tests. Migration timeline for `ClientLibraryTestBase` tests: 1. `class XYZ: ClientLibraryTestBase` (starting point) 2. `class XYZ: ClientLibraryTestRunnerMixin` (intermediate state) 3. `class XYZ: ClientLibraryTestRunnerMixin>` (end state))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Add `ClientLibraryTestRunnerMixin`.,"Add `ClientLibraryTestRunnerMixin`. `ClientLibraryTestRunnerMixin` is a sortof replacement for `ClientLibraryTestBase` to run tests on top of `HloTestBase` and friends (e.g. `HloRunnerAgnosticTestBase`).  This is to enable a future migration to PjRt and TFRT. Due to `ClientLibraryTestBase` containing many clientspecific calls, moving tests is not as trivial as simply dropping in a new base class. The idea with this class is just to make that migration simpler and to reduce (but not eliminate) the amount of code changes required in tests. Migration timeline for `ClientLibraryTestBase` tests: 1. `class XYZ: ClientLibraryTestBase` (starting point) 2. `class XYZ: ClientLibraryTestRunnerMixin` (intermediate state) 3. `class XYZ: ClientLibraryTestRunnerMixin>` (end state)",2025-01-09T00:32:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84414
1085,"以下是一个github上的tensorflow下的一个issue, 标题是(Unable to connect to TPU through Cloud VM (metadata issue?))， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version v2.18.0rc24g6550e4bd802 2.18.0  Custom code Yes  OS platform and distribution tpuubuntu2204base  Mobile device _No response_  Python version 3.11.2  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am on a VM instance trying to connect to a tpu v432 using a test script. I installed tensorflowtpu on both the VM (in venv) and TPU (globally) as per the instructions from the google website. It seems like there is an issue with getting TPU metadata. It is able to connect to the metadata server when I request manually from the VM:  Any help would be appreciated!  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,nathom,Unable to connect to TPU through Cloud VM (metadata issue?), Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version v2.18.0rc24g6550e4bd802 2.18.0  Custom code Yes  OS platform and distribution tpuubuntu2204base  Mobile device _No response_  Python version 3.11.2  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am on a VM instance trying to connect to a tpu v432 using a test script. I installed tensorflowtpu on both the VM (in venv) and TPU (globally) as per the instructions from the google website. It seems like there is an issue with getting TPU metadata. It is able to connect to the metadata server when I request manually from the VM:  Any help would be appreciated!  Standalone code to reproduce the issue   Relevant log output ,2025-01-09T00:04:51Z,stat:awaiting response type:bug comp:tpus TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84413,", Could you please provide more information and also steps you have followed to use the TPU's which helps to debug the issue in an effective way. Thank you!","I think this was my mistake. I was using a Google Cloud VM and trying to connect to the TPU pods from there. I was able to resolve the issue by connecting directly to one of the TPU hosts, and running commands on all workers using `gcloud`. Maybe the error messages could be made more helpful, though.",", Glad the issue was resolved by connecting the TPU hosts. Could you please feel free to move this issue to closed status. Thank you!",Are you satisfied with the resolution of your issue? Yes No
1135,"以下是一个github上的tensorflow下的一个issue, 标题是(bug)， 内容是 ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,rizkyy702,bug," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN",2025-01-09T00:02:56Z,TFLiteConverter,closed,0,1,https://github.com/tensorflow/tensorflow/issues/84412,Nothing provided in the template.
717,"以下是一个github上的tensorflow下的一个issue, 标题是(Split `RunAndCompare` with reference backend functionality into a mixin.)， 内容是 (Split `RunAndCompare` with reference backend functionality into a mixin. Many users don't require `RunAndCompare` functionality, but are forced to select and initialize a reference backend anyway. With this change, users can opt to extend their specific `HloRunnerAgnosticTestBase` implementation to add `RunAndCompare` functionality. The mixin acts as a wrapper around any `HloRunnerAgnosticTestBase` implementation, allowing a high degree of customization.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Split `RunAndCompare` with reference backend functionality into a mixin.,"Split `RunAndCompare` with reference backend functionality into a mixin. Many users don't require `RunAndCompare` functionality, but are forced to select and initialize a reference backend anyway. With this change, users can opt to extend their specific `HloRunnerAgnosticTestBase` implementation to add `RunAndCompare` functionality. The mixin acts as a wrapper around any `HloRunnerAgnosticTestBase` implementation, allowing a high degree of customization.",2025-01-08T17:34:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84390
649,"以下是一个github上的tensorflow下的一个issue, 标题是(Remove `RunAndCompare` functionality from `HloRunnerAgnosticTestBase`.)， 内容是 (Remove `RunAndCompare` functionality from `HloRunnerAgnosticTestBase`. This functionality is now fully contained in `HloRunnerAgnosticReferenceMixin` and therefore is no longer needed in `HloRunnerAgnosticTestBase`. This change temporarily adds the mixin to `HloPjRtTestBase`. Next, we'll go through all tests that extend these base classes and will move the uses of the mixins to the leaves.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Remove `RunAndCompare` functionality from `HloRunnerAgnosticTestBase`.,"Remove `RunAndCompare` functionality from `HloRunnerAgnosticTestBase`. This functionality is now fully contained in `HloRunnerAgnosticReferenceMixin` and therefore is no longer needed in `HloRunnerAgnosticTestBase`. This change temporarily adds the mixin to `HloPjRtTestBase`. Next, we'll go through all tests that extend these base classes and will move the uses of the mixins to the leaves.",2025-01-08T02:01:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84321
485,"以下是一个github上的tensorflow下的一个issue, 标题是(Add `HloPjRtInterpreterReferenceMixin` wrapper around `HloRunnerAgnosticReferenceMixin`.)， 内容是 (Add `HloPjRtInterpreterReferenceMixin` wrapper around `HloRunnerAgnosticReferenceMixin`. This mixin provides a default way to run comparison tests against an interpreter reference via the PjRtbased interpreter.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Add `HloPjRtInterpreterReferenceMixin` wrapper around `HloRunnerAgnosticReferenceMixin`.,Add `HloPjRtInterpreterReferenceMixin` wrapper around `HloRunnerAgnosticReferenceMixin`. This mixin provides a default way to run comparison tests against an interpreter reference via the PjRtbased interpreter.,2025-01-08T01:58:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84320
717,"以下是一个github上的tensorflow下的一个issue, 标题是(Split `RunAndCompare` with reference backend functionality into a mixin.)， 内容是 (Split `RunAndCompare` with reference backend functionality into a mixin. Many users don't require `RunAndCompare` functionality, but are forced to select and initialize a reference backend anyway. With this change, users can opt to extend their specific `HloRunnerAgnosticTestBase` implementation to add `RunAndCompare` functionality. The mixin acts as a wrapper around any `HloRunnerAgnosticTestBase` implementation, allowing a high degree of customization.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Split `RunAndCompare` with reference backend functionality into a mixin.,"Split `RunAndCompare` with reference backend functionality into a mixin. Many users don't require `RunAndCompare` functionality, but are forced to select and initialize a reference backend anyway. With this change, users can opt to extend their specific `HloRunnerAgnosticTestBase` implementation to add `RunAndCompare` functionality. The mixin acts as a wrapper around any `HloRunnerAgnosticTestBase` implementation, allowing a high degree of customization.",2025-01-08T01:56:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84318
1176,"以下是一个github上的tensorflow下的一个issue, 标题是([XLA:Python] Fix three concurrency problems.)， 内容是 ([XLA:Python] Fix three concurrency problems. These problems can be reproduced even with the GIL enabled, they are not noGIL bugs. In pmap_lib.cc, defend against a use after free in the following scenario: * thread A misses in the compilation cache and calls `cache_miss()` to populate the cache, relying on the new entry in executables_ remaining alive. * thread B calls `cache_clear()`, which erases the contents of `executables_` Use a std::shared_ptr to keep the entry alive. In pjit.cc, refactor PjitFunctionStore to use a doublylinked list of PjitFunctionObject entries. When consuming the list of functions in the store, take strong references to them. This prevents a useafterfree if the cache is cleared concurrently multiple times. In pjit.cc, do not add functions to the PjitFunctionStore until executables_ is populated. This avoids a null pointer dereference from a concurrent call to `cache_clear`. Problems found with some upcoming)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[XLA:Python] Fix three concurrency problems.,"[XLA:Python] Fix three concurrency problems. These problems can be reproduced even with the GIL enabled, they are not noGIL bugs. In pmap_lib.cc, defend against a use after free in the following scenario: * thread A misses in the compilation cache and calls `cache_miss()` to populate the cache, relying on the new entry in executables_ remaining alive. * thread B calls `cache_clear()`, which erases the contents of `executables_` Use a std::shared_ptr to keep the entry alive. In pjit.cc, refactor PjitFunctionStore to use a doublylinked list of PjitFunctionObject entries. When consuming the list of functions in the store, take strong references to them. This prevents a useafterfree if the cache is cleared concurrently multiple times. In pjit.cc, do not add functions to the PjitFunctionStore until executables_ is populated. This avoids a null pointer dereference from a concurrent call to `cache_clear`. Problems found with some upcoming",2025-01-07T21:07:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84296
1199,"以下是一个github上的tensorflow下的一个issue, 标题是(Fold `xla::PjRtXlaLayout` into `xla::PjRtLayout` for simplification)， 内容是 (Fold `xla::PjRtXlaLayout` into `xla::PjRtLayout` for simplification `xla::PjRtLayout` was designed as an abstract class so that it leaves options to represent layouts without depending on `xla::Layout`. In reality, `xla::PjRtXlaLayout` is the only concrete layout representation that will exist in the foreseeable future, and the lack of a proper typeerased layout creation interface forces everyone to use unsafe downcast to access the underlying layout. This causes an unnecessary code bloat without much extensibility because too many downcasts practically prevent new layout representations from being easily introduced. This CL folds `xla::PjRtXlaLayout` into `xla::PjRtLayout` and make `xla::PjRtLayout` a nonabstract class. Like `xla::Shape` that is used pervasively in PjRt, this CL makes layouts a concrete type based on `xla::Layout`. The benefit is that it simplifies many callers that use PjRt layouts: `xla::GetXlaLayoutUnsafe()` is)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Fold `xla::PjRtXlaLayout` into `xla::PjRtLayout` for simplification,"Fold `xla::PjRtXlaLayout` into `xla::PjRtLayout` for simplification `xla::PjRtLayout` was designed as an abstract class so that it leaves options to represent layouts without depending on `xla::Layout`. In reality, `xla::PjRtXlaLayout` is the only concrete layout representation that will exist in the foreseeable future, and the lack of a proper typeerased layout creation interface forces everyone to use unsafe downcast to access the underlying layout. This causes an unnecessary code bloat without much extensibility because too many downcasts practically prevent new layout representations from being easily introduced. This CL folds `xla::PjRtXlaLayout` into `xla::PjRtLayout` and make `xla::PjRtLayout` a nonabstract class. Like `xla::Shape` that is used pervasively in PjRt, this CL makes layouts a concrete type based on `xla::Layout`. The benefit is that it simplifies many callers that use PjRt layouts: `xla::GetXlaLayoutUnsafe()` is",2025-01-07T03:53:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84242
730,"以下是一个github上的tensorflow下的一个issue, 标题是(PR #21037: Typo Fix)， 内容是 (PR CC(tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory): Typo Fix Imported from GitHub PR https://github.com/openxla/xla/pull/21037 Copybara import of the project:  588990f2fee70a9237faeff6e1ed17161c770163 by flyingcat : Typo Fix Merging this change closes CC(tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21037 from knightXun:knightXunpatch1 588990f2fee70a9237faeff6e1ed17161c770163)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],PR #21037: Typo Fix,PR CC(tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory): Typo Fix Imported from GitHub PR https://github.com/openxla/xla/pull/21037 Copybara import of the project:  588990f2fee70a9237faeff6e1ed17161c770163 by flyingcat : Typo Fix Merging this change closes CC(tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21037 from knightXun:knightXunpatch1 588990f2fee70a9237faeff6e1ed17161c770163,2025-01-07T01:28:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84236
469,"以下是一个github上的tensorflow下的一个issue, 标题是(Fix `HloRunnerAgnosticTestBase` includes.)， 内容是 (Fix `HloRunnerAgnosticTestBase` includes. Many of the tests that extend `HloTestBase` rely on symbols included transitively.  The main ones are:  `PlatformUtil`  `LiteralUtil`  `LiteralTestUtil` This patch adds includes for these explicitly.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Fix `HloRunnerAgnosticTestBase` includes.,Fix `HloRunnerAgnosticTestBase` includes. Many of the tests that extend `HloTestBase` rely on symbols included transitively.  The main ones are:  `PlatformUtil`  `LiteralUtil`  `LiteralTestUtil` This patch adds includes for these explicitly.,2025-01-06T22:34:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84225
1181,"以下是一个github上的tensorflow下的一个issue, 标题是(Encountered unresolved custom op: XlaDynamicSlice)， 内容是 (Hi, i am doing a task in converting T5 model to TFLite for Android. Currently, i am using T5ModelForConditionalGeneration from Huggingface to convert. The conversion is done with some below logging but when load the `generator` from Interpretor, and run an inference example, i have faced this error. You can reproduce with the colab provided below. AFAIK, this XlaDynamicSlice is in TF ops but why this op cannot be resolved in this cases.  **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04  TensorFlow installed from (source or binary): PyPI 24.0 on Python 3.11.5  TensorFlow version (or github SHA if from source): 2.18.0 **Provide the text output from tflite_convert** In colab version, tflite_convert doesn't log anything, below log is in my local version  **Standalone code to reproduce the issue**  Provide a reproducible test case that is the bare minimum necessary to generate the probl)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",text generation,phandat128,Encountered unresolved custom op: XlaDynamicSlice,"Hi, i am doing a task in converting T5 model to TFLite for Android. Currently, i am using T5ModelForConditionalGeneration from Huggingface to convert. The conversion is done with some below logging but when load the `generator` from Interpretor, and run an inference example, i have faced this error. You can reproduce with the colab provided below. AFAIK, this XlaDynamicSlice is in TF ops but why this op cannot be resolved in this cases.  **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04  TensorFlow installed from (source or binary): PyPI 24.0 on Python 3.11.5  TensorFlow version (or github SHA if from source): 2.18.0 **Provide the text output from tflite_convert** In colab version, tflite_convert doesn't log anything, below log is in my local version  **Standalone code to reproduce the issue**  Provide a reproducible test case that is the bare minimum necessary to generate the probl",2025-01-06T10:46:54Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/84203,"Hi,   I apologize for the delayed response, I tried to replicate the same behavior from my end with your Google colab notebook and I'm also getting the same error message `RuntimeError: Encountered unresolved custom op: XlaDynamicSlice.` for reference here is gistfile so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention Thank you for your cooperation and patience.","Hi,   I apologize for the delayed response, I see in provided output log it says : `The following operation(s) need TFLite custom op implementation(s):Custom ops: XlaDynamicSlice` so it's unsupported ops since the LiteRT( Formerly knowns as TFLite) builtin operator library only supports a limited number of TensorFlow operators, not every model is convertible. For details, refer to operator compatibility. To allow conversion, you'll have to provide their own custom implementation of an unsupported TensorFlow operator in LiteRT, known as a custom operator in your case `XlaDynamicSlice` Op for more details please refer this official documentation If it's not mandatory to use T5 model in your use case or project then you can give it try with other models which can fulfill your usecase/project need the alternatives to the T5 model, some prominent options include: GPT3 (and its variants like GPT3.5 and GPT4), BERT, RoBERTa, XLNet, FlanT5 (an enhanced version of T5) Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
349,"以下是一个github上的tensorflow下的一个issue, 标题是(Improve signature runner test coverage by adding some tests of out-of-range cases.)， 内容是 (Improve signature runner test coverage by adding some tests of outofrange cases.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,copybara-service[bot],Improve signature runner test coverage by adding some tests of out-of-range cases.,Improve signature runner test coverage by adding some tests of outofrange cases.,2025-01-05T23:18:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84172
559,"以下是一个github上的tensorflow下的一个issue, 标题是([stream_executor] Always return non-const pointer to device memory from DeviceMemory/DeviceMemoryBase)， 内容是 ([stream_executor] Always return nonconst pointer to device memory from DeviceMemory/DeviceMemoryBase Constness of DeviceMemoryBase does not imply constness of underlying device memory (similar to how constness of absl::Span is not related to constness of underlying data))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],[stream_executor] Always return non-const pointer to device memory from DeviceMemory/DeviceMemoryBase,[stream_executor] Always return nonconst pointer to device memory from DeviceMemory/DeviceMemoryBase Constness of DeviceMemoryBase does not imply constness of underlying device memory (similar to how constness of absl::Span is not related to constness of underlying data),2025-01-04T04:22:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84132
1260,"以下是一个github上的tensorflow下的一个issue, 标题是(Change IFRT and PjRt layout API to return `std::shared_ptr<const xla::PjRtLayout>` instead of `std::unique_ptr<xla::PjRtLayout>`)， 内容是 (Change IFRT and PjRt layout API to return `std::shared_ptr` instead of `std::unique_ptr` The current API design that uses `std::unique_ptr` has several issues: * The API requires `xla::PjRtLayout` to be copied in some scenarios, e.g., `xla::ifrt::Array` internally stores a layout and returns its copy every time `layout()` is called. This forces implementations to break the abstraction boundary because `xla::PjRtLayout` is an abstract class and `std::unique_ptr` is not copyable. The current implementation either stores `xla::Layout` and creates `xla::PjRtLayout` every time, or downcasts `xla::PjRtLayout` to `xla::PjRtXlaLayout` to perform the copy. * `xla::Layout` is expensive to copy (`sizeof(xla::Layout)` is 248 bytes as of 20250103) and copying `xla::PjRtXlaLayout` requires copying or moving `xla::Layout`. To address these two problems, this CL changes PjRt and IFRT APIs that return `xla::PjRtLayout` to instead use `std::shared_p)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,copybara-service[bot],Change IFRT and PjRt layout API to return `std::shared_ptr<const xla::PjRtLayout>` instead of `std::unique_ptr<xla::PjRtLayout>`,"Change IFRT and PjRt layout API to return `std::shared_ptr` instead of `std::unique_ptr` The current API design that uses `std::unique_ptr` has several issues: * The API requires `xla::PjRtLayout` to be copied in some scenarios, e.g., `xla::ifrt::Array` internally stores a layout and returns its copy every time `layout()` is called. This forces implementations to break the abstraction boundary because `xla::PjRtLayout` is an abstract class and `std::unique_ptr` is not copyable. The current implementation either stores `xla::Layout` and creates `xla::PjRtLayout` every time, or downcasts `xla::PjRtLayout` to `xla::PjRtXlaLayout` to perform the copy. * `xla::Layout` is expensive to copy (`sizeof(xla::Layout)` is 248 bytes as of 20250103) and copying `xla::PjRtXlaLayout` requires copying or moving `xla::Layout`. To address these two problems, this CL changes PjRt and IFRT APIs that return `xla::PjRtLayout` to instead use `std::shared_p",2025-01-04T01:34:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84124
823,"以下是一个github上的tensorflow下的一个issue, 标题是(Tensortflow import issue after installation)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I resintalled Python and my Anaconda environment and reinstalled using pip from notebook. Please see attached installation log and then import logs  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,dnmaster1,Tensortflow import issue after installation, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I resintalled Python and my Anaconda environment and reinstalled using pip from notebook. Please see attached installation log and then import logs  Standalone code to reproduce the issue   Relevant log output ,2025-01-03T19:35:37Z,type:bug,closed,0,7,https://github.com/tensorflow/tensorflow/issues/84119,"check python version ,I think it need 3.10 version because it was giving the similar type of error in 3.12 if still error persists try with gpu","Hi Manoj  I tried 3.10 ad 3.12. I don't believe they are now available for download. I also tried tensorflowcpu, but it didn't help. Where do you find tensorflowgpu? I don't have a GPU, but my CPU is ARM arch. Thanks Dhimant On Sat, Jan 4, 2025 at 6:22 AM Manoj Nayak ***@***.***> wrote: > check python version ,I think it need 3.10 version because it was giving > the similar type of error in 3.12 > if still error persists try with gpu > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. Please only open new issues if there is information (like your CPU specs) that make your problem different than the existing one.,Are you satisfied with the resolution of your issue? Yes No,"I did search. You closed my previous issue. On Sun, Jan 5, 2025 at 9:37 AM Mihai Maruseac ***@***.***> wrote: > Duplicate of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) > . Please do a > search before opening new issues. Please only open new issues if there is > information (like your CPU specs) that make your problem different than the > existing one. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","Please reopen. This is not closed. On Sun, Jan 5, 2025 at 9:37 AM Mihai Maruseac ***@***.***> wrote: > Closed CC(Tensortflow import issue after installation)  as > completed. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",Opening multiple issues can be considered as spam. Let's move discussion to just one issue.
1266,"以下是一个github上的tensorflow下的一个issue, 标题是(KeyError: ""There is no item named 'PetImages\\Cat\\0.jpg' in the archive"" When Running TensorFlow Locally(CPU) on Anaconda in VS Code.)， 内容是 ( Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.1  Custom code Yes  OS platform and distribution window11  Mobile device _No response_  Python version 3.10.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am a beginner and encountering an issue while trying to run TensorFlow locally using Anaconda in VS Code. The same code runs smoothly on Google Colab, but when executed locally, it fails during the dataset download process with the following error. What I've Tried Redownloading TensorFlow and TensorFlow Datasets to ensure they are up to date. Manually Unzipping the Dataset and verifying if 'PetImages/Cat/0.jpg' exists in the archive. Readjusting Python, TensorFlow, and TensorFlow Datasets Versions to match those in Colab by using Python 3.10.11 )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,zzzHou01,"KeyError: ""There is no item named 'PetImages\\Cat\\0.jpg' in the archive"" When Running TensorFlow Locally(CPU) on Anaconda in VS Code."," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.1  Custom code Yes  OS platform and distribution window11  Mobile device _No response_  Python version 3.10.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am a beginner and encountering an issue while trying to run TensorFlow locally using Anaconda in VS Code. The same code runs smoothly on Google Colab, but when executed locally, it fails during the dataset download process with the following error. What I've Tried Redownloading TensorFlow and TensorFlow Datasets to ensure they are up to date. Manually Unzipping the Dataset and verifying if 'PetImages/Cat/0.jpg' exists in the archive. Readjusting Python, TensorFlow, and TensorFlow Datasets Versions to match those in Colab by using Python 3.10.11 ",2025-01-03T14:48:36Z,type:others awaiting PR merge 2.17,open,0,7,https://github.com/tensorflow/tensorflow/issues/84104,", Could you please provide the document which you are following to execute the code so that it helps to debug the issue. Thank you!","> , Could you please provide the document which you are following to execute the code so that it helps to debug the issue. Thank you! Thank you for your response! I am following the TensorFlow Cats vs Dogs tutorial provided on the official TensorFlow website. Source:https://www.tensorflow.org/tutorials/images/transfer_learning","I can replicate the issue on Windows 10 with TF 2.18, python 3.12 and tensorflowdatasets 4.9.7. It doesn't happen on Colab or MacOS.  I believe the issue has to do with how the file path is parsed in Windows vs Unix/Linux. I'll send out a CL to fix this shortly. This fix is going to be in tensorflow/datasets by the way.  Also, this is probably a duplicate of issues/3918 in tensorflow/datasets.  There is a hacky fix available in the issuecomment1892835410 in the meantime.","This fix for this was merged to tensorflow/datasets through https://github.com/tensorflow/datasets/commit/9969ce542f4b0e1cbf0a085e8e0df11bccea5c17. Once there is a new release, the problem should be fixed.","> I can replicate the issue on Windows 10 with TF 2.18, python 3.12 and tensorflowdatasets 4.9.7. It doesn't happen on Colab or MacOS. >  > I believe the issue has to do with how the file path is parsed in Windows vs Unix/Linux. I'll send out a CL to fix this shortly. This fix is going to be in tensorflow/datasets by the way. >  > Also, this is probably a duplicate of issues/3918 in tensorflow/datasets. [](https://github.com/zzzHou01) There is a hacky fix available in the issuecomment1892835410 in the meantime. > 此修復已透過 tensorflow/datasets @ 9969ce5合併到 tensorflow/datasets 。一旦有新版本發布，該問題就會解決。 Thanks a lot for your detailed explanation! It helped me solve my problem. I really appreciate your support.",> 我可以在裝有 TF 2.18、python 3.12 和 tensorflowdatasets 4.9.7 的 Windows 10 上複製該問題。這在 Colab 或 MacOS 上不會發生。 >  > 我相信這個問題與 Windows 和 Unix/Linux 中檔案路徑的解析方式有關。我將很快發送 CL 來修復此問題。順便說一下，這個修復將在tensorflow/datasets中進行。 >  > 此外，這可能是tensorflow/datasets 中issues/3918的重複。[](https://github.com/zzzHou01)同時，issuecomment1892835410中有一個可用的駭客修復程序。 Thanks a lot for your detailed explanation! It helped me solve my problem. I really appreciate your support.,"> This fix for this was merged to tensorflow/datasets through tensorflow/datasets. Once there is a new release, the problem should be fixed. Thanks a lot for your detailed explanation! It helped me solve my problem. I really appreciate your support."
705,"以下是一个github上的tensorflow下的一个issue, 标题是(Tensorflow not supported on Windows + ARM CPUs)， 内容是 ( Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I can't import tensorflow  Standalone code to reproduce the issue   Relevant log output )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,dnmaster1,Tensorflow not supported on Windows + ARM CPUs, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I can't import tensorflow  Standalone code to reproduce the issue   Relevant log output ,2025-01-03T14:33:01Z,stat:awaiting tensorflower type:feature type:build/install subtype:windows TF 2.18,open,0,21,https://github.com/tensorflow/tensorflow/issues/84102,Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.,Are you satisfied with the resolution of your issue? Yes No,"I have brand new PC with snapdragon X plus. It is not working. It is in fact working on my old pc On Fri, Jan 3, 2025, 11:59 AM Mihai Maruseac ***@***.***> wrote: > Duplicate of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) > . Please do a > search before opening new issues. This is a very old issue and manifests > because old PCs with Windows cannot load libraries needed by TF because > they have very old architecture sets. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","Please don't close the issue. Yourassumotion is incorrect On Fri, Jan 3, 2025, 11:59 AM Mihai Maruseac ***@***.***> wrote: > Closed CC(Tensorflow not supported on Windows + ARM CPUs)  as > completed. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",What are your CPU specs?,"[image: image.png] On Sat, Jan 4, 2025 at 12:09 PM Mihai Maruseac ***@***.***> wrote: > What are your CPU specs? > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",The image is not getting displayed. Can you paste them instead as text?,"Device name Master2024 Processor Snapdragon(R) X Plus  X1P42100  Qualcomm(R) Oryon(TM) CPU 3.24 GHz Installed RAM 16.0 GB (15.6 GB usable) Device ID BE82051361FA4460944DDA3541AEED2D Product ID 003422133297204AAOEM System type 64bit operating system, ARMbased processor Pen and touch Pen and touch support with 10 touch points Windows specs Edition Windows 11 Home Version 24H2 Installed on ‎12/‎30/‎2024 OS build 26100.2605 Serial number YX0ECPSK Experience Windows Feature Experience Pack 1000.26100.36.0 On Sun, Jan 5, 2025 at 9:31 AM Mihai Maruseac ***@***.***> wrote: > The image is not getting displayed. Can you paste them instead as text? > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","I think TensorFlow on Windows is only supported on Intel, not ARM. But this is indeed different than the other one, so reopening.","Confirmed that Windows support only exists for Intel CPUs. On https://pypi.org/project/tensorflow/files there is no Windows + ARM wheel. On https://pypi.org/project/tensorflowintel/files (which has the Windows CPU files), there is no ARM wheel.","This is also weird, because the pip installer should not have proceeded due to missing wheels. But, can you try installing the linux wheel, via WSL? Not guaranteed to work, but it might.","So my CPU configuration is not supported? On Mon, Jan 6, 2025, 8:36 AM Mihai Maruseac ***@***.***> wrote: > This is also weird, because the pip installer should not have proceeded > due to missing wheels. > > But, can you try installing the linux wheel, via WSL? Not guaranteed to > work, but it might. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","What is ""missing wheels""? I will try WSL now and get back to you. On Mon, Jan 6, 2025, 8:36 AM Mihai Maruseac ***@***.***> wrote: > This is also weird, because the pip installer should not have proceeded > due to missing wheels. > > But, can you try installing the linux wheel, via WSL? Not guaranteed to > work, but it might. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","> So my CPU configuration is not supported? It looks like that. It is different than CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) in that that issue refers to Intel CPUs that are too old and don't have AVX/AVX2 extensions, whereas here your CPU is ARM, which is a completely different instruction set. It needs files compiled specifically for this architecture and we currently only do that for Linux and Apple (due to Mac M* family). > What is ""missing wheels""? The unit of shipping a Python package is called a wheel. For most projects, there is just one single file for any combination of Python, operating system, architecture. But, since TF needs to compile C++ extensions and link to C Python code, TensorFlow needs to ship different files for different supported configurations. In this case, there is no file that can be served for Windows + ARM CPU. When installing a Python package (via `pip install`, but other installers should follow a relatively similar process), `pip` is looking at the list of files for the specified version and tries to find the one that matches all the architecture tags it knows (Python version, operating system, CPU architecture, GLIB version, manylinux standard, etc.). This ensures that what gets installed works on the system. There in an exception to the above process with CPU extensions: there is no tag for them (since they are quite a lot and can result in exponential explosion of configurations) so `pip` cannot determine them before downloading. This is why CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) exists: people have been trying to use TF on very old CPUs and the Windows error message is very confusing (compared to Linux/Mac where it states clearly that the instruction set is not supported). In your case, it is very weird that an ARM CPU and a Windows OS did result in a successful download, even though there is no wheel that matches these tags. > I will try WSL now and get back to you. I hope that works, but note that WSL support is best effort, added just so that people on Windows can run TF with some GPU support. Alternatively, and something I would recommend, is to use Colab. I'm actually using that for a lot of experiments and it's really nice.","I'm marking this as a subissue of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) since it has the same behavior: CPUs that don't support AVX instructions sets (includes ARM ones) are not able to run TF. The only difference, and why this is not marked as duplicate, is that this is a totally different family of CPUs, and likely compiling from source on your own system will produce a wheel that works.","Thank you Mihai! When can I expect resolution? Also, is this something I can compile on my computer? If so, do you have instructions to compile? Thank you again for your help. Dhimant On Thu, Jan 16, 2025 at 4:35 PM Mihai Maruseac ***@***.***> wrote: > I'm marking this as a subissue of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) >  since it has the > same behavior: CPUs that don't support AVX instructions sets (includes ARM > ones) are not able to run TF. The only difference, and why this is not > marked as duplicate, is that this is a totally different family of CPUs, > and likely compiling from source on your own system will produce a wheel > that works. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","It is unclear when support will come. There is/was some work to support ARM CPUs on Mac, I think it might come to Linux and maybe windows later but unclear.  might have more details on the plan. Regarding compiling on own computer, that should definitely be possible. There are some instructions at https://www.tensorflow.org/install/source_windows but I haven't checked how up to date they are.","> There is/was some work to support ARM CPUs on Mac, I think it might come to Linux and maybe windows later but unclear To my knowledge there are no current plans to support arm on windows natively.  We do currently publish wheels for Linux and macOS arm64. ","> > So my CPU configuration is not supported? >  > It looks like that. It is different than  CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) in that that issue refers to Intel CPUs that are too old and don't have AVX/AVX2 extensions, whereas here your CPU is ARM, which is a completely different instruction set. It needs files compiled specifically for this architecture and we currently only do that for Linux and Apple (due to Mac M* family). >  > > What is ""missing wheels""? >  > The unit of shipping a Python package is called a wheel. For most projects, there is just one single file for any combination of Python, operating system, architecture. But, since TF needs to compile C++ extensions and link to C Python code, TensorFlow needs to ship different files for different supported configurations. In this case, there is no file that can be served for Windows + ARM CPU. >  > When installing a Python package (via `pip install`, but other installers should follow a relatively similar process), `pip` is looking at the list of files for the specified version and tries to find the one that matches all the architecture tags it knows (Python version, operating system, CPU architecture, GLIB version, manylinux standard, etc.). This ensures that what gets installed works on the system. >  > There in an exception to the above process with CPU extensions: there is no tag for them (since they are quite a lot and can result in exponential explosion of configurations) so `pip` cannot determine them before downloading. This is why  CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) exists: people have been trying to use TF on very old CPUs and the Windows error message is very confusing (compared to Linux/Mac where it states clearly that the instruction set is not supported). >  > In your case, it is very weird that an ARM CPU and a Windows OS did result in a successful download, even though there is no wheel that matches these tags. >  > > I will try WSL now and get back to you. >  > I hope that works, but note that WSL support is best effort, added just so that people on Windows can run TF with some GPU support. >  > Alternatively, and something I would recommend, is to use Colab. I'm actually using that for a lot of experiments and it's really nice. I have the same issue. I am trying to use TensorFlow in a Windows VM running on my M2 Mac. Would you recommend collab for inference in production?", Were you able to compile Tensorflow yourself in your arm+windows setup as  suggested?,"No, I couldnt On Sun, Feb 9, 2025, 5:09 PM Sakhile Mamba ***@***.***> wrote: >   Were you able to compile > Tensorflow yourself in your arm+windows setup as  >  suggested? > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >"
1195,"以下是一个github上的tensorflow下的一个issue, 标题是(Error occured when compling TensorFlow C++ interface with Bazel)， 内容是 ( Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.15  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version 6.1  GCC/compiler version 8.9  CUDA/cuDNN version 12.2 / 8.9.6.50  GPU model and memory _No response_  Current behavior? I want to install Tensorflow C++ interface, and have followed the version matching and procedure using Bazel. Previously I have encountered the error for rules_python file. The corresponding file has been downloaded and the corresponding url link has been revised in the WORKSPACE for this file. But when fetching repository  and unknownlinuxgnu, the following error occured. But I cannot find the url or the command for these two file that I can revise the link with the location of the corresponding file stated in the error information. The configuration informati)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Myre29,Error occured when compling TensorFlow C++ interface with Bazel," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.15  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version 6.1  GCC/compiler version 8.9  CUDA/cuDNN version 12.2 / 8.9.6.50  GPU model and memory _No response_  Current behavior? I want to install Tensorflow C++ interface, and have followed the version matching and procedure using Bazel. Previously I have encountered the error for rules_python file. The corresponding file has been downloaded and the corresponding url link has been revised in the WORKSPACE for this file. But when fetching repository  and unknownlinuxgnu, the following error occured. But I cannot find the url or the command for these two file that I can revise the link with the location of the corresponding file stated in the error information. The configuration informati",2025-01-03T07:21:59Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84042,", According to the official document, for tensorflow v2.15, the compiler is Clang 16.0.0. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations. https://www.tensorflow.org/install/sourcegpu Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1167,"以下是一个github上的tensorflow下的一个issue, 标题是(Mixing Keras Layers and TF modules.)， 内容是 ( Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? tf.Module can trace tf.Variable but it cannot trace variables from tf.keras or tf.keras.Variable.   Standalone code to reproduce the issue  is empty.  Works. Specifically I am more interested in keras layers like   For which tracing does not appear to work.  I am interested in trying https://github.com/google/sequencelayers but they seem mostly broken on this TF version and python version due to the tracing issues.  I am curious to try fixing it, but not sure what's a supported path. Using keras layers in tf.Module does not work due to tracing issues.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jonasrsv42,Mixing Keras Layers and TF modules.," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? tf.Module can trace tf.Variable but it cannot trace variables from tf.keras or tf.keras.Variable.   Standalone code to reproduce the issue  is empty.  Works. Specifically I am more interested in keras layers like   For which tracing does not appear to work.  I am interested in trying https://github.com/google/sequencelayers but they seem mostly broken on this TF version and python version due to the tracing issues.  I am curious to try fixing it, but not sure what's a supported path. Using keras layers in tf.Module does not work due to tracing issues.",2025-01-02T10:46:55Z,stat:awaiting response type:support stale comp:keras 2.17,closed,1,7,https://github.com/tensorflow/tensorflow/issues/84019,"I just hit the same issue trying to upgrade some code that was previously working on tensorflow 2.14; My setup is the same as described:    Top level `tf.Module`    Submodules are `keras.Model`s The kerasbased submodules are now not being detected at the `tf.Module` level because the reflection based implementation is explicitly looking for submodules that extend `tf.Module` (here). It appears that since the 2.16 release that switched to keras 3.X, `keras.Model` / `keras.layers.Layer` no longer extends `tf.Module` but instead only extends the underlying `AutoTrackable` class via its own `TFLayer` class (here). This contradicts / invalidates the tensorflow documentation here: > tf.keras.layers.Layer is the base class of all Keras layers, and it inherits from tf.Module. Looking through some issues in the keras repo, it appears this is intentional, unfortunately. Specifically this comment. There is a section in the keras documentation  about this and gives some direction for downgrading keras to v2 in order to support this structure.",", By default Tensorflow v2.17, v2.18 contains the Keras3.0.  As mentioned in this comment, Keras 3 design supports multiple backends (like JAX and PyTorch) in addition to TensorFlow. To achieve this, core classes like Model and Layer no longer rely on TensorFlow `tf.Module`. As a result, variable tracking within models and layers is no longer automatic. As this issue is more related to Keras, Kindly raise the request in the Kerasteam/keras repo for further discussion. Thank you!","I can raise it in Keras. But out of curiousity, I am trying to use tensorflow without Keras because Keras is missing features I want. (E.g Composite objects for Tensors)  But it seems with this split Tensorflow loses implementations for many common layers? Is there some intended replacement implementation of these layers for tensorflow if Keras will no longer work?  Or is the expectation that we should roll our own using tensorflow primitives for all the layers that used to be in tf.keras? ",It is unlikely that TF would get these layers.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
