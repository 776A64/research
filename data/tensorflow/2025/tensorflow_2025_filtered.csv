sharding,copybara-service[bot],Cache the hash of `xla::ifrt::HloSharding`,"Cache the hash of `xla::ifrt::HloSharding` `xla::HloSharding`'s hash function isn't cheap because its current implementation unrolls iota tile assignment into a regular tile assignment. Since sharding objects are immutable, it is safe to cache its hash value. The cached value check uses the same pattern as `BasicDeviceList`.",2025-04-05T19:19:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90750
tpu,Andonvr,Build TFLite as static C++ library for use with WASM," Issue type Documentation Feature Request  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.19.0  Custom code No  OS platform and distribution WASM  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version Emscripten  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm writing a C++ project that I compile to WASM using Emscripten. I want to make use of TFLite, but am having issues getting it set up. I need to use TFLite directly from within C++. Calling Tensorflow.js via javascript does not work for my usecase. ONNXPack seems to have something like that (third question in this FAQ).  I found this really old Issue, which was closed due to inactivity.  Standalone code to reproduce the issue ```shell N/A ```  Relevant log output ```shell N/A ```",2025-04-05T11:17:18Z,type:feature comp:lite type:docs-feature TF 2.18,open,0,4,https://github.com/tensorflow/tensorflow/issues/90748,Is it open I would love to work on it.,"Hi,   I apologize for the delayed response, Please refer this blog and https://www.npmjs.com/package//tfjstflite which may help you to solve your issue and there are some similar issues which may help you https://github.com/emscriptencore/emsdk/issues/1424 and https://github.com/tensorflow/tensorflow/issues/46359 Thank you for your cooperation and understanding","Thanks for pointing me in that direction!   That guide uses 4 year old forks of XNNPack and Tensorflow, so I would have to recreate forks myself from the newer versions. This is not a _big_ problem, as the changes are quite small (https://github.com/google/XNNPACK/compare/master...visualcamp:XNNPACK:wasm and https://github.com/tensorflow/tensorflow/compare/master...visualcamp:tensorflow:PD135). But it feels like kind of a hack? Maybe you know a better way, using the official repos.  This is what happens when I use the original tensorflow and xnnpack repos: XNNPack is unhappy that I try to compile with CMAKE_SYSTEM_NAME as ""Emscripten"" (see their CMakeLists.txt). ```  The ASM compiler identification is unknown  Found assembler: /Users/anton/aniraweb/aniraweb/src/wasm/modules/emsdk/upstream/emscripten/emcc  Warning: Did not find file Compiler/ASM  Building for XNNPACK_TARGET_PROCESSOR: x86 CMake Error at wasmbuild/xnnpack/CMakeLists.txt:359 (MESSAGE):   Unrecognized CMAKE_SYSTEM_NAME value ""Emscripten"" ``` But it's good to know that I can create forks as a fallback, I'll try that some time this week. Thanks!","After taking a look at all links, and also trying the guide with custom forks of XNNPACK and Tensorflow, it is still unclear to me how it's supposed to work.   Does anyone have an actually working example?"
yi,copybara-service[bot],Run build_cleaner on xla/ directory.,"Run build_cleaner on xla/ directory. I've encountered a few CLs that attempted to fix this in local directories, so I figured I run this for all of xla to fix the lowhanging fruits. It resolves several unnecessary & missing dependencies and simplifying target paths, but not all of them. Here are the issues that came up that I didn't attempt to fix: * any conflicts that needs manual handling * conflicts that needs to choose between two ""valid"" targets * missing BUILD in a directory * missing target for a file (e.g. a python script) * missing targets for some `bzl_library` * platformspecific code (e.g. rocm) * ones that use filegroup instead of individual cc_library * and more. Before: ```  metric        median             Δ                  1pval             cpu: 3590.690s   ±91.6s                                           memory:     4533MB   ±2.6MB                                          system:  594.230s   ±10.5s                                             wall:  907.605s   ±83.0s                                               ``` After: ```  metric        median             Δ                  1pval             cpu: 3599.015s  ±131.4s    +8.3s, +0.2% 0.03 (not significant)    memory:     4533MB   ±2.3MB  +0.0MB, +0.0% 0.00 (not significant)    system:  582.305s    ±9.1s   11.9s, 2.0% 0.25 (not significant)      wall:  808.958s   ±95.5s  98.6s, 10.9% 0.57 (not significant)    ``` Overall, it has modest savings of ~1 minute of wall (physical) time. Since I've excluded some execution tests under `stream_executor/` and `service/` the estimated savings may be greater. Overall, it's a small improvement but should pay dividends in the long run. Note: I'll be sending a series of CLs to fix them in batches of subdirectories to simplify merging.",2025-04-04T23:38:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90729
opt,copybara-service[bot],Create a PjRt protos dir.,"Create a PjRt protos dir. This dir exists for common PjRt proto files. To avoid breaking Jax in OSS, we also create an empty forwarding header & BUILD target where the old `compile_options.proto` used to live, which will soon be deleted. We only move the protos in `third_party/tensorflow/compiler/xla/pjrt` into this new dir (i.e. don't include `pjrt/stream_executor_executable.proto`, `pjrt/distributed/protocol.proto`, `pjrt/gpu/gpu_topology.proto`, and `pjrt/plugin/xla_cpu/cpu_topology.proto`) since those are purpose specific.",2025-04-04T19:44:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90719
tpu,copybara-service[bot],Update profiling code/scripts to prevent dependency on unreliable GPU targets originating from TPU output.,Update profiling code/scripts to prevent dependency on unreliable GPU targets originating from TPU output.,2025-04-04T19:29:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90718
tpu,copybara-service[bot],Bumping up libtpu version to pick correct versioned nightlies,Bumping up libtpu version to pick correct versioned nightlies,2025-04-04T18:45:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90716
tpu,Usernadia122003,sfe, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2  Custom code Yes  OS platform and distribution Windows 11   Mobile device _No response_  Python version 3.13.2  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? none  Standalone code to reproduce the issue ```shell none ```  Relevant log output ```shell ```,2025-04-04T09:42:34Z,type:bug invalid,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90694,Doesn't seem to contain any relevant information
opt,copybara-service[bot],Add back deleted `xla_gpu_enable_nccl_clique_optimization` flag.,Add back deleted `xla_gpu_enable_nccl_clique_optimization` flag. Removing the flag breaks some users.,2025-04-04T09:21:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90690
multi-gpu,LukasMahieu,NaN loss on multi-GPU MirroredStrategy since tf 2.16," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version starting from 2.16 ([andcuda] versions)  Custom code Yes  OS platform and distribution Rocky Linux 8.9  Mobile device Rocky Linux 8.9  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory A100  80Gb  Current behavior? Starting from tensorflow version 2.16, models that train perfectly fine on a single A100 GPU will quickly devolve into an inf and then NaN loss on multiple GPUs. This is not the case for tensorflow version 2.15. This is not an isolated issue on my machine, as multiple users of our package which is built on tensorflow report the same exact issue (https://github.com/aertslab/CREsted/issues/100). Also, there's multiple mentions on here (for example https://github.com/tensorflow/tensorflow/issues/87432 and https://github.com/tensorflow/tensorflow/issues/62915) without a real resolution pertaining to the same issue. It's clearly a real issue as I've seen it happening on multiple different machines by multiple different users and it has a big impact since it stops us from scaling our models with tensorflow.  A couple of things I tried that didn't work:  Clipping the gradients  Lowering the learning rate  Ensuring no empty batches get created  Standalone code to reproduce the issue ```shell import tensorflow as tf def create_model():     """"""Create a larger and more complex model for demonstration purposes.""""""     model = tf.keras.Sequential(         [             tf.keras.layers.InputLayer(input_shape=(128, 128, 3)),             tf.keras.layers.Conv2D(128, 3, padding=""same"", activation=""relu""),             tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),             tf.keras.layers.BatchNormalization(),             tf.keras.layers.Conv2D(256, 3, padding=""same"", activation=""relu""),             tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),             tf.keras.layers.BatchNormalization(),             tf.keras.layers.Conv2D(512, 3, padding=""same"", activation=""relu""),             tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),             tf.keras.layers.BatchNormalization(),             tf.keras.layers.Conv2D(1024, 3, padding=""same"", activation=""relu""),             tf.keras.layers.GlobalAveragePooling2D(),             tf.keras.layers.Dense(2048, activation=""relu""),             tf.keras.layers.Dropout(0.5),             tf.keras.layers.Dense(2048, activation=""relu""),             tf.keras.layers.Dropout(0.5),             tf.keras.layers.Dense(1000, activation=""softmax""),         ]     )     return model def generate_synthetic_data(batch_size, num_batches):     """"""Generate synthetic data for training and validation.""""""     for _ in range(num_batches):         images = tf.random.normal((batch_size, 128, 128, 3))         labels = tf.random.uniform((batch_size,), maxval=10, dtype=tf.int32)         yield images, labels def train_multi_gpu(batch_size, epochs):     """"""Train the model using multiple GPUs.""""""      Detect and initialize GPUs     gpus_found = tf.config.list_physical_devices(""GPU"")     strategy = tf.distribute.MirroredStrategy()     print(""Number of replica devices in use: {}"".format(strategy.num_replicas_in_sync))     print(""Number of GPUs available: {}"".format(len(gpus_found)))     assert len(gpus_found) >= 1, ""Training requires at least 1 GPU""     strategy = tf.distribute.MirroredStrategy()     global_batch_size = batch_size * strategy.num_replicas_in_sync      Create a synthetic dataset     train_dataset = tf.data.Dataset.from_generator(         lambda: generate_synthetic_data(global_batch_size, 500),         output_signature=(             tf.TensorSpec(shape=(global_batch_size, 128, 128, 3), dtype=tf.float32),             tf.TensorSpec(shape=(global_batch_size,), dtype=tf.int32),         ),     ).repeat()     with strategy.scope():         model = create_model()         model.compile(             optimizer=""adam"", loss=tf.keras.losses.SparseCategoricalCrossentropy()         )         model.fit(train_dataset, epochs=epochs, steps_per_epoch=500) if __name__ == ""__main__"":     train_multi_gpu(batch_size=64, epochs=20) ```  Relevant log output ```shell Loss: NaN ```",2025-04-04T08:29:13Z,stat:awaiting tensorflower type:bug TF 2.16,open,1,4,https://github.com/tensorflow/tensorflow/issues/90686, Most Likely Causes 1. NCCL communication changes (modified gradient aggregation) 2. Mixed precision scaling synchronization updates 3. XLA graph optimization changes  Key Changes Between 2.15 → 2.16  Updated to CUDA 12.3/cuDNN 8.9  Modified `MirroredStrategy` gradient reduction logic  Changed `LossScaleOptimizer` behavior  Updated weight synchronization logic  Suggested Workarounds 1. Pin to TF 2.15 (verified working)    ```bash    pip install tensorflow==2.15.0 Try explicit FP32 mode:    ```python tf.keras.mixed_precision.set_global_policy('float32') ``` Test alternative reduction strategies:   ```python strategy = tf.distribute.MirroredStrategy(     cross_device_ops=tf.distribute.ReductionToOneDevice() ) ``` Enable NCCL debugging:  ```python export NCCL_DEBUG=INFO ```,"The problem was probably caused by changes made in TensorFlow 2.16. In this release, changes in how multi GPU training is managed like changes in gradient aggregation (particularly with NCCL communication), mixed precision scaling, and weight synchronization caused a regression that can lead to numerical instabilities. These instabilities appear in the form of the loss value rapidly becoming infinite and then NaN when training is done on more than one GPU, although single GPU training is stable. I'd love to work on this issue","Neither the explicit FP32 mode or alternative reduction strategy work, they still result in an inf > NaN loss.  Here's the full log in NCCL debug mode ``` 20250407 10:35:33.482430: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250407 10:35:33.495704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1744014933.509164 4105909 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1744014933.513302 4105909 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered W0000 00:00:1744014933.525145 4105909 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1744014933.525163 4105909 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1744014933.525165 4105909 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1744014933.525166 4105909 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. 20250407 10:35:33.528802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. I0000 00:00:1744014937.204507 4105909 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79195 MB memory:  > device: 0, name: NVIDIA A100SXM480GB, pci bus id: 0000:18:00.0, compute capability: 8.0 I0000 00:00:1744014937.206022 4105909 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 79195 MB memory:  > device: 1, name: NVIDIA A100SXM480GB, pci bus id: 0000:19:00.0, compute capability: 8.0 Number of replica devices in use: 2 Number of GPUs available: 2 .../envs/crested/lib/python3.12/sitepackages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.   warnings.warn( Epoch 1/20 I0000 00:00:1744014940.866528 4106006 cuda_dnn.cc:529] Loaded cuDNN version 90300 I0000 00:00:1744014941.293732 4106004 cuda_dnn.cc:529] Loaded cuDNN version 90300 i28g27:4105909:4106346 [0] NCCL INFO Bootstrap : Using ib0:172.23.4.51 i28g27:4105909:4106379 [0] NCCL INFO cudaDriverVersion 12060 i28g27:4105909:4106379 [0] NCCL INFO NCCL version 2.23.4+cudaCUDA_MAJOR.CUDA_MINOR i28g27:4105909:4106383 [0] NCCL INFO NET/Plugin: Could not find: libncclnet.so. Using internal network plugin. i28g27:4105909:4106383 [0] NCCL INFO NET/IB : Using [0]={[0] mlx5_0:1/IB, [1] mlx5_1:1/IB} [RO]; OOB ib0:172.23.4.51 i28g27:4105909:4106383 [0] NCCL INFO PROFILER/Plugin: Could not find: libncclprofiler.so. i28g27:4105909:4106383 [0] NCCL INFO Using network IB i28g27:4105909:4106384 [1] NCCL INFO PROFILER/Plugin: Could not find: libncclprofiler.so. i28g27:4105909:4106384 [1] NCCL INFO Using network IB i28g27:4105909:4106384 [1] NCCL INFO ncclCommInitRank comm 0x15230c060750 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 19000 commId 0x2eebe5b5ca38976b  Init START i28g27:4105909:4106383 [0] NCCL INFO ncclCommInitRank comm 0x15230c022a60 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 18000 commId 0x2eebe5b5ca38976b  Init START i28g27:4105909:4106384 [1] NCCL INFO Bootstrap timings total 0.000486 (create 0.000020, send 0.000105, recv 0.000122, ring 0.000045, delay 0.000000) i28g27:4105909:4106383 [0] NCCL INFO Bootstrap timings total 0.000467 (create 0.000014, send 0.000096, recv 0.000192, ring 0.000047, delay 0.000000) i28g27:4105909:4106384 [1] NCCL INFO Setting affinity for GPU 1 to 01ff i28g27:4105909:4106383 [0] NCCL INFO Setting affinity for GPU 0 to 01ff i28g27:4105909:4106384 [1] NCCL INFO comm 0x15230c060750 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0 i28g27:4105909:4106383 [0] NCCL INFO comm 0x15230c022a60 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0 i28g27:4105909:4106383 [0] NCCL INFO Channel 00/08 : 0 1 i28g27:4105909:4106383 [0] NCCL INFO Channel 01/08 : 0 1 i28g27:4105909:4106383 [0] NCCL INFO Channel 02/08 : 0 1 i28g27:4105909:4106384 [1] NCCL INFO Trees [0] 1/1/1>1>0 [1] 1/1/1>1>0 [2] 0/1/1>1>1 [3] 0/1/1>1>1 [4] 1/1/1>1>0 [5] 1/1/1>1>0 [6] 0/1/1>1>1 [7] 0/1/1>1>1 i28g27:4105909:4106384 [1] NCCL INFO P2P Chunksize set to 524288 i28g27:4105909:4106383 [0] NCCL INFO Channel 03/08 : 0 1 i28g27:4105909:4106383 [0] NCCL INFO Channel 04/08 : 0 1 i28g27:4105909:4106383 [0] NCCL INFO Channel 05/08 : 0 1 i28g27:4105909:4106383 [0] NCCL INFO Channel 06/08 : 0 1 i28g27:4105909:4106383 [0] NCCL INFO Channel 07/08 : 0 1 i28g27:4105909:4106383 [0] NCCL INFO Trees [0] 1/1/1>0>1 [1] 1/1/1>0>1 [2] 1/1/1>0>1 [3] 1/1/1>0>1 [4] 1/1/1>0>1 [5] 1/1/1>0>1 [6] 1/1/1>0>1 [7] 1/1/1>0>1 i28g27:4105909:4106383 [0] NCCL INFO P2P Chunksize set to 524288 i28g27:4105909:4106396 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 2 i28g27:4105909:4106397 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0 i28g27:4105909:4106395 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3 i28g27:4105909:4106394 [1] NCCL INFO [Proxy Service] Device 1 CPU core 1 i28g27:4105909:4106384 [1] NCCL INFO threadThresholds 8/8/64  512 i28g27:4105909:4106383 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 8 p2p channels per peer i28g27:4105909:4106383 [0] NCCL INFO CC Off, MultiGPU CC Off, workFifoBytes 1048576 i28g27:4105909:4106383 [0] NCCL INFO TUNER/Plugin: Could not find: libnccltuner.so libncclnet.so. Using internal tuner plugin. i28g27:4105909:4106383 [0] NCCL INFO ncclCommInitRank comm 0x15230c022a60 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 18000 commId 0x2eebe5b5ca38976b  Init COMPLETE i28g27:4105909:4106383 [0] NCCL INFO Init timings  ncclCommInitRank: rank 0 nranks 2 total 0.24 (kernels 0.13, alloc 0.04, bootstrap 0.00, allgathers 0.00, topo 0.05, graphs 0.00, connections 0.01, rest 0.01) i28g27:4105909:4106384 [1] NCCL INFO ncclCommInitRank comm 0x15230c060750 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 19000 commId 0x2eebe5b5ca38976b  Init COMPLETE i28g27:4105909:4106384 [1] NCCL INFO Init timings  ncclCommInitRank: rank 1 nranks 2 total 0.24 (kernels 0.13, alloc 0.04, bootstrap 0.00, allgathers 0.00, topo 0.05, graphs 0.00, connections 0.02, rest 0.00) i28g27:4105909:4106399 [0] NCCL INFO Channel 00/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 00/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106399 [0] NCCL INFO Channel 01/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 01/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106399 [0] NCCL INFO Channel 02/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 02/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 03/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 04/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106399 [0] NCCL INFO Channel 03/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 05/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106399 [0] NCCL INFO Channel 04/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 06/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106399 [0] NCCL INFO Channel 05/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 07/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106399 [0] NCCL INFO Channel 06/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106399 [0] NCCL INFO Channel 07/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Connected all rings i28g27:4105909:4106399 [0] NCCL INFO Connected all rings 130/500 ━━━━━━━━━━━━━━━━━━━━ 27s 75ms/step  loss: nan^CTraceback (most recent call last): ```",Any updates on this issue?
tpu,copybara-service[bot],Get correct num_sc_per_chip when doing AOT compilation for SC XLA ops.,"Get correct num_sc_per_chip when doing AOT compilation for SC XLA ops. In the case of doing AOT compilation, we do not have a tpu system available. Currently the code defaults to setting the number of sparsecores to 4, however this would break cases using chips that have 2 SCs. Therefore, we add an optional attribute to configure this.",2025-04-04T02:00:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90605
tpu,copybara-service[bot],Return error if tpu topology is not available when getting number of cores per chip.,Return error if tpu topology is not available when getting number of cores per chip. This is function is added to prevent returning cores_per_chip=4 when targeting hardware with 2 SCs.,2025-04-04T01:32:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90603
opt,copybara-service[bot],Avoid creating large constants in ConvertTFLBroadcastToMulOp optimization pass.,Avoid creating large constants in ConvertTFLBroadcastToMulOp optimization pass. This pattern is inherited from before and has proved to be increasing the model size due the introduction of large splat const. In its current form this pattern replaces a tfl.broadcast_to op (with rank<4) to a tfl.mul with allones tensor. This change will keep the broadcast_to ops as is because its clear that introducing MUL is not an optimization.,2025-04-03T23:02:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90596
quantization,copybara-service[bot],Replace tf/compiler/mlir/quantization/common/quantization_lib with its lite fork for tensorflow_lite_ops and all its corollaries,Replace tf/compiler/mlir/quantization/common/quantization_lib with its lite fork for tensorflow_lite_ops and all its corollaries FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/92240 from 372046933:fix_pointer_stability 3a1ca864e5c79c49c62ee312952cf8bb58d98d69,2025-04-03T22:56:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90595
yi,copybara-service[bot],Make TensorFlow CI properly read changes to `third_party/` which are Copybara'd to XLA,Make TensorFlow CI properly read changes to `third_party/` which are Copybara'd to XLA This is correct as files on GitHub in both `openxla/xla/third_party` and `tensorflow/tensorflow/third_party` are the same underlying file internally.,2025-04-03T21:18:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90588
tpu,Andonvr,Build Tensorflow Lite for WASM using Emscripten and CMake," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.19.0  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version Emcc: 4.0.5  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm trying to build Tensorflow Lite to WASM, using Emscripten and CMake. See my example repo here.   I can't use the prebuilt Tensorflow.js, even with the WASM backend, since I need to call the inference from several threads right within C++. Leaving WASM to call a tensorflow.js function that runs WASM again, would defeat the purpose for my usecase.   Anyway, the example I'm using here does not even use threads or anything, so that is not all too relevant, just some background.  I have these lines in my CMakeLists.txt: ``` include(${CMAKE_SOURCE_DIR}/cmake/SetupTFLite.cmake) add_dependencies(${MAIN} tensorflowlite) target_link_libraries(${MAIN} tensorflowlite) ``` The `cmake/SetupTFLite.cmake` clones tensorflow, checks out a tag, and then adds the tensorflow/lite subdirectory.   It is worth mentioning, I had to add `set(CMAKE_SYSTEM_NAME ""Linux"")`, otherwise I would get this error while configuring: ``` [cmake] CMake Error at build/xnnpack/CMakeLists.txt:359 (MESSAGE): [cmake]   Unrecognized CMAKE_SYSTEM_NAME value ""Emscripten"" ``` Anyway, so that variable was set.  Configuring all of this works, using the command shown in the repos README.   But building fails: And it seems to fail at different points. Here are a few from consecutive runs: ``` [build] [  6%] Linking CXX executable flatc.js [build] [  6%] Built target flatc [build] make: *** [all] Error 2 [proc] The command: /usr/local/bin/cmake build /Users/anton/tflitewasmexample/build config Debug target all j 13  exited with code: 2 ``` ``` [build] [  8%] Linking CXX static library libprotobufd.a [build] [  8%] Built target libprotobuf [build] make: *** [all] Error 2 [proc] The command: /usr/local/bin/cmake build /Users/anton/tflitewasmexample/build config Debug target all j 13  exited with code: 2 ``` ``` [build] /Users/anton/tflitewasmexample/build/xnnpack/src/xnnpack/math.h:446:52: note: include the header  or explicitly provide a declaration for 'rint' [build] 1 error generated. [build] make[2]: *** [_deps/xnnpackbuild/CMakeFiles/normalization.dir/src/normalization.c.o] Error 1 [build] make[1]: *** [_deps/xnnpackbuild/CMakeFiles/normalization.dir/all] Error 2 [build] 1 error generated. [build] make[2]: *** [_deps/xnnpackbuild/CMakeFiles/indirection.dir/src/indirection.c.o] Error 1 [build] make[1]: *** [_deps/xnnpackbuild/CMakeFiles/indirection.dir/all] Error 2 [build] make[1]: *** [_deps/cpuinfobuild/CMakeFiles/cpuinfo_internals.dir/all] Error 2 [build] make: *** [all] Error 2 [proc] The command: /usr/local/bin/cmake build /Users/anton/tflitewasmexample/build config Debug target all j 13  exited with code: 2 ``` (it then keeps failing at this spot!)  My first hunch is that building my main.cpp does not wait for tensorflow to be built? As in, something is wrong with the dependencies? And that's why Tensorflow compiles further and further, with the previous run still being cached? Sort of raceconditiony? But then it definitely always fails at the ""xnnpack/math.h"" thing?  Is there some simple example of using Tensorflow Lite directly within C++ and compiling it using CMake? I found this one, and oriented myself at it. Plus, it doesn't use WASM.  Standalone code to reproduce the issue I set up a minimal repo to reproduce here. Relevant files:  `CMakeLists.txt`: The CMake configuration file for the project.  `cmake/SetupTFLite.cmake`: A CMake script that clones TensorFlow Lite and includes it in the build. My repo does not include emscripten, nor flatbuffer, this you will have to adjust in the build commands. But maybe you will already see what mistake I made.  Relevant log output Described above.",2025-04-03T21:04:12Z,type:support,closed,0,5,https://github.com/tensorflow/tensorflow/issues/90586," Fix 3: Modify Build Flags Sometimes, the issue stems from Emscripten’s standard library setup. You can try: ```bash emcmake cmake .. DCMAKE_SYSTEM_NAME=Emscripten \   DCMAKE_C_COMPILER=emcc \   DCMAKE_CXX_COMPILER=em++ \   DCMAKE_CXX_FLAGS=""std=c++17 s USE_PTHREADS=1 s WASM=1 s USE_SDL=2 s ENVIRONMENT=web s MODULARIZE=1"" \   DCMAKE_C_FLAGS=""std=c11"" ``` Then run: ```bash cmake build . j ``` **Ensure that:**  You're using the latest compatible Emscripten (>=3.x).  You include `s USE_PTHREADS=1` *only if* you really use threading (and the browser supports it with crossorigin isolation).","When I use `DCMAKE_SYSTEM_NAME=Emscripten`, I get this error: ```  The ASM compiler identification is unknown  Found assembler: /Users/anton/emsdk/upstream/emscripten/emcc  Warning: Did not find file Compiler/ASM  Building for XNNPACK_TARGET_PROCESSOR: x86 CMake Error at build/xnnpack/CMakeLists.txt:359 (MESSAGE):   Unrecognized CMAKE_SYSTEM_NAME value ""Emscripten"" ``` But I got this before, and circumvented this by setting the CMAKE_SYSTEM_NAME to Linux? ","But after making that adjustment and building, I still get this error: ``` make[2]: *** [_deps/xnnpackbuild/CMakeFiles/microkernelsprod.dir/src/qu8gemm/gen/qu8gemm1x4c8minmaxfp32avxld128.c.o] Error 1 make[2]: *** [_deps/xnnpackbuild/CMakeFiles/microkernelsprod.dir/src/f16rminmax/gen/f16rmaxavx512fp16u128acc4.c.o] Error 1 In file included from /Users/anton/tflitewasmexample/build/xnnpack/src/qs8vadd/gen/qs8vaddminmaxscalaru4.c:12: /Users/anton/tflitewasmexample/build/xnnpack/src/xnnpack/math.h:446:52: error: call to undeclared library function 'rint' with type 'double (double)'; ISO C99 and later do not support implicit function declarations [Wimplicitfunctiondeclaration]   446                                                     ^ /Users/anton/tflitewasmexample/build/xnnpack/src/xnnpack/math.h:446:52: note: include the header  or explicitly provide a declaration for 'rint' 1 error generated. make[2]: *** [_deps/xnnpackbuild/CMakeFiles/microkernelsprod.dir/src/f32ibilinear/gen/f32ibilinearscalarc2.c.o] Error 1 1 error generated. make[2]: *** [_deps/xnnpackbuild/CMakeFiles/microkernelsprod.dir/src/qs8dwconv/gen/qs8dwconv25p1cminmaxfp32scalarimagic.c.o] Error 1 make[1]: *** [_deps/xnnpackbuild/CMakeFiles/microkernelsprod.dir/all] Error 2 [ 26%] Built target flatc make: *** [all] Error 2 ```","After some thinking, I will close this Issue and open a separate slightly different one. I think my problem does not lie with CMake, but on the overall building of a static WASM library.",Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],"Add optimization pass to rewrite FC(IsConst(x), y) as FC(y, x).","Add optimization pass to rewrite FC(IsConst(x), y) as FC(y, x). Many downstream optimizations rely on proper ordering of the input operands into FullyConnected. Some JAX programs produce graphs with the inputs swapped and this rewrite pattern protects against that behavior.",2025-04-03T20:11:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90582
tpu,plopresti,"Clang+NVCC: ./tensorflow/core/util/gpu_device_functions.h(198): error: identifier ""__nvvm_read_ptx_sreg_laneid"" is undefined"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version r2.19  Custom code No  OS platform and distribution Alma 9.5  Mobile device _No response_  Python version 3.12  Bazel version 6.5.0  GCC/compiler version N/A  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Attempting to build with NVCC+Clang fails.  Standalone code to reproduce the issue ```shell env TF_NEED_CLANG=1 TF_NEED_CUDA=1 TF_CUDA_CLANG=0 GCC_HOST_COMPILER_PATH=/usr/bin/clang ./configure bazel build repo_env=CUDA_NVCC=1 Just tell configure you want to use Clang for everything except compiling CUDA kernels. For that, tell it you want nvcc, but you want ""compilerbindir"" pointing to Clang. ```  Relevant log output ```shell ./tensorflow/core/util/gpu_device_functions.h(198): error: identifier ""__nvvm_read_ptx_sreg_laneid"" is undefined The problem is gpu_device_functions.h has a ""if __clang__"" conditional that invokes a Clangspecific function that nvcc does not understand. ```",2025-04-03T19:59:49Z,type:bug awaiting PR merge TF 2.18,closed,0,2,https://github.com/tensorflow/tensorflow/issues/90578,"Hi  , Apologies for the delay, and thank you for raising your concern here. I noticed that you have already raised a PR related to your issue. Once it gets merged, your issue should be resolved. Thank you!",Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],[NFC] Refactor `GetExecutableExtras()` to separate the compilation option update to another function `UpdateCompileOptions()`.,[NFC] Refactor `GetExecutableExtras()` to separate the compilation option update to another function `UpdateCompileOptions()`. The `UpdateCompileOptions()` will be used by the 'Compile()` implementation which returns an unloaded executable.,2025-04-03T19:12:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90576
quantization,copybara-service[bot],Add support for blockwise quantization to FC operator.,Add support for blockwise quantization to FC operator.,2025-04-03T18:39:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90570
tpu,plopresti,"gpu_prim.h: no instance of overloaded function ""cub::ThreadLoadVolatilePointer"" matches the specified type"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version r2.19  Custom code No  OS platform and distribution Alma Linux 9.5  Mobile device _No response_  Python version 3.12  Bazel version 6.5.0  GCC/compiler version _No response_  CUDA/cuDNN version 12.8.0  GPU model and memory _No response_  Current behavior? Compiling against CUDA 12.8.0 gives an error: ``` ./tensorflow/core/kernels/gpu_prim.h(48): error: no instance of overloaded function ""cub::ThreadLoadVolatilePointer"" matches the specified type                             Eigen::half ThreadLoadVolatilePointer( ``` Commit 5467ee9 for XLA needs to be adapted and applied to tensorflow/core/kernels/gpu_prim.h.  Standalone code to reproduce the issue ```shell `env HERMETIC_CUDA_VERSION=1.8.0 ./configure` `bazel build` ```  Relevant log output ```shell ./tensorflow/core/kernels/gpu_prim.h(48): error: no instance of overloaded function ""cub::ThreadLoadVolatilePointer"" matches the specified type                             Eigen::half ThreadLoadVolatilePointer( ```",2025-04-03T17:41:17Z,type:bug awaiting PR merge TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/90562,"Hi  , Apologies for the delay, and thank you for raising your concern here. I noticed that you have already raised a PR related to your issue. Once it gets merged, your issue should be resolved. Thank you!"
bert,copybara-service[bot],PR #24550: Support custom call stream assignment,"PR CC(Add python API for some quantized operations.): Support custom call stream assignment Imported from GitHub PR https://github.com/openxla/xla/pull/24550 Previously, in JAX if a user tried to use `compute_on(""gpu_stream:"")` on a computation that included a custom call, that custom call would not run on the specified stream, leading to errors.  To fix this, we simply use `GetStreamForExecution` instead of just relying on `params.stream` to decided which stream to pass to the FFI. This is the same logic that other thunks already have. We need to make this edit twice since there are two codepaths a custom call can follow, once via `ExecuteFfiHandler` and another in `ExecuteCustomCall`.  Copybara import of the project:  7ee230f7abf744fa40ad86e0521e9b8b9f7cc80e by chaser : Support custom call stream assignment Merging this change closes CC(Add python API for some quantized operations.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24550 from chaserileyroberts:chase/custom_call_stream_fix 7ee230f7abf744fa40ad86e0521e9b8b9f7cc80e",2025-04-03T14:55:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90555
sharding,copybara-service[bot],Allow not to have out shardings on collective ops and drop passing mesh to all reduce on explicit reshards.,Allow not to have out shardings on collective ops and drop passing mesh to all reduce on explicit reshards.,2025-04-03T13:59:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90552
memory leak,copybara-service[bot],PR #24513: [ROCm] Use code object version 5,PR CC(tfdbg memory leak): [ROCm] Use code object version 5 Imported from GitHub PR https://github.com/openxla/xla/pull/24513 Allows generating hsaco files on pre6.3 rocm Copybara import of the project:  084c021adf22a6d531feeb11f8c0daa7fd444f22 by Dragan Mladjenovic : [ROCm] Use code object version 5 Allows generating hsaco files on pre6.3 rocm Merging this change closes CC(tfdbg memory leak) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24513 from ROCm:ci_check 084c021adf22a6d531feeb11f8c0daa7fd444f22,2025-04-03T10:08:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90550
opt,copybara-service[bot],[XLA:GPU] Add an argument to `CompileAndOptionallyVerifyPtx` to decide whether the optimization pipeline should be run.,[XLA:GPU] Add an argument to `CompileAndOptionallyVerifyPtx` to decide whether the optimization pipeline should be run. Seems like many tests use this but actually don't need/expect optimizations to be run.,2025-04-03T07:54:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90539
yi,copybara-service[bot],Compute the number of warps we should have based on hardware properties,"Compute the number of warps we should have based on hardware properties Replaces the current placeholder value. We still have a slightly more distilled placeholder of what we are trying to achieve in terms of occupancy, but figuring that out is a problem for another day.",2025-04-03T07:26:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90538
fp16,copybara-service[bot],PR #22541: [ROCm] Cleanup atomics support,"PR CC(Java process crashes during model loading): [ROCm] Cleanup atomics support Imported from GitHub PR https://github.com/openxla/xla/pull/22541 Weaken the ordering barriers to match what atomicAdd does on rocm. Emulate fp16 atomic on top of packed fp16 atomic where possible. Also for bfloat16 atomics, albeit those don't get matched right now due to FloatNormalization. Left in support for fp16 and bfloat16 vector atomics. We might enable the vectorization for them in the future if we can prove the access satisfies 4byte aligment. Copybara import of the project:  b53668020f946207aa879eecd8e0b70173f75570 by Dragan Mladjenovic : [ROCm] Cleanup atomics support Merging this change closes CC(Java process crashes during model loading) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22541 from ROCm:atomics_cleanup b53668020f946207aa879eecd8e0b70173f75570",2025-04-03T06:57:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90532
yi,copybara-service[bot],Make XLA_GPU_JIT target depend on XLA's runtime,Make XLA_GPU_JIT target depend on XLA's runtime I previously removed the dependency of the XLA runtime from the GpuExecutable target which broke users of TF's xla_gpu_jit which was transitively relying on the removed dependency. This changes fixes the issue for users of xla_gpu_jit,2025-04-03T06:06:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90530
sharding,copybara-service[bot],Add basic support for RaggedDot in SPMD partitioner.,Add basic support for RaggedDot in SPMD partitioner. This cl is the 4th step to fully support RaggedDot in Shardy. 1. Import and export the RaggedDot into Shardy. 2. Add a sharding rule for the new operation. 3. Handle the new operation in the explicit reshard in Shardy. 4. Handle it in SPMD partitioner without resolving any conflicts. Steps 1 and 2 were in cl/737011229. We will proceed with Step 3 afterwards. This operation is a great example to demonstrate it is easy to support new and customized operations in Shardy system. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22541 from ROCm:atomics_cleanup b53668020f946207aa879eecd8e0b70173f75570,2025-04-03T05:10:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90528
opt,copybara-service[bot],"SliceOp, DynamicSliceOp : Direct StableHLO -> HLO translation.","SliceOp, DynamicSliceOp : Direct StableHLO > HLO translation. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24939 from shraiysh:dump_non_default_debug_options 5c50eea207f6ac3846bcc8b37ed03e1cb71a6a99",2025-04-03T01:11:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90508
sharding,copybara-service[bot],[IFRT] Strengthen the device list and memory kind checks of `ClientMakeArraysFromHostBufferShards()`,"[IFRT] Strengthen the device list and memory kind checks of `ClientMakeArraysFromHostBufferShards()` `ClientMakeArraysFromHostBufferShards()`, a fallback implementation for `Client::MakeArraysFromHostBufferShards()`, checks if supplied array specs meet the current API contract (the sharding of all array specs have equal device lists and memory kinds).",2025-04-02T23:58:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90504
opt,copybara-service[bot],"clang-tidy: No header providing ""xla::HloPrintOptions"" is directly included","clangtidy: No header providing ""xla::HloPrintOptions"" is directly included",2025-04-02T22:08:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90499
yi,aravindhbalaji1985,Fix compilation error due to overloads of cub::ThreadLoadVolatilePointer,Applying the same fix as detailed in https://github.com/tensorflow/tensorflow/commit/5467ee993e1d3e4709c1e99f3a15a978325ae536 in core/kernels/gpu_prim.h. Without this fix compilation of sparse_grad_op_gpu.cu..  `./tensorflow/core/kernels/gpu_prim.h:48:40: error: no function template matches function template specialization 'ThreadLoadVolatilePointer'    48                                   ^ 2 errors generated when compiling for sm_80.`,2025-04-02T19:44:26Z,ready to pull size:S comp:core,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90494,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
gemma,copybara-service[bot],[XLA] Skip gemma HLOs for `linux-arm64-t2a-48`,[XLA] Skip gemma HLOs for `linuxarm64t2a48`,2025-04-02T18:36:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90492
sharding,copybara-service[bot],[HLO Componentization] Migrate deprecated uses of sharding_builder,[HLO Componentization] Migrate deprecated uses of sharding_builder,2025-04-02T18:31:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90491
bfloat16,copybara-service[bot],[XLA] Make Bfloat16Propagation flow through execution threads,"[XLA] Make Bfloat16Propagation flow through execution threads This simply adds the needed control flow handling for kCall, kAsyncStart and kAsyncDone.",2025-04-02T17:04:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90483
yi,copybara-service[bot],[xla:gpu] CommandBuffer: return recorded commands from CommandBufferCmd::Record,"[xla:gpu] CommandBuffer: return recorded commands from CommandBufferCmd::Record Every CommandBufferCmd has to track commands in the underlying stream_executor::CommandBuffer, so it can use explicit update APIs (coming next). For now update API, so that CommandBufferCmdSequence can track dependencies between recorded commands.",2025-04-02T12:14:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90467
tpu,drhaozhong,Fail to build on Ubuntu 24.10," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version master  Custom code No  OS platform and distribution Ubuntu 24.10  Mobile device _No response_  Python version 3.12  Bazel version 7.4.1  GCC/compiler version _No response_  CUDA/cuDNN version 12.6  GPU model and memory _No response_  Current behavior? bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tf_nightly config=cuda_wheel copt=Wnognuoffsetofextensions The error message is as follows: clang failed: error executing CppCompile command (from target @//:upb) /usr/lib/llvm19/bin/clang MD MF bazelout/k8opt/bin/external/upb/_objs/upb/upb.pic.d 'frandomseed=bazelout/k8opt/bin/external/upb/_objs/upb/upb.pic.o' iquote external/upb iquote ... (remaining 43 arguments skipped) external/upb/upb/upb.c:192:10: error: defining a type within 'offsetof' is a C23 extension [Werror,Wc23extensions]   192                                            ^ 1 error generated.  Standalone code to reproduce the issue ```shell No code ```  Relevant log output ```shell ```",2025-04-02T10:10:51Z,type:bug type:build/install subtype: ubuntu/linux TF 2.18,open,0,3,https://github.com/tensorflow/tensorflow/issues/90459,"Hi  , Apologies for the delay, and thank you for raising your concern here. The main cause of your issue appears to be a version compatibility mismatch. Could you please try using Clang version 18.1.8? If the issue still persists, feel free to let us know we will be happy to assist you further. I am attaching the official documentation for your reference. Please ensure all compatibility requirements are met to avoid build failures. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"A update. I change clang to 18.1.8. The previous error disappears. However, I encounter another error: gcc U_FORTIFY_SOURCE fstackprotector Wall Wunusedbutsetparameter Wnofreenonheapobject fnoomitframepointer g0 O2 'D_FORTIFY_SOURCE=1' DNDEBUG ffunctionsections ... (remaining 212 arguments skipped) In file included from /usr/include/c++/14/memory:78,                  from tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:16: /usr/include/c++/14/bits/unique_ptr.h: In instantiation of 'std::__detail::__unique_ptr_t std::make_unique(_Args&& ...) [with _Tp = mlir::TFL::{anonymous}::DefaultQuantParamsPass; _Args = {const mlir::TFL::DefaultQuantParamsPassOptions&}; __detail::__unique_ptr_t = __detail::__unique_ptr_t]': tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:249:50:   required from here   249        ^~~~~~~~~~~~~~~~~~~~~~ tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:54:7: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(mlir::TFL::{anonymous}::DefaultQuantParamsPass&&)' (deleted) cc1plus: note: unrecognized commandline option 'Wnognuoffsetofextensions' may have been intended to silence earlier diagnostics Target //tensorflow/tools/pip_package:wheel failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 4275.864s, Critical Path: 432.16s INFO: 16480 processes: 587 internal, 15893 local. ERROR: Build did NOT complete successfully My gcc version  is 14.2.0 "
int8,Tessil,[mlir][tosa] Add int8 and int16 legalization of the LOG op,"Hi, This PR adds int8 and int16 TFL > TOSA legalization for the LOG operator.",2025-04-02T09:34:07Z,size:M comp:lite-tosa,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90456
xla compile,copybara-service[bot],Make TF's runtime target explicitly depend on XLA's runtime target,"Make TF's runtime target explicitly depend on XLA's runtime target So far TF only implicitly depended on the XLA runtime through the XLA compiler target (which pulls in the runtime). We are trying to fully separate runtime and compiler, therefore things will start breaking if TF doesn't explicitly depend on the runtime which this change is doing",2025-04-02T09:21:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90455
tpu,brianGriifin114,Starlark Transition Error: Unreadable type AutoValue_ExecutionInfoModifier for build setting //command_line_option:modify_execution_info," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version master branch commit 9ebac72  Custom code Yes  OS platform and distribution Win10  Mobile device _No response_  Python version 3.12.8  Bazel version 6.5.0  GCC/compiler version Microsoft Visual Studio 2022（v17.13.4）  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am part of the MSVC testing team at Microsoft. We build popular open source projects to test the compiler for any issues. During our regular update of recent commits, we found that when building TensorFlow with Bazel, we encountered the following error, causing the build to fail: ``` ERROR: C:/gitp/tensorflow/tensorflow/tensorflow/tensorflow.bzl:2986:5: before calling _local_exec_transition_impl: Input build setting //command_line_option:modify_execution_info is of type class com.google.devtools.build.lib.analysis.config.AutoValue_ExecutionInfoModifier, which is unreadable in Starlark. Please submit a feature request. ``` It looks like before calling _local_exec_transition_impl, the build setting //command_line_option:modify_execution_info is passed a type AutoValue_ExecutionInfoModifier which Starlark cannot read. This directly blocks the subsequent Starlark transformation and build process. From the error message, the TensorFlow build rule passes an internal type that Starlark cannot resolve when calling _local_exec_transition_impl. The error message suggests submitting a feature request. I'm not sure if this is a limitation of Bazel or an adjustable part of the TensorFlow build rule. Do I need to change TensorFlow's build configuration to avoid this problem, or do I need to wait for improvements to Bazel's support for this type?  Standalone code to reproduce the issue ```shell I'm just building tensorflow, so I can't provide this ```  Relevant log output ```shell [command] Command CC(未找到相关数据) (Output in ""Build.log""):  set VSCMD_SKIP_SENDTELEMETRY=1 & ""C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\Tools\VsDevCmd.bat"" host_arch=amd64 arch=amd64 & set _CL_= /Bcapture_repro C:\output\Tensorflow\preprocessed_repro_build & set _LINK_= /onfailrepro:C:\output\Tensorflow\link_repro_build ********************************************************************** ** Visual Studio 2022 Developer Command Prompt v17.13.4 ** Copyright (c) 2022 Microsoft Corporation ********************************************************************** [debug] Command CC(未找到相关数据) exited with code [0]. [command] Command CC(Add support for Python 3.x) (Output in ""Build.log""):  set PATH=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;%PATH% [debug] Command CC(Add support for Python 3.x) exited with code [0]. [command] Command CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"") (Output in ""Build.log""):  cd /d C:\gitP\tensorflow\tensorflow\build_amd64 [debug] Command CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"") exited with code [0]. [command] Command CC(JVM, .NET Language Support) (Output in ""Build.log""):  set BAZEL_VC=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC [debug] Command CC(JVM, .NET Language Support) exited with code [0]. [command] Command CC(Installation over pip fails to import with protobuf 2.6.1) (Output in ""Build.log""):  set BAZEL_VC_FULL_VERSION=14.43.34808 [debug] Command CC(Installation over pip fails to import with protobuf 2.6.1) exited with code [0]. [command] Command CC(Java interface) (Output in ""Build.log""):  set PATH=C:\tools\msys64\usr\bin;%path% [debug] Command CC(Java interface) exited with code [0]. [command] Command CC(Pretrained models) (Output in ""Build.log""):  bazel output_user_root C:\bazelTemp build jobs 16 config=opt local_ram_resources=16384  subcommands //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tf_nightly repo_env=TF_PYTHON_VERSION=3.12 2>&1 Extracting Bazel installation... Starting local Bazel server and connecting to it... INFO: Reading 'startup' options from c:\gitp\tensorflow\tensorflow\.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'build' from c:\gitp\tensorflow\tensorflow\.bazelrc:   Inherited 'common' options: announce_rc experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility noenable_bzlmod noincompatible_enable_cc_toolchain_resolution noincompatible_enable_android_toolchain_resolution experimental_repo_remote_exec java_runtime_version=remotejdk_21 INFO: Options provided by the client:   'build' options: python_path=C:/Python312/python.exe INFO: Reading rc options for 'build' from c:\gitp\tensorflow\tensorflow\.bazelrc:   'build' options: repo_env=ML_WHEEL_TYPE=snapshot repo_env=ML_WHEEL_BUILD_DATE= repo_env=ML_WHEEL_VERSION_SUFFIX= define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive host_features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 INFO: Reading rc options for 'build' from c:\gitp\tensorflow\tensorflow\.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=C:/Python312/python.exe action_env PYTHON_LIB_PATH=C:/Python312/Lib/sitepackages python_path=C:/Python312/python.exe copt=/d2ReducedOptimizeHugeFunctions host_copt=/d2ReducedOptimizeHugeFunctions define=override_eigen_strong_inline=true INFO: Found applicable config definition build:short_logs in file c:\gitp\tensorflow\tensorflow\.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file c:\gitp\tensorflow\tensorflow\.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:opt in file c:\gitp\tensorflow\tensorflow\.tf_configure.bazelrc: copt=/arch:AVX host_copt=/arch:AVX INFO: Found applicable config definition build:windows in file c:\gitp\tensorflow\tensorflow\.bazelrc: copt=/W0 host_copt=/W0 copt=/Zc:__cplusplus host_copt=/Zc:__cplusplus copt=/D_USE_MATH_DEFINES host_copt=/D_USE_MATH_DEFINES features=compiler_param_file features=archive_param_file copt=/d2ReducedOptimizeHugeFunctions host_copt=/d2ReducedOptimizeHugeFunctions copt=D_ENABLE_EXTENDED_ALIGNED_STORAGE host_copt=D_ENABLE_EXTENDED_ALIGNED_STORAGE enable_runfiles nobuild_python_zip dynamic_mode=off cxxopt=/std:c++17 host_cxxopt=/std:c++17 config=monolithic copt=DWIN32_LEAN_AND_MEAN host_copt=DWIN32_LEAN_AND_MEAN copt=DNOGDI host_copt=DNOGDI copt=/Zc:preprocessor host_copt=/Zc:preprocessor linkopt=/DEBUG host_linkopt=/DEBUG linkopt=/OPT:REF host_linkopt=/OPT:REF linkopt=/OPT:ICF host_linkopt=/OPT:ICF verbose_failures features=compiler_param_file config=no_tfrt INFO: Found applicable config definition build:monolithic in file c:\gitp\tensorflow\tensorflow\.bazelrc: define framework_shared_object=false define tsl_protobuf_header_only=false experimental_link_static_libraries_once=false INFO: Found applicable config definition build:no_tfrt in file c:\gitp\tensorflow\tensorflow\.bazelrc: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils Loading:  Loading:  Loading:  DEBUG: C:/bazeltemp/yasrpdtv/external/local_xla/third_party/py/python_repo.bzl:87:10:  ============================= Hermetic Python configuration: Version: ""3.12"" Kind: """" Interpreter: ""default"" (provided by rules_python) Requirements_lock label: ""//:requirements_lock_3_12.txt"" ===================================== Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading: 0 packages loaded Analyzing: target //tensorflow/tools/pip_package:wheel (1 packages loaded, 0 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (6 packages loaded, 16 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (7 packages loaded, 18 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (90 packages loaded, 59 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (93 packages loaded, 59 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (93 packages loaded, 59 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (104 packages loaded, 71 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (120 packages loaded, 274 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (199 packages loaded, 623 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (241 packages loaded, 1306 targets configured) ERROR: C:/gitp/tensorflow/tensorflow/tensorflow/tensorflow.bzl:2986:5: before calling _local_exec_transition_impl: Input build setting //command_line_option:modify_execution_info is of type class com.google.devtools.build.lib.analysis.config.AutoValue_ExecutionInfoModifier, which is unreadable in Starlark. Please submit a feature request. ERROR: C:/gitp/tensorflow/tensorflow/tensorflow/python/platform/BUILD:27:25: Errors encountered while applying Starlark transition INFO: Repository go_sdk instantiated at:   C:/gitp/tensorflow/tensorflow/WORKSPACE:72:14: in    C:/gitp/tensorflow/tensorflow/tensorflow/workspace0.bzl:135:20: in workspace   C:/bazeltemp/yasrpdtv/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps   C:/bazeltemp/yasrpdtv/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains   C:/bazeltemp/yasrpdtv/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk Repository rule _go_download_sdk defined at:   C:/bazeltemp/yasrpdtv/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in  INFO: Repository pypi__pip instantiated at:   C:/gitp/tensorflow/tensorflow/WORKSPACE:33:25: in    C:/bazeltemp/yasrpdtv/external/local_xla/third_party/py/python_init_repositories.bzl:23:20: in python_init_repositories   C:/bazeltemp/yasrpdtv/external/rules_python/python/private/py_repositories.bzl:70:14: in py_repositories   C:/bazeltemp/yasrpdtv/external/rules_python/python/private/pypi/deps.bzl:133:14: in pypi_deps   C:/bazeltemp/yasrpdtv/external/bazel_tools/tools/build_defs/repo/utils.bzl:233:18: in maybe Repository rule http_archive defined at:   C:/bazeltemp/yasrpdtv/external/bazel_tools/tools/build_defs/repo/http.bzl:372:31: in  ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted:  INFO: Elapsed time: 171.190s INFO: 0 processes. FAILED: Build did NOT complete successfully (262 packages loaded, 4048 targets configured) [debug] Command CC(Pretrained models) exited with code [1]. [error] Detected error code [1]. ```",2025-04-02T06:50:31Z,type:build/install subtype:windows TF 2.18,open,0,3,https://github.com/tensorflow/tensorflow/issues/90448,"It looks like the TensorFlow master branch requires Bazel 7.4.1 (specified in the .bazelversion file), but the build attempt used Bazel 6.5.0. Could you please try the build again using Bazel 7.4.1? This version mismatch is the most likely cause. Otherwise, if the error persists even with Bazel 7.4.1, the next best step would be to use git bisect on the TensorFlow repository between the last known working commit and the failing commit (9ebac72). This will pinpoint the exact TensorFlow change that introduced the incompatibility with the build environment.","Hi  , Apologies for the delay, and thanks for raising your concern here. The main cause of the issue might be a version compatibility mismatch. The master branch requires Bazel version 7.4.1, but you are currently using 6.5.0. Could you please try with the updated Bazel version? If the issue still persists, feel free to let us know for further assistance. Here, I am attaching the official documentation for your reference. Thank you!","Thanks for your reply. I upgraded bazel to 7.4.1 and rebuilt it, it works, but a new problem occurred. ``` ERROR: C:/bazeltemp/yasrpdtv/external/pthreadpool/BUILD.bazel:33:11: Compiling src/fastpath.c failed: (Exit 2): cl.exe failed: error executing CppCompile command (from target @//:pthreadpool)    cd /d C:/bazeltemp/yasrpdtv/execroot/org_tensorflow   SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.43.34808\include;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.43.34808\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um     SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.43.34808\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Team Tools\DiagnosticsHub\Collector;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\Extensions\Microsoft\CodeCoverage.Console;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\VC\Linux\bin\ConnectionManagerExe     SET PWD=/proc/self/cwd     SET PYTHON_BIN_PATH=C:/Python312/python.exe     SET PYTHON_LIB_PATH=C:/Python312/Lib/sitepackages     SET TEMP=C:\Users\vbrianli\AppData\Local\Temp\2     SET TF2_BEHAVIOR=1     SET TMP=C:\Users\vbrianli\AppData\Local\Temp\2     SET VSLANG=1033   C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.43.34808\bin\HostX64\x64\cl.exe out/x64_windowsopt/bin/external/pthreadpool/_objs/pthreadpool/fastpath.obj.params  Configuration: f095ba6628056f28f3c839506b748e73d997fb0bbfaa819139c5aafa31afd067  Execution platform: @//:platform cl : Command line warning D9002 : ignoring unknown option 'std=c11' C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.43.34808\include\vcruntime_c11_stdatomic.h(16): fatal error C1189: error:  ""C atomics require C11 or later"" ``` I think the root cause of this error is that MSVC does not really support the C11 standard. Even if the std=c11 parameter is not passed, the default C compilation mode of MSVC still does not define __STDC_VERSION__ as 201112L, which triggers an error when including vcruntime_c11_stdatomic.h: `error: ""C atomics require C11 or later""` Currently, MSVC does not fully support atomic operations in the C11 standard, so implementations that directly rely on  are prone to problems under MSVC. Considering crossplatform compatibility, the code under MSVC may need to distinguish the atomic operation implementations of different platforms. Are there plans or existing solutions to fix this, or is it possible to improve the situation in MSVC via a patch? Thanks! ``` Platform: Windows 10 Development tools: Visual Studio 2022 Developer Command Prompt v17.13.4 Compiler: MSVC 14.43.34808 Related source code: fastpath.c (including pthreadpool.h and threadpoolatomics.h) ```"
tpu,SnappierSoap318,TFLite Compilation help with External delegates," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version Git commit hash: 0ef1f026976c533aac524836f0d120a597e74081  Custom code Yes  OS platform and distribution Raspberry Pi 5 running Bookworm  Mobile device _No response_  Python version _No response_  Bazel version Using CMake, 3.25.1  GCC/compiler version 12.2.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi, i was trying to load an external delegate using the function `LoadDelegateFromSharedLibrary` and load settings using `TfLiteSettingsJsonParser` but when i am compiling the code, it gives me a function not defined error: ``` /usr/bin/ld: CMakeFiles/bench.dir/main.cpp.o: in function `main': main.cpp:(.text+0xa90): undefined reference to `tflite::delegates::utils::LoadDelegateFromSharedLibrary(std::__cxx11::basic_string, std::allocator > const&)' /usr/bin/ld: main.cpp:(.text+0xbd0): undefined reference to `tflite::delegates::utils::TfLiteSettingsJsonParser::TfLiteSettingsJsonParser()' /usr/bin/ld: main.cpp:(.text+0xbdc): undefined reference to `tflite::delegates::utils::TfLiteSettingsJsonParser::Parse(std::__cxx11::basic_string, std::allocator > const&)' collect2: error: ld returned 1 exit status gmake[2]: *** [CMakeFiles/bench.dir/build.make:188: bench] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:1300: CMakeFiles/bench.dir/all] Error 2 gmake: *** [Makefile:136: all] Error 2 ``` Trying to check the library also doesn't show the functions being linked.  ```bash $ readelf a ./tensorflowlite/libtensorflowlite.a  grep TfLiteSettingsJsonParser ``` Can I get some guidance on how I can load and use external delegates?   Standalone code to reproduce the issue ```shell https://gist.github.com/SnappierSoap318/bd0a4dadab9e118e20465b568990fbd9 ```  Relevant log output ```shell ``` edit: formatting",2025-04-02T05:59:52Z,type:support comp:lite,open,0,9,https://github.com/tensorflow/tensorflow/issues/90441,"It looks like the linker errors you're seeing are because the external delegate symbols aren't being compiled into your build. The functions tflite::delegates::utils::LoadDelegateFromSharedLibrary and TfLiteSettingsJsonParser are conditionally compiled, so you'll need to enable them with the TFLITE_ENABLE_EXTERNAL_DELEGATE flag.",That's enabled by default when I did `cmake LAH`, any idea?,"Hi,  I apologize for the delay in my response, As far I know those are linker errors during the linking process. The linker needs to resolve these references to create a complete executable for `LoadDelegateFromSharedLibrary()` and `TfLiteSettingsJsonParser()` and I believe you've used build command with correct delegate support flags something like below  ``` cmake .. DCMAKE_BUILD_TYPE=Release \          DTFLITE_ENABLE_EXTERNAL_DELEGATE=ON \ ``` You need to have **add_subdirectory()** for LiteRT directory and link `tensorflowlite` with `target_link_libraries()`, please refer this TensorFlow Lite C++ minimal example and CMakeLists.txt If possible could you please build it from master branch instead of this specific commit https://github.com/tensorflow/tensorflow/commit/0ef1f026976c533aac524836f0d120a597e74081 and see is it resolving your issue or not ? Thank you for your cooperation and patience.","Hi , I didn't use a specific commit, I pulled the latest commit from the master branch at that time. Will try the cmake part and get back to you","Hi ,  Yes, my cmake file is the same (albeit changed to match the project). I followed this guide.  I'm not using any flex operations, so I haven't added the flex subdirectory inside my CMakeLists.txt:  ```cmake cmake_minimum_required(VERSION 3.16) project(bench C CXX) set(TENSORFLOW_SOURCE_DIR """" CACHE PATH   ""Directory that contains the TensorFlow project"" ) if(NOT TENSORFLOW_SOURCE_DIR)   get_filename_component(TENSORFLOW_SOURCE_DIR     ""${CMAKE_CURRENT_LIST_DIR}/../../../../"" ABSOLUTE) endif() add_subdirectory(   ""${TENSORFLOW_SOURCE_DIR}/tensorflow/lite""   ""${CMAKE_CURRENT_BINARY_DIR}/tensorflowlite"" EXCLUDE_FROM_ALL) set(CMAKE_CXX_STANDARD 17) add_executable(bench main.cpp) target_link_libraries(bench     tensorflowlite ) ``` Was built with the command:  ```bash cmake ../ DTENSORFLOW_SOURCE_DIR=/home/pi/Desktop/cross/tensorflow_src DTFLITE_HOST_TOOLS_DIR=../flatcnativebuild DTFLITE_ENABLE_EXTERNAL_DELEGATE=ON ``` Are there any additions to the cmake file so it can link those required libraries? PS:  I tried manually building these using bazel and added the generated `so`  and `.a` files to a libs folder, I tried linking them in cmake by explicitly adding their absolute paths using `add_libraries()` and `target_link_libraries()` but that gave me other linker errors. ","Hi,   If possible could you please give it try with below command and modified **CMakeLists.txt** file and see is it working or not ? If not if possible please give it try with `TensorFlow v2.15.0` version and see is it resolving your issue or not ? ``` bash  Clean build directory rm rf build && mkdir build && cd build  Configure with explicit paths cmake .. \   DTENSORFLOW_SOURCE_DIR=/home/pi/Desktop/cross/tensorflow_src \   DTFLITE_HOST_TOOLS_DIR=../flatcnativebuild \   DTFLITE_ENABLE_EXTERNAL_DELEGATE=ON \   DBUILD_SHARED_LIBS=OFF   Better for Pi 5 deployment  Build with 4 threads (adjust based on Pi 5's RAM) make j4 ``` **Modified CMakeLists.txt :** ``` cmake cmake_minimum_required(VERSION 3.16) project(bench C CXX) set(TENSORFLOW_SOURCE_DIR """" CACHE PATH   ""Directory that contains the TensorFlow project"") if(NOT TENSORFLOW_SOURCE_DIR)   get_filename_component(TENSORFLOW_SOURCE_DIR     ""${CMAKE_CURRENT_LIST_DIR}/../../../../"" ABSOLUTE) endif()  Add these before tensorflow/lite subdirectory set(TFLITE_ENABLE_EXTERNAL_DELEGATE ON CACHE BOOL ""Enable external delegates"") set(BUILD_SHARED_LIBS OFF CACHE BOOL ""Build static libraries"")  Explicitly include delegates/utils sources add_subdirectory(   ""${TENSORFLOW_SOURCE_DIR}/tensorflow/lite/delegates/utils""   ""${CMAKE_CURRENT_BINARY_DIR}/tensorflowlitedelegatesutils""   EXCLUDE_FROM_ALL ) add_subdirectory(   ""${TENSORFLOW_SOURCE_DIR}/tensorflow/lite""   ""${CMAKE_CURRENT_BINARY_DIR}/tensorflowlite""   EXCLUDE_FROM_ALL ) set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) add_executable(bench main.cpp) target_link_libraries(bench   tensorflowlite   tensorflowlitedelegatesutils   pthread   dl ) ``` Thank you for your cooperation and patience.",I get a cmake error:  ```bash CMake Error at CMakeLists.txt:16 (add_subdirectory):   The source directory     /home/pi/Desktop/cross/tensorflow_src/tensorflow/lite/delegates/utils   does not contain a CMakeLists.txt file. ``` ```bash $ ls /home/pi/Desktop/cross/tensorflow_src/tensorflow/lite/delegates/utils async_type_helpers....h        sync_fence...h   dummy_delegate  ret_macros.h  simple_delegate.h   simple_opaque_delegate...h   utils.h, anything?
tpu,copybara-service[bot],Add `CustomCombiner` to TPU embedding V2 API.,Add `CustomCombiner` to TPU embedding V2 API.,2025-04-02T01:59:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90418
sharding,copybara-service[bot],Move the test-only `sharding_format_picker.cc` next to the test that uses it.,"Move the testonly `sharding_format_picker.cc` next to the test that uses it. While working to improve `xla/hlo/` component coverage, we found that this file was testonly and used exclusively by a test in the `xla/service/` component. We're therefore moving it into `xla/service/`, next to the test that uses it.",2025-04-02T00:26:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90416
sharding,copybara-service[bot],[PjRt-IFRT] Support a mix of addressable and non-addressable devices in Array creation methods,"[PjRtIFRT] Support a mix of addressable and nonaddressable devices in Array creation methods `xla::ifrt::PjRtClient::MakeArrayFromHostBuffer()` and `...::MakeErrorArrays()` now support creating multishard Array(s) that contains nonaddressable devices, as long as the sharding is fully replicated (preexistent condition). This does not change the current requirement that `xla::ifrt::PjRtClient::MakeArrayFromHostBuffer()` should have at least one addressable device (which may be relaxed in the future, but not in this change). Bumping `JAX_IFRT_VERSION_NUMBER` because when JAX relies on this feature, the old implementation will fail array creation with nonaddressable shards, and thus JAX has to take a fallback path based on the version.",2025-04-01T22:49:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90412
sharding,copybara-service[bot],[PjRt-IFRT] Treat IFRT HloSharding with a single tile as a fully replicated sharding,"[PjRtIFRT] Treat IFRT HloSharding with a single tile as a fully replicated sharding `xla::ifrt::HloSharding` that has a single tile would have the shard buffer to be the same as the global array both in the content and the shape. Thus, we treat it as a fully replicated sharding in the context of IFRT APIs even though `HloSharding` does not explicitly say full replication in the context of the XLA `HloSharding`. This allows taking a runtime path for array creation specialized for fully replicated sharding for a singletile `HloSharding`.",2025-04-01T22:48:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90411
opt,copybara-service[bot],[MLIR] Add additional optimize patterns for GeluOp approximation.,"[MLIR] Add additional optimize patterns for GeluOp approximation. The existing patterns look for pow(x, 3). Adding additional patterns that check mul(mul(x, x), x) and mul(x, mul(x, x))",2025-04-01T19:37:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90401
fp16,copybara-service[bot],PR #22541: [ROCm] Cleanup atomics support,"PR CC(Java process crashes during model loading): [ROCm] Cleanup atomics support Imported from GitHub PR https://github.com/openxla/xla/pull/22541 Weaken the ordering barriers to match what atomicAdd does on rocm. Emulate fp16 atomic on top of packed fp16 atomic where possible. Also for bfloat16 atomics, albeit those don't get matched right now due to FloatNormalization. Left in support for fp16 and bfloat16 vector atomics. We might enable the vectorization for them in the future if we can prove the access satisfies 4byte aligment. Copybara import of the project:  b53668020f946207aa879eecd8e0b70173f75570 by Dragan Mladjenovic : [ROCm] Cleanup atomics support Merging this change closes CC(Java process crashes during model loading) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22541 from ROCm:atomics_cleanup b53668020f946207aa879eecd8e0b70173f75570",2025-04-01T18:07:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90395
tpu,adrianpbe,Error in nested inputs recurrent neural networks when using layer.RNN," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version v2.18.0rc24g6550e4bd802  Custom code No  OS platform and distribution WSL2 Ubuntu 22.04.1 LTS  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I've been having problems building a custom RNN that takes nested inputs. I've replicated the error with this  documentation example of a custom nested inputs RNN. While the nested RNN cell works fine, when using the `layers.RNN` wrapper it fails, it seems that input_shape is not correctly passed to the nested cell build method. While I've found the problem with version 2.18.0, I've also tested 2.16.2 and tf.nightly and it keeps happening, it can also be replicated in Colab.  Standalone code to reproduce the issue ```shell import tensorflow as tf import tensorflow.keras as keras class NestedCell(keras.layers.Layer):     def __init__(self, unit_1, unit_2, unit_3, **kwargs):         self.unit_1 = unit_1         self.unit_2 = unit_2         self.unit_3 = unit_3         self.state_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]         self.output_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]         super().__init__(**kwargs)     def build(self, input_shapes):          expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]         i1 = input_shapes[0][1]         i2 = input_shapes[1][1]         i3 = input_shapes[1][2]         self.kernel_1 = self.add_weight(             shape=(i1, self.unit_1), initializer=""uniform"", name=""kernel_1""         )         self.kernel_2_3 = self.add_weight(             shape=(i2, i3, self.unit_2, self.unit_3),             initializer=""uniform"",             name=""kernel_2_3"",         )     def call(self, inputs, states):          inputs should be in [(batch, input_1), (batch, input_2, input_3)]          state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]         input_1, input_2 = tf.nest.flatten(inputs)         s1, s2 = states         output_1 = tf.matmul(input_1, self.kernel_1)         output_2_3 = tf.einsum(""bij,ijkl>bkl"", input_2, self.kernel_2_3)         state_1 = s1 + output_1         state_2_3 = s2 + output_2_3         output = (output_1, output_2_3)         new_states = (state_1, state_2_3)         return output, new_states     def get_config(self):         return {""unit_1"": self.unit_1, ""unit_2"": self.unit_2, ""unit_3"": self.unit_3} unit_1 = 10 unit_2 = 20 unit_3 = 30 i1 = 32 i2 = 64 i3 = 32 batch_size = 64 num_batches = 10 timestep = 50 cell = NestedCell(unit_1, unit_2, unit_3) rnn = keras.layers.RNN(cell) input_1 = keras.Input((None, i1)) input_2 = keras.Input((None, i2, i3)) outputs = rnn((input_1, input_2)) model = keras.models.Model([input_1, input_2], outputs) model.compile(optimizer=""adam"", loss=""mse"", metrics=[""accuracy""]) ```  Relevant log output ```shell 20250401 19:19:49.186743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1743527989.197181   20234 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1743527989.200246   20234 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250401 19:19:49.210967: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. Traceback (most recent call last):   File ""/home/user/projects/dl_stash/reproducing_bug.py"", line 67, in      outputs = rnn((input_1, input_2))               ^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/user/projects/dl_stash/.venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/home/user/projects/dl_stash/reproducing_bug.py"", line 18, in build     i2 = input_shapes[1][1]          ~~~~~~~~~~~~^^^ IndexError: tuple index out of range ```",2025-04-01T17:32:46Z,type:bug TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/90389,"I was able to reproduce this issue using TensorFlow 2.19.0, and the nightly version. Please find the gist attached for reference. Thank you!"
tpu,plopresti,Please allow configure to accept empty environment variables," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version master  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version N/A  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? `env LOCAL_CUDA_PATH= ./configure` ...still generates an interactive prompt ""Please specify the local CUDA path you want to use or leave empty to use the default version"". When I explicitly set LOCAL_CUDA_PATH to an empty string, I expect `configure` to behave as if I had left this setting empty interactively. (This is useful for running `configure` from another script.) Of course this applies to every setting controlled by an environment variable, including LOCAL_CUDNN_PATH, LOCAL_NCCL_PATH, HERMETIC_CUDA_VERSION, etc. The following trivial patch to get_from_env_or_user_or_default() implements this feature: ``` diff git a/configure.py b/configure.py index ec04fcfdd0c..4b21295e7e6 100644  a/configure.py +++ b/configure.py @@ 529,7 +529,7 @@ def get_from_env_or_user_or_default(environ_cp, var_name, ask_for_var,      string value for var_name    """"""    var = environ_cp.get(var_name)   if not var: +  if var is None:      var = get_input(ask_for_var)      print('\n')    if not var: ```  Standalone code to reproduce the issue ```shell `env LOCAL_CUDA_PATH= ./configure` ```  Relevant log output ```shell ```",2025-04-01T15:17:50Z,type:feature,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90383,Whoops. Just noticed this was fixed by commit ac0fca8559c2384240a00599a46816bbb5afb93f
yi,copybara-service[bot],"Copy insertion will elide unnecessary copies based on checking the live range of the current schedule.  Passes after copy-insertion might duplicate/reschedule nodes in a way that causes live range overlap and result in runtime corruption, this change adds dependencies to explicitly prevent that.","Copy insertion will elide unnecessary copies based on checking the live range of the current schedule.  Passes after copyinsertion might duplicate/reschedule nodes in a way that causes live range overlap and result in runtime corruption, this change adds dependencies to explicitly prevent that.",2025-04-01T13:36:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90378
tpu,copybara-service[bot],Add output tile selection logic to dynamic search space,Add output tile selection logic to dynamic search space,2025-04-01T13:36:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90377
xla compile,copybara-service[bot],Make xla compiler factory return absl::StatusOr.,Make xla compiler factory return absl::StatusOr. This should allows us to cacth errors during construction of the compiler objects.,2025-04-01T12:29:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90374
opt,copybara-service[bot],[XLA:GPU] Add triton support test for optim-barrier,[XLA:GPU] Add triton support test for optimbarrier Can be treated as an unary op,2025-04-01T10:48:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90372
tpu,copybara-service[bot],Reland Adjust PriorityFusion to allow forming simple multi-output Triton fusions.,Reland Adjust PriorityFusion to allow forming simple multioutput Triton fusions. Reverts a7ace96a10521859b80d9e3bbe041538b9fcc333,2025-04-01T10:15:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90371
tpu,dependabot[bot],Bump the github-actions group with 5 updates,"Bumps the githubactions group with 5 updates:          includegitroot         recursive         ./  Full Changelog: https://github.com/google/osvscanneraction/compare/v1.9.2...v2.0.0    Commits  98b584e Merge pull request  CC(Specify output tensor for ops from python) from renovatebot/renovate/workflows 256cd6a chore(deps): update github/codeqlaction action to v3.28.11 90fad54 Merge pull request  CC(Mac: OSError: [Errno 1] Operation not permitted: '/tmp/pipXcfgD6uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six1.4.1py2.7.egginfo') from google/updatetov2.0.0 f9d9b03 Include git root 6e516af Update unified workflow example to point to v2.0.0 reusable workflows 4299e5f Update reusable workflows to point to v2.0.0 actions 119c605 Update actions to use v2.0.0 osvscanner image 4d317bc Merge pull request  CC(CUDNN error on ""import tensorflow as tf"" for gpu version) from AbhishekSrikanth/fixreleasebadge b3fa036 fix: escape hyphen in release badge URL 38fe519 Merge pull request  CC(Try to convert to readable latex) from google/fixremainingskipgit Additional commits viewable in compare view    Updates `actions/setuppython` from 5.4.0 to 5.5.0  Release notes Sourced from actions/setuppython's releases.  v5.5.0 What's Changed Enhancements:  Support free threaded Python versions like '3.13t' by @​colesbury in actions/setuppython CC(Fix wrong variable name (pickle_name) in 1_notmnist.ipynb) Enhance Workflows: Include ubuntuarm runners, Add e2e Testing for free threaded and Upgrade @​action/cache from 4.0.0 to 4.0.3 by @​priyakinthali in actions/setuppython CC(Optimized build across different architecture) Add support for .toolversions file in setuppython by @​mahabaleshwars in actions/setuppython CC(Alexnet Multi GPU)  Bug fixes:  Fix architecture for pypy on Linux ARM64 by @​mayeut in actions/setuppython CC(Adding option for parallel build and serial tests) This update maps arm64 to aarch64 for Linux ARM64 PyPy installations.  Dependency updates:  Upgrade @​vercel/ncc from 0.38.1 to 0.38.3 by @​dependabot in actions/setuppython CC(Incorrect RNN documentation) Upgrade @​actions/glob from 0.4.0 to 0.5.0 by @​dependabot in actions/setuppython CC(Error when feeding model)  New Contributors  @​colesbury made their first contribution in actions/setuppython CC(Fix wrong variable name (pickle_name) in 1_notmnist.ipynb) @​mahabaleshwars made their first contribution in actions/setuppython CC(Alexnet Multi GPU)  Full Changelog: https://github.com/actions/setuppython/compare/v5...v5.5.0    Commits  8d9ed9a Add e2e Testing for free threaded and Bump @​action/cache from 4.0.0 to 4.0.3 ... 19e4675 Add support for .toolversions file in setuppython ( CC(Alexnet Multi GPU)) 6fd11e1 Bump @​actions/glob from 0.4.0 to 0.5.0 ( CC(Error when feeding model)) 9e62be8 Support free threaded Python versions like '3.13t' ( CC(Fix wrong variable name (pickle_name) in 1_notmnist.ipynb)) 6ca8e85 Bump @​vercel/ncc from 0.38.1 to 0.38.3 ( CC(Incorrect RNN documentation)) 8039c45 fix: install PyPy on Linux ARM64 ( CC(Adding option for parallel build and serial tests)) See full diff in compare view    Updates `peterevans/createpullrequest` from 7.0.7 to 7.0.8  Release notes Sourced from peterevans/createpullrequest's releases.  Create Pull Request v7.0.8 What's Changed  build(depsdev): bump tsjest from 29.2.5 to 29.2.6 by @​dependabot in peterevans/createpullrequest CC(Grid3LSTMCell running out of memory ) build(depsdev): bump eslintimportresolvertypescript from 3.8.1 to 3.8.3 by @​dependabot in peterevans/createpullrequest CC(Error: OK vs. Unimplemented: Explict cast of a nonempty tensor not implemented yet) build(deps): bump @​octokit/pluginpaginaterest from 11.4.2 to 11.4.3 by @​dependabot in peterevans/createpullrequest CC(Branch 130016968) build(depsdev): bump prettier from 3.5.1 to 3.5.2 by @​dependabot in peterevans/createpullrequest CC(Unresolved RNN performance issue) fix: suppress output for some git operations by @​peterevans in peterevans/createpullrequest CC(error with wide_n_deep_tutorial.py)  Full Changelog: https://github.com/peterevans/createpullrequest/compare/v7.0.7...v7.0.8    Commits  271a8d0 fix: suppress output for some git operations ( CC(error with wide_n_deep_tutorial.py)) 6f7efd1 test: update cprexamplecommand 13c47c5 build(depsdev): bump prettier from 3.5.1 to 3.5.2 ( CC(Unresolved RNN performance issue)) 63e5829 build(deps): bump @​octokit/pluginpaginaterest from 11.4.2 to 11.4.3 ( CC(Branch 130016968)) a92c90f build(depsdev): bump eslintimportresolvertypescript ( CC(Error: OK vs. Unimplemented: Explict cast of a nonempty tensor not implemented yet)) b23b62d build(depsdev): bump tsjest from 29.2.5 to 29.2.6 ( CC(Grid3LSTMCell running out of memory )) See full diff in compare view    Updates `actions/uploadartifact` from 4.6.1 to 4.6.2  Release notes Sourced from actions/uploadartifact's releases.  v4.6.2 What's Changed  Update to use artifact 2.3.2 package &amp; prepare for new uploadartifact release by @​salmanmkc in actions/uploadartifact CC(Could contens on tensorflow.org be mirrored on github or other places other than google server? )  New Contributors  @​salmanmkc made their first contribution in actions/uploadartifact CC(Could contens on tensorflow.org be mirrored on github or other places other than google server? )  Full Changelog: https://github.com/actions/uploadartifact/compare/v4...v4.6.2    Commits  ea165f8 Merge pull request  CC(Could contens on tensorflow.org be mirrored on github or other places other than google server? ) from salmanmkc/salmanmkc/3newuploadartifactsrelease 0839620 Prepare for new release of actions/uploadartifact with new toolkit cache ver... See full diff in compare view    Updates `github/codeqlaction` from 3.28.10 to 3.28.13  Release notes Sourced from github/codeqlaction's releases.  v3.28.13 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.13  24 Mar 2025 No user facing changes. See the full CHANGELOG.md for more information. v3.28.12 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.12  19 Mar 2025  Dependency caching should now cache more dependencies for Java buildmode: none extractions. This should speed up workflows and avoid inconsistent alerts in some cases. Update default CodeQL bundle version to 2.20.7.  CC(CUDA_ERROR_MISALIGNED_ADDRESS on MNIST example )  See the full CHANGELOG.md for more information. v3.28.11 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.11  07 Mar 2025  Update default CodeQL bundle version to 2.20.6.  CC(Clipping gradient w.r.t. inputs at each time step for RNN/LSTM)  See the full CHANGELOG.md for more information.    Changelog Sourced from github/codeqlaction's changelog.  CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. [UNRELEASED] No user facing changes. 3.28.13  24 Mar 2025 No user facing changes. 3.28.12  19 Mar 2025  Dependency caching should now cache more dependencies for Java buildmode: none extractions. This should speed up workflows and avoid inconsistent alerts in some cases. Update default CodeQL bundle version to 2.20.7.  CC(CUDA_ERROR_MISALIGNED_ADDRESS on MNIST example )  3.28.11  07 Mar 2025  Update default CodeQL bundle version to 2.20.6.  CC(Clipping gradient w.r.t. inputs at each time step for RNN/LSTM)  3.28.10  21 Feb 2025  Update default CodeQL bundle version to 2.20.5.  CC(Please consider adding flatten) Address an issue where the CodeQL Bundle would occasionally fail to decompress on macOS.  CC(Checkpoint Restore blocked by changed default bias variable name)  3.28.9  07 Feb 2025  Update default CodeQL bundle version to 2.20.4.  CC(C++ compilation of rule '//:sip_hash' failed (Tensorflow serving on Android))  3.28.8  29 Jan 2025  Enable support for Kotlin 2.1.10 when running with CodeQL CLI v2.20.3.  CC(Fix for build issue 2742;)  3.28.7  29 Jan 2025 No user facing changes. 3.28.6  27 Jan 2025  Reenable debug artifact upload for CLI versions 2.20.3 or greater.  CC(Modifying MNIST example to distributed version: could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR)  3.28.5  24 Jan 2025  Update default CodeQL bundle version to 2.20.3.  CC(Branch 124290852)  3.28.4  23 Jan 2025 No user facing changes.   ... (truncated)   Commits  1b549b9 Merge pull request  CC(Fixed bug in train_test_split operation.) from github/updatev3.28.13e0ea14102 82630c8 Update changelog for v3.28.13 e0ea141 Merge pull request  CC(tf.gradients casts away imaginary part of result when differentiating by a real value ) from github/cklin/emptyprdiffrange b361a91 Diffinformed analysis: fix empty PR handling bd1d9ab Merge pull request  CC(Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two failed) from github/cklin/overlayfilelist b98ae6c Add overlaydatabaseutils tests 9825184 Add getFileOidsUnderPath() tests ac67cff Merge pull request  CC([learn] update TensorFlowEstimator's functions (fit, partial_fit, predict)) from github/cklin/defaultsetupdiffinformed 9c674ba build: refresh js files d109dd5 Detect PR branches for Default Setup Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore  major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)  ` ignore  minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)  ` ignore ` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)  ` unignore ` will remove all of the ignore conditions of the specified dependency  ` unignore  ` will remove the ignore condition of the specified dependency and ignore conditions ",2025-04-01T08:48:52Z,ready to pull size:S dependencies github_actions,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90365
tpu,KangSukWoo1,TypeError in site-packages path detection when sys.path includes PosixPath (e.g. Streamlit)," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version Version: 2.16.1  Custom code Yes  OS platform and distribution CentOS Stream release 9  Mobile device _No response_  Python version 3.11.5   Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When running TensorFlow inside a Streamlit app, the following error occurs:  Standalone code to reproduce the issue ```shell I could not create a minimal reproducible example that triggers the issue 100% of the time, but this error consistently occurs in my actual project environment. It seems to happen when TensorFlow is used inside a Streamlit app, and `sys.path` contains `PosixPath` objects. Here’s a simplified version of the code I run: import streamlit as st import tensorflow as tf st.write(tf.__version__) ```  Relevant log output ```shell TypeError: argument of type 'PosixPath' is not iterable ```",2025-04-01T06:20:25Z,type:bug subtype:centos awaiting PR merge TF 2.16,open,0,1,https://github.com/tensorflow/tensorflow/issues/90353,I'll be working on this and submitting a PR soon.
int8,RahulSundarMCW,"feat: add datatype support for add, ceil,  mul, range, sign, sub","* TfLite add missing datatype support   Adds bf16, f16 support sub function   Adds bf16, f16 unit tests   Add nonquantized int8 type support and tests   Include check for broadcasting * Tflite ceil missing datatypes support   Adds bf16,f16,i8,i16,i32 for tflite ceil operations   Adds bf16,f16,i8,i16,i32 ceil unit tests   Add unquantized int8 support * TfLite Mul add missing datatype support   Adds f16,bf16 for mul   Adds f16,bf16  unit tests   Include nonquantized int8 type support * Tflite range missing datatypes support   Adds i8,i16,bf16,f16 for tflite range   Adds i8,i16,bf16,f16 range unit tests   Adds type support for nonquantized int8 and int16 * Tflite sign missing datatype support   Adds i16,f16,bf16 for tflite sign   Adds i16,f16,bf16 tflite sign unit tests   Add nonquantized int8 type support * TfLite sub missing datatype support   Adds bf16, f16 support sub function   Adds bf16, f16 unit tests   Include nonquantized int8&int16 type support * Includes EIGEN_TFLITE flag to resolve CONV Error",2025-04-01T05:49:32Z,comp:lite size:XL,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90351,This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.
tpu,copybara-service[bot],[XLA] Change AlignLayout to preserve relative order of dimensions when input shape is a subset of the output shape,"[XLA] Change AlignLayout to preserve relative order of dimensions when input shape is a subset of the output shape AlignLayout works by dropping all the degenerate dimensions before assigning the layout to the remaining dimensions so that they match. Then readds degenerate dimensions in the inferred shape at the end. If there are though degenerate dimensions in between the nondegenerate dimensions in the input layout then this is not going to be used to align the two layouts and can get in a situation like this: input: bf16[4,1,4096]{2,1,0} output shape: bf16[4,4096,1,1] inferred output: bf16[4,4096,1,1]{1,0,3,2} but instead we would like to infer: better inferred output: bf16[4,4096,1,1]{1,2,0,3} to better represent the fact that a ""1""dimension is present in the input layout between 4096 and 4.",2025-04-01T03:58:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90343
sharding,copybara-service[bot],Introduce `Client::MakeErrorArrays` for creating poisoned IFRT arrays,"Introduce `Client::MakeErrorArrays` for creating poisoned IFRT arrays This is useful primarily for tests that exercise error behavior. The API assumes that the sharding at least supports `Sharding::GetShardShape()`, which seems to be a reasonable assumption for the target use case and aligns with the general direction of IFRT sharding.",2025-04-01T02:39:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90342
tpu,aravindhbalaji1985,Build issues with local cuda installation: Target 'cuda_runtime' not declared in package 'cuda' defined in local_config_cuda/cuda/BUILD," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version r2.18  Custom code No  OS platform and distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.12.8  Bazel version 6.5.0  GCC/compiler version 11  CUDA/cuDNN version 12.8.1  GPU model and memory L40S  Current behavior? When compiling the source code of Tensorflow_v2.18 using the locally installed CUDA versions inside a Nvidia docker(nvcr.io/nvidia/cuda:12.8.1cudnndevelubuntu22.04) using clang17 tfBuildIssue.tar.gz , I observe that the compilation fails with the following error.  `load(""//third_party/gpus:cuda_configure.bzl"", ""cuda_configure"") cuda_configure(name = ""local_config_cuda"") load(""//third_party/nccl:nccl_configure.bzl"", ""nccl_configure"") nccl_configure(name = ""local_config_nccl"") ` **Error**:  `ERROR: /opt/tensorflow/tensorflow/compiler/tf2xla/ops/BUILD:49:21: no such target '//cuda:cuda_runtime': target 'cuda_runtime' not declared in package 'cuda' defined by /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/local_config_cuda/cuda/BUILD`  Standalone code to reproduce the issue ```shell Replace WORKSPACE file with the one attached.  Replace .bazelrc file with the one attached.  Use .tf_configure.bazelrc with the one attached.  Compile command: bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow config=cuda config=cuda_wheel config=opt ```  Relevant log output ```shell root:/opt/tensorflow bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow config=cuda config=cuda_wheel config=opt WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Reading 'startup' options from /opt/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=291 INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:   'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=/usr/bin/python action_env PYTHON_LIB_PATH=/usr/local/lib/python3.12/sitepackages python_path=/usr/bin/python action_env LD_LIBRARY_PATH=/usr/local/cuda/lib64 action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64linuxgnugcc11 config=cuda INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:cuda in file /opt/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /opt/tensorflow/.tf_configure.bazelrc: repo_env TF_CUDA_COMPUTE_CAPABILITIES=8.0,8.6,8.9 repo_env LOCAL_CUDA_PATH=/usr/local/cuda12.8/ repo_env LOCAL_CUDNN_PATH=/usr/ repo_env LOCAL_NCCL_PATH=/usr/ INFO: Found applicable config definition build:cuda in file /opt/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /opt/tensorflow/.tf_configure.bazelrc: repo_env TF_CUDA_COMPUTE_CAPABILITIES=8.0,8.6,8.9 repo_env LOCAL_CUDA_PATH=/usr/local/cuda12.8/ repo_env LOCAL_CUDNN_PATH=/usr/ repo_env LOCAL_NCCL_PATH=/usr/ INFO: Found applicable config definition build:cuda_wheel in file /opt/tensorflow/.bazelrc: //cuda:include_cuda_libs=false INFO: Found applicable config definition build:opt in file /opt/tensorflow/.tf_configure.bazelrc: copt=Wnoaddressofpackedmember host_copt=Wnoaddressofpackedmember copt=Wnodefaultedfunctiondeleted host_copt=Wnodefaultedfunctiondeleted copt=Wnoenumcompareswitch host_copt=Wnoenumcompareswitch copt=Wnoexpansiontodefined host_copt=Wnoexpansiontodefined copt=Wnoignoredattributes host_copt=Wnoignoredattributes copt=Wnoignoredqualifiers host_copt=Wnoignoredqualifiers copt=Wnoinconsistentmissingoverride host_copt=Wnoinconsistentmissingoverride copt=Wnointinboolcontext host_copt=Wnointinboolcontext copt=Wnomisleadingindentation host_copt=Wnomisleadingindentation copt=Wnopotentiallyevaluatedexpression host_copt=Wnopotentiallyevaluatedexpression copt=Wnopsabi host_copt=Wnopsabi copt=Wnorangeloopanalysis host_copt=Wnorangeloopanalysis copt=Wnoreturnstdmove host_copt=Wnoreturnstdmove copt=Wnosizeofpointerdiv host_copt=Wnosizeofpointerdiv copt=Wnosizeofarraydiv host_copt=Wnosizeofarraydiv copt=Wnostringconcatenation host_copt=Wnostringconcatenation copt=Wnotautologicalconstantcompare host_copt=Wnotautologicalconstantcompare copt=Wnotautologicaltypelimitcompare host_copt=Wnotautologicaltypelimitcompare copt=Wnotautologicalundefinedcompare host_copt=Wnotautologicalundefinedcompare copt=Wnotautologicalunsignedzerocompare host_copt=Wnotautologicalunsignedzerocompare copt=Wnotautologicalunsignedenumzerocompare host_copt=Wnotautologicalunsignedenumzerocompare copt=Wnoundefinedfunctemplate host_copt=Wnoundefinedfunctemplate copt=Wnounusedlambdacapture host_copt=Wnounusedlambdacapture copt=Wnounusedlocaltypedef host_copt=Wnounusedlocaltypedef copt=Wnovoidpointertointcast host_copt=Wnovoidpointertointcast copt=Wnouninitializedconstreference host_copt=Wnouninitializedconstreference copt=Wnocompoundtokensplit host_copt=Wnocompoundtokensplit copt=Wnoambiguousmembertemplate host_copt=Wnoambiguousmembertemplate copt=Wnocharsubscripts host_copt=Wnocharsubscripts copt=Wnoerror=deprecateddeclarations host_copt=Wnoerror=deprecateddeclarations copt=Wnoexternccompat host_copt=Wnoexternccompat copt=Wnognualignofexpression host_copt=Wnognualignofexpression copt=Wnognuvariablesizedtypenotatend host_copt=Wnognuvariablesizedtypenotatend copt=Wnoimplicitintfloatconversion host_copt=Wnoimplicitintfloatconversion copt=Wnoinvalidsourceencoding host_copt=Wnoinvalidsourceencoding copt=Wnomismatchedtags host_copt=Wnomismatchedtags copt=Wnopointersign host_copt=Wnopointersign copt=Wnoprivateheader host_copt=Wnoprivateheader copt=Wnosigncompare host_copt=Wnosigncompare copt=Wnosignedunsignedwchar host_copt=Wnosignedunsignedwchar copt=Wnostrictoverflow host_copt=Wnostrictoverflow copt=Wnotrigraphs host_copt=Wnotrigraphs copt=Wnounknownpragmas host_copt=Wnounknownpragmas copt=Wnounusedconstvariable host_copt=Wnounusedconstvariable copt=Wnounusedfunction host_copt=Wnounusedfunction copt=Wnounusedprivatefield host_copt=Wnounusedprivatefield copt=Wnouserdefinedwarnings host_copt=Wnouserdefinedwarnings copt=Wvla host_copt=Wvla copt=Wnoreserveduserdefinedliteral host_copt=Wnoreserveduserdefinedliteral copt=Wnoreturntypeclinkage host_copt=Wnoreturntypeclinkage copt=Wnoselfassignoverloaded host_copt=Wnoselfassignoverloaded copt=Woverloadedvirtual host_copt=Woverloadedvirtual copt=Wnonvirtualdtor host_copt=Wnonvirtualdtor copt=Wnodeprecated host_copt=Wnodeprecated copt=Wnoinvalidoffsetof host_copt=Wnoinvalidoffsetof copt=Wimplicitfallthrough host_copt=Wimplicitfallthrough copt=Wnofinaldtornonfinalclass host_copt=Wnofinaldtornonfinalclass copt=Wnoc++20designator host_copt=Wnoc++20designator copt=Wnoregister host_copt=Wnoregister copt=Wnodynamicexceptionspec host_copt=Wnodynamicexceptionspec INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: host_copt=w copt=Wnoall copt=Wnoextra copt=Wnodeprecated copt=Wnodeprecateddeclarations copt=Wnoignoredattributes copt=Wnoarraybounds copt=Wunusedresult copt=Werror=unusedresult copt=Wswitch copt=Werror=switch define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=dynamic_kernels experimental_guard_against_concurrent_changes INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS DEBUG: /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/local_tsl/third_party/py/python_repo.bzl:154:14:  HERMETIC_PYTHON_VERSION variable was not set correctly, using default version. Python 3.12 will be used. To select Python version, either set HERMETIC_PYTHON_VERSION env variable in your shell:   export HERMETIC_PYTHON_VERSION=3.12 OR pass it as an argument to bazel command directly or inside your .bazelrc file:   repo_env=HERMETIC_PYTHON_VERSION=3.12 DEBUG: /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/local_tsl/third_party/py/python_repo.bzl:87:10:  ============================= Hermetic Python configuration: Version: ""3.12"" Kind: """" Interpreter: ""default"" (provided by rules_python) Requirements_lock label: ""//:requirements_lock_3_12.txt"" ===================================== WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. ERROR: /opt/tensorflow/tensorflow/compiler/tf2xla/ops/BUILD:49:21: no such target '//cuda:cuda_runtime': target 'cuda_runtime' not declared in package 'cuda' defined by /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/local_config_cuda/cuda/BUILD (Tip: use `query ""//cuda:*""` to see all the targets in that package) and referenced by '//tensorflow/compiler/tf2xla/ops:_xla_ops.so' INFO: Repository jsoncpp_git instantiated at:   /opt/tensorflow/WORKSPACE:64:14: in    /opt/tensorflow/tensorflow/workspace2.bzl:941:21: in workspace   /opt/tensorflow/tensorflow/workspace2.bzl:480:20: in _tf_repositories   /opt/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /opt/tensorflow/third_party/repo.bzl:89:35: in  INFO: Repository pybind11 instantiated at:   /opt/tensorflow/WORKSPACE:64:14: in    /opt/tensorflow/tensorflow/workspace2.bzl:941:21: in workspace   /opt/tensorflow/tensorflow/workspace2.bzl:779:20: in _tf_repositories   /opt/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /opt/tensorflow/third_party/repo.bzl:89:35: in  INFO: Repository eigen_archive instantiated at:   /opt/tensorflow/WORKSPACE:64:14: in    /opt/tensorflow/tensorflow/workspace2.bzl:934:28: in workspace   /opt/tensorflow/tensorflow/workspace2.bzl:70:11: in _initialize_third_party   /opt/tensorflow/third_party/eigen3/workspace.bzl:14:20: in repo   /opt/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /opt/tensorflow/third_party/repo.bzl:89:35: in  INFO: Repository pybind11_abseil instantiated at:   /opt/tensorflow/WORKSPACE:64:14: in    /opt/tensorflow/tensorflow/workspace2.bzl:934:28: in workspace   /opt/tensorflow/tensorflow/workspace2.bzl:87:20: in _initialize_third_party   /opt/tensorflow/third_party/pybind11_abseil/workspace.bzl:13:20: in repo   /opt/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /opt/tensorflow/third_party/repo.bzl:89:35: in  INFO: Repository pybind11_protobuf instantiated at:   /opt/tensorflow/WORKSPACE:64:14: in    /opt/tensorflow/tensorflow/workspace2.bzl:941:21: in workspace   /opt/tensorflow/tensorflow/workspace2.bzl:788:20: in _tf_repositories   /opt/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /opt/tensorflow/third_party/repo.bzl:89:35: in  INFO: Repository nanobind instantiated at:   /opt/tensorflow/WORKSPACE:64:14: in    /opt/tensorflow/tensorflow/workspace2.bzl:934:28: in workspace   /opt/tensorflow/tensorflow/workspace2.bzl:83:13: in _initialize_third_party   /opt/tensorflow/third_party/nanobind/workspace.bzl:6:20: in repo   /opt/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /opt/tensorflow/third_party/repo.bzl:89:35: in  INFO: Repository rules_license instantiated at:   /opt/tensorflow/WORKSPACE:24:14: in    /opt/tensorflow/tensorflow/workspace3.bzl:34:17: in workspace Repository rule http_archive defined at:   /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/bazel_tools/tools/build_defs/repo/http.bzl:372:31: in  ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted:  INFO: Elapsed time: 1.754s INFO: 0 processes. FAILED: Build did NOT complete successfully (37 packages loaded, 5 targets configured)     currently loading: tensorflow/compiler/mlir/quantization/stablehlo ... (19 packages)     Fetching repository project; starting     Fetching repository ; starting     Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11/archive/v2.13.4.tar.gz     Fetching https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen//archive/33d0937c6bdf5ec999939fb17f2a553183d14a74/eigen33d0937c6bdf5ec999939fb17f2a553183d14a74.tar.gz     Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11_abseil/archive/2c4932ed6f6204f1656e245838f4f5eae69d2e29.tar.gz     Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11_protobuf/archive/80f3440cd8fee124e077e2e47a8a17b78b451363.zip     Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/wjakob/nanobind/archive/d79309197caaad83cda05df533136865d294f01e.tar.gz     Fetching https://mirror.bazel.build/github.com/bazelbuild/rules_license/releases/download/0.0.7/rules_license0.0.7.tar.gz ```",2025-03-31T22:39:32Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/90331,"Querying local_config_cuda using BAZEL, I get the following output. Should I be using cudart and not cuda_runtime. Please advise.  `//cuda:BUILD //cuda:_opt //cuda:any_cuda_libs //cuda:build_defs.bzl //cuda:build_defs_bzl //cuda:cub_headers //cuda:cublas //cuda:cublasinclude //cuda:cublasLt //cuda:cublas_headers //cuda:cublas_headers_virtual //cuda:cuda //cuda:cudaextras //cuda:cudainclude //cuda:cudanvvm //cuda:cuda/cuda_config.h //cuda:cuda/cuda_config.py //cuda:cuda/lib/libcublas.so //cuda:cuda/lib/libcublasLt.so //cuda:cuda/lib/libcuda.so //cuda:cuda/lib/libcudart.so //cuda:cuda/lib/libcudart_static.a //cuda:cuda/lib/libcudnn.so //cuda:cuda/lib/libcufft.so //cuda:cuda/lib/libcupti.so //cuda:cuda/lib/libcurand.so //cuda:cuda/lib/libcusolver.so //cuda:cuda/lib/libcusparse.so //cuda:cuda_config_py //cuda:cuda_driver //cuda:cuda_headers //cuda:cuda_headers_virtual //cuda:cuda_libs //cuda:cuda_tools //cuda:cuda_tools_and_libs //cuda:cudart //cuda:cudart_static //cuda:cudnn //cuda:cudnninclude //cuda:cudnn_header //cuda:cufft //cuda:cufftinclude //cuda:cufft_headers //cuda:cufft_headers_virtual //cuda:cupti_dsos //cuda:cupti_headers //cuda:cupti_headers_virtual //cuda:curand //cuda:curandinclude //cuda:curand_headers //cuda:curand_headers_virtual //cuda:cusolver //cuda:cusolverinclude //cuda:cusolver_headers //cuda:cusolver_headers_virtual //cuda:cusparse //cuda:cusparseinclude //cuda:cusparse_headers //cuda:cusparse_headers_virtual //cuda:implicit_cuda_headers_dependency //cuda:include_cuda_libs //cuda:libdevice_root //cuda:nvjitlink //cuda:nvml //cuda:nvml_headers //cuda:nvml_headers_virtual //cuda:nvptxcompiler //cuda:override_include_cuda_libs //cuda:overrided_cuda_libs //cuda:true_setting //cuda:using_clang //cuda:using_clang_opt //cuda:using_nvcc`","Hi  , Apologies for the delay, and thank you for raising your concern. It seems you have installed incompatible versions. Could you please update them as per the documentation and let us know if the issue still persists? Here is the documentation for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
fp16,copybara-service[bot],litert: Add fp16 support in GPU Accelerator,litert: Add fp16 support in GPU Accelerator Used fp16 calculation if target GPU supports it. Added naive fp16fp32 conversion as an initial step. It should be vectorized.,2025-03-31T17:55:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90320
opt,copybara-service[bot],[XLA:GPU] Define HS optimization at O1.,[XLA:GPU] Define HS optimization at O1.,2025-03-31T15:21:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90312
opt,copybara-service[bot],[XLA:GPU] Delete no-op `xla_gpu_enable_nccl_clique_optimization` flag and associated code.,"[XLA:GPU] Delete noop `xla_gpu_enable_nccl_clique_optimization` flag and associated code. The flag was used in the now deleted ([ CC(Getting Error  Exception: No data provided for ""activation_2""  Need data for each key in: ['input2', 'aux_input', 'input1'])](https://github.com/openxla/xla/pull/9329)) XLA GPU runtime.",2025-03-31T13:33:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90309
opt,copybara-service[bot],[XLA:GPU] Allow to extract collectives from the post-optimized module.,[XLA:GPU] Allow to extract collectives from the postoptimized module.,2025-03-31T11:46:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90307
tpu,saddamhijazi,TensorFlow issue with data generator used for training a Keras LSTM autoencoder," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.5 LTS  Mobile device _No response_  Python version Python 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to build a model which is a LSTMautoencoder using TensorFlow. The model generates the training data using 'tf.data.Dataset'.  The original dimension of the data which is loaded from a .mat file is `[79266,1001]`, I ran the code and then I got an error message saying : Training failed: None values not supported. I have tried to use the minimum batch size and I reduced the size of the data to check if the problem is related to memory load but still the issue is happening even for very small data and batch sizes. In the code I replaced the loading command with a random data generation just for the purposes of reproducing the error.  Standalone code to reproduce the issue ```shell import os os.environ['CUDA_VISIBLE_DEVICES'] = '1'   Force CPU execution import sys import tensorflow as tf import os.path import numpy as np import scipy.io import time from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.callbacks import Callback, ModelCheckpoint  Ultrasafe hyperparameters SPATIAL_SUBSAMPLE = 400   Now taking every 400th spatial point (~198 points) TEMPORAL_SUBSAMPLE = 50   Taking every 50th snapshot (~20 timesteps) SEQ_LENGTH = 1            Single timestep sequences LATENT_DIM = 1            1D latent space BATCH_SIZE = 1            Minimum batch size def load_and_validate():     """"""Load with maximum subsampling""""""     U_COM = tf.random.normal(shape = [79266,1001])      Aggressive subsampling     U_subsampled = U_COM[::SPATIAL_SUBSAMPLE, ::TEMPORAL_SUBSAMPLE]     print(f""Subsampled shape: {U_subsampled.shape} (spatial × temporal)"")      Validation     assert U_subsampled.shape[1] >= SEQ_LENGTH, ""Not enough timesteps""     assert not np.isnan(U_subsampled).any(), ""NaN values detected""      Normalize     U_min, U_max = np.min(U_subsampled), np.max(U_subsampled)     return 2 * (U_subsampled  U_min) / (U_max  U_min)  1  Load with cleanup tf.keras.backend.clear_session() U_norm = load_and_validate()  Create singletimestep sequences sequences = U_norm[:, :, np.newaxis]   Shape: (spatial, timesteps, 1) print(f""Sequences shape: {sequences.shape}"")  Create dataset dataset = tf.data.Dataset.from_tensor_slices(sequences) dataset = dataset.batch(BATCH_SIZE)  Verify sample = next(iter(dataset)) print(f""Sample batch shape: {sample.shape}"")  Micro LSTM model def build_micro_model():     inputs = tf.keras.Input(shape=(sequences.shape[1], 1))     x = layers.LSTM(2)(inputs)   Only 2 units     x = layers.Dense(LATENT_DIM)(x)     x = layers.RepeatVector(sequences.shape[1])(x)     outputs = layers.LSTM(1, return_sequences=True)(x)     return tf.keras.Model(inputs, outputs) model = build_micro_model() model.compile(optimizer='adam', loss='mse') model.summary()  Training os.makedirs('Tests', exist_ok=True) try:     history = model.fit(         dataset,         epochs=3,   Very few epochs         verbose=2     )     print(""Training completed successfully!"") except Exception as e:     print(f""Training failed: {str(e)}"")     print(""\nThis should never happen with these settings."")     print(""Please verify your input data structure."") ```  Relevant log output ```shell 20250331 13:08:33.404751: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250331 13:08:33.405142: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used. 20250331 13:08:33.407488: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used. 20250331 13:08:33.414088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1743419313.425745   24852 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1743419313.428932   24852 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250331 13:08:33.440581: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250331 13:08:35.116129: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303) Subsampled shape: (199, 21) (spatial × temporal) Sequences shape: (199, 21, 1) Sample batch shape: (1, 21, 1) Model: ""functional"" ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ ┃ Layer (type)                    ┃ Output Shape           ┃       Param  ┃ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ │ input_layer (InputLayer)        │ (None, 21, 1)          │             0 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ lstm (LSTM)                     │ (None, 2)              │            32 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ dense (Dense)                   │ (None, 1)              │             3 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ repeat_vector (RepeatVector)    │ (None, 21, 1)          │             0 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ lstm_1 (LSTM)                   │ (None, 21, 1)          │            12 │ └─────────────────────────────────┴────────────────────────┴───────────────┘  Total params: 47 (188.00 B)  Trainable params: 47 (188.00 B)  Nontrainable params: 0 (0.00 B) Epoch 1/3 Training failed: None values not supported. This should never happen with these settings. Please verify your input data structure. ```",2025-03-31T11:27:31Z,type:bug comp:keras TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/90305,"Hi  , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow 2.19.0 and the nightly version and encountered the same issue. Please find the gist here for reference. Based on my findings, this issue seems to be more related to Keras. I recommend posting this issue on the kerasteam/keras repository for better support. Thank you!","> Hi [](https://github.com/saddamhijazi) , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow 2.19.0 and the nightly version and encountered the same issue. Please find the gist here for reference. Based on my findings, this issue seems to be more related to Keras. I recommend posting this issue on the kerasteam/keras repository for better support. >  > Thank you! Thank you very much  for your response and help, I have submitted an issue on the Keras repository. Thank you !!"
tpu,copybara-service[bot],[XLA:GPU] Add support for multiple output tiles in triton_support_test,"[XLA:GPU] Add support for multiple output tiles in triton_support_test + removes dependency on `gettupleelement` for Reduce, BatchNormGrad & BatchNormTraining tests",2025-03-31T11:11:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90304
opt,copybara-service[bot],PR #23827: [DOC] Fix multihost HLO runner doc.,PR CC(Tensor cpu need cuda?): [DOC] Fix multihost HLO runner doc. Imported from GitHub PR https://github.com/openxla/xla/pull/23827  Omit c opt which is the default.   Omit dump_hlo_as_text which is the default.   Omit config=cuda which is nowadays handled by configure.py.   Remove xla_disable_all_hlo_passes which has no effect in presence of run_xla_backend_only.   Leave single mention of dynamic_mode=off which usually isn't needed.   Other minor fixes. Copybara import of the project:  bc64c89d3cb4db37945331d94faf9acc573ed6d9 by Ilia Sergachev : [DOC] Fix multihost HLO runner doc.  Omit c opt which is the default.   Omit dump_hlo_as_text which is the default.   Omit config=cuda which is nowadays handled by configure.py.   Remove xla_disable_all_hlo_passes which has no effect in presence of run_xla_backend_only.   Leave single mention of dynamic_mode=off which usually isn't needed.   Other minor fixes. Merging this change closes CC(Tensor cpu need cuda?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23827 from openxla:fix_doc bc64c89d3cb4db37945331d94faf9acc573ed6d9,2025-03-31T10:37:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90300
sharding,copybara-service[bot],#sdy add a sharding rule for mhlo::CopyOp as this op remains when converting from HLO to StableHLO.,sdy add a sharding rule for mhlo::CopyOp as this op remains when converting from HLO to StableHLO.,2025-03-31T09:22:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90299
opt,copybara-service[bot],[XLA:CPU] Ensure benchmark name uniqueness in `multi_benchmark_config` and pass benchmark options by value,[XLA:CPU] Ensure benchmark name uniqueness in `multi_benchmark_config` and pass benchmark options by value,2025-03-31T08:57:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90296
opt,copybara-service[bot],PR #24248: Move-to-memory-space custom calls use default layout,PR CC(Support for `skip_mismatch` in `tf.keras.engine.saving.load_weights_from_hdf5_group_by_name`): Movetomemoryspace custom calls use default layout Imported from GitHub PR https://github.com/openxla/xla/pull/24248 Using nondefault layout for MoveToHost makes the compiler insert a transpose operation on the host value if that value flows into the root of the entry computation. Such transposes cause the host offloader to emit a slow onhost transpose (and a warning on the console). We see those warnings on  maxtext llama27b with optimizer state offloading with fsdp=2. This patch enforces default layout for onhost values so that no transpose is necessary (as long as there is no override of layout for host values in entry computation). Note that such transposes cannot be sunk into the uses by the offloading lagalizer because there is nowhere to sink to  the value is returned from the computation. Copybara import of the project:  ad192bbba93040a88bda9e4e1074dd1de7153b32 by Jaroslav Sevcik : Use default layout for offloading ops  f8f0ddcf73c9a4792cbbfd99c2d465f4de80c418 by Jaroslav Sevcik : Address reviewer comments Merging this change closes CC(Support for `skip_mismatch` in `tf.keras.engine.saving.load_weights_from_hdf5_group_by_name`) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24248 from jarosevcik:movetomemoryspaceusesdefaultlayout f8f0ddcf73c9a4792cbbfd99c2d465f4de80c418,2025-03-31T08:33:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90295
tpu,copybara-service[bot],Direct StableHLO to HLO conversion : prototype with AddOp and ConstantOp.,"Direct StableHLO to HLO conversion : prototype with AddOp and ConstantOp. The cl demonstrates  1. codegen for stablehlo ops. 2. changes needed in conversion pipeline to allow stablehlo ops to convert directly to hlo without mhlo step.  Note: The new direct path is still disabled for the production until we add all stablehlo ops to the codegen. Example: ``` hlotranslate mlirtohlo $PWD/1.mlir ``` Input ``` func.func (%arg0: tensor) > tensor {   %c = stablehlo.constant dense : tensor   %0 = stablehlo.add %arg0, %c : tensor   %1 = mhlo.multiply %c, %0 : tensor   return %1 : tensor } ``` after StableHLO > MHLO conversion, stablehlo.add and stablehlo.constant are preserved, not converted to mhlo. ``` mlirhloopt stablehlolegalizetohlo=legalizepartially=true chlolegalizetohlo $PWD/1.mlir module {   func.func (%arg0: tensor) > tensor {     %c = stablehlo.constant dense : tensor     %0 = stablehlo.add %arg0, %c : tensor     %1 = mhlo.multiply %c, %0 : tensor     return %1 : tensor   } } ``` Final Output  ``` HloModule main, entry_computation_layout={(s32[])>s32[]} ENTRY %main.5 (Arg_0.1: s32[]) > s32[] {   %Arg_0.1 = s32[] parameter(0)   %constant.2 = s32[] constant(2)   %add.3 = s32[] add(%Arg_0.1, %constant.2), metadata=   ROOT %multiply.4 = s32[] multiply(%add.3, %constant.2), metadata= } ```",2025-03-31T07:08:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90294
tpu,weyn9q,TensorFlow with CUDA: RTX 5xxx series isn't supported (CUDA_ERROR_INVALID_HANDLE)," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code Yes  OS platform and distribution WSL2  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.8  GPU model and memory 5070TI  Current behavior? hi guys. i upgraded my GPU to 5070ti and found that tensorflow is still not working with new nvidia cards honestly i knew that can be problems. but its been 3 month since nvidia released new gpu, i thought now should be no prob already. any idea how long it will take until at least there will be a nightly version which will fix the problem? like 12 weeks or half a year ? :) or maybe someone found an easy way to fix it?  Standalone code to reproduce the issue ```shell import tensorflow as tf model = tf.keras.Sequential([     tf.keras.Input(shape=(20,)),     tf.keras.layers.Dense(10),     tf.keras.layers.Dense(1) ]) ```  Relevant log output ```shell 20250331 14:23:13.091967: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250331 14:23:13.734399: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1743398598.268271   13190 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jitcompiled from PTX, which could take 30 minutes or longer. W0000 00:00:1743398598.270049   13190 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jitcompiled from PTX, which could take 30 minutes or longer. I0000 00:00:1743398598.408238   13190 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13123 MB memory:  > device: 0, name: NVIDIA GeForce RTX 5070 Ti, pci bus id: 0000:01:00.0, compute capability: 12.0 20250331 14:23:18.851409: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_INVALID_PTX' 20250331 14:23:18.851466: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250331 14:23:18.851502: W tensorflow/core/framework/op_kernel.cc:1843] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250331 14:23:18.851526: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' Traceback (most recent call last):   File ""/mnt/c/Users/alex/desktop/test111.py"", line 3, in      model = tf.keras.Sequential([   File ""/home/alexflame/.local/lib/python3.10/sitepackages/keras/src/models/sequential.py"", line 76, in __init__     self._maybe_rebuild()   File ""/home/alexflame/.local/lib/python3.10/sitepackages/keras/src/models/sequential.py"", line 149, in _maybe_rebuild     self.build(input_shape)   File ""/home/alexflame/.local/lib/python3.10/sitepackages/keras/src/layers/layer.py"", line 229, in build_wrapper     original_build_method(*args, **kwargs)   File ""/home/alexflame/.local/lib/python3.10/sitepackages/keras/src/models/sequential.py"", line 195, in build     x = layer(x)   File ""/home/alexflame/.local/lib/python3.10/sitepackages/keras/src/utils/traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/home/alexflame/.local/lib/python3.10/sitepackages/keras/src/backend/tensorflow/core.py"", line 152, in convert_to_tensor     return tf.cast(x, dtype) tensorflow.python.framework.errors_impl.InternalError: {{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Cast] name: ```",2025-03-31T05:26:22Z,stat:awaiting tensorflower type:feature type:build/install wsl2 TF 2.18,open,0,3,https://github.com/tensorflow/tensorflow/issues/90291,"spend whole day to build custom tensorflow build but didnt succeed. now finally found how to solve this problem. not perfect solution as it could be, but still better then nothing :) https://docs.nvidia.com/deeplearning/frameworks/tensorflowreleasenotes/rel2502.html",ohh seems its impossible. thinking about move to pytorch. i dont want but :( but i really stuck here with this problems and i dont think will be any updates here soon.  i cant run any of my codes with new GPU. nvidia container works but its soo complicated. docker still not fully optimizated with many mistakes. cant work good. example  i have my own pretrained model which i am using to train RL agent. lets say i have 2 code. first one is for train model ( later  pretrained) second is use pretrained get embeddings train RL agent. without nvidia docker i just cant run any of them coz of CUDA errors. with docker first code can run if will fix some mistakes which happens only if use docker. but. second code i cant deserialize custom layers from pretrained idk why. coz of docker i think. probably because they were builded with another drivers. i decided to check if i will train model in docker again and will try to use it in second code will it work or no. it is not working. i still cant deserialize custom layers even with newly trained model. i am giving up. before on 4xxx videocard everything was perfect. now idk what to do,Same issue same scenario here. Tried updating every driver I could think of. Hoping for at least a nightly version soon so I can work again.
tpu,balasai075,Import Error," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.8  Custom code Yes  OS platform and distribution windows 11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError: Traceback (most recent call last):   File ""C:\Users\balas\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.  Standalone code to reproduce the issue ```shell import json import os import cv2 import numpy as np import tensorflow as tf import matplotlib.pyplot as plt  Paths for TuSimple Dataset label_paths = [     r""C:\Users\balas\Downloads\archive (2)\TUSimple\train_set\label_data_0313.json"",     r""C:\Users\balas\Downloads\archive (2)\TUSimple\train_set\label_data_0531.json"",     r""C:\Users\balas\Downloads\archive (2)\TUSimple\train_set\label_data_0601.json"" ] image_root = r""C:\Users\balas\Downloads\archive (2)\TUSimple\train_set""  Load JSON Labels def load_tusimple_labels(label_paths):     dataset = []     for label_path in label_paths:         with open(label_path, 'r') as f:             for line in f:                 dataset.append(json.loads(line))     return dataset  Extract Image Paths & Lane Annotations def parse_tusimple_data(dataset):     images = []     lanes = []     for item in dataset:         image_path = os.path.join(image_root, item[""raw_file""])         lane_annotations = item[""lanes""]         images.append(image_path)         lanes.append(lane_annotations)     return images, lanes  Load Dataset dataset = load_tusimple_labels(label_paths) image_paths, lane_data = parse_tusimple_data(dataset)  Image Preprocessing Function def preprocess_image(image_path):     image = cv2.imread(image_path)     image = cv2.resize(image, (256, 256))   Resize to match CNN input     image = image / 255.0   Normalize     return image  Load Images X_train = np.array([preprocess_image(img) for img in image_paths[:]])   Load first 500 for training X_train = X_train.reshape(1, 256, 256, 3) print(f""Loaded {len(X_train)} images from TuSimple dataset!"") ```  Relevant log output ```shell  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[5], line 5       3 import cv2       4 import numpy as np > 5 import tensorflow as tf       6 import matplotlib.pyplot as plt       8  Paths for TuSimple Dataset File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\balas\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```",2025-03-30T17:05:20Z,type:support,closed,0,2,https://github.com/tensorflow/tensorflow/issues/90264,"TF 2.8 is no longer supported. Also, this is a duplicate",Are you satisfied with the resolution of your issue? Yes No
tpu,Manashwinij,error in tensorflow installation," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version Version: 2.19.0  Custom code Yes  OS platform and distribution win 11  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory cpu  Current behavior?  ImportError                               Traceback (most recent call last) File ~\myenv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[5], line 2       1 import numpy as np > 2 import tensorflow as tf       3 from tensorflow.keras.models import load_model       4 from tensorflow.keras.preprocessing import image File ~\myenv\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\myenv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.')      97  pylint: enable=wildcardimport,gimportnotattop,unusedimport,linetoolong ImportError: Traceback (most recent call last):   File ""C:\Users\Hp\myenv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions.  Standalone code to reproduce the issue ```shell import numpy as np import tensorflow as tf from tensorflow.keras.models import load_model from tensorflow.keras.preprocessing import image ```  Relevant log output ```shell ```",2025-03-30T12:22:24Z,type:build/install,closed,0,2,https://github.com/tensorflow/tensorflow/issues/90262,"Unless there's additional information, this is a duplicate. Please always search for duplicates and only open a new issue if the duplicates don't apply, because you have additional info.",Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],Optional TQDM dependency,Optional TQDM dependency,2025-03-30T06:17:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90261
tpu,SarahClementine,Error fixing," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.19  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Failure in installation.  Standalone code to reproduce the issue ```shell . Location: c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\localcache\localpackages\python39\sitepackages Requires: Requiredby: h5py, KerasPreprocessing, ml_dtypes, scikitlearn, scipy, tensorboard, tensorflow PS C:\xampp\htdocs\SkinToneAnalysis> python version >> pip show tensorflow >> pip show numpy >> Python 3.10.11 WARNING: Package(s) not found: tensorflow WARNING: Package(s) not found: numpy PS C:\xampp\htdocs\SkinToneAnalysis> python m pip install upgrade pip setuptools wheel >> Requirement already satisfied: pip in c:\program files\windowsapps\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\sitepackages (23.0.1) Collecting pip   Downloading pip25.0.1py3noneany.whl (1.8 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 1.5 MB/s eta 0:00:00 Requirement already satisfied: setuptools in c:\program files\windowsapps\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\sitepackages (65.5.0) Collecting setuptools   Downloading setuptools78.1.0py3noneany.whl (1.3 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 2.2 MB/s eta 0:00:00 Collecting wheel   Downloading wheel0.45.1py3noneany.whl (72 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.5/72.5 kB 2.0 MB/s eta 0:00:00 Installing collected packages: wheel, setuptools, pip   WARNING: The script wheel.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The scripts pip.exe, pip3.10.exe and pip3.exe are installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation. Successfully installed pip25.0.1 setuptools78.1.0 wheel0.45.1 [notice] A new release of pip is available: 23.0.1 > 25.0.1 [notice] To update, run: C:\Users\USER PC\AppData\Local\Microsoft\WindowsApps\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\python.exe m pip install upgrade pip PS C:\xampp\htdocs\SkinToneAnalysis> pip install tensorflow==2.10.0 >> Collecting tensorflow==2.10.0   Downloading tensorflow2.10.0cp310cp310win_amd64.whl.metadata (3.1 kB) Collecting abslpy>=1.0.0 (from tensorflow==2.10.0)   Downloading absl_py2.2.1py3noneany.whl.metadata (2.4 kB) Collecting astunparse>=1.6.0 (from tensorflow==2.10.0)   Downloading astunparse1.6.3py2.py3noneany.whl.metadata (4.4 kB) Collecting flatbuffers>=2.0 (from tensorflow==2.10.0)   Downloading flatbuffers25.2.10py2.py3noneany.whl.metadata (875 bytes) Collecting gast=0.2.1 (from tensorflow==2.10.0)   Downloading gast0.4.0py3noneany.whl.metadata (1.1 kB) Collecting googlepasta>=0.1.1 (from tensorflow==2.10.0)   Downloading google_pasta0.2.0py3noneany.whl.metadata (814 bytes) Collecting h5py>=2.9.0 (from tensorflow==2.10.0)   Downloading h5py3.13.0cp310cp310win_amd64.whl.metadata (2.5 kB) Collecting keraspreprocessing>=1.1.1 (from tensorflow==2.10.0)   Downloading Keras_Preprocessing1.1.2py2.py3noneany.whl.metadata (1.9 kB) Collecting libclang>=13.0.0 (from tensorflow==2.10.0)   Downloading libclang18.1.1py2.py3nonewin_amd64.whl.metadata (5.3 kB) Collecting numpy>=1.20 (from tensorflow==2.10.0)   Downloading numpy2.2.4cp310cp310win_amd64.whl.metadata (60 kB) Collecting opteinsum>=2.3.2 (from tensorflow==2.10.0)   Downloading opt_einsum3.4.0py3noneany.whl.metadata (6.3 kB) Collecting packaging (from tensorflow==2.10.0)   Downloading packaging24.2py3noneany.whl.metadata (3.2 kB) Collecting protobuf=3.9.2 (from tensorflow==2.10.0)   Downloading protobuf3.19.6cp310cp310win_amd64.whl.metadata (806 bytes) Requirement already satisfied: setuptools in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (78.1.0) Collecting six>=1.12.0 (from tensorflow==2.10.0)   Downloading six1.17.0py2.py3noneany.whl.metadata (1.7 kB) Collecting termcolor>=1.1.0 (from tensorflow==2.10.0)   Downloading termcolor2.5.0py3noneany.whl.metadata (6.1 kB) Collecting typingextensions>=3.6.6 (from tensorflow==2.10.0)   Downloading typing_extensions4.13.0py3noneany.whl.metadata (3.0 kB) Collecting wrapt>=1.11.0 (from tensorflow==2.10.0)   Downloading wrapt1.17.2cp310cp310win_amd64.whl.metadata (6.5 kB) Collecting tensorflowiogcsfilesystem>=0.23.1 (from tensorflow==2.10.0)   Downloading tensorflow_io_gcs_filesystem0.31.0cp310cp310win_amd64.whl.metadata (14 kB) Collecting grpcio=1.24.3 (from tensorflow==2.10.0)   Downloading grpcio1.71.0cp310cp310win_amd64.whl.metadata (4.0 kB) Collecting tensorboard=2.10 (from tensorflow==2.10.0)   Downloading tensorboard2.10.1py3noneany.whl.metadata (1.9 kB) Collecting tensorflowestimator=2.10.0 (from tensorflow==2.10.0)   Downloading tensorflow_estimator2.10.0py2.py3noneany.whl.metadata (1.3 kB) Collecting keras=2.10.0 (from tensorflow==2.10.0)   Downloading keras2.10.0py2.py3noneany.whl.metadata (1.3 kB) Requirement already satisfied: wheel=0.23.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from astunparse>=1.6.0>tensorflow==2.10.0) (0.45.1) Collecting googleauth=1.6.3 (from tensorboard=2.10>tensorflow==2.10.0)   Downloading google_auth2.38.0py2.py3noneany.whl.metadata (4.8 kB) Collecting googleauthoauthlib=0.4.1 (from tensorboard=2.10>tensorflow==2.10.0)   Downloading google_auth_oauthlib0.4.6py2.py3noneany.whl.metadata (2.7 kB) Collecting markdown>=2.6.8 (from tensorboard=2.10>tensorflow==2.10.0)   Downloading Markdown3.7py3noneany.whl.metadata (7.0 kB) Collecting requests=2.21.0 (from tensorboard=2.10>tensorflow==2.10.0)   Downloading requests2.32.3py3noneany.whl.metadata (4.6 kB) Collecting tensorboarddataserver=0.6.0 (from tensorboard=2.10>tensorflow==2.10.0)   Downloading tensorboard_data_server0.6.1py3noneany.whl.metadata (1.1 kB) Collecting tensorboardpluginwit>=1.6.0 (from tensorboard=2.10>tensorflow==2.10.0)   Downloading tensorboard_plugin_wit1.8.1py3noneany.whl.metadata (873 bytes) Collecting werkzeug>=1.0.1 (from tensorboard=2.10>tensorflow==2.10.0)   Downloading werkzeug3.1.3py3noneany.whl.metadata (3.7 kB) Collecting cachetools=2.0.0 (from googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0)   Downloading cachetools5.5.2py3noneany.whl.metadata (5.4 kB) Collecting pyasn1modules>=0.2.1 (from googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0)   Downloading pyasn1_modules0.4.2py3noneany.whl.metadata (3.5 kB) Collecting rsa=3.1.4 (from googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0)   Downloading rsa4.9py3noneany.whl.metadata (4.2 kB) Collecting requestsoauthlib>=0.7.0 (from googleauthoauthlib=0.4.1>tensorboard=2.10>tensorflow==2.10.0)   Downloading requests_oauthlib2.0.0py2.py3noneany.whl.metadata (11 kB) Collecting charsetnormalizer=2 (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0)   Downloading charset_normalizer3.4.1cp310cp310win_amd64.whl.metadata (36 kB) Collecting idna=2.5 (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0)   Downloading idna3.10py3noneany.whl.metadata (10 kB) Collecting urllib3=1.21.1 (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0)   Downloading urllib32.3.0py3noneany.whl.metadata (6.5 kB) Collecting certifi>=2017.4.17 (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0)   Downloading certifi2025.1.31py3noneany.whl.metadata (2.5 kB) Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1>tensorboard=2.10>tensorflow==2.10.0)   Downloading MarkupSafe3.0.2cp310cp310win_amd64.whl.metadata (4.1 kB) Collecting pyasn1=0.6.1 (from pyasn1modules>=0.2.1>googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0)   Downloading pyasn10.6.1py3noneany.whl.metadata (8.4 kB) Collecting oauthlib>=3.0.0 (from requestsoauthlib>=0.7.0>googleauthoauthlib=0.4.1>tensorboard=2.10>tensorflow==2.10.0)   Downloading oauthlib3.2.2py3noneany.whl.metadata (7.5 kB) Downloading tensorflow2.10.0cp310cp310win_amd64.whl (455.9 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 455.9/455.9 MB 4.2 MB/s eta 0:00:00 Downloading absl_py2.2.1py3noneany.whl (277 kB) Downloading astunparse1.6.3py2.py3noneany.whl (12 kB) Downloading flatbuffers25.2.10py2.py3noneany.whl (30 kB) Downloading gast0.4.0py3noneany.whl (9.8 kB) Downloading google_pasta0.2.0py3noneany.whl (57 kB) Downloading grpcio1.71.0cp310cp310win_amd64.whl (4.3 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 2.7 MB/s eta 0:00:00 Downloading h5py3.13.0cp310cp310win_amd64.whl (3.0 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 2.4 MB/s eta 0:00:00 Downloading keras2.10.0py2.py3noneany.whl (1.7 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 2.8 MB/s eta 0:00:00 Downloading Keras_Preprocessing1.1.2py2.py3noneany.whl (42 kB) Downloading libclang18.1.1py2.py3nonewin_amd64.whl (26.4 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.4/26.4 MB 2.9 MB/s eta 0:00:00 Downloading numpy2.2.4cp310cp310win_amd64.whl (12.9 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/12.9 MB 3.6 MB/s eta 0:00:00 Downloading opt_einsum3.4.0py3noneany.whl (71 kB) Downloading protobuf3.19.6cp310cp310win_amd64.whl (895 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 895.7/895.7 kB 3.1 MB/s eta 0:00:00 Downloading six1.17.0py2.py3noneany.whl (11 kB) Downloading tensorboard2.10.1py3noneany.whl (5.9 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/5.9 MB 3.2 MB/s eta 0:00:00 Downloading tensorflow_estimator2.10.0py2.py3noneany.whl (438 kB) Downloading tensorflow_io_gcs_filesystem0.31.0cp310cp310win_amd64.whl (1.5 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 738.8 kB/s eta 0:00:00 Downloading termcolor2.5.0py3noneany.whl (7.8 kB) Downloading typing_extensions4.13.0py3noneany.whl (45 kB) Downloading wrapt1.17.2cp310cp310win_amd64.whl (38 kB) Downloading packaging24.2py3noneany.whl (65 kB) Downloading google_auth2.38.0py2.py3noneany.whl (210 kB) Downloading google_auth_oauthlib0.4.6py2.py3noneany.whl (18 kB) Downloading Markdown3.7py3noneany.whl (106 kB) Downloading requests2.32.3py3noneany.whl (64 kB) Downloading tensorboard_data_server0.6.1py3noneany.whl (2.4 kB) Downloading tensorboard_plugin_wit1.8.1py3noneany.whl (781 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 1.3 MB/s eta 0:00:00 Downloading werkzeug3.1.3py3noneany.whl (224 kB) Downloading cachetools5.5.2py3noneany.whl (10 kB) Downloading certifi2025.1.31py3noneany.whl (166 kB) Downloading charset_normalizer3.4.1cp310cp310win_amd64.whl (102 kB) Downloading idna3.10py3noneany.whl (70 kB) Downloading MarkupSafe3.0.2cp310cp310win_amd64.whl (15 kB) Downloading pyasn1_modules0.4.2py3noneany.whl (181 kB) Downloading requests_oauthlib2.0.0py2.py3noneany.whl (24 kB) Downloading rsa4.9py3noneany.whl (34 kB) Downloading urllib32.3.0py3noneany.whl (128 kB) Downloading oauthlib3.2.2py3noneany.whl (151 kB) Downloading pyasn10.6.1py3noneany.whl (83 kB) Installing collected packages: tensorboardpluginwit, libclang, keras, flatbuffers, wrapt, urllib3, typingextensions, termcolor, tensorflowiogcsfilesystem, tensorflowestimator, tensorboarddataserver, six, pyasn1, protobuf, packaging, opteinsum, oauthlib, numpy, MarkupSafe, markdown, idna, grpcio, gast, charsetnormalizer, certifi, cachetools, abslpy, werkzeug, rsa, requests, pyasn1modules, keraspreprocessing, h5py, googlepasta, astunparse, requestsoauthlib, googleauth, googleauthoauthlib, tensorboard, tensorflow   WARNING: The scripts f2py.exe and numpyconfig.exe are installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The script markdown_py.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The script normalizer.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The scripts pyrsadecrypt.exe, pyrsaencrypt.exe, pyrsakeygen.exe, pyrsapriv2pub.exe, pyrsasign.exe and pyrsaverify.exe are installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The script googleoauthlibtool.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The script tensorboard.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation. Successfully installed MarkupSafe3.0.2 abslpy2.2.1 astunparse1.6.3 cachetools5.5.2 certifi2025.1.31 charsetnormalizer3.4.1 flatbuffers25.2.10 gast0.4.0 googleauth2.38.0 googleauthoauthlib0.4.6 googlepasta0.2.0 grpcio1.71.0 h5py3.13.0 idna3.10 keras2.10.0 keraspreprocessing1.1.2 libclang18.1.1 markdown3.7 numpy2.2.4 oauthlib3.2.2 opteinsum3.4.0 packaging24.2 protobuf3.19.6 pyasn10.6.1 pyasn1modules0.4.2 requests2.32.3 requestsoauthlib2.0.0 rsa4.9 six1.17.0 tensorboard2.10.1 tensorboarddataserver0.6.1 tensorboardpluginwit1.8.1 tensorflow2.10.0 tensorflowestimator2.10.0 tensorflowiogcsfilesystem0.31.0 termcolor2.5.0 typingextensions4.13.0 urllib32.3.0 werkzeug3.1.3 wrapt1.17.2 PS C:\xampp\htdocs\SkinToneAnalysis> pip install tensorflow==2.10.0 >> Requirement already satisfied: tensorflow==2.10.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (2.10.0) Requirement already satisfied: abslpy>=1.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (2.2.1) Requirement already satisfied: astunparse>=1.6.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (1.6.3) Requirement already satisfied: flatbuffers>=2.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (25.2.10) Requirement already satisfied: gast=0.2.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (0.4.0) Requirement already satisfied: googlepasta>=0.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (0.2.0) Requirement already satisfied: h5py>=2.9.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (3.13.0) Requirement already satisfied: keraspreprocessing>=1.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (1.1.2) Requirement already satisfied: libclang>=13.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (18.1.1) Requirement already satisfied: numpy>=1.20 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (2.2.4) Requirement already satisfied: opteinsum>=2.3.2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (3.4.0) Requirement already satisfied: packaging in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (24.2) Requirement already satisfied: protobuf=3.9.2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (3.19.6) Requirement already satisfied: setuptools in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (78.1.0) Requirement already satisfied: six>=1.12.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (1.17.0) Requirement already satisfied: termcolor>=1.1.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (2.5.0) Requirement already satisfied: typingextensions>=3.6.6 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (4.13.0) Requirement already satisfied: wrapt>=1.11.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (1.17.2) Requirement already satisfied: tensorflowiogcsfilesystem>=0.23.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (0.31.0) Requirement already satisfied: grpcio=1.24.3 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (1.71.0) Requirement already satisfied: tensorboard=2.10 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (2.10.1) Requirement already satisfied: tensorflowestimator=2.10.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (2.10.0) Requirement already satisfied: keras=2.10.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (2.10.0) Requirement already satisfied: wheel=0.23.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from astunparse>=1.6.0>tensorflow==2.10.0) (0.45.1) Requirement already satisfied: googleauth=1.6.3 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow==2.10.0) (2.38.0) Requirement already satisfied: googleauthoauthlib=0.4.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow==2.10.0) (0.4.6) Requirement already satisfied: markdown>=2.6.8 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow==2.10.0) (3.7) Requirement already satisfied: requests=2.21.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow==2.10.0) (2.32.3) Requirement already satisfied: tensorboarddataserver=0.6.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow==2.10.0) (0.6.1) Requirement already satisfied: tensorboardpluginwit>=1.6.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow==2.10.0) (1.8.1) Requirement already satisfied: werkzeug>=1.0.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow==2.10.0) (3.1.3) Requirement already satisfied: cachetools=2.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0) (5.5.2)        Requirement already satisfied: pyasn1modules>=0.2.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0) (0.4.2)         Requirement already satisfied: rsa=3.1.4 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0) (4.9) Requirement already satisfied: requestsoauthlib>=0.7.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauthoauthlib=0.4.1>tensorboard=2.10>tensorflow==2.10.0) (2.0.0) Requirement already satisfied: charsetnormalizer=2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0) (3.4.1)        Requirement already satisfied: idna=2.5 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0) (3.10) Requirement already satisfied: urllib3=1.21.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0) (2.3.0) Requirement already satisfied: certifi>=2017.4.17 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0) (2025.1.31) Requirement already satisfied: MarkupSafe>=2.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from werkzeug>=1.0.1>tensorboard=2.10>tensorflow==2.10.0) (3.0.2) Requirement already satisfied: pyasn1=0.6.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from pyasn1modules>=0.2.1>googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0) (0.6.1) Requirement already satisfied: oauthlib>=3.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requestsoauthlib>=0.7.0>googleauthoauthlib=0.4.1>tensorboard=2.10>tensorflow==2.10.0) (3.2.2) PS C:\xampp\htdocs\SkinToneAnalysis> pip install scikitlearn >> python skin_analysis_model/train_model.py >> Collecting scikitlearn   Downloading scikit_learn1.6.1cp310cp310win_amd64.whl.metadata (15 kB) Requirement already satisfied: numpy>=1.19.5 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from scikitlearn) (2.2.4) Collecting scipy>=1.6.0 (from scikitlearn)   Downloading scipy1.15.2cp310cp310win_amd64.whl.metadata (60 kB) Collecting joblib>=1.2.0 (from scikitlearn)   Downloading joblib1.4.2py3noneany.whl.metadata (5.4 kB) Collecting threadpoolctl>=3.1.0 (from scikitlearn)   Downloading threadpoolctl3.6.0py3noneany.whl.metadata (13 kB) Downloading scikit_learn1.6.1cp310cp310win_amd64.whl (11.1 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.1/11.1 MB 3.7 MB/s eta 0:00:00 Downloading joblib1.4.2py3noneany.whl (301 kB) Downloading scipy1.15.2cp310cp310win_amd64.whl (41.2 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.2/41.2 MB 3.1 MB/s eta 0:00:00 Downloading threadpoolctl3.6.0py3noneany.whl (18 kB) Installing collected packages: threadpoolctl, scipy, joblib, scikitlearn Successfully installed joblib1.4.2 scikitlearn1.6.1 scipy1.15.2 threadpoolctl3.6.0 20250329 19:56:01.625860: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found 20250329 19:56:01.627102: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. A module that was compiled using NumPy 1.x cannot be run in NumPy 2.2.4 as it may crash. To support both 1.x and 2.x versions of NumPy, modules must be compiled with NumPy 2.0. Some module may need to rebuild instead e.g. with 'pybind11>=2.12'. If you are a user of the module, the easiest solution will be to downgrade to 'numpy     import tensorflow as tf   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\__init__.py"", line 37, in      from tensorflow.python.tools import module_util as _module_util   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\__init__.py"", line 37, in      from tensorflow.python.eager import context   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\eager\context.py"", line 35, in      from tensorflow.python.client import pywrap_tf_session   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\client\pywrap_tf_session.py"", line 19, in      from tensorflow.python.client._pywrap_tf_session import * AttributeError: _ARRAY_API not found A module that was compiled using NumPy 1.x cannot be run in NumPy 2.2.4 as it may crash. To support both 1.x and 2.x versions of NumPy, modules must be compiled with NumPy 2.0. Some module may need to rebuild instead e.g. with 'pybind11>=2.12'. If you are a user of the module, the easiest solution will be to downgrade to 'numpy     import tensorflow as tf   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\__init__.py"", line 37, in      from tensorflow.python.tools import module_util as _module_util   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\__init__.py"", line 42, in      from tensorflow.python import data   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\__init__.py"", line 21, in      from tensorflow.python.data import experimental   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\__init__.py"", line 96, in      from tensorflow.python.data.experimental import service   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\service\__init__.py"", line 419, in      from tensorflow.python.data.experimental.ops.data_service_ops import distribute   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\ops\data_service_ops.py"", line 24, in      from tensorflow.python.data.experimental.ops import compression_ops   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\ops\compression_ops.py"", line 16, in      from tensorflow.python.data.util import structure   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\util\structure.py"", line 23, in      from tensorflow.python.data.util import nest   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\util\nest.py"", line 36, in      from tensorflow.python.framework import sparse_tensor as _sparse_tensor   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\framework\sparse_tensor.py"", line 24, in      from tensorflow.python.framework import constant_op   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\framework\constant_op.py"", line 25, in      from tensorflow.python.eager import execute   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\eager\execute.py"", line 23, in      from tensorflow.python.framework import dtypes   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\framework\dtypes.py"", line 29, in      from tensorflow.python.lib.core import _pywrap_bfloat16 AttributeError: _ARRAY_API not found ImportError: numpy.core._multiarray_umath failed to import ImportError: numpy.core.umath failed to import Traceback (most recent call last):   File ""C:\xampp\htdocs\SkinToneAnalysis\skin_analysis_model\train_model.py"", line 2, in      import tensorflow as tf   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\__init__.py"", line 37, in      from tensorflow.python.tools import module_util as _module_util   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\__init__.py"", line 42, in      from tensorflow.python import data   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\__init__.py"", line 21, in      from tensorflow.python.data import experimental   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\__init__.py"", line 96, in      from tensorflow.python.data.experimental import service   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\service\__init__.py"", line 419, in      from tensorflow.python.data.experimental.ops.data_service_ops import distribute   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\ops\data_service_ops.py"", line 24, in      from tensorflow.python.data.experimental.ops import compression_ops   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\ops\compression_ops.py"", line 16, in      from tensorflow.python.data.util import structure   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\util\structure.py"", line 23, in      from tensorflow.python.data.util import nest   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\util\nest.py"", line 36, in      from tensorflow.python.framework import sparse_tensor as _sparse_tensor   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\framework\sparse_tensor.py"", line 24, in      from tensorflow.python.framework import constant_op   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\framework\constant_op.py"", line 25, in      from tensorflow.python.eager import execute   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\eager\execute.py"", line 23, in      from tensorflow.python.framework import dtypes   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\framework\dtypes.py"", line 34, in      _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type() TypeError: Unable to convert function return value to a Python type! The signature was         () > handle PS C:\xampp\htdocs\SkinToneAnalysis> conda create name tf_env python=3.9 >> conda activate tf_env >> pip install tensorflow >> conda : The term 'conda' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the  name, or if a path was included, verify that the path is correct and try again. At line:1 char:1 + conda create name tf_env python=3.9 + ~~~~~     + CategoryInfo          : ObjectNotFound: (conda:String) [], CommandNotFoundException     + FullyQualifiedErrorId : CommandNotFoundException conda : The term 'conda' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the  name, or if a path was included, verify that the path is correct and try again. At line:2 char:1 + conda activate tf_env + ~~~~~     + CategoryInfo          : ObjectNotFound: (conda:String) [], CommandNotFoundException     + FullyQualifiedErrorId : CommandNotFoundException Requirement already satisfied: tensorflow in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (2.10.0) Requirement already satisfied: abslpy>=1.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.2.1) Requirement already satisfied: astunparse>=1.6.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.6.3) Requirement already satisfied: flatbuffers>=2.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (25.2.10) Requirement already satisfied: gast=0.2.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (0.4.0) Requirement already satisfied: googlepasta>=0.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (0.2.0) Requirement already satisfied: h5py>=2.9.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (3.13.0) Requirement already satisfied: keraspreprocessing>=1.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.1.2) Requirement already satisfied: libclang>=13.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (18.1.1) Requirement already satisfied: numpy>=1.20 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.2.4) Requirement already satisfied: opteinsum>=2.3.2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (3.4.0) Requirement already satisfied: packaging in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (24.2) Requirement already satisfied: protobuf=3.9.2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (3.19.6) Requirement already satisfied: setuptools in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (78.1.0) Requirement already satisfied: six>=1.12.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.17.0) Requirement already satisfied: termcolor>=1.1.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.5.0) Requirement already satisfied: typingextensions>=3.6.6 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (4.13.0) Requirement already satisfied: wrapt>=1.11.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.17.2) Requirement already satisfied: tensorflowiogcsfilesystem>=0.23.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (0.31.0) Requirement already satisfied: grpcio=1.24.3 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.71.0) Requirement already satisfied: tensorboard=2.10 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.10.1) Requirement already satisfied: tensorflowestimator=2.10.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.10.0) Requirement already satisfied: keras=2.10.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.10.0) Requirement already satisfied: wheel=0.23.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from astunparse>=1.6.0>tensorflow) (0.45.1) Requirement already satisfied: googleauth=1.6.3 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow) (2.38.0) Requirement already satisfied: googleauthoauthlib=0.4.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow) (0.4.6) Requirement already satisfied: markdown>=2.6.8 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow) (3.7) Requirement already satisfied: requests=2.21.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow) (2.32.3) Requirement already satisfied: tensorboarddataserver=0.6.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow) (0.6.1) Requirement already satisfied: tensorboardpluginwit>=1.6.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow) (1.8.1) Requirement already satisfied: werkzeug>=1.0.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow) (3.1.3) Requirement already satisfied: cachetools=2.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauth=1.6.3>tensorboard=2.10>tensorflow) (5.5.2) Requirement already satisfied: pyasn1modules>=0.2.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauth=1.6.3>tensorboard=2.10>tensorflow) (0.4.2) Requirement already satisfied: rsa=3.1.4 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauth=1.6.3>tensorboard=2.10>tensorflow) (4.9) Requirement already satisfied: requestsoauthlib>=0.7.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauthoauthlib=0.4.1>tensorboard=2.10>tensorflow) (2.0.0)   Requirement already satisfied: charsetnormalizer=2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow) (3.4.1) Requirement already satisfied: idna=2.5 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow) (3.10) Requirement already satisfied: urllib3=1.21.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow) (2.3.0) Requirement already satisfied: certifi>=2017.4.17 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow) (2025.1.31) Requirement already satisfied: MarkupSafe>=2.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from werkzeug>=1.0.1>tensorboard=2.10>tensorflow) (3.0.2) Requirement already satisfied: pyasn1=0.6.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from pyasn1modules>=0.2.1>googleauth=1.6.3>tensorboard=2.10>tensorflow) (0.6.1) Requirement already satisfied: oauthlib>=3.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requestsoauthlib>=0.7.0>googleauthoauthlib=0.4.1>tensorboard=2.10>tensorflow) (3.2.2) PS C:\xampp\htdocs\SkinToneAnalysis>  >> pip install upgrade tensorflow >> Requirement already satisfied: tensorflow in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (2.10.0) Collecting tensorflow   Downloading tensorflow2.19.0cp310cp310win_amd64.whl.metadata (4.1 kB) Requirement already satisfied: abslpy>=1.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.2.1) Requirement already satisfied: astunparse>=1.6.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.6.3) Requirement already satisfied: flatbuffers>=24.3.25 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (25.2.10) Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (0.4.0) Requirement already satisfied: googlepasta>=0.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (0.2.0) Requirement already satisfied: libclang>=13.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (18.1.1) Requirement already satisfied: opteinsum>=2.3.2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (3.4.0) Requirement already satisfied: packaging in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (24.2) Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,=3.20.3 (from tensorflow)   Downloading protobuf5.29.4cp310abi3win_amd64.whl.metadata (592 bytes) Requirement already satisfied: requests=2.21.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.32.3) Requirement already satisfied: setuptools in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (78.1.0) Requirement already satisfied: six>=1.12.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.17.0) Requirement already satisfied: termcolor>=1.1.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.5.0) Requirement already satisfied: typingextensions>=3.6.6 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (4.13.0) Requirement already satisfied: wrapt>=1.11.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.17.2) Requirement already satisfied: grpcio=1.24.3 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.71.0) Collecting tensorboard~=2.19.0 (from tensorflow)   Downloading tensorboard2.19.0py3noneany.whl.metadata (1.8 kB) Collecting keras>=3.5.0 (from tensorflow)   Downloading keras3.9.1py3noneany.whl.metadata (6.1 kB) Collecting numpy=1.26.0 (from tensorflow)   Downloading numpy2.1.3cp310cp310win_amd64.whl.metadata (60 kB) Requirement already satisfied: h5py>=3.11.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (3.13.0) Collecting mldtypes=0.5.1 (from tensorflow)   Downloading ml_dtypes0.5.1cp310cp310win_amd64.whl.metadata (22 kB) Requirement already satisfied: tensorflowiogcsfilesystem>=0.23.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (0.31.0) Requirement already satisfied: wheel=0.23.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from astunparse>=1.6.0>tensorflow) (0.45.1) Collecting rich (from keras>=3.5.0>tensorflow)   Downloading rich13.9.4py3noneany.whl.metadata (18 kB) Collecting namex (from keras>=3.5.0>tensorflow)   Downloading namex0.0.8py3noneany.whl.metadata (246 bytes) Collecting optree (from keras>=3.5.0>tensorflow)   Downloading optree0.14.1cp310cp310win_amd64.whl.metadata (50 kB) Requirement already satisfied: charsetnormalizer=2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorflow) (3.4.1) Requirement already satisfied: idna=2.5 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorflow) (3.10) Requirement already satisfied: urllib3=1.21.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorflow) (2.3.0) Requirement already satisfied: certifi>=2017.4.17 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorflow) (2025.1.31) Requirement already satisfied: markdown>=2.6.8 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard~=2.19.0>tensorflow) (3.7) Collecting tensorboarddataserver=0.7.0 (from tensorboard~=2.19.0>tensorflow)   Downloading tensorboard_data_server0.7.2py3noneany.whl.metadata (1.1 kB) Requirement already satisfied: werkzeug>=1.0.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard~=2.19.0>tensorflow) (3.1.3) Requirement already satisfied: MarkupSafe>=2.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from werkzeug>=1.0.1>tensorboard~=2.19.0>tensorflow) (3.0.2) Collecting markdownitpy>=2.2.0 (from rich>keras>=3.5.0>tensorflow)   Downloading markdown_it_py3.0.0py3noneany.whl.metadata (6.9 kB) Collecting pygments=2.13.0 (from rich>keras>=3.5.0>tensorflow)   Downloading pygments2.19.1py3noneany.whl.metadata (2.5 kB) Collecting mdurl~=0.1 (from markdownitpy>=2.2.0>rich>keras>=3.5.0>tensorflow)   Downloading mdurl0.1.2py3noneany.whl.metadata (1.6 kB) Downloading tensorflow2.19.0cp310cp310win_amd64.whl (375.7 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 375.7/375.7 MB 2.0 MB/s eta 0:00:00 Downloading keras3.9.1py3noneany.whl (1.3 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 2.0 MB/s eta 0:00:00 Downloading ml_dtypes0.5.1cp310cp310win_amd64.whl (209 kB) Downloading numpy2.1.3cp310cp310win_amd64.whl (12.9 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/12.9 MB 2.2 MB/s eta 0:00:00 Downloading protobuf5.29.4cp310abi3win_amd64.whl (434 kB) Downloading tensorboard2.19.0py3noneany.whl (5.5 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 2.6 MB/s eta 0:00:00 Downloading tensorboard_data_server0.7.2py3noneany.whl (2.4 kB) Downloading namex0.0.8py3noneany.whl (5.8 kB) Downloading optree0.14.1cp310cp310win_amd64.whl (296 kB) Downloading rich13.9.4py3noneany.whl (242 kB) Downloading markdown_it_py3.0.0py3noneany.whl (87 kB) Downloading pygments2.19.1py3noneany.whl (1.2 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 742.6 kB/s eta 0:00:00 Downloading mdurl0.1.2py3noneany.whl (10.0 kB) Installing collected packages: namex, tensorboarddataserver, pygments, protobuf, optree, numpy, mdurl, tensorboard, mldtypes, markdownitpy, rich, keras, tensorflow   Attempting uninstall: tensorboarddataserver     Found existing installation: tensorboarddataserver 0.6.1     Uninstalling tensorboarddataserver0.6.1:       Successfully uninstalled tensorboarddataserver0.6.1   WARNING: The script pygmentize.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   Attempting uninstall: protobuf     Found existing installation: protobuf 3.19.6     Uninstalling protobuf3.19.6:       Successfully uninstalled protobuf3.19.6   Attempting uninstall: numpy     Found existing installation: numpy 2.2.4     Uninstalling numpy2.2.4:       Successfully uninstalled numpy2.2.4   WARNING: The scripts f2py.exe and numpyconfig.exe are installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   Attempting uninstall: tensorboard     Found existing installation: tensorboard 2.10.1     Uninstalling tensorboard2.10.1:       Successfully uninstalled tensorboard2.10.1   WARNING: The script tensorboard.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The script markdownit.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   Attempting uninstall: keras     Found existing installation: keras 2.10.0     Uninstalling keras2.10.0:       Successfully uninstalled keras2.10.0   Attempting uninstall: tensorflow     Found existing installation: tensorflow 2.10.0     Uninstalling tensorflow2.10.0:       Successfully uninstalled tensorflow2.10.0   WARNING: The scripts import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe and toco.exe are installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation. Successfully installed keras3.9.1 markdownitpy3.0.0 mdurl0.1.2 mldtypes0.5.1 namex0.0.8 numpy2.1.3 optree0.14.1 protobuf5.29.4 pygments2.19.1 rich13.9.4 tensorboard2.19.0 tensorboarddataserver0.7.2 tensorflow2.19.0 PS C:\xampp\htdocs\SkinToneAnalysis> python skin_analysis_model/train_model.py Traceback (most recent call last):   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\xampp\htdocs\SkinToneAnalysis\skin_analysis_model\train_model.py"", line 2, in      import tensorflow as tf   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. PS C:\xampp\htdocs\SkinToneAnalysis> ```  Relevant log output ```shell ```",2025-03-29T17:11:20Z,stat:awaiting response type:support stale TF 2.18,closed,0,9,https://github.com/tensorflow/tensorflow/issues/90245,What is all of this ,"Hi  , Apologies for the delay, and thanks for raising your concern here. Could you please specify exactly what you are trying to install? Additionally, could you provide the installation steps or code where you are encountering the issue? This will help us debug the problem more effectively. In the meantime, I am sharing the official documentation to check compatibility versions, it might be helpful to you. Looking forward to your response. Thank you!","Hi Venkat6871, Thanks so much for your help and patience! I'm really trying to learn this TensorFlow stuff on my own, without any formal classes, which is proving to be quite a challenge. I'm struggling a bit with the installation, and I'm wondering if you could recommend any good courses or training programs that might help me get a firmer grasp on things? Any suggestions you have would be greatly appreciated! Thanks again for your support! Best, Christopher Swain Sent from Yahoo Christopher Swain is the only one who can make a difference with the new system that we have installed on our website.Mail for iPhone.. On Tuesday, April 1, 2025, 9:43 AM, Venkat6871 ***@***.***> wrote: Hi  , Apologies for the delay, and thanks for raising your concern here. Could you please specify exactly what you are trying to install? Additionally, could you provide the installation steps or code where you are encountering the issue? This will help us debug the problem more effectively. In the meantime, I am sharing the official documentation to check compatibility versions, it might be helpful to you. Looking forward to your response. Thank you! — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***> Venkat6871 left a comment (tensorflow/tensorflow CC(Error fixing)) Hi  , Apologies for the delay, and thanks for raising your concern here. Could you please specify exactly what you are trying to install? Additionally, could you provide the installation steps or code where you are encountering the issue? This will help us debug the problem more effectively. In the meantime, I am sharing the official documentation to check compatibility versions, it might be helpful to you. Looking forward to your response. Thank you! — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***>","It seems there are 2 errors here. First, you are trying to combine code compiled with Numpy v1 with code compiled with Numpy v2. This cannot work. Next, you are running into CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.)",Don't have a clue what you're talking about?,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"This is now resolved, Thank you for your contributions.",Are you satisfied with the resolution of your issue? Yes No,"Dear Christopher Swain, Thank you for your feedback regarding the resolution of issue CC(Error fixing) on the TensorFlow GitHub repository. We appreciate you taking the time to let us know you are satisfied. Your input helps us improve our processes and better assist users in the future. If you encounter any further issues, please don't hesitate to open a new issue or utilize the existing channels for support. We value your contributions to the TensorFlow community. Sent from Yahoo Christopher Swain is the only one who can make a difference with the new system that we have installed on our website.Mail for iPhone.. On Tuesday, April 22, 2025, 12:54 AM, googlemlbutler[bot] ***@***.***> wrote: Are you satisfied with the resolution of your issue? Yes No — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***> googlemlbutler[bot] left a comment (tensorflow/tensorflow CC(Error fixing)) Are you satisfied with the resolution of your issue? Yes No — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***>"
tpu,RahulShridhar10,During pyinstaller," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19  Custom code Yes  OS platform and distribution Windows  Mobile device _No response_  Python version 3.11.7  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? This error happens because _pywrap_tensorflow_internal.pyd depends on missing or incompatible DLLs. Here’s how to fix it:  1. Check Python and TensorFlow Compatibility Run this to check your Python version: python version Now check which TensorFlow version supports your Python version: TensorFlow Python Compatibility If Python is 3.11+, TensorFlow 2.10 or earlier won’t work. If Python is 3.83.10, it should be fine. If needed, downgrade TensorFlow: pip install tensorflow==2.10.0  2. Ensure Microsoft Visual C++ Redistributable is Installed TensorFlow requires Microsoft Visual C++ 20152022 Redistributable. Download and install: VC++ Redistributable (x64) Then restart your computer.  3. Verify Missing DLLs with Dependency Walker 1. Download Dependency Walker here. 2. Open _pywrap_tensorflow_internal.pyd with it. 3. If any DLL is missing (marked in red), install it.  4. Add TensorFlow’s DLL Path to Environment Variables Run this in Command Prompt (Admin): set PATH=%PATH%;C:\Users\Rahul\OneDrive\Desktop\Reader\Palm_Leaf_Reader\Backend\venv\Lib\sitepackages\tensorflow\ Then retry running backend.exe.  5. Rebuild PyInstaller Executable Run PyInstaller with these options: pyinstaller onefile hiddenimport=tensorflow hiddenimport=tensorflow.python adddata ""venv\Lib\sitepackages\tensorflow;tensorflow"" name backend app.py Then run the new backend.exe.  Try these and let me know if the issue persists!  Standalone code to reproduce the issue ```shell Traceback (most recent call last): File ""tensorflow\python\pywrap_tensorflow.py"", line 73, in  ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization ro utine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""app.py"", line 22, in  File ""PyInstaller\loader\pyimod02_importers.py"", line 450, in exec_module File ""tensorflow\ __ init __. py"", line 40, in  File ""PyInstaller\loader\pyimod02_importers.py"", line 450, in exec_module File ""tensorflow\python\pywrap_tensorflow.py"", line 88, in  ImportError: Traceback (most recent call last): File ""tensorflow\python\pywrap_tensorflow.py"", line 73, in  ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization ro utine failed. + × Failed to load the native TensorFlow runtime. See https://www. tensorflow. org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. [PYI3652:ERROR] Failed to execute script 'app' due to unhandled exception! ```  Relevant log output ```shell ```",2025-03-29T17:03:41Z,stat:awaiting response type:build/install subtype:windows TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/90244,"Hi  , Apologies for the delay, could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!", CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.),Are you satisfied with the resolution of your issue? Yes No
tpu,default1360,`Aborted` in `tf.raw_ops.Unbatch`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.19.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tf.raw_ops.Unbatch in tf2.19.0. This bug can be used to trigger a denial of service attack. I have attached a gist for your reference.  Standalone code to reproduce the issue ```shell import tensorflow as tf from tensorflow.raw_ops import Unbatch batched_tensor = tf.constant([[1.0], [2.0], [3.0]], dtype=tf.float32) batch_index = tf.constant([0], dtype=tf.int64) id = tf.constant([0], dtype=tf.int64) timeout_micros = 1000000 unbatched_result = Unbatch(batched_tensor=batched_tensor, batch_index=batch_index, id=id, timeout_micros=timeout_micros) ```  Relevant log output ```shell Aborted ```",2025-03-29T13:03:28Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90243,Are you satisfied with the resolution of your issue? Yes No
tpu,default1360,`Aborted` in `tf.raw_ops.Transpose`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.19.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tf.raw_ops.Transpose in tf2.19.0. This bug can be used to trigger a denial of service attack. I have attached a gist for your reference.  Standalone code to reproduce the issue ```shell import tensorflow as tf def test_bug():     x = tf.zeros((3, 3), dtype=tf.float32)     x = tf.Variable(x, trainable=True)     permutation = tf.constant([1, 2])     y = tf.raw_ops.Transpose(x=x, perm=permutation)     with tf.GradientTape() as tape:         z = tf.reduce_sum(y)     z.backward() if __name__ == ""__main__"":     test_bug() ```  Relevant log output ```shell Aborted ```",2025-03-29T13:01:20Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90242,Are you satisfied with the resolution of your issue? Yes No
tpu,default1360,`Aborted` in `tf.raw_ops.BatchMatMulV2`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tf.raw_ops.BatchMatMulV2 in tf2.19.0. This bug can be used to trigger a denial of service attack. I have attached a gist for your reference.  Standalone code to reproduce the issue ```shell import tensorflow as tf import time def test_bug():     tf.config.experimental_run_functions_eagerly(True)     A = tf.fill((100, 100), 1e45)     B = tf.random.uniform((100, 100), minval=0, maxval=1, dtype=tf.float32)     for _ in range(10):         tf.raw_ops.BatchMatMulV2(x=A[tf.newaxis, ...], y=B[tf.newaxis, ...])     n_iter = 300     times = []     X = A     for i in range(n_iter):         t0 = time.time()         X = tf.raw_ops.BatchMatMulV2(x=X[tf.newaxis, ...], y=B[tf.newaxis, ...])         t1 = time.time()         times.append(t1  t0)     early_avg = sum(times[:50]) / 50     late_avg  = sum(times[50:]) / 50 if __name__ == ""__main__"":     test_bug() ```  Relevant log output ```shell Aborted ```",2025-03-29T13:00:07Z,stat:awaiting tensorflower type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/90241,"I can reproduce this issue in TensorFlow 2.19.0. The problem occurs when BatchMatMulV2 processes extremely small float32 values (near 1e45) in repeated multiplications, causing numerical underflow that results in a hard crash instead of graceful error handling. Reproduction case attached shows how these operations cause an ""Aborted"" error when values cascade below representable limits. Potential fixes could include: 1. Adding guards in BatchMatMulV2 implementation to detect potential underflow conditions 2. Gracefully raising exceptions instead of aborting when numerical instability occurs 3. Implementing value clamping to prevent catastrophic underflow This is a potential security concern since it can be used for denial of service attacks. I'm willing to work on a fix if guidance is provided on the appropriate approach. Would checking for extremely small values in the C++ implementation be the right direction?","I was able to reproduce this issue using TensorFlow  2.19.0, and the nightly version. Please find the gist attached for reference. Thank you!"
tpu,SarahClementine,Failed," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.9  Custom code Yes  OS platform and distribution win 11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Traceback (most recent call last):   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""c:\xampp\htdocs\SkinToneAnalysis\skin_analysis_model\train_model.py"", line 2, in      import tensorflow as tf   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.  Standalone code to reproduce the issue ```shell Traceback (most recent call last):   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""c:\xampp\htdocs\SkinToneAnalysis\skin_analysis_model\train_model.py"", line 2, in      import tensorflow as tf   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```  Relevant log output ```shell ```",2025-03-29T12:17:00Z,stat:awaiting response type:build/install subtype:windows TF 2.9,closed,0,2,https://github.com/tensorflow/tensorflow/issues/90240,"Hi  , Apologies for the delay, could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Are you satisfied with the resolution of your issue? Yes No
tpu,lixiaohui2020,undefined symbol: _ZN6tflite4impl18InterpreterBuilderclEPNSt3__110unique_ptrINS0_11InterpreterENS2_14default_deleteIS4_EEEE," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version no  Custom code No  OS platform and distribution Linux Ubuntu 16.04  Mobile device Android 15  Python version 3.9  Bazel version 7.4.1  GCC/compiler version clang14  CUDA/cuDNN version no  GPU model and memory no  Current behavior? When i build the application based tensorflowLite.so at the android platform, it shows ""undefined symbol: _ZN6tflite4impl18InterpreterBuilderclEPNSt3__110unique_ptrINS0_11InterpreterENS2_14default_deleteIS4_EEEE. I check the tensorflowLite.so by using the command nm D tensorflowLite.so | grep InterpreterBuilder, found _ZN6tflite4impl18InterpreterBuilderclEPNSt3__110unique_ptrINS0_11InterpreterENS2_14default_deleteIS4_EEEE. I cannot how to solve this problem, thanks My Android.bp is: cc_library {     name: ""libaw_speech_enhencement_reagle"", 	sdk_version:""current"", 	stl:""libc++"",     vendor_available: true, 	compile_multilib: ""both"",     export_include_dirs: [ 	""."", 	""./include"", 	""./3rdparty/include"",     ], 	rtti: true, 	cflags: [         ""Wall"",         ""Werror"",         ""Wextra"",         ""Wnounusedparameter"", 		""Wnounusedvariable"",         ""Wnounusedprivatefield"", 		""Wnounusedfunction"", 		""Wnowritablestrings"", 		""Wnononpodvarargs"",         ""frtti"",     ],     shared_libs: [ 		""liblog"", 		""libtensorflowLite"", 	],     static_libs:[ 		""libomp"", 	],     srcs: [         ""./src/*.cpp"",     ], 	header_libs: [         ""jni_headers"",     ], } cc_prebuilt_library_shared {     name: ""libtensorflowLite"",     sdk_version:""current"",     vendor_available: true,     arch: {         arm: {             srcs: [                 ""3rdparty/nativeLibs/armeabiv7a/libtensorflowLite.so"",             ],         },         arm64: {             srcs: [                 ""3rdparty/nativeLibs/arm64v8a/libtensorflowLite.so"",             ],         },     },     shared_libs: [         ""liblog"",     ], }  Standalone code to reproduce the issue ```shell My xx.: ```  Relevant log output ```shell ```",2025-03-29T09:20:10Z,stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux,closed,0,4,https://github.com/tensorflow/tensorflow/issues/90239,"Hi,  Apologize for the delay in my response, Please make sure that the prebuilt `libtensorflowLite.so` was compiled with the same NDK version as your project. Older NDKs (e.g. < r18) may cause symbol mismatches due to STL differences I believe you followed Set up build environment without Docker section of official documentation steps If possible could you please help us with minimal code or Github repo along with complete steps to replicate the similar behavior from our end to investigate this issue further from our end ? Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,siddij3,SeparableConv1D with causal padding is not supported," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version v1.12.1124066gef70275b27b 2.20.0dev20250328  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version Python 3.11.5   Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I had a download of tensorflow where tensorflow.keras.layers.SeparableConv1D had the padding layer of causal. I went through dependency hell, wiped everything and reinstalled the pip packages, and when trying to rerun my model training code, I get  ``` ValueError: The `padding` argument must be a list/tuple or one of {'valid', 'same'}. Received: causal. ``` It's very clearly not supported, but I was running it fine yesterday. I can go into the source code and just permit ""causal"" but I don't know if that would break something else.  Standalone code to reproduce the issue ```shell import tensorflow as tf print(tf.__version__) print(tf.version.GIT_VERSION, tf.version.VERSION) model = tf.keras.Sequential() model.add(tf.keras.Input((None,None,3))) model.add(tf.keras.layers.SeparableConv1D(                     dilation_rate=1,                     filters = 3,                     kernel_size=2,                     padding='causal', )) ```  Relevant log output ```shell ValueError                                Traceback (most recent call last) Cell In[1], line 7       5 model = tf.keras.Sequential()       6 model.add(tf.keras.Input((None,None,3))) > 7 model.add(tf.keras.layers.SeparableConv1D(       8                     dilation_rate=1,       9                     filters = 3,      10                     kernel_size=2,      11                     padding='causal',      12 )) File c:\Program Files\Python311\Lib\sitepackages\keras\src\layers\convolutional\separable_conv1d.py:121, in SeparableConv1D.__init__(self, filters, kernel_size, strides, padding, data_format, dilation_rate, depth_multiplier, activation, use_bias, depthwise_initializer, pointwise_initializer, bias_initializer, depthwise_regularizer, pointwise_regularizer, bias_regularizer, activity_regularizer, depthwise_constraint, pointwise_constraint, bias_constraint, **kwargs)      98 def __init__(      99     self,     100     filters,    (...)    119     **kwargs,     120 ): > 121     super().__init__(     122         rank=1,     123         depth_multiplier=depth_multiplier,     124         filters=filters,     125         kernel_size=kernel_size,     126         strides=strides,     127         padding=padding,     128         data_format=data_format,     129         dilation_rate=dilation_rate,     130         activation=activation,     131         use_bias=use_bias,     132         depthwise_initializer=depthwise_initializer,     133         pointwise_initializer=pointwise_initializer,     134         bias_initializer=bias_initializer,     135         depthwise_regularizer=depthwise_regularizer,     136         pointwise_regularizer=pointwise_regularizer,     137         bias_regularizer=bias_regularizer,     138         activity_regularizer=activity_regularizer,     139         depthwise_constraint=depthwise_constraint,     140         pointwise_constraint=pointwise_constraint,     141         bias_constraint=bias_constraint,     142         **kwargs,     143     ) File c:\Program Files\Python311\Lib\sitepackages\keras\src\layers\convolutional\base_separable_conv.py:118, in BaseSeparableConv.__init__(self, rank, depth_multiplier, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, depthwise_initializer, pointwise_initializer, bias_initializer, depthwise_regularizer, pointwise_regularizer, bias_regularizer, activity_regularizer, depthwise_constraint, pointwise_constraint, bias_constraint, trainable, name, **kwargs)     114 self.strides = standardize_tuple(strides, rank, ""strides"")     115 self.dilation_rate = standardize_tuple(     116     dilation_rate, rank, ""dilation_rate""     117 ) > 118 self.padding = standardize_padding(padding)     119 self.data_format = standardize_data_format(data_format)     120 self.activation = activations.get(activation) File c:\Program Files\Python311\Lib\sitepackages\keras\src\utils\argument_validation.py:65, in standardize_padding(value, allow_causal)      63     allowed_values = {""valid"", ""same""}      64 if padding not in allowed_values: > 65     raise ValueError(      66         ""The `padding` argument must be a list/tuple or one of ""      67         f""{allowed_values}. ""      68         f""Received: {padding}""      69     )      70 return padding ValueError: The `padding` argument must be a list/tuple or one of {'same', 'valid'}. Received: causal ```",2025-03-28T20:38:57Z,stat:awaiting response type:support stale TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/90213,"Hi  , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow 2.19.0, and encountered the following error: ``` ValueError: The `padding` argument must be a list/tuple or one of {'same', 'valid'}. Received: causal ``` The main cause of this error is that TensorFlow only supports same or valid padding, as clearly mentioned in the official documentation. I am providing a gist here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
bert,copybara-service[bot],OSS NOOP change,OSS NOOP change Reverts 56d1b2306f948c83b2b243484a6c16c037696a85 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23790 from chaserileyroberts:chase/nccl_group 36ad18d8c604a6cb2559dcca8c29530beb5da888,2025-03-28T18:31:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90205
sharding,copybara-service[bot],Reduce compile time when using large sharded shapes by constructing sharding_tree at most once per parameter.,Reduce compile time when using large sharded shapes by constructing sharding_tree at most once per parameter.,2025-03-28T16:48:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90196
sharding,copybara-service[bot],PR #23790: Add explicit collectives grouping pass.,"PR CC(fix tf.Saver save and restore bug under distributed setting(with ps, master and worker)): Add explicit collectives grouping pass. Imported from GitHub PR https://github.com/openxla/xla/pull/23790 This PR adds an explicit frontend attribute to jitted JAX methods that will force the computation to be run in a single NCCL Group.  The intention of this change is to enable ""multidirectional communications"" that better saturate NVLink systems. We reuse the existing NCCLGroupThunk logic during the IR emitter stage, so we only need to introduce the async wrapper and call inliner. Here is an example of using this feature from JAX. ```python import jax from jax._src.xla_metadata import set_xla_metadata from jax.experimental.shard_map import shard_map from jax.sharding import Mesh, PartitionSpec as P from functools import partial num_devices = 4 mesh = Mesh(np.array(jax.devices()), ('i',))  Unique source target pairs for our two ppermutes later. perm_up = [(i, (i+1) % num_devices) for i in range(num_devices)] perm_down = [(i, (i1) % num_devices) for i in range(num_devices)]  NCCL Group computations _must_ be jitted. .jit def bidir_comms(a):     b = jax.lax.ppermute(a, ""i"", perm_up)     c = jax.lax.ppermute(a, ""i"", perm_down)     return b, c .jit (shard_map, mesh=mesh, in_specs=P(None, 'i'), out_specs=P(None, 'i')) def groups(a):     Running our jitted function in this context will force the use of a NCCL Group.     with set_xla_metadata(_collectives_group="""", inlineable=""false""):         b, c = bidir_comms(a)     return b + c ``` This is nsys trace with the annotation. !Screenshot 20250316 at 8 32 44 PM Vs without the additional annotation !Screenshot 20250316 at 8 31 56 PM As you can see, there is only a single NCCL kernel in the annotated example. Copybara import of the project:  d5fa129a55c089ee8374df4982be748517775bbe by chaserileyroberts : Add explicit nccl grouping pass  36ad18d8c604a6cb2559dcca8c29530beb5da888 by chaserileyroberts : Nccl>Collectives Merging this change closes CC(fix tf.Saver save and restore bug under distributed setting(with ps, master and worker)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23790 from chaserileyroberts:chase/nccl_group 36ad18d8c604a6cb2559dcca8c29530beb5da888",2025-03-28T16:37:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90194
opt,copybara-service[bot],Add contracting split (split-K) selection logic to dynamic search space,"Add contracting split (splitK) selection logic to dynamic search space First step towards the full search space. We figure out the contracting split based on the problem size, and a few other properties of the search space (still hardcoded for now). We also have an option for forcing a specific split, which is helpful for both disabling autotuning that parameter, and will facilitate support for analytically setting it in the future.",2025-03-28T12:14:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90185
xla compile,copybara-service[bot],PR #24269: Fix for fusion wrapper on async computations,"PR CC([tensor_forest] Add total variance in the additional_data of tree leaf): Fix for fusion wrapper on async computations Imported from GitHub PR https://github.com/openxla/xla/pull/24269 Previously, in JAX if you do a simple stream annotated computation with just a single instruction, i.e. ```python ('gpu_stream:1') .jit def h(x, y):   return x * y ``` Then the XLA compiler would fail with an error like ``` jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Unsupported instruction opcode: multiply ``` This is because the fusion wrapper would not correctly look inside of async computations, leaving the instructions unfused.  To fix this, we simply add `kCall` and `kAsyncStart` to the set of instructions that are treated recursively in the fusion wrapper. Copybara import of the project:  bef9c2829a585727094c270b6c7e1e05ce304819 by chaserileyroberts : Added fix for some fusion issues using compute_on  1d909f1fb4a5bd11053be35f15e42796f7adb643 by chaserileyroberts : Update fusion wrapper test  0e39915f444607c5518bb7a5ce4a168801bfee8d by chaserileyroberts : Added {0} to dtype descriptions  5e51b3897c45354e929b061d131f8b2a6efde36a by chaser : xla_cc_test > xla_test  37df8115138ba07ed3f735787f62830d4abafffc by chaser : RunAndCompare > RunAndCompareNoHloPasses Merging this change closes CC([tensor_forest] Add total variance in the additional_data of tree leaf) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24269 from chaserileyroberts:chase/compute_on_fusion_fix 37df8115138ba07ed3f735787f62830d4abafffc",2025-03-28T11:24:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90183
opt,copybara-service[bot],[XLA:GPU] Remove p2p rewriter in favor of xla_gpu_experimental_pipeline_parallelism_opt_level,[XLA:GPU] Remove p2p rewriter in favor of xla_gpu_experimental_pipeline_parallelism_opt_level,2025-03-28T00:15:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90154
tpu,ymodak,Fix conv2d dimension mismatch,"Fix dimension mismatch in tf.nn.conv2d usage. Fixes https://github.com/tensorflow/tensorflow/issues/89852 The original code produced an error due to an incorrect transpose operation on the input tensor, leading to incompatible dimensions for the convolution. The fix removes the unnecessary transpose, adjusts the filter shape, and adds/removes dummy dimensions to ensure the tensors are 4D as expected by tf.nn.conv2d with 'NHWC' data format.  A test case is provided to verify the fix, asserting that the convolution operation runs without errors and produces the expected output shape.",2025-03-27T22:31:04Z,size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90150
opt,copybara-service[bot],Add HloParserOptions to CreateModuleFromString in hlo_module_util,Add HloParserOptions to CreateModuleFromString in hlo_module_util Allows passing through `HloParserOptions` to `ParseAndReturnUnverifiedModule`.,2025-03-27T22:08:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90148
tpu,melanierbutler,"Memory leak in tfp.math.minimize, reproducible from two tutorial notebooks."," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code No  OS platform and distribution M2 Mac MacOS  Sequoia 15.2  Mobile device _No response_  Python version 3.11.6  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I recently filed this as an issue in tensorflow/probability but perhaps I should file it here instead: Python version: 3.11.6 tensorflow==2.18.0      via        r requirements.in        tfkeras tensorflowprobability==0.24.0      via r requirements.in tfkeras==2.18.0      via r requirements.in I am trying to perform an analysis built on the multiple changepoint detection and Bayesian model selection code found in this tutorial notebook, specifically the ""Unknown number of states"" portion: https://www.tensorflow.org/probability/examples/Multiple_changepoint_detection_and_Bayesian_model_selection In the process of running the analysis for multiple datasets I observed that the memory used increases with each successive analysis, by about 20 MB per run in my code. I confirmed that this occurs with the original tutorial notebook as well to ensure that there is not an error specific to my code. The easiest way to reproduce is to download the notebook, execute all cells, then rerun from the ""Unknown number of states"" section. I have tried adding `tf_keras.backend.clear_session()` between each run, deleting all created objects before rerunning, and running garbage collection, but nothing has released the ~20MB of memory that is allocated with each `tfp.math.minimize` run. The example I show in the standalone code below is from the ""Known number of states"" example, which has about 12 MiB increase in memory with each run. I see a similar issue with the Probabilistic PCA tutorial notebook. Memory increase with each run is ~3 MiB per run.  Standalone code to reproduce the issue ```shell import numpy as np import tensorflow as tf import tf_keras import tensorflow_probability as tfp from tensorflow_probability import distributions as tfd import os import psutil import gc true_rates = [40, 3, 20, 50] true_durations = [10, 20, 5, 35] observed_counts = tf.concat(     [tfd.Poisson(rate).sample(num_steps)      for (rate, num_steps) in zip(true_rates, true_durations)], axis=0) num_states = 4 initial_state_logits = tf.zeros([num_states])  uniform distribution daily_change_prob = 0.05 transition_probs = tf.fill([num_states, num_states],                            daily_change_prob / (num_states  1)) transition_probs = tf.linalg.set_diag(transition_probs,                                       tf.fill([num_states],                                               1  daily_change_prob)) trainable_log_rates = tf.Variable(   tf.math.log(tf.reduce_mean(observed_counts)) +   tf.random.stateless_normal([num_states], seed=(42, 42)),   name='log_rates') for i in range(20):     tf_keras.backend.clear_session()     gc.collect()     hmm = tfd.HiddenMarkovModel(       initial_distribution=tfd.Categorical(           logits=initial_state_logits),       transition_distribution=tfd.Categorical(probs=transition_probs),       observation_distribution=tfd.Poisson(log_rate=trainable_log_rates),       num_steps=len(observed_counts))     rate_prior = tfd.LogNormal(5, 5)     def log_prob():      return (tf.reduce_sum(rate_prior.log_prob(tf.math.exp(trainable_log_rates))) +              hmm.log_prob(observed_counts))     losses = tfp.math.minimize(         lambda: log_prob(),         optimizer=tf_keras.optimizers.legacy.Adam(learning_rate=0.1),         num_steps=100)     print(f""[{i+1}] Memory usage: {psutil.Process(os.getpid()).memory_info().rss / 1024 **2} MiB"")     del hmm, losses ```  Relevant log output ```shell [ CC(Add support for Python 3.x)] Memory usage: 583.109375 MiB [ CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"")] Memory usage: 596.3125 MiB [ CC(JVM, .NET Language Support)] Memory usage: 608.5 MiB [ CC(Installation over pip fails to import with protobuf 2.6.1)] Memory usage: 620.40625 MiB [ CC(Java interface)] Memory usage: 629.28125 MiB [ CC(Pretrained models)] Memory usage: 639.65625 MiB [ CC(API docs does not list RNNs)] Memory usage: 648.78125 MiB [ CC(Setting lower gcc version for cuda)] Memory usage: 662.71875 MiB [ CC(Typo in getting started guide)] Memory usage: 673.234375 MiB [ CC(Go API)] Memory usage: 688.53125 MiB [ CC(0.5.0 wheel install on Mac OS X using Homebrew python broken)] Memory usage: 699.40625 MiB [ CC(Remote worker configuration)] Memory usage: 710.140625 MiB [ CC([doc] typo)] Memory usage: 719.234375 MiB [ CC(g3doc format)] Memory usage: 728.609375 MiB [ CC(Quantized ops?)] Memory usage: 736.859375 MiB [ CC(iOS Support and Example)] Memory usage: 746.84375 MiB [ CC(Windows Support and Documentation)] Memory usage: 758.03125 MiB [ CC(C api)] Memory usage: 767.5625 MiB [ CC(Swift API)] Memory usage: 777.453125 MiB [ CC(CUDA 7.5 fails with pip install and docker (Ubuntu 14.04))] Memory usage: 785.0625 MiB ```",2025-03-27T17:43:23Z,stat:awaiting tensorflower type:bug TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/90134,"I was able to reproduce this issue using TensorFlow 2.18.0, 2.19.0, and the nightly version. Please find the gist attached for reference. Thank you!"
fp16,copybara-service[bot],PR #24114: Triton/Nvidia: Fix fused fp8 <-> fp8 conversions,"PR CC(Fix version number in Bazel downgrade warning): Triton/Nvidia: Fix fused fp8  fp8 conversions Imported from GitHub PR https://github.com/openxla/xla/pull/24114 Converting FP8  FP8 fails because the Triton compiler does not support it. The proposed fix will make the conversion go through FP16. Two questions: 1) Are there any better approaches of solving this? 2) I could not find a place to put unit tests for this, and in the code there is a comment saying:     ```         // TODO(b/266862493): Add endtoend test once FP8 support lands in XLA as         // we can't test the code below without patching the feature.     ```     Wondering if there is a place where I can add a test?  Details When converting FP8 types, the XLA compiler emits a `fp_to_fp` Triton instruction. If the source type is FP8, no rounding strategy is specified. Concretely, this causes the following Triton to be emitted:   %24 = tt.fp_to_fp %20 : tensor > tensor  ``` module {   tt.func (%arg0: !tt.ptr {tt.divisibility = 16 : i32}, %arg1: !tt.ptr {tt.divisibility = 16 : i32}, %arg2: !tt.ptr {tt.divisibility = 16 : i32}) {     %cst = arith.constant dense : tensor     %cst_0 = arith.constant dense : tensor     %c90_i32 = arith.constant 90 : i32     %c32000_i64 = arith.constant 32000 : i64     %c64_i32 = arith.constant 64 : i32     %c90_i64 = arith.constant 90 : i64     %c768_i64 = arith.constant 768 : i64     %c0_i32 = arith.constant 0 : i32     %c1_i64 = arith.constant 1 : i64     %c32_i32 = arith.constant 32 : i32     %c24_i32 = arith.constant 24 : i32     %c8_i32 = arith.constant 8 : i32     %c4000_i32 = arith.constant 4000 : i32     %cst_1 = arith.constant dense : tensor     %0 = tt.get_program_id x : i32     %1 = arith.divsi %0, %c4000_i32 : i32     %2 = arith.muli %1, %c8_i32 : i32     %3 = arith.subi %c24_i32, %2 : i32     %4 = arith.cmpi slt, %3, %c8_i32 : i32     %5 = arith.select %4, %3, %c8_i32 : i32     %6 = arith.remsi %0, %5 : i32     %7 = arith.addi %2, %6 : i32     %8 = arith.remsi %0, %c4000_i32 : i32     %9 = arith.divsi %8, %5 : i32     %10 = arith.muli %7, %c32_i32 : i32     %11 = tt.make_tensor_ptr %arg1, [%c768_i64, %c90_i64], [%c1_i64, %c768_i64], [%c0_i32, %c0_i32] {order = array} : >     %12 = tt.advance %11, [%10, %c0_i32] : >     %13 = arith.muli %9, %c64_i32 : i32     %14 = tt.make_tensor_ptr %arg0, [%c90_i64, %c32000_i64], [%c1_i64, %c90_i64], [%c0_i32, %c0_i32] {order = array} : >     %15 = tt.advance %14, [%c0_i32, %13] : >     %16:3 = scf.for %arg3 = %c0_i32 to %c90_i32 step %c64_i32 iter_args(%arg4 = %12, %arg5 = %15, %arg6 = %cst_1) > (!tt.ptr>, !tt.ptr>, tensor)  : i32 {       %20 = tt.load %arg4 {boundaryCheck = array, padding = 1 : i32} : !tt.ptr>       %21 = tt.advance %arg4, [%c0_i32, %c64_i32] : >       %22 = tt.load %arg5 {boundaryCheck = array, padding = 1 : i32} : !tt.ptr>       %23 = tt.advance %arg5, [%c64_i32, %c0_i32] : >       %24 = tt.fp_to_fp %20 : tensor > tensor       %25 = arith.subi %c90_i32, %arg3 : i32       %26 = arith.cmpi slt, %25, %c64_i32 : i32       %27 = scf.if %26 > (tensor) {         %30 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor         %31 = tt.expand_dims %30 {axis = 0 : i32} : tensor > tensor         %32 = tt.splat %25 : i32 > tensor         %33 = arith.cmpi slt, %31, %32 : tensor         %34 = tt.broadcast %33 : tensor > tensor         %35 = arith.select %34, %24, %cst_0 : tensor, tensor         scf.yield %35 : tensor       } else {         scf.yield %24 : tensor       }       %28 = scf.if %26 > (tensor) {         %30 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor         %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor > tensor         %32 = tt.splat %25 : i32 > tensor         %33 = arith.cmpi slt, %31, %32 : tensor         %34 = tt.broadcast %33 : tensor > tensor         %35 = arith.select %34, %22, %cst : tensor, tensor         scf.yield %35 : tensor       } else {         scf.yield %22 : tensor       }       %29 = tt.dot %27, %28, %arg6, inputPrecision = tf32 {maxNumImpreciseAcc = 2147483647 : i32} : tensor * tensor > tensor       scf.yield %21, %23, %29 : !tt.ptr>, !tt.ptr>, tensor     }     %17 = tt.fp_to_fp %16 CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4""), rounding = rtne : tensor > tensor     %18 = tt.make_tensor_ptr %arg2, [%c768_i64, %c32000_i64], [%c1_i64, %c768_i64], [%c0_i32, %c0_i32] {order = array} : >     %19 = tt.advance %18, [%10, %13] : >     tt.store %19, %17 : !tt.ptr>     tt.return   } } ```  Which leads to a failing assertion: ``` CC(未找到相关数据)  0x000073413786d9fc in pthread_kill () from /lib/x86_64linuxgnu/libc.so.6 CC(Add support for Python 3.x)  0x0000734137819476 in raise () from /lib/x86_64linuxgnu/libc.so.6 CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"")  0x00007341377ff7f3 in abort () from /lib/x86_64linuxgnu/libc.so.6 CC(JVM, .NET Language Support)  0x00007341377ff71b in ?? () from /lib/x86_64linuxgnu/libc.so.6 CC(Installation over pip fails to import with protobuf 2.6.1)  0x0000734137810e96 in __assert_fail () from /lib/x86_64linuxgnu/libc.so.6 CC(Java interface)  0x000057d936b1777b in mlir::triton::gpu::(anonymous namespace)::FpToFpOpConversion::createDestOps (this=0x733d08425cc0, op=..., adaptor=..., rewriter=..., elemTy=..., operands=..., loc=...)     at external/triton/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ElementwiseOpToLLVM.cpp:500 CC(Pretrained models)  0x000057d936b17195 in mlir::triton::gpu::ElementwiseOpConversionBase::matchAndRewrite (this=0x733d08425cc0, op=..., adaptor=..., rewriter=...)     at external/triton/include/triton/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVMBase.h:188 [...] CC(minimum req: Cuda compute capability 3.5) 0x000057d93fa6cade in mlir::PassManager::run (this=0x733e80fba158, op=0x733d080bbc20) at external/llvmproject/mlir/lib/Pass/Pass.cpp:885 CC(Go API) 0x000057d9363f6b1b in xla::gpu::CompileTritonToLLVM (hlo_config=..., hlo_module_name=""gemm_fusion_dot.320"", device_info=..., block_level_parameters=..., triton_module=..., llvm_module=0x733d0816d6a0, mlir_context=..., is_xla_fusion=true, emit_kernel=true)     at xla/backends/gpu/codegen/triton/fusion_emitter.cc:1627 CC(Slack Channel) 0x000057d9363f5a5d in xla::gpu::TritonWrapper (fn_name=""gemm_fusion_dot_320_impl"", fusion=0x733d080a31c0, cc=std::variant [index 0] = {...}, device_info=..., block_level_parameters=...,     llvm_module=0x733d0816d6a0, mlir_context=...) at xla/backends/gpu/codegen/triton/fusion_emitter.cc:1531 ``` However, this fails Triton compilation: * First it hits an assertion that the rounding strategy when the destination type is FP8 must be specified * Adding the rounding strategy, then goes on to another issue, that no methods for converting FP8  FP8 are specified To work around the above two issues, I propose going through FP16 when both the source and destination types are FP8's. Copybara import of the project:  afd3929099fc4d1045275ca3210e0bc727a2b906 by Kasper Nielsen : Fix fused fp8  fp8 conversions  66340aa808f58e5dc6ab1c2e06790ceccde95540 by Kasper Nielsen : Add unit tests and refactor duplicated code  07ae307879eff24ad2f85607e94503deda1074e4 by Kasper Nielsen : Run clangformat  fe967ff94ffc5f34f07bff142b5d10d81d5e4dce by Kasper Nielsen : Fix support conversion tests Merging this change closes CC(Fix version number in Bazel downgrade warning) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24114 from kasper0406:kn/fp8conversionfix fe967ff94ffc5f34f07bff142b5d10d81d5e4dce",2025-03-27T15:34:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90124
bert,copybara-service[bot],Replace uses of deprecated `Shape::rank()` with:,"Replace uses of deprecated `Shape::rank()` with:  `dimensions().size()` if it's OK for the result to be changed to an unsigned number,  `dimensions_size()` if it's important that the result is a signed number. This should be a pure refactoring that doesn't affect the code's behavior. Note that `rank()` returns `int64_t` and `dimensions().size()` returns `size_t`. Sometimes the change of the signedness is not desirable, and we use `dimensions_size()`, which returns `int`, in such cases. Reverts 56d1b2306f948c83b2b243484a6c16c037696a85 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23790 from chaserileyroberts:chase/nccl_group 36ad18d8c604a6cb2559dcca8c29530beb5da888",2025-03-27T14:51:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90122
yi,mplatings,[TOSA] Fix legalizing CONV bias,The logic to handle bias already existed in varying degrees of completeness for the various CONV operations. This change unifies that logic in a single function and reuses it for operations from which it was missing. The TOSA 1.0 specification permits that bias may be of shape [1]. This change takes advantage of that fact to simplify the logic. The change in behaviour is reflected in the tests. ChangeId: I97371fae425ca8e45b18705c71bd106d08f203d6,2025-03-27T13:53:23Z,kokoro:force-run ready to pull size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90118
yi,copybara-service[bot],Update `googletest` used by XLA and Tensorflow from the 2022/6/30 version to the 2025/3/21 version. This:,"Update `googletest` used by XLA and Tensorflow from the 2022/6/30 version to the 2025/3/21 version. This:  makes the OSS build of OpenXLA catch up with the internal build in their use of gtest versions.  unlocks a bunch of planned improvements (enforcing that a test program has at least one test case, simplifying copybara setup, etc).",2025-03-26T23:24:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90082
yi,copybara-service[bot],Update OpenXLA's `googletest` from the 2022/6/30 version to the 2025/3/21 version. This:,"Update OpenXLA's `googletest` from the 2022/6/30 version to the 2025/3/21 version. This:  makes the internal build and the OSS build of OpenXLA use the same googletest version and thus be consistent.  unlocks a bunch of planned improvements (enforcing that a test program has at least one test case, simplifying copybara setup, etc).",2025-03-26T17:29:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90069
gemma,copybara-service[bot],[XLA] Reorder GEMMA2 benchmark from nightly build script due to temp failure,[XLA] Reorder GEMMA2 benchmark from nightly build script due to temp failure This has been causing build errors for a while.,2025-03-26T16:49:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90067
opt,copybara-service[bot],[XLA:GPU] Clean up `DebugOptions` and turn off autotuning in `dot_algorithms_test`.,[XLA:GPU] Clean up `DebugOptions` and turn off autotuning in `dot_algorithms_test`. This makes the test run >10x faster without loss of useful coverage.,2025-03-26T15:57:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90062
opt,copybara-service[bot],PR #24011: HLO op profiles for B200.,PR CC([INTEL MKL]Optimize CropAndResizeGradImage Op.): HLO op profiles for B200. Imported from GitHub PR https://github.com/openxla/xla/pull/24011 Copybara import of the project:  e270aa00005171d75864d52445d77c3364373954 by Dimitris Vardoulakis : HLO op profiles for B200. Merging this change closes CC([INTEL MKL]Optimize CropAndResizeGradImage Op.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24011 from dimvar:b200hloopprofiles e270aa00005171d75864d52445d77c3364373954,2025-03-26T10:30:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90050
opt,copybara-service[bot],[XLA:GPU] Dump hlo config only once before optimization for a GPU tasks.,[XLA:GPU] Dump hlo config only once before optimization for a GPU tasks.,2025-03-26T08:31:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90036
attention,copybara-service[bot],PR #24203: [XLA:GPU] fix unintended cuDNN flash attention dbias computation,PR CC(Markdown formatting issue in tf.keras.layers.Layer docs): [XLA:GPU] fix unintended cuDNN flash attention dbias computation Imported from GitHub PR https://github.com/openxla/xla/pull/24203 Fix a case where cudnn flash attention dbias is not required but computed. Only calculate dbias when there is dbias descriptor. Copybara import of the project:  14f15487cc33be2d9a80bd7f9a5d95d85346fc60 by cjkkkk : fix unintended dbias computation Merging this change closes CC(Markdown formatting issue in tf.keras.layers.Layer docs) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24203 from Cjkkkk:dbias_fix 14f15487cc33be2d9a80bd7f9a5d95d85346fc60,2025-03-26T06:32:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90032
tpu,copybara-service[bot],[XLA:GPU] Fix bug in triton emitter related to multi-output fusion.,[XLA:GPU] Fix bug in triton emitter related to multioutput fusion. We should not return early after emitting a scalar store.,2025-03-26T06:09:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90028
opt,njzjz,Build against TF 2.19: llvm/ADT/ArrayRef.h: No such file or directory," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.19.0  Custom code Yes  OS platform and distribution Linux  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ```   In file included from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/stream_executor/stream_executor.h:40,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/service/stream_pool.h:23,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/service/service_executable_run_options.h:25,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/service/executable.h:40,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/service/compiler.h:40,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/service/backend.h:34,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/service/allocation_tracker.h:30,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/service/service.h:33,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/client/client.h:32,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/tensorflow/compiler/tf2xla/xla_expression.h:21,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/tensorflow/compiler/tf2xla/xla_compiler.h:28,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/tensorflow/compiler/tf2xla/xla_op_kernel.h:23,                    from /tmp/pipinstallx2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/horovod/tensorflow/xla_mpi_ops.cc:25:   /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/stream_executor/gpu/tma_metadata.h:25:10: fatal error: llvm/ADT/ArrayRef.h: No such file or directory      25           ^~~~~~~~~~~~~~~~~~~~~   compilation terminated.   gmake[2]: *** [horovod/tensorflow/CMakeFiles/tensorflow.dir/build.make:513: horovod/tensorflow/CMakeFiles/tensorflow.dir/xla_mpi_ops.cc.o] Error 1   gmake[2]: *** Waiting for unfinished jobs.... ```",2025-03-26T04:38:07Z,type:bug TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/90026,`aptget update && aptget install y noinstallrecommends llvmdev` should solve this issue. 
tpu,Rolexo,Tensorflow_installation_issue," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.19.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? i expected the dataset to be loaded  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[7], line 1 > 1 import tensorflow as tf       2 print(tf.__version__) File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\User\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.  Standalone code to reproduce the issue ```shell  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[7], line 1 > 1 import tensorflow as tf       2 print(tf.__version__) File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\User\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. ```  Relevant log output ```shell  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[7], line 1 > 1 import tensorflow as tf       2 print(tf.__version__) File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\User\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. ```",2025-03-26T04:08:13Z,stat:awaiting response type:build/install TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/90025,"Hi  , could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Did you reinstall packages? Check if you are in the right env?,Given the currently provided information this is a duplicate. Please search for duplicates or provide information that differentiates from them.,Are you satisfied with the resolution of your issue? Yes No
tpu,jinxsfe,can not import tensorflow as tf when set tensorflow =2.15," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15  Custom code Yes  OS platform and distribution Colab notebook  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory A100  Current behavior? I can not import tensorflow as tf when set tensorflow ==2.15(I must set it in order to match environment), which correspond that I can not use colab GPU, I try to fill those issue according support for colab cummunity but failed, can you give me some ideas, issue link is https://github.com/googlecolab/colabtools/issues/5212, even set as order, also introduce the future error, if I keep newest tensorflow version, which aslo let model failed in training, detail link is   Standalone code to reproduce the issue ```shell .. ```  Relevant log output ```shell .. ```",2025-03-26T02:18:06Z,stat:awaiting response type:bug stale TF 2.15,closed,0,8,https://github.com/tensorflow/tensorflow/issues/90014,"if I run force run tensorflow 2.15 !Image, it only use cpu not gpu",Colab runtime is configured with just one GPU version. If you install a version of TF that is using a different GPU then you would only be able to use CPU.,"I changed runtime before I install it, restart session and delete runtime records, show an error, if I install tensorflow and inport","You need to initialise and check for your GPU, it might default to CPU if you do not have the right environment path and setup. ``import tensorflow as tf print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))`` ````gpus = tf.config.list_physical_devices('GPU') if gpus:    Restrict TensorFlow to only use the first GPU   try:     tf.config.set_visible_devices(gpus[0], 'GPU')     logical_gpus = tf.config.list_logical_devices('GPU')     print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPU"")   except RuntimeError as e:      Visible devices must be set before GPUs have been initialized     print(e)``", thanks you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],[XLA:GPU] Deprecate PIPELINE_PARALLELISM_OPT_LEVEL_ENABLE_CYCLE_DECOMPOSER,[XLA:GPU] Deprecate PIPELINE_PARALLELISM_OPT_LEVEL_ENABLE_CYCLE_DECOMPOSER,2025-03-26T01:20:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90010
tpu,copybara-service[bot],PR #24060: Exclude Circular Dependencies in FP8 Graph Convolutions,PR CC(Cpu tensorflow ImportError ): Exclude Circular Dependencies in FP8 Graph Convolutions Imported from GitHub PR https://github.com/openxla/xla/pull/24060 Excludes ops with external operands that can be reached from graph outputs of fused ops as well as ops with graph outputs that can reach external operands of fused ops from fusion into graphbased FP8 convolutions. Fusing these ops may lead to circular dependencies between fused and unfused instructions. Copybara import of the project:  5ebb2be3175c310b8e2212c1401d660eb5a36625 by Philipp Hack : Excludes ops that may cause circular dependencies from fusion into graphbased FP8 convolutions.  f247a7122b7b9c842142650789905490d388ef4e by Philipp Hack : Excludes ops that may cause circular dependencies from fusion into graphbased FP8 convolutions. Merging this change closes CC(Cpu tensorflow ImportError ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24060 from philipphack:u_conv_circular_xla f247a7122b7b9c842142650789905490d388ef4e,2025-03-26T01:16:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90009
opt,copybara-service[bot],Add a proto matcher library for XLA.,"Add a proto matcher library for XLA. This library defines `EqualsProto(expected)` for matching a proto by equality. It also defines two matcher transformers `Partially()` and `IgnoringRepeatedFieldOrdering()` for making the matcher ignore fields not set in the expected proto and ignore order of elements in repeated fields, respectively. Also use the library in tensorflow/compiler/xla/client/executable_build_options_test..",2025-03-25T23:55:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90006
opt,copybara-service[bot],Support assigning a multiple values encoded as a comma separated string into a repeated field in DebugOptions:,"Support assigning a multiple values encoded as a comma separated string into a repeated field in DebugOptions:   each occurrence of field, overwrites all previous values in the field,   empty string clears all values from the field,   splitting comma separated string skips empty values.",2025-03-25T20:12:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89989
opt,copybara-service[bot],Add metadata options to `Dataset::Params`.,Add metadata options to `Dataset::Params`.,2025-03-25T19:50:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89985
tpu,copybara-service[bot],"Content in result.stderr field doesn't always mean error. For example, function ""execute"" fails when we use clang with CCC_OVERRIDE_OPTIONS=""^--gcc-install-dir=/usr/lib/gcc/x86_64-linux-gnu/13"" variable because clang outputs gcc path into stderr even during successful command execution.","Content in result.stderr field doesn't always mean error. For example, function ""execute"" fails when we use clang with CCC_OVERRIDE_OPTIONS=""^gccinstalldir=/usr/lib/gcc/x86_64linuxgnu/13"" variable because clang outputs gcc path into stderr even during successful command execution.",2025-03-25T19:16:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89981
gemma,copybara-service[bot],Run gemma3_1b_flax_call.hlo and gemma3_1b_flax_sample_loop.hlo in CPU/GPU nightly workflows,Run gemma3_1b_flax_call.hlo and gemma3_1b_flax_sample_loop.hlo in CPU/GPU nightly workflows,2025-03-25T17:38:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89967
tpu,copybara-service[bot],Hlo-Diff is a semantic diff tool for HLO modules.,"HloDiff is a semantic diff tool for HLO modules. It compares the graph structure of two HLO Modules focusing on the computational differences ignoring irrelevant changes such as instruction names, parameter ordering, layouts (in some instances) etc.  The tool supports: 1. Diffing of large HLO Modules (>100k) nodes in <1 minute. 2. Summarized output of diffs highlighting what has changed for updated HLO instructions. Note only XLA's HLO format is supported at the moment.",2025-03-24T23:47:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89915
tpu,raghureddy-sripathi,from deepface import DeepFace," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: unable import DeepFace from deepface how to get rid of this error?  Standalone code to reproduce the issue ```shell  ImportError                               Traceback (most recent call last) File c:\Users\ADMIN\AppData\Local\Programs\Python\Python311\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[2], line 2       1 import cv2 > 2 from deepface import DeepFace File c:\Users\ADMIN\AppData\Local\Programs\Python\Python311\Lib\sitepackages\deepface\DeepFace.py:15      13 import numpy as np      14 import pandas as pd > 15 import tensorflow as tf      17  package dependencies      18 from deepface.commons import package_utils, folder_utils File c:\Users\ADMIN\AppData\Local\Programs\Python\Python311\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File c:\Users\ADMIN\AppData\Local\Programs\Python\Python311\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.')      97  pylint: enable=wildcardimport,gimportnotattop,unusedimport,linetoolong ImportError: Traceback (most recent call last):   File ""c:\Users\ADMIN\AppData\Local\Programs\Python\Python311\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```  Relevant log output ```shell ```",2025-03-24T20:40:57Z,type:bug TF 2.18,closed,0,9,https://github.com/tensorflow/tensorflow/issues/89906,"Hi sripathi , could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Are you satisfied with the resolution of your issue? Yes No,"> Hi sripathi , could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: >  > You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow.  CC(Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.) Also this is a duplicate of  CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) Thank you! HI   Thank you for response. Actually the thing is that i'm using python 3.11.5 and want to use deepface library but it doesn't work with latest version of python so i'm getting this error every time i'm trying to run. so i tried to download 3.9 version of python it works right now. maybe latest version doesn't have all modules.","If 3.9 works but 3.11 doesn't, then that's a different issue than CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.), sorry for that. Can you please post some samples to show how a simple Python script (could be as simple as `import tensorflow as tf`) fails to execute with 3.11 but fails with 3.9? For both versions, also please post output of `pip list`. And, just to confirm, are you building from source (given ""source"" answer in the form) or installing TF via `pip install`? Can you also list how you are installing TF/packages you use, if it's not `pip install`?","> Deepface is not TF, likely the issue should be opened there. >  > But, for the TF bits, this is a duplicate. Always search for duplicates. Hi  yeah I know but i tried to run this it doesn't work with latest version of python. I thought if i update tensorflow version it might change but I already have latest version. So, instead of  using python 3.11.5 i tried to solve in python 3.9. now the issue is sorted.","Yeah, if the issue is between python 3.9 and python 3.11 we need to sort this soon, since this is the last version of TF that would support 3.9, I think",Are you satisfied with the resolution of your issue? Yes No,"> Yeah, if the issue is between python 3.9 and python 3.11 we need to sort this soon, since this is the last version of TF that would support 3.9, I think I have no idea that latest version of python does'nt support full extent of  tensorflow. is that true?",We cannot support all versions of Python with all versions of TF. So each release of TF one version of Python might be dropped and another added in.
tpu,kossyrev-bg,Keras Hub Model Conversion Fails," 1. System information  Ubuntu 18.04, python 3.12.3  tf installed via `uv pip`   `tensorflow==2.18.1, keras==3.9.0`  2. Code ```  %% import os os.environ['KERAS_BACKEND'] = 'tensorflow'   Set the models from keras_hub to use tensorflow  %% import tensorflow as tf import tensorflow.keras as keras import keras_hub from PIL import Image from IPython.display import display import numpy as np  %%  %%  Input variables here  See https://keras.io/keras_hub/presets/ MODEL_NAME = ""efficientnet_b0_ra4_e3600_r224_imagenet""  Path to training data TRAINING_DATA_PATH = ...  How many classes we want to classify NUM_CLASSES = 2  Trainingrelated tuning  How many GPUs to use, assumes they are named GPU:0, GPU:1, etc. NUM_GPUS = 4 BATCH_SIZE = 32   Per GPU NUM_EPOCHS = 10 NUM_AUGS = 2   Number of random augmentations to apply to each image AUG_STRENGTH = 0.5   How strong the augmentations should be  Whether to quantize pertensor or perchannel PER_TENSOR_TFLITE = False  IMAGE_SIZE = 224   GLOBAL_BATCH = NUM_GPUS * BATCH_SIZE  %%  Create a MirroredStrategy. gpus = [f""GPU:{i}"" for i in range(NUM_GPUS)] strategy = tf.distribute.MirroredStrategy(gpus) print(""Number of devices: {}"".format(strategy.num_replicas_in_sync))  Open a strategy scope. with strategy.scope():      Everything that creates variables should be under the strategy scope.      In general this is only model construction & `compile()`.     image_classifier = keras_hub.models.ImageClassifier.from_preset(         MODEL_NAME,         activation=None,   outputs logits         num_classes=NUM_CLASSES     )     image_classifier.compile(         optimizer=keras.optimizers.Adam(             learning_rate=1e4,              clipnorm=1.0         ),         loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),   Consumes logits         metrics=[keras.metrics.SparseCategoricalAccuracy()]     )  %%  Create dataset train_dataset, val_dataset = keras.preprocessing.image_dataset_from_directory(     TRAINING_DATA_PATH,     labels=""inferred"",     label_mode=""int"",     batch_size=None,     image_size=(IMAGE_SIZE, IMAGE_SIZE),     shuffle=True,     validation_split=0.2,     subset=""both"",     interpolation=""bilinear"",     verbose=True,     seed=42 )  Define simple augmenter aug_layer = keras.layers.RandAugment(value_range=(0, 255), num_ops=NUM_AUGS, factor=AUG_STRENGTH) map_fn = lambda x, y: (aug_layer(x, training=True), y)  Prefetch the data, adding augmentation to the training dataset train_dataset = train_dataset.batch(GLOBAL_BATCH, drop_remainder=True).map(map_fn, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE) val_dataset = val_dataset.batch(GLOBAL_BATCH, drop_remainder=True).prefetch(tf.data.AUTOTUNE)  %%  %%  Freeze the base model to only retrain the head (optional)  image_classifier.backbone.trainable = False  Train the model on all available devices. image_classifier.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=val_dataset)  Test the model on all available devices.  TODO: Need a test dataset  image_classifier.evaluate(test_dataset) image_classifier.save(f""{MODEL_NAME}.keras"")  %%  Test the trained model on 10 validation images  This works fine, getting at least 8 correct for imgs, label in val_dataset.take(1):     for i in range(10):         img = imgs[i:i+1]         preds = image_classifier.predict(img)         print(f""Raw preds: {preds}, Predicted: {preds.argmax()}, Actual: {label[i]}"")  %%  Remove preprocessor model_preprocessor = image_classifier.preprocessor image_classifier.preprocessor = None  %%  Create endtoend model that takes in UINT8 image of size 224   Modify preprocessor, because GPU doesn't like bicubic? idk cfg = model_preprocessor.get_config() cfg[""image_converter""][""config""][""interpolation""] = ""bilinear"" new_preproc = keras_hub.models.ImageClassifierPreprocessor.from_config(cfg)  Input layer is (1, 224, 224, 3) uint8 input_layer = keras.layers.Input(shape=(224, 224, 3), dtype=tf.uint8, batch_size=1)  Convert to float 32 like the preprocessor expects (I'd like to skip this if possible,  because it might be part of why the tflite conversion is failing,  but I don't know how without dismantling the preprocessor) float_input = keras.layers.Lambda(lambda x: tf.cast(x, tf.float32))(input_layer)  Preprocess the image preprocessed_input = new_preproc(float_input, training=False)  Run through the model  no doublepreprocessing since we removed it logits = image_classifier(preprocessed_input, training=False)  Combine into one model full_model = keras.models.Model(inputs=float_input, outputs=logits)  Compile the model (unclear if necessary) full_model.compile(     optimizer=keras.optimizers.Adam(1e4),     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),   Consumes logits     metrics=[keras.metrics.SparseCategoricalAccuracy()] ) full_model.trainable = False  %%  Test the new model on 10 validation images. This works too, roughly same  accuracy as above num_correct = 0 num_wrong = 0 for imgs, label in val_dataset.take(1):     for i in range(GLOBAL_BATCH):         img = imgs[i:i+1]          preds = full_model.predict(tf.cast(img, tf.uint8))         preds = full_model.predict(img)         print(f""Raw preds: {preds}, Predicted: {preds.argmax()}, Actual: {label[i]}"")         if preds.argmax() == label[i]:             num_correct += 1         else:             num_wrong += 1 print(f""Correct: {num_correct}, Wrong: {num_wrong}"")  %%  Use data from the validation dataset to calibrate the quantized model def representative_data_gen():     batch_count = (300 // GLOBAL_BATCH) + 1     run = 0     for x, _ in val_dataset.take(batch_count):   Use a few samples         run += 1         print(f""Running batch {run}/{batch_count}"")          Pull one image at a time         for i in range(GLOBAL_BATCH):             img = x[i:i+1]             yield [tf.cast(img, tf.uint8)]             yield [img]  Convert to tflite, using pertensor quantization for ARTPEC8 converter = tf.lite.TFLiteConverter.from_keras_model(full_model) converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.representative_dataset = representative_data_gen converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] converter.inference_input_type = tf.uint8 converter.inference_output_type = tf.float32 if PER_TENSOR_TFLITE:     converter._experimental_disable_per_channel = True tflite_quant_model = converter.convert() with open(f""{MODEL_NAME}.tflite"", ""wb"") as f:     f.write(tflite_quant_model)  %%  Run the TFLite model on some test data interpreter = tf.lite.Interpreter(model_content=tflite_quant_model) input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() input_index = input_details[0][""index""] output_index = output_details[0][""index""] interpreter.allocate_tensors()  %% num_correct = 0 num_wrong = 0 for x, label in val_dataset.take(3):     for i in range(GLOBAL_BATCH):         img = x[i:i+1]          Need to cast to uint8 because tflite model expects it         interpreter.set_tensor(input_index, tf.cast(img, tf.uint8))         interpreter.invoke()         output_data = interpreter.get_tensor(output_index)         print(f""Raw preds: {output_data}, Predicted: {output_data.argmax()}, Actual: {label[i]}"")         if output_data.argmax() == label[i]:             num_correct += 1         else:             num_wrong += 1 print(f""{num_correct=}, {num_wrong=}"") ```  3. Failure after conversion Model accuracy drastically drops after conversion: ``` Raw preds: [[0.08086799  0.20216998]], Predicted: 1, Actual: 0 Raw preds: [[ 0.       0.040434]], Predicted: 0, Actual: 1 Raw preds: [[1.0512838   0.48520795]], Predicted: 1, Actual: 0 Raw preds: [[1.0108498   0.36390597]], Predicted: 1, Actual: 1 Raw preds: [[0.9704159   0.12130199]], Predicted: 1, Actual: 0 Raw preds: [[0.72781193  0.24260397]], Predicted: 1, Actual: 1 Raw preds: [[0.08086799 0.24260397]], Predicted: 0, Actual: 0 Raw preds: [[ 0.16173598 0.040434  ]], Predicted: 0, Actual: 0 Raw preds: [[1.1321518  0.040434 ]], Predicted: 1, Actual: 0 ... Raw preds: [[0.24260397  0.12130199]], Predicted: 1, Actual: 1 Raw preds: [[0.76824594 0.08086799]], Predicted: 1, Actual: 0 Raw preds: [[0.36390597  0.24260397]], Predicted: 1, Actual: 1 num_correct=196, num_wrong=188 ```",2025-03-24T20:37:10Z,comp:lite TFLiteConverter TF 2.18,closed,0,18,https://github.com/tensorflow/tensorflow/issues/89904,This belongs in keras repos.,"I found the problem with the model setup. However, after the tflite conversion, the model accuracy is still drastically lower than it was before. I can update the code snippet to show the new setup, but it's basically just combining preprocessing with the model into one allinone model. I would still like help with why tflite conversion is reducing accuracy by so much. ","Oh, I misunderstood (from the title), this is not about Keras but model conversion to TFLite. Can you minimize this to a model as small as possible?","This is an efficientnet model from the keras hub: https://www.kaggle.com/models/keras/efficientnet/keras/efficientnet_b0_ra4_e3600_r224_imagenet/ It's about 5MB after quantization to tflite. Is that too big? Do you need the keras model and the tflite model, and do you need the data?",The smaller the model the easier is to debug,"Hi, bg  I apologize for the delay in response, I am able to replicate the same behavior from my end with sample flowers dataset with your provided code snippet here is gistfile for reference so we will have to dig more into this issue and will update you, thank you for bringing this issue to our attention. Thank you for your cooperation. ``` Original model predictions: 1/1 ━━━━━━━━━━━━━━━━━━━━ 3s 3s/step Raw preds: [[2.7760098  3.1728508]], Predicted: 1, Actual: 1 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 326ms/step Raw preds: [[0.55148685  0.8257771 ]], Predicted: 1, Actual: 1 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 280ms/step Raw preds: [[ 3.8654664 4.097068 ]], Predicted: 0, Actual: 0 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 299ms/step Raw preds: [[ 15.905437 15.347878]], Predicted: 0, Actual: 0 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 313ms/step Raw preds: [[0.43868783  1.6053506 ]], Predicted: 1, Actual: 1 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 247ms/step Raw preds: [[ 13.696747 12.681302]], Predicted: 0, Actual: 0 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 199ms/step Raw preds: [[1.2708153   0.60328835]], Predicted: 1, Actual: 1 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 188ms/step Raw preds: [[0.7198984  0.65562236]], Predicted: 0, Actual: 1 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 207ms/step Raw preds: [[1.3494838  2.3646579]], Predicted: 1, Actual: 1 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 192ms/step Raw preds: [[1.334792   1.3840239]], Predicted: 1, Actual: 1 ``` ``` TFLite model predictions: TFLite output: [[2.4290873e+13  2.2772695e+13]], Predicted class: 1 TFLite output: [[6.0727183e+12  3.0363592e+12]], Predicted class: 1 TFLite output: [[0. 0.]], Predicted class: 0 TFLite output: [[2.1254514e+13  1.9736335e+13]], Predicted class: 1 TFLite output: [[4.2509028e+13  2.1254514e+13]], Predicted class: 1 TFLite output: [[2.2772695e+13  2.1254514e+13]], Predicted class: 1 TFLite output: [[7.5908980e+12  1.2145437e+13]], Predicted class: 1 TFLite output: [[3.1881771e+13  2.1254514e+13]], Predicted class: 1 TFLite output: [[2.4290873e+13  2.1254514e+13]], Predicted class: 1 TFLite output: [[0. 0.]], Predicted class: 0 TFLite output: [[2.4290873e+13 3.4918130e+13]], Predicted class: 1 TFLite output: [[1.3663616e+13  1.5181796e+12]], Predicted class: 1 TFLite output: [[1.5181796e+13  2.2772695e+13]], Predicted class: 1 TFLite output: [[0. 0.]], Predicted class: 0 TFLite output: [[3.4918130e+13  2.7327233e+13]], Predicted class: 1 TFLite output: [[1.0627257e+13  2.1254514e+13]], Predicted class: 1 ```","Ah I'm sorry, let me update the code in the example. I'm no longer running into quite the same scaling error issue. But the outputs are still wrong. I'll post again once it's updated","Okay  the code has been updated, with new example logits, thank you for your help!","I'll go ahead and update the gist file as well, if I can","Hi bg To facilitate further investigation and debugging of the current issue, we would greatly appreciate it if you could consider updating the existing gist file or creating a new Google Colab notebook including a sample flowers dataset within this resource would be particularly helpful for us to reproduce the reported behavior and analyze the underlying cause effectively. Thank you for your cooperation.","Hi, bg  I see you're using `int8`quantization, TensorFlow Lite offers other posttraining quantization options like `float16` quantization. This offers a smaller model size reduction and speedup compared to `int8` but often preserves accuracy much better so could you please give it try with `float16` quantization something like below and see are you getting better predictions as compared to `int8` quantization ?  ``` converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)     converter.optimizations = [tf.lite.Optimize.DEFAULT]     converter.target_spec.supported_types = [tf.float16]      tflite_model = converter.convert()     with open(save_path_tflite, ""wb"") as f:         f.write(tflite_model) ``` Thank you for your cooperation.","Okay, I gave that a shot here. You're totally right, the accuracy loss is much much less with float16.  Unfortunately, the edge system I'm building for requires int8 quantization. Is there anything I can do to make the int8 conversion better?","Oh, quantization is expected to lose precision, you have fewer bits to represent the numbers from the weights and inputs/outputs.","I am aware, but I didn't think it would be quite this bad. Would a larger model perhaps help with this penalty?","Hi, bg Larger models with more parameters can sometimes mitigate the effects of quantization because they have more capacity to learn and generalize. However, this is not a guaranteed solution and can lead to increased computational requirements. `int8` quantization is necessary for your edge system it often requires careful tuning and adjustments to maintain accuracy. Implementing quantization aware training is likely your best bet for improving performance please refer this Quantization aware training in Keras example. Additionally finetuning the model after quantization and ensuring a robust representative dataset can also help mitigate accuracy loss. Thank you for your cooperation.","I'll try out the example and update the gist accordingly, thank you","It seems that `keras_hub` does not play nicely with the `keras` from `from tensorflow_model_optimization.python.core.keras.compat import keras` If there's no easy way around this, I guess I'll have to abandon using kerashub for my models :/",Having better luck moving over to tensorflow_hub. Closing this as no longer relevant.
tpu,CourseraPrash,Compatibility issue with Python 3.11," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.19.0  Custom code Yes  OS platform and distribution windows 10  Mobile device _No response_  Python version 3.11.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Traceback (most recent call last):   File ""C:\Users\******\AppData\Roaming\Python\Python311\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\Users\******\Documents\Python Scripts\DFRC system\myenv\Old files\Training_OptNet_multicarrier.py"", line 9, in      import mimocmcmulticarrier as m   File ""C:\Users\******\Documents\Python Scripts\DFRC system\myenv\Old files\mimocmcmulticarrier.py"", line 8, in      import tensorflow.compat.v1 as tf   File ""C:\Users\******\AppData\Roaming\Python\Python311\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport   File ""C:\Users\******\AppData\Roaming\Python\Python311\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\******\AppData\Roaming\Python\Python311\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.  Standalone code to reproduce the issue ```shell import tensorflow ```  Relevant log output ```shell ```",2025-03-24T20:24:22Z,stat:awaiting response type:build/install stale subtype:windows TF 2.18,closed,0,8,https://github.com/tensorflow/tensorflow/issues/89900,"I have created a virtual environment and worked on it for the last three years. I haven't worked on this virtual environment for about three weeks. When I returned today and ran the same code, I got the above error.","Hi  , could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!","Hi, Thank you for the feedback. I didn't install TensorFlow. I created the virtual environment three years ago and used TensorFlow for my project for three years, but it is suddenly throwing this error now. I ran the same code that I used three weeks ago without any modification, and it gave the above error.",Something is contradicting in this bug. You say you use TF 2.19 and then you have a venv created 3 years ago which you haven't updated. But TF 2.19 was released only recently. Please make sure to provide accurate information so that we can properly debug.,"Also, Python 3.11 likely did not exist 3 years ago.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,copybara-service[bot],Bumping up libtpu version to pick correct versioned nightlies,Bumping up libtpu version to pick correct versioned nightlies,2025-03-24T18:30:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89890
opt,copybara-service[bot],[xla:cpu] Add a build rule that creates an optional `no_mkl` target.,"[xla:cpu] Add a build rule that creates an optional `no_mkl` target. This is for AOT targets that want to build Eigen contraction kernel without oneDNN microkernel by default. ``` $ bazel query ""somepath(//xla/service/cpu:cpu_aot_compilation_result_no_mkl, //xla/tsl/framework/contraction:eigen_contraction_kernel_with_mkl)"" INFO: Empty results $ bazel query ""somepath(//xla/service/cpu:cpu_aot_compilation_result_no_mkl, //xla/tsl/framework/contraction:eigen_contraction_kernel_no_mkl)"" //xla/service/cpu:cpu_aot_compilation_result_no_mkl //xla/backends/cpu/runtime:thunk_proto_serdes_no_mkl //xla/backends/cpu/runtime:dot_thunk_no_mkl //xla/tsl/framework/contraction:eigen_contraction_kernel_no_mkl ```",2025-03-24T14:36:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89877
yi,copybara-service[bot],[xla] Verify buffer related HLO.,"[xla] Verify buffer related HLO. We have introduced buffer_id field in Shape for representing HLO buffer types. We now extend the verifier to recognize customcall targets pin and unpin along with the existing target allocateBuffer for buffer related operations. When buffers aren't allowed in a program, no Shape can have a valid buffer_id and customcall targets pin and unpin can't be used. This is what all the existing HLO passes would expect. When buffers are allowed, we verify that pin and unpin are used properly. We allow other customcall targets to use buffers. We also allow instructions, such as kTuple, kWhile, kParameter and kGetTupleElement to pass through buffers. All other instructions aren't allowed to use buffers. We will introduce a new HLO pass to convert buffer information to XLA attributes that copyinsertion would understand and clear the buffer_id field in Shapes so that all existing HLO passes won't need to handle buffer_ids.",2025-03-24T14:22:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89875
yi,copybara-service[bot],[XLA:GPU] Add number of local participants to GpuCliqueKey.,"[XLA:GPU] Add number of local participants to GpuCliqueKey. The number of local participants doesn't add new information to the key, because we should never get two identical cliques that only different in the number of local participants. However, putting the number inside the key makes so many places in the codebase simpler and avoids annoying recalculations. We can easily count the number of local devices when we create the key, but later it requires getting runtime param and devices ids again.",2025-03-24T14:07:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89874
opt,copybara-service[bot],"Removed optimized batch_matmul in8 int32, due to assumption that it is not needed.","Removed optimized batch_matmul in8 int32, due to assumption that it is not needed.",2025-03-24T12:25:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89868
tpu,copybara-service[bot],Adjust PriorityFusion to allow forming simple multi-output Triton fusions.,"Adjust PriorityFusion to allow forming simple multioutput Triton fusions. With simple multioutput fusion we mean fusions that have only one root without users. We only allow fusions which are supported by the current Triton codegen. We further restrict to fuse only if there is a single user to fuse with, as otherwise we would also need to detect which is the best user to fuse with, and the priority updates become more complicated. This can be done in a later step.",2025-03-24T12:22:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89867
gemma,copybara-service[bot],[xla:cpu:benchmarks] Move Gemma 2 PyTorch benchmark to the correct folder.,[xla:cpu:benchmarks] Move Gemma 2 PyTorch benchmark to the correct folder. It was added to the old path by mistake.,2025-03-24T08:59:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89858
tpu,default1360,`Aborted` in `tf.compat.v1.nn.max_pool_v2`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tf.compat.v1.nn.max_pool_v2 in tfnightly 2.20.0dev20250302. This bug can be used to trigger a denial of service attack.  Standalone code to reproduce the issue ```shell import tensorflow as tf from tensorflow.compat.v1.nn import max_pool_v2 input_tensor = tf.fill([1, 10, 4, 1], 0.5)  ksize = [1, 1250999896764, 1, 1]   strides = [1, 1250999896764, 1, 1] padding = ""SAME"" max_pool_v2(input_tensor, ksize=ksize, strides=strides, padding=padding) ```  Relevant log output ```shell Status: INVALID_ARGUMENT: Attr ksize has value 1250999896764 out of range for an int32 Aborted ```",2025-03-24T08:28:02Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/89855," This error occurs because values greater than 32bit are not allowed for ksize and strides that is been used. It would be better to handle this scenario by raising a proper error message indicating that the size exceeds the int32 limit. This can prevent the current behavior, which results in an abrupt abort. I’d be happy to work on a fix for this by adding input validation. Let me know if this approach sounds good!","Hi  ,  , thanks for your contribution! Welcome to TensorFlow, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. I did not encounter any crashes; instead, I received the following error message: ``` InvalidArgumentError: {{function_node __wrapped__MaxPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} Attr ksize has value 1250999896764 out of range for an int32 [Op:MaxPool] name:  ``` Based on this error, I made some modifications, and the code is now working fine for me. I have attached a gist for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,default1360,`Aborted` in `depthwise_conv2d_backprop_input`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tf.compat.v2.nn.depthwise_conv2d_backprop_input in tfnightly 2.20.0dev20250302. This bug can be used to trigger a denial of service attack.  Standalone code to reproduce the issue ```shell import tensorflow as tf class DummyTensor:     def __init__(self, data):         self.data = data     def __tf_function__(self, func, types, args=(), kwargs=None):         if func is tf.compat.v2.nn.depthwise_conv2d_backprop_input:             return tf.constant(1.0, dtype=tf.float32)         return NotImplemented x = DummyTensor(None) input_sizes = tf.constant([1, 4, 4, 1], dtype=tf.int32) filter_tensor = tf.constant([[1.0]], dtype=tf.float32) out_backprop = tf.constant([[1.0]], dtype=tf.float32) result = tf.compat.v2.nn.depthwise_conv2d_backprop_input(input_sizes, filter_tensor, out_backprop, strides=[1, 1], padding='VALID') ```  Relevant log output ```shell 20250324 16:25:46.795503: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C Aborted ```",2025-03-24T08:25:55Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89853,"Hi  , Welcome to TensorFlow, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. I did not encounter any crashes; instead, I received the following error message: ``` InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNativeBackpropInput_device_/job:localhost/replica:0/task:0/device:CPU:0}} Sliding window strides field must specify 4 dimensions [Op:DepthwiseConv2dNativeBackpropInput] name:  ``` Based on this error, I made some modifications, and the code is now working fine for me. I have attached a gist for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,default1360,`Aborted` in `tf.conv`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tf.conv in tfnightly 2.20.0dev20250302. This bug can be used to trigger a denial of service attack.  Standalone code to reproduce the issue ```shell import tensorflow as tf num_channels = 2 kernel_size = 3 input_shape = [1, 30, num_channels]   input_tensor = tf.random.normal(input_shape) input_tensor = tf.transpose(input_tensor, perm=[0, 2, 1])  filter_tensor = tf.random.normal([kernel_size, 1, num_channels])  conv_output = tf.nn.conv2d(input=input_tensor, filters=filter_tensor, strides=[1, 1, 1, 1], padding='VALID', data_format='NHWC', dilations=[1, 1, 1, 1], name=None) ```  Relevant log output ```shell 20250324 16:23:56.706135: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C Aborted ```",2025-03-24T08:24:04Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89852,"Hi  , Welcome to TensorFlow, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. I did not encounter any crashes; instead, I received the following error message: ``` InvalidArgumentError: {{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:CPU:0}} convolution input must be 4dimensional: [1,2,30] [Op:Conv2D] name:  ``` Based on this error, I made some modifications, and the code is now working fine for me. I have attached a gist for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,default1360,`Aborted` in `tensorflow.keras.remat`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tensorflow.keras.remat in tfnightly 2.20.0dev20250302. This bug can be used to trigger a denial of service attack.  Standalone code to reproduce the issue ```shell from tensorflow.keras import layers, Model, remat import numpy as np class CustomRematLayer(layers.Layer):     def __init__(self, **kwargs):         super().__init__(**kwargs)         self.remat_function = remat(self.intermediate_function)     def intermediate_function(self, x):         return x * 1.0      def call(self, inputs):         return self.remat_function(inputs) inputs = layers.Input(shape=(0,))  x = CustomRematLayer()(inputs)  outputs = layers.Dense(1)(x) model = Model(inputs=inputs, outputs=outputs) model.compile(optimizer=""sgd"", loss=""mse"") model.predict(np.random.randn(32, 0)) ```  Relevant log output ```shell terminate called after throwing an instance of 'dnnl::error'   what():  could not create a primitive descriptor for an inner product forward propagation primitive Aborted ```",2025-03-24T08:21:36Z,stat:awaiting response type:bug stale comp:keras TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89851,"Hi  , Please post this issue on kerasteam/keras repo. as this issue is more related to keras Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,default1360,`Aborted` in `tf.nn.max_pool2d`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tf.nn.max_pool2d in tfnightly 2.20.0dev20250302. This bug can be used to trigger a denial of service attack.  Standalone code to reproduce the issue ```shell import tensorflow as tf t = tf.random.normal((8, 1, 1, 2), dtype=tf.float32)   Shape (batch, height, width, channels) result = tf.nn.max_pool2d(t,         ksize=(9223372036854775807, 5868783964474102731),         strides=(1, 100),         padding='VALID') ```  Relevant log output ```shell 20250324 16:15:37.041691: F tensorflow/core/common_runtime/mkl_layout_pass.cc:1597] NonOKstatus: GetNodeAttr(n>def(), ""ksize"", &ksize) Status: INVALID_ARGUMENT: Attr ksize has value 9223372036854775807 out of range for an int32 Aborted ```",2025-03-24T08:17:12Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,6,https://github.com/tensorflow/tensorflow/issues/89848,"Hi  , Welcome to TensorFlow, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. I did not encounter any crashes; instead, I received the following error message: ``` InvalidArgumentError: {{function_node __wrapped__MaxPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} Attr ksize has value 9223372036854775807 out of range for an int32 [Op:MaxPool] name:  ``` Based on this error, I made some modifications, and the code is now working fine for me. I have attached a gist for your reference. Thank you!","It is weird. I found that this bug only occurs in `2.20.0dev20250302`. It doesn't appear in `2.20.0dev20250324`, `2.18`, or `2.19`. I will continue investigating. Thanks!","Hi  , Apologies for the delay. I noticed that you are using the nightly version of TensorFlow. Please note that nightly versions are experimental and frequently updated, which may lead to unexpected issues. I recommend using stable, officially released versions for more consistent and reliable results. Here is the official release documentation for your reference. Let me know if you need any further assistance! Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,default1360,`Aborted` in `tensorflow.compat.v2.raw_ops.DepthwiseConv2dNativeBackpropInput`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with `tf.raw_ops.DepthwiseConv2dNativeBackpropInput` in tfnightly 2.20.0dev20250302.  Standalone code to reproduce the issue ```shell import tensorflow as tf input_sizes = tf.constant([1, 5, 5, 1], dtype=tf.int32) filter_shape = tf.constant([3, 3, 1, 1], dtype=tf.float32)  out_backprop = tf.constant([[0.0]], dtype=tf.float32)   gradients = tf.raw_ops.DepthwiseConv2dNativeBackpropInput(     input_sizes=input_sizes,     filter=filter_shape,     out_backprop=out_backprop,     strides=[1, 1],     padding='VALID' ) ```  Relevant log output ```shell 20250324 16:02:20.356862: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C Aborted ```",2025-03-24T08:07:29Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,6,https://github.com/tensorflow/tensorflow/issues/89845,"Hi  , Welcome to TensorFlow, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. I did not encounter any crashes; instead, I received the following error message: ``` InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNativeBackpropInput_device_/job:localhost/replica:0/task:0/device:CPU:0}} Sliding window strides field must specify 4 dimensions [Op:DepthwiseConv2dNativeBackpropInput] name:  ``` Based on this error, I made some modifications, and the code is now working fine for me. I have attached a gist for your reference. Thank you!","It is weird. I found that this bug only occurs in `2.20.0dev20250302`. It doesn't appear in `2.20.0dev20250324`, `2.18`, or `2.19`. I will continue investigating. Thanks!","Hi  , Apologies for the delay. I noticed that you are using the nightly version of TensorFlow. Please note that nightly versions are experimental and frequently updated, which may lead to unexpected issues. I recommend using stable, officially released versions for more consistent and reliable results. Here is the official release documentation for your reference. Let me know if you need any further assistance! Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
chatgpt,anushikhov,Move duplicate CUDA/XLA registration logs from INFO to VLOG,"This PR downgrades harmless duplicate cuDNN, cuBLAS, and cuFFT factory registration logs from INFO to VLOG(1), fully silencing them during normal usage. Upstream already reduced these from ERROR to INFO, but they still create unnecessary log noise when XLA and GPU backends initialize. Since the duplicate registration is safe and expected this change preserves visibility only for debugging sessions. Coinspired by ChatGPT during a deep dive into TensorFlow's logging system. Fixes: CC(cuDNN, cuFFT, and cuBLAS Errors)",2025-03-23T04:27:34Z,ready to pull comp:xla size:XS,open,0,1,https://github.com/tensorflow/tensorflow/issues/89808,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
opt,copybara-service[bot],Enable absl hardening for XLA unoptimized TAPs.,Enable absl hardening for XLA unoptimized TAPs.,2025-03-22T18:46:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89803
yi,mistersmee,"Replace deprecated ""pipes.quote"" with ""shlex.quote"" for ROCm","This is basically f6d09e2, but for ROCm. Quite a minor change, but one I ran across when trying to build the ROCm bits, because the build failed as it tried to import pipes.",2025-03-22T14:48:26Z,ready to pull comp:gpu size:XS,closed,0,1,https://github.com/tensorflow/tensorflow/issues/89799,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
tpu,ymlau,colab tpu initialization," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I was unable to initialize tpu in colab, it seems that the system failed to detect the tpu.  Standalone code to reproduce the issue ```shell https://colab.research.google.com/drive/1mGQ7_EeCtFnrrtUW6mYhyFx8k06MCTz?usp=sharing ```  Relevant log output ```shell  InvalidArgumentError                      Traceback (most recent call last) /usr/local/lib/python3.11/distpackages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system_impl(cluster_resolver, tpu_cluster_resolver_cls)     138       with ops.device(tpu._tpu_system_device_name(job)):   pylint: disable=protectedaccess > 139         output = _tpu_init_fn()     140       context.async_wait() 4 frames /usr/local/lib/python3.11/distpackages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)     152       filtered_tb = _process_traceback_frames(e.__traceback__) > 153       raise e.with_traceback(filtered_tb) from None     154     finally: /usr/local/lib/python3.11/distpackages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)      58     raise core._status_to_exception(e) from None > 59   except TypeError as e:      60     keras_symbolic_tensors = [x for x in inputs if _is_keras_symbolic_tensor(x)] InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [tpu_embedding_config="""", is_global_init=false, compilation_failure_closes_chips=false, embedding_config="""", enable_whole_mesh_compilations=false, tpu_cancellation_closes_chips=2] Registered devices: [CPU] Registered kernels:    	 [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4] During handling of the above exception, another exception occurred: NotFoundError                             Traceback (most recent call last)  in ()       3 tf.config.experimental_connect_to_cluster(resolver)       4  This is the TPU initialization code that has to be at the beginning. > 5 tf.tpu.experimental.initialize_tpu_system(resolver)       6 print(""All devices: "", tf.config.list_logical_devices('TPU')) /usr/local/lib/python3.11/distpackages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py in initialize_tpu_system(cluster_resolver)      70     NotFoundError: If no TPU devices found in eager mode.      71   """""" > 72   return tpu_strategy_util.initialize_tpu_system_impl(      73       cluster_resolver, TPUClusterResolver)      74  /usr/local/lib/python3.11/distpackages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system_impl(cluster_resolver, tpu_cluster_resolver_cls)     140       context.async_wait()     141     except errors.InvalidArgumentError as e: > 142       raise errors.NotFoundError(     143           None, None,     144           ""TPUs not found in the cluster. Failed in initialization: "" NotFoundError: TPUs not found in the cluster. Failed in initialization: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [tpu_embedding_config="""", is_global_init=false, compilation_failure_closes_chips=false, embedding_config="""", enable_whole_mesh_compilations=false, tpu_cancellation_closes_chips=2] Registered devices: [CPU] Registered kernels:    	 [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4] ```",2025-03-22T13:33:30Z,type:bug comp:tpus TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/89797
tpu,tortoise-ben,fastspeech2 中文中训练," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version v2.9.0rc242g8a20d54a3c1 2.9.0  Custom code Yes  OS platform and distribution Ubuntu 22.04.5 LTS  Mobile device Ubuntu 22.04.5 LTS  Python version Python 3.7.12  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0  CUDA/cuDNN version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052021 NVIDIA Corporation Built on Thu_Nov_18_09:45:30_PST_2021 Cuda compilation tools, release 11.5, V11.5.119 Build cuda_11.5.r11.5/compiler.30672275_0  GPU model and memory  NVIDIASMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2  NVIDIA GeForce RTX 4090  Current behavior? 我通过TensorFlowTTS项目，训练了一个自己的音色模型，但是合成的音频声音很奇怪。以下是我用同一段文字在自己训练的模型和huggingface下载的模型【tensorspeech/ttsfastspeech2bakerch】合成的语音的对比。melgan用的是同一个，是从huggingface下载的【tensorspeech/ttsmb_melganbakerch】。难道melgan也要自己重新训练吗？ !Image 用我生成的模型合成出来的音频能微弱的听到声音，但是很小声。不知道是什么问题导致的。  Standalone code to reproduce the issue ```shell import soundfile as sf import numpy as np import tensorflow as tf import sys sys.path.append(""."") from tensorflow_tts.inference import AutoConfig from tensorflow_tts.inference import AutoProcessor from tensorflow_tts.inference import TFAutoModel mel_cfg = AutoConfig.from_pretrained(""/data/benben/TensorFlowTTSmaster/my_test/mel.yml"") mb_melgan = TFAutoModel.from_pretrained(""/data/benben/TensorFlowTTSmaster/my_test/mel.h5"", config=mel_cfg) config1 = AutoConfig.from_pretrained(""examples/fastspeech2/conf/fastspeech2.baker.v2.yaml"")  fastspeech2 = TFAutoModel.from_pretrained(""dump_mandarinfm/exp/train.fastspeech2.baker.v2/checkpoints/model10000.h5"", config=config1)  processor = AutoProcessor.from_pretrained(""dump_mandarinfm/baker_mapper.json"") fastspeech2 = TFAutoModel.from_pretrained(""my_test/model.h5"", config=config1) processor = AutoProcessor.from_pretrained(""my_test/processor.json"") text = ""通用在去年第三季度每天售出逾二点七万辆新车，业界关注的是转型创业企业生态圈运营商之后"" text = ""通用在去年第三季度每天售出逾二点七万辆新车"" input_ids = processor.text_to_sequence(text, inference=True) print(""***********"",input_ids) mel_before, mel_after, duration_outputs, _, _ = fastspeech2.inference(     input_ids=tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0),     speaker_ids=tf.convert_to_tensor([0], dtype=tf.int32),     speed_ratios=tf.convert_to_tensor([1.0], dtype=tf.float32),     f0_ratios =tf.convert_to_tensor([1.0], dtype=tf.float32),     energy_ratios =tf.convert_to_tensor([1.0], dtype=tf.float32), ) print(mel_before) print(mel_after)  melgan inference (meltowav) audio = mb_melgan.inference(mel_after)[0, :, 0]  save to file sf.write('./audio.wav', audio, 24000, ""PCM_16"")  sf.write('./audio.wav', audio, 24000) ```  Relevant log output ```shell ```",2025-03-22T09:15:42Z,type:performance TF 2.9,open,0,0,https://github.com/tensorflow/tensorflow/issues/89795
opt,copybara-service[bot],[xla] Autotuner: Make ExecutableConfig a virtual base class to give executable providers more implementation freedom,[xla] Autotuner: Make ExecutableConfig a virtual base class to give executable providers more implementation freedom As an example XLA:GPU backends can use existing protobuf configs to select codegen options for the autotuner,2025-03-21T17:05:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89749
yi,copybara-service[bot],[XLA:GPU] Add a pattern to fold vector.insert(vector.extract).,"[XLA:GPU] Add a pattern to fold vector.insert(vector.extract). When there is nothing fused in the transpose, we read the input and then write it to shmem. We can fold away the packing/unpacking of the vectors. ``` %0 = vector.transfer_read scf.for %arg2 = %c0 to %c2 step %c1 iter_args(%arg3 = %cst) > (vector) {          405:  %3 = vector.extract %0[%arg2] : f32 from vector          406:  %4 = vector.insert %3, %arg3 [%arg2] : f32 into vector          407:  scf.yield %4 : vector          408:  }  ``` vector.transfer_write",2025-03-21T15:07:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89741
opt,weilhuan-quic,Qualcomm AI Engine Direct - More Op Builders, WHAT 1. FloorDiv 2. NotEqual 3. Logistic 4. Pad PadV2 5. MaxPool2d 6. Cumsum 7. GatherNd 8. Pow 9. TransposeConv  TEST qnn_compiler_plugin_test ``` [] Global test environment teardown [==========] 137 tests from 5 test suites ran. (3593 ms total) [  PASSED  ] 137 tests. ``` litert_options_test ``` [] Global test environment teardown [==========] 26 tests from 1 test suite ran. (0 ms total) [  PASSED  ] 26 tests. ```,2025-03-21T09:32:50Z,cla: yes size:L,closed,0,1,https://github.com/tensorflow/tensorflow/issues/89727,I will pseudo approve this PR for testing purpose only.
mirroredstrategy,copybara-service[bot],PR #23056: Remove std::move for trivially copyable types,"PR CC(Losses collection is not thread local so it can't be used inside model_fn call when using MirroredStrategy): Remove std::move for trivially copyable types Imported from GitHub PR https://github.com/openxla/xla/pull/23056 Changes:  Removed unnecessary std::move calls for some trivially_copyable classes  Literal::CopyFrom expects a reference (&), not an rvalue reference (&&). Removed std::move on the first parameter. Copybara import of the project:  6051f15383d63210c313ab712eaa3a00c7b0105f by Alexander Pivovarov : Remove std::move for trivially copyable types Merging this change closes CC(Losses collection is not thread local so it can't be used inside model_fn call when using MirroredStrategy) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23056 from apivovarov:no_move_for_KernelMetadata 6051f15383d63210c313ab712eaa3a00c7b0105f",2025-03-21T09:12:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89726
opt,copybara-service[bot],[xla:cpu] Add new pthreadpool API to XNN thread pool integration,"[xla:cpu] Add new pthreadpool API to XNN thread pool integration This is to match the recently added API in pthreadpool.h from https://github.com/google/pthreadpool/commit/bd09d5ca9155863fc47a9cbf0df1908ef9d8cad2 Otherwise, we would get a linker error when building xnn_threadpool_test:  ``` ld: error: undefined symbol: pthreadpool_parallelize_4d_tile_2d_dynamic ``` Command to test: ``` bazel build c opt define=pthreadpool_header_only=true \   //xla/backends/cpu/runtime/xnnpack:xnn_threadpool_test ```",2025-03-21T06:58:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89721
tpu,Arshpreet98,Failed to load the native TensorFlow runtime.," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tensorflowcpu2.19.0  Custom code Yes  OS platform and distribution Windows 10  Mobile device _No response_  Python version python 3.12.1  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Traceback (most recent call last):   File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""d:\training\python\arun_sir\assignment\data science project\mask_detection\src\train_mask_detector.py"", line 1, in      import tensorflow as tf   File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.  Standalone code to reproduce the issue ```shell python c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))"" ```  Relevant log output ```shell Traceback (most recent call last):   File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File """", line 1, in    File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```",2025-03-21T05:46:11Z,stat:awaiting response type:build/install subtype:windows TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89717,"Hi  , Apologies for the delay, could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Please search for duplicate issues before opening new ones.,Are you satisfied with the resolution of your issue? Yes No
tpu,copybara-service[bot],Create better infrastructure for BatchMatMul optimizations. And add a new optimization pattern.,"Create better infrastructure for BatchMatMul optimizations. And add a new optimization pattern. Add infrastructure and utilities so its easy and intuitive to create and maintain BatchMatMul optimizations. The optimization pattern introduced in this CL will eliminate a transpose from Transpose>reshape>bmm_rhs if the transpose is used exclusively to transpose the unflattened contracting and output dims.  Consider the following transpose>reshape>bmm pattern. ``` %37 = ""tfl.transpose""(%28, %36) : (tensor, tensor) > tensor %38 = ""tfl.pseudo_const""()  : tensor}> : () > tensor %39 = ""tfl.reshape""(%37, %38) : (tensor, tensor) > tensor %40 = ""tfl.pseudo_const""()  : tensor}> : () > tensor %41 = ""tfl.batch_matmul""(%40, %39)  : (tensor, tensor) > tensor ``` This can be rewritten to use fully_connected instead of bmm, and we can avaoid the transpose ``` %39 = ""tfl.reshape""(%37, %abc) : (tensor, tensor) > tensor %40 = ""tfl.pseudo_const""()  : tensor}> : () > tensor %41 = ""tfl.fully_connected""(%40, %39) (tensor, tensor) > tensor ```",2025-03-21T05:29:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89715
yi,copybara-service[bot],"Conceptually, a shape is a union of several cases: an invalid shape, a token, an opaque, an array, or a tuple. These cases are mutually exclusive (a shape must be exactly one of these cases.)","Conceptually, a shape is a union of several cases: an invalid shape, a token, an opaque, an array, or a tuple. These cases are mutually exclusive (a shape must be exactly one of these cases.) Currently, the exclusivity of the shape cases is not enforced, allowing bugs where a shape is used as the wrong case (e.g. accessing the dimension fields of a tuple shape). This change makes `Shape` safer by representing its state as an `std::variant`. By construction, this guarantees that it can hold the state for just one case, and trying to access the state for another case would result in a crash. Given the large number of existing bugs uncovered by the intended change, this change is deliberately conservative: in some cases where the caller violates the precondition of a `Shape` method, we choose to return a default value instead of crashing (e.g. calling `dimensions()` on a nonarray shape would return an empty span even though it's a programmer error). We will tighten the enforcement of the preconditions in future CLs.",2025-03-21T02:57:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89700
tpu,copybara-service[bot],Bumping up libtpu version to pick correct versioned nightlies,Bumping up libtpu version to pick correct versioned nightlies,2025-03-21T02:36:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89698
opt,copybara-service[bot],"If the `dynamic_dimensions` parameter is empty in the `Shape` ctor, assume all dimensions are static.","If the `dynamic_dimensions` parameter is empty in the `Shape` ctor, assume all dimensions are static. Some callers call the `Shape(element_type, dimensions, dynamic_dimensions)` ctor with a nonempty `dimensions` and an empty `dynamic_dimensions`. This breaks the shape object's invariant that the two should have the same size. We have two options for fixing this: 1. Force the caller to always provide a `dynamic_dimensions` whose size matches that of `dimensions`. 2. Provide a sensible default behavior when `dynamic_dimensions` is empty. I chose CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"") as: 1. CC(Add support for Python 3.x) is more risky as it may cause the compiler to crash in production (e.g. if we don't have adequate test coverage). 2. It's very common for an array to have only static dimensions. Therefore it's good to optimize the user experience for this common case.",2025-03-20T22:51:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89684
tpu,copybara-service[bot],Preserve shapes for TPUPartitionedInputV2Op.,Preserve shapes for TPUPartitionedInputV2Op.,2025-03-20T20:30:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89678
opt,copybara-service[bot],PR #23997: Fix the build issue after commit df841fe,"PR CC(Tensor.eval() hangs with Tensorflow 1.10+): Fix the build issue after commit df841fe Imported from GitHub PR https://github.com/openxla/xla/pull/23997 When running tests, like `bazel test //xla/service/gpu/model:hlo_op_profiler_test`, on a machine with only CUDA configured, the build system will try to build ROCm tests and fail. Before this change: ``` ERROR: xla/xla/stream_executor/rocm/BUILD:791:13: Compiling xla/stream_executor/rocm/rocm_helpers.cu.: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing CppCompile command (from target //xla/stream_executor/rocm:rocm_helpers) external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8opt/bin/xla/stream_executor/rocm/_objs/rocm_helpers/rocm_helpers.cu.pic.d ... (remaining 37 arguments skipped) /home/svaishay/.cache/bazel/_bazel_svaishay/3b9a2c84bcbdad9b9781b5eb1069603a/execroot/xla/external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc:227: SyntaxWarning: invalid escape sequence '\.'   re.search('\.cpp$          ^~~~~~~~~~~~~~~~~~~~ 1 error generated. Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 3.333s, Critical Path: 0.18s INFO: 28 processes: 28 internal. ERROR: Build did NOT complete successfully //xla/service/gpu/model:hlo_op_profiler_test_gpu_a100                 NO STATUS //xla/service/gpu/model:hlo_op_profiler_test_gpu_any                  NO STATUS //xla/service/gpu/model:hlo_op_profiler_test_gpu_b200                 NO STATUS //xla/service/gpu/model:hlo_op_profiler_test_gpu_h100                 NO STATUS ``` After this change: ``` INFO: Build completed successfully, 5 total actions //xla/service/gpu/model:hlo_op_profiler_test_gpu_a100                    PASSED in 14.9s //xla/service/gpu/model:hlo_op_profiler_test_gpu_any                     PASSED in 15.4s //xla/service/gpu/model:hlo_op_profiler_test_gpu_b200                    PASSED in 16.2s //xla/service/gpu/model:hlo_op_profiler_test_gpu_h100                    PASSED in 14.3s ``` Copybara import of the project:  e0c179778cc2fcc1f2f2a3bb74c9d6c6daaec5ea by Shraiysh Vaishay : Fix the build issue after commit df841fe When running tests, like `bazel test //xla/tests:hlo_op_profiler_test`, on a machine with only CUDA configured, the build system will try to build ROCm tests and fail. This commit fixes the issue by only building ROCm tests if ROCm is configured and vice versa. Merging this change closes CC(Tensor.eval() hangs with Tensorflow 1.10+) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23997 from shraiysh:fix_build_issue e0c179778cc2fcc1f2f2a3bb74c9d6c6daaec5ea",2025-03-20T19:22:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89673
quantization,distlibs,TensorFlow Lite 2.19 ARM compilation failed: TfLiteQuantizationType : int expected identifier or ...," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19  OS platform and distribution Raspberry Pi OS Bookworm  GCC/compiler version gcc (Debian 12.2.014) 12.2.0  Current behavior? Getting error when trying to compile simple program using TensorFlow Lite 2.19 library on Raspberry Pi. main.c: ``` include  include  include  int main() {     return 0; } ``` Compilation error: ``` In file included from build/usr/local/include/tensorflow/lite/c/common.h:31,                  from main.c:2: build/usr/local/include/tensorflow/lite/core/c/common.h:325:37: error: expected identifier or ‘(’ before ‘:’ token   325    ^~~~~~~~~~~~~~~~~~~~~~ ``` Compilation command: ``` gcc main.c Ibuild/usr/local/include Lbuild/usr/local/lib o test ltensorflowlite_c ``` No problems using TensorFlow Lite 2.18. This commit https://github.com/tensorflow/tensorflow/commit/977257e45ea83169c2dd6ff24a403cdd05b138bd caused issue.",2025-03-20T17:57:36Z,type:build/install comp:lite subtype: raspberry pi awaiting PR merge TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89666,"Hi,   I apologize for the delay in my response, thank you for bringing this issue to our attention now I have created a PR https://github.com/tensorflow/tensorflow/pull/91416 to take care of this issue. Thank you for your cooperation and patience.","Hi,  I closing this issue because this PR https://github.com/tensorflow/tensorflow/pull/91416 got merged, if you face any issue please feel free to post your comments if required I'll open this issue. Thank you for your cooperation and patience.",Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],PR #89210: Qualcomm AI Engine Direct - Provide op optimization,PR CC(Qualcomm AI Engine Direct  Provide op optimization): Qualcomm AI Engine Direct  Provide op optimization Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/89210 Summary:  Support FC>CONV2D optimization  Add CONV2D op builder with CPU transpose Test:  [x] a8w8 Gemma3 compile and execution successfully  [x] qnn_compiler_plugin_test all pass Copybara import of the project:  8ff43628b495ce8ae730a5ed42fa6bea1ef15844 by jiunkaiy : Qualcomm AI Engine Direct  Op optimization Summary:  Support FC>CONV2D optimization  Add CONV2D op builder with CPU transpose Merging this change closes CC(Qualcomm AI Engine Direct  Provide op optimization) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89210 from jiunkaiy:dev/jiunkaiy/gemma2_fc 8ff43628b495ce8ae730a5ed42fa6bea1ef15844,2025-03-20T17:36:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89662
yi,copybara-service[bot],`ArrayImpl` is now a proper subclass of `jax.Array`,"`ArrayImpl` is now a proper subclass of `jax.Array` This allows to make `jax.Array` a ""strict"" ABC which doesn't support virtual subclasses and is thus can do faster isinstance/issubclass checks. Note that I had to move `StrictABC` from `util` into a separate submodule to avoid an import cycle with `basearray` and `xla_client`.",2025-03-20T14:16:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89651
opt,copybara-service[bot],[XLA] Add an option to dump full HLO config proto,[XLA] Add an option to dump full HLO config proto,2025-03-20T09:29:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89629
tpu,eewindfly,Add support for SeparableConv2DTranspose (Depthwise Conv2DTranspose)," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.16.1  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Feature Request TensorFlow currently lacks a SeparableConv2DTranspose operation, which is essential for efficient depthwise transposed convolutions. PyTorch already supports this easily via the groups parameter in ConvTranspose2d. However, TensorFlow’s Conv2DTranspose does not have a groups argument, making it impossible to achieve the same functionality. Workarounds Currently, the only way to approximate this functionality in TensorFlow is: 	•	Manually splitting input channels and applying Conv2DTranspose separately (inefficient and slow). Proposed Solution 	•	Either add a groups parameter to Conv2DTranspose, similar to PyTorch, 	•	Or implement a native SeparableConv2DTranspose API to complement SeparableConv2D. Use Case Separable transposed convolutions are useful for: 	•	Efficient upsampling in lightweight CNN architectures. 	•	Mobile and edge computing models requiring optimized computations. Would be great to have this officially supported in TensorFlow! Thanks. CC(Feature Request: Add separable_conv2d_transpose operation)  Previous discussion has been closed which is not able to reopen, so I create a new one for it.  Standalone code to reproduce the issue ```shell None ```  Relevant log output ```shell ```",2025-03-20T09:27:50Z,type:feature,open,0,0,https://github.com/tensorflow/tensorflow/issues/89628
yi,copybara-service[bot],Remove runtime dependencies from gpu_executable,Remove runtime dependencies from gpu_executable `GpuExecutable` is a type that is used by both the compiler and the runtime. Therefore it shouldn't link runtimeonly dependencies. So this change removes those dependencies which required to add some explicit dependencies to downstream users of GpuExecutable that was transitively relying on the dependency. This is a prerequisite for turning XLA into a proper AOT/deviceless compiler and also for the CUDA/ROCm modularization,2025-03-20T09:20:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89626
opt,copybara-service[bot],Add a option to apply the plugin to the subgraph with given index only.,Add a option to apply the plugin to the subgraph with given index only.,2025-03-20T07:14:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89619
tpu,zhouxiaoyaozzz,NotImplementedError: StreamingModel.call() not implemented," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.18.0  Custom code Yes  OS platform and distribution windows 11  Mobile device _No response_  Python version 3.12.6  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When I try to compile and run my custom Keras model (StreamingModel), I encounter the following error: `NotImplementedError: Exception encountered when calling StreamingModel.call(). Could not automatically infer the output shape / dtype of 'streaming_model' (of type StreamingModel). Either the `StreamingModel.call()` method is incorrect, or you need to implement the `StreamingModel.compute_output_spec() / compute_output_shape()` method. Error encountered: Model StreamingModel does not have a `call()` method implemented.` Expected Behavior I expect the StreamingModel to correctly implement the call() method and infer the output shape/dtype automatically, or allow me to manually specify it using compute_output_spec() or compute_output_shape().  Standalone code to reproduce the issue ```shell import tensorflow as tf  Define CircularBufferLayer class CircularBufferLayer(tf.keras.layers.Layer):     def __init__(self, num_features, buffer_size, stride, **kwargs):         super().__init__(**kwargs)         self.num_features = num_features         self.buffer_size = buffer_size         self.stride = stride         self.gradient_scale = 0.1         self.buffer = self.add_weight(name='buffer', shape=(1, buffer_size, num_features),                                       initializer='zeros', trainable=False, dtype=tf.float32)         self.call_count = self.add_weight(name='call_count', shape=(), initializer='zeros',                                           dtype=tf.int32, trainable=False)         self.total_call_count = self.add_weight(name='total_call_count', shape=(), initializer='zeros',                                                 dtype=tf.int32, trainable=False)     def call(self, inputs):         scaled_input = tf.multiply(inputs, self.gradient_scale)         new_buffer = tf.concat([scaled_input, self.buffer[:, :1]], axis=1)         self.buffer.assign(new_buffer)         return self.buffer  Define StreamingModel class StreamingModel(tf.keras.Model):     def call(self, inputs):         x, _ = super().call(inputs)   Assume another branch is truncated         return x  Instantiate the model buffer_layer = CircularBufferLayer(num_features=64, buffer_size=10, stride=1) model = StreamingModel()  Define input shape input_shape = (None, 10, 64)   (batch_size, sequence_length, num_features) inputs = tf.keras.Input(shape=input_shape[1:])   Ignore batch_size outputs = model(inputs)  Build the model model = tf.keras.Model(inputs=inputs, outputs=outputs)  Compile the model model.compile(     optimizer='adam',   Use Adam optimizer     loss='mse',        Use mean squared error as the loss function     metrics=['mae']    Use mean absolute error as the evaluation metric ) ```  Relevant log output ```shell 20250319 16:09:45.545622: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250319 16:09:46.283611: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250319 16:09:48.149403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. WARNING:tensorflow:From D:\project\DLComplier\.venv\Lib\sitepackages\keras\src\backend\tensorflow\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. Traceback (most recent call last):   File ""D:\project\DLComplier\.venv\statiblity.py"", line 37, in      outputs = model(inputs)               ^^^^^^^^^^^^^   File ""D:\project\DLComplier\.venv\Lib\sitepackages\keras\src\utils\traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""D:\project\DLComplier\.venv\statiblity.py"", line 27, in call     x, _ = super().call(inputs)   Assume another branch is truncated            ^^^^^^^^^^^^^^^^^^^^ NotImplementedError: Exception encountered when calling StreamingModel.call(). Could not automatically infer the output shape / dtype of 'streaming_model' (of type StreamingModel). Either the `StreamingModel.call()` method is incorrect, or you need to implement the `StreamingModel.compute_output_spec() / compute_output_shape()` method. Error encountered: Model StreamingModel does not have a `call()` method implemented. Arguments received by StreamingModel.call():   • args=('',)   • kwargs= ```",2025-03-20T02:55:01Z,type:bug TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/89600
tpu,jq,How to adapt TFRA with Tensorflow Parameter Strategy V2," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.16  Custom code Yes  OS platform and distribution all OS  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? tensorflow recomendation addon 's keras + Ps has been broken for sometime, following are the issues so far https://github.com/tensorflow/recommendersaddons/issues/182 https://github.com/tensorflow/recommendersaddons/issues/401 https://github.com/tensorflow/recommendersaddons/issues/365 Specifically, the incompatibility stems from changes introduced by TensorFlow’s ParameterServerStrategy. Two issues have been identified so far, though there may be more:  Changes in ParameterServerStrategy.extended.worker_devices return values – This issue has already been addressed in https://github.com/tensorflow/recommendersaddons/pull/488  Graph creation and variable placement differences – ParameterServerStrategy creates the computation graph on the PS and then distributes it to workers. TFRA,  uses proxy variables on each worker. These proxy variables wrapped in DistributedVariableWrapper and inherit from DistributedVariable, and tfra rely on DistributedVariable to get the correct device placement i.e. ._get_on_device_or_primary method,  however, this  only returns device 0, i.e. /job:worker/replica:0/task:0/device:GPU:0 , under ParameterServerStrategy, so all other workers keep crashing. I wonder what will be correct path to fix the TensorFlow’s ParameterServerStrategy + TFRA + keras?  I had explored PerWorkerVariable  to get a PerWorkerVariable, we need to use something like  `variables.Variable(initial_value=(),   shape=shape, dtype=dtype, name=name, per_worker_de_variable=True)`       this is not clean for complex classes, i.e. `class TrainableWrapper(resource_variable_ops.ResourceVariable):` `class ShadowVariable(EmbeddingWeights, TrainableWrapper):`       to make ShadowVariable a PerWorkerVariable,   new classes needed with full duplication  ` class TrainableWrapperPerWorker(PerWorkerVariable):` `class ShadowVariablePerWorker(EmbeddingWeights, TrainableWrapperPerWorker): ` is this the right path?  Standalone code to reproduce the issue ```shell any of the code in the issues https://github.com/tensorflow/recommendersaddons/issues/182 https://github.com/tensorflow/recommendersaddons/issues/401 https://github.com/tensorflow/recommendersaddons/issues/365 ```  Relevant log output ```shell RROR:tensorflow:Worker /job:worker/replica:0/task:1 failed with InvalidArgumentError():/job:worker/replica:0/task:0/device:GPU:0 unknown device. Additional GRPC error information from remote target /job:worker/replica:0/task:1 while calling /tensorflow.eager.EagerService/Enqueue: :{""created"":"".320488513"",""description"":""Error received from peer ipv4:127.0.0.1:2232"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""/job:worker/replica:0/task:0/device:GPU:0 unknown device."",""grpc_status"":3} [Op:__inference_train_function_2473] E0320 02:48:37.325277 139838372021824 cluster_coordinator.py:949] Worker /job:worker/replica:0/task:1 failed with InvalidArgumentError():/job:worker/replica:0/task:0/device:GPU:0 unknown device. Additional GRPC error information from remote target /job:worker/replica:0/task:1 while calling /tensorflow.eager.EagerService/Enqueue: :{""created"":"".320488513"",""description"":""Error received from peer ipv4:127.0.0.1:2232"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""/job:worker/replica:0/task:0/device:GPU:0 unknown device."",""grpc_status"":3} [Op:__inference_train_function_2473] INFO:tensorflow:[Worker 1] Putting back a closure after it failed. I0320 02:48:37.325551 139838372021824 cluster_coordinator.py:1077] [Worker 1] Putting back a closure after it failed. INFO:tensorflow:[Worker 1] Clearing all resources. I0320 02:48:37.325744 139838372021824 cluster_coordinator.py:1065] [Worker 1] Clearing all resources. INFO:tensorflow:Cluster now being recovered. I0320 02:48:37.325959 139838355236416 cluster_coordinator.py:991] Cluster now being recovered. 20250320 02:48:37.326268: W tensorflow/core/common_runtime/eager/context_distributed_manager.cc:694] Device filters can only be specified when initializing the cluster. Any changes in device filters are ignored when updating the server def. INFO:tensorflow:Cluster successfully recovered. I0320 02:48:37.334258 139838355236416 cluster_coordinator.py:997] Cluster successfully recovered. INFO:tensorflow:Worker /job:worker/replica:0/task:1 has been recovered. I0320 02:48:37.334562 139838372021824 cluster_coordinator.py:964] Worker /job:worker/replica:0/task:1 has been recovered. INFO:tensorflow:Worker /job:worker/replica:0/task:1 calling on_recovery_fn I0320 02:48:37.334733 139838372021824 cluster_coordinator.py:967] Worker /job:worker/replica:0/task:1 calling on_recovery_fn INFO:tensorflow:[Worker 1] calling _on_worker_recovery I0320 02:48:37.334853 139838372021824 cluster_coordinator.py:1103] [Worker 1] calling _on_worker_recovery 20250320 02:48:37.361068: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:553] The `assert_cardinality` transformation is currently not handled by the autoshard rewrite and will be removed. 20250320 02:48:37.362102: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:553] The `assert_cardinality` transformation is currently not handled by the autoshard rewrite and will be removed. ```",2025-03-20T02:35:58Z,type:feature,open,0,0,https://github.com/tensorflow/tensorflow/issues/89599
opt,copybara-service[bot],Disable `//xla/hlo/tools/tests:hlo_opt_emit_proto.hlo.test` on ARM due to avoid flakes,Disable `//xla/hlo/tools/tests:hlo_opt_emit_proto.hlo.test` on ARM due to avoid flakes,2025-03-20T00:17:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89594
quantization,copybara-service[bot],Fork fake_quant_utils within TF to use tf_quantization_lib (fork using TF Quant Dialect),Fork fake_quant_utils within TF to use tf_quantization_lib (fork using TF Quant Dialect),2025-03-20T00:15:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89593
sharding,copybara-service[bot],Adjust sharding rules for ragged_dot operations.,Adjust sharding rules for ragged_dot operations.  Mode 3 (the ragged dimension is a batch dimension) Do not propagate shardings to/from the group_sizes (the 3rd operand) since it is redundant. We will discard this tensor and convert ragged_dot into a standard dot in the partitioner.  Mode 1 (the ragged dimension is a lhs noncontracting dimension) The ragged dimension and the group_size dimension needs full replication in the partitioner.  Mode 2 (the ragged dimension is a contracting dimension) The ragged dimension and the group_size dimension needs full replication in the partitioner. We do not mark the ragged dimension as replicated although it is a contracting dimension.,2025-03-20T00:11:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89592
quantization,copybara-service[bot],Fork quantization_lib within TF to use TF Quant Dialect,Fork quantization_lib within TF to use TF Quant Dialect,2025-03-19T21:21:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89583
opt,copybara-service[bot],IFRT: Introduce `LoadedExecutable::GetDonatableInputIndices()`.,"IFRT: Introduce `LoadedExecutable::GetDonatableInputIndices()`. The function returns indices of parameters that will be donated whenever `Execute` gets called, provided they are not present in `execute_options.non_donatable_input_indices`. This change will be used in an upcoming IFRTproxy commit that will allow make incurring RPC roundtrips for `Array::IsDeleted()` unnecessary.",2025-03-19T17:59:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89569
yi,copybara-service[bot],internal BUILD rule visibility,internal BUILD rule visibility,2025-03-19T17:52:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89568
opt,copybara-service[bot],[XLA:GPU] Decompose all applicable collective-permutes when pipeline parallelism optimizations enabled,[XLA:GPU] Decompose all applicable collectivepermutes when pipeline parallelism optimizations enabled,2025-03-19T17:48:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89567
yi,copybara-service[bot],[XLA:GPU] new fusion kind for nested dot fusions,[XLA:GPU] new fusion kind for nested dot fusions To prevent priority fusion from modifying the construct produced by nest_gemm_fusion pass.  For example priority fusion may insert back bitcasts that we have carefully extracted before.,2025-03-19T16:48:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89562
quantization,RaulHerreroMuela,Efficientnet - TFLite Inference Issue,"**System information**  OS Platform and Distribution: Win 11  TensorFlow installed from: source  TensorFlow version: 2.9.3 **Description of the problem** I have trained EfficientNetB0 to do regression. I convert to TFLite the model according to the above code. I make inference on the same set of images with both the Keras model and the converted to TFLite and compare the obtained estimates by means of the percentage difference. As shown in the image (each error column corresponds to a different model), sometimes the estimates are practically similar and other times they present differences of more than 1000%. What can this be due to? Other types of models created by me layer by layer do not present this problem and I have used the same conversion code. I need to quantization to float16 because the device i run the TFLite model have low computational power. **Code from tflite_convert** ``` def saved_model_to_tflite_convert(saved_model_dir, save_path_tflite):     converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)     converter.optimizations = [tf.lite.Optimize.DEFAULT]     converter.target_spec.supported_types = [tf.float16]      tflite_model = converter.convert()     with open(save_path_tflite, ""wb"") as f:         f.write(tflite_model) ``` **Attached image** Each row belong to a different image (input of the model). !Image",2025-03-19T14:37:18Z,stat:awaiting response stale comp:lite subtype:windows TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89556,"Hi,   I apologize for the delay in my response, I see you have mentioned `TensorFlow version 2.9.3` which is older version if possible could you please try with latest `TensorFlow version 2.19.0` and see is it resolving your issue or not ? If issue still persists with latest TensorFlow version 2.19.0, I see `EfficientNet` models have documented compatibility problems with `float16` precision https://github.com/tensorflow/tensorflow/issues/50171 Unlike custom layerbylayer models (which you mentioned work fine) EfficientNet's architecture contains complex interactions between normalization layers and activation functions that may not translate well to reduced precision The preprocessing logic built into `EfficientNet` models might be handled differently between the Keras implementation and the TFLite conversion. This could create inconsistencies in how input data is normalized before being processed by the actual model. I would suggest you to try alternative quantization approaches like dynamic range quantization for detailed information please refer this official documentation for other quantization schemes: ``` converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) converter.optimizations = [tf.lite.Optimize.DEFAULT] tflite_model = converter.convert() ``` If I have missed something here please let me know. Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
bfloat16,copybara-service[bot],[xla:cpu] Disable SVE LLVM codegen by default on AArch64 CPUs.,"[xla:cpu] Disable SVE LLVM codegen by default on AArch64 CPUs. There are many missing SVE lowerings in LLVM, especially for bf16 type. This causes program termination on SVEavailable machines such as Google Axion. Example error:  ``` LLVM ERROR: Cannot select: 0xf7eb74164950: nxv4bf16 = AArch64ISD::UINT_TO_FP_MERGE_PASSTHRU 0xf7eb74186d60, 0xf7eb74186200, undef:nxv4bf16   0xf7eb74186d60: nxv4i1 = AArch64ISD::PTRUE TargetConstant:i32     0xf7eb74191580: i32 = TargetConstant   0xf7eb74186200: nxv4i32,ch = load) from %ir.scevgep16, !noalias !5), zext from nxv4i8> 0xf7eb740d8830, 0xf7eb74194070, undef:i64     0xf7eb74194070: i64 = add 0xf7eb741922f0, 0xf7eb74193c10       0xf7eb741922f0: i64,ch = CopyFromReg 0xf7eb740d8830, Register:i64 %7         0xf7eb74191e90: i64 = Register %7       0xf7eb74193c10: i64,ch = CopyFromReg 0xf7eb740d8830, Register:i64 %11         0xf7eb74192750: i64 = Register %11     0xf7eb74186350: i64 = undef   0xf7eb74164a30: nxv4bf16 = undef In function: convert.2_kernel Fatal Python error: Aborted ``` Since most AArch64 machines still use 128bit registers, SVE and NEON shouldn't have significant performance difference, so we disable SVE codegen in public builds for the time being. After JAX uses an XLA commit that has changes from this PR, the following JAX tests will pass on Axion: ``` bazel test //tests:shape_poly_test_cpu test_filter=*test_harness_vmap_convert_element_type_dtypes_to_dtypes_shape_bool_100_100_olddtype_bool_newdtype_bfloat16* bazel test //tests:export_harnesses_multi_platform_test_cpu test_filter=*test_prim_convert_element_type_dtypes_to_dtypes_shape_uint8_100_100_olddtype_uint8_newdtype_bfloat16*  ``` Add `test_env=XLA_FLAGS=xla_cpu_max_isa=""""` to the options to get the errors back.",2025-03-19T14:37:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89555
tpu,copybara-service[bot],[XLA:CPU] Reuse output memory for aliased inputs in benchmarking,[XLA:CPU] Reuse output memory for aliased inputs in benchmarking Benchmarking HLOs that alias inputs would fail with this library. The change allows aliasing by reusing the memory from the execution outputs.,2025-03-19T10:57:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89546
opt,copybara-service[bot],PR #23786: Multihost HLO runner: fix --while_execution_count behavior.,"PR CC(Update graph_transformations.h): Multihost HLO runner: fix while_execution_count behavior. Imported from GitHub PR https://github.com/openxla/xla/pull/23786 Optimized HLO can have known_trip_count while loop annotation, which has to be updated using WhileLoopTripCountAnnotator after HloControlFlowFlattening to apply the value of while_execution_count at runtime. Copybara import of the project:  5648a58d78726f116a64030f48dd63835818fbf7 by Ilia Sergachev : [NFC] Factor common test logic out into a function.  21d4e89aab6fc1dfe66edde6d11eabaec5ea6a84 by Ilia Sergachev : Multihost HLO runner: fix while_execution_count behavior. Optimized HLO can have known_trip_count while loop annotation, which has to be updated using WhileLoopTripCountAnnotator after HloControlFlowFlattening to apply the value of while_execution_count at runtime.  fd93a631e6b818df7d98f29a55b53a8819eae9a3 by Ilia Sergachev : Address review request. Merging this change closes CC(Update graph_transformations.h) Reverts b68ce06d697405d1a911babef312dd80513edd79 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23786 from openxla:fix_while_execution_count fd93a631e6b818df7d98f29a55b53a8819eae9a3",2025-03-19T09:25:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89543
tpu,zhouxiaoyaozzz,Compilation error（The call method of the StreamingModel class is not implemented correctly）," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Is this a new compilation error？The call method of the StreamingModel class is not implemented correctly, causing TensorFlow/Karas to be unable to infer the output shape and data type of the model.  Standalone code to reproduce the issue ```shell import tensorflow as tf  定义 CircularBufferLayer class CircularBufferLayer(tf.keras.layers.Layer):     def __init__(self, num_features, buffer_size, stride, **kwargs):         super().__init__(**kwargs)         self.num_features = num_features         self.buffer_size = buffer_size         self.stride = stride         self.gradient_scale = 0.1         self.buffer = self.add_weight(name='buffer', shape=(1, buffer_size, num_features),                                       initializer='zeros', trainable=False, dtype=tf.float32)         self.call_count = self.add_weight(name='call_count', shape=(), initializer='zeros',                                           dtype=tf.int32, trainable=False)         self.total_call_count = self.add_weight(name='total_call_count', shape=(), initializer='zeros',                                                 dtype=tf.int32, trainable=False)     def call(self, inputs):         scaled_input = tf.multiply(inputs, self.gradient_scale)         new_buffer = tf.concat([scaled_input, self.buffer[:, :1]], axis=1)         self.buffer.assign(new_buffer)         return self.buffer  定义 StreamingModel class StreamingModel(tf.keras.Model):     def call(self, inputs):         x, _ = super().call(inputs)   假设另一个分支被截断         return x  实例化模型 buffer_layer = CircularBufferLayer(num_features=64, buffer_size=10, stride=1) model = StreamingModel()  定义输入形状 input_shape = (None, 10, 64)   (batch_size, sequence_length, num_features) inputs = tf.keras.Input(shape=input_shape[1:])   忽略 batch_size outputs = model(inputs)  构建模型 model = tf.keras.Model(inputs=inputs, outputs=outputs)  编译模型 model.compile(     optimizer='adam',   使用 Adam 优化器     loss='mse',        使用均方误差作为损失函数     metrics=['mae']    使用平均绝对误差作为评估指标 )  准备训练数据 x_train = tf.random.normal((100, 10, 64))   100 个样本，每个样本的形状为 (10, 64) y_train = tf.random.normal((100, 10, 64))   100 个样本，每个样本的形状为 (10, 64)  训练模型 history = model.fit(     x_train, y_train,     batch_size=32,   批量大小     epochs=10,       训练轮数     validation_split=0.2   使用 20% 的数据作为验证集 )  保存模型 model.save('streaming_model.h5') ```  Relevant log output ```shell D:\project\DLComplier\.venv\Scripts\python.exe D:\project\DLComplier\.venv\statiblity.py  20250319 16:09:45.545622: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250319 16:09:46.283611: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250319 16:09:48.149403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. WARNING:tensorflow:From D:\project\DLComplier\.venv\Lib\sitepackages\keras\src\backend\tensorflow\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. Traceback (most recent call last):   File ""D:\project\DLComplier\.venv\statiblity.py"", line 37, in      outputs = model(inputs)               ^^^^^^^^^^^^^   File ""D:\project\DLComplier\.venv\Lib\sitepackages\keras\src\utils\traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""D:\project\DLComplier\.venv\statiblity.py"", line 27, in call     x, _ = super().call(inputs)   假设另一个分支被截断            ^^^^^^^^^^^^^^^^^^^^ NotImplementedError: Exception encountered when calling StreamingModel.call(). Could not automatically infer the output shape / dtype of 'streaming_model' (of type StreamingModel). Either the `StreamingModel.call()` method is incorrect, or you need to implement the `StreamingModel.compute_output_spec() / compute_output_shape()` method. Error encountered: Model StreamingModel does not have a `call()` method implemented. Arguments received by StreamingModel.call():   • args=('',)   • kwargs= ```",2025-03-19T08:11:56Z,type:bug TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/89540
sharding,copybara-service[bot],[JAX] Use `xla::ifrt::Client::MakeArraysFromHostBufferShards()` in Array creation when possible,"[JAX] Use `xla::ifrt::Client::MakeArraysFromHostBufferShards()` in Array creation when possible This changes makes use of the new `xla::ifrt::Client::MakeArraysFromHostBufferShards()` API when possible. This API needs a single call to create a multishard IFRT Array (to be wrapped as a JAX `PyArray`), which provides more optimization opportunities for the runtime than creating singledevice IFRT Arrays and then assembling them. Please note that `xla::ifrt::Client::MakeArraysFromHostBufferShards()` implementation in PjRtIFRT is not yet optimized, so there is no immediate performance benefits for McJAX. As an exception, it takes the previous path of array assembly if any shard for `BatchedDevicePut` is not a host buffer, but already a singledevice array, because `xla::ifrt::Client::MakeArraysFromHostBufferShards()` works only if all the sharded input to be host buffers. With batching possible at IFRT level, we now skip `DevicePutResultFn` step; `DevicePut` (now `DevicePutWithDevice` and `DevicePutWithSharding`) internally calls pershard functions (with GIL released) and returns a final IFRT Array. This change includes a code cleanup for `xla::DevicePutResult::owning_pybuffer`, which was originally intended to hold a Python object to keep an IFRT Array valid when it is created from `DevicePut()` implementations, but this role has been entirely covered by `on_done_with_host_buffer` function supplied at IFRT Array creation time.",2025-03-19T01:59:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89513
opt,copybara-service[bot],[re-land][xla:cpu] enable scatter fusion emitter,"[reland][xla:cpu] enable scatter fusion emitter Known issues:  bf16 performance is poor. This is because in the emitters we are missing an optimization that we have in XLA thunks. We will fix this soon.  No parallel scatter. We are leaving this as future work since the singlethreaded implementation is already bringing significant performance improvements. Scatter microbenchmarks: ```                                                                            │      thunks      │           scatter                   │                                                                            │    cpusec/op    │  cpusec/op   vs base               │ BM_ScatterS32_R1/262144/262144/process_time                                       590.2µ ± 0%   203.0µ ±  1%  65.61% (p=0.002 n=6) BM_ScatterS32_R2/512/512/process_time                                             78.61µ ± 2%   49.90µ ±  3%  36.52% (p=0.002 n=6) BM_ScatterS32_R3/64/64/process_time                                               54.68µ ± 2%   50.74µ ±  3%   7.21% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:1/d1:64/d2:8/num_slices:1/process_time            696.2n ± 2%   680.3n ±  4%   2.29% (p=0.026 n=6) BM_SimpleScatterReduceF32_R3/d0:50/d1:64/d2:8/num_slices:10/process_time         106.92µ ± 9%   17.73µ ±  3%  83.42% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:500/d1:64/d2:8/num_slices:100/process_time       10.351m ± 0%   2.584m ±  2%  75.04% (p=0.002 n=6) BM_SelectAndScatterF32/128/process_time                                           36.18µ ± 3%   27.78µ ±  0%  23.21% (p=0.002 n=6) BM_SelectAndScatterF32/256/process_time                                          117.37µ ± 1%   86.42µ ±  0%  26.37% (p=0.002 n=6) BM_SelectAndScatterF32/512/process_time                                           1.438m ± 4%   1.487m ± 31%   +3.40% (p=0.041 n=6) geomean                                                                           131.7µ        72.76µ        44.76% ``` The gap from a few days ago was wider (geomean improvement of 76%), but the recent work on improving performance of small while loops (https://github.com/openxla/xla/commit/db734148ec74) narrowed that to the numbers above. Legacy emitters (""nothunks"") compile all while loops, and are therefore a tougher baseline to compare against. Still, scatter fusion emitters are faster (all singlethreaded): ```                                                                            │   nothunks       │          scatter                    │                                                                            │    cpusec/op    │  cpusec/op   vs base               │ BM_ScatterS32_R1/262144/262144/process_time                                      301.6µ ±  0%   203.0µ ±  1%  32.70% (p=0.002 n=6) BM_ScatterS32_R2/512/512/process_time                                            50.53µ ±  1%   49.90µ ±  3%        ~ (p=0.180 n=6) BM_ScatterS32_R3/64/64/process_time                                              50.12µ ±  1%   50.74µ ±  3%   +1.23% (p=0.009 n=6) BM_SimpleScatterReduceF32_R3/d0:1/d1:64/d2:8/num_slices:1/process_time           593.0n ±  1%   680.3n ±  4%  +14.72% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:50/d1:64/d2:8/num_slices:10/process_time         17.46µ ±  1%   17.73µ ±  3%        ~ (p=0.093 n=6) BM_SimpleScatterReduceF32_R3/d0:500/d1:64/d2:8/num_slices:100/process_time       2.731m ±  1%   2.584m ±  2%   5.38% (p=0.002 n=6) BM_SelectAndScatterF32/128/process_time                                          28.95µ ±  2%   27.78µ ±  0%   4.04% (p=0.002 n=6) BM_SelectAndScatterF32/256/process_time                                          97.36µ ±  1%   86.42µ ±  0%  11.23% (p=0.002 n=6) BM_SelectAndScatterF32/512/process_time                                          1.211m ± 32%   1.487m ± 31%        ~ (p=0.065 n=6) geomean                                                                          74.85µ         72.76µ         2.79% ``` Reverts b68ce06d697405d1a911babef312dd80513edd79",2025-03-19T00:05:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89507
tpu,copybara-service[bot],Reorder `BroadcastToOp` with all unary operators except `CastOp`.,"Reorder `BroadcastToOp` with all unary operators except `CastOp`. `CastOp` requires special care as it determines the output type from the output tensor, so we can't use graph transformation rules directly.",2025-03-18T23:23:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89502
int8,copybara-service[bot],Style improvements to `primitive_util`:,"Style improvements to `primitive_util`:  `CastPreservesValues()` is too complex to be a good inline function candidate. Move its implementation to `.cc` to avoid cluttering the header file.  The name `IsCanonicalRepresentation(primitive_type)` is misleading. It returns true if T can losslessly represent all values of the given type. E.g. even though the canonical representation of `U8` is `uint8_t`, `IsCanonicalRepresentation(U8)` still returns true. Rename the function to avoid confusion.",2025-03-18T22:24:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89497
sharding,copybara-service[bot],Fix sharding_util_ops_test dependency.,Fix sharding_util_ops_test dependency.,2025-03-18T22:06:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89496
opt,copybara-service[bot],"Adds two new methods to the `ifrt::Client` interface: (1) `CreateContext` allows the user to create a `UserContext` object that captures the current runtime context that may include data such as the current call stack. This context can then be passed to other IFRT operations so they can be tagged by the client context that triggered them, and this is expected to substantially simplify the performance analysis and debugging process. (2) A variant of `MakeArrayFromHostBuffer` that makes use of this and accepts a UserContext as the last parameter.","Adds two new methods to the `ifrt::Client` interface: (1) `CreateContext` allows the user to create a `UserContext` object that captures the current runtime context that may include data such as the current call stack. This context can then be passed to other IFRT operations so they can be tagged by the client context that triggered them, and this is expected to substantially simplify the performance analysis and debugging process. (2) A variant of `MakeArrayFromHostBuffer` that makes use of this and accepts a UserContext as the last parameter. The broader plan is to make all the IFRT operations uniformly accept the UserContext as the last optional parameter.",2025-03-18T21:06:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89491
opt,copybara-service[bot],Add option to propagate scheduling annotation through the data/control dependency chain between pairs of annotated instructions with the same ID.,Add option to propagate scheduling annotation through the data/control dependency chain between pairs of annotated instructions with the same ID.,2025-03-18T21:04:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89490
yi,copybara-service[bot],[XLA] Adds a helper function for copying original value,[XLA] Adds a helper function for copying original value,2025-03-18T21:01:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89489
tpu,copybara-service[bot],[XLA] Remove `--log_output` and use ARM64 T2A machines to run CPU workflows,[XLA] Remove `log_output` and use ARM64 T2A machines to run CPU workflows,2025-03-18T19:42:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89481
opt,AYUHSPATIL,Fix doc bug in tf.keras.losses.SparseCategoricalCrossentropy,"Fixes CC(tf.keras.losses.SparseCategoricalCrossentropy has logical/Doc bug) This PR enhances the error handling for invalid values of the reduction parameter. Previously, if reduction was None or an incorrect value (including variations like ""None"", ""Auto"", ""Sum"", etc.), the error message was not specific enough, making it difficult for users to understand what went wrong. If the value does not match exactly (casesensitive), a clear error message is raised, specifying the invalid value and listing the expected options. This improves usability by guiding the user toward the correct values. Impact: 1. Improves error clarity for developers using the reduction parameter. 2. Ensures better debugging by explicitly pointing out incorrect values. 3. Reduces confusion caused by incorrect casing or unexpected inputs.  For more details, open the Copilot Workspace session.",2025-03-18T18:47:30Z,size:XL invalid python,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89480,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , can you please sign the CLA? Many thanks!",This seems like AI generated content which goes against contributors guidelines and could be considered spam.
opt,copybara-service[bot],Removed optimized batch_matmul to redirect to XNNPACK.,Removed optimized batch_matmul to redirect to XNNPACK.,2025-03-18T16:32:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89472
tpu,jdub4asdfg,tf.math.argmin() returning the wrong index," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version I'm doing it google Colab  Custom code Yes  OS platform and distribution Windows x64  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm performing argmin on a tensor but it returns the index of a value that is larger than the minimum. argmax works fine though. Pardon my lack of terminology, I just started learning Tensorflow 2 days ago. !Image  Standalone code to reproduce the issue ```shell import tensorflow as tf random_tensor = tf.constant([[10.39, 12.92], [7.6, 5.53]]) print(random_tensor) print(tf.argmin(random_tensor)) ```  Relevant log output ```shell ```",2025-03-18T15:24:33Z,type:bug,closed,0,2,https://github.com/tensorflow/tensorflow/issues/89467,Are you satisfied with the resolution of your issue? Yes No,Sorry I misunderstood the use of argmin
opt,copybara-service[bot],PR #23786: Multihost HLO runner: fix --while_execution_count behavior.,"PR CC(Update graph_transformations.h): Multihost HLO runner: fix while_execution_count behavior. Imported from GitHub PR https://github.com/openxla/xla/pull/23786 Optimized HLO can have known_trip_count while loop annotation, which has to be updated using WhileLoopTripCountAnnotator after HloControlFlowFlattening to apply the value of while_execution_count at runtime. Copybara import of the project:  5648a58d78726f116a64030f48dd63835818fbf7 by Ilia Sergachev : [NFC] Factor common test logic out into a function.  21d4e89aab6fc1dfe66edde6d11eabaec5ea6a84 by Ilia Sergachev : Multihost HLO runner: fix while_execution_count behavior. Optimized HLO can have known_trip_count while loop annotation, which has to be updated using WhileLoopTripCountAnnotator after HloControlFlowFlattening to apply the value of while_execution_count at runtime.  fd93a631e6b818df7d98f29a55b53a8819eae9a3 by Ilia Sergachev : Address review request. Merging this change closes CC(Update graph_transformations.h) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23786 from openxla:fix_while_execution_count fd93a631e6b818df7d98f29a55b53a8819eae9a3",2025-03-18T15:07:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89465
tpu,AdithyaSrivastava01,calling exp_on_negative_values() once,"As a reference to ISSUE CC(Fixedpoint Softmax() calls exp_on_negative_values() twice), the essential changes are made to call exp_on_negative_values() only once. The exp is stored in the output_data(temp buffer), and later, the scaling operation is performed on the precomputed exponents. Fixes CC(Fixedpoint Softmax() calls exp_on_negative_values() twice)",2025-03-18T08:42:37Z,stat:awaiting response stale ready to pull size:S comp:lite-kernels,closed,1,7,https://github.com/tensorflow/tensorflow/issues/89441,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",Hey   could you let me know whether the PR is ready to merge or should I make some changes?,"So, the process to get a PR merged is not as simple as for other projects, due to the fact that the internal version of TF also needs to work with other internal projects, etc. See the process as documented here. I cannot check right now, but looking at the statuses here it is likely that the change causes a test or more to fail. Will come back with details when I can get to them",Greetings  is it possible for you to share the potential issues that the code is facing?,"There seems to be numerical errors in the tests. Can you run the tests locally? At least `tensorflow/lite/kernels/activations_test.cc` ``` Value of: m.GetDequantizedOutput() Expected: has 8 elements where element CC(未找到相关数据) float abs rel near (value: 0.09766, max_abs_err: 0.0078125, max_rel_err: 0), element CC(Add support for Python 3.x) float abs rel near (value: 0.05469, max_abs_err: 0.0078125, max_rel_err: 0), element CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"") float abs rel near (value: 0.12109, max_abs_err: 0.0078125, max_rel_err: 0), element CC(JVM, .NET Language Support) float abs rel near (value: 0.14453, max_abs_err: 0.0078125, max_rel_err: 0), element CC(Installation over pip fails to import with protobuf 2.6.1) float abs rel near (value: 0.13281, max_abs_err: 0.0078125, max_rel_err: 0), element CC(Java interface) float abs rel near (value: 0.07813, max_abs_err: 0.0078125, max_rel_err: 0), element CC(Pretrained models) float abs rel near (value: 0.26563, max_abs_err: 0.0078125, max_rel_err: 0), element CC(API docs does not list RNNs) float abs rel near (value: 0.10938, max_abs_err: 0.0078125, max_rel_err: 0)   Actual: { 0, 0, 0, 0, 0, 0, 0, 0 }, whose element CC(未找到相关数据) (0) not (float abs rel near (value: 0.09766, max_abs_err: 0.0078125, max_rel_err: 0)), which is 0.09766 from 0.09766 ```",This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,This PR was closed because it has been inactive for 14 days since being marked as stale. Please reopen if you'd like to work on this further.
tpu,vemqos99,FP32 gives good results but int8 predicts only single class," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): 2.15.0  2. Code Provide code to help us reproduce your issues using one of the following options: Standard code to convert from fp32 int8 taken directly from website. The representative dataset is the validation set The model used is Nanodet  for 2 classes. used with images of size 160x160. Each class has atleat 2000 images. Total training set size is 5500. validation set size is 1800. nanodet model converted to onnx then converted to tflite. All fp32 models give the same results as below. int8 model results is skewed. nanodet_detection_results_fp32.txt nanodet_detection_results_int8.txt As you can see from the results, int8 model predicts only single class. In the detection results, 130 is class0, 3160 is class1, rest should not detect and class with high confidence. This is the .py code for int8. for fp32 the items related to quantization is removed import cv2 import numpy as np import tensorflow as tf import math  Thresholds CONF_THRESH = 0.5   Confidence threshold IOU_THRESH = 0.1    IoU threshold for NMS REG_MAX = 7  Define colors for visualization _COLORS = np.array([     [0.000, 0.447, 0.741],   Class 0 (blue)     [0.850, 0.325, 0.098],   Class 1 (orange) ])  Load INT8 TFLite model def load_tflite_model(model_path):     interpreter = tf.lite.Interpreter(model_path=model_path)     interpreter.allocate_tensors()     return interpreter  Preprocess image for INT8 model def preprocess_image(image_path, input_details):     img = cv2.imread(image_path, cv2.IMREAD_COLOR)     orig_shape = img.shape[:2]     img = cv2.resize(img, (160, 160))      Normalize to INT8 scale     input_dtype = input_details[0]['dtype']     input_scale, input_zero_point = input_details[0]['quantization']     img = img.astype(np.float32) / 255.0   Normalize to [0,1]     img = (img / input_scale + input_zero_point).astype(input_dtype)   Quantize     img = np.expand_dims(img, axis=0)   NHWC format     return img, orig_shape  Generate grid center priors def generate_grid_center_priors(input_height, input_width, strides):     center_priors = []     for stride in strides:         feat_w = math.ceil(input_width / stride)         feat_h = math.ceil(input_height / stride)         for y in range(feat_h):             for x in range(feat_w):                 center_priors.append([x, y, stride])     return center_priors  Softmax activation function def activation_function_softmax(src):     alpha = max(src)     denominator = sum(np.exp(src  alpha))     return np.exp(src  alpha) / denominator  Decode distributional predictions to bounding boxes def disPred2Bbox(dfl_det, label, score, x, y, stride):     ct_x = x * stride     ct_y = y * stride     dis_pred = []     for i in range(4):         dis = sum(j * activation_function_softmax(dfl_det[i * (REG_MAX + 1):(i + 1) * (REG_MAX + 1)])[j] for j in range(REG_MAX + 1))         dis *= stride         dis_pred.append(dis)     xmin, ymin = max(ct_x  dis_pred[0], 0), max(ct_y  dis_pred[1], 0)     xmax, ymax = min(ct_x + dis_pred[2], 160), min(ct_y + dis_pred[3], 160)     return [xmin, ymin, xmax, ymax, score, label]  Decode NanoDet output (with INT8 dequantization) def decode_infer(pred, center_priors, score_threshold, output_details, num_classes=2):     output_scale, output_zero_point = output_details[0]['quantization']     pred = (pred.astype(np.float32)  output_zero_point) * output_scale   Dequantize     pred = pred.flatten()     results = {i: [] for i in range(num_classes)}     num_channels = num_classes + (REG_MAX + 1) * 4     for idx, center_prior in enumerate(center_priors):         ct_x, ct_y, stride = center_prior         scores = pred[idx * num_channels:idx * num_channels + num_classes]         cur_label = np.argmax(scores)         score = scores[cur_label]         if score > score_threshold:             bbox_pred = pred[idx * num_channels + num_classes:]             results[cur_label].append(disPred2Bbox(bbox_pred, cur_label, score, ct_x, ct_y, stride))     return results  NonMaximum Suppression def nms(dets, thresh):     if len(dets) == 0:         return []     x1, y1, x2, y2, scores = dets[:, 0], dets[:, 1], dets[:, 2], dets[:, 3], dets[:, 4]     areas = (x2  x1 + 1) * (y2  y1 + 1)     order = scores.argsort()[::1]     keep = []     while order.size > 0:         i = order[0]         keep.append(i)         xx1 = np.maximum(x1[i], x1[order[1:]])         yy1 = np.maximum(y1[i], y1[order[1:]])         xx2 = np.minimum(x2[i], x2[order[1:]])         yy2 = np.minimum(y2[i], y2[order[1:]])         w = np.maximum(0.0, xx2  xx1 + 1)         h = np.maximum(0.0, yy2  yy1 + 1)         inter = w * h         ovr = inter / (areas[i] + areas[order[1:]]  inter)         order = order[np.where(ovr <= thresh)[0] + 1]     return keep  Run inference and visualize results def run_inference(model_path, image_path):     interpreter = load_tflite_model(model_path)     input_details, output_details = interpreter.get_input_details(), interpreter.get_output_details()     image, orig_shape = preprocess_image(image_path, input_details)     interpreter.set_tensor(input_details[0]['index'], image)     interpreter.invoke()     dets = interpreter.get_tensor(output_details[0]['index'])     center_priors = generate_grid_center_priors(160, 160, [8, 16, 32])     preds = decode_infer(dets, center_priors, CONF_THRESH, output_details)     results = []     for key, pred in preds.items():         idxs = nms(np.array(pred), IOU_THRESH)         results.extend(pred[idx] for idx in idxs)      Map bbox to original image size     src_w, src_h = orig_shape[1], orig_shape[0]     img = cv2.imread(image_path)     for result in results:         x1, y1, x2, y2, score, label = result         x1, y1, x2, y2 = int(x1 * src_w / 160), int(y1 * src_h / 160), int(x2 * src_w / 160), int(y2 * src_h / 160)         color = (_COLORS[label] * 255).astype(np.uint8).tolist()         cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)         cv2.putText(img, f""Class {label}: {score:.2f}"", (x1, y1  10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)     cv2.imwrite(""output.jpg"", img)     return results if __name__ == ""__main__"":     model_file = ""nanodet_int8.tflite""     image_file = ""D:\\regressionTest\\data\\carLP\\IMG_008.jpg""     results = run_inference(model_file, image_file)     print(results)",2025-03-18T08:37:59Z,stat:awaiting response stale comp:lite TFLiteConverter TF 2.15,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89438,"Hi,   I apologize for the delay in my response, if possible could you please help us with your Google colab notebook along with models or your Github repo along with models to reproduce the same behavior from our end ? Meanwhile as far I know there might be some root causes like `INT8` quantization compresses the dynamic range of activations and weights. If the representative dataset is not truly representative or if the quantization parameters (scale/zero point) are not set properly one class can be squeezed out and its logits suppressed causing all outputs to favor a single class.If the output tensor is not dequantized correctly (i.e. using the wrong scale/zero point) class scores can be skewed leading to incorrect argmax results `INT8` quantization may change the distribution of logits or scores especially if softmax is applied after dequantization. This can cause one class to always dominate or confidence scores to be outside the expected range, breaking NMS or thresholding logic. If the representative dataset used for calibration is not balanced across classes quantization may favor the more frequent class causing the other class to be invisible to the quantized model I would recommend you to please make sure output values before and after dequantization and ensure that after dequantization the class logits are in a reasonable range (not all negative, not all the same) also compare raw logits, softmax outputs and final predictions between `FP32` and `INT8` models for the same input. consistently higher `INT8` values for a single class suggest a quantization scale problem. Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
yi,copybara-service[bot],Unify implicit dependencies of OSS and internal xla_test macro,Unify implicit dependencies of OSS and internal xla_test macro Both macro implementations used to add different implicit runtime dependency targets. This change is unifying that.,2025-03-18T08:08:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89436
tpu,copybara-service[bot],fix GPU model execution failures:,"fix GPU model execution failures: 1. MLD failure on cl tensor with layout == BHWC && batch ==1, it need to be HWC layout instead. 2. MLD delegate failure when the graph is partially delegated, the output tensor can not be found in the buffer context  3. mem leak on the GPU global environment creation.",2025-03-18T05:30:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89420
opt,copybara-service[bot],[XLA:GPU/TMA] Migrating legacy fusion emitter to use TritonXLA dialect for Load/Store instead of Triton's equivalents. This is important to allow for TMA optionality later on.,[XLA:GPU/TMA] Migrating legacy fusion emitter to use TritonXLA dialect for Load/Store instead of Triton's equivalents. This is important to allow for TMA optionality later on.,2025-03-17T23:43:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89408
opt,copybara-service[bot],Add a test for the optimization-barrier expander pass.,Add a test for the optimizationbarrier expander pass.,2025-03-17T20:44:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89396
opt,copybara-service[bot],Clean up BUILD rules for `generate_hlo_opt_test_checks.py`.,"Clean up BUILD rules for `generate_hlo_opt_test_checks.py`. Suffix the `py_strict_library` target's name with `_lib`, and add a `py_strict_binary` target with the old name.",2025-03-17T19:50:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89392
sharding,copybara-service[bot],Support ragged_dot in Shardy.,"Support ragged_dot in Shardy. 1. Allow_xla_features (mhlo.ragged_dot) in MhloToStablehlo translation. 2. Add mhlo_extensions for ShardyXLA, and define the sharding rule for ragged_dot operations.",2025-03-17T18:43:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89388
tpu,copybara-service[bot],ml_drift_cl_litert: Implement async execution mode,"ml_drift_cl_litert: Implement async execution mode In async mode, call HandleOutputEvents() after Dispatch() to create and bind the output event. Once all the enqueued commands are completed, the output event will be signaled by OpenCL framework.",2025-03-17T18:22:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89383
opt,copybara-service[bot],[XLA] Adjust the `CallInliner` pass to allow uniquifying channel ids after inlining.,"[XLA] Adjust the `CallInliner` pass to allow uniquifying channel ids after inlining. This is an option that can be enabled specifically on the pass, and will now enabled in the XLA:GPU compilation pipeline if `xla_ignore_channel_id` is `true`. This works around an issue whereby the `CallInliner` might inline several computations involving collectives with the same `channel_id` into the same computation. This is actually not wellbehaved and can result in the creation of cyclic HLO graphs. This change ensures that inlining does not create cyclic graphs. Note that the transformation is actually not semanticspreserving, in that a computation involving several `call` instructions wrapping collectives that use the same `channel_id` is already illformed. Nevertheless, `channel_id`s are only used as a ""hack"" for pipeline parallelism in XLA:GPU currently, and are mostly vestigial at this point. The flagdriven enablement can be deleted once this feature is deprecated (WIP). The proper solution would be to completely get rid of them, but they are currently enforced by the HLO verifier and removing them will take time.",2025-03-17T18:14:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89382
tpu,copybara-service[bot],[XLA] Fix a bug where the aliasing logic in graph creation could create graphs that can be stuck because of overlap limit.,[XLA] Fix a bug where the aliasing logic in graph creation could create graphs that can be stuck because of overlap limit. Send instructions have as input a token and this token is piped to the output. From a buffer assignment point of view the input/output tokens alias which is kind of unique across async instructions. If the fix aliasing logic triggers on multiple of these sends where the token aliases the logic adds edges between sendstarts and some senddones. These new edges between sendstarts and dones can create situations where to schedule some starts we need to schedule dones causing us to bust overlap limit.,2025-03-17T18:08:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89381
tpu,copybara-service[bot],Add an API to bind event without owning it,"Add an API to bind event without owning it  SetEvent(Event&& event) always takes ownership  SetEvent(LiteRtEvent& litert_event, bool transfer_ownership) depends on   the parameter. GPU Accelerator creates an output LiteRtEvent and owns it without transferring the ownership.",2025-03-17T18:02:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89380
tpu,marenive,'LNK1120: 12 unresolved externals' when building and using TFLite C library with CMake on Windows," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes (using current master)  Source source  TensorFlow version tf2.19.0  Custom code Yes  OS platform and distribution Windows 10  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I want to build the tflite c library on windows. Therefore I looked at the guide https://ai.google.dev/edge/litert/build/cmakebuild_litert_c_library and did the following steps: ```shell git clone https://github.com/tensorflow/tensorflow.git tensorflow_src cd tensorflow_src git checkout v2.19.0 cd .. mkdir tflite_build cd tflite_build cmake ../tensorflow_src/tensorflow/lite/c cmake build . j ``` As output I got as expected the `tensorflowlite_c.dll` and `tensorflowlite_c.lib` files. I created a sample project for testing the library, where the code is mentioned below.  I was able to build the library on Ubuntu 22.04. and run the example described below.  CMake Version: `3.25.1` Windows SDK version `10.0.22621.0`  Standalone code to reproduce the issue ```cpp include  include  define TFLITE_MINIMAL_CHECK(x)                              \   if (!(x)) {                                                \     fprintf(stderr, ""Error at %s:%d\n"", __FILE__, __LINE__); \     exit(1);                                                 \   } int main(int argc, char* argv[]) {   fprintf(stdout, ""Hello TensorFlow Lite!\n"");   TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();   TFLITE_MINIMAL_CHECK(options != nullptr);   TfLiteModel* model = TfLiteModelCreateFromFile(""C:/Users/user1/models/docker/testingeq.tflite"");   TFLITE_MINIMAL_CHECK(model != nullptr);   TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);   TFLITE_MINIMAL_CHECK(interpreter != nullptr);   return 0; } ``` ```cmake cmake_minimum_required(VERSION 3.16) project(minimal) set(CMAKE_CXX_STANDARD 17) set(CMAKE_INSTALL_RPATH ""$ORIGIN"") set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE) set(CMAKE_EXPORT_COMPILE_COMMANDS ON CACHE INTERNAL """") get_filename_component(TensorflowLite_DIR ""${CMAKE_SOURCE_DIR}/deps/tensorflow_src/"" ABSOLUTE) set(TensorflowLite_INCLUDE_DIR ""${TensorflowLite_DIR}"") set(TensorflowLite_LIBRARIES ""${TensorflowLite_DIR}/bin/"") if(MSVC)     set(TENSORFLOWLITE_C ""${TensorflowLite_LIBRARIES}/tensorflowlite_c.lib"")     set(TENSORFLOWLITE_SHARED ""${TensorflowLite_LIBRARIES}/tensorflowlite_c.dll"") elseif(${TARGET_AARCH64})     set(TENSORFLOWLITE_C ""${TensorflowLite_LIBRARIES_AARCH64}/libtensorflowlite_c.so"")     set(TensorflowLite_SHARED ""${TensorflowLite_LIBRARIES_AARCH64}/libtensorflowlite_c.so"") else()     set(TENSORFLOWLITE_C ""${TensorflowLite_LIBRARIES}/libtensorflowlite_c.so"")     set(TENSORFLOWLITE_SHARED ""${TensorflowLite_LIBRARIES}/libtensorflowlite_c.so"") endif() include_directories(${TensorflowLite_INCLUDE_DIR}) add_executable(minimal minimal.cc) target_link_libraries(minimal ${TENSORFLOWLITE_C}) ```  Relevant log output ```shell [main] Building folder: c:/Users/user1/dev/tf_example/build ALL_BUILD [build] Starting build [proc] Executing command: ""C:\Program Files\CMake\bin\cmake.EXE"" build c:/Users/user1/dev/tf_example/build config Debug target ALL_BUILD j 14  [build] MSBuild version 17.13.9+e0f243f1e for .NET Framework [build]  [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorCreate referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalCreate''(void)"" (??__ETfLiteRegistrationExternalCreate@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorDelete referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalDelete''(void)"" (??__ETfLiteRegistrationExternalDelete@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorGetBuiltInCode referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalGetBuiltInCode''(void)"" (??__ETfLiteRegistrationExternalGetBuiltInCode@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorGetCustomName referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalGetCustomName''(void)"" (??__ETfLiteRegistrationExternalGetCustomName@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorGetVersion referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalGetVersion''(void)"" (??__ETfLiteRegistrationExternalGetVersion@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorSetInit referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalSetInit''(void)"" (??__ETfLiteRegistrationExternalSetInit@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorSetFree referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalSetFree''(void)"" (??__ETfLiteRegistrationExternalSetFree@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorSetPrepare referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalSetPrepare''(void)"" (??__ETfLiteRegistrationExternalSetPrepare@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorSetInvoke referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalSetInvoke''(void)"" (??__ETfLiteRegistrationExternalSetInvoke@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteModelCreateFromFile referenced in function main [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteInterpreterOptionsCreate referenced in function main [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteInterpreterCreate referenced in function main [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] C:\Users\user1\dev\tf_example\build\Debug\minimal.exe : fatal error LNK1120: 12 unresolved externals [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [proc] The command: ""C:\Program Files\CMake\bin\cmake.EXE"" build c:/Users/user1/dev/tf_example/build config Debug target ALL_BUILD j 14  exited with code: 1 [driver] Build completed: 00:00:02.951 [build] Build finished with exit code 1 ```",2025-03-17T16:11:11Z,type:build/install comp:lite TF 2.18,open,0,7,https://github.com/tensorflow/tensorflow/issues/89374,I have the very same issue and could not find any fix. I am very interrested in a solution too!,"Hi,  and   I apologize for the delay in my response, if possible could you please refer these Youtube videos [Ref1], [Ref2] which may help you to solve your issue ? Also please refer to TensorFlow Lite C++ minimal example and CMakeLists.txt . There is a known issue where the Windows CMake build defaults to debug configuration instead of release to ensure a Release build, explicitly specify the configuration like below : `cmake build . j config Release` If issue still persists please let us know with updated error log to investigate this issue further from our end.  Thank you for your cooperation and patience.","Hi , thanks for your reply. Unfortunately the links you provided are not relevant for this issue. The first video you mentioned doesn't explain how to compile, it just download the lib from a github site. Your other links are related to C++ API, not C API. I tried to compile in Release, but in this case the .lib is not generated (only the .dll), so I can't compile. Thanks anyway","Hi, I found what is the issue: the CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS flag is missing in the CMakeLists.txt To fix it, just add the flag on the command line: cmake .\tensorflow\tensorflow\lite\c DCMAKE_WINDOWS_EXPORT_ALL_SYMBOLS=ON Then it produces both a .lib and a .dll with all required symbols. This should be added by default in the CMakeLists.txt BR,","Hello,   I also figured out that you could use Bazel to build the binaries.   For this, I needed a clean Windows install (without any Conda environments) with Bazelisk and MSVC (or some other compiler) installed.   Then I followed these steps:   1. Create a virtual environment with `python m venv .venv`.   2. Activate the virtual environment: `./.venv/Scripts/activate`.   3. Clone the TensorFlow repository: `git clone https://github.com/tensorflow/tensorflow.git`.   4. Navigate to the TensorFlow directory: `cd tensorflow`.   5. Run the configuration script: `./configure` (Select ""Yes"" for everything except using the Clang compiler).   6. Build the binaries: `bazelisk build config=opt //tensorflow/lite/c:tensorflowlite_c`.   I'll check out your solution too, .   Thank you,  ","Hi, ,   Good to hear that your issue has been resolved by running this command `cmake ..\tensorflow\lite\c DTFLITE_ENABLE_XNNPACK=OFF DCMAKE_WINDOWS_EXPORT_ALL_SYMBOLS=ON` The `CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS` flag was introduced in `CMake 3.4` to simplify porting C and C++ libraries from Linux/UNIX to Windows here is reference. When set to `ON`, this flag instructs CMake to:  Automatically create a `.def` (module definition) file with all symbols found in the input `.obj `files for SHARED libraries on Windows  Pass this `.def` file to the linker during the build process  Ensure all symbols are exported from the resulting DLL without requiring manual decoration with `__declspec(dllexport)` Thank you for your cooperation.","Hi , , I compiled it like   suggested like this: ```shell cmake ..\tensorflow_src\tensorflow\lite\c DCMAKE_WINDOWS_EXPORT_ALL_SYMBOLS=ON DTFLITE_ENABLE_XNNPACK=ON DCMAKE_BUILD_TYPE=Release ``` I set `XNNPACK` to `ON` which is the default anyway, but when I inspect and use the .dll the TfLiteXNNPackDelegate is missing. Dumpbin outbut ```shell . . . 414  19D 0000C8A0 ?what@         415  19E 0000C320 ?window_dilations@?$Vector@         416  19F 0000C550 ?window_reversal@?$Vector@         417  1A0 0000C4F0 ?window_strides@?$Vector@         418  1A1 0000C320 ?zero_point@?$Vector@         419  1A2 0000EE00 TfLiteDelegateCreate         420  1A3 0000C8C0 TfLiteExtensionApisVersion         421  1A4 0000EE20 TfLiteFloatArrayCopy         422  1A5 0000EEB0 TfLiteFloatArrayCreate         423  1A6 0000EEF0 TfLiteFloatArrayFree         424  1A7 0000EF00 TfLiteFloatArrayGetSizeInBytes         425  1A8 0000EE20 TfLiteIntArrayCopy         426  1A9 0000EEB0 TfLiteIntArrayCreate         427  1AA 0000EF10 TfLiteIntArrayEqual         428  1AB 0000EF60 TfLiteIntArrayEqualsArray         429  1AC 0000EEF0 TfLiteIntArrayFree         430  1AD 0000EFB0 TfLiteIntArrayGetSizeInBytes         431  1AE 0000C8D0 TfLiteInterpreterAllocateTensors         432  1AF 0000C8E0 TfLiteInterpreterCancel         433  1B0 0000C8F0 TfLiteInterpreterCreate         434  1B1 0000E210 TfLiteInterpreterCreateWithSelectedOps         435  1B2 0000C950 TfLiteInterpreterDelete         436  1B3 0000E260 TfLiteInterpreterEnsureTensorDataIsReadable         437  1B4 0000E270 TfLiteInterpreterGetBufferHandle         438  1B5 0000C9F0 TfLiteInterpreterGetInputTensor         439  1B6 0000CA30 TfLiteInterpreterGetInputTensorCount         440  1B7 0000E280 TfLiteInterpreterGetInputTensorIndex         441  1B8 0000CA50 TfLiteInterpreterGetOutputTensor         442  1B9 0000CA90 TfLiteInterpreterGetOutputTensorCount         443  1BA 0000E2A0 TfLiteInterpreterGetOutputTensorIndex         444  1BB 0000CAB0 TfLiteInterpreterGetSignatureCount         445  1BC 0000CB20 TfLiteInterpreterGetSignatureKey         446  1BD 0000CC20 TfLiteInterpreterGetSignatureRunner         447  1BE 0000CC60 TfLiteInterpreterGetTensor         448  1BF 0000E2C0 TfLiteInterpreterGetVariableTensor         449  1C0 0000E300 TfLiteInterpreterGetVariableTensorCount         450  1C1 0000CC90 TfLiteInterpreterInputTensorIndices         451  1C2 0000CCB0 TfLiteInterpreterInvoke         452  1C3 0000E320 TfLiteInterpreterModifyGraphWithDelegate         453  1C4 0000E330 TfLiteInterpreterOptionsAddBuiltinOp         454  1C5 0000E340 TfLiteInterpreterOptionsAddCustomOp         455  1C6 0000CCD0 TfLiteInterpreterOptionsAddDelegate         456  1C7 0000CD20 TfLiteInterpreterOptionsAddOperator         457  1C8 0000CD70 TfLiteInterpreterOptionsCopy         458  1C9 0000CF30 TfLiteInterpreterOptionsCreate         459  1CA 0000D040 TfLiteInterpreterOptionsDelete         460  1CB 0000D140 TfLiteInterpreterOptionsEnableCancellation         461  1CC 0000E350 TfLiteInterpreterOptionsSetEnableDelegateFallback         462  1CD 0000D150 TfLiteInterpreterOptionsSetErrorReporter         463  1CE 0000D160 TfLiteInterpreterOptionsSetNumThreads         464  1CF 0000E360 TfLiteInterpreterOptionsSetOpResolver         465  1D0 0000E4F0 TfLiteInterpreterOptionsSetOpResolverExternal         466  1D1 0000E680 TfLiteInterpreterOptionsSetOpResolverExternalWithFallback         467  1D2 0000E8E0 TfLiteInterpreterOptionsSetOpResolverV1         468  1D3 0000EA70 TfLiteInterpreterOptionsSetOpResolverV2         469  1D4 0000EC00 TfLiteInterpreterOptionsSetOpResolverV3         470  1D5 0000ED90 TfLiteInterpreterOptionsSetTelemetryProfiler         471  1D6 0000EDA0 TfLiteInterpreterOptionsSetUseNNAPI         472  1D7 0000D170 TfLiteInterpreterOutputTensorIndices         473  1D8 0000EDB0 TfLiteInterpreterResetVariableTensors         474  1D9 0000D190 TfLiteInterpreterResizeInputTensor         475  1DA 0000EDC0 TfLiteInterpreterSetBufferHandle         476  1DB 0000EDD0 TfLiteInterpreterSetCustomAllocationForTensor         477  1DC 0000D2E0 TfLiteModelCreate         478  1DD 0000D420 TfLiteModelCreateFromFile         479  1DE 0000D540 TfLiteModelCreateFromFileWithErrorReporter         480  1DF 0000D6A0 TfLiteModelCreateWithErrorReporter         481  1E0 0000D810 TfLiteModelDelete         482  1E1 0000F900 TfLiteOperatorCreate         483  1E2 0000F990 TfLiteOperatorDelete         484  1E3 0000F9A0 TfLiteOperatorGetBuiltInCode         485  1E4 000010F0 TfLiteOperatorGetCustomName         486  1E5 0000F9B0 TfLiteOperatorGetUserData         487  1E6 0000F9C0 TfLiteOperatorGetVersion         488  1E7 0000F9D0 TfLiteOperatorSetAsyncKernel         489  1E8 0000F9E0 TfLiteOperatorSetAsyncKernelWithData         490  1E9 0000F9F0 TfLiteOperatorSetFree         491  1EA 0000FA00 TfLiteOperatorSetFreeWithData         492  1EB 0000FA10 TfLiteOperatorSetInit         493  1EC 0000FA20 TfLiteOperatorSetInitWithData         494  1ED 0000FA30 TfLiteOperatorSetInplaceOperator         495  1EE 0000FA40 TfLiteOperatorSetInvoke         496  1EF 0000FA50 TfLiteOperatorSetInvokeWithData         497  1F0 0000FA60 TfLiteOperatorSetPrepare         498  1F1 0000FA70 TfLiteOperatorSetPrepareWithData         499  1F2 0000EFC0 TfLiteQuantizationFree         500  1F3 0000D880 TfLiteSchemaVersion         501  1F4 0000EDF0 TfLiteSetAllowBufferHandleOutput         502  1F5 0000D890 TfLiteSignatureRunnerAllocateTensors         503  1F6 0000D8A0 TfLiteSignatureRunnerCancel         504  1F7 0000D8B0 TfLiteSignatureRunnerDelete         505  1F8 0000D8C0 TfLiteSignatureRunnerGetInputCount         506  1F9 0000D8E0 TfLiteSignatureRunnerGetInputName         507  1FA 0000D910 TfLiteSignatureRunnerGetInputTensor         508  1FB 0000D920 TfLiteSignatureRunnerGetOutputCount         509  1FC 0000D940 TfLiteSignatureRunnerGetOutputName         510  1FD 0000D970 TfLiteSignatureRunnerGetOutputTensor         511  1FE 0000D980 TfLiteSignatureRunnerInvoke         512  1FF 0000D990 TfLiteSignatureRunnerResizeInputTensor         513  200 0000F040 TfLiteSparsityFree         514  201 0000DAD0 TfLiteTensorByteSize         515  202 0000F110 TfLiteTensorCopy         516  203 0000DAE0 TfLiteTensorCopyFromBuffer         517  204 0000DB10 TfLiteTensorCopyToBuffer         518  205 0000DB40 TfLiteTensorData         519  206 0000F240 TfLiteTensorDataFree         520  207 0000DB50 TfLiteTensorDim         521  208 0000F2A0 TfLiteTensorFree         522  209 0000F430 TfLiteTensorGetAllocationStrategy         523  20A 0000F490 TfLiteTensorGetBufferAddressStability         524  20B 0000F4F0 TfLiteTensorGetDataKnownStep         525  20C 0000F550 TfLiteTensorGetDataStability         526  20D 0000F5A0 TfLiteTensorGetShapeKnownStep         527  20E 0000DB60 TfLiteTensorName         528  20F 0000DB70 TfLiteTensorNumDims         529  210 0000DB90 TfLiteTensorQuantizationParams         530  211 0000F600 TfLiteTensorRealloc         531  212 0000F690 TfLiteTensorReset         532  213 0000F730 TfLiteTensorResizeMaybeCopy         533  214 000010E0 TfLiteTensorType         534  215 0000F7E0 TfLiteTypeGetName         535  216 0000C8C0 TfLiteVersion ```"
opt,copybara-service[bot],Align accelerator application during compiled model creation to the expected behaviour.,"Align accelerator application during compiled model creation to the expected behaviour. This adds a new function field in the accelerator implementation structure and associated functions to set/call that function. The function queries the accelerator to know if the underlying TFLite delegate does JIT compilation. When that's the case, even if the accelerator is registered, it won't be used to create and apply a delegate unless explicitly requested through the compilation options.",2025-03-17T12:31:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89364
opt,copybara-service[bot],[XLA:CPU] Allow passing nullptr run options to Eigen dot,[XLA:CPU] Allow passing nullptr run options to Eigen dot,2025-03-17T11:54:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89363
yi,copybara-service[bot],Use `UniquifyName(HloModule*)` and `UniquifyId(HloModule*)` whenever possible.,Use `UniquifyName(HloModule*)` and `UniquifyId(HloModule*)` whenever possible.,2025-03-17T10:53:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89360
opt,copybara-service[bot],[XLA] fix use after free in HloPassPipeline,[XLA] fix use after free in HloPassPipeline There is no guarantee that module config is will not be updated by passes so any access of debug_options will be use after free.,2025-03-17T10:20:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89354
tpu,woegerbauerJ,Conv1D Layer with kernel size 1 does not match with iteratively processing," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.14.1  Custom code Yes  OS platform and distribution Linux Ubuntu  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I was trying to set up a realtime/causal Conv2D layers, by multiple Conv1D layers. I noticed an error in the order of 1e7, so i started to narrow down the issue. This is why i ended up with the following code. I essentially create 2 identical conv1D layers where i then load the weights from one into the other. When iterating over the time dimension (axis 1; as one would to in an realtime/causal implementation) this error occurs. Note:  this issue only happens for ""large"" filters (> 10)  this issue only happens for ""large"" features (> 30)  when setting the weights to integer this issue is not reproducible  Standalone code to reproduce the issue ```shell import numpy as np import tensorflow as tf class Conv1D_wrapper(tf.keras.layers.Layer):     def __init__(self, filters):         super(Conv1D_wrapper, self).__init__()         self.conv1d = tf.keras.layers.Conv1D(             filters = filters,             kernel_size = 1,             padding='valid',             activation='relu',             strides = 1,             use_bias = False         )     def load(self, other_layer):         kernel = other_layer.get_weights()         self.conv1d.set_weights([kernel[0]])     def call(self, input):         x = self.conv1d(input)         return x if __name__ == ""__main__"":     dummy data     x1 = np.ones([1,3,1,10])      init the layers     conv1 = Conv1D_wrapper(30)     conv2 = Conv1D_wrapper(30)     conv1(x1.copy())     conv2(x1.copy())      load conv1 into conv2     conv2.load(conv1)      output1 (calculated over all time steps)     output1 = conv1(x1.copy())     output2 = np.zeros_like(output1)     for i in range(output1.shape[1]): calculated iteratively over time steps         output2[:,i,:] = conv2(x1[:,i,:][:,None,:])     print(""not iterated matching"", np.allclose(conv1(x1.copy()), conv2(x1.copy())))     print(""matching kernels?"",np.allclose(conv2.get_weights()[0],conv1.get_weights()[0]))     print(""bias?"",conv2.conv1d.use_bias, conv1.conv1d.use_bias)     print(""max error"", np.max(abs(output1output2))) ```  Relevant log output ```shell not iterated matching True matching kernels? True bias? False False max error 1.1920929e07 ```",2025-03-17T10:20:20Z,stat:awaiting response type:bug comp:ops TF2.14,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89353,"addon: this issue also happens for `    x1 = np.ones([1,11,10,10])      init the layers     conv1 = tf.keras.layers.Conv1D(filters = 30,kernel_size=1, use_bias =False)     output1 = conv1(x1.copy())     output2 = np.zeros_like(output1)     for i in range(output1.shape[1]): calculated iteratively over time steps         output2[:,i,:] = conv1(x1[:,i,:])     print(""max error"", np.max(abs(output1output2)))` and also leads to the same error!","Hi  , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. However, I encountered a different issue. After making some modifications, the code worked for me. Additionally, I noticed that you are using an older version of TensorFlow. I recommend upgrading to the latest version for better results. I have attached a gist for your reference. Thank you!","Hi, thanks for your response! Due to our hardware we somewhat bounded to tf 2.14 (we are working with an NPU).  Thanks for finding out the issue with the dimension. I still assume this is somewhat of an issue, since mathematically it should be identical right? Even for 4 dimensions. However i am happy with the solution. Thank you very much!",Are you satisfied with the resolution of your issue? Yes No
quantization,copybara-service[bot],PR #23759: Enhance Flag Usage Text with Missing Input Formats,PR CC(longer latency after posttraining quantization): Enhance Flag Usage Text with Missing Input Formats Imported from GitHub PR https://github.com/openxla/xla/pull/23759 This PR updates the flag usage text in `multihost_hlo_runner/hlo_runner_main.cc` to include the missing input formats. Flag: input_format Missing input formats:  unoptimized_snapshot_proto_binary  unoptimized_snapshot_proto_text Copybara import of the project:  483b0ef6a484cd94e06dc1f728d3009404797774 by Alexander Pivovarov : Enhance Flag Usage Text with Missing Input Formats Merging this change closes CC(longer latency after posttraining quantization) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23759 from apivovarov:fix_input_format_help 483b0ef6a484cd94e06dc1f728d3009404797774,2025-03-17T10:05:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89351
opt,copybara-service[bot],Create LiteRT benchmark model class,Create LiteRT benchmark model class This benchmark model class is based on the standard TensorFlow benchmark model class. It will compile the model with the given options before running the benchmarks.,2025-03-15T18:37:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89299
tpu,02f01a020,compiling code error," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError: Traceback (most recent call last):   File ""C:\Users\ACER\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\localpackages\Python311\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.  Standalone code to reproduce the issue ```shell from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```  Relevant log output ```shell ImportError: Traceback (most recent call last):   File ""C:\Users\ACER\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\localpackages\Python311\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```",2025-03-15T11:17:14Z,stat:awaiting response type:bug TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89297,please update the MSVC 2019 redistributable，download links: https://download.visualstudio.microsoft.com/download/pr/285b28c73cf947fb9be801cf5323a8df/8F9FB1B3CFE6E5092CF1225ECD6659DAB7CE50B8BF935CB79BFEDE1F3C895240/VC_redist.x64.exe,"Hi  , Apologies for the delay, could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Please search for duplicates before opening a new issue,Are you satisfied with the resolution of your issue? Yes No
tpu,devnas-2004,Failed to load the native TensorFlow runtime.," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tensorflow2.19.0   Custom code Yes  OS platform and distribution windows  Mobile device _No response_  Python version 3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[2], line 5       3 stemmer = LancasterStemmer()       4 import numpy as np > 5 import tflearn       6 import tensorflow as tf       7 import random File ~\anaconda3\Lib\sitepackages\tflearn\__init__.py:4       1 from __future__ import absolute_import       3  Disable TF eager mode > 4 import tensorflow.compat.v1 as tf       5 tf.disable_v2_behavior()       7  Config File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\saxen\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime.  Standalone code to reproduce the issue ```shell import nltk from nltk.stem.lancaster import LancasterStemmer stemmer = LancasterStemmer() import numpy as np import tflearn import tensorflow as tf import random ```  Relevant log output ```shell ```",2025-03-15T10:58:30Z,stat:awaiting response type:build/install subtype:windows TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89296,"Hi 2004 , Apologies for the delay, could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:  You need to install the MSVC 2019 redistributable  Your CPU does not support AVX2 instructions  Your CPU/Python is on 32 bits  There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Please search for duplicates before opening a new issue.,Are you satisfied with the resolution of your issue? Yes No
quantization,copybara-service[bot],Copy quantization_lib/quantization_driver and quantization_utils to Lite,Copy quantization_lib/quantization_driver and quantization_utils to Lite,2025-03-15T07:06:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89278
tpu,maludwig,TensorFlow on RTX 5090," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.20.0.dev20250314  Custom code No  OS platform and distribution Windows 11  WSL2  Ubuntu 22.04.5 LTS  Mobile device _No response_  Python version 3.10.12  Bazel version 7.4.1  GCC/compiler version gcc (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0  CUDA/cuDNN version CUDA Version: 12.8  GPU model and memory RTX 5090 32GB  Current behavior? I had hoped that tensorflow would work on the RTX 5090 at all. It does not, sadly. I tried building from source but that didn't work either. I tried running the environment script but that didn't work either. At least bash is my primary programming language, so I was able to tidy that one up here: https://github.com/tensorflow/tensorflow/pull/89271 But I wasn't able to get tensorflow running. I had a similar issue with PyTorch, which needed to use CUDA 12.8.* to work on the Blackwell cards, but no dice with the nightly build of tensorflow. Below is my test and the output, and under that is the `tf_env.txt` from my patched script. It may be helpful to know that nvidia themselves seem to have it running here: https://docs.nvidia.com/deeplearning/frameworks/tensorflowreleasenotes/rel2502.html But I get the same errors that this other guy does when I try it out: https://www.reddit.com/r/tensorflow/comments/1iutjoj/tensorflow_2501_cuda_128_rtx_5090_on_wsl2_cuda/ This conversation was another one I found that may be helpful, according to these guys, you need to support CUDA 12.8.1 to support Blackwell (aka the RTX 50 series cards): https://discuss.ai.google.dev/t/buildingtensorflowfromsourceforrtx5000gpuseries/65171/15 ``` (tfnightie) mitch:~/stable_diff $ cat tfnightie/test_2.py import tensorflow as tf import time  Check if TensorFlow sees the GPU print(""TensorFlow version:"", tf.__version__) print(""Available GPUs:"", tf.config.experimental.list_physical_devices('GPU'))  Matrix multiplication test shape = (5000, 5000) a = tf.random.normal(shape) b = tf.random.normal(shape)  Time execution on GPU with tf.device('/GPU:0'):     print(""Running on GPU..."")     start_time = time.time()     c = tf.matmul(a, b)     tf.print(""Matrix multiplication (GPU) done."")     print(""Execution time (GPU):"", time.time()  start_time, ""seconds"")  Time execution on CPU for comparison with tf.device('/CPU:0'):     print(""Running on CPU..."")     start_time = time.time()     c = tf.matmul(a, b)     tf.print(""Matrix multiplication (CPU) done."")     print(""Execution time (CPU):"", time.time()  start_time, ""seconds"") (tfnightie) mitch:~/stable_diff $ python tfnightie/test_2.py 20250314 21:35:33.400099: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. TensorFlow version: 2.20.0dev20250314 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1742009735.413544  326199 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jitcompiled from PTX, which could take 30 minutes or longer. Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] W0000 00:00:1742009735.417720  326199 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jitcompiled from PTX, which could take 30 minutes or longer. I0000 00:00:1742009735.572153  326199 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29043 MB memory:  > device: 0, name: NVIDIA GeForce RTX 5090, pci bus id: 0000:09:00.0, compute capability: 12.0 20250314 21:35:36.969440: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_INVALID_PTX' 20250314 21:35:36.969480: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250314 21:35:36.969505: W tensorflow/core/framework/op_kernel.cc:1843] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250314 21:35:36.969533: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' Traceback (most recent call last):   File ""/home/mitch/stable_diff/tfnightie/test_2.py"", line 10, in      a = tf.random.normal(shape)   File ""/home/mitch/.virtualenvs/tfnightie/lib/python3.10/sitepackages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/home/mitch/.virtualenvs/tfnightie/lib/python3.10/sitepackages/tensorflow/python/framework/ops.py"", line 6027, in raise_from_not_ok_status     raise core._status_to_exception(e) from None   pylint: disable=protectedaccess tensorflow.python.framework.errors_impl.InternalError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Mul] name: ``` Also, while nvidia's site says that the Compute Capability of the RTX5090 is ""10.0"", the card itself seems to report ""12.0"". I am not so sure that info will be helpful, but it spun me for a loop: ``` $ cat  card_details.cu > include  include  int main() {     cudaDeviceProp prop;     int device;     cudaGetDevice(&device); // Get the current device ID     cudaGetDeviceProperties(&prop, device); // Get device properties     size_t free_mem, total_mem;     cudaMemGetInfo(&free_mem, &total_mem); // Get VRAM usage     std::cout _75' will be removed in a future release (Use Wnodeprecatedgputargets to suppress warning). GPU Name: NVIDIA GeForce RTX 5090 Compute Capability: 12.0 VRAM Usage: 1763 MB / 32606 MB ```  tf_env.txt ``` == check python ==================================================== python version: 3.10.12 python branch: python build version: ('main', 'Feb  4 2025 14:57:36') python compiler version: GCC 11.4.0 python implementation: CPython == check os platform =============================================== os: Linux os kernel version: CC(Add support for Python 3.x) SMP Tue Nov 5 00:21:55 UTC 2024 os release version: 5.15.167.4microsoftstandardWSL2 os platform: Linux5.15.167.4microsoftstandardWSL2x86_64withglibc2.35 freedesktop os release: {'NAME': 'Ubuntu', 'ID': 'ubuntu', 'PRETTY_NAME': 'Ubuntu 22.04.5 LTS', 'VERSION_ID': '22.04', 'VERSION': '22.04.5 LTS (Jammy Jellyfish)', 'VERSION_CODENAME': 'jammy', 'ID_LIKE': 'debian', 'HOME_URL': 'https://www.ubuntu.com/', 'SUPPORT_URL': 'https://help.ubuntu.com/', 'BUG_REPORT_URL': 'https://bugs.launchpad.net/ubuntu/', 'PRIVACY_POLICY_URL': 'https://www.ubuntu.com/legal/termsandpolicies/privacypolicy', 'UBUNTU_CODENAME': 'jammy'} mac version: ('', ('', '', ''), '') uname: uname_result(system='Linux', node='win11ml', release='5.15.167.4microsoftstandardWSL2', version=' CC(Add support for Python 3.x) SMP Tue Nov 5 00:21:55 UTC 2024', machine='x86_64') architecture: ('64bit', 'ELF') machine: x86_64 == are we in docker ================================================ No == c++ compiler ==================================================== /usr/bin/c++ c++ (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Copyright (C) 2021 Free Software Foundation, Inc. This is free software; see the source for copying conditions.  There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. == check pips ====================================================== numpy                   2.1.3 protobuf                5.29.3 tf_nightly              2.20.0.dev20250314 == check for virtualenv ============================================ Running inside a virtual environment. == tensorflow import =============================================== 20250314 21:02:48.002965: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1742007769.198398  317963 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jitcompiled from PTX, which could take 30 minutes or longer. W0000 00:00:1742007769.202246  317963 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jitcompiled from PTX, which could take 30 minutes or longer. I0000 00:00:1742007769.355021  317963 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29043 MB memory:  > device: 0, name: NVIDIA GeForce RTX 5090, pci bus id: 0000:09:00.0, compute capability: 12.0 tf.version.VERSION = 2.20.0dev20250314 tf.version.GIT_VERSION = v1.12.1123444g07ff428d432 tf.version.COMPILER_VERSION = Ubuntu Clang 18.1.8 (++20240731024944+3b5b5c1ec4a31~exp1~20240731145000.144) Sanity check:  libcudnn not found == env ============================================================= LD_LIBRARY_PATH /usr/local/cuda12.8/lib64: DYLD_LIBRARY_PATH is unset == nvidiasmi ====================================================== Fri Mar 14 21:02:52 2025 ++  ++ == cuda libs ======================================================= /usr/local/cuda11.8/targets/x86_64linux/lib/libcudart_static.a /usr/local/cuda11.8/targets/x86_64linux/lib/libcudart.so.11.8.89 /usr/local/cuda12.8/targets/x86_64linux/lib/libcudart_static.a /usr/local/cuda12.8/targets/x86_64linux/lib/libcudart.so.12.8.90 == tensorflow installation ========================================= tensorflow not found == tf_nightly installation ========================================= Name: tf_nightly Version: 2.20.0.dev20250314 Summary: TensorFlow is an open source machine learning framework for everyone. Homepage: https://www.tensorflow.org/ Authoremail: packages.org License: Apache 2.0 Location: /home/mitch/.virtualenvs/tfnightie/lib/python3.10/sitepackages Requiredby: == python version ================================================== (major, minor, micro, releaselevel, serial) (3, 10, 12, 'final', 0) == bazel version =================================================== Bazelisk version: v1.25.0 Build label: 7.4.1 Build time: Mon Nov 11 21:24:53 2024 (1731360293) Build timestamp: 1731360293 Build timestamp as int: 1731360293 ```  Standalone code to reproduce the issue ```shell Try running anything with an RTX 5090. My test script is above. ```  Relevant log output ```shell ```",2025-03-15T03:40:16Z,stat:awaiting tensorflower type:bug wsl2 TF 2.18,open,2,71,https://github.com/tensorflow/tensorflow/issues/89272,same problem,"I should mention that I'm a Senior AI Developer by trade and I'm more than happy to invest my personal time in helping to fix this, I'm just not sure where to start.","I should also mention that the latest clang release here supports building for compute_100/sm_100+ https://github.com/llvm/llvmproject/releases/tag/llvmorg20.1.0 It's not supported in LLVM 18. But it compiles this on my GPU just fine (extra logs attached just in case they help someone else). ``` mitch:~/stable_diff/build_tf/hello/hello_nvcc $ clang++ version clang version 20.1.0 (https://github.com/llvm/llvmproject 24a30daaa559829ad079f2ff7f73eb4e18095f88) Target: x86_64unknownlinuxgnu Thread model: posix InstalledDir: /home/mitch/stable_diff/fix_tf/llvm/LLVM20.1.0LinuxX64/bin mitch:~/stable_diff/build_tf/hello/hello_nvcc $ cat card_details.cu include  include  int main() {     cudaDeviceProp prop;     int device;     cudaGetDevice(&device); // Get the current device ID     cudaGetDeviceProperties(&prop, device); // Get device properties     size_t free_mem, total_mem;     cudaMemGetInfo(&free_mem, &total_mem); // Get VRAM usage     std::cout << ""GPU Name: "" << prop.name << std::endl;     std::cout << ""Compute Capability: "" << prop.major << ""."" << prop.minor << std::endl;     std::cout << ""VRAM Usage: "" << (total_mem  free_mem) / (1024 * 1024) << "" MB / "" << total_mem / (1024 * 1024) << "" MB"" << std::endl;     return 0; } mitch:~/stable_diff/build_tf/hello/hello_nvcc $ clang++ std=c++17 cudagpuarch=sm_120 x cuda cudapath=""$CUDA_HOME"" I""$CUDA_HOME/include"" L""$CUDA_HOME/lib64""  lcudart card_details.cu o card_details clang++: warning: CUDA version 12.8 is only partially supported [Wunknowncudaversion] mitch:~/stable_diff/build_tf/hello/hello_nvcc $ ./card_details GPU Name: NVIDIA GeForce RTX 5090 Compute Capability: 12.0 VRAM Usage: 1763 MB / 32606 MB mitch:~/stable_diff/build_tf/hello/hello_nvcc $ echo ""$CUDA_HOME"" /usr/local/cuda12.8 mitch:~/stable_diff/build_tf/hello/hello_nvcc $ ls ""$CUDA_HOME"" DOCS  EULA.txt  README  bin  computesanitizer  doc  extras  gds  include  lib64  libnvvp  nsightee_plugins  nvml  nvvm  share  src  targets  tools  version.json mitch:~/stable_diff/build_tf/hello/hello_nvcc $ cat /usr/local/cuda12.8/version.json | head n5 {    ""cuda"" : {       ""name"" : ""CUDA SDK"",       ""version"" : ""12.8.1""    }, ```","I'm going to keep writing my attempts to get things working here. I've cut a branch on my fork, still no luck, but here's some halfdiscoveries. More and more of the project is building as I continue, zero idea how far away I am from victory. He's the branch I'm on, compared with the base: https://github.com/maludwig/tensorflow/compare/ml/fixing_tf_env...maludwig:tensorflow:ml/attempting_build_rtx5090?expand=1 A few findings:  CUDA 12.8.1 adds support for the RTX 5090 (and other Blackwells), so we need that  There's a bug in cutlass, which was forked for tensorflow for a reason I don't know, the bug was fixed here: https://github.com/NVIDIA/cutlass/pull/1784/files  The old fork, done by  was certainly done for a reason, no idea what I'm breaking by going back to the NVIDIA main branch here. Not sure how to message people on GitHub, but maybe they'll get notified on this?  I updated NCCL to the latest 2.26.2 wheel  Build is still failing, but it's taking WAY longer to fail now. This is possibly a good sign.","Yep, I'm stopping for the night, it's currently stuck on what seems to be duplicate logging macros, looks like maybe two different logging libraries are somehow being included at the same time. Two very very similar logging libraries. But instead of taking 30 seconds before it fails, now it takes 17 minutes to fail, which I define as progress! ``` external/com_google_absl/absl/log/check.h:122:9: warning: 'CHECK_LT' macro redefined [Wmacroredefined]   122          ^ In file included from tensorflow/core/kernels/fill_empty_rows_functor_gpu.cu.cc:21: In file included from ./tensorflow/core/common_runtime/gpu/gpu_event_mgr.h:21: In file included from ./tensorflow/core/common_runtime/device/device_event_mgr.h:30: In file included from ./tensorflow/core/platform/stream_executor.h:21: In file included from external/local_xla/xla/stream_executor/dnn.h:47: In file included from external/local_xla/xla/stream_executor/scratch_allocator.h:26: In file included from external/local_xla/xla/stream_executor/device_memory_allocator.h:22: ```"," VICTORY Ok I didn't stop for the night. Instead, I just ignored all manner of warnings that shouldn't be ignored: ``` bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow config=cuda config=cuda_wheel  copt=Wnognuoffsetofextensions copt=Wnoerror copt=Wnoc23extensions verbose_failures copt=Wnomacroredefined ``` And bam! ``` INFO: Found 1 target... Target //tensorflow/tools/pip_package:wheel uptodate:   bazelbin/tensorflow/tools/pip_package/wheel_house/tensorflow2.20.0.dev0+selfbuiltcp310cp310linux_x86_64.whl INFO: Elapsed time: 87.690s, Critical Path: 86.67s INFO: 2 processes: 1 internal, 1 local. INFO: Build completed successfully, 2 total actions ``` No idea if it'll work, but it **did build**! I've pushed the latest code changes to my branch. https://github.com/maludwig/tensorflow/compare/ml/fixing_tf_env...maludwig:tensorflow:ml/attempting_build_rtx5090?expand=1","It passed one test! ``` (tfnightie) mitch:~/stable_diff/fix_tf/tensorflow $ bazel test repo_env=WHEEL_NAME=tensorflow config=cuda config=cuda_wheel  copt=Wnognuoffsetofextensions copt=Wnoerror copt=Wnoc23extensions verbose_failures copt=Wnomacroredefined tensorflow/python/kernel_tests/nn_ops:softmax_op_test WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Reading 'startup' options from /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=243 INFO: Reading rc options for 'test' from /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc:   Inherited 'common' options: announce_rc experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility noenable_bzlmod noincompatible_enable_cc_toolchain_resolution noincompatible_enable_android_toolchain_resolution experimental_repo_remote_exec java_runtime_version=remotejdk_21 INFO: Reading rc options for 'test' from /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc:   Inherited 'build' options: repo_env=ML_WHEEL_TYPE=snapshot repo_env=ML_WHEEL_BUILD_DATE= repo_env=ML_WHEEL_VERSION_SUFFIX= define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive host_features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 INFO: Reading rc options for 'test' from /home/mitch/stable_diff/fix_tf/tensorflow/.tf_configure.bazelrc:   Inherited 'build' options: action_env PYTHON_BIN_PATH=/home/mitch/.virtualenvs/tfnightie/bin/python action_env PYTHON_LIB_PATH=/home/mitch/.virtualenvs/tfnightie/lib/python3.10/sitepackages python_path=/home/mitch/.virtualenvs/tfnightie/bin/python action_env LD_LIBRARY_PATH=/usr/local/cuda12.8/lib64:/home/mitch/stable_diff/fix_tf/libs/cudnnlinuxx86_649.8.0.87_cuda12archive/lib: config=cuda_clang action_env CLANG_CUDA_COMPILER_PATH=/home/mitch/stable_diff/fix_tf/llvm/LLVM20.1.0LinuxX64/bin/clang20 config=cuda_clang INFO: Reading rc options for 'test' from /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc:   'test' options: test_env=GTEST_INSTALL_FAILURE_SIGNAL_HANDLER=1 INFO: Reading rc options for 'test' from /home/mitch/stable_diff/fix_tf/tensorflow/.tf_configure.bazelrc:   'test' options: test_size_filters=small,medium test_env=LD_LIBRARY_PATH INFO: Found applicable config definition build:short_logs in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition test:v2 in file /home/mitch/stable_diff/fix_tf/tensorflow/.tf_configure.bazelrc: test_tag_filters=benchmarktest,no_oss,oss_excluded,no_gpu,oss_serial,v1only build_tag_filters=benchmarktest,no_oss,oss_excluded,no_gpu,v1only INFO: Found applicable config definition build:cuda_clang in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: config=cuda //:cuda_compiler=clang copt=Qunusedarguments repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 copt=Wnounknowncudaversion host_linkopt=fuseld=lld host_linkopt=lm linkopt=fuseld=lld linkopt=lm INFO: Found applicable config definition build:cuda in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda repo_env=HERMETIC_CUDA_VERSION=12.5.1 repo_env=HERMETIC_CUDNN_VERSION=9.3.0 //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /home/mitch/stable_diff/fix_tf/tensorflow/.tf_configure.bazelrc: repo_env HERMETIC_CUDA_VERSION=12.8.1 repo_env HERMETIC_CUDNN_VERSION=9.8.0 repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=compute_120 INFO: Found applicable config definition build:cuda_clang in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: config=cuda //:cuda_compiler=clang copt=Qunusedarguments repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 copt=Wnounknowncudaversion host_linkopt=fuseld=lld host_linkopt=lm linkopt=fuseld=lld linkopt=lm INFO: Found applicable config definition build:cuda in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda repo_env=HERMETIC_CUDA_VERSION=12.5.1 repo_env=HERMETIC_CUDNN_VERSION=9.3.0 //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /home/mitch/stable_diff/fix_tf/tensorflow/.tf_configure.bazelrc: repo_env HERMETIC_CUDA_VERSION=12.8.1 repo_env HERMETIC_CUDNN_VERSION=9.8.0 repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=compute_120 INFO: Found applicable config definition build:cuda in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda repo_env=HERMETIC_CUDA_VERSION=12.5.1 repo_env=HERMETIC_CUDNN_VERSION=9.3.0 //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /home/mitch/stable_diff/fix_tf/tensorflow/.tf_configure.bazelrc: repo_env HERMETIC_CUDA_VERSION=12.8.1 repo_env HERMETIC_CUDNN_VERSION=9.8.0 repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=compute_120 INFO: Found applicable config definition build:cuda_wheel in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: //cuda:include_cuda_libs=false INFO: Found applicable config definition build:linux in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: host_copt=w copt=Wnoall copt=Wnoextra copt=Wnodeprecated copt=Wnodeprecateddeclarations copt=Wnoignoredattributes copt=Wnoarraybounds copt=Wunusedresult copt=Werror=unusedresult copt=Wswitch copt=Werror=switch define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=dynamic_kernels experimental_guard_against_concurrent_changes INFO: Found applicable config definition build:dynamic_kernels in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS DEBUG: /home/mitch/.cache/bazel/_bazel_mitch/98f54844abcf3e1cdc99e9d96b271d9e/external/local_xla/third_party/py/python_repo.bzl:154:14: HERMETIC_PYTHON_VERSION variable was not set correctly, using default version. Python 3.10 will be used. To select Python version, either set HERMETIC_PYTHON_VERSION env variable in your shell:   export HERMETIC_PYTHON_VERSION=3.12 OR pass it as an argument to bazel command directly or inside your .bazelrc file:   repo_env=HERMETIC_PYTHON_VERSION=3.12 DEBUG: /home/mitch/.cache/bazel/_bazel_mitch/98f54844abcf3e1cdc99e9d96b271d9e/external/local_xla/third_party/py/python_repo.bzl:87:10: ============================= Hermetic Python configuration: Version: ""3.10"" Kind: """" Interpreter: ""default"" (provided by rules_python) Requirements_lock label: ""//:requirements_lock_3_10.txt"" ===================================== WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. WARNING: Build options @//cuda:include_cuda_libs, copt, cxxopt, and 2 more have changed, discarding analysis cache (this can be expensive, see https://bazel.build/advanced/performance/iterationspeed). INFO: Analyzed 2 targets (749 packages loaded, 56015 targets configured). INFO: Found 2 test targets... INFO: Elapsed time: 270.116s, Critical Path: 245.71s INFO: 2560 processes: 378 internal, 2182 local. INFO: Build completed successfully, 2560 total actions //tensorflow/python/kernel_tests/nn_ops:softmax_op_test_cpu              PASSED in 217.4s //tensorflow/python/kernel_tests/nn_ops:softmax_op_test_gpu              PASSED in 218.4s Executed 2 out of 2 tests: 2 tests pass. ``` I also installed the wheel generated in the last step to a new python venv, and it worked! ``` (test5090build) mitch:~/stable_diff/fix_tf/test5090build $ python c ""import tensorflow as tf; print(tf.__version__)"" 20250317 04:37:51.455319: I external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1742207871.466384  646442 cuda_dnn.cc:8670] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered I0000 00:00:1742207871.469996  646442 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered W0000 00:00:1742207871.479137  646442 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207871.479166  646442 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207871.479169  646442 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207871.479172  646442 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. 20250317 04:37:51.481701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 2.20.0dev0+selfbuilt (test5090build) mitch:~/stable_diff/fix_tf/test5090build $ python c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"" 20250317 04:38:02.348770: I external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1742207882.360431  646471 cuda_dnn.cc:8670] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered I0000 00:00:1742207882.364089  646471 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered W0000 00:00:1742207882.373383  646471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207882.373422  646471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207882.373426  646471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207882.373437  646471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. 20250317 04:38:02.376028: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] (test5090build) mitch:~/stable_diff/fix_tf/test5090build $ cat test_gpu.py import tensorflow as tf  Check if GPU is available gpus = tf.config.list_physical_devices('GPU') if not gpus:     print(""🚫 No GPU found!"") else:     print(f""✅ Found GPU(s): {[gpu.name for gpu in gpus]}"")  Place operations on GPU with tf.device('/GPU:0'):      Create two tensors     a = tf.constant([[1.0, 2.0], [3.0, 4.0]])     b = tf.constant([[5.0, 6.0], [7.0, 8.0]])      Add tensors     add_result = tf.add(a, b)     print(""\nAddition result:"")     print(add_result)      Matrix multiplication     matmul_result = tf.matmul(a, b)     print(""\nMatrix multiplication result:"")     print(matmul_result)  Print device placement info (optional, debug) print(""\nDevice placement log:"") tf.debugging.set_log_device_placement(True) (test5090build) mitch:~/stable_diff/fix_tf/test5090build $ python test_gpu.py 20250317 04:38:25.409242: I external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1742207905.420314  646517 cuda_dnn.cc:8670] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered I0000 00:00:1742207905.423851  646517 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered W0000 00:00:1742207905.432651  646517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207905.432680  646517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207905.432684  646517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207905.432686  646517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. 20250317 04:38:25.435305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. ✅ Found GPU(s): ['/physical_device:GPU:0'] I0000 00:00:1742207906.790435  646517 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29043 MB memory:  > device: 0, name: NVIDIA GeForce RTX 5090, pci bus id: 0000:09:00.0, compute capability: 12.0 Addition result: tf.Tensor( [[ 6.  8.]  [10. 12.]], shape=(2, 2), dtype=float32) Matrix multiplication result: tf.Tensor( [[19. 22.]  [43. 50.]], shape=(2, 2), dtype=float32) Device placement log: ``` I...am...going...to...run all the tests overnight? My build process is complete trash and I have no idea what I'm doing, but I COULD also PR this code, but like, that's slightly terrifying. I've ignoring probably thousands of warnings that a competent C++ developer could probably actually solve, rather than just ignore...","Tests didn't pass, but it did build! And it could do basic matrix addition and multiplication in Python! NOW I'm definitely going to bed though.","It also is able to do the classic ""hello world"" ML task of learning digits on MNIST, but the warnings are PLENTIFUL and cryptic. I don't know what they mean, but the final model happens to work great! ``` (test5090build) mitch:~/stable_diff/fix_tf/test5090build $ cat mnist_test.py !/usr/bin/env python import tensorflow as tf from tensorflow.keras import layers, models import numpy as np  Load MNIST dataset (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()  Normalize pixel values to [0,1] x_train = x_train / 255.0 x_test = x_test / 255.0  Build the model model = models.Sequential([     layers.Flatten(input_shape=(28, 28)),       Flatten 28x28 to 784     layers.Dense(128, activation='relu'),       Hidden layer     layers.Dense(10, activation='softmax')      Output layer ])  Compile the model model.compile(optimizer='adam',               loss='sparse_categorical_crossentropy',               metrics=['accuracy'])  Train the model model.fit(x_train, y_train, epochs=5, validation_split=0.1)  Evaluate the model test_loss, test_acc = model.evaluate(x_test, y_test) print(f""Test accuracy: {test_acc:.4f}"")  Make predictions predictions = model.predict(x_test)  Example: Print prediction for the first image print(f""First test sample  Predicted: {np.argmax(predictions[0])}, Actual: {y_test[0]}"") (test5090build) mitch:~/stable_diff/fix_tf/test5090build $ ./mnist_test.py 20250317 11:23:11.786039: I external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1742232191.796888  662647 cuda_dnn.cc:8670] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered I0000 00:00:1742232191.800405  662647 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered W0000 00:00:1742232191.809207  662647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742232191.809234  662647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742232191.809238  662647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742232191.809259  662647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. 20250317 11:23:11.811904: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. /home/mitch/.virtualenvs/test5090build/lib/python3.10/sitepackages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.   super().__init__(**kwargs) I0000 00:00:1742232193.910442  662647 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29043 MB memory:  > device: 0, name: NVIDIA GeForce RTX 5090, pci bus id: 0000:09:00.0, compute capability: 12.0 Epoch 1/5 20250317 11:23:15.366974: I external/local_xla/xla/service/service.cc:152] XLA service 0x7f5928008d30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 20250317 11:23:15.367006: I external/local_xla/xla/service/service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 5090, Compute Capability 12.0 20250317 11:23:15.376904: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. I0000 00:00:1742232195.434380  662725 cuda_dnn.cc:529] Loaded cuDNN version 90800 20250317 11:23:16.211437: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 80 bytes spill stores, 80 bytes spill loads 20250317 11:23:16.220559: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95_0', 164 bytes spill stores, 164 bytes spill loads 20250317 11:23:16.340804: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 392 bytes spill stores, 392 bytes spill loads 20250317 11:23:16.364181: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 76 bytes spill stores, 76 bytes spill loads 20250317 11:23:16.374280: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_232', 176 bytes spill stores, 176 bytes spill loads 20250317 11:23:16.385374: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 76 bytes spill stores, 76 bytes spill loads 20250317 11:23:16.393417: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 292 bytes spill stores, 292 bytes spill loads 20250317 11:23:16.451825: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 532 bytes spill stores, 532 bytes spill loads 20250317 11:23:16.522600: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_232', 168 bytes spill stores, 168 bytes spill loads 20250317 11:23:16.556430: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 1040 bytes spill stores, 1040 bytes spill loads 20250317 11:23:16.607519: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_232', 112 bytes spill stores, 112 bytes spill loads 20250317 11:23:16.806055: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 4920 bytes spill stores, 4992 bytes spill loads 20250317 11:23:16.867917: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 5084 bytes spill stores, 5028 bytes spill loads WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1742232197.511568  662725 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process. 1684/1688 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  accuracy: 0.8767  loss: 0.446420250317 11:23:20.774706: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_232', 32 bytes spill stores, 32 bytes spill loads 20250317 11:23:20.804309: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 288 bytes spill stores, 288 bytes spill loads 20250317 11:23:20.807183: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 76 bytes spill stores, 76 bytes spill loads 20250317 11:23:20.828074: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 76 bytes spill stores, 76 bytes spill loads 20250317 11:23:20.895561: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 752 bytes spill stores, 752 bytes spill loads 20250317 11:23:21.076717: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_232', 80 bytes spill stores, 80 bytes spill loads 20250317 11:23:21.096547: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_232', 72 bytes spill stores, 72 bytes spill loads 20250317 11:23:21.177785: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 4920 bytes spill stores, 4992 bytes spill loads 20250317 11:23:21.227351: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 5084 bytes spill stores, 5028 bytes spill loads 1688/1688 ━━━━━━━━━━━━━━━━━━━━ 8s 3ms/step  accuracy: 0.8768  loss: 0.4459  val_accuracy: 0.9668  val_loss: 0.1275 Epoch 2/5 1688/1688 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step  accuracy: 0.9598  loss: 0.1359  val_accuracy: 0.9710  val_loss: 0.0985 Epoch 3/5 1688/1688 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step  accuracy: 0.9748  loss: 0.0853  val_accuracy: 0.9728  val_loss: 0.0920 Epoch 4/5 1688/1688 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step  accuracy: 0.9810  loss: 0.0640  val_accuracy: 0.9775  val_loss: 0.0809 Epoch 5/5 1688/1688 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step  accuracy: 0.9855  loss: 0.0473  val_accuracy: 0.9782  val_loss: 0.0797 313/313 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  accuracy: 0.9720  loss: 0.0911 Test accuracy: 0.9753 313/313 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step First test sample  Predicted: 7, Actual: 7 ```", you should upgrade commit hash256 XLA on bazel file and it should work,"Sorry that's a bit cryptic for me. I'm normally a Python dev, apologies. Did you mean in my commits on my branch above? https://github.com/maludwig/tensorflow/compare/ml/fixing_tf_env...maludwig:tensorflow:ml/attempting_build_rtx5090?expand=1",+1," Steps to get it running on your RTX 5000 series card  Guide for all platforms  Install llvm 20.1.0 LLVM 20.1.0 is required to compile code for compute capability 10.0 and 12.0 (RTX 5000 series). All platforms here: https://github.com/llvm/llvmproject/releases/tag/llvmorg20.1.0  Install CUDA 12.8.1 CUDA 12.8.1 is required to compile code for compute capability 10.0 and 12.0 (RTX 5000 series). Also install cuDNN 9.8.0 and NCCL 2, for CUDA 12.  Install Python 3.10.12 This just happens to be the version I'm using and may be completely unnecessary. I personally love pyenv because it installs it to your local user, so you don't need to fret about admin/root permissions.  Make a Python venv for tensorflow This will prevent your system from being polluted by tensorflow dependencies, and will make it much much much easier to clean up if you want to start over.  Install Bazelisk Bazelisk is a wrapper for Bazel that downloads the correct version of Bazel for the project.  Clone tensorflow ```bash echo ""Clone tensorflow"" git clone git.com:tensorflow/tensorflow.git cd tensorflow echo ""Add my remote to the repo"" git remote add maludwig 'git.com:maludwig/tensorflow.git' echo ""Fetch my remote"" git fetch all echo ""Checkout my branch"" git checkout ml/attempting_build_rtx5090 echo ""Pull my branch"" git pull maludwig ml/attempting_build_rtx5090 ```  Configure bazel ```bash echo ""Configure bazel, these are the settings I used, but I'm not sure if they're correct, or if they just happened to work for me."" export HERMETIC_CUDA_VERSION=12.8.1 export HERMETIC_CUDNN_VERSION=9.8.0 export HERMETIC_CUDA_COMPUTE_CAPABILITIES=compute_120 export LOCAL_CUDA_PATH=/usr/local/cuda12.8 export LOCAL_NCCL_PATH=/usr/lib/x86_64linuxgnu/libnccl.so.2.26.2 export TF_NEED_CUDA=1 export CLANG_CUDA_COMPILER_PATH=""$(which clang)"" python configure.py ```  Build tensorflow ```bash echo ""Good luck building!"" echo ""Note, I have trust issues with bazel now, so I always run 'bazel clean expunge' before building. This may be a personal psychological issue rather than a requirement."" bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow config=cuda config=cuda_wheel copt=Wnognuoffsetofextensions copt=Wnoerror copt=Wnoc23extensions verbose_failures copt=Wnomacroredefined ```  Script for WSL Ubuntu 22.04 This script should let you compile for RTX 5000 series on WSL Ubuntu 22.04. Before running this script, be sure to install the latest drivers for your RTX 5000 series card on the Windows side, install WSL2, and use Ubuntu 22.04. Then reboot your PC, that way, WSL2 will be able to see your GPU. It probably also works on nonWSL Ubuntu 22.04. It might maybe work on other Ubuntu versions. It's not going to work for Windows except in WSL. It may not work at all. Consider copying it line by line and handle errors manually. ```bash mkdir p ""$HOME/rtx5000"" cd ""$HOME/rtx5000"" echo ""Installing essential dev tools"" sudo aptget update sudo aptget install y buildessential wget patchelf echo ""Installing Python 3.10"" sudo apt install y make buildessential libssldev zlib1gdev libbz2dev libreadlinedev libsqlite3dev wget curl llvm libncurses5dev libncursesw5dev xzutils tkdev libffidev liblzmadev curl https://pyenv.run | bash pyenv install 3.10.12 pyenv global 3.10.12 echo ""Restart your shell to use Python 3.10"" echo ""After restarting, confirm this says python 3.10.12"" python version echo ""Make a virtualenv for tensorflow"" python3.10 m venv ~/rtx5000/venv echo ""Activate the python virtualenv"" source ~/rtx5000/venv/bin/activate echo ""Installing LLVM 20.1.0"" wget https://github.com/llvm/llvmproject/releases/download/llvmorg20.1.0/LLVM20.1.0LinuxX64.tar.xz tar xvf LLVM20.1.0LinuxX64.tar.xz echo ""Installing NVIDIA packages"" wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cudakeyring_1.11_all.deb sudo dpkg i cudakeyring_1.11_all.deb sudo aptget update echo ""Installing NVIDIA CUDA 12.8"" sudo aptget y install cudatoolkit128 echo ""Installing NVIDIA cuDNN 9, for CUDA 12"" sudo aptget y install cudnn9cuda12 echo ""Installing NVIDIA NCCL 2"" sudo apt install libnccl2=2.26.21+cuda12.8 libnccldev=2.26.21+cuda12.8 echo ""Installing Bazelisk for Bazel"" mkdir p ~/rtx5000/bin cd ~/rtx5000/bin wget 'https://github.com/bazelbuild/bazelisk/releases/download/v1.25.0/bazelisklinuxamd64' chmod +x bazelisklinuxamd64 mv bazelisklinuxamd64 bazel ``` Add these lines to your `~/.bashrc` or `~/.zshrc` file: ``` export LLVM_HOME=""$HOME/rtx5000/LLVM20.1.0LinuxX64"" export CUDA_HOME=""/usr/local/cuda12.8"" export PATH=""${LLVM_HOME}/bin:${CUDA_HOME}/bin:${HOME}/rtx5000/bin:$PATH"" export LD_LIBRARY_PATH=""$CUDA_HOME/lib64:$LD_LIBRARY_PATH"" export CPATH=""$CUDA_HOME/include:$CPATH"" ``` Restart your terminal. Test that the LLVM installation worked: Make this file in `~/rtx5000/card_details.cu`: ```cpp include  include   // Add cuDNN header include  int main() {     cudaDeviceProp prop;     int device;     cudaGetDevice(&device); // Get the current device ID     cudaGetDeviceProperties(&prop, device); // Get device properties     size_t free_mem, total_mem;     cudaMemGetInfo(&free_mem, &total_mem); // Get VRAM usage     std::cout  GPU Name: ""  Compute Capability: ""  VRAM Usage: ""  cuDNN Version: ""                GPU Name: NVIDIA GeForce RTX 5090 > Compute Capability: 12.0 > VRAM Usage: 1763 MB / 32606 MB > cuDNN Version: 9.8.0 echo ""This should compile the code with clang++"" clang++ std=c++17 cudagpuarch=sm_120 x cuda cudapath=""$CUDA_HOME"" I""$CUDA_HOME/include"" L""$CUDA_HOME/lib64""  lcudart card_details.cu o card_details_clang echo ""This should print your card details again, just the same as before"" ./card_details_clang > GPU Name: NVIDIA GeForce RTX 5090 > Compute Capability: 12.0 > VRAM Usage: 1763 MB / 32606 MB > cuDNN Version: 9.8.0 echo ""This should be Bazel v8.8.1"" bazel version echo ""Activate the python virtualenv"" source ~/rtx5000/venv/bin/activate echo ""This should be Python 3.10.12"" python version echo ""Clone tensorflow"" cd ~/rtx5000 git clone git.com:tensorflow/tensorflow.git cd tensorflow echo ""Add my remote to the repo"" git remote add maludwig 'git.com:maludwig/tensorflow.git' echo ""Fetch my remote"" git fetch all echo ""Checkout my branch"" git checkout ml/attempting_build_rtx5090 echo ""Pull my branch"" git pull maludwig ml/attempting_build_rtx5090 echo ""Configure bazel, these are the settings I used, but I'm not sure if they're correct, or if they just happened to work for me."" export HERMETIC_CUDA_VERSION=12.8.1 export HERMETIC_CUDNN_VERSION=9.8.0 export HERMETIC_CUDA_COMPUTE_CAPABILITIES=compute_120 export LOCAL_CUDA_PATH=/usr/local/cuda12.8 export LOCAL_NCCL_PATH=/usr/lib/x86_64linuxgnu/libnccl.so.2.26.2 export TF_NEED_CUDA=1 export CLANG_CUDA_COMPILER_PATH=""$(which clang)"" python configure.py echo ""Good luck building!"" echo ""Note, I have trust issues with bazel now, so I always run 'bazel clean expunge' before building. This may be a personal psychological issue rather than a requirement."" bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow config=cuda config=cuda_wheel  copt=Wnognuoffsetofextensions copt=Wnoerror copt=Wnoc23extensions verbose_failures copt=Wnomacroredefined ```  NOTE You mayyyybe need to get the very latest cuDNN with this, but I don't think so. ``` cd ~/rtx5000 wget https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linuxx86_64/cudnnlinuxx86_649.8.0.87_cuda12archive.tar.xz tar xvf cudnnlinuxx86_649.8.0.87_cuda12archive.tar.xz echo add this to your ~/.bashrc export LD_LIBRARY_PATH=""$HOME/rtx5000/cudnnlinuxx86_649.8.0.87_cuda12archive/lib:$LD_LIBRARY_PATH"" ``` NOTE: If this doesn't work for you, let me know which error you got, and maybe I missed something in my environment. Since this was already my dev box, I'm not sure if this is a complete guide, but it's what I did to get it working.","Hey  , just seeing your tags you added. To be clear, this is on tf_nightly, not tf 2.18, and I have no idea really what I'm doing, so I'm not gonna PR my extremely busted and testsfailing branch, even though it does build. I put it here so that someone who knows what they're doing could fold in the new stuff more easily, or so that other normal humans like me could run tensorflow on an RTX 5000, instead of just being unable to run it. An actual human who knows what they're doing should look this over and figure it out.",cd ~/rtx5000 nvcc o card_details_nvcc card_details.cu bash: cd: /home/nicolai/rtx5000: No such file or directory cc1plus: fatal error: card_details.cu: No such file or directory compilation terminated.,"I tryed to let it run on my wsl. Build dosen't work, how can I use a prebuiled nightly build?   Configuration: 8850a00e136a9e8be32c557a177e77f38f3c27b70c44518acb5ba0af47f7836b  Execution platform: @//:platform In file included from external/local_xla/xla/stream_executor/cuda/cuda_status.cc:16: external/local_xla/xla/stream_executor/cuda/cuda_status.h:22:10: fatal error: 'third_party/gpus/cuda/include/cuda.h' file not found    22           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 1 error generated. Target //tensorflow/tools/pip_package:wheel failed to build ERROR: /mnt/c/Projekte/tmp/tensorflow/tensorflow/tools/pip_package/BUILD:293:9 Action tensorflow/tools/pip_package/wheel_house/tensorflow2.20.0.dev0+selfbuiltcp312cp312linux_x86_64.whl failed: (Exit 1): clang20 failed: error executing CppCompile command (from target @//xla/stream_executor/cuda:cuda_status)   (cd /root/.cache/bazel/_bazel_root/509ab554767d44265e0030c4731aba07/execroot/org_tensorflow && \   exec env  \     CLANG_CUDA_COMPILER_PATH=/usr/local/bin/clang20 \     PATH=/root/.cache/bazelisk/downloads/sha256/c97f02133adce63f0c28678ac1f21d65fa8255c80429b588aeeba8a1fac6202b/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \     PWD=/proc/self/cwd \     PYTHON_BIN_PATH=/mnt/c/Projekte/env/bin/python3 \     PYTHON_LIB_PATH=/mnt/c/Projekte/env/lib/python3.12/sitepackages \     TF2_BEHAVIOR=1 \   /usr/local/bin/clang20 MD MF bazelout/k8opt/bin/external/local_xla/xla/stream_executor/cuda/_objs/cuda_status/cuda_status.pic.d 'frandomseed=bazelout/k8opt/bin/external/local_xla/xla/stream_executor/cuda/_objs/cuda_status/cuda_status.pic.o' iquote external/local_xla iquote bazelout/k8opt/bin/external/local_xla iquote external/com_google_absl iquote bazelout/k8opt/bin/external/com_google_absl iquote external/local_config_cuda iquote bazelout/k8opt/bin/external/local_config_cuda iquote external/cuda_cudart iquote bazelout/k8opt/bin/external/cuda_cudart iquote external/cuda_cublas iquote bazelout/k8opt/bin/external/cuda_cublas iquote external/cuda_cccl iquote bazelout/k8opt/bin/external/cuda_cccl iquote external/cuda_nvtx iquote bazelout/k8opt/bin/external/cuda_nvtx iquote external/cuda_nvcc iquote bazelout/k8opt/bin/external/cuda_nvcc iquote external/cuda_cusolver iquote bazelout/k8opt/bin/external/cuda_cusolver iquote external/cuda_cufft iquote bazelout/k8opt/bin/external/cuda_cufft iquote external/cuda_cusparse iquote bazelout/k8opt/bin/external/cuda_cusparse iquote external/cuda_curand iquote bazelout/k8opt/bin/external/cuda_curand iquote external/cuda_cupti iquote bazelout/k8opt/bin/external/cuda_cupti iquote external/cuda_nvml iquote bazelout/k8opt/bin/external/cuda_nvml iquote external/cuda_nvjitlink iquote bazelout/k8opt/bin/external/cuda_nvjitlink iquote external/local_tsl iquote bazelout/k8opt/bin/external/local_tsl Ibazelout/k8opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers Ibazelout/k8opt/bin/external/cuda_cudart/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cublas/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cccl/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_nvtx/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_nvcc/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cusolver/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cufft/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cusparse/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_curand/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cupti/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_nvml/_virtual_includes/headers Ibaroot199root199P461:/mroot199P461:/mroot199P461:/mroot199P461:/mroot199P461:/mroot199P461:/mnt/c/Projekte/tmp/tensorflow    ", and  is there a build that I can use? (like nightly build),"Hey  , scroll up until you see ""Script for WSL Ubuntu 22.04"" in the comments.  The issue I raised is that there is no build, nightly or otherwise, that supports the latest Blackwell GPUs. I arguably managed to build one myself. You also could. But read through the script I put up above slowly. It looks like you missed some steps. HOPEFULLY the script I wrote will work for someone else, but since I got it working on an old dev box, rather than a brand fresh new black docker container or something, it's likely that I missed a dependency or two."," dosen't work for me. echo ""This should be Bazel v8.8.1"" bazel version here I get only 8.1.1 and I get some Errors for the build. is there any chance when tensorflow will support the 5090 on its own and I can simply use the next version of tensorflow? If so, please give me a date when.","Yes, I feel problem with NVIDIA RTX 5090  32GB Blackwell (not nightly version of PyTorch). I cannot see GPU with TensorFlow success. Can you take a look at https://gist.github.com/donhuvy/6cd637a09b034168d01181d5ce98a5fe  . I catch `Num GPUs Available:  0` . My environment: Windows 11 pro, JupyterLab latest version, Python 3.11.x .",wait so you got it to work 100%. it sucks my system has 2 5090's and i'm using a cpu for training.,"Whenever I could not get drivers to work, it usually resolved after installing, reinstalling and changing versions of different packages, since the shortage I doubt there is overwhelming support for the 5090, I remember all launches to have crashing and minimal error bugs that disappear over a relatively short period of time. I got stuck a while ago similarly on different cards and in general, it might be a tiny thing somewhere with your paths and env. Try on Linux and see if that works, I don't know why you are using Windows as a senior Dev. My speeds on the 4090 doubled on render times for anything AI/ML related and loading times of nearly everything python vanished.",">  dosen't work for me. > echo ""This should be Bazel v8.8.1"" > bazel version here I get only 8.1.1 and I get some Errors for the build. >  >  > is there any chance when tensorflow will support the 5090 on its own and I can simply use the next version of tensorflow? >  > If so, please give me a date when. >  Sorry  , I'm not a tensorflow employee. I'm just some dude. Can't guess when it will be fixed. I just got my build to work and my personal projects running fine. My tests are failing and I assume that needs resolving. If your Bazel version is wrong, try installing Bazelisk. See above for instructions. ","> wait so you got it to work 100%. it sucks my system has 2 5090's and i'm using a cpu for training. >  Yep. For my workflow (training StableDiffusion LoRAs) it works fine. The tests are failing locally, but they must be testing tensorflow components that I am not using. You could presumably try following in my footsteps and use your 5090s. ","> Try on Linux and see if that works, I don't know why you are using Windows as a senior Dev. My speeds on the 4090 doubled on render times for anything AI/ML related and loading times of nearly everything python vanished. I'm also a senior dev, and while I agree in general that Linux is better and faster, Windows is still a perfectly legit OS. In fact, Apple Silicon is quite nice for training too. There's no distinction between RAM and VRAM in arm64a. Huge models run on consumer hardware. Not near as fast as on nvidia, but OSX is a legit OS too. "," I tried following the WSL script and got this error in the final step: ""external/local_tsl/tsl/profiler/lib/nvtx_utils.cc:32:10: fatal error: 'third_party/gpus/cuda/include/cuda.h' file not found"". All previous steps were OK, such as the one building the .cu file using clang. ","  What's your HERMETIC_CUDA_VERSION? It should be 12.8.1 Apart from that, maybe try cleaning the Bazel cache? ```  Double check CUDA echo ""HERMETIC_CUDA_VERSION: $HERMETIC_CUDA_VERSION""  I have trust issues with every cache thing bazel clean expunge ```"," It is 12.8.1 correctly. I also tried cleaning the Bazel cache, the error was same: header files in 'third_party/gpus/cuda/include' cannot be found. "," Some additional info: among the error verbose text, it displayed some environmental variables such as ""LD_LIBRARY_PATH"" ""PATH"", etc, but no ""CPATH"" can be seen. Could this be related to the issue?"," find nvtx_utils. include ""cuda.h""  > [](https://github.com/maludwig) I tried following the WSL script and got this error in the final step: ""external/local_tsl/tsl/profiler/lib/nvtx_utils.cc:32:10: fatal error: 'third_party/gpus/cuda/include/cuda.h' file not found"". All previous steps were OK, such as the one building the .cu file using clang."
tpu,maludwig,Fixing bugs and old code in the tf_env script,"I was trying to make a bug report, and the Github Issues thing told me to run this script, which failed to run and produced a lot of buggy output. I didn't try to make any changes to the script, since I'm not strictly sure what all of these help with, I just tried to stabilize the script, make the code cleaner, and account for common bugs in bash scripts (like a space in the path to python). I'll make comments on the PR to explain each change.",2025-03-15T03:11:04Z,ready to pull size:M,closed,0,7,https://github.com/tensorflow/tensorflow/issues/89271,"Sorry, maybe I should have merged right away rather than making the changes. Not sure what the process is for letting you know, but I've made extremely minor modifications as suggested to the top of the file.","Sorry if I'm doing it wrong, new to contributing to this project, but I added the comment lines you suggested, can you rereview,  ?","Sorry for the delay, I was not able to do much TF reviews over the past few days.","So, now what happens. I can't seem to merge this. Who does the merging?","If you look at https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.mdcontributorlicenseagreements, there's a graph just above the section I just linked to. The PR needs to get imported internally and there are more checks and reviews there. You only see what is above the topmost line.","Oh, ok, I expected it to be like an automated thing, where once it gets approved then it rather quickly gets merged, if tests pass. So now I just wait then I guess? I just found a different PR (unrelated) that you approved and then it got merged like 2 weeks later. Is this normal? https://github.com/tensorflow/tensorflow/pull/73692","Yeah, sadly we only have one single engineer that is pinged when the internal change gets imported, plus the people that review the PR. But if I review the PR I cannot also approve the internal change, so either I need to add someone else or the pinged engineer has to review. The nice thing is that here everything seems to pass, so once we get the internal approval this should be ready to merge."
tpu,kaushikn02,A dynamic link library (DLL) initialization routine failed.," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.8  Custom code Yes  OS platform and distribution windows 11  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[3], line 1 > 1 import tensorflow as tf       2 print(tf.__version__) File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\hp\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.  Standalone code to reproduce the issue ```shell ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[3], line 1 > 1 import tensorflow as tf       2 print(tf.__version__) File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\hp\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```  Relevant log output ```shell ```",2025-03-15T01:59:40Z,type:build/install,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89269,please update the MSVC 2019 redistributable，download links: https://download.visualstudio.microsoft.com/download/pr/285b28c73cf947fb9be801cf5323a8df/8F9FB1B3CFE6E5092CF1225ECD6659DAB7CE50B8BF935CB79BFEDE1F3C895240/VC_redist.x64.exe,Please search for duplicates before opening a new issue,Are you satisfied with the resolution of your issue? Yes No
autograph,hhoppe,"MemoryError during ""import tensorflow"" with tensorflow-cpu==2.19.0"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tensorflowcpu 2.19.0  Custom code No  OS platform and distribution WSL Ubuntu 22.04.5  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ""import tensorflow"" crashes.  Standalone code to reproduce the issue ```shell pip install U tensorflowcpu==2.19.0 numpy keras python c ""import tensorflow""   Crashes; see log below.  It works OK with 2.18.0 and 2.18.1 ```  Relevant log output ```shell 20250314 18:21:25.978099: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. Traceback (most recent call last):   File """", line 1, in    File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/__init__.py"", line 49, in      from tensorflow._api.v2 import __internal__   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/_api/v2/__internal__/__init__.py"", line 8, in      from tensorflow._api.v2.__internal__ import autograph   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/_api/v2/__internal__/autograph/__init__.py"", line 8, in      from tensorflow.python.autograph.core.ag_ctx import control_status_ctx  line: 34   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/python/autograph/core/ag_ctx.py"", line 21, in      from tensorflow.python.autograph.utils import ag_logging   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/python/autograph/utils/__init__.py"", line 17, in      from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/python/autograph/utils/context_managers.py"", line 19, in      from tensorflow.python.framework import ops   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/python/framework/ops.py"", line 33, in      from tensorflow.core.framework import attr_value_pb2   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/core/framework/attr_value_pb2.py"", line 14, in      from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/core/framework/tensor_pb2.py"", line 14, in      from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/core/framework/resource_handle_pb2.py"", line 14, in      from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/core/framework/tensor_shape_pb2.py"", line 18, in      _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/google/protobuf/internal/builder.py"", line 67, in BuildMessageAndEnumDescriptors     BuildNestedDescriptors(msg_des, module_name + '_')   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/google/protobuf/internal/builder.py"", line 57, in BuildNestedDescriptors     for (name, nested_msg) in msg_des.nested_types_by_name.items(): MemoryError ```",2025-03-15T01:42:02Z,stat:awaiting response type:bug wsl2 TF 2.18,closed,0,9,https://github.com/tensorflow/tensorflow/issues/89268,"Hello  , please try to install the dependencies in a virtual environment. The python version I have in my env is `3.12.8`. I ran the codes as below: ``` mamba env create n tensor mamba activate tensor pip install U tensorflowcpu==2.19.0 numpy keras python c ""import tensorflow"" ``` The result: ``` 20250327 08:19:31.700350: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. ```","Thanks  . (I used `python m venv` instead of `mamba`.) With `pip install U tensorflowcpu==2.19.0 numpy keras`, it works OK. The problem is that I also include `tensorflowdatasets`. With `pip install U tensorflowcpu==2.19.0 numpy keras tensorflowdatasets`, it fails: Package `tensorflowdatasets` depends on `tensorflowmetadata`; `tensorflowmetadata 1.16.1 requires protobuf=3.20.3; python_version < ""3.11"", but you have protobuf 5.29.4 which is incompatible.` The resulting older `protobuf3.20.3` causes the failure. So the problem for me is the dependency in `tensorflowmetadata`, which may be outdated?","(In addition, `tensorflowcpu` should somehow depend on the more recent version of `protobuf` ?)","Hello  ,  I tested the Python command you mentioned earlier with the new dependencies you provided and encountered no errors. Additionally, I’ve attached the requirments.txt file I used. The issue might be resolved by changing the Python version in your virtual environment. requirments.txt","Hi  , Apologies for the delay, and thank you for raising your concern here. I tried running your code in the same environment you mentioned, but I did not encounter any issues. I’m attaching a screenshot for your reference. !Image Please let me know if I made any mistakes. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"With `pip install U tensorflowcpu==2.19.0 numpy keras tensorflowdatasets`, I now see: `Successfully installed MarkupSafe3.0.2 abslpy2.2.2 array_record0.7.2 astunparse1.6.3 attrs25.3.0 certifi2025.4.26 charsetnormalizer3.4.2 dmtree0.1.9 docstringparser0.16 einops0.8.1 etils1.12.2 flatbuffers25.2.10 fsspec2025.3.2 gast0.6.0 googlepasta0.2.0 grpcio1.71.0 h5py3.13.0 idna3.10 immutabledict4.2.1 importlib_resources6.5.2 keras3.9.2 libclang18.1.1 markdown3.8 markdownitpy3.0.0 mdurl0.1.2 mldtypes0.5.1 namex0.0.9 numpy2.1.3 opteinsum3.4.0 optree0.15.0 packaging25.0 promise2.3 protobuf4.21.12 psutil7.0.0 pyarrow20.0.0 pygments2.19.1 requests2.32.3 rich14.0.0 simple_parsing0.1.7 six1.17.0 tensorboard2.19.0 tensorboarddataserver0.7.2 tensorflowcpu2.19.0 tensorflowdatasets4.9.8 tensorflowiogcsfilesystem0.37.1 tensorflowmetadata1.17.1 termcolor3.1.0 toml0.10.2 tqdm4.67.1 typingextensions4.13.2 urllib32.4.0 werkzeug3.1.3 wheel0.45.1 wrapt1.17.2 zipp3.21.0` Possibly the upgrade `tensorflowmetadata1.16.1 > tensorflowmetadata1.17.1` allowed the upgrade `protobuf3.20.3 > protobuf4.21.12`. And now it works fine. So we can close this issue.","Hi  , Glad to see your issue has been resolved! Please feel free to close this issue if everything is working as expected. Thank you!",Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],"Remove canonicalization from stablehlo <-> hlo conversion apis. Keeping conversion as a pure conversion, without any optimizations/canonicalization. Canonicalization was anyway disabled for current users as it has side effects.","Remove canonicalization from stablehlo  hlo conversion apis. Keeping conversion as a pure conversion, without any optimizations/canonicalization. Canonicalization was anyway disabled for current users as it has side effects. There was a duplicate call to StableToMhlo function, removed.",2025-03-15T00:21:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89267
tpu,copybara-service[bot],Lower lax.ragged_dot_general to chlo.ragged_dot in some cases on tpu.,Lower lax.ragged_dot_general to chlo.ragged_dot in some cases on tpu.,2025-03-14T22:54:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89262
opt,copybara-service[bot],[XLA] fix use after free in HloPassPipeline,[XLA] fix use after free in HloPassPipeline There is no guarantee that module config is will not be updated by passes so any access of debug_options will be use after free.,2025-03-14T21:18:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89257
tpu,sjh0849,Inconsistent results when running tf.keras.layers.InputLayer," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? The expectation is that the API should raise a ValueError, but it does not pass the test.  Standalone code to reproduce the issue ```shell def test_input_layer_with_none_shape(self):          Test with None as input shape         with self.assertRaises(ValueError):             tf.keras.layers.InputLayer(input_shape=None) ```  Relevant log output ```shell Traceback (most recent call last):   File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/zero_shot/tf/tf.keras.layers.InputLayer.py"", line 55, in test_input_layer_with_none_shape     tf.keras.layers.InputLayer(input_shape=None) AssertionError: ValueError not raised ```",2025-03-14T19:41:05Z,stat:awaiting response type:bug stale comp:keras TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89252,"Hi  , Apologies for the delay, and thank you for your patience. I tried running your code on Colab using TensorFlow version 2.19.0 and nightly, but I did not encounter the error you are facing. I am attaching a gist for your reference. Based on my findings, this issue seems to be more related to Keras. I recommend posting this issue on the kerasteam/keras repository for better support. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,sjh0849,Getting AttributeError when running tf.ones_like," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux 22.04  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? Getting AttributeError when running tf.ones_like.  Standalone code to reproduce the issue ```shell def test_ones_like_with_name(self):          Test with name         input_tensor = tf.constant([1, 2, 3])         result = tf.ones_like(input_tensor, name=""test_ones_like"")         self.assertEqual(result.op.name, ""test_ones_like"") ```  Relevant log output ```shell Traceback (most recent call last):   File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.ones_like.py"", line 66, in test_ones_like_with_name     self.assertEqual(result.op.name, ""test_ones_like"")   File ""/home/user/anaconda3/lib/python3.8/sitepackages/tensorflow/python/framework/ops.py"", line 444, in __getattr__     self.__getattribute__(name)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/tensorflow/python/framework/ops.py"", line 1305, in op     raise AttributeError( AttributeError: Tensor.op is undefined when eager execution is enabled. ```",2025-03-14T19:34:37Z,type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89250,"Hi  , Apologies for the delay, and thanks for raising your concern here. Could you please provide the full code snippet? This would help debug your issue more accurately. I tried running your code on Colab using TensorFlow versions 2.18.0 and 2.19.0, but I did not encounter any issues. Please find the gist here for your reference. Let me know if you need further assistance. Thank you!","sorry for that again. here is the complete class! ```python import tensorflow as tf import numpy as np import unittest class TestTfOnesLike(unittest.TestCase):     def test_ones_like_with_name(self):          Test with name         input_tensor = tf.constant([1, 2, 3])         result = tf.ones_like(input_tensor, name=""test_ones_like"")         self.assertEqual(result.op.name, ""test_ones_like"") if __name__ == '__main__':     unittest.main() ``` many thanks!"
tpu,sjh0849,The API documentation of tf.no_op specifies that tf.no_op() should return a TensorFlow Operation.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution N\A  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? The API documentation of tf.no_op specifies that tf.no_op() should return a TensorFlow Operation.  The tests expect that calling tf.no_op() returns an instance of tf.Operation (and that its name property can be set by providing a name, e.g., ""my_no_op""). The error message indicates that tf.no_op() is returning None rather than an Operation (None has no attribute ""name""), causing the tests to fail. Given that the test suite is properly written according to the API's specification and the expected behavior, the issue stems from the source code implementation.  Standalone code to reproduce the issue ```shell def test_no_op_name(self):         """"""Test that a no_op can be created with a specific name.""""""         no_op = tf.no_op(name=""my_no_op"")         self.assertEqual(no_op.name, ""my_no_op"")    def test_no_op_creation(self):         """"""Test that a no_op can be created without errors.""""""         try:             no_op = tf.no_op()             self.assertIsInstance(no_op, tf.Operation)         except Exception as e:             self.fail(f""tf.no_op raised an exception: {e}"") ```  Relevant log output ```shell Traceback (most recent call last):   File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.no_op.py"", line 17, in test_no_op_name     self.assertEqual(no_op.name, ""my_no_op"") AttributeError: 'NoneType' object has no attribute 'name' Traceback (most recent call last):   File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.no_op.py"", line 12, in test_no_op_creation     self.fail(f""tf.no_op raised an exception: {e}"") AssertionError: tf.no_op raised an exception: None is not an instance of  ```",2025-03-14T19:01:19Z,type:bug comp:apis comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89248,"Hi  , Apologies for the delay, and thanks for raising your concern here. Could you please provide the full code snippet? This would help debug your issue more accurately. I tried running your code on Colab using TensorFlow versions 2.18.0 and 2.19.0, but I did not encounter any issues. Please find the gist here for your reference. Let me know if you need further assistance. Thank you!","sorry for that. here is the complete class! ```python import unittest import tensorflow as tf class TestNoOpFunctionality(unittest.TestCase):     def test_no_op_name(self):         """"""Test that a no_op can be created with a specific name.""""""         no_op = tf.no_op(name=""my_no_op"")         self.assertEqual(no_op.name, ""my_no_op"")     def test_no_op_creation(self):         """"""Test that a no_op can be created without errors.""""""         try:             no_op = tf.no_op()             self.assertIsInstance(no_op, tf.Operation)         except Exception as e:             self.fail(f""tf.no_op raised an exception: {e}"") if __name__ == '__main__':     unittest.main() ``` many thanks!"
tpu,sjh0849,tf.keras.losses.SparseCategoricalCrossentropy has logical/Doc bug," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? API documentation of tf.keras.losses.SparseCategoricalCrossentropy mentions that one of the parameters can be None, but the implementation does not check None, it checks 'none' which is a string.  Standalone code to reproduce the issue ```shell def test_reduction_none(self):          Test with reduction set to None         y_true = np.array([0, 1, 2])         y_pred = np.array([[0.9, 0.05, 0.05],                            [0.05, 0.9, 0.05],                            [0.05, 0.05, 0.9]])         loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=None)         loss = loss_fn(y_true, y_pred).numpy()         expected_loss = np.log([0.9, 0.9, 0.9])         np.testing.assert_almost_equal(loss, expected_loss, decimal=5) ```  Relevant log output ```shell Traceback (most recent call last):   File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.keras.losses.SparseCategoricalCrossentropy.py"", line 49, in test_reduction_none     loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=None)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/losses.py"", line 1026, in __init__     super().__init__(   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/losses.py"", line 262, in __init__     super().__init__(reduction=reduction, name=name)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/losses.py"", line 93, in __init__     losses_utils.ReductionV2.validate(reduction)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/utils/losses_utils.py"", line 88, in validate     raise ValueError( ValueError: Invalid Reduction Key: None. Expected keys are ""('auto', 'none', 'sum', 'sum_over_batch_size')"" ```",2025-03-14T18:50:50Z,type:bug comp:keras TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89246,"This should be opened against the keras repository (tf_keras or keras directly). From the TF side, the only change that could be done is to update the documentation of the function, make sure the docstring uses the correct phrasing. Looking at https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy I see an example where `None` is being used instead of the string `""none""`, so if the example was a doctest then some test would have failed. We should convert the examples to be doctests. Probably though all of this would happen on keras side, not TF.", I see! Thanks for the clarification. I will report this to the keras repository. Thanks!
tpu,sjh0849,I get an invalid shape error when running tf.keras.losses.BinaryCrossentropy," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux 22.04  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? I get an invalid shape error when running ```tf.keras.losses.BinaryCrossentropy```  Standalone code to reproduce the issue ```shell def test_binary_crossentropy_invalid_inputs(self):          Test with invalid inputs         y_true = np.array([0, 1, 0, 1], dtype=np.float32)         y_pred = np.array([0.1, 0.9, 0.2], dtype=np.float32)   Mismatched shape         bce = tf.keras.losses.BinaryCrossentropy()         with self.assertRaises(ValueError):             bce(y_true, y_pred) ```  Relevant log output ```shell Traceback (most recent call last):   File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.keras.losses.BinaryCrossentropy.py"", line 70, in test_binary_crossentropy_invalid_inputs     bce(y_true, y_pred)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/losses.py"", line 152, in __call__     losses = call_fn(y_true, y_pred)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/losses.py"", line 284, in call     return ag_fn(y_true, y_pred, **self._fn_kwargs)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/losses.py"", line 2176, in binary_crossentropy     backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/backend.py"", line 5688, in binary_crossentropy     bce = target * tf.math.log(output + epsilon()) tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [4] vs. [3] [Op:Mul] ```",2025-03-14T18:42:02Z,type:bug comp:keras TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89245,"Hi  , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18, 2.19, and the nightly, but I did not encounter any issues. Please find the gist attached for your reference. Thank you!","Hi  , Thanks for getting back to me! I noticed, like the other issue, that the replicable code wasn't sufficient, leading to the failure to call and execute the function. However, I reran the test, but it's not giving me the errors it used to give me. I think it's being flaky."
tpu,sjh0849,AssertionError when calling tf.keras.layers.Conv2DTranspose," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux 22.4  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? When running ```tf.keras.layers.Conv2DTranspose```, I get AssertionError. The minimum reproducing example is attached.  Standalone code to reproduce the issue ```shell def test_kernel_initializer():          Test with a custom kernel initializer         model = tf.keras.Sequential([             tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, kernel_initializer='ones', input_shape=(4, 4, 1))         ])         input_data = np.ones((1, 4, 4, 1), dtype=np.float32)         output_data = model(input_data)         expected_output = np.full((1, 6, 6, 1), 9.0)   Since kernel is initialized with ones         np.testing.assert_array_almost_equal(output_data.numpy(), expected_output) ```  Relevant log output ```shell Traceback (most recent call last):   File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.keras.layers.Conv2DTranspose.py"", line 74, in test_kernel_initializer     np.testing.assert_array_almost_equal(output_data.numpy(), expected_output)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/numpy/testing/_private/utils.py"", line 1046, in assert_array_almost_equal     assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,   File ""/home/user/anaconda3/lib/python3.8/sitepackages/numpy/testing/_private/utils.py"", line 844, in assert_array_compare     raise AssertionError(msg) AssertionError:  Arrays are not almost equal to 6 decimals Mismatched elements: 32 / 36 (88.9%) Max absolute difference: 8. Max relative difference: 0.88888889  x: array([[[[1.],          [2.],          [3.],...  y: array([[[[9.],          [9.],          [9.],...  Ran 8 tests in 0.155s FAILED (failures=2) ```",2025-03-14T18:37:22Z,type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89244,"Hi  , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18, 2.19, and the nightly, but I did not encounter any issues. Please find the gist attached for your reference. Thank you!","> Hi [](https://github.com/sjh0849) , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18, 2.19, and the nightly, but I did not encounter any issues. Please find the gist attached for your reference. >  > Thank you! Hi  , Sorry, I think my replication code earlier was not sufficient. (The colab seems like it's not executing the test.) Here is the updated replicable code. ```python import tensorflow as tf import numpy as np import unittest class TestConv2DTranspose(unittest.TestCase):     def test_kernel_initializer(self):          Test with a custom kernel initializer         model = tf.keras.Sequential([             tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, kernel_initializer='ones', input_shape=(4, 4, 1))         ])         input_data = np.ones((1, 4, 4, 1), dtype=np.float32)         output_data = model(input_data)         expected_output = np.full((1, 6, 6, 1), 9.0)   Since kernel is initialized with ones         np.testing.assert_array_almost_equal(output_data.numpy(), expected_output) if __name__ == '__main__':     unittest.main() ``` > Hi [](https://github.com/sjh0849) , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18, 2.19, and the nightly, but I did not encounter any issues. Please find the gist attached for your reference. >  > Thank you!"
tpu,copybara-service[bot],Propagate volatility attribute when creating QConst ops during quantization,Propagate volatility attribute when creating QConst ops during quantization * This ensures that output of a qconst will have the same volatility as the source quantize op. * This will help with subsequently identify and remove unnecessary QConst>Dequantize ops after quantization.,2025-03-14T17:39:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89241
tpu,panda123dd,build from source: TypeError: 'NoneType' object is not iterable," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19.0  Custom code Yes  OS platform and distribution linux ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version 6.5.0  GCC/compiler version clang 11.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When use this command ""bazel build [config=option] //tensorflow/tools/pip_package:build_pip_package "",it can't build successfully. and I have done this ""py_binary( name = ""build_pip_package"",  changed from ""build_pip_package_py"" srcs = [""build_pip_package.py""], main = ""build_pip_package.py"","" but it can't solve the problem.  Standalone code to reproduce the issue ```shell (tensorflow2.19test) root:/frameworks/tensorflow2.19test ./bazelbin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg usage: build_pip_package.py [h] outputname OUTPUT_NAME projectname PROJECT_NAME platform                             PLATFORM [headers HEADERS] [srcs SRCS] [dests DESTS]                             [xla_aot XLA_AOT] [version VERSION] [collab COLLAB] build_pip_package.py: error: the following arguments are required: outputname, projectname, platform (tensorflow2.19test) root:/frameworks/tensorflow2.19test ./bazelbin/tensorflow/tools/pip_package/build_pip_package outputname /tmp/ttt.whl projectname ttt platform linux_86_64 Traceback (most recent call last):   File ""/frameworks/tensorflow2.19test/./bazelbin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/tools/pip_package/build_pip_package.py"", line 461, in      prepare_wheel_srcs(   File ""/frameworks/tensorflow2.19test/./bazelbin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/tools/pip_package/build_pip_package.py"", line 271, in prepare_wheel_srcs     prepare_headers(headers, os.path.join(srcs_dir, ""tensorflow/include""))   File ""/frameworks/tensorflow2.19test/./bazelbin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/tools/pip_package/build_pip_package.py"", line 142, in prepare_headers     for file in headers: TypeError: 'NoneType' object is not iterable ```  Relevant log output ```shell ```",2025-03-14T17:22:32Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89240,"Hi **** , Apologies for the delay, and thank you for bringing up this issue. I noticed a version mismatch in Clang, which could be one of the possible reasons for issue. TensorFlow 2.18+ requires Clang 17.0.6, whereas you are using Clang 11. Upgrading to the recommended version might help resolve the issue. For reference, I am attaching the official documentation please check the compatibility requirements to ensure everything aligns correctly. Additionally, here is a related issue that might provide further insights. Let me know if you still facing issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],PR #23718: Add missing hlo_argument_modes to the error message,"PR CC( Failed to load the native TensorFlow runtime.): Add missing hlo_argument_modes to the error message Imported from GitHub PR https://github.com/openxla/xla/pull/23718 The `multihost_hlo_runner` supports five `hlo_argument_mode` options, but existing error message lists only three modes. This PR adds missing modes to the error message  specifically ""use_zeros_as_input"" and ""uninitialized"" Additionally, the error message now uses raw string literals (R""()"") to simplify the code and avoid unnecessary escape characters. Copybara import of the project:  f0abbde8f3cd514acfdfb68701483c73e77ef50e by Alexander Pivovarov : Add missing hlo_argument_modes to error message Merging this change closes CC( Failed to load the native TensorFlow runtime.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23718 from apivovarov:fix_err_msg f0abbde8f3cd514acfdfb68701483c73e77ef50e",2025-03-14T14:12:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89233
oom,copybara-service[bot],Revert default value of `print_large_constants` back to `false`.,"Revert default value of `print_large_constants` back to `false`. This causes some tests to OOM. Printing large constants is required for parsability. However, it is not clear that the default print options should produce a parsable result. Users that require parsability, can use e.g. `HloPrintOptions::ShortParsable`.",2025-03-14T10:28:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89225
tpu,Jhe-Cyuan,Getting 0 nodes delegated while using TFLite C Library with TFLite Flex Delegate," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version Tensroflow 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version 6.5.0  GCC/compiler version 13.3.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am using the TFLite C library to generate model inference C code for an edge device. In my use case, I require certain operations, such as tf.unpack with shape (0, 2), which are supported by TensorFlow but not currently available in TFLite.  How I built the TFLite C Library I found a solution article on the Google AI Edge website (here), which provides instructions on building the TFLite C library using CMake. I followed these steps and successfully built the library without any errors. As a result, I obtained `libtensorflowlite_c.so` and placed it in `/usr/local/lib`, which is the system library directory on Ubuntu.  How I built the Tensorflow Flex Delegate library Similarly, there is an article here that explains how to build `libtensorflowlite_flex.so.` I followed the instructions provided and successfully built the library without encountering any issues. To verify the build, I ran the command: ```bash strings /usr/local/lib/libtensorflowlite_flex.so | grep Unpack ``` This command displayed several symbols containing Unpack, which indicates that the build was successful, I think.  Click to show output ``` UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp '(new gtl::FlatSet{""ArgMax"", ""ArgMin"", ""AudioSpectrogram"", ""AvgPool"", ""BatchMatMul"", ""BatchMatMulV2"", ""BatchNormWithGlobalNormalization"", ""BatchToSpace"", ""BatchToSpaceND"", ""Bincount"", ""BroadcastArgs"", ""BroadcastGradientArgs"", ""Bucketize"", ""CTCBeamSearchDecoder"", ""CTCGreedyDecoder"", ""CTCLoss"", ""CompareAndBitpack"", ""ComplexAbs"", ""Concat"", ""ConcatOffset"", ""ConcatV2"", ""Conv2D"", ""Copy"", ""CopyHost"", ""Cross"", ""CudnnRNN"", ""CudnnRNNBackprop"", ""CudnnRNNBackpropV2"", ""CudnnRNNBackpropV3"", ""CudnnRNNCanonicalToParams"", ""CudnnRNNCanonicalToParamsV2"", ""CudnnRNNParamsSize"", ""CudnnRNNParamsToCanonical"", ""CudnnRNNParamsToCanonicalV2"", ""CudnnRNNV2"", ""CudnnRNNV3"", ""CumProd"", ""CumSum"", ""DebugNanCount"", ""DebugNumericSummary"", ""DecodeProtoV2"", ""DecodeWav"", ""DeepCopy"", ""DepthToSpace"", ""Dequantize"", ""Diag"", ""DiagPart"", ""EditDistance"", ""Empty"", ""EncodeProtoV2"", ""EncodeWav"", ""ExtractImagePatches"", ""ExtractVolumePatches"", ""Fill"", ""Gather"", ""GatherNd"", ""GatherV2"", ""HistogramFixedWidth"", ""InvertPermutation"", ""IsInf"", ""IsNan"", ""Isfinite"", ""LinSpace"", ""LowerBound"", ""MatMul"", ""MatrixDiag"", ""MatrixDiagPart"", ""MatrixDiagPartV2"", ""MatrixDiagV2"", ""Mfcc"", ""Multinomial"", ""OneHot"", ""Pack"", ""ParameterizedTruncatedNormal"", ""PopulationCount"", ""RandomGamma"", ""RandomPoisson"", ""RandomPoissonV2"", ""RandomStandardNormal"", ""RandomUniform"", ""RandomUniformInt"", ""Range"", ""Rank"", ""RequantizationRange"", ""Requantize"", ""ReverseSequence"", ""Shape"", ""ShapeN"", ""Size"", ""SpaceToBatch"", ""SpaceToBatchND"", ""SpaceToDepth"", ""SparseMatMul"", ""Split"", ""SplitV"", ""TruncatedNormal"", ""Unique"", ""UniqueV2"", ""UniqueWithCounts"", ""UniqueWithCountsV2"", ""Unpack"", ""UnravelIndex"", ""UpperBound"", ""Where""})' Must be non NULL TensorArrayUnpack Unpack UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackGrad tfg.Unpack N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEmEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceElEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEjEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEtEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEsEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEhEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEaEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEiEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceENS1_4halfEEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceENS1_8bfloat16EEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEfEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEdEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceESt7complexIfEEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceESt7complexIdEEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEbEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEN3tsl7tstringEEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceENS_14ResourceHandleEEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceENS_7VariantEEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEN9ml_dtypes15float8_internal11float8_e5m2EEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEN9ml_dtypes15float8_internal13float8_e4m3fnEEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEmLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEmLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceElLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceElLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEjLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEjLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEtLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEtLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEsLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEsLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEhLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEhLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEaLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEaLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEiLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEiLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS1_4halfELb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS1_4halfELb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS1_8bfloat16ELb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS1_8bfloat16ELb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEfLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEfLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEdLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEdLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceESt7complexIfELb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceESt7complexIfELb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceESt7complexIdELb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceESt7complexIdELb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEbLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEbLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEN3tsl7tstringELb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEN3tsl7tstringELb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS_14ResourceHandleELb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS_14ResourceHandleELb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS_7VariantELb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS_7VariantELb0EEE Unpack ```    How I use C Library Function to load model and delegate library I have check the model that some ops do have `Flex` prefix in the op names. !Image It was converted using the following settings: ```python converter = tf.lite.TFLiteConverter.from_saved_model(model_folder) converter.optimizations = [tf.lite.Optimize.DEFAULT]  converter.target_spec.supported_types = [tf.float16] converter.target_spec.supported_ops = [     tf.lite.OpsSet.SELECT_TF_OPS,  tf.lite.OpsSet.TFLITE_BUILTINS,  ] converter.allow_custom_ops = True converter.legalize_custom_tensor_list_ops = True converter._experimental_lower_tensor_list_ops = False  converter.experimental_enable_resource_variables = True tflite_model = converter.convert() ``` Then, I use the code below to load the model and delegate operations that are not supported by TFLite to the Flex library. However, it always displays a message indicating that no nodes have been delegated by the Flex library. ```bash INFO: Created TensorFlow Lite delegate for select TF ops. 20250314 17:43:31.412379: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 623 nodes with 0 partitions. ModifyGraphWithDelegate Ok Load model successfully! ``` How should I resolve this issue? I would really appreciate any insights or suggestions. Thanks in advance for your help!  Standalone code to reproduce the issue ```shell void tflite_load(const string model_path, map& sig_map) {     // Load model     TfLiteModel* model = TfLiteModelCreateFromFile(model_path.c_str());     if (model == nullptr) {         cerr (SharedLibrary::GetLibrarySymbol(hdll, ""TF_AcquireFlexDelegate""));     if (TF_AcquireFlexDelegate == NULL) {         cerr  delegate = TF_AcquireFlexDelegate();     auto TfLiteStatus = TfLiteInterpreterModifyGraphWithDelegate(interpreter, delegate.get());     if(TfLiteStatus==0)         cout << ""ModifyGraphWithDelegate Ok"" << endl;     cout << ""Load model successfully!"" << endl; } ```  Relevant log output ```shell ```",2025-03-14T10:11:51Z,stat:awaiting tensorflower type:bug comp:lite TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89224,"Hi, Cyuan  I apologize for the delay in response, If possible could you please want to try removing `converter.optimizations = [tf.lite.Optimize.DEFAULT]` temporarily to see if that changes the behavior. Please make sure that both libraries (TFLite C library and Flex delegate) are built from the same TensorFlow version. Version mismatches can cause delegation failures even when the code is correct. Your current approach applies the delegate after creating the interpreter. The recommended way is to add the delegate to the options before creating the interpreter something like below : ``` TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate(); // Acquire and add the delegate to options auto hdll = SharedLibrary::LoadLibrary(""libtensorflowlite_flex.so""); auto TF_AcquireFlexDelegate = reinterpret_cast(     SharedLibrary::GetLibrarySymbol(hdll, ""TF_AcquireFlexDelegate"")); std::unique_ptr delegate = TF_AcquireFlexDelegate(); // Add delegate to options before creating interpreter TfLiteInterpreterOptionsAddDelegate(options, delegate.get()); // Create interpreter with configured options TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options); ``` The Flex delegate has a specific allowlist of supported TensorFlow operations only operations in this list can be delegated Could you please check and try above things and see is it resolving your issue or not ? If I have missed something here please let me know. Thank you for your cooperation and patience.","Hi,   Thank you very much for replying me. I've edit my code as you told. ```C++ auto hdll = SharedLibrary::LoadLibrary(""libtensorflowlite_flex.so""); if (hdll == nullptr) {     cerr (     SharedLibrary::GetLibrarySymbol(hdll, ""TF_AcquireFlexDelegate"")); std::unique_ptr delegate = TF_AcquireFlexDelegate(); TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate(); TfLiteInterpreterOptionsSetEnableDelegateFallback(options, true); TfLiteInterpreterOptionsAddDelegate(options, delegate.get()); TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options); TfLiteInterpreterOptionsDelete(options); ``` But it still delegate 0 node with my model. ```bash INFO: Created TensorFlow Lite delegate for select TF ops. INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 623 nodes with 0 partitions. ``` As you mentioned, I built the `TFLite C Library` and `Flex Delegate` using the commit tagged v2.18.0. It seems that from that version onwards, we need to build the external_delegate library as well, although I haven’t tried that yet. My current test aims to explore ondevice training. To do this, I’ve incorporated some loss computation designs using TensorFlow ops (not Keras, but in SavedModel format). These models contain many operations such as `IF,` `UNPACK`, and others that are not natively supported by TFLite. They also involve tensors with dynamic or zero dimensions (e.g., shape (0, 10)), which adds further complexity. Therefore, understanding how to use delegates to enable more TensorFlow ops to be executed within the TFLite environment is crucial to my study. I hope this context helps clarify my direction. Thank you again for your support and discussion."
yi,copybara-service[bot],[XLA:CPU] Make the small while loop hoisting threshold configurable & increase default to 1024,"[XLA:CPU] Make the small while loop hoisting threshold configurable & increase default to 1024 This change also made an underlying bug apparent where dot ops would call into Eigen runtime and crash, this resolves that by not hoisting while loops which transitively call dot.",2025-03-14T10:01:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89223
opt,jiunkaiy,Qualcomm AI Engine Direct - Provide op optimization,Summary:  Support FC>CONV2D optimization  Add CONV2D op builder with CPU transpose Test:  [x] a8w8 Gemma3 compile and execution successfully  [x] qnn_compiler_plugin_test all pass,2025-03-14T07:08:28Z,awaiting review ready to pull size:L,closed,0,2,https://github.com/tensorflow/tensorflow/issues/89210,Any test results?,> Any test results? I have left some comments in the first conversation block. Thanks!
tpu,copybara-service[bot],PR #23386: [debug_options] Printing all the fields in debug options,PR CC(InvalidArgumentError: Invalid name: An op that loads optimization parameters into HBM for embedding. (ConfigureTPUEmbeddingHost)): [debug_options] Printing all the fields in debug options Imported from GitHub PR https://github.com/openxla/xla/pull/23386 This patch prints all the values of the debug options while dumping it under VLOG. This is specifically required for boolean fields which have the default value set to true in `xla/debug_options_flags.cc`. These values will not be printed in `DebugString()` if `XLA_FLAGS` overrides it to `false`. Copybara import of the project:  4746d058005f9fff6ebbcc37818a288979d05165 by Shraiysh Vaishay : [debug_options] Printing all the fields in debug options This patch prints all the values of the debug options while dumping it under VLOG. This is specifically required for boolean fields which have the default value set to true in `xla/debug_options_flags.cc`. These values will not be printed in `DebugString()` if `XLA_FLAGS` overrides it to `false`. Merging this change closes CC(InvalidArgumentError: Invalid name: An op that loads optimization parameters into HBM for embedding. (ConfigureTPUEmbeddingHost)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23386 from shraiysh:print_debug_options_full 4746d058005f9fff6ebbcc37818a288979d05165,2025-03-14T05:36:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89207
sharding,copybara-service[bot],Enable `jax.device_put` to a sharding with no local devices.,Enable `jax.device_put` to a sharding with no local devices.,2025-03-14T00:39:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89201
sharding,copybara-service[bot],Expose a method in spmd_partitioner for propagating sharding from module parameters and outputs to the module's sharding configs.,Expose a method in spmd_partitioner for propagating sharding from module parameters and outputs to the module's sharding configs.,2025-03-13T23:30:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89197
tpu,copybara-service[bot],Allow different scales/zero-points on inputs+outputs for uint8 w/tfl.concat.,"Allow different scales/zeropoints on inputs+outputs for uint8 w/tfl.concat. For legacy reasons which are not well understood, uint8 allows different different scales/zeropoints across its inputs and the output scale/zeropoint being the union across the different quant params. This change disables requantizing inputs and outputs to a single scale/zeropoint and replicates behavior exhibited by the now defunct toco converter in order to support some legacy clients for which this functionality was broken during migration.",2025-03-13T23:04:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89196
yi,copybara-service[bot],Update PyArray::BatchedDevicePut to support zero buffers and destination devices. Use batched_device_put instead of ArrayImpl to build arrays with no local shards.,Update PyArray::BatchedDevicePut to support zero buffers and destination devices. Use batched_device_put instead of ArrayImpl to build arrays with no local shards.,2025-03-13T19:16:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89180
opt,copybara-service[bot],Optimization patterns to promote folding of more constants.,"Optimization patterns to promote folding of more constants. This pattern optimizes:    (x + cst1) + cst2 > x + cst    (x  cst1)  cst2 > x  cst  Where: cst = cst1 + cst2 Similar patterns can be possible with other binary ewise ops. But this CL only tackles Add, Sub, Mul and Div.",2025-03-13T18:42:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89179
sharding,copybara-service[bot],[XLA:Python] Small optimizations to NamedSharding::NamedSharding().,[XLA:Python] Small optimizations to NamedSharding::NamedSharding(). * don't repeatedly look up the _internal_device_list attribute. * only look up the `check_pspec` function once.,2025-03-13T18:10:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89173
opt,copybara-service[bot],[JAX] Add missing preset for X9 dot optimization on BF16/BF16 -> F32.,[JAX] Add missing preset for X9 dot optimization on BF16/BF16 > F32. Two PRs that support this feature have been submitted to stablehlo and openxla. Now we could do the last step  enable it in JAX.,2025-03-13T12:59:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89145
opt,copybara-service[bot],Change `c:litert_dispatch_delegate` to depend on `c:environment_options`.,Change `c:litert_dispatch_delegate` to depend on `c:environment_options`. This breaks the following dependency cycle in future CLs. ``` .> c:litert_environment    cc:litert_dispatch_delegate ` c:litert_environment ```,2025-03-13T11:39:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89144
opt,copybara-service[bot],[xla:cpu] enable scatter fusion emitter,"[xla:cpu] enable scatter fusion emitter Known issues:  bf16 performance is poor. This is because in the emitters   we are missing an optimization that we have in XLA thunks.   We will fix this soon.  No parallel scatter. We are leaving this as future work   since the singlethreaded implementation is already bringing   significant performance improvements. Scatter microbenchmarks: ```                                                                            │ fusion_emitters  │          scatter                    │                                                                            │    cpusec/op    │  cpusec/op   vs base               │ BM_ScatterS32_R1/262144/262144/process_time                                       600.1µ ± 1%   206.5µ ±  2%  65.59% (p=0.002 n=6) BM_ScatterS32_R2/512/512/process_time                                             77.45µ ± 1%   49.88µ ±  3%  35.59% (p=0.002 n=6) BM_ScatterS32_R3/64/64/process_time                                               54.28µ ± 0%   50.49µ ±  3%   6.99% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:1/d1:64/d2:8/num_slices:1/process_time            755.0n ± 1%   702.3n ±  4%   6.98% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:50/d1:64/d2:8/num_slices:10/process_time         106.82µ ± 0%   17.66µ ±  3%  83.47% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:500/d1:64/d2:8/num_slices:100/process_time       10.301m ± 2%   2.635m ±  2%  74.42% (p=0.002 n=6) BM_SelectAndScatterF32/128/process_time                                           37.00µ ± 1%   27.76µ ±  3%  24.97% (p=0.002 n=6) BM_SelectAndScatterF32/256/process_time                                          117.86µ ± 2%   86.36µ ±  1%  26.73% (p=0.002 n=6) BM_SelectAndScatterF32/512/process_time                                           1.613m ± 2%   1.528m ± 61%        ~ (p=0.065 n=6) geomean                                                                           134.8µ        73.45µ        45.53% ``` The gap from a few days ago was wider (geomean improvement of 76%), but the recent work on improving performance of small while loops (https://github.com/openxla/xla/commit/db734148ec74) narrowed that to the numbers above. Legacy emitters (""nothunks"") compile all while loops, and are therefore a tougher baseline to compare against. Still, scatter fusion emitters are faster (all singlethreaded): ```                                                                            │ nothunks         │          scatter                    │                                                                            │      sec/op      │    sec/op     vs base               │ BM_ScatterS32_R1/262144/262144/process_time                                      360.4µ ±  1%   203.0µ ±  0%  43.67% (p=0.002 n=6) BM_ScatterS32_R2/512/512/process_time                                            50.03µ ±  2%   49.81µ ±  3%        ~ (p=0.699 n=6) BM_ScatterS32_R3/64/64/process_time                                              49.92µ ±  0%   50.41µ ±  0%   +0.96% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:1/d1:64/d2:8/num_slices:1/process_time           617.5n ±  5%   722.7n ±  1%  +17.03% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:50/d1:64/d2:8/num_slices:10/process_time         17.60µ ±  3%   17.82µ ±  4%        ~ (p=0.937 n=6) BM_SimpleScatterReduceF32_R3/d0:500/d1:64/d2:8/num_slices:100/process_time       2.708m ±  2%   2.657m ±  1%        ~ (p=0.093 n=6) BM_SelectAndScatterF32/128/process_time                                          31.26µ ±  1%   27.90µ ±  2%  10.74% (p=0.002 n=6) BM_SelectAndScatterF32/256/process_time                                         101.54µ ±  1%   86.53µ ±  4%  14.78% (p=0.002 n=6) BM_SelectAndScatterF32/512/process_time                                          850.9µ ± 60%   909.6µ ± 16%        ~ (p=0.240 n=6) geomean                                                                          74.60µ         69.60µ         6.70% ``` For the rest of the microbenchmarks, the scatter emitter does not affect performance, except when bf16 is involved due to the known issue mentioned above.  Microbenchmarks ```                                                                                     │ fusion_emitters  │           scatter                    │                                                                                     │    cpusec/op    │  cpusec/op    vs base               │ BM_ConcatenateTwoR3F32/parallel:0/batch:128/width:64/height:256/axis:0/process_time       618.9µ ±  1%    820.4µ ±  1%  +32.57% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:128/width:64/height:256/axis:1/process_time       636.3µ ±  2%    654.3µ ±  6%        ~ (p=1.000 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:128/width:64/height:256/axis:2/process_time       1.002m ±  0%    1.003m ±  1%        ~ (p=0.485 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:256/width:128/height:64/axis:0/process_time       620.9µ ±  0%    607.5µ ±  2%   2.14% (p=0.009 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:256/width:128/height:64/axis:1/process_time       618.5µ ±  1%    750.9µ ±  3%  +21.42% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:256/width:128/height:64/axis:2/process_time       1.380m ±  5%    1.285m ±  6%   6.86% (p=0.041 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:64/width:256/height:128/axis:0/process_time       627.1µ ±  2%    818.4µ ±  6%  +30.50% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:64/width:256/height:128/axis:1/process_time       620.6µ ±  2%    715.8µ ± 14%  +15.36% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:64/width:256/height:128/axis:2/process_time       1.045m ±  2%    1.020m ±  0%   2.45% (p=0.015 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:128/width:64/height:256/axis:0/process_time       1.780m ± 10%    1.779m ± 12%        ~ (p=1.000 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:128/width:64/height:256/axis:1/process_time       1.703m ±  6%    1.540m ± 25%        ~ (p=0.180 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:128/width:64/height:256/axis:2/process_time       2.573m ± 10%    2.375m ± 10%        ~ (p=0.132 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:256/width:128/height:64/axis:0/process_time       1.671m ±  8%    1.790m ± 10%        ~ (p=0.132 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:256/width:128/height:64/axis:1/process_time       1.682m ±  8%    1.792m ±  7%   +6.56% (p=0.026 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:256/width:128/height:64/axis:2/process_time       2.361m ± 14%    2.190m ± 18%        ~ (p=0.093 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:64/width:256/height:128/axis:0/process_time       1.787m ± 12%    1.787m ± 17%        ~ (p=0.699 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:64/width:256/height:128/axis:1/process_time       1.668m ± 10%    1.751m ±  8%        ~ (p=0.485 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:64/width:256/height:128/axis:2/process_time       2.286m ± 12%    2.227m ± 16%        ~ (p=0.937 n=6) BM_Conv1DStrided/1/129/process_time                                                       53.14m ±  3%    50.15m ±  3%   5.62% (p=0.002 n=6) BM_Conv1DStrided/3/129/process_time                                                       161.4m ±  7%    151.3m ±  7%   6.21% (p=0.041 n=6) BM_Conv1DTransposedStrided/129/1/process_time                                             43.65m ± 16%    50.79m ±  5%  +16.35% (p=0.026 n=6) BM_Conv1DTransposedStrided/129/3/process_time                                             139.1m ± 15%    158.1m ±  5%  +13.68% (p=0.015 n=6) BM_Conv1DTransposedStridedNonDefaultLayout/129/1/process_time                             30.11m ±  7%    32.61m ± 14%        ~ (p=0.093 n=6) BM_Conv1DTransposedStridedNonDefaultLayout/129/3/process_time                             107.6m ±  5%    123.8m ±  5%  +15.06% (p=0.002 n=6) BM_Conv2D/16/32/32/128/1/1/1024/process_time                                         121.6m ± 21%    116.0m ± 22%        ~ (p=0.310 n=6) BM_Conv2D/16/32/32/128/3/3/1024/process_time                                         962.9m ± 10%    985.1m ±  8%        ~ (p=0.589 n=6) BM_Conv2D/32/256/256/4/1/1/16/process_time                                           180.9m ±  7%    196.0m ±  5%   +8.35% (p=0.002 n=6) BM_Conv2D/32/256/256/4/3/3/16/process_time                                           434.8m ± 15%    347.8m ± 13%  20.01% (p=0.009 n=6) BM_Conv2D/32/32/32/128/1/1/1024/process_time                                         235.2m ± 25%    244.3m ± 21%        ~ (p=0.093 n=6) BM_Conv2D/32/32/32/128/3/3/1024/process_time                                          1.922 ±  9%     2.074 ±  1%   +7.92% (p=0.041 n=6) BM_Conv2D/32/32/32/96/1/1/96/process_time                                            24.70m ±  2%    22.88m ±  6%   7.37% (p=0.004 n=6) BM_Conv2D/32/32/32/96/3/3/96/process_time                                            138.0m ±  2%    145.5m ±  9%   +5.45% (p=0.002 n=6) BM_Conv2D/32/64/64/32/1/1/64/process_time                                            30.90m ±  7%    27.09m ±  6%  12.33% (p=0.002 n=6) BM_Conv2D/32/64/64/32/3/3/64/process_time                                            164.0m ±  3%    161.8m ±  2%        ~ (p=0.180 n=6) BM_Conv2D/32/64/64/4/1/1/16/process_time                                             1.232m ±  7%    1.191m ±  4%        ~ (p=0.394 n=6) BM_Conv2D/32/64/64/4/3/3/16/process_time                                             30.67m ±  7%    24.93m ± 16%  18.72% (p=0.002 n=6) BM_Conv2D/8/128/128/4/1/1/8/process_time                                             459.2µ ±  2%    434.1µ ±  3%   5.46% (p=0.002 n=6) BM_Conv2D/8/128/128/4/3/3/8/process_time                                             14.52m ±  4%    14.46m ± 10%        ~ (p=0.699 n=6) BM_Conv2D/8/32/32/128/1/1/1024/process_time                                          53.96m ± 11%    51.73m ±  9%   4.15% (p=0.041 n=6) BM_Conv2D/8/32/32/128/3/3/1024/process_time                                          435.6m ±  4%    471.7m ± 21%   +8.29% (p=0.004 n=6) BM_Conv2D/8/5/5/1/1/1/32/process_time                                                3.260µ ±  2%    3.316µ ±  2%   +1.74% (p=0.004 n=6) BM_Conv2D/8/5/5/1/3/3/32/process_time                                                13.26µ ±  1%    13.34µ ±  0%        ~ (p=0.065 n=6) BM_Conv2D/8/5/5/4/1/1/32/process_time                                                1.794µ ±  1%    1.880µ ±  1%   +4.84% (p=0.002 n=6) BM_Conv2D/8/5/5/4/3/3/32/process_time                                                17.97µ ±  2%    18.02µ ±  1%        ~ (p=0.394 n=6) BM_Conv2DStrided/process_time                                                             56.48m ±  5%    54.79m ± 10%        ~ (p=0.132 n=6) BM_Conv2DTransposedStrided/process_time                                                   43.19m ±  9%    48.14m ±  2%  +11.44% (p=0.002 n=6) BM_GroupedConv2D/1/45/45/1024/5/5/1024/1024/process_time                                  414.7m ±  7%    434.2m ± 10%        ~ (p=0.093 n=6) BM_GroupedConv2DStrided/128/128/128/process_time                                          61.65m ±  9%    59.23m ±  6%        ~ (p=0.180 n=6) BM_GroupedConv2DStrided/128/128/16/process_time                                           59.17m ±  5%    59.62m ±  6%        ~ (p=0.937 n=6) BM_GroupedConv2DTransposedStrided/128/128/128/process_time                                 4.425 ±  7%     4.522 ±  3%        ~ (p=0.240 n=6) BM_GroupedConv2DTransposedStrided/128/128/16/process_time                                  3.878 ± 11%     4.496 ± 18%        ~ (p=0.132 n=6) BM_CustomCall_16FloatBuffers/process_time                                                 1.977µ ±  7%    1.982µ ±  6%        ~ (p=0.937 n=6) BM_CustomCall_16IntAttributes/process_time                                                612.8n ±  2%    657.2n ±  3%   +7.24% (p=0.002 n=6) BM_CustomCall_Minimal/process_time                                                        522.2n ±  1%    564.4n ±  8%   +8.07% (p=0.002 n=6) BM_DagExecution/1024/process_time                                                         7.606m ±  1%    7.259m ±  3%   4.56% (p=0.002 n=6) BM_DagExecution/128/process_time                                                          684.3µ ±  4%    502.5µ ± 21%  26.57% (p=0.002 n=6) BM_DagExecution/16384/process_time                                                        174.0m ± 11%    172.3m ± 27%        ~ (p=0.937 n=6) BM_DagExecution/256/process_time                                                          1.917m ±  3%    1.767m ±  4%   7.84% (p=0.002 n=6) BM_DagExecution/512/process_time                                                          3.464m ±  0%    3.296m ±  1%   4.83% (p=0.002 n=6) BM_DagExecution/8192/process_time                                                         67.37m ± 11%    69.95m ± 15%        ~ (p=0.485 n=6) BM_BatchedDot/11/1/128/process_time                                                       37.65µ ±  0%    37.53µ ±  0%   0.31% (p=0.004 n=6) BM_BatchedDot/11/1/2/process_time                                                         540.1n ±  2%    476.2n ±  7%  11.82% (p=0.002 n=6) BM_BatchedDot/11/1/256/process_time                                                       761.4µ ±  2%    720.0µ ±  7%   5.43% (p=0.026 n=6) BM_BatchedDot/11/1/32/process_time                                                        1.804µ ±  0%    1.713µ ±  2%   5.06% (p=0.002 n=6) BM_BatchedDot/11/1/512/process_time                                                       9.297m ±  4%    9.149m ±  1%        ~ (p=0.310 n=6) BM_BatchedDot/11/1/64/process_time                                                        6.452µ ±  0%    6.385µ ±  1%   1.05% (p=0.002 n=6) BM_BatchedDot/11/2/128/process_time                                                       74.46µ ±  0%    74.51µ ±  0%        ~ (p=0.394 n=6) BM_BatchedDot/11/2/2/process_time                                                         542.4n ±  1%    479.0n ± 14%        ~ (p=0.093 n=6) BM_BatchedDot/11/2/256/process_time                                                       1.702m ±  9%    1.472m ±  4%  13.53% (p=0.002 n=6) BM_BatchedDot/11/2/32/process_time                                                        2.837µ ±  0%    2.820µ ±  1%        ~ (p=0.065 n=6) BM_BatchedDot/11/2/512/process_time                                                       21.84m ±  3%    21.67m ±  2%        ~ (p=0.394 n=6) BM_BatchedDot/11/2/64/process_time                                                        12.11µ ±  1%    12.13µ ±  1%        ~ (p=0.818 n=6) BM_BatchedDot/11/4/128/process_time                                                       148.8µ ±  0%    149.0µ ±  0%   +0.13% (p=0.004 n=6) BM_BatchedDot/11/4/2/process_time                                                         546.2n ±  0%    550.1n ±  1%   +0.72% (p=0.002 n=6) BM_BatchedDot/11/4/256/process_time                                                       3.171m ± 17%    3.177m ± 16%        ~ (p=0.937 n=6) BM_BatchedDot/11/4/32/process_time                                                        4.820µ ±  0%    4.824µ ±  0%        ~ (p=0.394 n=6) BM_BatchedDot/11/4/512/process_time                                                       47.88m ±  2%    49.00m ±  6%        ~ (p=0.394 n=6) BM_BatchedDot/11/4/64/process_time                                                        23.28µ ±  0%    23.46µ ±  1%   +0.78% (p=0.002 n=6) BM_BatchedDot/11/8/128/process_time                                                       298.1µ ±  0%    297.3µ ±  0%   0.28% (p=0.002 n=6) BM_BatchedDot/11/8/2/process_time                                                         556.4n ±  1%    557.5n ±  1%        ~ (p=0.699 n=6) BM_BatchedDot/11/8/256/process_time                                                       6.350m ±  3%    7.168m ±  6%  +12.88% (p=0.002 n=6) BM_BatchedDot/11/8/32/process_time                                                        8.760µ ±  1%    8.830µ ±  1%   +0.80% (p=0.015 n=6) BM_BatchedDot/11/8/512/process_time                                                      101.53m ±  3%    97.02m ±  2%   4.44% (p=0.002 n=6) BM_BatchedDot/11/8/64/process_time                                                        45.88µ ±  0%    45.93µ ±  0%        ~ (p=0.240 n=6) BM_BatchedDot/16/1/128/process_time                                                       40.97µ ±  0%    40.97µ ±  0%        ~ (p=0.937 n=6) BM_BatchedDot/16/1/2/process_time                                                         610.1n ±  1%    603.9n ±  2%   1.00% (p=0.026 n=6) BM_BatchedDot/16/1/256/process_time                                                       1.080m ±  2%    1.125m ± 11%        ~ (p=0.065 n=6) BM_BatchedDot/16/1/32/process_time                                                        2.089µ ±  1%    2.062µ ±  1%   1.28% (p=0.026 n=6) BM_BatchedDot/16/1/512/process_time                                                       11.21m ±  2%    11.05m ±  1%   1.40% (p=0.041 n=6) BM_BatchedDot/16/1/64/process_time                                                        7.380µ ±  1%    7.356µ ±  1%        ~ (p=0.063 n=6) BM_BatchedDot/16/2/128/process_time                                                       81.12µ ±  0%    81.06µ ±  0%        ~ (p=0.240 n=6) BM_BatchedDot/16/2/2/process_time                                                         624.0n ±  1%    613.7n ±  1%   1.65% (p=0.002 n=6) BM_BatchedDot/16/2/256/process_time                                                       2.219m ±  2%    2.085m ±  1%   6.06% (p=0.002 n=6) BM_BatchedDot/16/2/32/process_time                                                        3.371µ ±  1%    3.358µ ±  0%        ~ (p=0.132 n=6) BM_BatchedDot/16/2/512/process_time                                                       25.65m ±  3%    24.98m ±  3%        ~ (p=0.065 n=6) BM_BatchedDot/16/2/64/process_time                                                        13.79µ ±  0%    13.80µ ±  1%        ~ (p=0.310 n=6) BM_BatchedDot/16/4/128/process_time                                                       163.6µ ±  0%    163.9µ ±  0%   +0.23% (p=0.002 n=6) BM_BatchedDot/16/4/2/process_time                                                         636.3n ±  1%    627.1n ±  3%        ~ (p=0.240 n=6) BM_BatchedDot/16/4/256/process_time                                                       4.720m ±  9%    4.489m ±  8%   4.91% (p=0.041 n=6) BM_BatchedDot/16/4/32/process_time                                                        5.752µ ±  1%    5.757µ ±  1%        ~ (p=0.485 n=6) BM_BatchedDot/16/4/512/process_time                                                       53.96m ±  2%    55.37m ±  5%        ~ (p=0.310 n=6) BM_BatchedDot/16/4/64/process_time                                                        26.56µ ±  1%    26.69µ ±  1%   +0.47% (p=0.041 n=6) BM_BatchedDot/16/8/128/process_time                                                       742.6µ ±  2%    896.0µ ± 17%  +20.65% (p=0.002 n=6) BM_BatchedDot/16/8/2/process_time                                                         650.6n ±  1%    649.3n ±  1%        ~ (p=0.818 n=6) BM_BatchedDot/16/8/256/process_time                                                       9.378m ±  6%    9.013m ±  4%   3.89% (p=0.015 n=6) BM_BatchedDot/16/8/32/process_time                                                        10.49µ ±  0%    10.49µ ±  0%        ~ (p=0.937 n=6) BM_BatchedDot/16/8/512/process_time                                                       115.7m ±  4%    110.4m ±  8%        ~ (p=0.132 n=6) BM_BatchedDot/16/8/64/process_time                                                        52.45µ ±  0%    52.64µ ±  0%   +0.37% (p=0.002 n=6) BM_DynamicUpdateSliceF32/1024/process_time                                                25.79µ ±  3%    26.61µ ±  3%        ~ (p=0.093 n=6) BM_DynamicUpdateSliceF32/128/process_time                                                 3.024µ ±  2%    3.085µ ±  1%   +2.02% (p=0.004 n=6) BM_DynamicUpdateSliceF32/16384/process_time                                               788.0µ ±  6%    754.3µ ±  5%   4.28% (p=0.009 n=6) BM_DynamicUpdateSliceF32/256/process_time                                                 5.649µ ±  2%    5.743µ ±  1%   +1.67% (p=0.041 n=6) BM_DynamicUpdateSliceF32/512/process_time                                                 13.40µ ±  4%    13.63µ ±  3%   +1.68% (p=0.041 n=6) BM_DynamicUpdateSliceF32/8192/process_time                                                447.0µ ±  8%    422.9µ ±  4%   5.39% (p=0.041 n=6) BM_AddBF16/1024/process_time                                                              199.3µ ±  3%    166.9µ ±  0%  16.24% (p=0.002 n=6) BM_AddBF16/128/process_time                                                               10.16µ ±  0%    10.15µ ±  0%        ~ (p=0.132 n=6) BM_AddBF16/16384/process_time                                                             2.255m ± 12%    2.279m ± 18%        ~ (p=0.937 n=6) BM_AddBF16/256/process_time                                                               38.19µ ±  4%    31.23µ ±  1%  18.23% (p=0.002 n=6) BM_AddBF16/32768/process_time                                                             6.166m ±  5%    6.915m ± 11%  +12.15% (p=0.009 n=6) BM_AddBF16/512/process_time                                                              105.86µ ±  1%    85.75µ ±  1%  19.00% (p=0.002 n=6) BM_AddBF16/8192/process_time                                                              1.096m ±  7%    1.034m ±  9%        ~ (p=0.310 n=6) BM_AddF32/1024/process_time                                                               322.9µ ±  4%    303.1µ ±  5%   6.13% (p=0.004 n=6) BM_AddF32/128/process_time                                                                21.62µ ±  1%    19.13µ ±  2%  11.51% (p=0.002 n=6) BM_AddF32/16384/process_time                                                              7.754m ± 33%    5.547m ± 35%        ~ (p=0.093 n=6) BM_AddF32/256/process_time                                                                67.31µ ±  1%    58.99µ ±  2%  12.36% (p=0.002 n=6) BM_AddF32/32768/process_time                                                              15.90m ±  9%    15.18m ±  9%        ~ (p=0.180 n=6) BM_AddF32/512/process_time                                                                144.5µ ±  1%    131.3µ ±  1%   9.17% (p=0.002 n=6) BM_AddF32/8192/process_time                                                               1.859m ± 17%    1.755m ± 13%        ~ (p=0.180 n=6) BM_ConvertF32ToBF16/1024/process_time                                                     179.5µ ±  2%    174.6µ ±  1%   2.77% (p=0.002 n=6) BM_ConvertF32ToBF16/128/process_time                                                      5.814µ ±  1%    5.926µ ±  1%   +1.93% (p=0.004 n=6) BM_ConvertF32ToBF16/16384/process_time                                                    1.998m ±  8%    1.992m ± 13%        ~ (p=0.589 n=6) BM_ConvertF32ToBF16/256/process_time                                                      29.26µ ±  1%    28.69µ ±  2%   1.92% (p=0.004 n=6) BM_ConvertF32ToBF16/32768/process_time                                                    6.877m ± 46%    6.017m ± 12%  12.50% (p=0.004 n=6) BM_ConvertF32ToBF16/512/process_time                                                      91.14µ ±  4%    83.40µ ±  1%   8.49% (p=0.002 n=6) BM_ConvertF32ToBF16/8192/process_time                                                     1.091m ± 10%    1.066m ± 13%        ~ (p=0.818 n=6) BM_BcastFusionF32/1024/process_time                                                       294.0µ ±  2%    262.2µ ±  2%  10.80% (p=0.002 n=6) BM_BcastFusionF32/128/process_time                                                        22.16µ ±  4%    16.54µ ±  1%  25.36% (p=0.002 n=6) BM_BcastFusionF32/16384/process_time                                                      3.567m ±  7%    3.486m ± 11%        ~ (p=1.000 n=6) BM_BcastFusionF32/256/process_time                                                        57.94µ ±  3%    47.47µ ±  3%  18.08% (p=0.002 n=6) BM_BcastFusionF32/512/process_time                                                        125.8µ ±  4%    104.9µ ±  4%  16.58% (p=0.002 n=6) BM_BcastFusionF32/8192/process_time                                                       1.779m ±  9%    1.685m ± 13%        ~ (p=0.132 n=6) BM_ChainOfAddF32/1024/process_time                                                        74.55µ ±  3%    71.41µ ±  1%   4.21% (p=0.002 n=6) BM_ChainOfAddF32/128/process_time                                                         12.63µ ±  1%    12.46µ ±  1%   1.36% (p=0.002 n=6) BM_ChainOfAddF32/256/process_time                                                         20.47µ ±  2%    20.09µ ±  2%        ~ (p=0.093 n=6) BM_ChainOfAddF32/512/process_time                                                         37.49µ ±  3%    36.21µ ±  1%   3.42% (p=0.002 n=6) BM_ChainOfAddF32/64/process_time                                                          8.909µ ±  3%    8.844µ ±  2%        ~ (p=0.132 n=6) BM_DynamicUpdateSliceFusionF32/1024/process_time                                          26.50µ ±  3%    25.52µ ±  4%   3.69% (p=0.009 n=6) BM_DynamicUpdateSliceFusionF32/128/process_time                                           2.936µ ±  1%    2.895µ ±  0%   1.40% (p=0.002 n=6) BM_DynamicUpdateSliceFusionF32/16384/process_time                                         786.4µ ±  5%    785.2µ ±  4%        ~ (p=0.937 n=6) BM_DynamicUpdateSliceFusionF32/256/process_time                                           5.539µ ±  2%    5.466µ ±  1%   1.30% (p=0.002 n=6) BM_DynamicUpdateSliceFusionF32/512/process_time                                           13.49µ ±  1%    13.84µ ±  2%   +2.58% (p=0.026 n=6) BM_DynamicUpdateSliceFusionF32/8192/process_time                                          428.0µ ±  4%    409.5µ ±  6%        ~ (p=0.132 n=6) BM_FusionF32/1024/process_time                                                            339.0µ ±  3%    306.2µ ±  1%   9.67% (p=0.002 n=6) BM_FusionF32/128/process_time                                                             26.59µ ± 14%    19.27µ ±  3%  27.55% (p=0.002 n=6) BM_FusionF32/16384/process_time                                                           6.154m ± 20%    6.126m ± 11%        ~ (p=1.000 n=6) BM_FusionF32/256/process_time                                                             73.46µ ±  2%    59.92µ ±  2%  18.43% (p=0.002 n=6) BM_FusionF32/512/process_time                                                             162.4µ ±  1%    132.1µ ±  1%  18.61% (p=0.002 n=6) BM_FusionF32/8192/process_time                                                            2.007m ±  8%    2.039m ± 12%        ~ (p=0.937 n=6) BM_FusionF32_2/160/process_time                                                           1.257µ ±  1%    1.231µ ±  1%   2.05% (p=0.002 n=6) BM_FusionF32_2/240/process_time                                                           1.533µ ±  1%    1.507µ ±  1%   1.66% (p=0.002 n=6) BM_FusionF32_2/40/process_time                                                            848.2n ±  1%    830.0n ±  1%   2.15% (p=0.002 n=6) BM_FusionF32_2/80/process_time                                                            993.7n ±  2%    955.7n ±  3%   3.82% (p=0.004 n=6) BM_GatherS32/10/128/1/process_time                                                        466.3n ± 12%    463.8n ±  1%        ~ (p=0.310 n=6) BM_GatherS32/10/128/2/process_time                                                        473.0n ±  1%    470.3n ±  1%   0.57% (p=0.015 n=6) BM_GatherS32/10/128/32/process_time                                                       648.0n ±  2%    636.9n ±  1%   1.72% (p=0.026 n=6) BM_GatherS32/10/256/1/process_time                                                        473.8n ±  1%    472.5n ±  7%        ~ (p=0.699 n=6) BM_GatherS32/10/256/2/process_time                                                        484.8n ±  1%    480.4n ±  1%   0.91% (p=0.015 n=6) BM_GatherS32/10/256/64/process_time                                                       1.209µ ±  2%    1.215µ ±  3%        ~ (p=0.310 n=6) BM_GatherS32/10/3/1/process_time                                                          457.0n ±  3%    457.0n ±  1%        ~ (p=0.937 n=6) BM_GatherS32/10/3/2/process_time                                                          464.9n ±  3%    459.4n ±  1%        ~ (p=0.132 n=6) BM_GatherS32/10/3/4/process_time                                                          466.6n ±  3%    463.6n ±  1%        ~ (p=0.180 n=6) BM_GatherS32/10/32/1/process_time                                                         460.8n ±  1%    460.7n ±  1%        ~ (p=1.000 n=6) BM_GatherS32/10/32/2/process_time                                                         465.6n ±  2%    460.9n ±  5%        ~ (p=0.180 n=6) BM_GatherS32/10/32/8/process_time                                                         473.4n ±  1%    466.9n ±  0%   1.37% (p=0.002 n=6) BM_GatherS32/10/512/1/process_time                                                        485.7n ±  2%    485.8n ±  9%        ~ (p=0.699 n=6) BM_GatherS32/10/512/128/process_time                                                      3.955µ ±  2%    3.934µ ±  0%        ~ (p=0.240 n=6) BM_GatherS32/10/512/2/process_time                                                        511.5n ±  2%    507.1n ±  1%   0.86% (p=0.002 n=6) BM_GatherS32/10/64/1/process_time                                                         462.4n ±  4%    457.5n ±  3%        ~ (p=0.093 n=6) BM_GatherS32/10/64/16/process_time                                                        501.8n ±  1%    498.0n ±  1%        ~ (p=0.093 n=6) BM_GatherS32/10/64/2/process_time                                                         465.3n ±  2%    463.0n ±  2%        ~ (p=0.240 n=6) BM_GatherS32/100/128/1/process_time                                                       466.7n ±  3%    460.8n ±  1%   1.26% (p=0.026 n=6) BM_GatherS32/100/128/2/process_time                                                       477.7n ±  1%    469.9n ±  2%   1.63% (p=0.015 n=6) BM_GatherS32/100/128/32/process_time                                                      770.8n ±  1%    773.6n ±  1%        ~ (p=0.394 n=6) BM_GatherS32/100/256/1/process_time                                                       475.0n ±  8%    470.7n ±  1%        ~ (p=0.065 n=6) BM_GatherS32/100/256/2/process_time                                                       484.8n ±  2%    478.0n ±  1%   1.39% (p=0.002 n=6) BM_GatherS32/100/256/64/process_time                                                      1.600µ ±  3%    1.596µ ±  0%        ~ (p=0.240 n=6) BM_GatherS32/100/3/1/process_time                                                         460.4n ±  2%    455.0n ±  1%   1.17% (p=0.002 n=6) BM_GatherS32/100/3/2/process_time                                                         463.6n ±  2%    460.4n ±  1%        ~ (p=0.180 n=6) BM_GatherS32/100/3/4/process_time                                                         467.2n ±  1%    463.2n ±  1%   0.86% (p=0.009 n=6) BM_GatherS32/100/32/1/process_time                                                        462.3n ±  1%    460.7n ±  2%        ~ (p=0.310 n=6) BM_GatherS32/100/32/2/process_time                                                        465.3n ±  1%    461.9n ±  1%        ~ (p=0.240 n=6) BM_GatherS32/100/32/8/process_time                                                        476.2n ±  1%    469.4n ±  1%   1.43% (p=0.009 n=6) BM_GatherS32/100/512/1/process_time                                                       488.4n ±  1%    484.6n ±  1%   0.77% (p=0.041 n=6) BM_GatherS32/100/512/128/process_time                                                     5.184µ ±  3%    5.174µ ±  1%        ~ (p=0.589 n=6) BM_GatherS32/100/512/2/process_time                                                       510.9n ±  1%    506.5n ±  1%        ~ (p=0.065 n=6) BM_GatherS32/100/64/1/process_time                                                        462.8n ±  3%    458.6n ±  1%   0.90% (p=0.002 n=6) BM_GatherS32/100/64/16/process_time                                                       507.3n ±  1%    499.3n ±  1%   1.58% (p=0.002 n=6) BM_GatherS32/100/64/2/process_time                                                        466.0n ±  1%    462.6n ±  0%   0.73% (p=0.002 n=6) BM_GatherS32/3/128/1/process_time                                                         465.4n ±  1%    462.0n ±  3%        ~ (p=0.180 n=6) BM_GatherS32/3/128/2/process_time                                                         473.2n ±  2%    469.4n ±  3%        ~ (p=0.132 n=6) BM_GatherS32/3/128/32/process_time                                                        629.8n ±  2%    633.4n ±  5%        ~ (p=0.937 n=6) BM_GatherS32/3/256/1/process_time                                                         472.3n ±  0%    470.1n ±  1%        ~ (p=0.818 n=6) BM_GatherS32/3/256/2/process_time                                                         486.1n ±  1%    479.1n ±  8%        ~ (p=0.180 n=6) BM_GatherS32/3/256/64/process_time                                                        1.100µ ±  1%    1.104µ ±  5%        ~ (p=1.000 n=6) BM_GatherS32/3/3/1/process_time                                                           460.6n ±  1%    460.3n ±  2%        ~ (p=0.818 n=6) BM_GatherS32/3/3/2/process_time                                                           465.4n ±  1%    462.2n ±  7%        ~ (p=0.589 n=6) BM_GatherS32/3/3/4/process_time                                                           466.4n ±  1%    465.6n ±  2%        ~ (p=0.699 n=6) BM_GatherS32/3/32/1/process_time                                                          461.6n ±  2%    460.4n ±  1%        ~ (p=0.240 n=6) BM_GatherS32/3/32/2/process_time                                                          464.6n ±  1%    461.4n ±  5%        ~ (p=0.394 n=6) BM_GatherS32/3/32/8/process_time                                                          475.2n ±  6%    468.5n ±  1%   1.41% (p=0.009 n=6) BM_GatherS32/3/512/1/process_time                                                         484.4n ±  3%    484.6n ±  1%        ~ (p=0.937 n=6) BM_GatherS32/3/512/128/process_time                                                       3.068µ ±  2%    3.070µ ±  1%        ~ (p=0.937 n=6) BM_GatherS32/3/512/2/process_time                                                         512.9n ±  3%    508.1n ±  2%        ~ (p=0.310 n=6) BM_GatherS32/3/64/1/process_time                                                          461.5n ±  2%    457.4n ±  1%   0.88% (p=0.004 n=6) BM_GatherS32/3/64/16/process_time                                                         496.8n ±  1%    498.9n ±  1%        ~ (p=0.589 n=6) BM_GatherS32/3/64/2/process_time                                                          465.5n ±  1%    462.4n ±  1%        ~ (p=0.065 n=6) BM_Optimizer0/1024/process_time                                                           5.001m ±  1%    4.892m ± 11%        ~ (p=0.394 n=6) BM_Optimizer0/128/process_time                                                            460.4µ ± 12%    380.2µ ±  2%  17.41% (p=0.002 n=6) BM_Optimizer0/16384/process_time                                                          62.65m ±  6%    62.65m ± 12%        ~ (p=0.937 n=6) BM_Optimizer0/256/process_time                                                            1.215m ± 10%    1.066m ±  3%  12.21% (p=0.004 n=6) BM_Optimizer0/512/process_time                                                            2.436m ± 18%    2.236m ±  3%   8.21% (p=0.002 n=6) BM_Optimizer0/8192/process_time                                                           35.19m ±  5%    32.69m ±  6%        ~ (p=0.065 n=6) BM_PadF32/1024/process_time                                                               29.85m ±  8%    28.86m ± 10%        ~ (p=0.310 n=6) BM_PadF32/128/process_time                                                                340.1µ ±  1%    318.1µ ±  0%   6.48% (p=0.002 n=6) BM_PadF32/256/process_time                                                                1.178m ±  5%    1.141m ±  4%        ~ (p=0.065 n=6) BM_PadF32/4096/process_time                                                               423.3m ±  0%    423.2m ±  3%        ~ (p=0.937 n=6) BM_PadF32/512/process_time                                                                4.229m ± 10%    4.489m ±  8%        ~ (p=0.240 n=6) BM_ReduceAddBF16/1024/process_time                                                        3.387m ±  3%    3.409m ±  6%        ~ (p=0.310 n=6) BM_ReduceAddBF16/128/process_time                                                         239.0µ ±  0%    239.5µ ±  0%   +0.22% (p=0.002 n=6) BM_ReduceAddBF16/16384/process_time                                                       51.13m ±  1%    53.88m ±  6%        ~ (p=0.132 n=6) BM_ReduceAddBF16/256/process_time                                                         719.8µ ±  8%   1121.9µ ± 13%  +55.87% (p=0.002 n=6) BM_ReduceAddBF16/512/process_time                                                         1.674m ±  2%    2.152m ±  7%  +28.60% (p=0.002 n=6) BM_ReduceAddBF16/8192/process_time                                                        25.97m ±  1%    27.34m ±  8%        ~ (p=0.065 n=6) BM_ReduceAddF32/1024/process_time                                                         487.0µ ±  2%    469.0µ ±  5%   3.70% (p=0.041 n=6) BM_ReduceAddF32/128/process_time                                                          23.40µ ±  0%    23.42µ ±  0%        ~ (p=0.310 n=6) BM_ReduceAddF32/16384/process_time                                                        6.683m ±  0%    6.657m ±  0%   0.39% (p=0.002 n=6) BM_ReduceAddF32/256/process_time                                                          62.92µ ±  4%    60.37µ ±  1%   4.05% (p=0.002 n=6) BM_ReduceAddF32/512/process_time                                                          246.6µ ±  0%    238.0µ ±  0%   3.48% (p=0.002 n=6) BM_ReduceAddF32/8192/process_time                                                         3.471m ±  0%    3.459m ±  1%        ~ (p=0.132 n=6) BM_ScatterS32_R1/262144/262144/process_time                                               600.1µ ±  1%    206.5µ ±  2%  65.59% (p=0.002 n=6) BM_ScatterS32_R2/512/512/process_time                                                     77.45µ ±  1%    49.88µ ±  3%  35.59% (p=0.002 n=6) BM_ScatterS32_R3/64/64/process_time                                                       54.28µ ±  0%    50.49µ ±  3%   6.99% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:1/d1:64/d2:8/num_slices:1/process_time                    755.0n ±  1%    702.3n ±  4%   6.98% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:50/d1:64/d2:8/num_slices:10/process_time                 106.82µ ±  0%    17.66µ ±  3%  83.47% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:500/d1:64/d2:8/num_slices:100/process_time               10.301m ±  2%    2.635m ±  2%  74.42% (p=0.002 n=6) BM_SelectAndScatterF32/128/process_time                                                   37.00µ ±  1%    27.76µ ±  3%  24.97% (p=0.002 n=6) BM_SelectAndScatterF32/256/process_time                                                  117.86µ ±  2%    86.36µ ±  1%  26.73% (p=0.002 n=6) BM_SelectAndScatterF32/512/process_time                                                   1.613m ±  2%    1.528m ± 61%        ~ (p=0.065 n=6) BM_TanhF16/1024/process_time                                                              687.7n ±  1%    686.2n ±  2%        ~ (p=0.589 n=6) BM_TanhF16/128/process_time                                                               439.0n ±  2%    443.6n ±  1%        ~ (p=0.180 n=6) BM_TanhF16/256/process_time                                                               481.3n ±  1%    482.0n ±  1%        ~ (p=0.240 n=6) BM_TanhF16/4096/process_time                                                              1.522µ ±  0%    1.522µ ±  1%        ~ (p=0.818 n=6) BM_TanhF16/512/process_time                                                               553.5n ±  0%    556.9n ±  2%        ~ (p=0.394 n=6) BM_TanhF32/1024/process_time                                                              751.8n ±  1%    693.2n ±  1%   7.79% (p=0.002 n=6) BM_TanhF32/128/process_time                                                               507.8n ±  0%    451.7n ±  3%  11.05% (p=0.002 n=6) BM_TanhF32/256/process_time                                                               552.0n ±  1%    494.1n ±  1%  10.49% (p=0.002 n=6) BM_TanhF32/4096/process_time                                                              1.608µ ±  2%    1.580µ ±  1%   1.75% (p=0.026 n=6) BM_TanhF32/512/process_time                                                               623.4n ±  1%    559.9n ±  1%  10.18% (p=0.002 n=6) BM_TanhF64/1024/process_time                                                              12.59µ ±  0%    12.60µ ±  0%        ~ (p=0.699 n=6) BM_TanhF64/128/process_time                                                               1.945µ ±  0%    1.947µ ±  0%        ~ (p=0.937 n=6) BM_TanhF64/256/process_time                                                               3.464µ ±  0%    3.471µ ±  1%        ~ (p=0.310 n=6) BM_TanhF64/4096/process_time                                                              49.05µ ±  0%    49.05µ ±  0%        ~ (p=0.937 n=6) BM_TanhF64/512/process_time                                                               6.506µ ±  0%    6.522µ ±  1%        ~ (p=0.132 n=6) geomean                                                                                   91.38µ          88.09µ         3.59% ``` ",2025-03-13T02:19:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89124
opt,copybara-service[bot],[xla:cpu] flip xla_cpu_use_fusion_emitters to true,"[xla:cpu] flip xla_cpu_use_fusion_emitters to true Note, however, that no fusion emitter is enabled yet. Flipping this flag does change the HLO pipeline, and is therefore worth submitting as its own CL to get test coverage and measure performance differences. The largest perf difference can be seen in Gather:  Microbenchmarks thunks vs. fusion emitters ```                                                                                     │ thunks           │           fusion_emitters            │                                                                                     │    cpusec/op    │  cpusec/op    vs base               │ BM_ConcatenateTwoR3F32/parallel:0/batch:128/width:64/height:256/axis:0/process_time       602.3µ ±  0%    603.0µ ±  1%        ~ (p=0.485 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:128/width:64/height:256/axis:1/process_time       624.8µ ±  2%    806.0µ ±  1%  +29.00% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:128/width:64/height:256/axis:2/process_time       998.4µ ±  0%   1127.5µ ±  1%  +12.93% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:256/width:128/height:64/axis:0/process_time       605.0µ ±  4%    609.5µ ±  2%        ~ (p=0.132 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:256/width:128/height:64/axis:1/process_time       620.0µ ±  2%    854.8µ ± 28%        ~ (p=0.132 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:256/width:128/height:64/axis:2/process_time       1.284m ±  3%    1.936m ±  7%  +50.79% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:64/width:256/height:128/axis:0/process_time       602.2µ ±  3%    602.8µ ±  1%        ~ (p=0.699 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:64/width:256/height:128/axis:1/process_time       615.2µ ±  0%    828.1µ ±  8%  +34.61% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:64/width:256/height:128/axis:2/process_time       1.031m ±  1%    1.175m ±  6%  +13.99% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:128/width:64/height:256/axis:0/process_time       1.753m ± 13%    1.650m ±  8%        ~ (p=0.180 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:128/width:64/height:256/axis:1/process_time       1.711m ±  6%    1.804m ± 11%        ~ (p=0.310 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:128/width:64/height:256/axis:2/process_time       2.602m ±  8%    2.434m ±  7%   6.47% (p=0.041 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:256/width:128/height:64/axis:0/process_time       1.684m ± 12%    1.730m ± 14%        ~ (p=0.485 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:256/width:128/height:64/axis:1/process_time       1.798m ± 10%    1.668m ±  5%   7.25% (p=0.026 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:256/width:128/height:64/axis:2/process_time       2.287m ± 12%    2.251m ±  7%        ~ (p=0.180 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:64/width:256/height:128/axis:0/process_time       1.703m ±  7%    1.666m ±  6%        ~ (p=0.485 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:64/width:256/height:128/axis:1/process_time       1.727m ±  5%    1.717m ±  6%        ~ (p=0.589 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:64/width:256/height:128/axis:2/process_time       2.232m ± 11%    2.235m ± 16%        ~ (p=0.818 n=6) BM_Conv1DStrided/1/129/process_time                                                       54.79m ±  8%    54.83m ±  6%        ~ (p=0.699 n=6) BM_Conv1DStrided/3/129/process_time                                                       171.4m ±  9%    170.4m ±  6%        ~ (p=1.000 n=6) BM_Conv1DTransposedStrided/129/1/process_time                                             47.47m ±  3%    53.37m ±  7%  +12.43% (p=0.002 n=6) BM_Conv1DTransposedStrided/129/3/process_time                                             155.8m ±  5%    155.1m ±  9%        ~ (p=0.937 n=6) BM_Conv1DTransposedStridedNonDefaultLayout/129/1/process_time                             37.36m ±  9%    34.52m ±  2%        ~ (p=0.180 n=6) BM_Conv1DTransposedStridedNonDefaultLayout/129/3/process_time                             122.1m ±  5%    128.4m ±  7%        ~ (p=0.132 n=6) BM_Conv2D/16/32/32/128/1/1/1024/process_time                                         123.6m ± 21%    121.2m ± 14%        ~ (p=0.180 n=6) BM_Conv2D/16/32/32/128/3/3/1024/process_time                                         888.2m ± 10%   1032.4m ± 27%        ~ (p=0.093 n=6) BM_Conv2D/32/256/256/4/1/1/16/process_time                                           202.9m ±  8%    190.3m ±  8%   6.24% (p=0.026 n=6) BM_Conv2D/32/256/256/4/3/3/16/process_time                                           427.8m ± 15%    376.0m ±  5%  12.10% (p=0.041 n=6) BM_Conv2D/32/32/32/128/1/1/1024/process_time                                         249.9m ±  4%    249.0m ±  3%        ~ (p=0.937 n=6) BM_Conv2D/32/32/32/128/3/3/1024/process_time                                          1.948 ±  8%     1.890 ±  6%        ~ (p=0.310 n=6) BM_Conv2D/32/32/32/96/1/1/96/process_time                                            21.98m ±  8%    22.39m ±  3%        ~ (p=0.818 n=6) BM_Conv2D/32/32/32/96/3/3/96/process_time                                            141.6m ±  4%    144.9m ±  3%        ~ (p=0.310 n=6) BM_Conv2D/32/64/64/32/1/1/64/process_time                                            29.17m ±  3%    27.90m ±  4%   4.35% (p=0.015 n=6) BM_Conv2D/32/64/64/32/3/3/64/process_time                                            161.0m ±  4%    159.4m ±  3%        ~ (p=0.310 n=6) BM_Conv2D/32/64/64/4/1/1/16/process_time                                             1.202m ±  5%    1.207m ± 14%        ~ (p=0.699 n=6) BM_Conv2D/32/64/64/4/3/3/16/process_time                                             26.16m ±  4%    26.60m ±  3%        ~ (p=0.240 n=6) BM_Conv2D/8/128/128/4/1/1/8/process_time                                             457.0µ ±  4%    453.2µ ±  4%        ~ (p=0.589 n=6) BM_Conv2D/8/128/128/4/3/3/8/process_time                                             14.59m ±  4%    14.81m ±  4%        ~ (p=0.485 n=6) BM_Conv2D/8/32/32/128/1/1/1024/process_time                                          62.50m ± 10%    57.55m ±  8%        ~ (p=0.065 n=6) BM_Conv2D/8/32/32/128/3/3/1024/process_time                                          449.5m ±  6%    451.0m ±  6%        ~ (p=0.937 n=6) BM_Conv2D/8/5/5/1/1/1/32/process_time                                                3.269µ ±  4%    3.262µ ±  1%        ~ (p=0.589 n=6) BM_Conv2D/8/5/5/1/3/3/32/process_time                                                13.38µ ±  9%    13.31µ ±  0%   0.52% (p=0.009 n=6) BM_Conv2D/8/5/5/4/1/1/32/process_time                                                1.821µ ±  1%    1.842µ ±  0%   +1.10% (p=0.041 n=6) BM_Conv2D/8/5/5/4/3/3/32/process_time                                                17.93µ ±  4%    18.00µ ±  2%        ~ (p=0.310 n=6) BM_Conv2DStrided/process_time                                                             57.50m ±  5%    59.22m ±  6%        ~ (p=0.394 n=6) BM_Conv2DTransposedStrided/process_time                                                   47.48m ±  2%    48.09m ±  4%        ~ (p=0.310 n=6) BM_GroupedConv2D/1/45/45/1024/5/5/1024/1024/process_time                                  439.2m ±  5%    425.9m ±  3%        ~ (p=0.093 n=6) BM_GroupedConv2DStrided/128/128/128/process_time                                          65.37m ±  8%    60.74m ±  6%        ~ (p=0.132 n=6) BM_GroupedConv2DStrided/128/128/16/process_time                                           57.54m ±  7%    60.12m ± 11%        ~ (p=0.485 n=6) BM_GroupedConv2DTransposedStrided/128/128/128/process_time                                 4.373 ±  4%     4.342 ±  5%        ~ (p=0.937 n=6) BM_GroupedConv2DTransposedStrided/128/128/16/process_time                                  4.997 ±  5%     4.696 ±  7%   6.02% (p=0.026 n=6) BM_CustomCall_16FloatBuffers/process_time                                                 2.039µ ±  4%    1.916µ ±  4%   5.99% (p=0.004 n=6) BM_CustomCall_16IntAttributes/process_time                                                677.2n ±  2%    642.6n ±  1%   5.10% (p=0.002 n=6) BM_CustomCall_Minimal/process_time                                                        590.2n ±  1%    557.3n ±  1%   5.57% (p=0.002 n=6) BM_DagExecution/1024/process_time                                                         7.481m ±  1%    7.649m ±  9%   +2.24% (p=0.009 n=6) BM_DagExecution/128/process_time                                                          682.5µ ±  5%    689.5µ ±  2%        ~ (p=0.065 n=6) BM_DagExecution/16384/process_time                                                        171.4m ±  9%    197.4m ± 13%  +15.17% (p=0.015 n=6) BM_DagExecution/256/process_time                                                          1.926m ±  4%    1.939m ±  1%        ~ (p=0.699 n=6) BM_DagExecution/512/process_time                                                          3.449m ±  1%    3.467m ±  2%        ~ (p=0.240 n=6) BM_DagExecution/8192/process_time                                                         59.51m ± 15%    62.34m ± 15%        ~ (p=0.310 n=6) BM_BatchedDot/11/1/128/process_time                                                       37.62µ ±  0%    37.50µ ±  0%   0.33% (p=0.002 n=6) BM_BatchedDot/11/1/2/process_time                                                         488.7n ± 11%    458.3n ±  3%   6.21% (p=0.002 n=6) BM_BatchedDot/11/1/256/process_time                                                       766.9µ ±  3%    769.9µ ±  3%        ~ (p=0.818 n=6) BM_BatchedDot/11/1/32/process_time                                                        1.770µ ±  2%    1.716µ ±  0%   3.05% (p=0.002 n=6) BM_BatchedDot/11/1/512/process_time                                                       9.177m ±  1%    9.440m ±  2%   +2.86% (p=0.004 n=6) BM_BatchedDot/11/1/64/process_time                                                        6.431µ ±  2%    6.376µ ±  0%   0.85% (p=0.002 n=6) BM_BatchedDot/11/2/128/process_time                                                       74.44µ ±  0%    74.38µ ±  0%        ~ (p=0.310 n=6) BM_BatchedDot/11/2/2/process_time                                                         495.0n ±  1%    462.9n ±  4%   6.49% (p=0.002 n=6) BM_BatchedDot/11/2/256/process_time                                                       1.623m ± 26%    1.575m ±  3%        ~ (p=1.000 n=6) BM_BatchedDot/11/2/32/process_time                                                        2.804µ ±  0%    2.750µ ±  0%   1.92% (p=0.002 n=6) BM_BatchedDot/11/2/512/process_time                                                       26.11m ±  3%    22.17m ±  2%  15.08% (p=0.002 n=6) BM_BatchedDot/11/2/64/process_time                                                        12.10µ ±  1%    12.00µ ±  1%   0.81% (p=0.015 n=6) BM_BatchedDot/11/4/128/process_time                                                       148.8µ ±  0%    148.6µ ±  0%   0.13% (p=0.002 n=6) BM_BatchedDot/11/4/2/process_time                                                         500.0n ±  1%    465.5n ±  2%   6.91% (p=0.002 n=6) BM_BatchedDot/11/4/256/process_time                                                       3.716m ±  2%    3.192m ±  4%  14.11% (p=0.002 n=6) BM_BatchedDot/11/4/32/process_time                                                        4.818µ ±  1%    4.788µ ±  4%        ~ (p=0.394 n=6) BM_BatchedDot/11/4/512/process_time                                                       47.46m ±  3%    46.98m ±  1%        ~ (p=0.065 n=6) BM_BatchedDot/11/4/64/process_time                                                        23.36µ ±  0%    23.26µ ±  0%   0.45% (p=0.002 n=6) BM_BatchedDot/11/8/128/process_time                                                       298.0µ ±  0%    297.8µ ±  0%   0.05% (p=0.041 n=6) BM_BatchedDot/11/8/2/process_time                                                         514.2n ±  2%    475.1n ±  1%   7.60% (p=0.002 n=6) BM_BatchedDot/11/8/256/process_time                                                       7.223m ±  7%    6.327m ±  4%  12.40% (p=0.002 n=6) BM_BatchedDot/11/8/32/process_time                                                        8.787µ ±  0%    8.739µ ±  0%   0.55% (p=0.002 n=6) BM_BatchedDot/11/8/512/process_time                                                      101.90m ±  5%    98.02m ±  3%        ~ (p=0.180 n=6) BM_BatchedDot/11/8/64/process_time                                                        46.00µ ±  0%    45.93µ ±  0%        ~ (p=0.065 n=6) BM_BatchedDot/16/1/128/process_time                                                       40.95µ ±  0%    40.84µ ±  0%   0.27% (p=0.002 n=6) BM_BatchedDot/16/1/2/process_time                                                         557.7n ±  5%    523.7n ±  6%   6.10% (p=0.004 n=6) BM_BatchedDot/16/1/256/process_time                                                       1.113m ±  4%    1.078m ±  3%        ~ (p=0.093 n=6) BM_BatchedDot/16/1/32/process_time                                                        2.059µ ±  1%    2.001µ ±  1%   2.85% (p=0.002 n=6) BM_BatchedDot/16/1/512/process_time                                                       11.57m ±  2%    11.10m ±  1%   4.02% (p=0.002 n=6) BM_BatchedDot/16/1/64/process_time                                                        7.363µ ±  1%    7.293µ ±  1%   0.95% (p=0.002 n=6) BM_BatchedDot/16/2/128/process_time                                                       81.31µ ±  0%    81.00µ ±  0%   0.38% (p=0.002 n=6) BM_BatchedDot/16/2/2/process_time                                                         594.0n ±  4%    530.4n ±  2%  10.71% (p=0.002 n=6) BM_BatchedDot/16/2/256/process_time                                                       2.270m ±  1%    2.212m ±  4%        ~ (p=0.180 n=6) BM_BatchedDot/16/2/32/process_time                                                        3.348µ ±  1%    3.297µ ±  1%   1.54% (p=0.002 n=6) BM_BatchedDot/16/2/512/process_time                                                       25.68m ±  2%    25.40m ±  2%        ~ (p=0.485 n=6) BM_BatchedDot/16/2/64/process_time                                                        13.89µ ±  1%    13.73µ ±  0%   1.19% (p=0.002 n=6) BM_BatchedDot/16/4/128/process_time                                                       163.8µ ±  0%    163.6µ ±  0%        ~ (p=0.093 n=6) BM_BatchedDot/16/4/2/process_time                                                         607.9n ±  1%    542.8n ±  1%  10.71% (p=0.002 n=6) BM_BatchedDot/16/4/256/process_time                                                       4.782m ±  1%    4.668m ±  2%   2.39% (p=0.009 n=6) BM_BatchedDot/16/4/32/process_time                                                        5.754µ ±  1%    5.675µ ±  2%   1.38% (p=0.041 n=6) BM_BatchedDot/16/4/512/process_time                                                       56.58m ±  3%    55.00m ±  1%   2.78% (p=0.015 n=6) BM_BatchedDot/16/4/64/process_time                                                        26.67µ ±  0%    26.59µ ±  0%        ~ (p=0.093 n=6) BM_BatchedDot/16/8/128/process_time                                                       749.6µ ± 16%    753.7µ ±  4%        ~ (p=0.180 n=6) BM_BatchedDot/16/8/2/process_time                                                         632.7n ±  4%    636.7n ±  1%        ~ (p=0.699 n=6) BM_BatchedDot/16/8/256/process_time                                                       9.479m ±  2%    9.313m ±  2%   1.75% (p=0.009 n=6) BM_BatchedDot/16/8/32/process_time                                                        10.52µ ±  0%    10.53µ ±  0%        ~ (p=0.699 n=6) BM_BatchedDot/16/8/512/process_time                                                       114.5m ±  7%    113.0m ±  4%        ~ (p=0.240 n=6) BM_BatchedDot/16/8/64/process_time                                                        52.56µ ±  0%    52.50µ ±  0%   0.13% (p=0.015 n=6) BM_DynamicUpdateSliceF32/1024/process_time                                                25.78µ ±  1%    25.95µ ±  3%        ~ (p=0.065 n=6) BM_DynamicUpdateSliceF32/128/process_time                                                 3.151µ ±  3%    3.026µ ±  0%   3.97% (p=0.002 n=6) BM_DynamicUpdateSliceF32/16384/process_time                                               798.0µ ±  7%    764.4µ ±  4%   4.22% (p=0.015 n=6) BM_DynamicUpdateSliceF32/256/process_time                                                 5.810µ ±  1%    5.680µ ±  3%        ~ (p=0.065 n=6) BM_DynamicUpdateSliceF32/512/process_time                                                 13.70µ ±  3%    13.65µ ±  3%        ~ (p=0.240 n=6) BM_DynamicUpdateSliceF32/8192/process_time                                                435.5µ ±  2%    427.8µ ±  5%        ~ (p=0.180 n=6) BM_AddBF16/1024/process_time                                                              198.4µ ±  1%    183.6µ ±  2%   7.45% (p=0.002 n=6) BM_AddBF16/128/process_time                                                               10.23µ ±  0%    10.17µ ±  0%   0.56% (p=0.002 n=6) BM_AddBF16/16384/process_time                                                             2.231m ±  4%    2.164m ± 27%        ~ (p=1.000 n=6) BM_AddBF16/256/process_time                                                               37.87µ ±  3%    33.62µ ±  1%  11.23% (p=0.002 n=6) BM_AddBF16/32768/process_time                                                             6.420m ± 15%    5.611m ± 16%  12.61% (p=0.015 n=6) BM_AddBF16/512/process_time                                                              110.95µ ±  1%    95.40µ ±  1%  14.02% (p=0.002 n=6) BM_AddBF16/8192/process_time                                                              1.050m ±  6%    1.022m ±  7%        ~ (p=0.589 n=6) BM_AddF32/1024/process_time                                                               343.5µ ±  5%    320.5µ ±  4%   6.70% (p=0.002 n=6) BM_AddF32/128/process_time                                                                21.95µ ±  1%    21.79µ ±  2%        ~ (p=0.240 n=6) BM_AddF32/16384/process_time                                                              6.284m ± 15%    5.409m ± 11%  13.91% (p=0.002 n=6) BM_AddF32/256/process_time                                                                67.42µ ±  1%    67.63µ ±  1%        ~ (p=0.485 n=6) BM_AddF32/32768/process_time                                                              15.24m ±  6%    14.96m ±  5%        ~ (p=0.394 n=6) BM_AddF32/512/process_time                                                                164.0µ ± 14%    146.4µ ±  1%        ~ (p=0.394 n=6) BM_AddF32/8192/process_time                                                               1.967m ± 19%    1.792m ± 24%   8.90% (p=0.041 n=6) BM_ConvertF32ToBF16/1024/process_time                                                     154.1µ ±  5%    185.0µ ±  1%  +20.04% (p=0.002 n=6) BM_ConvertF32ToBF16/128/process_time                                                      5.918µ ±  1%    5.861µ ±  1%        ~ (p=0.093 n=6) BM_ConvertF32ToBF16/16384/process_time                                                    1.901m ± 10%    1.973m ±  7%        ~ (p=0.589 n=6) BM_ConvertF32ToBF16/256/process_time                                                      25.28µ ±  1%    30.70µ ±  1%  +21.43% (p=0.002 n=6) BM_ConvertF32ToBF16/32768/process_time                                                    5.758m ± 45%    6.767m ± 18%        ~ (p=0.310 n=6) BM_ConvertF32ToBF16/512/process_time                                                      73.36µ ±  1%    91.98µ ±  1%  +25.38% (p=0.002 n=6) BM_ConvertF32ToBF16/8192/process_time                                                     977.4µ ± 10%   1046.1µ ±  9%        ~ (p=0.394 n=6) BM_BcastFusionF32/1024/process_time                                                       279.5µ ±  3%    274.0µ ±  2%   1.96% (p=0.015 n=6) BM_BcastFusionF32/128/process_time                                                        20.29µ ±  2%    19.91µ ±  2%   1.90% (p=0.015 n=6) BM_BcastFusionF32/16384/process_time                                                      3.623m ±  7%    3.459m ±  8%        ~ (p=0.093 n=6) BM_BcastFusionF32/256/process_time                                                        52.64µ ±  3%    52.06µ ±  9%        ~ (p=0.394 n=6) BM_BcastFusionF32/512/process_time                                                        113.9µ ±  1%    117.3µ ±  1%   +2.92% (p=0.002 n=6) BM_BcastFusionF32/8192/process_time                                                       1.739m ±  9%    1.657m ±  6%        ~ (p=0.240 n=6) BM_ChainOfAddF32/1024/process_time                                                        72.65µ ±  1%    71.52µ ±  2%   1.56% (p=0.041 n=6) BM_ChainOfAddF32/128/process_time                                                         12.72µ ±  8%    12.43µ ±  1%        ~ (p=0.394 n=6) BM_ChainOfAddF32/256/process_time                                                         20.55µ ±  1%    20.06µ ±  0%   2.39% (p=0.002 n=6) BM_ChainOfAddF32/512/process_time                                                         37.05µ ±  2%    36.13µ ±  2%   2.50% (p=0.004 n=6) BM_ChainOfAddF32/64/process_time                                                          8.835µ ±  1%    8.864µ ±  1%        ~ (p=0.394 n=6) BM_DynamicUpdateSliceFusionF32/1024/process_time                                          26.06µ ±  2%    25.82µ ±  2%        ~ (p=0.310 n=6) BM_DynamicUpdateSliceFusionF32/128/process_time                                           2.891µ ±  0%    2.952µ ±  2%   +2.12% (p=0.002 n=6) BM_DynamicUpdateSliceFusionF32/16384/process_time                                         780.4µ ±  4%    774.4µ ±  7%        ~ (p=0.394 n=6) BM_DynamicUpdateSliceFusionF32/256/process_time                                           5.538µ ±  1%    5.766µ ±  3%   +4.11% (p=0.002 n=6) BM_DynamicUpdateSliceFusionF32/512/process_time                                           13.23µ ±  2%    13.42µ ±  2%   +1.45% (p=0.041 n=6) BM_DynamicUpdateSliceFusionF32/8192/process_time                                          431.9µ ±  4%    431.0µ ±  5%        ~ (p=0.818 n=6) BM_FusionF32/1024/process_time                                                            322.1µ ±  1%    319.2µ ±  1%        ~ (p=0.310 n=6) BM_FusionF32/128/process_time                                                             22.09µ ±  2%    21.97µ ±  1%        ~ (p=0.240 n=6) BM_FusionF32/16384/process_time                                                           5.473m ± 13%    6.237m ± 10%  +13.95% (p=0.015 n=6) BM_FusionF32/256/process_time                                                             67.53µ ±  2%    68.25µ ±  8%   +1.07% (p=0.009 n=6) BM_FusionF32/512/process_time                                                             146.9µ ±  7%    147.0µ ±  7%        ~ (p=0.818 n=6) BM_FusionF32/8192/process_time                                                            1.855m ± 10%    1.989m ± 16%   +7.25% (p=0.041 n=6) BM_FusionF32_2/160/process_time                                                           1.227µ ±  4%    1.283µ ±  1%   +4.55% (p=0.002 n=6) BM_FusionF32_2/240/process_time                                                           1.501µ ±  3%    1.560µ ±  1%   +3.90% (p=0.002 n=6) BM_FusionF32_2/40/process_time                                                            827.2n ±  2%    873.4n ±  1%   +5.60% (p=0.002 n=6) BM_FusionF32_2/80/process_time                                                            963.5n ±  2%   1009.0n ±  1%   +4.73% (p=0.002 n=6) BM_GatherS32/10/128/1/process_time                                                        564.5n ± 19%    541.5n ±  2%        ~ (p=0.394 n=6) BM_GatherS32/10/128/2/process_time                                                        466.5n ±  3%    549.0n ±  2%  +17.69% (p=0.002 n=6) BM_GatherS32/10/128/32/process_time                                                       639.6n ±  5%    718.4n ±  2%  +12.31% (p=0.002 n=6) BM_GatherS32/10/256/1/process_time                                                        469.1n ±  5%    552.1n ±  0%  +17.70% (p=0.002 n=6) BM_GatherS32/10/256/2/process_time                                                        477.7n ±  1%    562.1n ±  1%  +17.66% (p=0.002 n=6) BM_GatherS32/10/256/64/process_time                                                       1.219µ ±  1%    1.286µ ±  1%   +5.50% (p=0.002 n=6) BM_GatherS32/10/3/1/process_time                                                          558.5n ±  0%    454.7n ±  3%  18.58% (p=0.002 n=6) BM_GatherS32/10/3/2/process_time                                                          565.3n ±  1%    454.6n ±  1%  19.59% (p=0.002 n=6) BM_GatherS32/10/3/4/process_time                                                          566.3n ±  0%    462.2n ±  1%  18.38% (p=0.002 n=6) BM_GatherS32/10/32/1/process_time                                                         561.1n ±  1%    458.6n ±  1%  18.26% (p=0.002 n=6) BM_GatherS32/10/32/2/process_time                                                         561.6n ±  5%    457.5n ± 21%  18.53% (p=0.002 n=6) BM_GatherS32/10/32/8/process_time                                                         573.4n ±  9%    560.8n ±  2%   2.19% (p=0.004 n=6) BM_GatherS32/10/512/1/process_time                                                        489.2n ±  3%    576.0n ±  2%  +17.74% (p=0.002 n=6) BM_GatherS32/10/512/128/process_time                                                     18.982µ ±  2%    4.127µ ±  3%  78.26% (p=0.002 n=6) BM_GatherS32/10/512/2/process_time                                                        506.1n ±  2%    587.1n ±  2%  +15.99% (p=0.002 n=6) BM_GatherS32/10/64/1/process_time                                                         563.9n ±  4%    534.6n ±  1%   5.20% (p=0.002 n=6) BM_GatherS32/10/64/16/process_time                                                        603.1n ±  2%    582.6n ±  1%   3.41% (p=0.002 n=6) BM_GatherS32/10/64/2/process_time                                                         567.9n ±  1%    548.6n ±  1%   3.40% (p=0.002 n=6) BM_GatherS32/100/128/1/process_time                                                       460.2n ±  0%    550.9n ±  1%  +19.71% (p=0.002 n=6) BM_GatherS32/100/128/2/process_time                                                       469.1n ±  3%    562.0n ±  1%  +19.80% (p=0.002 n=6) BM_GatherS32/100/128/32/process_time                                                      768.6n ±  1%    849.3n ±  1%  +10.51% (p=0.002 n=6) BM_GatherS32/100/256/1/process_time                                                       468.1n ±  1%    561.8n ±  1%  +20.02% (p=0.002 n=6) BM_GatherS32/100/256/2/process_time                                                       479.0n ±  1%    567.1n ±  2%  +18.38% (p=0.002 n=6) BM_GatherS32/100/256/64/process_time                                                      1.604µ ±  0%    1.674µ ±  0%   +4.33% (p=0.002 n=6) BM_GatherS32/100/3/1/process_time                                                         452.1n ±  2%    533.6n ±  1%  +18.02% (p=0.002 n=6) BM_GatherS32/100/3/2/process_time                                                         455.0n ±  1%    556.9n ±  3%  +22.40% (p=0.002 n=6) BM_GatherS32/100/3/4/process_time                                                         463.5n ±  4%    537.0n ±  1%  +15.85% (p=0.002 n=6) BM_GatherS32/100/32/1/process_time                                                        460.3n ±  1%    535.5n ±  3%  +16.35% (p=0.002 n=6) BM_GatherS32/100/32/2/process_time                                                        461.6n ±  1%    549.1n ±  1%  +18.96% (p=0.002 n=6) BM_GatherS32/100/32/8/process_time                                                        467.6n ±  0%    566.6n ±  1%  +21.15% (p=0.002 n=6) BM_GatherS32/100/512/1/process_time                                                       479.8n ±  4%    567.8n ±  2%  +18.35% (p=0.002 n=6) BM_GatherS32/100/512/128/process_time                                                    20.158µ ±  3%    5.464µ ±  3%  72.89% (p=0.002 n=6) BM_GatherS32/100/512/2/process_time                                                       507.9n ±  3%    585.8n ±  1%  +15.34% (p=0.002 n=6) BM_GatherS32/100/64/1/process_time                                                        458.4n ±  4%    544.9n ±  3%  +18.87% (p=0.002 n=6) BM_GatherS32/100/64/16/process_time                                                       496.1n ±  2%    594.7n ±  3%  +19.88% (p=0.002 n=6) BM_GatherS32/100/64/2/process_time                                                        461.6n ±  1%    554.9n ±  1%  +20.21% (p=0.002 n=6) BM_GatherS32/3/128/1/process_time                                                         567.6n ±  1%    463.7n ±  5%  18.32% (p=0.002 n=6) BM_GatherS32/3/128/2/process_time                                                         569.7n ±  2%    470.7n ±  1%  17.38% (p=0.002 n=6) BM_GatherS32/3/128/32/process_time                                                        722.9n ±  0%    625.5n ±  2%  13.47% (p=0.002 n=6) BM_GatherS32/3/256/1/process_time                                                         573.1n ±  2%    473.1n ±  2%  17.44% (p=0.002 n=6) BM_GatherS32/3/256/2/process_time                                                         582.2n ±  1%    480.9n ±  2%  17.41% (p=0.002 n=6) BM_GatherS32/3/256/64/process_time                                                        1.202µ ±  1%    1.107µ ±  1%   7.91% (p=0.002 n=6) BM_GatherS32/3/3/1/process_time                                                           566.0n ±  1%    460.1n ± 16%  18.70% (p=0.002 n=6) BM_GatherS32/3/3/2/process_time                                                           564.1n ±  0%    456.7n ±  3%  19.05% (p=0.002 n=6) BM_GatherS32/3/3/4/process_time                                                           567.6n ±  3%    461.9n ±  0%  18.63% (p=0.002 n=6) BM_GatherS32/3/32/1/process_time                                                          563.9n ±  1%    459.4n ±  1%  18.53% (p=0.002 n=6) BM_GatherS32/3/32/2/process_time                                                          562.9n ±  0%    459.0n ±  5%  18.46% (p=0.002 n=6) BM_GatherS32/3/32/8/process_time                                                          572.5n ±  1%    469.5n ±  1%  18.01% (p=0.002 n=6) BM_GatherS32/3/512/1/process_time                                                         585.8n ±  1%    482.0n ±  1%  17.73% (p=0.002 n=6) BM_GatherS32/3/512/128/process_time                                                      18.549µ ±  3%    3.101µ ±  1%  83.28% (p=0.002 n=6) BM_GatherS32/3/512/2/process_time                                                         612.2n ±  7%    505.5n ±  1%  17.42% (p=0.002 n=6) BM_GatherS32/3/64/1/process_time                                                          562.2n ±  3%    457.4n ±  1%  18.63% (p=0.002 n=6) BM_GatherS32/3/64/16/process_time                                                         604.1n ±  4%    494.6n ±  0%  18.13% (p=0.002 n=6) BM_GatherS32/3/64/2/process_time                                                          567.1n ±  1%    463.8n ±  1%  18.22% (p=0.002 n=6) BM_Optimizer0/1024/process_time                                                           5.125m ± 43%    5.046m ±  1%        ~ (p=0.132 n=6) BM_Optimizer0/128/process_time                                                            468.4µ ±  7%    449.4µ ± 12%   4.07% (p=0.026 n=6) BM_Optimizer0/16384/process_time                                                          65.03m ±  5%    62.68m ±  7%        ~ (p=0.485 n=6) BM_Optimizer0/256/process_time                                                            1.219m ±  5%    1.208m ±  7%        ~ (p=0.485 n=6) BM_Optimizer0/512/process_time                                                            3.541m ± 32%    2.392m ±  3%  32.45% (p=0.015 n=6) BM_Optimizer0/8192/process_time                                                           33.71m ±  2%    32.97m ±  5%        ~ (p=0.132 n=6) BM_PadF32/1024/process_time                                                               28.24m ±  9%    28.63m ±  6%        ~ (p=0.485 n=6) BM_PadF32/128/process_time                                                                339.8µ ±  1%    341.0µ ±  0%        ~ (p=0.310 n=6) BM_PadF32/256/process_time                                                                1.192m ±  4%    1.195m ±  3%        ~ (p=0.589 n=6) BM_PadF32/4096/process_time                                                               422.4m ±  1%    423.9m ±  0%        ~ (p=0.310 n=6) BM_PadF32/512/process_time                                                                4.339m ±  6%    4.170m ±  7%        ~ (p=0.065 n=6) BM_ReduceAddBF16/1024/process_time                                                        3.435m ±  2%    3.401m ±  9%        ~ (p=0.180 n=6) BM_ReduceAddBF16/128/process_time                                                         239.5µ ±  0%    239.0µ ±  0%   0.20% (p=0.002 n=6) BM_ReduceAddBF16/16384/process_time                                                       51.15m ±  4%    50.94m ±  2%        ~ (p=0.818 n=6) BM_ReduceAddBF16/256/process_time                                                         914.1µ ± 11%    777.5µ ±  8%  14.94% (p=0.004 n=6) BM_ReduceAddBF16/512/process_time                                                         1.704m ±  6%    1.680m ±  1%        ~ (p=0.093 n=6) BM_ReduceAddBF16/8192/process_time                                                        26.21m ±  3%    25.97m ±  7%        ~ (p=0.699 n=6) BM_ReduceAddF32/1024/process_time                                                         488.1µ ±  1%    487.1µ ±  0%        ~ (p=0.093 n=6) BM_ReduceAddF32/128/process_time                                                          23.36µ ±  0%    23.42µ ±  0%   +0.26% (p=0.026 n=6) BM_ReduceAddF32/16384/process_time                                                        6.756m ±  1%    6.672m ±  0%   1.23% (p=0.002 n=6) BM_ReduceAddF32/256/process_time                                                          62.44µ ±  1%    62.76µ ±  5%        ~ (p=0.132 n=6) BM_ReduceAddF32/512/process_time                                                          246.5µ ±  3%    246.9µ ±  0%        ~ (p=0.310 n=6) BM_ReduceAddF32/8192/process_time                                                         3.479m ±  0%    3.462m ±  0%   0.47% (p=0.004 n=6) BM_ScatterS32_R1/262144/262144/process_time                                               592.3µ ±  1%    590.2µ ±  1%        ~ (p=0.240 n=6) BM_ScatterS32_R2/512/512/process_time                                                     77.23µ ±  2%    77.87µ ±  2%        ~ (p=0.394 n=6) BM_ScatterS32_R3/64/64/process_time                                                       54.41µ ±  0%    54.79µ ±  2%        ~ (p=0.180 n=6) BM_SimpleScatterReduceF32_R3/d0:1/d1:64/d2:8/num_slices:1/process_time                    727.3n ±  1%    720.6n ±  2%   0.92% (p=0.026 n=6) BM_SimpleScatterReduceF32_R3/d0:50/d1:64/d2:8/num_slices:10/process_time                  108.0µ ±  1%    107.3µ ±  3%        ~ (p=0.394 n=6) BM_SimpleScatterReduceF32_R3/d0:500/d1:64/d2:8/num_slices:100/process_time                10.38m ±  1%    10.33m ±  1%        ~ (p=0.132 n=6) BM_SelectAndScatterF32/128/process_time                                                   35.65µ ±  1%    35.72µ ±  4%        ~ (p=0.699 n=6) BM_SelectAndScatterF32/256/process_time                                                   118.7µ ±  1%    117.9µ ±  3%        ~ (p=0.310 n=6) BM_SelectAndScatterF32/512/process_time                                                   1.657m ±  5%    1.640m ±  6%        ~ (p=0.240 n=6) BM_TanhF16/1024/process_time                                                              684.4n ±  1%    716.9n ±  3%   +4.75% (p=0.002 n=6) BM_TanhF16/128/process_time                                                               435.9n ±  2%    466.8n ±  7%   +7.07% (p=0.002 n=6) BM_TanhF16/256/process_time                                                               474.4n ±  2%    504.8n ±  1%   +6.41% (p=0.002 n=6) BM_TanhF16/4096/process_time                                                              1.521µ ±  0%    1.545µ ±  1%   +1.56% (p=0.002 n=6) BM_TanhF16/512/process_time                                                               547.4n ±  1%    578.6n ±  1%   +5.70% (p=0.002 n=6) BM_TanhF32/1024/process_time                                                              688.6n ±  0%    716.3n ±  1%   +4.02% (p=0.002 n=6) BM_TanhF32/128/process_time                                                               446.9n ±  3%    466.9n ±  2%   +4.48% (p=0.002 n=6) BM_TanhF32/256/process_time                                                               476.8n ±  1%    507.2n ±  1%   +6.36% (p=0.002 n=6) BM_TanhF32/4096/process_time                                                              1.580µ ±  0%    1.616µ ±  1%   +2.31% (p=0.002 n=6) BM_TanhF32/512/process_time                                                               556.7n ±  1%    584.9n ±  1%   +5.06% (p=0.002 n=6) BM_TanhF64/1024/process_time                                                              12.60µ ±  2%    12.61µ ±  0%        ~ (p=0.310 n=6) BM_TanhF64/128/process_time                                                               1.944µ ±  0%    1.973µ ±  0%   +1.49% (p=0.002 n=6) BM_TanhF64/256/process_time                                                               3.463µ ±  1%    3.493µ ±  1%   +0.86% (p=0.002 n=6) BM_TanhF64/4096/process_time                                                              49.11µ ±  0%    49.10µ ±  0%        ~ (p=0.394 n=6) BM_TanhF64/512/process_time                                                               6.510µ ±  0%    6.527µ ±  0%   +0.25% (p=0.002 n=6) geomean                                                                                   94.44µ          92.38µ         2.19% ``` ",2025-03-13T02:17:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89123
tpu,sjh0849,API Doc Error for tf.train.ClusterSpec API," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Mac  Mobile device N/A  Python version 3.9  Bazel version N/A  GCC/compiler version N/A  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? The test conversion expects the resulting ClusterDef to have its first job named ""worker"" (matching the insertion order of the original dictionary). However, the test reveals that the first job ends up being ""ps"", which deviates from the API’s expected behavior (as evidenced in the provided examples API doc).  Standalone code to reproduce the issue ```shell import unittest import tensorflow as tf class TestClusterSpec(unittest.TestCase):     def test_cluster_def_conversion(self):          Test conversion to ClusterDef         cluster_dict = {             ""worker"": [""worker0.example.com:2222"", ""worker1.example.com:2222""],             ""ps"": [""ps0.example.com:2222""]         }         cluster_spec = tf.train.ClusterSpec(cluster_dict)         cluster_def = cluster_spec.as_cluster_def()         self.assertEqual(len(cluster_def.job), 2)         self.assertEqual(cluster_def.job[0].name, ""worker"")         self.assertEqual(cluster_def.job[1].name, ""ps"") if __name__ == '__main__':     unittest.main() ```  Relevant log output ```shell ====================================================================== FAIL: test_cluster_def_conversion (__main__.TestClusterSpec)  Traceback (most recent call last):   File ""/Users/sjh/test.py"", line 16, in test_cluster_def_conversion     self.assertEqual(cluster_def.job[0].name, ""worker"") AssertionError: 'ps' != 'worker'  ps + worker  Ran 1 test in 0.000s FAILED (failures=1) ```",2025-03-13T00:34:44Z,type:bug comp:apis comp:ops TF 2.18,open,0,4,https://github.com/tensorflow/tensorflow/issues/89118,I was able to reproduce the same issue on Colab using TensorFlow 2.18 and the nightly version. Please find the gist attached for your reference. Thank you!, I have added this colablink in which I have provided two solutions or approach for now to handle this error please go through it.,"  Let me know If there is anything else, I can do to work on this issue.",Thank you for getting back to me with a solution!
sharding,copybara-service[bot],[IFRT] Introduce `Client::MakeArraysFromHostBufferShards()`,"[IFRT] Introduce `Client::MakeArraysFromHostBufferShards()` `Client::MakeArraysFromHostBufferShards()` is a new IFRT Client API that lets creating multidevice arrays in bulk without breaking down the process to many IFRT API calls. This API makes it easy to skip the intermediate step of creating singledevice arrays and allows using the same host buffer for replicated shards within an array, which reduces redundant resource use regardless of the array sharding. Similar to `Client::MakeArrayFromHostBuffer()`, it allows using a different host buffer layout from the IFRT Array layout, which the runtime is responsible for relayouting the buffer content as needed. To faciliate transition, an implementationagnostic static method `ifrt::Client::MakeArraysFromHostBufferShards()` is provided.",2025-03-13T00:23:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89117
bfloat16,copybara-service[bot],[XLA:Python] Remove references to xla_client.bfloat16.,"[XLA:Python] Remove references to xla_client.bfloat16. This is an alias to ml_dtypes.bfloat16 and has been for a long time. Use that name (or jax.numpy.bfloat16, which is another alias) instead.",2025-03-12T23:47:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89116
tpu,adamvvu,Gradients are zero when clipping values in function definition," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18; 2.10  Custom code Yes  Python version 3.10  Current behavior? I encountered a strange issue where gradients are zero when clippingrelated TensorFlow Ops are defined in functions. For context, I was implementing a numerically stable version of sigmoid where the inputs are clipped to `[10, 10]`. However this bug can be reproduced with other functions.  In the provided sample code, I define a truncated version of f(x) = x^2 where the function `truncate_domain` is used to clip the input tensors. However gradients only work as expected when the clipping occurs inside `GradientTape` and not when it's part of the function definition. Reproduced on TensorFlow 2.10 with GPU, and latest 2.18 with CPU  Standalone code to reproduce the issue ```shell import tensorflow as tf def square1(x):     return x**2 def truncate_domain(x, lb, ub):      Truncate for numerical stability     x = tf.clip_by_value(x, lb, ub)     x = tf.math.maximum(x, lb)     x = tf.math.minimum(x, ub)     return x def square2(x):     x = truncate_domain(x, 10., 10.)     return x**2 def square3(x):     x = truncate_domain(x, 10., 10.)     return square1(x)  Note: All 3 square functions are exactly mathematically equivalent when x in [10, 10].  Checking gradients x = tf.constant(15., dtype=tf.float32) with tf.GradientTape() as tape:     tape.watch(x)     x = truncate_domain(x, 10., 10.)     l = square1(x) grad1 = tape.gradient(l, x) print(l) print(grad1)  >> tf.Tensor(100.0, shape=(), dtype=float32)  >> tf.Tensor(20.0, shape=(), dtype=float32) x = tf.constant(15., dtype=tf.float32) with tf.GradientTape() as tape:     tape.watch(x)     l = square2(x) grad2 = tape.gradient(l, x) print(l) print(grad2)  >> tf.Tensor(100.0, shape=(), dtype=float32)  >> tf.Tensor(0.0, shape=(), dtype=float32) x = tf.constant(15., dtype=tf.float32) with tf.GradientTape() as tape:     tape.watch(x)     l = square3(x) grad3 = tape.gradient(l, x) print(l) print(grad3)  >> tf.Tensor(100.0, shape=(), dtype=float32)  >> tf.Tensor(0.0, shape=(), dtype=float32)  Note: Gradients for square2 and square3 are 0, even though they are mathematically and (Python) syntactically equivalent to square1's expected behavior ```  Relevant log output ```shell ```",2025-03-12T22:33:15Z,stat:awaiting tensorflower type:bug comp:ops comp:autograph TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/89110,I was able to reproduce the same issue on Colab using TensorFlow 2.18 and the nightly version. Please find the gist attached for your reference. Thank you!
tpu,sjh0849,Error in tf.convert_to_tensor API," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Ubuntu  Mobile device N/A  Python version 3.9  Bazel version N/A  GCC/compiler version N/A  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? The two values (one from numpy and one from tf) have the same value, but the assertion equality is not working as expected.  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np import unittest class TestConvertToTensor(unittest.TestCase):     def test_convert_float(self):          Test conversion of a float to a tensor         float_data = 3.14         tensor = tf.convert_to_tensor(float_data)         self.assertTrue(isinstance(tensor, tf.Tensor))         self.assertEqual(tensor.dtype, tf.float32)         self.assertEqual(tensor.numpy(), float_data) if __name__ == '__main__':     unittest.main() ```  Relevant log output ```shell Traceback (most recent call last):   File ""/sjh/tf.convert_to_tensor.py"", line 45, in test_convert_float     self.assertEqual(tensor.numpy(), float_data) AssertionError: 3.14 != 3.14 ```",2025-03-12T21:40:29Z,stat:awaiting response type:bug comp:apis TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89107,"Hi  , Apologies for the delay, and thanks for bringing this up! I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. While I did not encounter the exact issue you mentioned, I did face a different problem related to unittest. After adjusting the unittest setup, the code worked fine for me. I have attached a gist for your reference, it might help resolve the issue on your end. Let me know if you need further assistance! Thank you!","Hi  , Thanks for the assistance! It resolves the issues.",Are you satisfied with the resolution of your issue? Yes No
tpu,sjh0849,Doc Error: tf.boolean_mask API," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Ubuntu  Mobile device _No response_  Python version 3.9  Bazel version N/A  GCC/compiler version N/A  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? The API documentation states that the mask parameter must be a Boolean tensor, but the implementation does not fully enforce this. In the test test_mask_with_different_dtype, an int32 mask is passed, and while a TypeError is expected, no error is raised. The expectation is that the documentation should mention that nonBoolean can be handled in the API implementation.  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np import unittest class TestBooleanMask(unittest.TestCase):     def test_mask_with_different_dtype(self):          Test with a mask of different dtype (int)         tensor = tf.constant([1, 2, 3, 4, 5])         mask = tf.constant([1, 0, 1, 0, 1], dtype=tf.int32)         with self.assertRaises(TypeError):             tf.boolean_mask(tensor, mask) if __name__ == '__main__':     unittest.main() ```  Relevant log output ```shell ```",2025-03-12T21:23:42Z,type:docs-bug type:bug comp:apis comp:ops awaiting PR merge TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89106,"Hi  , Apologies for the delay, and thanks for raising your issue here. I have submitted a PR addressing your issue. Once it gets merged, the issue should be resolved. Thank you!","Hi  , Thanks for getting back to me! Awesome!"
tpu,shinjh0849,Doc Error: tf.boolean_mask API," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Ubuntu   Mobile device _No response_  Python version 3.9  Bazel version N/A  GCC/compiler version N/A  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? The API documentation states that the mask parameter must be a Boolean tensor, but the implementation does not fully enforce this. In the test test_mask_with_different_dtype, an int32 mask is passed, and while a TypeError is expected, no error is raised. The expectation is that the documentation should mention that nonBoolean can be handled in the API implementation.  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np import unittest class TestBooleanMask(unittest.TestCase):     def test_mask_with_different_dtype(self):          Test with a mask of different dtype (int)         tensor = tf.constant([1, 2, 3, 4, 5])         mask = tf.constant([1, 0, 1, 0, 1], dtype=tf.int32)         with self.assertRaises(TypeError):             tf.boolean_mask(tensor, mask) if __name__ == '__main__':     unittest.main() ```  Relevant log output ```shell ```",2025-03-12T21:14:51Z,type:docs-bug type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/89105,Are you satisfied with the resolution of your issue? Yes No
tpu,copybara-service[bot],PR #23386: [debug_options] Printing all the fields in debug options,PR CC(InvalidArgumentError: Invalid name: An op that loads optimization parameters into HBM for embedding. (ConfigureTPUEmbeddingHost)): [debug_options] Printing all the fields in debug options Imported from GitHub PR https://github.com/openxla/xla/pull/23386 This patch prints all the values of the debug options while dumping it under VLOG. This is specifically required for boolean fields which have the default value set to true in `xla/debug_options_flags.cc`. These values will not be printed in `DebugString()` if `XLA_FLAGS` overrides it to `false`. Copybara import of the project:  4746d058005f9fff6ebbcc37818a288979d05165 by Shraiysh Vaishay : [debug_options] Printing all the fields in debug options This patch prints all the values of the debug options while dumping it under VLOG. This is specifically required for boolean fields which have the default value set to true in `xla/debug_options_flags.cc`. These values will not be printed in `DebugString()` if `XLA_FLAGS` overrides it to `false`. Merging this change closes CC(InvalidArgumentError: Invalid name: An op that loads optimization parameters into HBM for embedding. (ConfigureTPUEmbeddingHost)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23386 from shraiysh:print_debug_options_full 4746d058005f9fff6ebbcc37818a288979d05165,2025-03-12T19:08:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89097
opt,copybara-service[bot],Split the environment options map out from the `LiteRtEnvironmentT`.,Split the environment options map out from the `LiteRtEnvironmentT`.  Add a C function to get the environment option map from the environment.  Add a C function to get environment options values from the option map.  Add a header only target for `c/litert_environment_options` to avoid a cycle   between `c/litert_environment_options` and `core/environment_options`. litert,2025-03-12T16:53:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89090
opt,chuntl,Qualcomm AI Engine Direct - Enable QNN profiler in LiteRt,Summary:  Use kProfilingOff profiling_level as default for now  Implement QNN profiler with simple format  Can be tested with dispatch_api_qualcomm_test  Need to add into dispatch option after header interface merged,2025-03-12T16:16:12Z,size:M,open,0,3,https://github.com/tensorflow/tensorflow/issues/89088,"**Description** For now, the profiling is disabled by setting profiling_level in the code, you could simply change the profiling_level to kBasicProfiling (equal to 1) or kDetailedProfiling (equal to 2) in the tensorflow/lite/experimental/litert/vendors/qualcomm/dispatch/litert_dispatch_invocation_context.(TensorBoard logdir path, if relative, is relative to $HOME) **TODO**  Add profiling_level as dispatch option after header interface merged  Add viewer tools for profiling result to format **Sample Result** The sample of profiling result with DETAILED: !image The sample of profiling result with BASIC: !image",Can we have a test that ensures graph execute success when profile is on (basic/detailed)?,"> Can we have a test that ensures graph execute success when profile is on (basic/detailed)? Discussed internally, will add some tests based on the incoming option interface PR merged, thanks!"
opt,copybara-service[bot],Use fingerprint options when converting the HloModule to a string.,"Use fingerprint options when converting the HloModule to a string. We use the string to compute a fingerprint, so we should also use the fingerprint options.",2025-03-12T15:23:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89086
tpu,t-kalinowski,Update Docs to latest release," Issue type Documentation Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The website at https://www.tensorflow.org/api_docs/python currently hosts docs for TensorFlow version 2.16.1, released March 8, 2024 (over 1 year ago) The current release version of TensorFlow is 2.19.0. Will the website be updated for the current TensorFlow release? Or is it possible to build the docs locally with only opensource tools?  Standalone code to reproduce the issue ```shell Visit https://www.tensorflow.org/api_docs/python ```  Relevant log output ```shell ```",2025-03-12T14:52:21Z,stat:awaiting tensorflower type:feature type:docs-feature TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89084,toplay this is step that's being skipped during latest releases...,Thank you very much for flagging!   Can you take a look and address?
tpu,novamcbo,from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version TENSORFLOW 2.19  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? :\Users\ray_v\PycharmProjects\pythonProject\.venv1\Scripts\python.exe C:\Users\ray_v\PycharmProjects\pythonProject\backend\ai\train_ml_model.py  Traceback (most recent call last):   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\Users\ray_v\PycharmProjects\pythonProject\backend\ai\train_ml_model.py"", line 10, in      import tensorflow as tf   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. Process finished with exit code 1  Standalone code to reproduce the issue ```shell :\Users\ray_v\PycharmProjects\pythonProject\.venv1\Scripts\python.exe C:\Users\ray_v\PycharmProjects\pythonProject\backend\ai\train_ml_model.py  Traceback (most recent call last):   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\Users\ray_v\PycharmProjects\pythonProject\backend\ai\train_ml_model.py"", line 10, in      import tensorflow as tf   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. Process finished with exit code 1 ```  Relevant log output ```shell ```",2025-03-12T14:38:51Z,type:bug TF 2.18,closed,1,4,https://github.com/tensorflow/tensorflow/issues/89083,"Hi **** , Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:  You need to install the MSVC 2019 redistributable  Your CPU does not support AVX2 instructions  Your CPU/Python is on 32 bits  There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. CC(Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.) Also this is a duplicate of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) Thank you!",please update the MSVC 2019 redistributable，download links: https://download.visualstudio.microsoft.com/download/pr/285b28c73cf947fb9be801cf5323a8df/8F9FB1B3CFE6E5092CF1225ECD6659DAB7CE50B8BF935CB79BFEDE1F3C895240/VC_redist.x64.exe,Please search for duplicates before opening new issues ,Are you satisfied with the resolution of your issue? Yes No
tpu,GaganaMD,ImportError: DLL load failed while importing _pywrap_tensorflow_internal on Windows (TensorFlow CPU)," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.9  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am facing an issue while trying to import TensorFlow on a CPU machine. The error traceback suggests that a DLL load failed while importing _pywrap_tensorflow_internal, which prevents TensorFlow from initializing properly. Traceback (most recent call last):   File ""C:\Users\gagan\Desktop\EGU_replication\venv\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\Users\gagan\Desktop\EGU_replication\IEEE_TNNLS_EGUNet\EGUNetpw.py"", line 5, in      import tensorflow as tf   File ""C:\Users\gagan\Desktop\EGU_replication\venv\lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport   File ""C:\Users\gagan\Desktop\EGU_replication\venv\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\gagan\Desktop\EGU_replication\venv\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for common causes and solutions.  Standalone code to reproduce the issue ```shell Please test this on your local windows machine https://colab.research.google.com/drive/1PPNyc5LP5ngeVadUCx6vBxmRQN5MhiVq?usp=sharing ```  Relevant log output ```shell ```",2025-03-12T10:23:53Z,type:bug,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89072,TensorFlow version: 2.18.0,Please search for duplicates before opening a new issue,Are you satisfied with the resolution of your issue? Yes No,Update the MSVC like this download link ：https://download.visualstudio.microsoft.com/download/pr/285b28c73cf947fb9be801cf5323a8df/8F9FB1B3CFE6E5092CF1225ECD6659DAB7CE50B8BF935CB79BFEDE1F3C895240/VC_redist.x64.exe
opt,copybara-service[bot],PR #23613: [ROCm] Pass correct warp size to Triton pipeline,PR CC(ERROR: Config value opt is not defined in any .rc file): [ROCm] Pass correct warp size to Triton pipeline Imported from GitHub PR https://github.com/openxla/xla/pull/23613 Copybara import of the project:  dc43a7518d690038398ae3cf301de477d1ca715f by Dragan Mladjenovic : [ROCm] Pass correct warp size to Triton pipeline Merging this change closes CC(ERROR: Config value opt is not defined in any .rc file) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23613 from ROCm:triton_wrap_size dc43a7518d690038398ae3cf301de477d1ca715f,2025-03-12T09:48:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89067
tpu,AntonBondarenkoArm,Build error when building benchmark_model from TensorFlow Lite using CMake," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version master (tf 2.20)  Custom code No  OS platform and distribution MacOS 15.3.1  Mobile device _No response_  Python version _No response_  Bazel version CMake 3.31.6  GCC/compiler version Apple clang version 16.0.0 (clang1600.0.26.6)  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Build error when building benchmark_model from TensorFlow Lite using CMake Failing on source code  commit d4aca8a0919cc01cae8855de7d83a8e7e9848cf7 (HEAD, origin/master)  Standalone code to reproduce the issue ```shell cmake S tensorflow/lite B buildxxx DXNNPACK_ENABLE_ARM_I8MM=TRUE DTFLITE_ENABLE_XNNPACK=ON DTFLITE_ENABLE_GPU=OFF DTFLITE_ENABLE_METAL=OFF cmake build buildxxx j1 t benchmark_model ```  Relevant log output ```shell [ 82%] Building CXX object CMakeFiles/tensorflowlite.dir/core/c/c_api.cc.o /Library/Developer/CommandLineTools/usr/bin/c++ DCPUINFO_SUPPORTED_PLATFORM=1 DEIGEN_MPL2_ONLY DNOMINMAX=1 DPTHREADPOOL_NO_DEPRECATED_API=1 DXNN_LOG_LEVEL=0 Ithird_party/xla/third_party/tsl Ithird_party/xla Ibuildxxx/pthreadpoolsource/include Ibuildxxx/FP16source/include Ibuildxxx/xnnpack/include Ibuildxxx/cpuinfo I/Users/antbon01/work/ml/kai/third_party/tensorflow Ibuildxxx/eigen Ibuildxxx/abseilcpp Ibuildxxx/farmhash/src Ibuildxxx/flatbuffers/include Ibuildxxx/gemmlowp Ibuildxxx/ml_dtypes Ibuildxxx/ml_dtypes/ml_dtypes Ibuildxxx/ruy Ibuildxxx/cpuinfo/include O3 DNDEBUG std=gnu++20 arch arm64 isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX15.2.sdk fPIC DEIGEN_NEON_GEBP_NR=4 DTFLITE_KERNEL_USE_XNNPACK DTFLITE_BUILD_WITH_XNNPACK_DELEGATE DXNNPACK_DELEGATE_ENABLE_QS8 DXNNPACK_DELEGATE_ENABLE_QU8 DXNNPACK_DELEGATE_USE_LATEST_OPS DXNNPACK_DELEGATE_ENABLE_SUBGRAPH_RESHAPING DTFL_STATIC_LIBRARY_BUILD Wnodeprecateddeclarations MD MT CMakeFiles/tensorflowlite.dir/core/c/c_api.cc.o MF CMakeFiles/tensorflowlite.dir/core/c/c_api.cc.o.d o CMakeFiles/tensorflowlite.dir/core/c/c_api.cc.o c tensorflow/lite/core/c/c_api./lite/core/c/c_api.cc:67:38: error: expected ')'    67    ^ 2 errors generated. make[3]: *** [CMakeFiles/tensorflowlite.dir/core/c/c_api.cc.o] Error 1 make[2]: *** [CMakeFiles/tensorflowlite.dir/all] Error 2 make[1]: *** [tools/benchmark/CMakeFiles/benchmark_model.dir/rule] Error 2 make: *** [benchmark_model] Error 2 ```",2025-03-12T09:35:46Z,type:build/install comp:lite subtype:macOS,open,0,6,https://github.com/tensorflow/tensorflow/issues/89065,The build issue seems to be introduced with commit 805775fcb5f9272e4c52dce751b00cf7f70364f2.,"I got the same issue when crosscompiling for arm with GCC8 with commit 83283e1. From the comment above, I can imagine that it should be something like:  ``` // e.g. ""0.5.0"" or ""0.6.0alpha"". define TF_VERSION_STRING (_TF_STR(TF_MAJOR_VERSION) ""."" _TF_STR(TF_MINOR_VERSION) ""."" _TF_STR(TF_PATCH_VERSION) """" _TF_STR(TF_VERSION_SUFFIX)) ``` ?","> I got the same issue when crosscompiling for arm with GCC8 with commit 83283e1. >  > From the comment above, I can imagine that it should be something like: >  > ``` > // e.g. ""0.5.0"" or ""0.6.0alpha"". > define TF_VERSION_STRING (_TF_STR(TF_MAJOR_VERSION) ""."" _TF_STR(TF_MINOR_VERSION) ""."" _TF_STR(TF_PATCH_VERSION) """" _TF_STR(TF_VERSION_SUFFIX)) > ``` >  > ? Your solution works for me 👍 ","Hi, ,   Hi,  Thank you for providing the workaround for this issue I have submitted a PR to take care of this issue https://github.com/tensorflow/tensorflow/pull/91562 Thank you for your cooperation and patience.","Hi, , ,   Please refer these comments https://github.com/tensorflow/tensorflow/pull/91562issuecomment2813863024 and https://github.com/tensorflow/tensorflow/issues/90533issuecomment2813855694 If issue has been resolved please feel free to close this issue from your end.  Thank you for your cooperation and understanding.", Could you please clarify which additional variables must be specified? I've tried Bazel ones with ML_WHEEL_TYPE and it does not work. In the commit mentioned it seems like we need to provide version defines directly via C/CXX flags. If so this way is quite fragile and does not give us any compatibility as these defines could be changed at any time and I see no value in giving them instead of using a default ones provided by the project.
tensorrt,takutarou,"Error: Unable to register cuFFT, cuDNN, and cuBLAS factories in TensorFlow with CUDA"," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0gpu（docker hub）  Custom code Yes  OS platform and distribution Ubuntu 22.04 LTS (WSL2)  Mobile device _No response_  Python version 3.11.0  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 12.8, cuDNN 8.x (inside TensorFlow container)  GPU model and memory nvidia GeForce RTX4080 SUPER（16GB）  Current behavior? ・Summary I can’t resolve this error, so I need help. I am running an Ubuntu environment on WSL2 and using Docker via CLI. I am using TensorFlow 2.18.0GPU from Docker Hub. The GPU driver I am using is listed below. To prevent any issues from previous paths, I have reset my PC once, and there are no other development environments installed. As shown in the log below, I am encountering an error that I cannot resolve, and I would like to know the solution. ・NVIDIA Studio driver version 572.60 ・Error logs 20250305 17:54:21.254014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20250305 17:54:21.333890: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20250305 17:54:21.358644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250305 17:54:21.492257: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250305 17:54:23.035426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT  Standalone code to reproduce the issue ```shell docker pull tensorflow/tensorflow:2.18.0gpu ```  Relevant log output ```shell ```",2025-03-12T09:35:46Z,stat:awaiting tensorflower type:support comp:gpu TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89064,"Hi **** , Thanks for raising your concern here. A similar issue ( CC(cuDNN, cuFFT, and cuBLAS Errors)) is still open. Please follow it for further updates and details. Thank you!","Hi  ,  Thank you, I overlooked a similar question. Now I have a much better understanding of the situation. I’ll refer to everyone’s comments while also looking for a solution myself. If I find any useful information, I’ll share it in CC(cuDNN, cuFFT, and cuBLAS Errors). With that, I’ll close this issue. Appreciate all the hard work from the development team—thank you for your dedication!",Are you satisfied with the resolution of your issue? Yes No
tpu,copybara-service[bot],Also generate TPU and CPU tests by default.,"Also generate TPU and CPU tests by default. The previous logic assumed that configcudaonly tag meant that we want to generate just GPU tests. That was a misunderstanding of the tag name. The tag means the test should only be run with config=cuda, as that is required by the generated GPU tests.",2025-03-12T09:30:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89063
yi,copybara-service[bot],Internal change only,Internal change only,2025-03-12T01:45:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89052
quantization,copybara-service[bot],Fork quantization_config into Lite,Fork quantization_config into Lite,2025-03-12T01:15:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89051
opt,copybara-service[bot],Clean up version checks and acceleration compilation options ,"Clean up version checks and acceleration compilation options  Drivers for Dispatch API and Compiler Plugins must ensure that the loaded library have the same exact version, since we expect them to be distributed together with the LiteRT runtime.",2025-03-11T21:08:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89044
int8,copybara-service[bot],PR #88546: Qualcomm AI Engine Direct - Wrapper tests & Refactor tensor wrapper & Fix rms norm,PR CC(Qualcomm AI Engine Direct  Wrapper tests & Refactor tensor wrapper & Fix rms norm): Qualcomm AI Engine Direct  Wrapper tests & Refactor tensor wrapper & Fix rms norm Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/88546  What 1. Refactor tensor wrapper      Safer tensor data getter and setter. 2. Add wrapper tests. 3. Fix rms norm builder      Change 0 beta tensor type to float32 for float32 input and uint8 for other input data types. See op support types here.  Tests `qnn_compiler_plugin_test` ``` [] Global test environment teardown [==========] 99 tests from 5 test suites ran. (3995 ms total) [  PASSED  ] 99 tests. ``` Copybara import of the project:  c9511b8677d7a565416db5a317bfa239ee189d08 by chunhsue : refine tensor wrapper  a86a562667edb6fa81afff73b09d785dabfb5e8c by chunhsue : add wrapper tests  04116197eaa5c802c980198dcca675a15b60c2fd by chunhsue : fix rms norm builder Merging this change closes CC(Qualcomm AI Engine Direct  Wrapper tests & Refactor tensor wrapper & Fix rms norm) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88546 from jiunkaiy:dev/chunhsue/wrapper_tests 04116197eaa5c802c980198dcca675a15b60c2fd,2025-03-11T18:36:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89033
opt,elfprince13,Allow empty configuration values to be supplied from environment,For example it may be desirable for `CC_OPT_FLAGS` to be empty.,2025-03-11T18:07:48Z,size:XS,closed,0,6,https://github.com/tensorflow/tensorflow/issues/89032,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , Can you please sign the CLA? Many thanks!","> Hi , Can you please sign the CLA? Many thanks! Hi, I actually already signed it and the CI bot just didn't rerun afterward =)","So the bot requires someone (contributor or reviewer) to retrigger a scan. From the check page, there's a link for that, when the CLA check fails.", anything remaining before it can be merged?,"Sorry, I was away while you sent the last message, but the delay here was caused by the need to have this PR imported into the internal systems, more CI to run on the result of the import and a different set of approvers. The process is documented somewhat in CONTRIBUTING.md"
tpu,copybara-service[bot],[XLA] Upload HLO test output to GCS during presubmit.,[XLA] Upload HLO test output to GCS during presubmit.,2025-03-11T17:41:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89027
yi,copybara-service[bot],Allow appending a custom error message when using error checking macros & other improvements.,"Allow appending a custom error message when using error checking macros & other improvements.  Rename and move `ErrorStatusReturnHelper` to `litert::ErrorStatusBuilder`.  Add extra logging capabilities to `litert::ErrorStatusBuilder`.   It is now possible to stream data to the builder. This creates an extra   message that is appended to the original error message.   ```cpp   LITERT_RETURN_IF_ERROR(expr) << ""Failed while trying to ..."";   ```  Refactor `LITERT_RETURN_IF_ERROR` so that the default `ErrorStatusBuilder`   can be used (see example above).  Refactor `LITERT_ASSIGN_OR_RETURN` so that the return expression can   reference a variable called `_` that holds an `ErrorStatusBuilder` built with   the result of the expression.   ```cpp   LITERT_ASSIGN_OR_RETURN(auto var, expr, _ << ""Failed while trying to ..."");   ```  In functions returning a `LiteRtStatus`, this logs the message automatically   upon conversion.   This makes it possible to easily log messages.   ```cpp   LiteRtStatus LiteRtCFunction(LiteRtEnvironment environment, ...) {     LITERT_RETURN_IF_ERROR(environment, litert::Error(kLiteRtStatusErrorInvalidArgument,                                         ""`environment` handle must not be null.""));   }   ```  The log severity upon conversion can be adjusted with the `Log*()` functions   and silenced with `NoLog()`.",2025-03-11T15:45:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89023
tpu,18marie05,gemma2-2b SavedModel to tflite conversion,"Hello,  I am trying to convert a Gemma Saved Model in tflite format to run on an edge device.  1. System information I am running my code on a Jupyter Notebook. My tensorflow version is 2.18.0  2. Code  Code Here is the code I used once I have my Saved Model :  Since it is too large to compress and I can't share my Saved Model folder, here is how I got it:  I downloaded the Keras model from Kaggle  Here is the code I used to get de Saved Model Colab link ``` converter = tf.lite.TFLiteConverter.from_saved_model(""gemma22b_model"") converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]  Convert the model tflite_model = converter.convert()  Save the converted model with open('gemma22b_model/gemma2_2b.tflite', 'wb') as f:     f.write(tflite_model) ``` I tried different solutions:   with the converter.target_spec.supported_ops  without the converter.target_spec.supported_ops   only with TFLITE_BUILTINS and only with SELECT_TF_OPS  with an optimization like this converter.optimizations = [tf.lite.Optimize.DEFAULT] And I also tried this code with the method I saw on another resolved issue: ``` converter = tf.lite.TFLiteConverter.from_saved_model(""gemma22b_model"") converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.target_spec.supported_types = [tf.float16] converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS,     tf.lite.OpsSet.SELECT_TF_OPS] converter.experimental_new_converter = True converter.allow_custom_ops = True tflite_model = converter.convert()  Save the model. with open('gemma22b_model/test_ops_gemma2_2b.tflite', 'wb') as f:   f.write(tflite_model) ```  3. Failure after conversion After running it, the code produces a .tflite file but the StridedSlice is not supported, making the tflite model impossible to use. Here are the logs:  ``` W0000 00:00:1741686810.500306   38515 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format. W0000 00:00:1741686810.500796   38515 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency. 20250311 10:53:30.505166: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: gemma22b_model 20250311 10:53:30.526521: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve } 20250311 10:53:30.526654: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: gemma22b_model 20250311 10:53:30.741628: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle. 20250311 10:53:36.765240: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: gemma22b_model 20250311 10:53:37.149345: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 6644559 microseconds. 20250311 10:53:39.223577: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3825] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s): Flex ops: FlexStridedSlice Details: 	tf.StridedSlice(tensor, tensor, tensor, tensor) > (tensor) : {begin_mask = 7 : i64, ellipsis_mask = 0 : i64, end_mask = 7 : i64, new_axis_mask = 8 : i64, shrink_axis_mask = 0 : i64} 	tf.StridedSlice(tensor, tensor, tensor, tensor) > (tensor) : {begin_mask = 25 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 25 : i64, new_axis_mask = 6 : i64, shrink_axis_mask = 0 : i64} 	tf.StridedSlice(tensor, tensor, tensor, tensor) > (tensor) : {begin_mask = 25 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 25 : i64, new_axis_mask = 6 : i64, shrink_axis_mask = 0 : i64} See instructions: https://www.tensorflow.org/lite/guide/ops_select ```  I still tried to run the ```tf.lite.Interpreter``` with this code: ``` import numpy as np import tensorflow as tf  Load the TFLite model interpreter = tf.lite.Interpreter(model_path=""gemma22btflite/test_ops_gemma2_2b.tflite"") interpreter.allocate_tensors()  Get input and output details input_details = interpreter.get_input_details() output_details = interpreter.get_output_details()  Print the input and output details print(""Input Details:"", input_details) print(""Output Details:"", output_details)  Prepare input data (match the shape and dtype) input_data = np.array([[1]], dtype=np.int32)   Scalar value as input, matching the shape [1, 1] and dtype int32  Set the input tensor interpreter.set_tensor(input_details[0]['index'], input_data)  Run inference interpreter.invoke()  Get the output tensor output_data = interpreter.get_tensor(output_details[0]['index'])  Print the output print(""Output data:"", output_data ``` and this is what I got :  ``` INFO: Created TensorFlow Lite delegate for select TF ops. INFO: TfLiteFlexDelegate delegate: 52 nodes delegated out of 5669 nodes with 26 partitions. INFO: Created TensorFlow Lite XNNPACK delegate for CPU.  RuntimeError                              Traceback (most recent call last) Cell In[11], line 23      20 interpreter.set_tensor(input_details[0]['index'], input_data)      22  Run inference > 23 interpreter.invoke()      25  Get the output tensor      26 output_data = interpreter.get_tensor(output_details[0]['index']) File ~/Documents/.venv/lib/python3.11/sitepackages/tensorflow/lite/python/interpreter.py:965, in Interpreter.invoke(self)     953 """"""Invoke the interpreter.     954      955 Be sure to set the input sizes, allocate tensors and fill values before    (...)     962   ValueError: When the underlying interpreter fails raise ValueError.     963 """"""     964 self._ensure_safe() > 965 self._interpreter.Invoke() RuntimeError: tensorflow/lite/kernels/read_variable.cc:67 variable != nullptr was not true.Node number 297 (READ_VARIABLE) failed to invoke. ``` I tried to resolve my issue by consulting a few issues like this one  CC(error while converting to tflite) but none of the codes seem to work for my use case. I would be so grateful for your help. Thank you!",2025-03-11T13:38:13Z,stat:awaiting response stale comp:lite TFLiteConverter TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/89018,"Hi, Is there a solution to fix this conversion issue  ? Thank you for your answer","Hi,   I aplogize for the delay in my response, if possible could you please give it try with below conversion script by removing experimental flag and optimizations and see is it resolving your issue or not ? ``` converter = tf.lite.TFLiteConverter.from_saved_model(""gemma22b_model"") converter.target_spec.supported_types = [tf.float16] converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS,     tf.lite.OpsSet.SELECT_TF_OPS] tflite_model = converter.convert()  Save the model. with open('gemma22b_model/test_ops_gemma2_2b.tflite', 'wb') as f:   f.write(tflite_model) ``` Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,"i am having the same issue. I tried  's advice, unfortunately it doesn't work. My TF version is 2.19.0.  could you tell us how you solved this?"
opt,copybara-service[bot],Introduce an advanced configuration option for the profiler.,Introduce an advanced configuration option for the profiler.,2025-03-11T12:12:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89014
opt,copybara-service[bot],API cleanup for CompilationOptions,"API cleanup for CompilationOptions With this change, the caller owns litert::CompilationOptions or LiteRtCompilationOptions when calling litert::CompileModel::Create or LiteRtCompiledModelCreate. This behavior is consistent with other LiteRT APIs.",2025-03-11T05:22:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89004
yi,copybara-service[bot],Remove TF_CUDNN_USE_FRONTEND env var.,"Remove TF_CUDNN_USE_FRONTEND env var. This environment variable has defaulted to true for over three years, and no known users set it to false. There is a lot of code maintaining the nonfrontend legacy cuDNN API which we'd like to delete. Removing the env var is the first step. I don't remove any code relying on the env var yet, in order to make this CL easier to rollback if necessary.",2025-03-11T02:42:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89000
opt,copybara-service[bot],litert: Update CompiledModel::Create() comments,litert: Update CompiledModel::Create() comments  Clearly mentioned that options are used for JIT compilation of the model.  Also mentioned that JIT options are meaningless for fully AOT compiled models.,2025-03-11T02:41:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88999
sharding,copybara-service[bot],Improve the partitioner for the dot pattern where,"Improve the partitioner for the dot pattern where 1. LHS and output is fully replicated 2. RHS is fully partitioned along the noncontracting dimensions 3. ShapeSize(RHS) >= ShapeSize(result) Two improvements. 1. Reduce the size of dot operation since we partition along noncontracting dimensions. 2. Reduce or keep the size of allgather due to Condition 3 ShapeSize(RHS) >= ShapeSize(result). Given the following input HLO ``` %lhs = bf16[64,32] parameter(0), sharding={devices=[2,1]<=[2]} %rhs = bf16[32,16] parameter(1), sharding={replicated} %dot = bf16[64,16] dot(%lhs, %rhs), lhs_contracting_dims={1}, rhs_contracting_dims={0}, sharding={replicated} ``` Previously, we (1) replicate LHS through allgather and (2) do the dot operation. ``` %param = bf16[32,32]{1,0} parameter(0), sharding={devices=[2,1]0,1} %allgather = bf16[64,32]{1,0} allgather(bf16[32,32]{1,0} %param), channel_id=1, replica_groups={{0,1}}, dimensions={0}, use_global_device_ids=true %param.1 = bf16[32,16]{1,0} parameter(1), sharding={replicated} %dot.1 = bf16[64,16]{1,0} dot(bf16[64,32]{1,0} %allgather, bf16[32,16]{1,0} %param.1), lhs_contracting_dims={1}, rhs_contracting_dims={0} ``` With this change, we (1) do the dot operation, and (2) replicate the result through allgather. We reduce the size of dot operation on each device. ``` %param = bf16[32,32]{1,0} parameter(0), sharding={devices=[2,1]<=[2]} %param.1 = bf16[32,16]{1,0} parameter(1), sharding={replicated} %dot.1 = bf16[32,16]{1,0} dot(bf16[32,32]{1,0} %param, bf16[32,16]{1,0} %param.1), lhs_contracting_dims={1}, rhs_contracting_dims={0} %allgather = bf16[64,16]{1,0} allgather(bf16[32,16]{1,0} %dot.1), channel_id=1, replica_groups=[1,2]<=[2], dimensions={0}, use_global_device_ids=true ```",2025-03-11T01:52:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88997
yi,copybara-service[bot],Remove flag --xla_gpu_enable_cudnn_frontend.,"Remove flag xla_gpu_enable_cudnn_frontend. This flag has defaulted to true for over three years, and no known users set it to false. There is a lot of code maintaining the nonfrontend legacy cuDNN API which we'd like to delete. Removing the flag is the first step. I don't remove any code relying on the flag yet, in order to make this CL easier to rollback if necessary.",2025-03-11T00:46:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88996
opt,copybara-service[bot],Optimize utility function for getting op options. Single dispatch has a lot of overhead,Optimize utility function for getting op options. Single dispatch has a lot of overhead,2025-03-10T22:58:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88988
opt,wonjeon,[mlir][tosa] Update control flow variable names and Fix lit tests for max_pool2d and pad op,We've been pushing TOSA v1.0 LLVM patches to upstream. This PR includes commit(s) to keep the following operator(s) to be aligned with LLVM: [mlir][tosa] Remove optional for pad_const and remove input_zp attr for PadOp https://github.com/llvm/llvmproject/pull/129336 [mlir][tosa] Add more verifiers for the following operators https://github.com/llvm/llvmproject/pull/127923 This PR addresses the current issue of broken code and lit tests.,2025-03-10T22:35:58Z,kokoro:force-run ready to pull size:S comp:lite-tosa,closed,0,2,https://github.com/tensorflow/tensorflow/issues/88987,"Local testing done successfully: INFO: Analyzed 33 targets (1 packages loaded, 33392 targets configured). INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 4.039s, Critical Path: 2.52s INFO: 18 processes: 34 local. INFO: Build completed successfully, 18 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test         PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test             PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test      PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test                PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test   PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test           PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test  PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test      PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test         PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 2.1s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test     PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test        PASSED in 0.1s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test   PASSED in 0.2s Executed 17 out of 17 tests: 17 tests pass.",Updated the description to include the matching llvm patches.
opt,copybara-service[bot],[HLO-OPT] Minor documentation update,[HLOOPT] Minor documentation update,2025-03-10T19:45:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88971
opt,copybara-service[bot],Avoid creating large constants in ConvertTFLBroadcastToMulOp optimization pass.,Avoid creating large constants in ConvertTFLBroadcastToMulOp optimization pass. This pattern is inherited from before and has proved to be increasing the model size due the introduction of large splat const. In its current form this pattern replaces a tfl.broadcast_to op (with rank<4) to a tfl.mul with allones tensor. This change will keep the broadcast_to ops as is because its clear that introducing MUL is not an optimization.,2025-03-10T18:32:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88967
tpu,copybara-service[bot],PR #23441: Up HERMETIC_CUDA_VERSION default value to 12.6.3,PR CC(Fix the compute_output_shape issue in tf.keras.layers.Bidirectional): Up HERMETIC_CUDA_VERSION default value to 12.6.3 Imported from GitHub PR https://github.com/openxla/xla/pull/23441 Copybara import of the project:  caacac1e24c0f72354b80d1b3e836c4c35c194de by Shraiysh Vaishay : Up HERMETIC_CUDA_VERSION default value to 12.6.3 Merging this change closes CC(Fix the compute_output_shape issue in tf.keras.layers.Bidirectional) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23441 from shraiysh:uphermeticcudadefault caacac1e24c0f72354b80d1b3e836c4c35c194de,2025-03-10T17:34:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88964
tpu,nassimus26,could not find registered transfer manager for platform Host -- check target linkage 	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_33145]," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.8  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi since I added some conv2D to my model I am getting this error on the Colab TPU  :  ``` NotFoundError: Graph execution error: Detected at node StatefulPartitionedCall defined at (most recent call last):   File """", line 198, in _run_module_as_main   File """", line 88, in _run_code   File ""/usr/local/lib/python3.11/distpackages/colab_kernel_launcher.py"", line 37, in    File ""/usr/local/lib/python3.11/distpackages/traitlets/config/application.py"", line 992, in launch_instance   File ""/usr/local/lib/python3.11/distpackages/ipykernel/kernelapp.py"", line 712, in start   File ""/usr/local/lib/python3.11/distpackages/tornado/platform/asyncio.py"", line 205, in start   File ""/usr/lib/python3.11/asyncio/base_events.py"", line 608, in run_forever   File ""/usr/lib/python3.11/asyncio/base_events.py"", line 1936, in _run_once   File ""/usr/lib/python3.11/asyncio/events.py"", line 84, in _run   File ""/usr/local/lib/python3.11/distpackages/ipykernel/kernelbase.py"", line 510, in dispatch_queue   File ""/usr/local/lib/python3.11/distpackages/ipykernel/kernelbase.py"", line 499, in process_one   File ""/usr/local/lib/python3.11/distpackages/ipykernel/kernelbase.py"", line 406, in dispatch_shell   File ""/usr/local/lib/python3.11/distpackages/ipykernel/kernelbase.py"", line 730, in execute_request   File ""/usr/local/lib/python3.11/distpackages/ipykernel/ipkernel.py"", line 383, in do_execute   File ""/usr/local/lib/python3.11/distpackages/ipykernel/zmqshell.py"", line 528, in run_cell   File ""/usr/local/lib/python3.11/distpackages/IPython/core/interactiveshell.py"", line 2975, in run_cell   File ""/usr/local/lib/python3.11/distpackages/IPython/core/interactiveshell.py"", line 3030, in _run_cell   File ""/usr/local/lib/python3.11/distpackages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner   File ""/usr/local/lib/python3.11/distpackages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async   File ""/usr/local/lib/python3.11/distpackages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes   File ""/usr/local/lib/python3.11/distpackages/IPython/core/interactiveshell.py"", line 3553, in run_code   File """", line 48, in    File """", line 47, in train   File """", line 82, in prep   File """", line 78, in train_step   File ""/usr/local/lib/python3.11/distpackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler   File ""/usr/local/lib/python3.11/distpackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit   File ""/usr/local/lib/python3.11/distpackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function   File ""/usr/local/lib/python3.11/distpackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator could not find registered transfer manager for platform Host  check target linkage 	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_33145] ```  Standalone code to reproduce the issue Here's the Conv2D which I added before getting this Error :  ``` cnn = layers.Conv2D(300, (3, 3), strides=(3, 3), activation=""relu"")(input) cnn = layers.MaxPooling2D((2, 2))(cnn) cnn = layers.Conv2D(120, (3, 3), strides=(3, 3), activation=""relu"")(cnn) cnn = layers.AveragePooling2D((2, 2))(cnn) cnn = layers.Conv2D(64, (2, 2), strides=(2, 2), activation=""sigmoid"")(cnn) ```",2025-03-10T16:27:36Z,stat:awaiting response type:bug stale comp:dist-strat TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88960,"Hi **** , Apologies for the delay, and thank you for raising your concern here. Could you please share the complete code snippet? This would help us investigate the issue more effectively. Additionally, I noticed that you are using an older version of TensorFlow (2.8). I recommend upgrading to the latest version and checking if the issue persists. If the problem still occurs, please let us know, and we will be happy to assist further. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],PR #23553: Remove unused CUPTI/NVTX #include,"PR CC(Relationships between Grappler, GraphOptimizer and GraphOptimizationPass): Remove unused CUPTI/NVTX include Imported from GitHub PR https://github.com/openxla/xla/pull/23553 All the other lines are insisted upon by the `clangformat` presubmit check. Copybara import of the project:  152733177dd35876b6c0614917e31bab188fc54c by Olli Lupton : Remove unused CUPTI/NVTX include  970f55f476a9b4bf2dcb829152d4104f21864436 by Olli Lupton : clangformat Merging this change closes CC(Relationships between Grappler, GraphOptimizer and GraphOptimizationPass) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23553 from olupton:removeunusedinclude 970f55f476a9b4bf2dcb829152d4104f21864436",2025-03-10T15:18:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88954
tpu,VTaPo,Saving and Reloading custom model," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution MacOS Apple Sillicon  Mobile device _No response_  Python version 3.11.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I have my custom model like this: () class CategoricalPreprocessor(tf.keras.Model):     def __init__(self, categorical_features, num_oov_indices=1, vocabularies=None, **kwargs):         """"""         A class for preprocessing categorical features using StringLookup.         Args:             categorical_features (list): List of categorical feature names.             num_oov_indices (int): Number of outofvocabulary (OOV) indices for unknown values.             **kwargs: Additional arguments passed to the parent class.         """"""         super().__init__(**kwargs)         self.categorical_features = categorical_features          Initialize InputLayers for each categorical feature.         self.input_layers = {             feature: tf.keras.Input(shape=(None,), dtype=tf.string, name=f""{feature}_input"")             for feature in categorical_features         }          Initialize StringLookup layers for each feature with OOV handling.         self.string_lookups = {             feature: tf.keras.layers.StringLookup(num_oov_indices=num_oov_indices, name=f""{feature}_lookup"")             for feature in categorical_features         }         if vocabularies!=None:             for feature in categorical_features:                 self.string_lookups[feature].set_vocabulary(vocabularies[feature])     def adapt(self, data):         """"""         Prepares the StringLookup layers with the vocabulary from the given data.         Args:             data (dict): Dictionary where keys are feature names and values are numpy arrays.         """"""         for feature in self.categorical_features:              Adapt each StringLookup layer to the corresponding feature's data.             self.string_lookups[feature].adapt(data[feature])     def call(self, inputs):         """"""         Applies StringLookup transformation to each categorical feature.         Args:             inputs (dict): A dictionary where keys are feature names and values are tensors or numpy arrays.         Returns:             dict: A dictionary where keys are feature names and values are tensors transformed by StringLookup.         """"""         return {             feature: self.string_lookupsfeature             for feature in self.categorical_features         }     def get_config(self):         config = super().get_config()         vocabularies = {}         for feature in self.categorical_features:             vocabularies[feature] = self.string_lookups[feature].get_vocabulary()         config.update({             ""num_oov_indices"": num_oov_indices,             ""categorical_features"": categorical_features,             ""vocabularies"": vocabularies         })         return config          def from_config(cls, config):         return cls(**config) I saved my model by model.save('preprocessor.keras') Then, I reloaded it for inference, but I got an error ""Unknown layer 'CategoricalPreprocessor' if I didn't redefine my code about CategoricalPreprocessor in the inference file. Is there a way to save and load the custom model so that when I use it for the inference phase, I can just reload the checkpoint and use it without having to redefine the class (without having to import my class into the inference file)? Please help me, thank you very much!  Standalone code to reproduce the issue ```shell I can't share full code. ```  Relevant log output ```shell ```",2025-03-10T15:04:16Z,stat:awaiting response type:support stale TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/88953,PLease tell me any way that I can reload model without access model definition?,"All custom operations and custom layers need to be accessible to the side that uses the model. Otherwise, the graph cannot be interpretted. You don't need to share the full model architecture/definition, just make sure the code that implements the custom addition is available.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,copybara-service[bot],PR #23509: [GPU] Add cuDNN 9.8.0.,PR CC(Does keras saved Model can output tensor?): [GPU] Add cuDNN 9.8.0. Imported from GitHub PR https://github.com/openxla/xla/pull/23509 Copybara import of the project:  913743e282a952ca475678a5c713bd1ac502dd2f by Ilia Sergachev : [GPU] Add cuDNN 9.8.0. Merging this change closes CC(Does keras saved Model can output tensor?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23509 from openxla:cudnn_980 913743e282a952ca475678a5c713bd1ac502dd2f,2025-03-10T14:02:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88950
oom,copybara-service[bot],PR #23552: [ROCm] Add rocm deps for ragged_all_to_all_kernel,"PR CC(if i set the inputshape with (None, None, featdim),how can I split the input by second dimention(axis=1)?): [ROCm] Add rocm deps for ragged_all_to_all_kernel Imported from GitHub PR https://github.com/openxla/xla/pull/23552 Kernel added in https://github.com/openxla/xla/commit/be68e80894862fe97757ea2b6110958ef4244c21. For ROCm build is currently failing with: ``` [20250309T01:01:48.040Z] ERROR: /tf/xla/xla/service/gpu/kernels/BUILD:291:13: Compiling xla/service/gpu/kernels/ragged_all_to_all_kernel.cu.: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing CppCompile command (from target //xla/service/gpu/kernels:ragged_all_to_all_kernel_gpu) external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc U_FORTIFY_SOURCE fstackprotector Wall Wunusedbutsetparameter Wnofreenonheapobject fnoomitframepointer ... (remaining 148 arguments skipped) [20250309T01:01:48.040Z] /root/.cache/bazel/_bazel_root/217377b0e928b171b843eb11ea7bc36e/execroot/xla/external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc:23: DeprecationWarning: 'pipes' is deprecated and slated for removal in Python 3.13 [20250309T01:01:48.040Z]   import pipes [20250309T01:01:48.040Z] xla/service/gpu/kernels/ragged_all_to_all_kernel.cu.cc:52:1: error: ‘__global__’ does not name a type [20250309T01:01:48.040Z]    52     ``` This PR just adds necessary deps for rocm  Copybara import of the project:  80f96ff15de134bf14ff00a4483f7b6707744445 by Milica Makevic : Add rocm deps for ragged_all_to_all_kernel Merging this change closes CC(if i set the inputshape with (None, None, featdim),how can I split the input by second dimention(axis=1)?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23552 from ROCm:hotfix_250310 80f96ff15de134bf14ff00a4483f7b6707744445",2025-03-10T14:00:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88949
tpu,copybara-service[bot],Allow to replace an instruction in the HloDfsReachability data structure.,"Allow to replace an instruction in the HloDfsReachability data structure. This will be needed when we want to use HloDfsReachability in PriorityFusion. When doing a regular (nonmultioutput) fusion, we don't need to rebuild the whole data structure and can just replace a single instruction with the created fusion instruction if needed.",2025-03-10T13:06:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88947
tpu,copybara-service[bot],Make MultiOutputFusionCreatesCycle a standalone method in anonymous namespace (NFC).,Make MultiOutputFusionCreatesCycle a standalone method in anonymous namespace (NFC). The method is not called outside of instruction_fusion.cc,2025-03-10T12:35:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88944
tpu,copybara-service[bot],PR #23551: [XLA:GPU] Warn if tensor memory usage is too high,"PR CC([Resolved] Inconsistent output shape for tf.image.resize_images() when preserve_aspect_ratio=True): [XLA:GPU] Warn if tensor memory usage is too high Imported from GitHub PR https://github.com/openxla/xla/pull/23551 Provide a better error message if the selected Triton gemm config uses too much TMEM (tensor memory) on Blackwell, similar to the shared memory warning. Copybara import of the project:  45f3580601098a8a1af489dccb54803acd315143 by Sergey Kozub : [XLA:GPU] Warn if tensor memory usage is too high Merging this change closes CC([Resolved] Inconsistent output shape for tf.image.resize_images() when preserve_aspect_ratio=True) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23551 from openxla:skozub/warn_tmem 45f3580601098a8a1af489dccb54803acd315143",2025-03-10T12:34:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88943
strategy,copybara-service[bot],PR #23468: Ensure PTX version compatibility w/ Clang & ptxas,"PR CC(Add option for inferring op attributes from inputs): Ensure PTX version compatibility w/ Clang & ptxas Imported from GitHub PR https://github.com/openxla/xla/pull/23468 Using the flag `cudafeature=+ptx`, Clang can be instructed to emit a specific PTX version from the NVPTX backend. If this flag is omitted, then Clang might emit a newer version of PTX than what ptxas from Hermetic CUDA can recognize which can lead to compilation errors. This commit adds a mapping from Clang & CUDA version to PTX version in `third_party/gpus/cuda/hermetic/cuda_redist_versions.bzl` which will need to be updated over time. If either the version for Clang or CUDA cannot be mapped to a PTX version, then configuration will fail. Resolves openxla/xla CC(Dataset shard index automatic change in Estimator DistributionStrategy) Copybara import of the project:  49c5940498f608b82539243b286431a74cdfc0dd by Jack Wolfard : Ensure PTX version compatibility w/ Clang & ptxas Using the flag `cudafeature=+ptx`, Clang can be instructed to emit a specific PTX version from the NVPTX backend. If this flag is omitted, then Clang might emit a newer version of PTX than what ptxas from Hermetic CUDA can recognize which can lead to compilation errors. This commit adds a mapping from Clang & CUDA version to PTX version in `third_party/gpus/cuda/hermetic/cuda_redist_versions.bzl` which will need to be updated over time. If either the version for Clang or CUDA cannot be mapped to a PTX version, then configuration will fail. Resolves openxla/xla CC(Dataset shard index automatic change in Estimator DistributionStrategy) Merging this change closes CC(Add option for inferring op attributes from inputs) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23468 from JackWolfard:clangptxasversion 49c5940498f608b82539243b286431a74cdfc0dd",2025-03-10T07:38:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88939
tpu,jimwang118,[pcl] build failure," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 1.15.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? E:\vcpkg\buildtrees\pcl\src\pcl1.15.0cf88c69527.clean\simulation\src\glsl_shader.cpp(163): error C3861: 'gluErrorString': identifier not found E:\vcpkg\buildtrees\pcl\src\pcl1.15.0cf88c69527.clean\simulation\src\glsl_shader.cpp(163): error C2593: 'operator <<' is ambiguous  Standalone code to reproduce the issue ```shell vcpkg install pcl[core,opengl,qt,simulation,surfaceonnurbs,tools,visualization,vtk]:x64windows ```  Relevant log output ```shell E:\vcpkg\buildtrees\pcl\src\pcl1.15.0cf88c69527.clean\simulation\src\glsl_shader.cpp(163): error C3861: 'gluErrorString': identifier not found E:\vcpkg\buildtrees\pcl\src\pcl1.15.0cf88c69527.clean\simulation\src\glsl_shader.cpp(163): error C2593: 'operator <<' is ambiguous ```",2025-03-10T06:42:15Z,stat:awaiting response type:bug stale TF 1.15,closed,0,3,https://github.com/tensorflow/tensorflow/issues/88936,"Hi **** , Apologies for the delay, and thanks for raising your concern here. This issue does not appear to be related to TensorFlow. It seems to be associated with PCL (Point Cloud Library) and vcpkg. I recommend raising the issue in the appropriate repository for a quicker resolution. Additionally, I noticed that you are using an older version of TensorFlow (1.15.0), which is no longer supported. Please consider upgrading to the latest version for better compatibility and support. Here is the official TensorFlow upgrade documentation, and windows for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Are you satisfied with the resolution of your issue? Yes No
tpu,pureayu,Non-quantized MobileNetV2 model gives unexpectedly low accuracy in TensorFlow Lite evaluation," 1. **System Information**  **Device**: Xiaomi 14 (Android)  **TensorFlow Installation**: Built from source  2. **Link**  **Evaluation Task Link**: [TensorFlow Lite ImageNet Classification Evaluation](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification)  3. **Problem** After following the instructions on the page, I successfully compiled and ran `run_eval`, but the accuracy is very low. **Here is the output after running the evaluation:**  **Tested 10 images**  **Accuracy**: 0 for all metrics ```bash INFO: Num evaluation runs: 10 INFO: Preprocessing latency: avg=5540(us), std_dev=0(us) INFO: Inference latency: avg=96199(us), std_dev=91761(us) INFO: Top1 Accuracy: 0 INFO: Top2 Accuracy: 0 INFO: Top3 Accuracy: 0 INFO: Top4 Accuracy: 0 INFO: Top5 Accuracy: 0 INFO: Top6 Accuracy: 0 INFO: Top7 Accuracy: 0 INFO: Top8 Accuracy: 0 INFO: Top9 Accuracy: 0 INFO: Top10 Accuracy: 0 ``` **Then I tested with `mobilenet_v2.tflite` (nonquantized model):** ```bash INFO: Num evaluation runs: 10 INFO: Preprocessing latency: avg=5101.7(us), std_dev=0(us) INFO: Inference latency: avg=17079.5(us), std_dev=12222(us) INFO: Top1 Accuracy: 0.1 INFO: Top2 Accuracy: 0.1 INFO: Top3 Accuracy: 0.2 INFO: Top4 Accuracy: 0.2 INFO: Top5 Accuracy: 0.2 INFO: Top6 Accuracy: 0.2 INFO: Top7 Accuracy: 0.2 INFO: Top8 Accuracy: 0.2 INFO: Top9 Accuracy: 0.2 INFO: Top10 Accuracy: 0.2 ``` These results are not consistent with the demo accuracy shown on the page. **Demo output (with `mobilenet_v1_quant.tflite` model)**: ```bash INFO: Num evaluation runs: 300  Total images evaluated INFO: Preprocessing latency: avg=13772.5(us), std_dev=0(us) INFO: Inference latency: avg=76578.4(us), std_dev=600(us) INFO: Top1 Accuracy: 0.733333 INFO: Top2 Accuracy: 0.826667 INFO: Top3 Accuracy: 0.856667 INFO: Top4 Accuracy: 0.87 INFO: Top5 Accuracy: 0.89 INFO: Top6 Accuracy: 0.903333 INFO: Top7 Accuracy: 0.906667 INFO: Top8 Accuracy: 0.913333 INFO: Top9 Accuracy: 0.92 INFO: Top10 Accuracy: 0.923333 ```  4. **Steps to Reproduce** 1. Follow the steps outlined in the evaluation task documentation. 2. Compile and run the `run_eval` binary with the respective model (`mobilenet_v2.tflite` or `mobilenet_v1_quant.tflite`). 3. Observe the output accuracy.  5. **Expected Behavior** I expect that the accuracy for the nonquantized `mobilenet_v2.tflite` model should be higher and closer to the demo output shown in the example above.  6. **Actual Behavior** The nonquantized `mobilenet_v2.tflite` model is giving very low accuracy, and the results are inconsistent with the demo output.  7. **Environment**  **TensorFlow Lite version tf 2.18.0  **Device**: Xiaomi 14, Android version 11  **Model**: `mobilenet_v2.tflite` (nonquantized)  8. **Additional Information**  I have verified that the images and labels are correctly set up and placed.  I followed the exact steps provided in the official documentation, but the results are still lower than expected.",2025-03-08T09:17:55Z,TFLiteConverter,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88906
quantization,copybara-service[bot],"Move tfl_quantization_driver, originally from quantization_lib, to the rest of the quantization_lib fork in lite","Move tfl_quantization_driver, originally from quantization_lib, to the rest of the quantization_lib fork in lite",2025-03-07T22:27:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88864
tpu,cousteaulecommandant,Fixed-point Softmax() calls exp_on_negative_values() twice,"Fixedpoint `Softmax()` calls `exp_on_negative_values()` twice per element: once to calculate the value of the exp of each element https://github.com/tensorflow/tensorflow/blob/4266e9ab53bb3f7fca0dd816b965b82862b69b4e/tensorflow/lite/kernels/internal/reference/softmax.hL131 and another one a few lines earlier to calculate the sum of all exps https://github.com/tensorflow/tensorflow/blob/4266e9ab53bb3f7fca0dd816b965b82862b69b4e/tensorflow/lite/kernels/internal/reference/softmax.hL109L110 The `exp_on_negative_values()` function can be rather slow, taking most of the time of `Softmax()` (which can be especially concerning in embedded implementations), so calling it twice per element makes the whole `Softmax()` function be about twice as slow as necessary. Could this be optimized so that it is only called once per element?  Store the exp results in a temporary array (for example, reusing `output_data`), computing the sum as they're stored, and then applying the scaling to the precomputed exp results rather than computing them again. (`SoftmaxInt16()` seems to follow a similar approach.)",2025-03-07T21:48:15Z,comp:lite awaiting PR merge,open,0,0,https://github.com/tensorflow/tensorflow/issues/88861
bert,copybara-service[bot],[XLA] Add an online topological sort.,"[XLA] Add an online topological sort. This PR adds an online topological sort implementation based on: Bender, M.A., Fineman, J.T., Gilbert, S. and Tarjan, R.E., 2015. A new approach to incremental cycle detection and related problems. ACM Transactions on Algorithms (TALG), 12(2), pp.122. (https://dl.acm.org/doi/abs/10.1145/2756553). XLA uses (offline) topological sorts for both computations (HloModule::MakeComputationPostOrder) and instructions (HloComputation::MakeInstructionPostOrder), and these are significant time consumers during the HLO pipeline. It is very common for an HLO pass to want to iterate over either computations or instructions in postorder or reverse postorder, and so we repeatedly compute an offline topological sort in many HLO passes. Rather than repeatedly recomputing the topological ordering using an offline topological sort, we can save time if we instead maintain instructions and computations in a topological ordering at all times using an online topological sort. Future PRs will change the HLO data structures to use an online topological sort. The implementation in this PR uses an intrusive data structure that can be embedded into other objects (e.g., HloInstruction or HloComputation).",2025-03-07T21:06:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88855
oom,copybara-service[bot],PR #23340: [ROCm] Add -Wno-stringop-truncation to build flags,"PR CC(TFLite object detection toco error(the size of detect.tflite is 0kb)): [ROCm] Add Wnostringoptruncation to build flags Imported from GitHub PR https://github.com/openxla/xla/pull/23340 Build error emerged after https://github.com/openxla/xla/commit/bd281e6ecdf5ede173e39f7402e547bbb9e1dc90. Added a workaround until the issue is fixed in upb. Log: ``` ERROR: /root/.cache/bazel/_bazel_root/217377b0e928b171b843eb11ea7bc36e/external/upb/BUILD:57:11: Compiling upb/upb.c failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing CppCompile command (from target @//:upb) external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc U_FORTIFY_SOURCE fstackprotector Wall Wunusedbutsetparameter Wnofreenonheapobject fnoomitframepointer ... (remaining 33 arguments skipped) In file included from /usr/include/string.h:535,                  from external/upb/upb/upb.h:16,                  from external/upb/upb/upb.c:2: In function ‘strncpy’,     inlined from ‘upb_status_seterrmsg’ at external/upb/upb/upb.c:40:3: /usr/include/x86_64linuxgnu/bits/string_fortified.h:95:10: error: ‘__builtin_strncpy’ specified bound 127 equals destination size [Werror=stringoptruncation]    95                                    ~~~~~~~~~~~~~~~~~~~~~~~~~ ``` There is a similar workaround in Jax as well > https://github.com/jaxml/jax/pull/4974/files Copybara import of the project:  bb1d0bbb7913e1feb79f4e9cb0a748a030ad0fd2 by Milica Makevic : Add Wnostringoptruncation to build flags for ROCm Merging this change closes CC(TFLite object detection toco error(the size of detect.tflite is 0kb)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23340 from ROCm:ci_hotfix_250304 bb1d0bbb7913e1feb79f4e9cb0a748a030ad0fd2",2025-03-07T19:46:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88849
strategy,copybara-service[bot],PR #23485: Add `-Wno-c23-extensions` flag with Clang >= 19,"PR CC(未找到相关数据): Add `Wnoc23extensions` flag with Clang >= 19 Imported from GitHub PR https://github.com/openxla/xla/pull/23485 XLA's version of gRPC depends on an ancient version of upb which is unable to compile on Clang >= 19 without this flag. Also, added clarification for why the `Wnognuoffsetofextensions` is used for Clang 16 > 18. To test this change, run the following locally: ```sh $ docker run rm it \     name xla \     v $PWD:/xla \     w /tmp \     silkeh/clang:19 $ wget https://github.com/bazelbuild/bazelisk/releases/download/v1.25.0/bazeliskamd64.deb $ dpkg i bazeliskamd64.deb $ cd /xla $ ./configure.py backend=CPU $ bazel \     build \     repo_env=HERMETIC_PYTHON_VERSION=3.11 \     spawn_strategy=sandboxed \     //:upb ``` Copybara import of the project:  42567b5f1fb339adae9ea574749513c62c0287f6 by Jack Wolfard : Add `Wnoc23extensions` flag with Clang >= 19 XLA's version of gRPC depends on an ancient version of upb which is unable to compile on Clang >= 19 without this flag. Also, added clarification for why the `Wnognuoffsetofextensions` is used for Clang 16 > 18. Merging this change closes CC(未找到相关数据) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23485 from JackWolfard:clang19upb 42567b5f1fb339adae9ea574749513c62c0287f6",2025-03-07T18:04:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88844
yi,copybara-service[bot],#litert Detect memory sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`.,"litert Detect memory sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`. Trying to load a library using `RTLD_DEEPBIND` is not supported by memory sanitizers. In an effort to enable testing we strip the flag. If this leads to unintended behaviour, either remove the `RTLD_DEEPBIND` flag or run without a memory sanitizer. See https://github.com/google/sanitizers/issues/611 for more information.",2025-03-07T17:47:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88843
tpu,wonjeon,[mlir][tosa] Add Rescale Attribute changes to TOSA legalizations and Switch zero point of avgpool2d to input variable type,We've been pushing TOSA v1.0 LLVM patches to upstream. This PR includes commit(s) to keep the following operator(s) to be aligned with LLVM: [mlir][tosa] Make RESCALE op input_unsigned and output_unsigned attributes required https://github.com/llvm/llvmproject/pull/129339 [mlir][tosa] Switch zero point of avgpool2d to input variable type https://github.com/llvm/llvmproject/pull/128983,2025-03-07T16:31:09Z,kokoro:force-run ready to pull size:M,closed,0,1,https://github.com/tensorflow/tensorflow/issues/88835,"Local testing done successfully: INFO: Analyzed 33 targets (2 packages loaded, 33382 targets configured). INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 142.222s, Critical Path: 96.99s INFO: 426 processes: 1 internal, 425 local. INFO: Build completed successfully, 426 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test         PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test             PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test      PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test                PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test   PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test           PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test  PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test      PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test         PASSED in 0.1s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 2.5s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test     PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test        PASSED in 0.1s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test   PASSED in 0.2s Executed 17 out of 17 tests: 17 tests pass."
sharding,copybara-service[bot],Remove axes sized 1 in the sharding but not the mesh itself,Remove axes sized 1 in the sharding but not the mesh itself,2025-03-07T15:01:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88829
fp16,copybara-service[bot],PR #22541: [ROCm] Cleanup atomics support,"PR CC(Java process crashes during model loading): [ROCm] Cleanup atomics support Imported from GitHub PR https://github.com/openxla/xla/pull/22541 Weaken the ordering barriers to match what atomicAdd does on rocm. Emulate fp16 atomic on top of packed fp16 atomic where possible. Also for bfloat16 atomics, albeit those don't get matched right now due to FloatNormalization. Left in support for fp16 and bfloat16 vector atomics. We might enable the vectorization for them in the future if we can prove the access satisfies 4byte aligment. Copybara import of the project:  06907ef930c76c824788e86db8d3b30eeb141175 by Dragan Mladjenovic : [ROCm] Cleanup atomics support Merging this change closes CC(Java process crashes during model loading) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22541 from ROCm:atomics_cleanup 06907ef930c76c824788e86db8d3b30eeb141175",2025-03-07T14:45:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88828
tpu,yjiangling,The CTC loss is Numeric Instability inTensorFlow2.x when use sparse tensor as input," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf2.9，tf2.12，tf2.15  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.2  Mobile device _No response_  Python version 3.8, 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA11.6，CUDA11.8，CUDA12.0  GPU model and memory RTX3080，A10，A100，L40，......  Current behavior? Hi, all,         I find when train CTC model in the TF2.x environment，if the dense of the labels are feed into the function `tf.nn.ctc_loss`，it will assume a lot of GPU memory and the train speed is very slow; when the labels are convert to sparse tensor and feed to the function, the train speed will be much faster, but it is very easy to get NAN during the training, resulting the failure of the train. When compare the reult of ctc loss for this two type input, as well as use ctc loss function in TF1.x (tf.compat.v1.nn.ctc_loss), it seems OK. That is to say, when use sparse tensor as input for tf.nn.ctc_loss function in TF2.x, the result is numeric unstable? Or there are some problem in the gradient computation for it? Which leading the gradient explosion. And it seems that it's a common problem in TensorFlow2, because we have tried it in TensorFlow2.9，TensorFlow2.12 and TensorFlow2.15, both of them have this issue. All the  train ard conducted with the same network, same training parameters and same train dataset.   Standalone code to reproduce the issue When use sparse tensor as input in tf.nn.ctc_loss function, the output of the Conformer network (ctc_logits) will become **nan** after a few train steps, which lead to the train loss become **nan** and the failure of the train... Here is the code we used: 	ctc_logits, the final output of the conformer network, with shape [frames, batch_size, num_labels] 	xlen, length of input sequence in ctc_logits, with shape [batch_size] 	ys, the labels of each train sample, with shape [batch_size, max_label_seq_length]  	ylen, length of reference label sequence in ys, with shape [batch_size] 	ys_sparse = tf.sparse.from_dense(ys)  convert dense tensor to sparse tensor 	ctc_loss = tf.compat.v1.nn.ctc_loss(labels=ys_sparse, inputs=ctc_logits, sequence_length=xlen, ignore_longer_outputs_than_inputs=True)  train with ctc loss function in TF1.x, the labels use dense or sparse tensor are all fine 	ctc_loss = tf.nn.ctc_loss(labels=ys, logits=ctc_logits, label_length=ylen, logit_length=xlen, blank_index=1)  train with ctc loss function in TF2.x, use dense as input for labels, it OK 	ctc_loss = tf.nn.ctc_loss(labels=ys_sparse, logits=ctc_logits, label_length=None, logit_length=xlen, blank_index=1)  train with ctc loss function in TF2.x, use sparse tensor as input for labels will leadingt to gradient explosion 1. Why this happend? Is it a common phenomenon in TF2.x? 2. How can I solve it? By the way, I also tried to add gradient clip in optimizer function like `optimizer = tf.keras.optimizers.Adam(learning_rate=1e4, clipnorm=1.0)` or `optimizer = tf.keras.optimizers.Adam(learning_rate=1e4, clipvalue=1.0)`, but does't work. I will be great appreciate if anyone can give some suggestions. kindly help...  Relevant log output Even when we use a pretrained model with ctc loss function in TF1.x or dense input in TF2,x, and then convert to use sparse tensor as input in TF2.x, the gradient will explode soon... !Image",2025-03-07T09:38:27Z,stat:awaiting response type:bug stale comp:ops TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88817,"Hi **** , Apologies for the delay, and thanks for raising your concern here. Could you please provide the complete code snippet? This would help us investigate further. Additionally, I noticed that you are using an older version of TensorFlow. Could you try updating to the latest version and let us know if the issue still persists? Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],PR #23414: [GPU] Let cuDNN fusion compiler process graphs with assigned workspace. ,PR CC(Improve shape function of tf.sparse_reduce_sum): [GPU] Let cuDNN fusion compiler process graphs with assigned workspace.  Imported from GitHub PR https://github.com/openxla/xla/pull/23414 This enables running optimized HLO. Copybara import of the project:  2933f6f84e612d0e47efb6875ea7c272c719edff by Ilia Sergachev : [GPU][NFC] Cleanup a test.  c5d96a27927699979f09e93248ba389d54795b8e by Ilia Sergachev : [GPU] Let cuDNN fusion compiler process graphs with assigned workspace. This enables running optimized HLO. Merging this change closes CC(Improve shape function of tf.sparse_reduce_sum) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23414 from openxla:fix_compilation_with_workspace c5d96a27927699979f09e93248ba389d54795b8e,2025-03-07T08:31:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88813
tpu,xxHn-pro,cuSteamSynchronize take tons of time," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.14  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.6  GPU model and memory 4070S  Current behavior? I am trying to benchmark the inference.  A simple code as  ``` // Configure a `CallableHandle` that feeds from and fetches to a device. Status SetupCallable(std::unique_ptr& session,                      std::vector& input_info,                      std::vector& output_info,                      const string& device_name,                      bool input_from_device,                      bool output_to_host,                      tensorflow::Session::CallableHandle* handle) {   tensorflow::CallableOptions opts;   for (const auto& info : input_info) {     const string& name = info.name();     opts.add_feed(name);     if (input_from_device) {       opts.mutable_feed_devices()>insert({name, device_name});     }   }   for (const auto& info : output_info) {     const string& name = info.name();     opts.add_fetch(name);     if (!output_to_host) {       opts.mutable_fetch_devices()>insert({name, device_name});     }   }   opts.set_fetch_skip_sync(true);   return session>MakeCallable(opts, handle); } ``` The inference is run by ``` start_time = std::chrono::steady_clock::now();       TFTRT_ENSURE_OK(         bundle.session>RunCallable(handle, inputs_device, &outputs, nullptr));       // Sync, as `set_fetch_skip_sync(false)` is currently not implemented       TFTRT_ENSURE_OK(device>Sync());       end_time = std::chrono::steady_clock::now(); ``` The profile is collected by `nsys profile w true t cuda,nvtx,cudnn,cublas f true x true o profile_c /opt/tensorflow/tensorflowsource/bazelbin/tensorflow/examples/image_classification/MiniBatch/mini_tftrt model_path=""./resnet50_saved_model_RT"" batch_size=64 output_to_host=False` And I found that cuSteamSynchronize takes most of time, as shown below: profile_c.zip !Image I think the real computation is done and the GPU is wasting its time. Is that right? I don't see any other kernel working. So I try to skip that Sync by setting `opts.set_fetch_skip_sync(true);`. However, the  cuSteamSynchronize  is still on there. No mater whether `device>Sync()` is used or not.  The time consumption and cuSteamSynchronize  are always unchanged even I set the output to host. Here is the code and readme to reproduce the issue. MiniBatch2.zip  Standalone code to reproduce the issue ```shell See the zip file at the end. ```  Relevant log output ```shell ```",2025-03-07T04:27:50Z,type:bug TF2.14,open,0,2,https://github.com/tensorflow/tensorflow/issues/88796,"Hi **pro** , Apologies for the delay, and thanks for raising your concern here.I noticed a version compatibility issue. I am attaching the official documentation for your reference, please verify all compatibility requirements. Additionally, I see that you are using an older version of TensorFlow (2.14). Could you please try updating to the latest version for better results? Thank you!","Hi  , Thanks for your reply. Actually I try the same code at other machine. And there are more information from the new test, which indicate that CUDA kernel is running all the time during the waiting of tensorRT kernel. I think the tensorflow is running asynchronously. It turns out that is not a problem of cuSteamSynchronize !Image PS, the new machine with better GPU uses singularity to run the docker. It runs real CentOS Linux instead of WSL. It is still not clear why I can not get all information at my first try. Anyway, Thanks you. If there is any misunderstand above, pls correct it."
sharding,copybara-service[bot],[XLA] Allow sharding to propagate across pin to device memory and pin to vmem custom calls.,[XLA] Allow sharding to propagate across pin to device memory and pin to vmem custom calls.,2025-03-07T00:59:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88783
tpu,jsuj1th,import error," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.8  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior?  ImportError                               Traceback (most recent call last) File c:\Users\SUJITH\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[1], line 3       1  !pip install tensorflow > 3 import tensorflow as tf       4 from tensorflow.keras.applications import MobileNetV3Small       5 from tensorflow.keras.models import Model File c:\Users\SUJITH\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File c:\Users\SUJITH\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:85      83     sys.setdlopenflags(_default_dlopen_flags)      84 except ImportError: > 85   raise ImportError(      86       f'{traceback.format_exc()}'      87       f'\n\nFailed to load the native TensorFlow runtime.\n'      88       f'See https://www.tensorflow.org/install/errors '      89       f'for some common causes and solutions.\n'      90       f'If you need help, create an issue '      91       f'at https://github.com/tensorflow/tensorflow/issues '      92       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""c:\Users\SUJITH\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.   Standalone code to reproduce the issue ```shell pip install tensorflow ```  Relevant log output ```shell ```",2025-03-06T23:45:28Z,stat:awaiting response type:build/install stale TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88780,"Hi ****, Apologies for the delay, and thanks for raising your concern here. I noticed that you are using an older version of TensorFlow (2.8). Could you please check with the latest version for better results? Also, please verify all compatibility requirements. I am attaching the official documentation for your reference. Additionally, a similar issue is currently being discussed, please follow that thread for further updates. CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Please always search for duplicate issues. Please do a minimum of effort for that and for properly formatting the issue.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #21638: Add the hlo verifier before host offloader to check host memory space,"PR CC([XLA] ResourceExhaustedError when trying to define a Sequential model in Keras under jit_scope context manager): Add the hlo verifier before host offloader to check host memory space Imported from GitHub PR https://github.com/openxla/xla/pull/21638 Ensure No Instructions Have Host Memory Space S(5) Before Host Offloader This change verifies that no instruction possesses host memory space S(5) prior to the host offloader pass. It addresses an issue where the HLO passes before the host offloader could inadvertently leak memory space annotations from the entry computation layout to the graph. In PR https://github.com/openxla/xla/pull/20426, the layout assignment pass was corrected to prevent instructions from inheriting memory space S(5) from the entry computation layout. This commit further ensures that such annotations are not propagated, keeping host memory space not changed until the host offloader pass. Copybara import of the project:  a785e186e919d3921a2922caca5fbca1f6eb0f37 by Jane Liu : Add the hlo verifier before host offloader to check host memory space  bebf8b97743e34b59114c1c0966b9cf1c5877b90 by Jane Liu : remove extra std::move and change InvalidArgumentError to Internal  9ac2bf59d93759f67d9a41578cd70dba78498b0f by Jane Liu : use ForEachSubshapeWithStatus Merging this change closes CC([XLA] ResourceExhaustedError when trying to define a Sequential model in Keras under jit_scope context manager) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21638 from zhenyingliu:verifier 9ac2bf59d93759f67d9a41578cd70dba78498b0f",2025-03-06T20:33:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88768
yi,copybara-service[bot],PR #23395: [ROCm] gfx950 support,PR CC(deploying the Tensorflow model in Python): [ROCm] gfx950 support Imported from GitHub PR https://github.com/openxla/xla/pull/23395 rotation please have a look Thanks Copybara import of the project:  6c6bfad5a896154c1a21c263cda433253e9f8597 by Chao Chen : gfx950 support Merging this change closes CC(deploying the Tensorflow model in Python) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23395 from ROCm:ci_gfx950 6c6bfad5a896154c1a21c263cda433253e9f8597,2025-03-06T19:48:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88765
int8,copybara-service[bot],PR #88546: Qualcomm AI Engine Direct - Wrapper tests & Refactor tensor wrapper & Fix rms norm,PR CC(Qualcomm AI Engine Direct  Wrapper tests & Refactor tensor wrapper & Fix rms norm): Qualcomm AI Engine Direct  Wrapper tests & Refactor tensor wrapper & Fix rms norm Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/88546  What 1. Refactor tensor wrapper      Safer tensor data getter and setter. 2. Add wrapper tests. 3. Fix rms norm builder      Change 0 beta tensor type to float32 for float32 input and uint8 for other input data types. See op support types here.  Tests `qnn_compiler_plugin_test` ``` [] Global test environment teardown [==========] 99 tests from 5 test suites ran. (3995 ms total) [  PASSED  ] 99 tests. ``` Copybara import of the project:  c4a8ccebc910318f6afaaccd552129fa92cfa29f by chunhsue : refine tensor wrapper  9ef17b23072f7d50e76d7c7cef37fa4fbf0ad577 by chunhsue : add wrapper tests  0feee34f0192e726ab40192a32c551ed396896d3 by chunhsue : fix rms norm builder Merging this change closes CC(Qualcomm AI Engine Direct  Wrapper tests & Refactor tensor wrapper & Fix rms norm) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88546 from jiunkaiy:dev/chunhsue/wrapper_tests 0feee34f0192e726ab40192a32c551ed396896d3,2025-03-06T19:27:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88764
quantization,copybara-service[bot],Copy tf/compiler/mlir/quantization/common/quantization_lib/quantization.td to Lite,Copy tf/compiler/mlir/quantization/common/quantization_lib/quantization.td to Lite,2025-03-06T18:26:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88751
tpu,copybara-service[bot],PR #22808: [ds-fusion] Enable GpuReduceScatterCombiner before ds-fusion,PR CC([Documentation] Format example list.): [dsfusion] Enable GpuReduceScatterCombiner before dsfusion Imported from GitHub PR https://github.com/openxla/xla/pull/22808 This PR depends on CC(1.12rc0 cherrypick request: Mark tensorflow/contrib/tpu:datasets_test flaky). This patch runs GpuReduceScatterCombiner before dsfusion so that the overhead of multiple nccl thunks is avoided in `gpu_compiler`. Copybara import of the project:  9334995a20cd7fcc501d873349475135eda726d1 by Shraiysh Vaishay : [dsfusion] Allow multiple buffers in reducescatter for dsfusion This patch adds support for multibuffer reducescatter in dynamicslicefusion. All of these will be a part of the same NCCL reducescatter thunk to avoid overheads of multiple NCCL thunks.  b3e6d0ee0ed54d122a5fcc9958beed898aac8324 by Shraiysh Vaishay : Address comments  d8a34e545c61f7bc1a44ee20da4efe8198738617 by Shraiysh Vaishay : Address comments  817c274ad4b715b28ae60cfabe88ee86bb6e1671 by Shraiysh Vaishay : Address comment  00743cdcf02a9a213519cbb89be55ea76376b4a0 by Shraiysh Vaishay : Move reducescatter combiner before dynamicslicefusion Merging this change closes CC([Documentation] Format example list.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22808 from shraiysh:gpu_compiler_enable_ds_fusion_rs_multiple_buffers 00743cdcf02a9a213519cbb89be55ea76376b4a0,2025-03-06T17:12:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88749
tpu,copybara-service[bot],PR #22799: [ds-fusion] Allow multiple buffers in reduce-scatter for ds-fusion,PR CC(1.12rc0 cherrypick request: Mark tensorflow/contrib/tpu:datasets_test flaky): [dsfusion] Allow multiple buffers in reducescatter for dsfusion Imported from GitHub PR https://github.com/openxla/xla/pull/22799 This patch adds support for multibuffer reducescatter in dynamicslicefusion. We also now call the reducescatter combiner pass before the dynamicslicefusion rewriter to ensure that we do not incur the overheads of multiple nccl calls. Copybara import of the project:  9334995a20cd7fcc501d873349475135eda726d1 by Shraiysh Vaishay : [dsfusion] Allow multiple buffers in reducescatter for dsfusion This patch adds support for multibuffer reducescatter in dynamicslicefusion. All of these will be a part of the same NCCL reducescatter thunk to avoid overheads of multiple NCCL thunks.  b3e6d0ee0ed54d122a5fcc9958beed898aac8324 by Shraiysh Vaishay : Address comments  d8a34e545c61f7bc1a44ee20da4efe8198738617 by Shraiysh Vaishay : Address comments  817c274ad4b715b28ae60cfabe88ee86bb6e1671 by Shraiysh Vaishay : Address comment Merging this change closes CC(1.12rc0 cherrypick request: Mark tensorflow/contrib/tpu:datasets_test flaky) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22799 from shraiysh:ds_fusion_rs_multiple_buffers 817c274ad4b715b28ae60cfabe88ee86bb6e1671,2025-03-06T17:04:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88748
yi,copybara-service[bot],#litert Detect address sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`.,"litert Detect address sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`. Trying to load a library using `RTLD_DEEPBIND` is not supported by address sanitizers. In an effort to enable testing we strip the flag. If this leads to unintended behaviour, either remove the `RTLD_DEEPBIND` flag or run without an address sanitizer. See https://github.com/google/sanitizers/issues/611 for more information.",2025-03-06T15:46:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88743
tpu,JiahuaZhao,tf.experimental.dlpack.to_dlpack() becomes performance bottleneck," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.9.0  Custom code Yes  OS platform and distribution Rocky Linux 8.6  Mobile device _No response_  Python version 3.9.7  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA11.7.0/cuDNN8.4.1.50  GPU model and memory NVIDIA A100SXM440GB  Current behavior? Hi, my code uses CuPy and TensorFlow, where CuPy is used for data preprocessing and postprocessing, and TensorFlow is responsible for loading models and inference. For the conversion between CuPy arrays and TF tensors, we use DLpack: https://docs.cupy.dev/en/stable/user_guide/interoperability.htmldlpack. However, it seems that `tf.experimental.dlpack.to_dlpack()` has become a performance bottleneck because I need to convert the inference result (TF tensor) into a CuPy array for postprocessing. And a single conversion takes about **0.13 seconds** but converting from a CuPy array to a TF tensor (`tf.experimental.dlpack.from_dlpack()`) takes less than **0.01 seconds**. The entire calculation process needs to be carried out in a loop, which usually requires hundreds of iterations, so the data conversion time will be magnified. So I want to know how to reduce the time taken by `tf.experimental.dlpack.to_dlpack()` if there are any suggestions. Thanks in advance :)  Standalone code to reproduce the issue ```shell tf_tensor= tf_model(tf.experimental.dlpack.from_dlpack(CuPy_array.toDlpack()), training=False) cp.from_dlpack(tf.experimental.dlpack.to_dlpack(tf_tensor)) ```  Relevant log output ```shell ```",2025-03-06T09:31:12Z,comp:gpu type:performance TF 2.9,open,0,3,https://github.com/tensorflow/tensorflow/issues/88731,"I get the same problem. But my goal is to get the pointer of the tensor. Time for `tf.experimental.dlpack.from_dlpack` is much longer than the inference itself, while it is extremely fast to get an pointer in C++ API.","Hi **** , Apologies for the delay, and thanks for raising your concern here. I noticed that you are using an older version of TensorFlow (12.9) and found a compatibility mismatch. Could you please check with the latest version? Ensuring version compatibility can help achieve better results. I am attaching the official documentation for your reference. If you are still facing the same issue with the latest version, please let us know so we can investigate further. Thank you!","> Hi **[](https://github.com/JiahuaZhao)** , Apologies for the delay, and thanks for raising your concern here. I noticed that you are using an older version of TensorFlow (12.9) and found a compatibility mismatch. Could you please check with the latest version? Ensuring version compatibility can help achieve better results. I am attaching the official documentation for your reference. If you are still facing the same issue with the latest version, please let us know so we can investigate further. >  > Thank you! Hi , thank you for your reply. I have tried TensorFlow up to version 2.16 (due to environmental restrictions, I cannot use the latest version), but this problem still occurs. But I found more details. When I don’t need to use TF inference in my code, both `tf.experimental.dlpack.from_dlpack()` and `tf.experimental.dlpack.to_dlpack()` are fast (both <0.01 sec, even in the loop body), for example: ``` for i in range(num):   a = cp.random.randn(248, 288, 112, 2).astype(cp.float32)   a_tf = tf.experimental.dlpack.from_dlpack(a.toDlpack())   cap = cp.from_dlpack(tf.experimental.dlpack.to_dlpack(a_tf)) ``` As soon as I used TF inference in it, the previous situation appeared (`tf.experimental.dlpack.to_dlpack()` has become a performance bottleneck), for example: ``` for i in range(num):   a = cp.random.randn(248, 288, 112, 2).astype(cp.float32)   a_tf = tf.experimental.dlpack.from_dlpack(a.toDlpack())   pred = model(a_tf, training=False)   cap = cp.from_dlpack(tf.experimental.dlpack.to_dlpack(pred)) ``` So I suspect that there is blocking, and I haven't done any professional performance analysis yet. Do you have any suggestions? By the way, CuPy version is 13.3. Thanks!"
opt,copybara-service[bot],PR #23307: [ROCm] Support multiple ROCm paths,"PR CC(Missing Documentation: Multiple links return 404 in Python API Guide): [ROCm] Support multiple ROCm paths Imported from GitHub PR https://github.com/openxla/xla/pull/23307 This PR provides support for multiple ROCm paths.  Usually ROCm is installed in a single location like ""/opt/rocm"" but that isn't always the case (ex. when using spack). To build with multiple ROCm paths set the environment variable `TF_ROCM_MULTIPLE_PATHS` to a "":"" seperated list of all the ROCm component paths and `LLVM_PATH` to the ROCm LLVM path. ~~To set the rpaths of the multiple ROCm lib directories use:~~ ~~`//rocm:multiple_rocm_rpath=True`~~ Edit: To set the rpaths of the multiple ROCm lib directories use: `//rocm:rocm_path_type=multiple` To set the rpaths of the hermetic libs use: `//rocm:rocm_path_type=hermetic` Copybara import of the project:  310f9cbdd2773e4aa3330c5c18e69a0bb845bbcf by Afzal Patel : Support multiple ROCm paths  572a3d46f54f7686915327f038d9a80927b7a081 by Afzal Patel : replace bool flags with rocm_path_type  f672353d47c990b0222801fc53ee589579e559c5 by Afzal Patel : set hermetic in tensorflow.bazelrc Merging this change closes CC(Missing Documentation: Multiple links return 404 in Python API Guide) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23307 from ROCm:multiplerocmpaths dc60ee34af499fc8c54913c297fba906086d7467",2025-03-06T08:49:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88718
tpu,priyanshujiiii,Graph Execution Error," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0dev20240917  Custom code Yes  OS platform and distribution Rocky Linux  Mobile device Rocky Linux  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 9.3.0.  GPU model and memory NIVIDIA SMI 560.28.03  Current behavior? The code is showing a Graph execution error while training a model  Standalone code to reproduce the issue ```shell import os import numpy as np import tensorflow as tf from sklearn.model_selection import train_test_split def load_and_preprocess_data(base_path, batch_size=2, val_split=0.2):     noise_files = []     fullstack_files = []      Collect file paths     for subdir, _, files in os.walk(base_path):         for file in files:             if file.endswith("".npy""):                 full_path = os.path.join(subdir, file)                 if ""noise"" in file:                     noise_files.append(full_path)                 elif ""fullstack"" in file:                     fullstack_files.append(full_path)      Ensure files are in correct order     noise_files.sort()     fullstack_files.sort()      Split data into train and validation sets     noise_train, noise_val, fullstack_train, fullstack_val = train_test_split(         noise_files, fullstack_files, test_size=val_split, random_state=42     )     def create_generator(noise_list, fullstack_list):         def generator():             for noise_path, fullstack_path in zip(noise_list, fullstack_list):                  Load files in memorymapped mode                 noise_data = np.load(noise_path, mmap_mode='r', allow_pickle=True)                 fullstack_data = np.load(fullstack_path, mmap_mode='r', allow_pickle=True)                 for i in range(300):                     x_sample = noise_data[:, :, i]   Only loads this slice into RAM                     y_sample = fullstack_data[:, :, i]                     yield np.expand_dims(x_sample, axis=1), np.expand_dims(y_sample, axis=1)         return generator      Create TensorFlow datasets     train_dataset = tf.data.Dataset.from_generator(         create_generator(noise_train, fullstack_train),         output_signature=(             tf.TensorSpec(shape=(1259, 300, 1), dtype=tf.float32),             tf.TensorSpec(shape=(1259, 300, 1), dtype=tf.float32)         )     ).batch(batch_size).prefetch(tf.data.AUTOTUNE)     val_dataset = tf.data.Dataset.from_generator(         create_generator(noise_val, fullstack_val),         output_signature=(             tf.TensorSpec(shape=(1259, 300, 1), dtype=tf.float32),             tf.TensorSpec(shape=(1259, 300, 1), dtype=tf.float32)         )     ).batch(batch_size).prefetch(tf.data.AUTOTUNE)     return train_dataset, val_dataset  Example Usage base_path = ""/home/simlab120/Denoise_comp/Pragyant/imageimpeccabletraindatapart1"" train_dataset, val_dataset = load_and_preprocess_data(base_path)  Display a sample batch from the training set for x_batch, y_batch in train_dataset.take(1):     import matplotlib.pyplot as plt     plt.subplot(1, 2, 1)     plt.imshow(x_batch[0, :, :, 0], cmap='gray')   Removing channel dimension for visualization     plt.title(""Noisy Image"")     plt.subplot(1, 2, 2)     plt.imshow(y_batch[0, :, :, 0], cmap='gray')     plt.title(""Fullstack Image"")     plt.show()     break Output:  20250306 11:47:58.814180: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250306 11:47:58.827279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741241878.841060  263395 cuda_dnn.cc:8321] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741241878.844820  263395 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250306 11:47:58.859322: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. I0000 00:00:1741241880.476719  263395 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5694 MB memory:  > device: 0, name: NVIDIA T1000 8GB, pci bus id: 0000:52:00.0, compute capability: 7.5 Input:  import tensorflow as tf from tensorflow.keras.layers import Conv2D, Conv2DTranspose, ZeroPadding2D, Cropping2D, BatchNormalization, MaxPooling2D, UpSampling2D from tensorflow.keras import models def build_encdec(input_shape=(1259, 300, 1)):   Ensure it's divisible by 2^6     model = models.Sequential(name=""encoder_decoder"")      Encoder     model.add(ZeroPadding2D(padding=((21, 0), (10, 10)), input_shape=input_shape))   Add extra padding     model.add(Conv2D(16, (3, 3), activation='swish', padding=""same"", strides=1))      model.add(BatchNormalization())     model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))     model.add(UpSampling2D((2, 2)))     model.add(Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same', strides=1))   Change activation to sigmoid     model.add(BatchNormalization())      Remove padding added in encoder     model.add(Cropping2D(cropping=((21, 0), (10, 10))))       return model     def ssim_metric(y_true, y_pred):      Ensure y_true has a channel dimension if missing.     y_true = y_true if len(y_true.shape) == 4 else tf.expand_dims(y_true, 1)     return tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))  Create the autoencoder model instance autoencoder = build_encdec()  Compile the model autoencoder.compile(optimizer=""adam"", loss=""mse"", metrics=[ssim_metric])  Build the model by specifying an input shape (optional) autoencoder.build((None, 1259, 300, 1)) autoencoder.summary() output: Model: ""encoder_decoder"" ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ ┃ Layer (type)                    ┃ Output Shape           ┃       Param  ┃ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ │ zero_padding2d_1                │ (None, 1280, 320, 1)   │             0 │ │ (ZeroPadding2D)                 │                        │               │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ conv2d_6 (Conv2D)               │ (None, 1280, 320, 16)  │           160 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ batch_normalization_12          │ (None, 1280, 320, 16)  │            64 │ │ (BatchNormalization)            │                        │               │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ max_pooling2d_6 (MaxPooling2D)  │ (None, 640, 160, 16)   │             0 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ up_sampling2d_6 (UpSampling2D)  │ (None, 1280, 320, 16)  │             0 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ conv2d_transpose_6              │ (None, 1280, 320, 1)   │           145 │ │ (Conv2DTranspose)               │                        │               │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ batch_normalization_13          │ (None, 1280, 320, 1)   │             4 │ │ (BatchNormalization)            │                        │               │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ cropping2d_1 (Cropping2D)       │ (None, 1259, 300, 1)   │             0 │ └─────────────────────────────────┴────────────────────────┴───────────────┘  Total params: 373 (1.46 KB)  Trainable params: 339 (1.32 KB)  Nontrainable params: 34 (136.00 B)  import os os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""  history = autoencoder.fit(     train_dataset,   Use the dataset directly     validation_data=val_dataset,   Use validation dataset     epochs=1000,     verbose=1, ) ```  Relevant log output ```shell W0000 00:00:1741242567.222588  263744 assert_op.cc:38] Ignoring Assert operator SSIM/Assert/Assert W0000 00:00:1741242567.222702  263744 assert_op.cc:38] Ignoring Assert operator SSIM/Assert_1/Assert W0000 00:00:1741242567.222812  263744 assert_op.cc:38] Ignoring Assert operator SSIM/Assert_2/Assert W0000 00:00:1741242567.222874  263744 assert_op.cc:38] Ignoring Assert operator SSIM/Assert_3/Assert E0000 00:00:1741242567.262839  263744 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. E0000 00:00:1741242567.282689  263744 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. 20250306 11:59:27.290025: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at xla_ops.cc:577 : FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.  FailedPreconditionError                   Traceback (most recent call last) Cell In[8], line 3       1 import os       2 os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""  > 3 history = autoencoder.fit(       4     train_dataset,   Use the dataset directly       5     validation_data=val_dataset,   Use validation dataset       6     epochs=1000,       7     verbose=1,       8 ) File ~/anaconda3/envs/tf/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py:122, in filter_traceback..error_handler(*args, **kwargs)     119     filtered_tb = _process_traceback_frames(e.__traceback__)     120      To get the full stack trace, call:     121      `keras.config.disable_traceback_filtering()` > 122     raise e.with_traceback(filtered_tb) from None     123 finally:     124     del filtered_tb File ~/anaconda3/envs/tf/lib/python3.12/sitepackages/tensorflow/python/eager/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)      51 try:      52   ctx.ensure_initialized() > 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,      54                                       inputs, attrs, num_outputs)      55 except core._NotOkStatusException as e:      56   if name is not None: FailedPreconditionError: Graph execution error: Detected at node StatefulPartitionedCall defined at (most recent call last):   File """", line 198, in _run_module_as_main   File """", line 88, in _run_code   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel_launcher.py"", line 18, in    File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/traitlets/config/application.py"", line 1075, in launch_instance   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/kernelapp.py"", line 739, in start   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/tornado/platform/asyncio.py"", line 205, in start   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/asyncio/base_events.py"", line 641, in run_forever   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/asyncio/base_events.py"", line 1986, in _run_once   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/asyncio/events.py"", line 88, in _run   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/kernelbase.py"", line 545, in dispatch_queue   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/kernelbase.py"", line 534, in process_one   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/kernelbase.py"", line 437, in dispatch_shell   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/ipkernel.py"", line 362, in execute_request   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/kernelbase.py"", line 778, in execute_request   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/ipkernel.py"", line 449, in do_execute   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/zmqshell.py"", line 549, in run_cell   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"", line 3075, in run_cell   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"", line 3130, in _run_cell   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/IPython/core/async_helpers.py"", line 128, in _pseudo_sync_runner   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"", line 3334, in run_cell_async   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"", line 3517, in run_ast_nodes   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"", line 3577, in run_code   File ""/tmp/ipykernel_263395/279222490.py"", line 3, in    File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 320, in fit   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 121, in one_step_on_iterator DNN library initialization failed. Look at the errors above for more details. 	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_iterator_14559] ```",2025-03-06T06:42:15Z,stat:awaiting response stale type:performance TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88708,"Hi **** , Apologies for the delay, and thanks for raising your concern here. Could you please provide a Colab gist for troubleshooting this issue more accurately? Alternatively, you can share the specific code where you are facing the issue. I attempted to replicate the provided code but encountered a different issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],Fix dependency on absl/strings:str_format for tool_options.h,Fix dependency on absl/strings:str_format for tool_options.h,2025-03-06T02:51:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88706
tpu,copybara-service[bot],Test tpu job,Test tpu job,2025-03-05T23:28:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88689
tpu,Corey4005,Tensorflow does not recognize GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0  Custom code No  OS platform and distribution WSL Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.0  Bazel version _No response_  GCC/compiler version 13.3.0  CUDA/cuDNN version _No response_  GPU model and memory RTX A4000 16GB  Current behavior? First, I installed Anaconda32025.10.1Linux using wget and the shell script.   1.  Install Anaconda Commands: ``` wget https://repo.anaconda.com/archive/Anaconda32024.101Linuxx86_64.sh bash Anaconda32024.101Linuxx86_64.sh ``` I selected yes on the location in which it generally installs. Anaconda was installed in /home/user/anaconda3.  I also allowed anaconda to update my shell profile by selecting yes.  I rebooted the shell and went to step 2.   2. I made sure my drivers were up to date.  I ran the nvidiasmi command to check this and saw the latest driver for my NVIDIA RTX A4000 GPUs. The image below shows the output with the most up to date drivers.  !Image  3. I installed the latest cuda toolkit using the following commands.  ``` sudo aptkey del 7fa2af80 wget https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda_12.8.0_570.86.10_linux.run sudo sh cuda_12.8.0_570.86.10_linux.run ```  4. I set my path to the cuda toolkit and the lib64 in my .bashrc file I put the at the bottom of the file and rebooted the shell  ``` export PATH=/usr/local/cuda12.8/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda12.8/lib64:$PATH ```  5. I install cuDNN I downloaded the tar file for linux here. It does seem that there is a file for 24.04, but the tar file did work when I installed it.  ``` tar xvf cudnnlinuxx86_648.9.7.29_cuda12archive.tar.xz ``` This unraveled a lot of files on my machine. I followed the installation instructions here specifically, I did the following:  ``` sudo cp cudnn*archive/include/cudnn*.h /usr/local/cuda/include  sudo cp P cudnn*archive/lib/libcudnn* /usr/local/cuda/lib64  sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn* ``` When I run nvcc V, I get:  !Image  6 I install a virtual environment and activate it.  ``` conda create n tf python==3.12 conda activate tf python3 m pip install tensorflow[andcuda] ``` Then, when I check for GPU support, I get errors and my GPU is not recognized:  7 The errors are shown below:  ``` python3 c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"" ``` Errors:  ``` 20250305 15:55:46.327193: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250305 15:55:46.338383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741211746.350376   17653 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741211746.354146   17653 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250305 15:55:46.365820: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250305 15:55:48.179254: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: RESOURCE_EXHAUSTED: Failed call to cuInit: CUDA_ERROR_OUT_OF_MEMORY: out of memory ``` What am I doing wrong?   Standalone code to reproduce the issue ```shell I am getting a CUDA_ERROR_OUT_OF_MEMORY upon install and my GPU is not being listed. ```  Relevant log output ```shell ```",2025-03-05T21:57:51Z,type:bug comp:gpu TF 2.18,open,0,12,https://github.com/tensorflow/tensorflow/issues/88676,"The highest version of CUDA supported by your drivers is 12.3, whereas the version of CUDA you installed is 12.8 Try downgrading CUDA to <=12.3 and try again.  Look up here to find compatible versions of CUDA, cuDNN, gcc and Python for your installed Tensorflow version.","Hi ****, thanks for raising your concern here. Hi **AT**, thanks for your response. The latest CUDA version is not compatible with your machines. When installing TensorFlow with GPU support, the CUDA version is installed automatically by using the following command: ``` python3 m pip install tensorflow[andcuda] ``` There is no need to manually install the CUDA Toolkit. I am attaching the official documentation1, documentation2 for installing TensorFlow using pip and checking compatibility versions. Based on this documentation, I installed it, and it worked fine for me. Please find the details below for your reference. ``` (tf_env) maayaragpu1:~$ python3 Python 3.9.21 (main, Dec 11 2024, 16:24:11)  [GCC 11.2.0] :: Anaconda, Inc. on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf >>> print(tf.__version__) 2.18.0 >>> print(tf.config.list_physical_devices(""GPU"")) [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')] >>> exit() (tf_env) maayaragpu1:~$ nvidiasmi Thu Mar  6 05:55:43 2025        ++  ++ ``` Thank you!","> Hi **[](https://github.com/Corey4005)**, thanks for raising your concern here. Hi **AT**, thanks for your response. The latest CUDA version is not compatible with your machines. When installing TensorFlow with GPU support, the CUDA version is installed automatically by using the following command: >  > ``` > python3 m pip install tensorflow[andcuda] > ``` >  > There is no need to manually install the CUDA Toolkit. I am attaching the official documentation1, documentation2 for installing TensorFlow using pip and checking compatibility versions. Based on this documentation, I installed it, and it worked fine for me. Please find the details below for your reference. >  > ``` > (tf_env) maayaragpu1:~$ python3 > Python 3.9.21 (main, Dec 11 2024, 16:24:11)  > [GCC 11.2.0] :: Anaconda, Inc. on linux > Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. > >>> import tensorflow as tf > >>> print(tf.__version__) > 2.18.0 > >>> print(tf.config.list_physical_devices(""GPU"")) > [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')] > >>> exit() > (tf_env) maayaragpu1:~$ nvidiasmi > Thu Mar  6 05:55:43 2025        > ++ >  > ++ > ``` >  > Thank you! Hi, . Thank you for the reply. I am going to provide exact steps I am following after seeing your recommendation that I do not install cuda manually and provide outputs for your review.  First, I wiped WSL and reinstalled to get a fresh vm.   1. In powershell ```  wsl unregister Ubuntu  wsl install ``` I now have a new install.   2. Opening WSL and looking at my version  ``` lsb_release a  ``` The following image shows the output.  !Image  3. I pull the latest conda and install ``` wget https://repo.anaconda.com/archive/Anaconda32024.101Linuxx86_64.sh bash Anaconda32024.101Linuxx86_64.sh ``` The installation location is set by default to `/home/user/anaconda3`.  I also allow conda to initialize by selecting `yes` when prompted. I then restart wsl and see that conda environments are now available by seeing `(base)` in my shell.   4. I create a conda environment with python 3.12 as suggested for tensorflow 2.18.0 in the documentation you provided and activate.  ``` conda create n tf python==3.12 conda activate tf ```  5. I install tensorflow and cuda using:  ``` python3 m pip install tensorflow[andcuda] ``` python version shows:  ``` python3 version ``` !Image pip version shows:  ``` pip version ``` !Image  7 I run the following command and get: ``` python3 c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"" ``` !Image nvidiasmi shows:  !Image Am I installing the wrong versions? ","> The highest version of CUDA supported by your drivers is 12.3, whereas the version of CUDA you installed is 12.8 Try downgrading CUDA to <=12.3 and try again. Look up here to find compatible versions of CUDA, cuDNN, gcc and Python for your installed Tensorflow version. How would I downgrade? Is there a command I can provide pip to get 12.3 instead?","Hello   AT.  I am still unable to get Tensorflow to work. I am going to walk through some new developments, however in an effort to work towards a transparent solution that others can follow. To just start over again with a fresh install, I just unregistered WSL as I had done in prior steps and reinstalled ubuntu.  !Image First, I have double checked my driver. I looked online and found that the RTX A4000 has driver 572.60 available. I installed this driver such that nvidiasmi now lists CUDA 12.8, shown below.  !Image Could ,  or AT please verify I have installed the correct driver for my GPUs?  Then, I updated and upgraded using the following commands:  ``` sudo apt update sudo apt upgrade ``` Afterwards, I installed anaconda as prior, which installed anaconda in /home/user/anaconda3. I also let it initialize conda by changing my bashrc file shown below: ``` wget https://repo.anaconda.com/archive/Anaconda32024.101Linuxx86_64.sh bash Anaconda32024.101Linuxx86_64.sh ``` Does this `.bashrc` file look ok?  ```  ~/.bashrc: executed by bash(1) for nonlogin shells.  see /usr/share/doc/bash/examples/startupfiles (in the package bashdoc)  for examples  If not running interactively, don't do anything case $ in     *i*) ;;       *) return;; esac  don't put duplicate lines or lines starting with space in the history.  See bash(1) for more options HISTCONTROL=ignoreboth  append to the history file, don't overwrite it shopt s histappend  for setting history length see HISTSIZE and HISTFILESIZE in bash(1) HISTSIZE=1000 HISTFILESIZE=2000  check the window size after each command and, if necessary,  update the values of LINES and COLUMNS. shopt s checkwinsize  If set, the pattern ""**"" used in a pathname expansion context will  match all files and zero or more directories and subdirectories. shopt s globstar  make less more friendly for nontext input files, see lesspipe(1) [ x /usr/bin/lesspipe ] && eval ""$(SHELL=/bin/sh lesspipe)""  set variable identifying the chroot you work in (used in the prompt below) if [ z ""${debian_chroot:}"" ] && [ r /etc/debian_chroot ]; then     debian_chroot=$(cat /etc/debian_chroot) fi  set a fancy prompt (noncolor, unless we know we ""want"" color) case ""$TERM"" in     xtermcolor]\s*alert$//'\'')""'  Alias definitions.  You may want to put all your additions into a separate file like  ~/.bash_aliases, instead of adding them here directly.  See /usr/share/doc/bashdoc/examples in the bashdoc package. if [ f ~/.bash_aliases ]; then     . ~/.bash_aliases fi  enable programmable completion features (you don't need to enable  this, if it's already enabled in /etc/bash.bashrc and /etc/profile  sources /etc/bash.bashrc). if ! shopt oq posix; then   if [ f /usr/share/bashcompletion/bash_completion ]; then     . /usr/share/bashcompletion/bash_completion   elif [ f /etc/bash_completion ]; then     . /etc/bash_completion   fi fi  >>> conda initialize >>>  !! Contents within this block are managed by 'conda init' !! __conda_setup=""$('/home/user/anaconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)"" if [ $? eq 0 ]; then     eval ""$__conda_setup"" else     if [ f ""/home/user/anaconda3/etc/profile.d/conda.sh"" ]; then         . ""/home/user/anaconda3/etc/profile.d/conda.sh""     else         export PATH=""/home/user/anaconda3/bin:$PATH""     fi fi unset __conda_setup  >> import tensorflow as tf 20250310 15:55:25.474408: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250310 15:55:25.484407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741640125.496393    4606 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741640125.500028    4606 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250310 15:55:25.512041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. >>> tf.config.list_physical_devices('GPU') 20250310 15:55:37.500622: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: RESOURCE_EXHAUSTED: Failed call to cuInit: CUDA_ERROR_OUT_OF_MEMORY: out of memory [] ```","Also , I install python 3.9.21 and used pip install tensorflow[andcuda] and got the same result: ``` python Python 3.9.21 (main, Dec 11 2024, 16:24:11) [GCC 11.2.0] :: Anaconda, Inc. on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf 20250310 16:15:17.136808: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250310 16:15:17.146938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741641317.158879    5197 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741641317.162466    5197 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250310 16:15:17.174416: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. >>> print(tf.__version__) 2.18.0 >>> print(tf.config.list_physical_devices(""GPU"")) 20250310 16:15:49.531676: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: RESOURCE_EXHAUSTED: Failed call to cuInit: CUDA_ERROR_OUT_OF_MEMORY: out of memory [] >>> exit() nvidiasmi Mon Mar 10 16:16:00 2025 ++  ++ ``` so, I installed the same python and the same tensorflow versions as you and it seems I am still getting the same error.  There must be something wrong with my system that I am not understanding.  Exact packages I installed to replicate your terminal outputs:  ```  packages in environment at /home/user/anaconda3/envs/tf:   Name                    Version                   Build  Channel _libgcc_mutex             0.1                        main _openmp_mutex             5.1                       1_gnu abslpy                   2.1.0                    pypi_0    pypi astunparse                1.6.3                    pypi_0    pypi cacertificates           2025.2.25            h06a4308_0 certifi                   2025.1.31                pypi_0    pypi charsetnormalizer        3.4.1                    pypi_0    pypi flatbuffers               25.2.10                  pypi_0    pypi gast                      0.6.0                    pypi_0    pypi googlepasta              0.2.0                    pypi_0    pypi grpcio                    1.71.0                   pypi_0    pypi h5py                      3.13.0                   pypi_0    pypi idna                      3.10                     pypi_0    pypi importlibmetadata        8.6.1                    pypi_0    pypi keras                     3.9.0                    pypi_0    pypi ld_impl_linux64          2.40                 h12ee557_0 libclang                  18.1.1                   pypi_0    pypi libffi                    3.4.4                h6a678d5_1 libgccng                 11.2.0               h1234567_1 libgomp                   11.2.0               h1234567_1 libstdcxxng              11.2.0               h1234567_1 markdown                  3.7                      pypi_0    pypi markdownitpy            3.0.0                    pypi_0    pypi markupsafe                3.0.2                    pypi_0    pypi mdurl                     0.1.2                    pypi_0    pypi mldtypes                 0.4.1                    pypi_0    pypi namex                     0.0.8                    pypi_0    pypi ncurses                   6.4                  h6a678d5_0 numpy                     2.0.2                    pypi_0    pypi nvidiacublascu12        12.5.3.2                 pypi_0    pypi nvidiacudacupticu12    12.5.82                  pypi_0    pypi nvidiacudanvcccu12     12.5.82                  pypi_0    pypi nvidiacudanvrtccu12    12.5.82                  pypi_0    pypi nvidiacudaruntimecu12  12.5.82                  pypi_0    pypi nvidiacudnncu12         9.3.0.75                 pypi_0    pypi nvidiacufftcu12         11.2.3.61                pypi_0    pypi nvidiacurandcu12        10.3.6.82                pypi_0    pypi nvidiacusolvercu12      11.6.3.83                pypi_0    pypi nvidiacusparsecu12      12.5.1.3                 pypi_0    pypi nvidiancclcu12          2.21.5                   pypi_0    pypi nvidianvjitlinkcu12     12.5.82                  pypi_0    pypi openssl                   3.0.16               h5eee18b_0 opteinsum                3.4.0                    pypi_0    pypi optree                    0.14.1                   pypi_0    pypi packaging                 24.2                     pypi_0    pypi pip                       25.0             py39h06a4308_0 protobuf                  5.29.3                   pypi_0    pypi pygments                  2.19.1                   pypi_0    pypi python                    3.9.21               he870216_1 readline                  8.2                  h5eee18b_0 requests                  2.32.3                   pypi_0    pypi rich                      13.9.4                   pypi_0    pypi setuptools                75.8.0           py39h06a4308_0 six                       1.17.0                   pypi_0    pypi sqlite                    3.45.3               h5eee18b_0 tensorboard               2.18.0                   pypi_0    pypi tensorboarddataserver   0.7.2                    pypi_0    pypi tensorflow                2.18.0                   pypi_0    pypi tensorflowiogcsfilesystem 0.37.1                   pypi_0    pypi termcolor                 2.5.0                    pypi_0    pypi tk                        8.6.14               h39e8969_0 typingextensions         4.12.2                   pypi_0    pypi tzdata                    2025a                h04d1e81_0 urllib3                   2.3.0                    pypi_0    pypi werkzeug                  3.1.3                    pypi_0    pypi wheel                     0.45.1           py39h06a4308_0 wrapt                     1.17.2                   pypi_0    pypi xz                        5.6.4                h5eee18b_1 zipp                      3.21.0                   pypi_0    pypi zlib                      1.2.13               h5eee18b_1 ```", What is the output of `nvcc version`,"> [](https://github.com/Corey4005) What is the output of `nvcc version` Hi AT. Good morning to you! When I run `nvcc version`, I get: ``` $ nvcc version Command 'nvcc' not found, but can be installed with: sudo apt install nvidiacudatoolkit ``` Is it supposed to be installed when `pip install tensorflow[andcuda]` is ran?  If I am understanding  correctly, he stated:  ""There is no need to manually install the CUDA Toolkit. I am attaching the official documentation1, documentation2 for installing TensorFlow using pip and checking compatibility versions. Based on this documentation, I installed it, and it worked fine for me. Please find the details below for your reference."" Should I have to install it with `sudo apt install nvidiacudatoolkit`? After I run that command I get: !Image I rerun the commands as above after nvcc is installed:  ```  python Python 3.9.21 (main, Dec 11 2024, 16:24:11) [GCC 11.2.0] :: Anaconda, Inc. on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf 20250312 09:26:58.433368: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250312 09:26:58.442961: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741789618.454737    2772 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741789618.458211    2772 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250312 09:26:58.469874: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. >>> print(tf.__version__) 2.18.0 >>> print(tf.config.list_physical_devices(""GPU"")) \20250312 09:27:29.037109: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: RESOURCE_EXHAUSTED: Failed call to cuInit: CUDA_ERROR_OUT_OF_MEMORY: out of memory [] >>> ```"," Based on your output of `nvidiasmi`, the highest supported version of CUDA on your machine is 12.3, for which the corresponding TensorFlow version is 2.17.  I would suggest deleting the current CUDA toolkit  ``` sudo apt purge remove nvidiacudatoolkit sudo apt autoremove ``` Then installing CUDA 12.3 using the runfile option  ``` wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda_12.3.0_545.23.06_linux.run sudo sh cuda_12.3.0_545.23.06_linux.run silent toolkit ``` Copy over the include and lib files to /usr/local as mentioned in the postinstall screen `nvcc version` now should report 12.3 Then install tensorflow 2.17","> [](https://github.com/Corey4005) Based on your output of `nvidiasmi`, the highest supported version of CUDA on your machine is 12.3, for which the corresponding TensorFlow version is 2.17. I would suggest deleting the current CUDA toolkit >  > ``` > sudo apt purge remove nvidiacudatoolkit > sudo apt autoremove > ``` >  > Then installing CUDA 12.3 using the runfile option >  > ``` > wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda_12.3.0_545.23.06_linux.run > sudo sh cuda_12.3.0_545.23.06_linux.run silent toolkit > ``` >  > Copy over the include and lib files to /usr/local as mentioned in the postinstall screen `nvcc version` now should report 12.3 >  > Then install tensorflow 2.17 Hi, AT. Just to clarify... I need to install CUDA? It seems like it was suggested I do not need to install it per 's comments:  ""There is no need to manually install the CUDA Toolkit. I am attaching the official documentation1, documentation2 for installing TensorFlow using pip and checking compatibility versions. Based on this documentation, I installed it, and it worked fine for me. Please find the details below for your reference."" When I run nvidiasmi, I get 12.8 now after installing this driver Also see the output: !Image To follow the documentation, it seems like it is suggesting I need to install CUDA 12.5, and provided the image above, it looks like I can support up to 12.8. Therefore, Tensorflow 2.18 should run per the `nvidiasmi` output I am showing above, correct?  I ran the following commands to get 12.5:  ``` wget https://developer.download.nvidia.com/compute/cuda/12.5.0/local_installers/cuda_12.5.0_555.42.02_linux.run sudo sh cuda_12.5.0_555.42.02_linux.run ```  I should note that it does not give me any output to copy files to any directory. However, the ouptut suggest that I set my paths. I do that here:  ``` export PATH=/usr/local/cuda12.5/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda12.5/lib64:$PATH ``` Then, when I run `nvcc version` I get: !Image I am still getting the same error even after nvcc is installed.  ``` Python 3.9.21 (main, Dec 11 2024, 16:24:11) [GCC 11.2.0] :: Anaconda, Inc. on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf 20250314 12:52:00.348718: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250314 12:52:00.358413: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741974720.370189    1088 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741974720.373698    1088 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250314 12:52:00.387155: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. >>> tf.__version__ '2.18.0' >>> print(tf.config.list_physical_devices('GPU')) 20250314 12:52:25.012176: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: RESOURCE_EXHAUSTED: Failed call to cuInit: CUDA_ERROR_OUT_OF_MEMORY: out of memory [] ``` Can you help me understand what files I am supposed to be copying over? The only output I got from the cuda runfile was to set my paths. ","Hi,  AT. Have there been any updates here? Is there any information you need me to provide further? I am beginning to think I just wont be able to use tensorflow for my project. Thank you for your time.","Good morning  and AT. Just wanted to share that pytorch works when I swap from WSL2 to anaconda prompt. See the following issue where I show that installing pytorch in WSL2 causes a memory error similar to what I am seeing in this issue thread. However, when I install pytorch in anaconda prompt, it is able to recognize the GPU and run. However, I tried to install tensorflow using anaconda prompt using  ``` pip install tensorflow[andcuda] ``` and recieved a bunch of errors. I assume this is because it can only be installed on WSL2 in a windows environment. If this is the case, then I am not sure how I can solve the issue because there is only one method of installing tensorflow on windows. "
tpu,tensorflow-jenkins,"r2.19 cherry-pick: 3a79ec99874 ""Bump libtpu versions to pick correct versioned nightlies""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/3a79ec9987413e0824a3d243eeb538e51bf3132a,2025-03-05T20:26:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88658
tpu,VHursevich,Wrong list of labels in an introduction notebook," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? https://github.com/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb x_labels = ['no', 'yes', 'down', 'go', 'left', 'up', 'right', 'stop'] !Image Correct list of labels depends on system. For Ubuntu 22.04 correct labels:  x_labels = ['down', 'go', 'left', 'no', 'right', 'stop', 'up', 'yes']  !Image  Standalone code to reproduce the issue ```shell https://github.com/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb ```  Relevant log output ```shell ```",2025-03-05T19:20:25Z,type:bug TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/88643,"Is the problem that the label order changes for different operatiing systems? Or that you want the order of the labels to change with the operating system. If its the first option, you could explicitly by adding  `sorted(os.listdir(data_dir))`",Are you satisfied with the resolution of your issue? Yes No,"The issue stems from how labels are handled in the original code. I've fixed this issue by ensuring consistent label ordering throughout the pipeline: Python :        def main():            Get commands (classes)           commands = []           for item in data_dir.iterdir():               if item.is_dir() and item.name not in ['README.md', '.DS_Store']:                   commands.append(item.name)           commands = sorted(commands)  Sort for consistency        Load and preprocess dataset       train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(           directory=data_dir,           batch_size=64,           validation_split=0.2,           seed=0,           output_sequence_length=16000,           subset='both'       )        Get label names       label_names = np.array(train_ds.class_names)        Save label names properly       labels_list = label_names.tolist() if hasattr(label_names, 'tolist') else label_names       with open(""label_names.json"", ""w"") as f:           json.dump(labels_list, f) And for prediction, I've implemented proper preprocessing and consistent label handling:  Proper prediction code     def preprocess_audiobuffer(waveform):         """"""Preprocess audio buffer for model prediction""""""          Normalize from [32768, 32767] to [1, 1]         waveform = waveform / 32768         waveform = tf.convert_to_tensor(waveform, dtype=tf.float32)         spectrogram = get_spectrogram(waveform)          Add batch dimension         spectrogram = tf.expand_dims(spectrogram, 0)         return spectrogram  Test prediction with consistent label handling     prediction = trained_model(spec, training=False)     probabilities = tf.nn.softmax(prediction[0])     predicted_index = tf.argmax(probabilities).numpy()     predicted_word = label_names[predicted_index]"
tpu,copybara-service[bot],[xla:gpu] Expect fully-ranked output_tiles sizes in nested fusions.,[xla:gpu] Expect fullyranked output_tiles sizes in nested fusions. This makes it consistent with the spec (rank corresponds to the rank of the roots) and what we produce in NestGemmFusion pass. It drops the 'rid_x' from the output tile offset indexing map and replaces them with ordinary `pid_x`. I don't think the variable names provide significant value. This allows us to drop the `RootIndexing::num_reduction_dims`.,2025-03-05T08:57:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88623
quantization,copybara-service[bot],Fork quantization driver in tflite codebase,Fork quantization driver in tflite codebase This would allow tflite specific changes to QSV propagation logic.,2025-03-05T04:19:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88602
opt,copybara-service[bot],PR #23355: [ROCm] Add rocm_base and rocm_gcc configs,"PR CC(Failed to load): [ROCm] Add rocm_base and rocm_gcc configs Imported from GitHub PR https://github.com/openxla/xla/pull/23355 Changed `rocm` config to `rocm_base`, and added `rocm_gcc`  so we can have gcc specific options. Since a lot of scripts still depend on `config=rocm` I have temporarily left it in .bazelrc. It will be removed once I transition everything to use `rocm_gcc` Copybara import of the project:  eaf35d55cb1b7700b4607678a9e31913f5a226b6 by Milica Makevic : Change rocm config to rocm_base and add rocm_gcc Merging this change closes CC(Failed to load) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23355 from ROCm:addrocmbase eaf35d55cb1b7700b4607678a9e31913f5a226b6",2025-03-05T00:49:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88596
opt,copybara-service[bot],PR #23060: Refacting xla/tests/client_library_test_base to remove duplicated codes.,PR CC(Attributes values not inferred by TFE C API (eager mode)): Refacting xla/tests/client_library_test_base to remove duplicated codes. Imported from GitHub PR https://github.com/openxla/xla/pull/23060 Copybara import of the project:  217e312c7f5bd5834f0b4a2d4206aa735b72476a by Shawn Wang : code refacotring  c62b8f587da2886287376246086c9bf550fe7fcb by Shawn Wang : fix  b98044083e6dd6f2a37152e9638b93ac7cee6a06 by Shawn Wang : fix  a291d6e332ab1cc3fd11ff5ff19d414ac9973896 by Shawn Wang : fix: Merging this change closes CC(Attributes values not inferred by TFE C API (eager mode)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23060 from shawnwang18:shawnw/options_iterator a291d6e332ab1cc3fd11ff5ff19d414ac9973896,2025-03-05T00:46:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88593
opt,copybara-service[bot],PR #88221: Qualcomm AI Engine Direct - Op Builders for 1P Models,"PR CC(Qualcomm AI Engine Direct  Op Builders for 1P Models): Qualcomm AI Engine Direct  Op Builders for 1P Models Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/88221  WHAT 1. Conv2d 2. DepthwiseConv2d 3. AveragePool 4. MaxPool 5. DepthToSpace 6. SpaceToDepth 7. HardSwish 8. LeakyRelu 9. ResizeBilinear 10. Litert options for these op builders, unit test 11. update qnn_compiler_plugin_test with these op builders  TEST  qnn_compiler_plugin_test ``` [] Global test environment teardown [==========] 115 tests from 5 test suites ran. (3489 ms total) [  PASSED  ] 115 tests. ```  litert_options_test ``` [] Global test environment teardown [==========] 22 tests from 1 test suite ran. (0 ms total) [  PASSED  ] 22 tests ``` Copybara import of the project:  542a108226dcb1c31abc30578cba2b74b20e8e0b by weilhuanquic : Qualcomm AI Engine Direct  Op Builders for 1P Models 1. Conv2d 2. DepthwiseConv2d 3. AveragePool 4. MaxPool 5. DepthToSpace 6. SpaceToDepth 7. HardSwish 8. LeakyRelu 9. ResizeBilinear 10. Litert options for these op builders, unit test 11. update qnn_compiler_plugin_test with these op builders Merging this change closes CC(Qualcomm AI Engine Direct  Op Builders for 1P Models) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88221 from jiunkaiy:dev/weilhuan/more_op_builders 542a108226dcb1c31abc30578cba2b74b20e8e0b",2025-03-05T00:33:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88592
quantization,copybara-service[bot],Initialize TfLiteQuantization before parsing,Initialize TfLiteQuantization before parsing If the tensor has no quantization this makes sure that the params field is initialized to null.,2025-03-04T23:53:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88590
tpu,copybara-service[bot],Updated the LiteRT Google implementation to dlopen libedgetpu_litert.so when,Updated the LiteRT Google implementation to dlopen libedgetpu_litert.so when possible.,2025-03-04T23:31:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88587
bfloat16,copybara-service[bot],PR #74506: Adds missing datatype support for various tflite operations,"PR CC(Adds missing datatype support for various tflite operations): Adds missing datatype support for various tflite operations Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/74506  Adds bf16, f16 support for tfl.atan2  Adds bf16,f16,int8,int16 for tfl.neg  Adds bf16, f16 support for tfl.min, tfl.max  Adds bf16, f16 support for tfl.slice  Adds bf16, f16 support for tfl.round  Adds bf16, f16 support for tfl.reverse  Adds bf16, f16 support for tfl.pad  Adds bf16, f16 support for tfl.tanh and tfl.logistic  Adds bf16, f16 support for tfl.floor Copybara import of the project:  775ae2b6aec166a107f41070cad1ed9bea25b696 by swatheeshmcw : TfLite neg_op add type support ( CC(1st Class Windows Support)) * TfLite neg_op add type support  Adds type support for bfloat16  Adds type supoort for float16  Adds type support for Int8  Adds type support for Int16 Coauthoredby: prathammcw   e108d4654398e7eb6eea17d38362001e0f36e054 by swatheeshmcw : Tflite tanh and logistic adds missing datatype support ( CC(cuDNN v2 (6.5) not available anymore)) * Tflite tanh and logistic adds missing datatype support Adds f16,bf16 for tanh and logistic Adds f16,bf16 for tanh and logistic unit test Coauthoredby: nitheshsrikanthmcw   18581a6cb30cfb0b66b01b5a4925cb7fb84a65cf by swatheeshmcw : Tflite floor adds missing datatype support ( CC(No module named copy_reg  Installation Issue)) * Tflite floor missing datatype support Adds f16,bf16 for floor Adds f16,bf16 for floor unit test Coauthoredby: nitheshsrikanthmcw   e0aaea195d9a0927cbaddff9cc9e5953bc4d5b72 by swatheeshmcw : TfLite Slice missing datatype support ( CC(Error while installing tensorflow using pip on Ubuntu 14.04 32bit system)) ( CC(Cannot run the android example on Android 5.1.1)) * TfLite Slice missing datatype support Adds bf16, f16 support for slice Adds bf16, f16 slice unit tests  458c6a4063c9b7c345d5cf68240e1f0a8f3ba277 by swatheeshmcw : TfLite Round missing datatype support ( CC(Connectionist Temporal Classification example)) ( CC(No plan for official doc of any other languages than English?)) * TfLite Round missing datatype support Adds bf16, f16 support for round Adds bf16, f16 round unit tests  5c6def1d96f7f60dac83501870670ba63308b983 by swatheeshmcw : Tflite min max missing datatypes support ( CC(Updated links in documentation.)) ( CC(Target //tensorflow/tools/pip_package:build_pip_package failed to build on OSX)) Adds bf16,f16 for tflite min max operations Adds bf16,f16 min max unit tests  baea4f1b7c53d1bb4f31ccdac5fcb1e75143c8b9 by swatheeshmcw : TfLite Reverse missing datatype support ( CC(Slack Channel)) ( CC(Can't install from source if I don't have a GPU? )) * TfLite Reverse missing datatype support  Adds bf16, f16 support for reverse  Adds bf16, f16 reverse unit tests  96ccf49a5534f3462f29572a492ec3ecb360d7a0 by Swatheesh Muralidharan : Addressed redefinition of TFLITE_TENSOR_TYPE_ASSOC error in test_util.h file  2ff70a8c0a6be4eabb5f6c49faabd98299afe918 by RahulSudarMCW : Include nonquantized int8 & int16 type support for tfl.pad Merging this change closes CC(Adds missing datatype support for various tflite operations) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/74506 from MCWDev:master 2ff70a8c0a6be4eabb5f6c49faabd98299afe918",2025-03-04T22:15:11Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/88580,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
opt,copybara-service[bot],[XLA] Optimize computation of computation postorders by directly representing the HloComputation dependency graph.,"[XLA] Optimize computation of computation postorders by directly representing the HloComputation dependency graph. Previously to compute a postorder of computations we had to look at every instruction to find HloComputations that called subcomputations. This is an expensive thing to do. But if we instead simply maintain the computation graph explicitly then computing a postorder of computations becomes a cheap things to do. The previous code also had another efficiency bug. It used `MakeEmbeddedComputationsList` to determine the child computations of a computation for the purposes of a depthfirst search. But that function computes the *transitive* callees, not just the direct children, so it did redundant work.",2025-03-04T20:33:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88573
opt,copybara-service[bot],PR #23351: [ROCm] Fix build break in external udp due to stringop-truncation,PR CC(fake_quant_min_max_args/vars works unexpected): [ROCm] Fix build break in external udp due to stringoptruncation Imported from GitHub PR https://github.com/openxla/xla/pull/23351 Copybara import of the project:  d039829ba6a45807a13a2230cfb35e17590cd497 by Harsha HS : [ROCm] Fix build break in external udp due to stringoptruncation Merging this change closes CC(fake_quant_min_max_args/vars works unexpected) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23351 from ROCm:ci_fix_stringop_trunc_20250304 d039829ba6a45807a13a2230cfb35e17590cd497,2025-03-04T19:23:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88568
opt,copybara-service[bot],PR #23355: [ROCm] Add rocm_base and rocm_gcc configs,"PR CC(Failed to load): [ROCm] Add rocm_base and rocm_gcc configs Imported from GitHub PR https://github.com/openxla/xla/pull/23355 Changed `rocm` config to `rocm_base`, and added `rocm_gcc`  so we can have gcc specific options. Since a lot of scripts still depend on `config=rocm` I have temporarily left it in .bazelrc. It will be removed once I transition everything to use `rocm_gcc` Copybara import of the project:  eaf35d55cb1b7700b4607678a9e31913f5a226b6 by Milica Makevic : Change rocm config to rocm_base and add rocm_gcc Merging this change closes CC(Failed to load) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23355 from ROCm:addrocmbase eaf35d55cb1b7700b4607678a9e31913f5a226b6",2025-03-04T17:49:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88563
tpu,Liu-Jitai,[RNN] tensorflow/lite/kernels/transpose.cc:62 op_context->perm->dims->data[0] != dims (3 != 2)Node number 3 (TRANSPOSE) failed to prepare," 1. System information  Window 10  PyCharm 2024.2.3  TensorFlow 2.13  Python 3.8  2. Code Keras model convert to TFLite model fail, please help find the debug method.  TensorFlow Model Colab (Train a TensorFlow Keras LSTM Model for SIN funtion regression using random generated dataset) ([TensorFlow Model Colab]: (https://colab.research.google.com/gist/LiuJitai/08faad02c37315eb09576e85f6df44eb/tensorflowdatasets.ipynb)  Keras model convert to TFLite model Code [Keras model convert to TFLite model]  Fail at tflite_model_quant = converter.convert()  Import libraries import logging import tensorflow as tf from tensorflow import keras import numpy as np from tensorflow.keras.models import load_model from sklearn.preprocessing import MinMaxScaler  Helper functions def generate_data(seq_length, sequences_num):     x = []     y = []     for _ in range(sequences_num):         start = np.random.rand() * 2 * np.pi         sequence = np.sin(np.linspace(start, start + 3 * np.pi, seq_length + 1))         x.append(sequence[:1])         y.append(sequence[1:])   shift sequence by one to predict next value     return np.array(x), np.array(y) def representative_data_gen():     for k in model_input:         yield [k]  Generate dataset seq_length = 50 num_sequences = 100 model_input, model_output = generate_data(seq_length, num_sequences)  Normalization scaler = MinMaxScaler(feature_range=(0, 1)) model_input = scaler.fit_transform(model_input) model_output = scaler.transform(model_output)    Reshape the data to fit the LSTM input. The LSTM model expects input of shape (batch_size, time_steps, features), which here corresponds to (num_sequences, seq_length, 1). model_input = model_input.reshape((model_input.shape[0], model_input.shape[1], 1)) model_input = np.array(model_input, dtype=np.float32)  Load Keras model model = load_model('LSTM_Sin_model.keras')  coverter converter = tf.lite.TFLiteConverter.from_keras_model(model)  set the ops configuration converter.optimizations = [tf.lite.Optimize.DEFAULT]  converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS_INT8,       tf.lite.OpsSet.SELECT_TF_OPS       ] converter.inference_input_type = tf.int8  converter.inference_output_type = tf.int8  converter.representative_dataset = representative_data_gen tflite_model_quant = converter.convert() ** **fail at this step: tensorflow/lite/kernels/transpose.cc:62 op_context>perm>dims>data[0] != dims (3 != 2)Node number 3 (TRANSPOSE) failed to prepare****  save .tflite model tflite_model_path = 'LSTM_sin_model.tflite' with open(tflite_model_path, 'wb') as f:     f.write(tflite_model_quant) print(f""Model successfully converted to TFLite format and saved to {tflite_model_path}"")",2025-03-04T12:10:24Z,comp:lite TFLiteConverter TF 2.13,closed,0,3,https://github.com/tensorflow/tensorflow/issues/88549,"Hi, Jitai  Thank you for bringing this issue to our attention, I have been able to replicate the same behavior from my end with your provided code snippet but I think there was issue with directly yields samples without a batch dimension: `yield [k]` This produces data with shape (50, 1) causing a dimension mismatch. The model expects 3D inputs but receives 2D data leading to the TRANSPOSE node error so I've modified your provided code for missing batch dimension in the representative dataset and it seems like working as expected please refer this gistfile. Please give it try from your end and see is it working as expected or not ? If I have missed something here please let me know. Thank you for your cooperation and understanding. ``` import tensorflow as tf import numpy as np from tensorflow.keras.models import load_model from sklearn.preprocessing import MinMaxScaler def generate_data(seq_length, sequences_num):     x, y = [], []     for _ in range(sequences_num):         start = np.random.rand() * 2 * np.pi         sequence = np.sin(np.linspace(start, start + 3 * np.pi, seq_length + 1))         x.append(sequence[:1])         y.append(sequence[1:])     return np.array(x), np.array(y) def representative_data_gen():     for input_value in model_input:         yield [np.array([input_value], dtype=np.float32)]   seq_length = 50 num_sequences = 100 model_input, model_output = generate_data(seq_length, num_sequences) scaler = MinMaxScaler(feature_range=(0, 1)) model_input = scaler.fit_transform(model_input) model_output = scaler.transform(model_output) model_input = model_input.reshape((1, seq_length, 1)).astype(np.float32) model = load_model('/content/LSTM_Sin_model.keras') converter = tf.lite.TFLiteConverter.from_keras_model(model) converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS_INT8,       tf.lite.OpsSet.SELECT_TF_OPS      ] converter.representative_dataset = representative_data_gen converter.inference_input_type = tf.int8 converter.inference_output_type = tf.int8 tflite_model_quant = converter.convert() tflite_model_path = '/content/LSTM_sin_model.tflite' with open(tflite_model_path, 'wb') as f:     f.write(tflite_model_quant) print(f""Model successfully converted to TFLite format and saved to {tflite_model_path}"") ```",您好，您所发的邮件我已经收到，我会尽快阅读，祝您工作愉快！,The error was solved. Thank you so much for your help! 
int8,chunhsue,Qualcomm AI Engine Direct - Wrapper tests & Refactor tensor wrapper & Fix rms norm, What 1. Refactor tensor wrapper      Safer tensor data getter and setter. 2. Add wrapper tests. 3. Fix rms norm builder      Change 0 beta tensor type to float32 for float32 input and uint8 for other input data types. See op support types here.   Tests `qnn_compiler_plugin_test` ``` [] Global test environment teardown [==========] 99 tests from 5 test suites ran. (3995 ms total) [  PASSED  ] 99 tests. ```,2025-03-04T10:17:17Z,comp:lite size:XL,closed,0,1,https://github.com/tensorflow/tensorflow/issues/88546,", quic,   Can you also help review? "
tpu,copybara-service[bot],PR #23271: [XLA] Improve GPU memory limit handling and shape size calculation,"PR CC(Improve shape function of tf.sparse_reduce_sum): [XLA] Improve GPU memory limit handling and shape size calculation Imported from GitHub PR https://github.com/openxla/xla/pull/23271 When running MaxText Llama27b with FP16 optimizer state offloading, an out of memory failure occurred. The root cause was the int64_t memory limit being incorrectly calculated and interpreted as a large uint64_t memory limit close to UINT64_MAX, leading to overly aggressive buffer allocation by the LHS. This changelist addresses the out of memory failure by correctly handling the uint64_t memory limit and accurately calculating the device memory limit, excluding host memory space.  Change memory limit type from int64_t to uint64_t to prevent negative values and better represent memory sizes.  Add memory space filtering to exclude host memory when calculating input/output sizes that impact GPU memory usage. This ensures we only count buffers that actually reside in device memory.  Replace direct GetSizeOfShape() calls with ShapeSizeBytesFunction() wrapper, which provides:   * Consistent shape size calculation across the codebase   * Optional memory space filtering capability   * Proper handling of dynamic shapes and their metadata Copybara import of the project:  8b3184065c78f98f62e97e9a94e7d49e1797bd7b by Jane Liu : [XLA] Improve GPU memory limit handling and shape size calculation Merging this change closes CC(Improve shape function of tf.sparse_reduce_sum) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23271 from zhenyingliu:lhsoom 8b3184065c78f98f62e97e9a94e7d49e1797bd7b",2025-03-04T03:53:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88523
tpu,copybara-service[bot],Change a Q/DQ->Q to a Requant op,"Change a Q/DQ>Q to a Requant op When DQ>Q have different scales, they can be squashed into one quantize op with different quantized types for input and output.",2025-03-04T02:43:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88521
sharding,copybara-service[bot],Use the full contracting dimensions instead of the partitioned contracting dimensions in `GetDotGroupPartitionContractingOutputShardings`.,Use the full contracting dimensions instead of the partitioned contracting dimensions in `GetDotGroupPartitionContractingOutputShardings`.,2025-03-04T01:50:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88519
opt,copybara-service[bot],Use plugin's `GpuClientOptions` to replace `TfrtGpuClient::Options` when initialize TfrtGpuClient,Use plugin's `GpuClientOptions` to replace `TfrtGpuClient::Options` when initialize TfrtGpuClient,2025-03-04T01:25:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88518
tpu,copybara-service[bot],Open visibility for ComputePerTpuStepDataAcrossCores.,Open visibility for ComputePerTpuStepDataAcrossCores.,2025-03-03T23:48:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88514
sharding,copybara-service[bot],PR #23301: Uses absl::string_view in RegisterCustomCallPartitioner,PR CC(Fix Tdistribution sampling parameter): Uses absl::string_view in RegisterCustomCallPartitioner Imported from GitHub PR https://github.com/openxla/xla/pull/23301 Copybara import of the project:  3cf40c14097619973533fad244b054b6a4ea4b3a by Yunlong Liu : Uses absl::string_view  c8c1a80c537be6c7f33448c92fe00dbd7605043a by Yunlong Liu : Update custom_call_sharding_helper. : Update BUILD Merging this change closes CC(Fix Tdistribution sampling parameter) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23301 from yliu120:patch2 6d9c2be1ed28e55dce6afb71bbb9663a138a89c6,2025-03-03T23:46:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88513
tpu,copybara-service[bot],PR #23271: [XLA] Improve GPU memory limit handling and shape size calculation,"PR CC(Improve shape function of tf.sparse_reduce_sum): [XLA] Improve GPU memory limit handling and shape size calculation Imported from GitHub PR https://github.com/openxla/xla/pull/23271 When running MaxText Llama27b with FP16 optimizer state offloading, an out of memory failure occurred. The root cause was the int64_t memory limit being incorrectly calculated and interpreted as a large uint64_t memory limit close to UINT64_MAX, leading to overly aggressive buffer allocation by the LHS. This changelist addresses the out of memory failure by correctly handling the uint64_t memory limit and accurately calculating the device memory limit, excluding host memory space.  Change memory limit type from int64_t to uint64_t to prevent negative values and better represent memory sizes.  Add memory space filtering to exclude host memory when calculating input/output sizes that impact GPU memory usage. This ensures we only count buffers that actually reside in device memory.  Replace direct GetSizeOfShape() calls with ShapeSizeBytesFunction() wrapper, which provides:   * Consistent shape size calculation across the codebase   * Optional memory space filtering capability   * Proper handling of dynamic shapes and their metadata Copybara import of the project:  8b3184065c78f98f62e97e9a94e7d49e1797bd7b by Jane Liu : [XLA] Improve GPU memory limit handling and shape size calculation Merging this change closes CC(Improve shape function of tf.sparse_reduce_sum) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23271 from zhenyingliu:lhsoom 8b3184065c78f98f62e97e9a94e7d49e1797bd7b",2025-03-03T23:28:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88510
tpu,copybara-service[bot],[MPMD][NFC] Make MPMD device agonostic by replacing `tpu::TpuTopology` with `PjRtTopologyDescriptionProto` in the stack.,[MPMD][NFC] Make MPMD device agonostic by replacing `tpu::TpuTopology` with `PjRtTopologyDescriptionProto` in the stack.,2025-03-03T22:16:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88502
agent,copybara-service[bot],Added `WatchJobState` to coordination service agent.,Added `WatchJobState` to coordination service agent.,2025-03-03T22:09:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88500
opt,copybara-service[bot],PR #23314: [debug_options] Dump DebugOptions while compiling.,PR CC(Changing learning rate of the optimizer in eager mode): [debug_options] Dump DebugOptions while compiling. Imported from GitHub PR https://github.com/openxla/xla/pull/23314 This patch adds VLOG(2) for dumping debug options while compiling. This is a reliable way to retrieve the debug options based on the log. Copybara import of the project:  414e97757ea299d10f90dfad987d1898c9f30feb by Shraiysh Vaishay : [debug_options] Dump DebugOptions while compiling. This patch adds VLOG(2) for dumping debug options while compiling. This is a reliable way to retrieve the debug options based on the log. Merging this change closes CC(Changing learning rate of the optimizer in eager mode) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23314 from shraiysh:dump_debug_options 414e97757ea299d10f90dfad987d1898c9f30feb,2025-03-03T20:55:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88497
tpu,copybara-service[bot],Bump libtpu versions to pick correct versioned nightlies,Bump libtpu versions to pick correct versioned nightlies,2025-03-03T20:04:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88494
tpu,copybara-service[bot],Do not eliminate DQ-Q if the quant params are not the same.,Do not eliminate DQQ if the quant params are not the same. The pattern in the optimize pass was overly permissive. The removed pattern used to eliminate DQ>Q even when the input and output types were different as long as the defining op of DQ is not a Q.,2025-03-03T19:54:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88493
opt,copybara-service[bot],Move `--verbose_failures` option to the command that builds the first wheel.,Move `verbose_failures` option to the command that builds the first wheel.,2025-03-03T17:04:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88486
tpu,vwrewsge,`Floating point exception` in `tf.nn.max_pool`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In tfnightly (2.20.0dev20250302), a floating point exception occurs, but this issue does not occur in the current stable version.  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np input_tensor = tf.constant(np.random.rand(1, 1, 1, 3), dtype=tf.float32) output = tf.nn.max_pool(input_tensor,                         ksize=[1, 2, 2, 1],                         strides=[1, 1, 1, 1],                         padding='VALID')  Floating point exception ```  Relevant log output ```shell Floating point exception ```",2025-03-03T14:10:23Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88481,"Hi **** , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly version, but I did not encounter any issues. Please find the gist attached for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
sharding,copybara-service[bot],#sdy Add extra `export_test.py` tests for using different meshes..,"sdy Add extra `export_test.py` tests for using different meshes.. Under Shardy, we can:  use the same mesh on save and load  use one mesh on save and another mesh on load with different axis names  use one mesh on save and another mesh on load with different axis names and sizes. For this case Shardy propagation may not be optimal if the module doesn't specify out shardings. This is very hard to write a unit test for, and is rare to happen, and is something we have been considering adding in Shardy b/399957785. This will be something we can allow for during Shardy propagation. This can be a standalone fix in Shardy without making any changes to JAX or XLA.",2025-03-03T13:58:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88478
tpu,vwrewsge,`Aborted` error when using XLA with JIT compiled conv2d function on XLA_CPU device," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When running the following code with TensorFlow, an `Aborted` error occurs.  Standalone code to reproduce the issue ```shell import os os.environ[""XLA_FLAGS""] = ""xla_cpu_use_thunk_runtime=false tf_xla_enable_xla_devices"" import tensorflow as tf .function(jit_compile=True) def conv_fn(x, filt):     return tf.nn.conv2d(x, filt, strides=[1, 2, 2, 1], padding=""SAME"") input_tensor = tf.random.normal([1, 224, 224, 3]) filter_tensor = tf.random.normal([7, 7, 3, 64]) with tf.device(""/device:XLA_CPU:0""):     _ = conv_fn(input_tensor, filter_tensor) ```  Relevant log output ```shell Aborted ```",2025-03-03T11:46:38Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/88476,Are you satisfied with the resolution of your issue? Yes No
tpu,971766904,"build tensorflow 2.15 from source code successfully,but some support doesn't build"," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.15  Custom code Yes  OS platform and distribution linux  Mobile device _No response_  Python version 3.12  Bazel version 6.10  GCC/compiler version 9.1  CUDA/cuDNN version 12.2/8.1  GPU model and memory _No response_  Current behavior? the tensorflow 2.15 gpu is successfully built, but some support is not include like absl, eigen and unsupported. The libtensorflow_framework.so didn't build as well.  Standalone code to reproduce the issue ```shell main.cpp include  include ""tensorflow/cc/client/client_session.h"" ```  Relevant log output ```shell /usr/local/include/tensorflow/core/framework/tensor.h:25:10: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory    25 | include ""unsupported/Eigen/CXX11/Tensor""  // from  ```",2025-03-03T06:18:31Z,type:build/install subtype: ubuntu/linux TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88471,Try manually install the missing components (Eigen Headers) as well as build and install the libtensorflow_framework.so shared library.,"Try something like this...  ``` !/bin/bash if [ ! d ""tensorflow"" ]; then   echo ""Error: Please run this script from the TensorFlow source root directory""   exit 1 fi echo ""Installing build dependencies..."" sudo aptget update sudo aptget install y python3dev python3pip python3numpy echo ""Configuring TensorFlow build..."" python3 m pip install U pip numpy wheel python3 m pip install U keras_preprocessing ./configure echo ""Building TensorFlow with framework library and C++ API..."" bazel build config=opt config=cuda \     //tensorflow:libtensorflow_cc.so \     //tensorflow:libtensorflow_framework.so \     //tensorflow:install_headers echo ""Installing TensorFlow libraries and headers..."" sudo mkdir p /usr/local/include/tensorflow sudo mkdir p /usr/local/lib sudo cp r bazelbin/tensorflow/include/* /usr/local/include/ sudo mkdir p /usr/local/include/third_party sudo cp r bazeltensorflow/external/eigen_archive/Eigen /usr/local/include/ sudo cp r bazeltensorflow/external/eigen_archive/unsupported /usr/local/include/ sudo cp bazelbin/tensorflow/libtensorflow_cc.so /usr/local/lib/ sudo cp bazelbin/tensorflow/libtensorflow_framework.so /usr/local/lib/ sudo ldconfig echo ""Verifying installation..."" ls la /usr/local/include/unsupported/Eigen/CXX11/Tensor ls la /usr/local/lib/libtensorflow_framework.so echo ""Installation completed. You can now build C++ applications with TensorFlow."" echo ""Creating and building a test program..."" cat > test.cpp  include ""tensorflow/cc/client/client_session.h"" include ""tensorflow/cc/ops/standard_ops.h"" include ""tensorflow/core/framework/tensor.h"" int main() {     tensorflow::Scope scope = tensorflow::Scope::NewRootScope();     auto a = tensorflow::ops::Const(scope, 3.0f);     auto b = tensorflow::ops::Const(scope, 2.0f);     auto c = tensorflow::ops::Add(scope, a, b);     tensorflow::ClientSession session(scope);     std::vector outputs;     session.Run({c}, &outputs);     std::cout () << std::endl;     return 0; } EOF g++ std=c++14 test.cpp o test_tensorflow \     I/usr/local/include \     L/usr/local/lib \     ltensorflow_cc ltensorflow_framework echo ""If compilation succeeded, you can run the test with: ./test_tensorflow"" ```"," Thank you for your answer.  ``` bazel build config=opt config=cuda \     //tensorflow:libtensorflow_cc.so \     //tensorflow:libtensorflow_framework.so \     //tensorflow:install_headers ``` this works for me. ``` sudo cp r bazelbin/tensorflow/include/* /usr/local/include/ ```  copying the include folder will help. ``` sudo cp r bazeltensorflow/external/eigen_archive/Eigen /usr/local/include/ sudo cp r bazeltensorflow/external/eigen_archive/unsupported /usr/local/include/ ```  these are not necessory. Anyway, thanks a lot.",Are you satisfied with the resolution of your issue? Yes No
tpu,971766904,build tensorflow from source code, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf r2.10  Custom code Yes  OS platform and distribution linux  Mobile device _No response_  Python version 3.8  Bazel version 5.11  GCC/compiler version 9.1  CUDA/cuDNN version 11.8/8.1  GPU model and memory _No response_  Current behavior? FAILED: Build did NOT complete successfully I want to finish the building  Standalone code to reproduce the issue ```shell bazel build config=opt config=cuda //tensorflow:libtensorflow_cc.so ```  Relevant log output ```shell /mypool/zy/tf_compile/tf210file/tensorflowr2.10/tensorflow/cc/BUILD:780:22: Executing genrule //tensorflow/cc:resource_variable_ops_genrule failed: (Exit 127): bash failed: error executing command /bin/bash c ... (remaining 1 argument skipped) bazelout/k8opt/bin/tensorflow/cc/ops/resource_variable_ops_gen_cc: symbol lookup error: bazelout/k8opt/bin/tensorflow/cc/ops/resource_variable_ops_gen_cc: undefined symbol: _ZN10tensorflow12OpDefBuilder10SetShapeFnESt8functionIFNS_6StatusEPNS_15shape_inference16InferenceContextEEE Target //tensorflow:libtensorflow_cc.so failed to build Use verbose_failures to see the command lines of failed build steps. ```,2025-03-03T06:02:55Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88469,"Hi **** , Apologies for the delay, and thanks for raising your concern. It looks like a version compatibility issue might be causing the problem. I am attaching the official documentation for your reference, please review it to check the compatibility of different versions for better results. Additionally, I noticed that you are using an older version. I recommend updating to the latest version to avoid potential issues. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],[xla] Literal: optimize broadcasts,"[xla] Literal: optimize broadcasts ``` name                                                                       old cpu/op   new cpu/op   delta BM_BroadcastScalarToMatrix/16/16     [s64[] to s64[16,16]               ]  3.77µs ± 1%  0.60µs ± 4%  84.05%  (p=0.000 n=37+37) BM_BroadcastScalarToMatrix/16/1024   [s64[] to s64[16,1024]             ]   153µs ± 1%     3µs ± 2%  97.75%  (p=0.000 n=38+37) BM_BroadcastScalarToMatrix/1024/1024 [s64[] to s64[1024,1024]           ]  9.70ms ± 1%  0.18ms ± 1%  98.17%  (p=0.000 n=39+39) BM_BroadcastVectorToMatrix/16/16     [s64[16] to s64[16,16]             ]  4.31µs ± 1%  3.87µs ± 2%  10.14%  (p=0.000 n=35+39) BM_BroadcastVectorToMatrix/16/1024   [s64[16] to s64[16,1024]           ]   177µs ± 2%   150µs ± 2%  15.21%  (p=0.000 n=37+40) BM_BroadcastVectorToMatrix/1024/1024 [s64[1024] to s64[1024,1024]       ]  11.2ms ± 2%   9.5ms ± 1%  15.23%  (p=0.000 n=36+40) BM_BroadcastMatrixToTensor/16/16     [s64[16,16] to s64[4,16,16]        ]  17.0µs ± 1%  15.9µs ± 2%   6.10%  (p=0.000 n=37+37) BM_BroadcastMatrixToTensor/16/1024   [s64[16,1024] to s64[4,16,1024]    ]   943µs ± 2%   925µs ± 2%   1.83%  (p=0.000 n=38+39) BM_BroadcastMatrixToTensor/1024/1024 [s64[1024,1024] to s64[4,1024,1024]]  60.3ms ± 1%  59.4ms ± 2%   1.54%  (p=0.000 n=36+40) ```",2025-03-02T19:50:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88453
opt,copybara-service[bot],[xla] Literal: optimize scalar broadcasts,[xla] Literal: optimize scalar broadcasts FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23302 from yliu120:patch3 692eac6426cae82f2cec76eead8908e7b47b4101,2025-03-02T18:51:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88452
tpu,dicotom,Tensorflow with C++ Builder 12," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version libtensorflowcpuwindowsx86_64.zip  Custom code Yes  OS platform and distribution windows 10  Mobile device N/A  Python version N/A  Bazel version N/A  GCC/compiler version N/A  CUDA/cuDNN version _No response_  GPU model and memory N/A  Current behavior? this simple c file should compile: bcc64 test6.c D__NO_INLINE DWIN64 I ""N:\DOWNLOADS\FASTEST\26WORKS\TOTEST\include"" L ""N:\DOWNLOADS\FASTEST\26WORKS\TOTEST\lib"" l""tensorflow"" v ``` include  //pragma comment(lib, ""tensorflow.lib"")  //pragma link ""tensorflow.lib"" int main() {     // Your code using TensorFlow's C API functions     TF_Graph* graph = TF_NewGraph();     // More TensorFlow code...     return 0; } ``` but it does not: Embarcadero C++ 7.70 for Win64 Copyright (c) 20122024 Embarcadero Technologies, Inc. test6.c: Turbo Incremental Link64 6.99 Copyright (c) 19972024 Embarcadero Technologies, Inc. Error: Unresolved external 'TF_NewGraph' referenced from C:\USERS\USER\APPDATA\LOCAL\TEMP\TEST6AF147F.O if I add: pragma comment(lib, ""tensorflow.lib"")  I got invalid object file tensorflow.dll  Standalone code to reproduce the issue ```shell include  //pragma comment(lib, ""tensorflow.lib"") ; DOES NOT WORK //pragma link ""tensorflow.lib""          ; DOES NOT WORK int main() {     // Your code using TensorFlow's C API functions     TF_Graph* graph = TF_NewGraph();     // More TensorFlow code...     return 0; } ```  Relevant log output ```shell Embarcadero C++ 7.70 for Win64 Copyright (c) 20122024 Embarcadero Technologies, Inc. test6.c: Turbo Incremental Link64 6.99 Copyright (c) 19972024 Embarcadero Technologies, Inc. Error: Unresolved external 'TF_NewGraph' referenced from C:\USERS\USER\APPDATA\LOCAL\TEMP\TEST6AF147F.O ```",2025-03-02T16:05:15Z,stat:awaiting tensorflower type:bug subtype:windows comp:core,open,0,0,https://github.com/tensorflow/tensorflow/issues/88451
tpu,xtrizeShino,Failed to parse TfLiteSettingsJsonParser on TensorFlow Lite C++," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.1  Custom code No  OS platform and distribution Debian aarch64   Mobile device VIA VAB5000  Python version Only C++  Bazel version 6.5.0  GCC/compiler version gcc version 12.2.0 (Debian 12.2.014) / Debian clang version 14.0.6  CUDA/cuDNN version None  GPU model and memory MediaTek Genio 700 MDLA  Current behavior? I am trying to use Delegate on VIA VAB5000 (aarch64) by referring to the following site. However, an error occurs when loading the json file to be used for Delegate, and I cannot use Delegate. Do you have any good ideas? I apologize for bothering you during your busy schedule. Thank you in advance. https://mediatek.gitlab.io/genio/doc/tao/npu_acceleration.html Below is the json file I am trying to parse. ``` { 	""stable_delegate_loader_settings"": { 		""delegate_path"": ""/usr/lib/libneuron_stable_delegate.so"" 	}, 	""neuron_delegate_settings"": { 		""execution_priority"": NEURON_PRIORITY_HIGH, 		""optimization_hint"": NEURON_OPTIMIZATION_NONE, 		""execution_preference"": NEURON_FAST_SINGLE_ANSWER, 		""allow_fp16"": true, 		""use_ahwb"": true 	} } ```  Standalone code to reproduce the issue ```shell  move to /home/debian $ cd ~   get TensorFlow v2.16.1 $ wget https://github.com/tensorflow/tensorflow/archive/refs/tags/v2.16.1.zip $ unzip v2.16.1.zip $ mv tensorflow2.16.1 tensorflow   get my source codes $ git.com:xtrizeShino/peoplenet_onnx_to_tflite.git $ cd peoplenet_onnx_to_tflite $ cd cpp_infer_vab5000  remove old source  $ rm rf abseilcpp $ rm rf flatbuffers  get abseil $ wget https://github.com/abseil/abseilcpp/archi ve/refs/tags/20230802.3.zip $ unzip 20230802.3.zip $ mv abseilcpp20230802.3 abseilcpp  get flatbuffers and build  $ wget https://github.com/google/flatbuffers/archive/refs/tags/v23.5.26.zip $ unzip v23.5.26.zip $ mv flatbuffers23.5.26 flatbuffers  $ cd flatbuffers $ mkdir build $ cd build $ cmake .. $ make  $ cd ../../  build my source codes $ mkdir build $ cd build  $ cmake .. $ make  exec $ ./PeopleNetInfer ```  Relevant log output ```shell $ ./PeopleNetInfer  original width=596, height=336 model file name : /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/build/resource/resnet34_peoplenet_int8.tflite ERROR: Failed to parse the delegate settings file (/home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/build/resource/stable_delegate_settings.json). Error at /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/peoplenet_main.cpp:235 ```",2025-03-02T05:56:37Z,type:support comp:lite TF 2.16,closed,0,7,https://github.com/tensorflow/tensorflow/issues/88435,"The TensorFlow Lite ""*.so"" files were crosscompiled using the following steps. ``` target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libtensorflowlite.so) target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libdelegate_loader.so) target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libtflite_settings_json_parser.so) target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libandroid_info.so) ``` ``` $ bazel build config=elinux_aarch64 c opt //tensorflow/lite/c:libtensorflowlite_c.so $ bazel build config=elinux_aarch64 c opt //tensorflow/lite:libtensorflowlite.so $ bazel build config=elinux_aarch64 c opt //tensorflow/lite/experimental/acceleration/compatibility:android_info $ bazel build config=elinux_aarch64 c opt //tensorflow/lite/delegates/utils/experimental/stable_delegate:tflite_settings_json_parser $ bazel build config=elinux_aarch64 c opt //tensorflow/lite/delegates/utils/experimental/stable_delegate:delegate_loader ``` And I am building it with cmake using the following CMakeLists.txt: ``` cmake_minimum_required(VERSION 2.8) project(PeopleNetInfer)  Create Main project add_executable(PeopleNetInfer 	peoplenet_main.cpp )  For OpenCV find_package(OpenCV REQUIRED) if(OpenCV_FOUND) 	target_include_directories(PeopleNetInfer PUBLIC ${OpenCV_INCLUDE_DIRS}) 	target_link_libraries(PeopleNetInfer ${OpenCV_LIBS}) endif()  Avseil.io (absl) add_subdirectory(abseilcpp) set(protobuf_ABSL_USED_TARGETS absl::absl_check absl::absl_log absl::algorithm absl::base absl::bind_front absl::bits absl::btree absl::cleanup absl::cord absl::core_headers absl::debugging absl::die_if_null absl::dynamic_annotations absl::flags absl::flat_hash_map absl::flat_hash_set absl::function_ref absl::hash absl::layout absl::log_initialize absl::log_severity absl::memory absl::node_hash_map absl::node_hash_set absl::optional absl::span absl::status absl::statusor absl::strings absl::synchronization absl::time absl::type_traits absl::utility absl::variant ) target_link_libraries(PeopleNetInfer ${protobuf_ABSL_USED_TARGETS})  For Tensorflow Lite target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libtensorflowlite.so) target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libdelegate_loader.so) target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libtflite_settings_json_parser.so) target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libandroid_info.so) target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/flatbuffers/build/libflatbuffers.a) target_include_directories(PeopleNetInfer PUBLIC /home/debian/tensorflow) target_include_directories(PeopleNetInfer PUBLIC /home/debian/tensorflow/tensorflow) target_include_directories(PeopleNetInfer PUBLIC /home/debian/tensorflow/tensorflow/lite) target_include_directories(PeopleNetInfer PUBLIC /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/flatbuffers/include) set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS}  std=c++17 lstdc++"")  Copy resouce file(COPY ${CMAKE_SOURCE_DIR}/resource/ DESTINATION ${PROJECT_BINARY_DIR}/resource/) add_definitions(DRESOURCE_DIR=""${PROJECT_BINARY_DIR}/resource/"") ```","Hi,   I apologize for the delay in my response, I see after analyzing the error you're encountering with the `TfLiteSettingsJsonParser` maybe the values `NEURON_PRIORITY_HIGH, NEURON_OPTIMIZATION_NONE` and `NEURON_FAST_SINGLE_ANSWER` are not valid JSON as they're not enclosed in quotes. In proper JSON, string values must be enclosed in double quotes. If these are meant to be string constants: ``` { 	""stable_delegate_loader_settings"": { 		""delegate_path"": ""/usr/lib/libneuron_stable_delegate.so"" 	}, 	""neuron_delegate_settings"": { 		""execution_priority"": ""NEURON_PRIORITY_HIGH"", 		""optimization_hint"": ""NEURON_OPTIMIZATION_NONE"", 		""execution_preference"": ""NEURON_FAST_SINGLE_ANSWER"", 		""allow_fp16"": true, 		""use_ahwb"": true 	} } ``` JSON parsing errors can also occur due to file encoding problems. The parser might be failing due to hidden characters or incorrect encoding so create a new file with a text editor (like Notepad++ if available) save it with `UTF8` encoding (not UTF8BOM) and use this new file and see is it resolving your error or not ? Thank you for your cooperation and patience.","Thank you for your reply!! However, I tried the method you taught me, but unfortunately the result did not change. The output is as follows.  When I open the file in Notepad++, it shows as UTF8. ``` $ cat /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/build/resource/stable_delegate_settings.json  libneuron_stable_delegate.so setting {         ""stable_delegate_loader_settings"": {                 ""delegate_path"": ""/usr/lib/libneuron_stable_delegate.so""         },         ""neuron_delegate_settings"": {                 ""execution_priority"": ""NEURON_PRIORITY_HIGH"",                 ""optimization_hint"": ""NEURON_OPTIMIZATION_NONE"",                 ""execution_preference"": ""NEURON_FAST_SINGLE_ANSWER"",                 ""allow_fp16"": true,                 ""use_ahwb"": true         } } $ ./PeopleNetInfer original width=596, height=336 model file name : /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/build/resource/resnet34_peoplenet_int8.tflite ERROR: Failed to parse the delegate settings file (/home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/build/resource/stable_delegate_settings.json). Error at /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/peoplenet_main.cpp:235 ``` Since the character code contains only alphabets, the nkf command displayed it as ""ASCII"". ``` $ sudo apt install nkf $ nkf g /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/build/resource/stable_delegate_settings.json ASCII ``` https://github.com/xtrizeShino/peoplenet_onnx_to_tflite/blob/main/cpp_infer_vab5000/peoplenet_main.cppL232 ``` define TFLITE_MINIMAL_CHECK(x) \     if (!(x)) {  \         fprintf(stderr, ""Error at %s:%d\n"", __FILE__, __LINE__); \         exit(1);  \     } ... constexpr char kSettingsPath[] = ""/home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/build/resource/stable_delegate_settings.json"";  ... /* Load settings */ TfLiteSettingsJsonParser parser; const tflite::TFLiteSettings* settings = parser.Parse(kSettingsPath); TFLITE_MINIMAL_CHECK(settings != nullptr); ```","Hi, Did you find a fix for this by anychance? ",Not yet.,"The embedded environment I'm using may be special. This issue occurs on the Debian OS of the VAB5000, but the JSON file seems to be read normally from TensorFlow Lite on the Yocto OS. If you can't reproduce this issue on your end, I'll ask the vendor.",Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],[xla] HloEvaluator: optimize evaluating scatter with trivial update computation,[xla] HloEvaluator: optimize evaluating scatter with trivial update computation,2025-03-02T03:14:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88427
tpu,cybersupersoap,"`tf.compat.v1.linalg.set_diag` aborts with ""Check failed: d < dims() (2 vs. 2)"""," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.20.0dev20250225  Custom code Yes  OS platform and distribution Ubuntu 20.04 LTS   Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an aborted issue in TensorFlow when I used API tf.compat.v1.linalg.set_diag . I have confirmed that below code would crash on tfnightly 2.20.0dev20250225 (nightlybuild).  Standalone code to reproduce the issue ```shell import tensorflow as tf input_tensor = tf.random.uniform([5, 4, 4, 4], dtype=tf.float32) input = tf.identity(input_tensor) diagonal_0_0 = 2.0 diagonal_0_1 = 3.0 diagonal_0_2 = 4.0 diagonal_0_3 = 5.0 diagonal_0 = [diagonal_0_0, diagonal_0_1, diagonal_0_2, diagonal_0_3] diagonal_1_0 = 1.0 diagonal_1_1 = 2.0 diagonal_1_2 = 3.0 diagonal_1_3 = 4.0 diagonal_1 = [diagonal_1_0, diagonal_1_1, diagonal_1_2, diagonal_1_3] diagonal = [diagonal_0, diagonal_1] name = 'set_diag' k_0 = 0 k_1 = 1 k = [k_0, k_1] align = 'LEFT_RIGHT' out = tf.compat.v1.linalg.set_diag(input=input, diagonal=diagonal, name=name, k=k, align=align) ```  Relevant log output ```shell 20250302 02:25:05.721633: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (2 vs. 2) Aborted (core dumped) ```",2025-03-02T02:21:06Z,type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/88426,I was able to reproduce this issue on Colab using TensorFlow 2.18 and the nightly version. Please find the gist attached for your reference. Thank you!
tpu,cybersupersoap,`tf.config.threading.set_intra_op_parallelism_threads` can cause a crash," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.20.0dev20250225  Custom code Yes  OS platform and distribution Ubuntu 20.04 LTS  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `aborted issue` in TensorFlow when I used API `tf.config.threading.set_intra_op_parallelism_threads` . I have confirmed that below code would crash on `tfnightly 2.20.0dev20250225` (nightlybuild).  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np num_streams = 1 tf.config.threading.set_intra_op_parallelism_threads(num_streams) x = np.array([[1, 2], [3, 4]], dtype=np.float32) y = tf.cast(x, dtype=tf.int32) ```  Relevant log output ```shell 20250226 14:41:57.365578: F external/local_xla/xla/tsl/platform/threadpool.cc:126] Check failed: num_threads >= 1 (1 vs. 1) Aborted (core dumped) ```",2025-03-01T14:22:02Z,type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/88422,I was able to reproduce this issue on Colab using TensorFlow 2.18 and the nightly version. Please find the gist attached for your reference. Thank you!
opt,dependabot[bot],Bump ubuntu from `80dd3c3` to `7229784` in /tensorflow/tools/gcs_test,"Bumps ubuntu from `80dd3c3` to `7229784`. ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2025-03-01T08:46:43Z,ready to pull size:XS dependencies docker,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88413
opt,dependabot[bot],Bump ubuntu from `0e5e4a5` to `ed1544e` in /tensorflow/tools/tf_sig_build_dockerfiles,"Bumps ubuntu from `0e5e4a5` to `ed1544e`. ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2025-03-01T08:31:11Z,ready to pull size:XS dependencies docker,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88412
tpu,dependabot[bot],Bump the github-actions group with 6 updates,"Bumps the githubactions group with 6 updates:  Updates `peterevans/createpullrequest` from 7.0.6 to 7.0.7  Release notes Sourced from peterevans/createpullrequest's releases.  Create Pull Request v7.0.7 ⚙️ Fixes an issue with commit signing where modifications to the same file in multiple commits squash into the first commit. What's Changed  build(deps): bump @​octokit/core from 6.1.2 to 6.1.3 by @​dependabot in peterevans/createpullrequest CC(How to run custom encoderdecoder in Tensorflow using available APIs?) build(depsdev): bump @​types/node from 18.19.68 to 18.19.70 by @​dependabot in peterevans/createpullrequest CC(Moved eightbit graph trimming to before output_nodes definition) Update distribution by @​actionsbot in peterevans/createpullrequest CC(RC 0.10 3X Slower than 0.9 and Error Compiling From Source Under Certain Conditions) build(depsdev): bump typescript from 5.7.2 to 5.7.3 by @​dependabot in peterevans/createpullrequest CC(Graph optimization and other features) build(deps): bump octokit dependencies by @​peterevans in peterevans/createpullrequest CC(gcc: error: unrecognized command line option 'fnocanonicalsystemheaders') docs: add workflow tip for showing message via workflow command by @​ybiquitous in peterevans/createpullrequest CC(contrib/makefile:  error: conflicting return type) build(depsdev): bump eslintpluginprettier from 5.2.1 to 5.2.3 by @​dependabot in peterevans/createpullrequest CC(Unable to import frozen graph with batchnorm) build(deps): bump nodefetchnative from 1.6.4 to 1.6.6 by @​dependabot in peterevans/createpullrequest CC(Tensorflow inability to kill processes using more than 1 GPU) build(depsdev): bump undici from 6.21.0 to 6.21.1 by @​dependabot in peterevans/createpullrequest CC(TF works in python3.4 for some users but not for others under RedHat7 ) build(depsdev): bump @​types/node from 18.19.70 to 18.19.71 by @​dependabot in peterevans/createpullrequest CC(Problems in ""Implement the gradient in Python"" docs) Update distribution by @​actionsbot in peterevans/createpullrequest CC(Mismatch in gradient of 'abs' for complex values) build(depsdev): bump @​types/node from 18.19.71 to 18.19.74 by @​dependabot in peterevans/createpullrequest CC(0.10.0rc0: Contrib distributions crash when sampling ""n"" is scalar) build(depsdev): bump @​types/node from 18.19.74 to 18.19.75 by @​dependabot in peterevans/createpullrequest CC(Error when using TensorArray and variables in nested loops) build(deps): bump @​octokit/pluginrestendpointmethods from 13.3.0 to 13.3.1 by @​dependabot in peterevans/createpullrequest CC(Update CUDA/cuDNN in Dockerfiles) build(depsdev): bump prettier from 3.4.2 to 3.5.0 by @​dependabot in peterevans/createpullrequest CC(Add layer_norm op to contrib.layers.) Update distribution by @​actionsbot in peterevans/createpullrequest CC(error: can't copy 'tensorflow/models/embedding/gen_word2vec.py': doesn't exist) build(deps): bump @​octokit/requesterror from 6.1.6 to 6.1.7 by @​dependabot in peterevans/createpullrequest CC(Unable to generate signed APK for project based on Android demo) build(deps): bump @​octokit/pluginpaginaterest from 11.4.0 to 11.4.1 by @​dependabot in peterevans/createpullrequest CC(Error when trying to run tensorboard logdir=some_path) build(deps): bump @​octokit/endpoint from 10.1.2 to 10.1.3 by @​dependabot in peterevans/createpullrequest CC(Cuda8) Update distribution by @​actionsbot in peterevans/createpullrequest CC(Benchmarking example for iOS profiling) build(depsdev): bump prettier from 3.5.0 to 3.5.1 by @​dependabot in peterevans/createpullrequest CC(random_uniform for int32 broken on GPU) build(depsdev): bump eslintimportresolvertypescript from 3.7.0 to 3.8.1 by @​dependabot in peterevans/createpullrequest CC(function.Defun can't be applied to tf.Variable) build(deps): bump @​octokit/pluginpaginaterest from 11.4.1 to 11.4.2 by @​dependabot in peterevans/createpullrequest CC(Explanation of blank label in ctc_loss) build(depsdev): bump @​types/node from 18.19.75 to 18.19.76 by @​dependabot in peterevans/createpullrequest CC(gather_nd not working with API examples) build(deps): bump @​octokit/core from 6.1.3 to 6.1.4 by @​dependabot in peterevans/createpullrequest CC(mnist_with_summaries.py error 'module' object has no attribute 'DT_HALF') Update distribution by @​actionsbot in peterevans/createpullrequest CC(Fix typo in TensorFlow Linear Model Tutorial) Use showFileAtRefBase64 to read percommit file contents by @​grahamc in peterevans/createpullrequest CC(extract element from list when py_func's output type is a single tensorflow type)  New Contributors  @​ybiquitous made their first contribution in peterevans/createpullrequest CC(contrib/makefile:  error: conflicting return type) @​grahamc made their first contribution in peterevans/createpullrequest CC(extract element from list when py_func's output type is a single tensorflow type)  Full Changelog: https://github.com/peterevans/createpullrequest/compare/v7.0.6...v7.0.7    Commits  dd2324f fix: use showFileAtRefBase64 to read percommit file contents ( CC(extract element from list when py_func's output type is a single tensorflow type)) 367180c ci: remove testv5 cmd 25575a1 build: update distribution ( CC(Fix typo in TensorFlow Linear Model Tutorial)) a56e7a5 build(deps): bump @​octokit/core from 6.1.3 to 6.1.4 ( CC(mnist_with_summaries.py error 'module' object has no attribute 'DT_HALF')) eac17dc build(depsdev): bump @​types/node from 18.19.75 to 18.19.76 ( CC(gather_nd not working with API examples)) a2e685f build(deps): bump @​octokit/pluginpaginaterest from 11.4.1 to 11.4.2 ( CC(Explanation of blank label in ctc_loss)) 6cfd146 build(depsdev): bump eslintimportresolvertypescript ( CC(function.Defun can't be applied to tf.Variable)) b38e8d3 build(depsdev): bump prettier from 3.5.0 to 3.5.1 ( CC(random_uniform for int32 broken on GPU)) 8a41570 build: update distribution ( CC(Benchmarking example for iOS profiling)) 2e9b4cc build(deps): bump @​octokit/endpoint from 10.1.2 to 10.1.3 ( CC(Cuda8)) Additional commits viewable in compare view    Updates `ossf/scorecardaction` from 2.4.0 to 2.4.1  Release notes Sourced from ossf/scorecardaction's releases.  v2.4.1 What's Changed  This update bumps the Scorecard version to the v5.1.1 release. For a complete list of changes, please refer to the v5.1.0 and v5.1.1 release notes. Publishing results now uses half the API quota as before. The exact savings depends on the repository in question.  use Scorecard library entrypoint instead of Cobra hooking by @​spencerschrock in ossf/scorecardaction CC(Need force_gpu_if_available for tests)   Some errors were made into annotations to make them more visible  Make default branch error more prominent by @​jsoref in ossf/scorecardaction CC(partial_run segfault)   There is now an optional file_mode input which controls how repository files are fetched from GitHub. The default is archive, but git produces the most accurate results for repositories with .gitattributes files at the cost of analysis speed.  add input for specifying filemode by @​spencerschrock in ossf/scorecardaction CC(Fix python3 b)   The underlying container for the action is now hosted on GitHub Container Registry. There should be no functional changes.  :seedling: publish docker images to GitHub Container Registry by @​spencerschrock in ossf/scorecardaction CC(Multidimensional RNN)    Docs  Installation docs update by @​JeremiahAHoward in ossf/scorecardaction CC(ci_build  debian jessie)  New Contributors  @​JeremiahAHoward made their first contribution in ossf/scorecardaction CC(ci_build  debian jessie) @​jsoref made their first contribution in ossf/scorecardaction CC(partial_run segfault) Full Changelog: https://github.com/ossf/scorecardaction/compare/v2.4.0...v2.4.1     Commits  f49aabe bump docker to ghcr v2.4.1 ( CC(Hardcoded bash path)) 30a595b :seedling: Bump github.com/sigstore/cosign/v2 from 2.4.2 to 2.4.3 ( CC(rnn.bidirectional_rnn  cause a problem)) 69ae593 omit vcs info from build ( CC(Bugfix to test/run_and_gather_logs.)) 6a62a1c add input for specifying filemode ( CC(Fix python3 b)) 2722664 :seedling: Bump the githubactions group with 2 updates ( CC(Delete useless directory)) ae0ef31 :seedling: Bump github.com/spf13/cobra from 1.8.1 to 1.9.1 ( CC(some learning decays from Stanford CS231n Karpathy lecture 6)) 3676bbc :seedling: Bump golang from 1.23.6 to 1.24.0 in the dockerimages group ( CC(seems issues with softmax_cross_entropy_with_logits)) ae7548a Limit codeQL push trigger to main branch ( CC(quick python3 fix)) 9165624 upgrade scorecard to v5.1.0 ( CC(Fix python3 breakage (oldstyle exception block))) 620fd28 :seedling: Bump the githubactions group with 2 updates ( CC(typos fix and ign temp files in gitignore)) Additional commits viewable in compare view    Updates `actions/uploadartifact` from 4.6.0 to 4.6.1  Release notes Sourced from actions/uploadartifact's releases.  v4.6.1 What's Changed  Update to use artifact 2.2.2 package by @​yacaovsnc in actions/uploadartifact CC(PoolAlloc: Remove div by zero, demote WARN>INFO)  Full Changelog: https://github.com/actions/uploadartifact/compare/v4...v4.6.1    Commits  4cec3d8 Merge pull request  CC(PoolAlloc: Remove div by zero, demote WARN>INFO) from actions/yacaovsnc/artifact_2.2.2 e9fad96 license cache update for artifact b26fd06 Update to use artifact 2.2.2 package See full diff in compare view    Updates `github/codeqlaction` from 3.28.8 to 3.28.10  Release notes Sourced from github/codeqlaction's releases.  v3.28.10 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.10  21 Feb 2025  Update default CodeQL bundle version to 2.20.5.  CC(Please consider adding flatten) Address an issue where the CodeQL Bundle would occasionally fail to decompress on macOS.  CC(Checkpoint Restore blocked by changed default bias variable name)  See the full CHANGELOG.md for more information. v3.28.9 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.9  07 Feb 2025  Update default CodeQL bundle version to 2.20.4.  CC(C++ compilation of rule '//:sip_hash' failed (Tensorflow serving on Android))  See the full CHANGELOG.md for more information.    Changelog Sourced from github/codeqlaction's changelog.  CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. [UNRELEASED] No user facing changes. 3.28.10  21 Feb 2025  Update default CodeQL bundle version to 2.20.5.  CC(Please consider adding flatten) Address an issue where the CodeQL Bundle would occasionally fail to decompress on macOS.  CC(Checkpoint Restore blocked by changed default bias variable name)  3.28.9  07 Feb 2025  Update default CodeQL bundle version to 2.20.4.  CC(C++ compilation of rule '//:sip_hash' failed (Tensorflow serving on Android))  3.28.8  29 Jan 2025  Enable support for Kotlin 2.1.10 when running with CodeQL CLI v2.20.3.  CC(Fix for build issue 2742;)  3.28.7  29 Jan 2025 No user facing changes. 3.28.6  27 Jan 2025  Reenable debug artifact upload for CLI versions 2.20.3 or greater.  CC(Modifying MNIST example to distributed version: could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR)  3.28.5  24 Jan 2025  Update default CodeQL bundle version to 2.20.3.  CC(Branch 124290852)  3.28.4  23 Jan 2025 No user facing changes. 3.28.3  22 Jan 2025  Update default CodeQL bundle version to 2.20.2.  CC(Update roadmap.md) Fix an issue downloading the CodeQL Bundle from a GitHub Enterprise Server instance which occurred when the CodeQL Bundle had been synced to the instance using the CodeQL Action sync tool and the Actions runner did not have Zstandard installed.  CC(Branch 124251558) Uploading debug artifacts for CodeQL analysis is temporarily disabled.  CC(Tensorflow with Pyinstaller)  3.28.2  21 Jan 2025 No user facing changes. 3.28.1  10 Jan 2025  CodeQL Action v2 is now deprecated, and is no longer updated or supported. For better performance, improved security, and new features, upgrade to v3. For more information, see this changelog post.  CC(Import error)    ... (truncated)   Commits  b56ba49 Merge pull request  CC(Isn't current tensorflowgit r0.9? ) from github/updatev3.28.109856c48b1 60c9c77 Update changelog for v3.28.10 9856c48 Merge pull request  CC(Segmentation fault on tensorflow 0.9.0) from github/redsun82/rust 9572e09 Rust: fix log string 1a52936 Rust: special case default setup cf7e909 Merge pull request  CC(Please consider adding flatten) from github/updatebundle/codeqlbundlev2.20.5 b7006aa Merge branch 'main' into updatebundle/codeqlbundlev2.20.5 cfedae7 Rust: throw configuration errors if requested and not correctly enabled 3971ed2 Merge branch 'main' into redsun82/rust d38c6e6 Merge pull request  CC(Bazel fail to resolve submodule tensorflow) from github/angelapwen/bumpoctokit Additional commits viewable in compare view    Updates `docker/setupbuildxaction` from 3.8.0 to 3.10.0  Release notes Sourced from docker/setupbuildxaction's releases.  v3.10.0  Bump @​docker/actionstoolkit from 0.54.0 to 0.56.0 in docker/setupbuildxaction CC(Can't find pngwutil.c building tensorflow)  Full Changelog: https://github.com/docker/setupbuildxaction/compare/v3.9.0...v3.10.0 v3.9.0  Bump @​docker/actionstoolkit from 0.48.0 to 0.54.0 in docker/setupbuildxaction CC(More details of Inception model?) docker/setupbuildxaction CC(gcc4.8.1 is unhappy with usage of auto* in conv_grad_ops.cc)  Full Changelog: https://github.com/docker/setupbuildxaction/compare/v3.8.0...v3.9.0    Commits  b5ca514 Merge pull request  CC(Can't find pngwutil.c building tensorflow) from docker/dependabot/npm_and_yarn/docker/actionsto... 1418a4e chore: update generated content 93acf83 build(deps): bump @​docker/actionstoolkit from 0.54.0 to 0.56.0 f7ce87c Merge pull request  CC(gcc4.8.1 is unhappy with usage of auto* in conv_grad_ops.cc) from docker/dependabot/npm_and_yarn/docker/actionsto... aa1e2a0 chore: update generated content 673e008 build(deps): bump @​docker/actionstoolkit from 0.53.0 to 0.54.0 ba31df4 Merge pull request  CC(More details of Inception model?) from docker/dependabot/npm_and_yarn/docker/actionsto... 5475af1 chore: update generated content acacad9 build(deps): bump @​docker/actionstoolkit from 0.48.0 to 0.53.0 6a25f98 Merge pull request  CC(Cifar10 example bug (batch 5 not loading)) from crazymax/bakev6 Additional commits viewable in compare view    Updates `docker/buildpushaction` from 6.13.0 to 6.15.0  Release notes Sourced from docker/buildpushaction's releases.  v6.15.0  Bump @​docker/actionstoolkit from 0.55.0 to 0.56.0 in docker/buildpushaction CC(Problematic links in official website)  Full Changelog: https://github.com/docker/buildpushaction/compare/v6.14.0...v6.15.0 v6.14.0  Bump @​docker/actionstoolkit from 0.53.0 to 0.55.0 in docker/buildpushaction CC(GLIBC error)  Full Changelog: https://github.com/docker/buildpushaction/compare/v6.13.0...v6.14.0    Commits  471d1dc Merge pull request  CC(Problematic links in official website) from docker/dependabot/npm_and_yarn/docker/actionst... b89ff0a chore: update generated content 1e3ae3a chore(deps): Bump @​docker/actionstoolkit from 0.55.0 to 0.56.0 b16f42f Merge pull request  CC(tf.get_variable() cannot recognize existing variables) from crazymax/buildxedge dc0fea5 ci: update buildx to edge and buildkit to latest 0adf995 Merge pull request  CC(GLIBC error) from docker/dependabot/npm_and_yarn/docker/actionst... d88cd28 chore: update generated content 3d09a6b chore(deps): Bump @​docker/actionstoolkit from 0.53.0 to 0.55.0 See full diff in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore  major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)  ` ignore  minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)  ` ignore ` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)  ` unignore ` will remove all of the ignore conditions of the specified dependency  ` unignore  ` will remove the ignore condition of the specified dependency and ignore conditions ",2025-03-01T08:27:20Z,ready to pull size:S dependencies github_actions,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88411
yi,copybara-service[bot],litert: LiteRT changes for GPU support,litert: LiteRT changes for GPU support  Update CheckCpuTensors() to check nodes with execution_plan() instead of   checking all nodes_and_registration(). This aligns with SubGraph::Invoke()   and works well after applying a Delegate.  Added ExternalLiteRtBufferContext::RegisterLiteRtBufferRequirement()   to register with C type LiteRtTensorBufferRequirements.  Update visibility,2025-03-01T01:30:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88396
llm,copybara-service[bot],Part2: Remove moved code createLegalizeTFXlaCallModuleToStablehloPass from tensorflow/compiler/mlir/lite/stablehlo,Part2: Remove moved code createLegalizeTFXlaCallModuleToStablehloPass from tensorflow/compiler/mlir/lite/stablehlo,2025-02-28T23:45:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88390
tpu,copybara-service[bot],Add constraint for the DQD removal pattern in the quantize pass.,"Add constraint for the DQD removal pattern in the quantize pass. Add a constraint that requires the input and the result types to be the same. This prevents the quantize pass from emitting invalid IR when the requantization is actually necessary for the surrounding ops, e.g. ""tfl.reshape"" often require the input to the requantized to the same scale as its output's.",2025-02-28T21:44:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88379
llm,copybara-service[bot],Part1: Migrate createLegalizeTFXlaCallModuleToStablehloPass to tensorflow/compiler/mlir/stablehlo,Part1: Migrate createLegalizeTFXlaCallModuleToStablehloPass to tensorflow/compiler/mlir/stablehlo,2025-02-28T21:37:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88378
tpu,nassimus26,ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors,"for this code  (on colab https://colab.research.google.com/drive/1WKSgxQUSZp4Q5dHeq2HgJvjjVzxJUoqA?usp=sharing) :  ``` cnn = tf.keras.applications.EfficientNetV2B3(       include_top=False,       weights='imagenet',       input_tensor=None,       include_preprocessing=True,       input_shape = (300, 300, 3),       pooling=None,       classes = 2   ) def representative_dataset_gen():     for i in range(0):         yield [] converter = tf.lite.TFLiteConverter.from_keras_model(cnn) converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] converter.target_spec.supported_types = [tf.int8]   extra line missing converter.experimental_new_quantizer = True converter.experimental_new_dynamic_range_quantizer = True converter.inference_output_type = tf.uint8 converter.experimental_new_converter = True converter.experimental_mlir_quantizer = True converter.experimental_enable_resource_variables = False converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.representative_dataset = representative_dataset_gen tflite_model = converter.convert() open(""cnn.tflite"", ""wb"").write(tflite_model) ``` And then  `!edgetpu_compiler s cnn.tflite` I am getting :  ``` Edge TPU Compiler version 16.0.384591198 Started a compilation timeout timer of 180 seconds. ERROR: Attempting to use a delegate that only supports staticsized tensors with a graph that has dynamicsized tensors. Compilation failed: Model failed in Tflite interpreter. Please ensure model can be loaded/run in Tflite interpreter. Compilation child process completed within timeout period. Compilation failed!  ``` I am fully aware that the Coral Dev Board team release the EdgeTPU Model version (efficientnetedgetpuL_quant_edgetpu.tflite) But I am not interested in the default generated EdgeTPU file, indeed I want to do something like this :  ``` cnn = tf.keras.applications.EfficientNetV2B3(..pooling=None,..) // no pooling  x = cnn(cnn.input) x = layers.AveragePooling2D(pool_size=(5, 5), strides=(5, 5), padding=""same"")(x) x = layers.Flatten()(x) mdl = models.Model(inputs = cnn.input, outputs =x) ``` And then build the TFLite and the EdgeTPU. Unfortunately it doesn't seems possible to do this directly on a TFLite or TFliteEdgeTPU model :        For example : how to apply the code above with this : EfficientB3 EDGE_TPU version ? And it seems the Coral Dev Board Team has rewrite the EfficientNet model from scratch, but they don't explain why or how to import their model implemented here TPU repo  By import I mean how to replace the pretrainded **tf.keras.applications.EfficientNetV2B3** with their model ?",2025-02-28T20:35:22Z,stat:awaiting response stale comp:lite TFLiteConverter,closed,0,5,https://github.com/tensorflow/tensorflow/issues/88361,"Hi,   I apologize for the delay in my response, I have been able to replicate the similar issue with your provided code and I'm also getting same error message please refer this gistfile so we will have to dig more into this issue and will update you. **Here is error log output for reference :** ``` Edge TPU Compiler version 16.0.384591198 Started a compilation timeout timer of 180 seconds. ERROR: Attempting to use a delegate that only supports staticsized tensors with a graph that has dynamicsized tensors. Compilation failed: Model failed in Tflite interpreter. Please ensure model can be loaded/run in Tflite interpreter. Compilation child process completed within timeout period. Compilation failed!  ``` Thank you for your cooperation and patience.","Hi  , thank you, what do you mean by : please refer this [gistfile] ?  What do you want me to do about it ?, I already write you a test case and publish it on this page. And I hope that the Edge compiler project is not dead, most of their converted models has been written with TF 1.x, and the compiler has not been updated since many years, and it seems not Open source.","Hi,  I apologize for the delayed response, sorry for the confusion I mean to say I am able to replicate the same behavior which you reported from my end so for further investigation from our end as reference I added gistfile As per the official documentation of TensorFlow models on the Edge TPU specifically Model requirements section the model must meet these basic requirements:  Tensor parameters are quantized (8bit fixedpoint numbers; int8 or uint8).  Tensor sizes are constant at compiletime (no dynamic sizes).  Model parameters (such as bias tensors) are constant at compiletime.  Tensors are either 1, 2, or 3dimensional. If a tensor has more than 3 dimensions, then only the 3 innermost dimensions may have a size greater than 1.  The model uses only the operations supported by the Edge TPU (see table 1 below). Please use tools like Netron to visualize your model and check for operations that produce dynamicshaped tensors. The Edge TPU requires models to be quantized to (8bit fixedpoint numbers; int8 or uint8). Make sure that the representative_dataset is correctly set and that the model is fully quantized. some layers or operations might not be supported by the Edge TPU. Please make sure that all layers in your model are compatible with Edge TPU. You can refer to the coral documentation for a list of supported operations. In some cases there are no dynamic size tensors in the graph but it has a control flow op, While op which classifies graph as dynamic graph that might be the cause for this. Thank you for your cooperation and understanding.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
tpu,copybara-service[bot],[PJRT:CPU] Implement `PjRtDevice::PoisonExecution()` API.,"[PJRT:CPU] Implement `PjRtDevice::PoisonExecution()` API. `PjRtDevice::PoisonExecution()` API provides a mechanism to mark output buffers of a pending asynchronous execution with an error, where the error is constructed above PjRt and potentially reflect a broader view on the system state. This change implements this API for the PJRT CPU client to improve support for cancelling pending executions. The state of poisonable executions is owned by `TfrtCpuDevice` and accessed from `TfrtCpuExecutable` as via a new class `TfrtCpuAsyncExecutionTracker`.",2025-02-28T20:09:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88360
tpu,AD-lite24,Disabling GPU delegate for particular tflite nodes, Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17.1  Custom code Yes  OS platform and distribution Linux Ubuntu 18.04 ARM  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The OpenCL GPU delegate on our device doesn't support floor operations causing the delegation for the entire graph to fail.  ``` INFO: Loaded OpenCL library with dlopen. ERROR: No selector for floor ERROR: Falling back to OpenGL ``` Is there anyway I can disable delegation specifically for the nodes involving floor ops so that it doesn't tank my entire delegation?   Standalone code to reproduce the issue ```shell N/A ```  Relevant log output ```shell ```,2025-02-28T19:45:40Z,type:feature comp:lite 2.17,open,0,6,https://github.com/tensorflow/tensorflow/issues/88357,"Hi, lite24 I apologize for the delay in my response, As far I know to resolve delegation failures caused by unsupported `floor` operations in TFLite's OpenCL GPU delegate use custom delegate with Op filtering implement a delegate that skips `floor` operations during GPU delegation, force floor ops to CPU via explicit partitioning or replace floor with a GPU compatible approximation during model conversion if precision allows.  Please refer official documentation of Implementing a Custom Delegate  and GPU ML operations support Thank you for your cooperation and understanding."," I went through the docs but unfortunately none of the delegates seem to implement the interface and all the invoking API does not work with the interface either. Regardless, I figured out on my own how to disable delegating the floor op to GPU, but then I have another set of problems ``` GATHER_ND: Operation is not supported. GREATER: Not supported logical op case. LESS: Not supported logical op case. LOGICAL_NOT: Operation is not supported. LOGICAL_OR: Operation is not supported. MUL: MUL requires one tensor that not less than second in all dimensions. RESHAPE: OP is supported, but tensor type/shape isn't compatible. SCATTER_ND: Operation is not supported. TOPK_V2: Operation is not supported. TRANSPOSE: OP is supported, but tensor type/shape isn't compatible. 28 operations will run on the GPU, and the remaining 164 operations will run on the CPU. VERBOSE: Replacing 28 out of 192 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 3 partitions for the whole graph. ERROR: TfLiteGpuDelegate Init: MUL: Batch size is not equal to 1. INFO: Created 0 GPU delegate kernels. ERROR: TfLiteGpuDelegate Prepare: delegate is not initialized ERROR: Node number 192 (TfLiteGpuDelegateV2) failed to prepare. ERROR: Restored original execution plan after delegate application failure. GPU delegate error. INFO: Applying 1 TensorFlow Lite delegate(s) lazily. INFO: Created TensorFlow Lite XNNPACK delegate for CPU. INFO: XNNPack weight cache not enabled ``` What I fail to understand is why the delegate fails to apply to the entire graph every time a singular node fails"," Please find the output from the tensorflow analyzer. I find it rather odd that the MUL op has a problem with the batch dimension not being 1 (why else would I have a batch dimension in that case) and more importantly the exception is thrown during delegate init and not during checking, causing the entire delegate to fail. ``` === TFLite ModelAnalyzer === Your TFLite model has '1' subgraph(s). In the subgraph description below, T represents the Tensor numbers. For example, in Subgraph CC(未找到相关数据), the DEQUANTIZE op takes tensor CC(Specify output tensor for ops from python) as input and produces tensor CC(Mac: OSError: [Errno 1] Operation not permitted: '/tmp/pipXcfgD6uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six1.4.1py2.7.egginfo') as output. Subgraph CC(未找到相关数据) main(T CC(未找到相关数据)) > [T CC(No documentation for Saver class), T CC(error in computation graph tutorials)]   Op CC(未找到相关数据) DEQUANTIZE(T CC(Specify output tensor for ops from python)) > [T CC(Mac: OSError: [Errno 1] Operation not permitted: '/tmp/pipXcfgD6uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six1.4.1py2.7.egginfo')]   Op CC(Add support for Python 3.x) DEQUANTIZE(T CC(CUDNN error on ""import tensorflow as tf"" for gpu version)) > [T CC(FR: Change TensorBoard image interpolation method)]   Op CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"") DEQUANTIZE(T CC(ImportError: No module named core.framework.graph_pb2)) > [T CC(compilation error)]   Op CC(JVM, .NET Language Support) DEQUANTIZE(T CC(Try to convert to readable latex)) > [T CC(no such package '//': Error downloading from ijg.org)]   Op CC(Installation over pip fails to import with protobuf 2.6.1) DEQUANTIZE(T CC(Error in final step of installation)) > [T CC(Typo)]   Op CC(Java interface) DEQUANTIZE(T CC(No gradient defined for the Reverse op)) > [T CC(models/rnn/ptb not included in pip package)]   Op CC(Pretrained models) DEQUANTIZE(T CC(virtualenv python 2.7.6 import tensorflow. TypeError: __init__() got an unexpected keyword argument 'syntax')) > [T CC(Human Consumable Language Independent DSL)]   Op CC(API docs does not list RNNs) DEQUANTIZE(T CC(pip error: No such file or directory: '/tmp/pip...build/setup.py')) > [T CC(Can't install from source if I don't have a GPU? )]   Op CC(Setting lower gcc version for cuda) DEQUANTIZE(T CC(Javascript > JavaScript)) > [T CC(Target //tensorflow/tools/pip_package:build_pip_package failed to build on OSX)]   Op CC(Typo in getting started guide) DEQUANTIZE(T CC(Can't install on ubuntu 12.04.5 LTS)) > [T CC(No plan for official doc of any other languages than English?)]   Op CC(Go API) DEQUANTIZE(T CC(Problem running RNN example)) > [T CC(Cannot run the android example on Android 5.1.1)]   Op CC(0.5.0 wheel install on Mac OS X using Homebrew python broken) DEQUANTIZE(T CC(error __init__() got an unexpected keyword argument 'syntax')) > [T CC(No module named copy_reg  Installation Issue)]   Op CC(Remote worker configuration) DEQUANTIZE(T CC(OSX PIP Install: Setup.py missing)) > [T CC(cuDNN v2 (6.5) not available anymore)]   Op CC([doc] typo) DEQUANTIZE(T CC(Pretrained models)) > [T CC(1st Class Windows Support)]   Op CC(g3doc format) DEQUANTIZE(T CC(Error in the Getting started/Variables section of the website)) > [T CC(Scalar Equation is incorrect ? Documentation)]   Op CC(Quantized ops?) DEQUANTIZE(T CC(is a Python 3 support coming soon ?)) > [T CC(segmentation fault when running convolutional.py)]   Op CC(iOS Support and Example) DEQUANTIZE(T CC(Updated links in documentation.)) > [T CC(tensorboard gulp  analytics.js missing)]   Op CC(Windows Support and Documentation) DEQUANTIZE(T CC(Node.js (JavaScript) Wrapper API)) > [T CC(Installed from source; unable to import tensorflow)]   Op CC(C api) DEQUANTIZE(T CC(No module named tensorflow.python.platform)) > [T CC(bazel compile error)]   Op CC(Swift API) DEQUANTIZE(T CC(Can't run TensorBoard on El Captain)) > [T CC(can't install on ubuntu 12.04)]   Op CC(CUDA 7.5 fails with pip install and docker (Ubuntu 14.04)) DEQUANTIZE(T CC(Error when run docker image on Mac OS X 10.11.1)) > [T CC(Septation of Generalised DAG / Data Flow Programming Framework and ML Components)]   Op CC(GPU_Base dockerfile image not found) DEQUANTIZE(T CC(Error while installing tensorflow using pip on Ubuntu 14.04 32bit system)) > [T CC(C++ API neural net examples)]   Op CC(OpenCL support) DEQUANTIZE(T CC(Connectionist Temporal Classification example)) > [T CC(GPU implementations for more ops)]   Op CC(Distributed Version) DEQUANTIZE(T CC(Slack Channel)) > [T CC(fixed link to tutorial and some typos)]   Op CC(Problems running the image example (Python 2.7.10, PyEnv, Xubuntu 14.04 64bit)) DEQUANTIZE(T CC(Go API)) > [T CC(Unable to run tensorboard)]   Op CC(Cuda 3.0?) DEQUANTIZE(T CC(minimum req: Cuda compute capability 3.5)) > [T CC(Typo in `/tutorials/mnist/beginners/index.md`)]   Op CC(simplify contributing process) DEQUANTIZE(T CC(Could port to OpenCL?)) > [T CC(Fix 89)]   Op CC(Warning while creating Session on Mac OS X: can't determine number of CPU cores) DEQUANTIZE(T CC(Warning while creating Session on Mac OS X: can't determine number of CPU cores)) > [T CC(import six.moves.copyreg as copyreg error)]   Op CC(Could port to OpenCL?) DEQUANTIZE(T CC(simplify contributing process)) > [T CC(do we have plans for java api?)]   Op CC(minimum req: Cuda compute capability 3.5) DEQUANTIZE(T CC(CUDA 7.5 fails with pip install and docker (Ubuntu 14.04))) > [T CC( build failed!  File ""/usr/lib/python2.7/encodings/__init__.py"", line 123       raise CodecRegistryError,\)]   Op CC(Go API) DEQUANTIZE(T CC(Swift API)) > [T CC(Abandon gerrit and use github for everything)]   Op CC(Slack Channel) DEQUANTIZE(T CC(C api)) > [T CC(Greedy heuristics may not find the optimal node placement)]   Op CC(Connectionist Temporal Classification example) DEQUANTIZE(T CC(Windows Support and Documentation)) > [T CC(tensorflow0.5.0cp27nonelinux_x86_64.whl is not a supported wheel on this platform.)]   Op CC(Error while installing tensorflow using pip on Ubuntu 14.04 32bit system) DEQUANTIZE(T CC(iOS Support and Example)) > [T CC(How to extract predictions)]   Op CC(Error when run docker image on Mac OS X 10.11.1) DEQUANTIZE(T CC(Quantized ops?)) > [T CC(MAC pip install operation not permitted)]   Op CC(Can't run TensorBoard on El Captain) DEQUANTIZE(T CC(Go API)) > [T CC(Fix when installation on OSX)]   Op CC(No module named tensorflow.python.platform) DEQUANTIZE(T CC(Setting lower gcc version for cuda)) > [T CC(Installing from source  problem with bazel)]   Op CC(Node.js (JavaScript) Wrapper API) DEQUANTIZE(T CC(Quantized ops?)) > [T CC(Neural Translation Model example fails due to missing EN tokens )]   Op CC(Updated links in documentation.) DEQUANTIZE(T CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"")) > [T CC(Fix 'Fetches' example in basic_usage)]   Op CC(is a Python 3 support coming soon ?) DEQUANTIZE(T CC(Add support for Python 3.x)) > [T CC(Alpine Linux: __isnanf: symbol not found )]   Op CC(Error in the Getting started/Variables section of the website) CONV_2D(T CC(未找到相关数据), T CC(Fix 'Fetches' example in basic_usage), T CC(Alpine Linux: __isnanf: symbol not found )) > [T CC(Support for python 3)]   Op CC(Pretrained models) CONV_2D(T CC(Support for python 3), T CC(fixed link to tutorial and some typos), T CC(Target //tensorflow/tools/pip_package:build_pip_package failed to build on OSX)) > [T CC(Can't build from source?)]   Op CC(Windows support) MAX_POOL_2D(T CC(Can't build from source?)) > [T CC(Ubuntu installation error using pip)]   Op CC(Cannot import after installing with pip) CONV_2D(T CC(Ubuntu installation error using pip), T CC(GPU implementations for more ops), T CC(Can't install from source if I don't have a GPU? )) > [T CC(Truncated backprop docs are confusing)]   Op CC(Can't install on El Capitan  probably python again :() CONV_2D(T CC(Truncated backprop docs are confusing), T CC(C++ API neural net examples), T CC(Human Consumable Language Independent DSL)) > [T CC(Building a shared libary)]   Op CC(OSX PIP Install: Setup.py missing) MAX_POOL_2D(T CC(Building a shared libary)) > [T CC( C++ compilation of rule '//tensorflow/python:tf_session_helper' failed)]   Op CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"") CONV_2D(T CC( C++ compilation of rule '//tensorflow/python:tf_session_helper' failed), T CC(Septation of Generalised DAG / Data Flow Programming Framework and ML Components), T CC(models/rnn/ptb not included in pip package)) > [T CC(Support for Redhat, Centos and many superclusters)]   Op CC(Integration with blaze ecosystem numba python to llvm compiler?) CONV_2D(T CC(Support for Redhat, Centos and many superclusters), T CC(can't install on ubuntu 12.04), T CC(Typo)) > [T CC(configure script hardcodes location of cuda that makes it fail on OSX)]   Op CC(Object Detection) MAX_POOL_2D(T CC(configure script hardcodes location of cuda that makes it fail on OSX)) > [T CC(EC2 g2.2xlarge: Ignoring gpu device (GRID K520) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.)]   Op CC(error __init__() got an unexpected keyword argument 'syntax') CONV_2D(T CC(EC2 g2.2xlarge: Ignoring gpu device (GRID K520) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.), T CC(bazel compile error), T CC(no such package '//': Error downloading from ijg.org)) > [T CC(AlexNet with FC layers: backward is very slow?)]   Op CC(Ruby API) CONV_2D(T CC(AlexNet with FC layers: backward is very slow?), T CC(Installed from source; unable to import tensorflow), T CC(compilation error)) > [T CC(Matrix multiplication in softmax documentation carried out incorrectly)]   Op CC(Ubuntu ImportError: No module named core.framework.graph_pb2) CONV_2D(T CC(Matrix multiplication in softmax documentation carried out incorrectly), T CC(tensorboard gulp  analytics.js missing), T CC(FR: Change TensorBoard image interpolation method)) > [T CC(Incorrect matrix math in tutorial at:  http://www.tensorflow.org/tutorials/mnist/beginners/index.md)]   Op CC(Problem running RNN example) CONV_2D(T CC(Incorrect matrix math in tutorial at:  http://www.tensorflow.org/tutorials/mnist/beginners/index.md), T CC(segmentation fault when running convolutional.py), T CC(Cannot run the android example on Android 5.1.1)) > [T CC(When will you have a version of TensorFlow for Win10/8/7?)]   Op CC(Can't install on ubuntu 12.04.5 LTS) SOFTMAX(T CC(When will you have a version of TensorFlow for Win10/8/7?)) > [T CC(TensorBoard logdir path, if relative, is relative to $HOME)]   Op CC(Support cuda 7.5 and cudnn 7.0) CONV_2D(T CC(Matrix multiplication in softmax documentation carried out incorrectly), T CC(Scalar Equation is incorrect ? Documentation), T CC(Mac: OSError: [Errno 1] Operation not permitted: '/tmp/pipXcfgD6uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six1.4.1py2.7.egginfo')) > [T CC(""help wanted"" I cannot get the TensorBoard working, I am following the given tutorial)]   Op CC(Javascript > JavaScript) CONV_2D(T CC(""help wanted"" I cannot get the TensorBoard working, I am following the given tutorial), T CC(1st Class Windows Support), T CC(No plan for official doc of any other languages than English?)) > [T CC(Bazel can't build protobuf)]   Op CC(pip error: No such file or directory: '/tmp/pip...build/setup.py') MUL(T CC(Bazel can't build protobuf), T CC(Bazel can't build protobuf)) > [T CC(TensorFlow session.run() overhead for graphs with few flops)]   Op CC(virtualenv python 2.7.6 import tensorflow. TypeError: __init__() got an unexpected keyword argument 'syntax') SUM(T CC(TensorFlow session.run() overhead for graphs with few flops), T CC(0.5.0 wheel install on Mac OS X using Homebrew python broken)[3]) > [T CC(ImportError: undefined symbol: clock_gettime)]   Op CC(No gradient defined for the Reverse op) SQRT(T CC(ImportError: undefined symbol: clock_gettime)) > [T CC(Windows Installation?)]   Op CC(Error in final step of installation) MAXIMUM(T CC(Windows Installation?), T CC(MAC pip install operation not permitted)) > [T CC(Cant install, Mac  El Capitan  not a supported wheel)]   Op CC(Try to convert to readable latex) MUL(T CC(Cant install, Mac  El Capitan  not a supported wheel), T CC(How to extract predictions)) > [T CC(tools/jdk: BUILD file not found on package path.)]   Op CC(ImportError: No module named core.framework.graph_pb2) DIV(T CC(Bazel can't build protobuf), T CC(tools/jdk: BUILD file not found on package path.)) > [T CC(Can anyone install it with cuda7.5 and cudnn 7.0?)]   Op CC(CUDNN error on ""import tensorflow as tf"" for gpu version) STRIDED_SLICE(T CC(TensorBoard logdir path, if relative, is relative to $HOME), T CC(Windows support)[0, 0, 0, 0], T CC(Cannot import after installing with pip)[0, 0, 0, 64], T CC(Can't install on El Capitan  probably python again :()[1, 1, 1, 1]) > [T CC(cpu version Installed successfully, but cannot import tensorflow in python.)]   Op CC(Specify output tensor for ops from python) TRANSPOSE(T CC(cpu version Installed successfully, but cannot import tensorflow in python.), T CC(Remote worker configuration)[0, 3, 1, 2]) > [T CC(cannot use bazel to compile tensorflow example codes)]   Op CC(Mac: OSError: [Errno 1] Operation not permitted: '/tmp/pipXcfgD6uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six1.4.1py2.7.egginfo') RESHAPE(T CC(cannot use bazel to compile tensorflow example codes), T CC(Problems running the image example (Python 2.7.10, PyEnv, Xubuntu 14.04 64bit))[2, 8, 8, 64, 64]) > [T CC(Lots of C++ compiletime warnings)]   Op CC(FR: Change TensorBoard image interpolation method) TRANSPOSE(T CC(Lots of C++ compiletime warnings), T CC([doc] typo)[0, 3, 1, 4, 2]) > [T CC(Update docs to instruct to use Bazel 0.1.1 installer)]   Op CC(compilation error) RESHAPE(T CC(Update docs to instruct to use Bazel 0.1.1 installer), T CC(Support cuda 7.5 and cudnn 7.0)[2, 512, 512, 1]) > [T CC(Use Bazel 0.1.1)]   Op CC(no such package '//': Error downloading from ijg.org) MAX_POOL_2D(T CC(Use Bazel 0.1.1)) > [T CC(CUDA 7.0 is hardcoded in `configure` script for Linux)]   Op CC(Typo) EQUAL(T CC(Use Bazel 0.1.1), T CC(CUDA 7.0 is hardcoded in `configure` script for Linux)) > [T CC(Communicating channels: gitter.im + discourse)]   Op CC(models/rnn/ptb not included in pip package) CAST(T CC(Communicating channels: gitter.im + discourse)) > [T CC(Linux installation problem with VirtualEnv)]   Op CC(Human Consumable Language Independent DSL) MAX_POOL_2D(T CC(Linux installation problem with VirtualEnv)) > [T CC(Linux installation issue for GPUenabled version)]   Op CC(Can't install from source if I don't have a GPU? ) CAST(T CC(Linux installation issue for GPUenabled version)) > [T CC(osx 10.11 installation issues)]   Op CC(Target //tensorflow/tools/pip_package:build_pip_package failed to build on OSX) LOGICAL_NOT(T CC(osx 10.11 installation issues)) > [T CC(ResourceExhaustedError in CNN/MNIST example (with GPU))]   Op CC(No plan for official doc of any other languages than English?) SELECT(T CC(osx 10.11 installation issues), T CC(Fix 89), T CC(Use Bazel 0.1.1)) > [T CC(bazel always redownloaded the dependency libraries)]   Op CC(Cannot run the android example on Android 5.1.1) MAX_POOL_2D(T CC(bazel always redownloaded the dependency libraries)) > [T CC(Out of Memory in mnist?)]   Op CC(No module named copy_reg  Installation Issue) EQUAL(T CC(bazel always redownloaded the dependency libraries), T CC(Out of Memory in mnist?)) > [T CC(unable to use nn.moments when the dimension of the axis is None)]   Op CC(cuDNN v2 (6.5) not available anymore) LOGICAL_AND(T CC(unable to use nn.moments when the dimension of the axis is None), T CC(ResourceExhaustedError in CNN/MNIST example (with GPU))) > [T CC(Questions about using LSTM )]   Op CC(1st Class Windows Support) LOGICAL_OR(T CC(Communicating channels: gitter.im + discourse), T CC(Questions about using LSTM )) > [T CC(typo in decaying the learning rate example)]   Op CC(Scalar Equation is incorrect ? Documentation) CAST(T CC(typo in decaying the learning rate example)) > [T CC(TF not compatible with AWS GPU instances?)]   Op CC(segmentation fault when running convolutional.py) MAX_POOL_2D(T CC(TF not compatible with AWS GPU instances?)) > [T CC(Truncated backdrop with max pooling over time)]   Op CC(tensorboard gulp  analytics.js missing) CAST(T CC(Truncated backdrop with max pooling over time)) > [T CC(tutorial GPU issue)]   Op CC(Installed from source; unable to import tensorflow) LOGICAL_NOT(T CC(tutorial GPU issue)) > [T CC(unable to install Inside the virtualenv, install TensorFlow:)]   Op CC(bazel compile error) SELECT(T CC(tutorial GPU issue), T CC(Fix 89), T CC(Use Bazel 0.1.1)) > [T CC(Wrong multiplication in MNIST beginner tutorial)]   Op CC(can't install on ubuntu 12.04) MAX_POOL_2D(T CC(Wrong multiplication in MNIST beginner tutorial)) > [T CC(add multiplemachine support)]   Op CC(Septation of Generalised DAG / Data Flow Programming Framework and ML Components) EQUAL(T CC(Wrong multiplication in MNIST beginner tutorial), T CC(add multiplemachine support)) > [T CC(Wrong link in the ""common problems"" docs)]   Op CC(C++ API neural net examples) LOGICAL_AND(T CC(Wrong link in the ""common problems"" docs), T CC(unable to install Inside the virtualenv, install TensorFlow:)) > [T CC(Official Tensorflow Docker Image)]   Op CC(GPU implementations for more ops) LOGICAL_OR(T CC(typo in decaying the learning rate example), T CC(Official Tensorflow Docker Image)) > [T CC(Is there 3D ConvNets support ? )]   Op CC(fixed link to tutorial and some typos) SELECT(T CC(Is there 3D ConvNets support ? ), T CC(Use Bazel 0.1.1), T CC(Fix 89)) > [T CC(Typo in reshape documentation)]   Op CC(Unable to run tensorboard) GATHER(T CC(Typo in reshape documentation), T CC(API docs does not list RNNs)[0]) > [T CC(Segmentation fault when GPUs are already used)]   Op CC(Typo in `/tutorials/mnist/beginners/index.md`) TRANSPOSE(T CC(Segmentation fault when GPUs are already used), T CC(Typo in getting started guide)[0, 2, 1]) > [T CC(Library not loaded: /usr/lib/libc++.1.dylib)]   Op CC(Fix 89) SCATTER_ND(T CC(Integration with blaze ecosystem numba python to llvm compiler?)[0, 0, 0, 1, 0, ...], T CC(cuDNN v2 (6.5) not available anymore), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(Changes to word2vec_basic.py)]   Op CC(import six.moves.copyreg as copyreg error) SUB(T CC(Fix when installation on OSX), T CC(Changes to word2vec_basic.py)) > [T CC(Complete the loop before returning the words)]   Op CC(do we have plans for java api?) MUL(T CC(Complete the loop before returning the words), T CC(Library not loaded: /usr/lib/libc++.1.dylib)) > [T CC(RuntimeError: Broken toolchain: cannot link a simple C program)]   Op CC( build failed!  File ""/usr/lib/python2.7/encodings/__init__.py"", line 123       raise CodecRegistryError,\) SCATTER_ND(T CC(Integration with blaze ecosystem numba python to llvm compiler?)[0, 0, 0, 1, 0, ...], T CC(do we have plans for java api?), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(Change test set in mnist demo to use batches to avoid being OOM (>4GB) on gpu.)]   Op CC(Abandon gerrit and use github for everything) ADD(T CC(RuntimeError: Broken toolchain: cannot link a simple C program), T CC(Change test set in mnist demo to use batches to avoid being OOM (>4GB) on gpu.)) > [T CC(Switch int to uint to remove some warnings)]   Op CC(Greedy heuristics may not find the optimal node placement) TRANSPOSE(T CC(Switch int to uint to remove some warnings), T CC(Typo in getting started guide)[0, 2, 1]) > [T CC(tensorflow binary image for ARM architecture)]   Op CC(tensorflow0.5.0cp27nonelinux_x86_64.whl is not a supported wheel on this platform.) SCATTER_ND(T CC(Object Detection)[0, 508, 0, 509, 0, ...], T CC(cuDNN v2 (6.5) not available anymore), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(doc for install from source of pip+gpu missing config=cuda and use_gpu)]   Op CC(How to extract predictions) SUB(T CC(Fix when installation on OSX), T CC(doc for install from source of pip+gpu missing config=cuda and use_gpu)) > [T CC(Can't install tensorflow on OS X  problem with virtualenv)]   Op CC(MAC pip install operation not permitted) MUL(T CC(Can't install tensorflow on OS X  problem with virtualenv), T CC(tensorflow binary image for ARM architecture)) > [T CC(Any Roadmap Available?)]   Op CC(Fix when installation on OSX) SCATTER_ND(T CC(Object Detection)[0, 508, 0, 509, 0, ...], T CC(do we have plans for java api?), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(Published Roadmap)]   Op CC(Installing from source  problem with bazel) ADD(T CC(Any Roadmap Available?), T CC(Published Roadmap)) > [T CC(Direct Native Code / LLVM IR generation / JIT Compilation / Staging / Incremental Computing)]   Op CC(Neural Translation Model example fails due to missing EN tokens ) TRANSPOSE(T CC(Direct Native Code / LLVM IR generation / JIT Compilation / Staging / Incremental Computing), T CC(Typo in getting started guide)[0, 2, 1]) > [T CC(in_top_k op does not work with int64 labels)]   Op CC(Fix 'Fetches' example in basic_usage) SCATTER_ND(T CC(Ruby API)[0, 0, 0, 0, 0, ...], T CC(No module named copy_reg  Installation Issue), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(Does TensorFlow support temporal convolution)]   Op CC(Alpine Linux: __isnanf: symbol not found ) SUB(T CC(Fix when installation on OSX), T CC(Does TensorFlow support temporal convolution)) > [T CC(g3doc is not installed when using pip )]   Op CC(Support for python 3) MUL(T CC(g3doc is not installed when using pip ), T CC(in_top_k op does not work with int64 labels)) > [T CC(tensor flow does not support operator.__truediv__)]   Op CC(Can't build from source?) SCATTER_ND(T CC(Ruby API)[0, 0, 0, 0, 0, ...], T CC(import six.moves.copyreg as copyreg error), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(Small typo in Deep MNIST Tutorial)]   Op CC(Ubuntu installation error using pip) ADD(T CC(tensor flow does not support operator.__truediv__), T CC(Small typo in Deep MNIST Tutorial)) > [T CC(Not a gzipped file)]   Op CC(Truncated backprop docs are confusing) TRANSPOSE(T CC(Not a gzipped file), T CC(Typo in getting started guide)[0, 2, 1]) > [T CC(Tensorboard creates unecessary loops in graph)]   Op CC(Building a shared libary) SCATTER_ND(T CC(Ubuntu ImportError: No module named core.framework.graph_pb2)[0, 0, 508, 0, 0, ...], T CC(No module named copy_reg  Installation Issue), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(Wrong Logistic Loss)]   Op CC( C++ compilation of rule '//tensorflow/python:tf_session_helper' failed) SUB(T CC(Fix when installation on OSX), T CC(Wrong Logistic Loss)) > [T CC(Unable to restore trained models on the enfr translate model:  tensorflow.python.framework.errors.NotFoundError: Tensor name ""embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Matrix"" not found)]   Op CC(Support for Redhat, Centos and many superclusters) MUL(T CC(Unable to restore trained models on the enfr translate model:  tensorflow.python.framework.errors.NotFoundError: Tensor name ""embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Matrix"" not found), T CC(Tensorboard creates unecessary loops in graph)) > [T CC(run from script only instead of compiling with bazel?)]   Op CC(configure script hardcodes location of cuda that makes it fail on OSX) SCATTER_ND(T CC(Ubuntu ImportError: No module named core.framework.graph_pb2)[0, 0, 508, 0, 0, ...], T CC(import six.moves.copyreg as copyreg error), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(einsumlike function?)]   Op CC(EC2 g2.2xlarge: Ignoring gpu device (GRID K520) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.) ADD(T CC(run from script only instead of compiling with bazel?), T CC(einsumlike function?)) > [T CC(Split a tensor with Tensor 0D int32 as num_split argument)]   Op CC(AlexNet with FC layers: backward is very slow?) TRANSPOSE(T CC(Split a tensor with Tensor 0D int32 as num_split argument), T CC(Typo in getting started guide)[0, 2, 1]) > [T CC(ImportError: /lib64/libc.so.6: version `GLIBC_2.17')]   Op CC(Matrix multiplication in softmax documentation carried out incorrectly) RESHAPE(T CC(ImportError: /lib64/libc.so.6: version `GLIBC_2.17'), T CC(Cuda 3.0?)[2, 262144]) > [T CC(Installation error)]   Op CC(Incorrect matrix math in tutorial at:  http://www.tensorflow.org/tutorials/mnist/beginners/index.md) TOPK_V2(T CC(Installation error), T CC(GPU_Base dockerfile image not found)[512]) > [T CC(Typo in TensorBoard: Visualizing Learning docs), T CC('state' is not defined )]   Op CC(When will you have a version of TensorFlow for Win10/8/7?) CAST(T CC('state' is not defined )) > [T CC(Typo in tf.Graph.name_scope(name) docs)]   Op CC(TensorBoard logdir path, if relative, is relative to $HOME) RESHAPE(T CC(Typo in tf.Graph.name_scope(name) docs), T CC(OpenCL support)[2, 512, 1]) > [T CC(The icon is assymetric)]   Op CC(""help wanted"" I cannot get the TensorBoard working, I am following the given tutorial) CAST(T CC(The icon is assymetric)) > [T CC(does tensorboard allow for building/editing models?)]   Op CC(Bazel can't build protobuf) MUL(T CC(does tensorboard allow for building/editing models?), T CC(Typo in `/tutorials/mnist/beginners/index.md`)) > [T CC(Dropout Loses Shape Inference Information)]   Op CC(TensorFlow session.run() overhead for graphs with few flops) CAST(T CC(Dropout Loses Shape Inference Information)) > [T CC(Incorrect matrix in beginner tutorial on www.tensorflow.org)]   Op CC(ImportError: undefined symbol: clock_gettime) FLOOR_MOD(T CC(Incorrect matrix in beginner tutorial on www.tensorflow.org), T CC(g3doc format)) > [T CC(No documentation for Saver class)]   Op CC(Windows Installation?) CAST(T CC(No documentation for Saver class)) > [T CC(Make Python/Numpy include paths configurable)]   Op CC(Cant install, Mac  El Capitan  not a supported wheel) SUB(T CC(Make Python/Numpy include paths configurable), T CC( build failed!  File ""/usr/lib/python2.7/encodings/__init__.py"", line 123       raise CodecRegistryError,\)) > [T CC(Extra exp in softmax formula?)]   Op CC(tools/jdk: BUILD file not found on package path.) MUL(T CC(Extra exp in softmax formula?), T CC(tensorflow0.5.0cp27nonelinux_x86_64.whl is not a supported wheel on this platform.)) > [T CC(No model directory)]   Op CC(Can anyone install it with cuda7.5 and cudnn 7.0?) MUL(T CC(No model directory), T CC(Unable to run tensorboard)) > [T CC(when install from sources, I encounter ERROR: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed)]   Op CC(cpu version Installed successfully, but cannot import tensorflow in python.) SUB(T CC(when install from sources, I encounter ERROR: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed), T CC(Fix when installation on OSX)) > [T CC(Unrecognized option: data_dir)]   Op CC(cannot use bazel to compile tensorflow example codes) ADD(T CC(Unrecognized option: data_dir), T CC(Fix when installation on OSX)) > [T CC(ImportError: No module named copyreg)]   Op CC(Lots of C++ compiletime warnings) MUL(T CC(ImportError: No module named copyreg), T CC(Greedy heuristics may not find the optimal node placement)) > [T CC(Building from source for BSD)]   Op CC(Update docs to instruct to use Bazel 0.1.1 installer) RESHAPE(T CC(Building from source for BSD), T CC(Pretrained models)[2, 1, 512, 2]) > [T CC(g3doc tutorial mnist.py tf.range missing argument)]   Op CC(Use Bazel 0.1.1) SPLIT(T CC(Installation over pip fails to import with protobuf 2.6.1)[1], T CC(g3doc tutorial mnist.py tf.range missing argument)) > [T CC(Maybe the website in other languages?), T CC(Padding type definition is swapped in the documentation.)]   Op CC(CUDA 7.0 is hardcoded in `configure` script for Linux) FLOOR(T CC(Maybe the website in other languages?)) > [T CC(Small typo in Beginners MNIST Tutorial)]   Op CC(Communicating channels: gitter.im + discourse) MINIMUM(T CC(Small typo in Beginners MNIST Tutorial), T CC(Installing from source  problem with bazel)) > [T CC(cumulative longer epochs)]   Op CC(Linux installation problem with VirtualEnv) RELU(T CC(cumulative longer epochs)) > [T CC(a Makefile would be supremely helpful)]   Op CC(Linux installation issue for GPUenabled version) ADD(T CC(a Makefile would be supremely helpful), T CC(Fix when installation on OSX)) > [T CC(Broken link on beginner's tutorial page)]   Op CC(osx 10.11 installation issues) MINIMUM(T CC(Broken link on beginner's tutorial page), T CC(Installing from source  problem with bazel)) > [T CC(How To doc for tensor indexing and assigning ops)]   Op CC(ResourceExhaustedError in CNN/MNIST example (with GPU)) RELU(T CC(How To doc for tensor indexing and assigning ops)) > [T CC(Better error message for tf.assign)]   Op CC(bazel always redownloaded the dependency libraries) SUB(T CC(Maybe the website in other languages?), T CC(a Makefile would be supremely helpful)) > [T CC(Promote usage of docker above all other installation methods)]   Op CC(Out of Memory in mnist?) SUB(T CC(Fix when installation on OSX), T CC(Promote usage of docker above all other installation methods)) > [T CC(Using 3d Input for seq2seq Models  Word Vector Input)]   Op CC(unable to use nn.moments when the dimension of the axis is None) FLOOR(T CC(Padding type definition is swapped in the documentation.)) > [T CC(Remove unnecessary null pointer checks)]   Op CC(Questions about using LSTM ) MINIMUM(T CC(Remove unnecessary null pointer checks), T CC(Installing from source  problem with bazel)) > [T CC(Generalize slicing and slice assignment ops (including gather and scatter))]   Op CC(typo in decaying the learning rate example) RELU(T CC(Generalize slicing and slice assignment ops (including gather and scatter))) > [T CC(confused by device placement on Amazon AWS)]   Op CC(TF not compatible with AWS GPU instances?) ADD(T CC(confused by device placement on Amazon AWS), T CC(Fix when installation on OSX)) > [T CC(Symbolic loops (like ""scan"" in Theano))]   Op CC(Truncated backdrop with max pooling over time) MINIMUM(T CC(Symbolic loops (like ""scan"" in Theano)), T CC(Installing from source  problem with bazel)) > [T CC(Yann LeCun's web page is not available  cannot download data from there is there a mirror?)]   Op CC(tutorial GPU issue) RELU(T CC(Yann LeCun's web page is not available  cannot download data from there is there a mirror?)) > [T CC(feature request: softmax target axes / multidimensional softmax)]   Op CC(unable to install Inside the virtualenv, install TensorFlow:) CONCATENATION(T CC(feature request: softmax target axes / multidimensional softmax), T CC(a Makefile would be supremely helpful)) > [T CC(Add gpu support for LRN)]   Op CC(Wrong multiplication in MNIST beginner tutorial) CAST(T CC(Add gpu support for LRN)) > [T CC(Multiple models in one session)]   Op CC(add multiplemachine support) CONCATENATION(T CC(JVM, .NET Language Support), T CC(Multiple models in one session)) > [T CC(tensorflow howto for sharing variable is confusing)]   Op CC(Wrong link in the ""common problems"" docs) GATHER_ND(T CC(Can anyone install it with cuda7.5 and cudnn 7.0?), T CC(tensorflow howto for sharing variable is confusing)) > [T CC(Beam Search)]   Op CC(Official Tensorflow Docker Image) CONCATENATION(T CC(feature request: softmax target axes / multidimensional softmax), T CC(Better error message for tf.assign)) > [T CC(Question:  Example on how can TensorFlow be used for Text classification?)]   Op CC(Is there 3D ConvNets support ? ) CAST(T CC(Question:  Example on how can TensorFlow be used for Text classification?)) > [T CC(MatMul Broadcasting / tensordot)]   Op CC(Typo in reshape documentation) CONCATENATION(T CC(JVM, .NET Language Support), T CC(MatMul Broadcasting / tensordot)) > [T CC(Is pep8 compatibility necessary right now for tensorflow?)]   Op CC(Segmentation fault when GPUs are already used) GATHER_ND(T CC(Can anyone install it with cuda7.5 and cudnn 7.0?), T CC(Is pep8 compatibility necessary right now for tensorflow?)) > [T CC(bazel run error)]   Op CC(Library not loaded: /usr/lib/libc++.1.dylib) CONCATENATION(T CC(confused by device placement on Amazon AWS), T CC(a Makefile would be supremely helpful)) > [T CC(ImportError:No module named setuptools after exectued the step for bazelbin)]   Op CC(Changes to word2vec_basic.py) CAST(T CC(ImportError:No module named setuptools after exectued the step for bazelbin)) > [T CC(how can I export androidtensorflow into my android stuido)]   Op CC(Complete the loop before returning the words) CONCATENATION(T CC(JVM, .NET Language Support), T CC(how can I export androidtensorflow into my android stuido)) > [T CC(tf.matrix_inverse() is slow compared to numpy.linalg.inv)]   Op CC(RuntimeError: Broken toolchain: cannot link a simple C program) GATHER_ND(T CC(Can anyone install it with cuda7.5 and cudnn 7.0?), T CC(tf.matrix_inverse() is slow compared to numpy.linalg.inv)) > [T CC(Error in api_docs/images/Gather.png )]   Op CC(Change test set in mnist demo to use batches to avoid being OOM (>4GB) on gpu.) CONCATENATION(T CC(confused by device placement on Amazon AWS), T CC(Better error message for tf.assign)) > [T CC(extra cpus not recognized when using docker on windows 10)]   Op CC(Switch int to uint to remove some warnings) CAST(T CC(extra cpus not recognized when using docker on windows 10)) > [T CC(TensorFlow for Jetson TK1 (ARM + Cuda))]   Op CC(tensorflow binary image for ARM architecture) CONCATENATION(T CC(JVM, .NET Language Support), T CC(TensorFlow for Jetson TK1 (ARM + Cuda))) > [T CC(Cannot get TensorBoard example working)]   Op CC(doc for install from source of pip+gpu missing config=cuda and use_gpu) GATHER_ND(T CC(Can anyone install it with cuda7.5 and cudnn 7.0?), T CC(Cannot get TensorBoard example working)) > [T CC(`from tensorflow.g3doc...` is Broken)]   Op CC(Can't install tensorflow on OS X  problem with virtualenv) SUB(T CC(Padding type definition is swapped in the documentation.), T CC(confused by device placement on Amazon AWS)) > [T CC(Compute capability  [T CC(AttributeError in Tensor)]   Op CC(Published Roadmap) MUL(T CC(AttributeError in Tensor), T CC(Beam Search)) > [T CC(from __future__ import division gives error in word2vec_basic when dividing tensors.Just comment out ""from __future__...."")]   Op CC(Direct Native Code / LLVM IR generation / JIT Compilation / Staging / Incremental Computing) MUL(T CC(Compute capability  [T CC(document error)]   Op CC(in_top_k op does not work with int64 labels) MUL(T CC(document error), T CC(bazel run error)) > [T CC(cannot enable peer access from device ordinal 0 to device ordinal 1)]   Op CC(Does TensorFlow support temporal convolution) SUB(T CC(Fix when installation on OSX), T CC(Compute capability  [T CC(NameError: name 'init' is not defined)]   Op CC(g3doc is not installed when using pip ) MUL(T CC(NameError: name 'init' is not defined), T CC(Using 3d Input for seq2seq Models  Word Vector Input)) > [T CC(Need a way to ask users what version of tensorflow they are running)]   Op CC(tensor flow does not support operator.__truediv__) MUL(T CC(Need a way to ask users what version of tensorflow they are running), T CC(Error in api_docs/images/Gather.png )) > [T CC(translate example is missing '' in its 'run' command)]   Op CC(Small typo in Deep MNIST Tutorial) ADD(T CC(translate example is missing '' in its 'run' command), T CC(from __future__ import division gives error in word2vec_basic when dividing tensors.Just comment out ""from __future__...."")) > [T CC(the tutorial Sequence to Sequence Models has errors)]   Op CC(Not a gzipped file) ADD(T CC(the tutorial Sequence to Sequence Models has errors), T CC(cannot enable peer access from device ordinal 0 to device ordinal 1)) > [T CC(Getting Started variable name mismatch)]   Op CC(Tensorboard creates unecessary loops in graph) MUL(T CC(NameError: name 'init' is not defined), T CC(Promote usage of docker above all other installation methods)) > [T CC(Error when running code from seq2seq translate model)]   Op CC(Wrong Logistic Loss) MUL(T CC(Error when running code from seq2seq translate model), T CC(`from tensorflow.g3doc...` is Broken)) > [T CC(Empty input to conv2d causes floating point exception)]   Op CC(Unable to restore trained models on the enfr translate model:  tensorflow.python.framework.errors.NotFoundError: Tensor name ""embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Matrix"" not found) ADD(T CC(Getting Started variable name mismatch), T CC(Empty input to conv2d causes floating point exception)) > [T CC(Basic Usage  Variables: incorrect variable name)]   Op CC(run from script only instead of compiling with bazel?) GREATER(T CC(Maybe the website in other languages?), T CC(Installing from source  problem with bazel)) > [T CC(Mistake in matrix multiplication(http://tensorflow.org/tutorials/mnist/beginners/index.md))]   Op CC(einsumlike function?) GREATER(T CC(Padding type definition is swapped in the documentation.), T CC(Installing from source  problem with bazel)) > [T CC(Minor change for read consistency. (Highlight MD Fix))]   Op CC(Split a tensor with Tensor 0D int32 as num_split argument) LESS(T CC(Maybe the website in other languages?), T CC(Neural Translation Model example fails due to missing EN tokens )) > [T CC(Minor change for tutorial.)]   Op CC(ImportError: /lib64/libc.so.6: version `GLIBC_2.17') LOGICAL_OR(T CC(Minor change for tutorial.), T CC(Mistake in matrix multiplication(http://tensorflow.org/tutorials/mnist/beginners/index.md))) > [T CC(Wheel for binary installation is outdated; consider automating release process)]   Op CC(Installation error) LESS(T CC(Padding type definition is swapped in the documentation.), T CC(Neural Translation Model example fails due to missing EN tokens )) > [T CC(Example word2vec.py)]   Op CC(Typo in TensorBoard: Visualizing Learning docs) LOGICAL_OR(T CC(Example word2vec.py), T CC(Minor change for read consistency. (Highlight MD Fix))) > [T CC(word2vec tutorial plot labels are incorrect)]   Op CC('state' is not defined ) LOGICAL_OR(T CC(Wheel for binary installation is outdated; consider automating release process), T CC(word2vec tutorial plot labels are incorrect)) > [T CC(Examples for loop control flow ops (Enter/Leave/NextIteration))]   Op CC(Typo in tf.Graph.name_scope(name) docs) SELECT_V2(T CC(Examples for loop control flow ops (Enter/Leave/NextIteration)), T CC(Neural Translation Model example fails due to missing EN tokens ), T CC(Basic Usage  Variables: incorrect variable name)) > [T CC(translate module not present in binary pip installation)]   Op CC(The icon is assymetric) TRANSPOSE(T CC(translate module not present in binary pip installation), T CC(Remote worker configuration)[0, 3, 1, 2]) > [T CC(typo in placeholder docs)]   Op CC(does tensorboard allow for building/editing models?) RESHAPE(T CC(typo in placeholder docs), T CC(Distributed Version)[2, 256, 512]) > [T CC(Documentation: 'typo' in softmax explanation image)]   Op CC(Dropout Loses Shape Inference Information) MUL(T CC(typo in placeholder docs), T CC(typo in placeholder docs)) > [T CC(Outofsource builds)]   Op CC(Incorrect matrix in beginner tutorial on www.tensorflow.org) RESHAPE(T CC(Outofsource builds), T CC(Distributed Version)[2, 256, 512]) > [T CC(Install tensorflow from source )]   Op CC(No documentation for Saver class) SUM(T CC(Install tensorflow from source ), T CC(Java interface)[1]) > [T CC(Make TensorFlow compatible with PyPy)]   Op CC(Make Python/Numpy include paths configurable) SQRT(T CC(Make TensorFlow compatible with PyPy)) > [T CC(Need help, how to choose one column of a tensor)]   Op CC(Extra exp in softmax formula?) MAXIMUM(T CC(Need help, how to choose one column of a tensor), T CC(MAC pip install operation not permitted)) > [T CC(Tensorflow on Raspberry Pi)]   Op CC(No model directory) MUL(T CC(Tensorflow on Raspberry Pi), T CC(Abandon gerrit and use github for everything)) > [T CC(CUDA_ERROR_NO_DEVICE)]   Op CC(when install from sources, I encounter ERROR: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed) DIV(T CC(Documentation: 'typo' in softmax explanation image), T CC(CUDA_ERROR_NO_DEVICE)) > [T CC(Transpose convolution layer for tensorflow (was deconvolution))]   Op CC(Unrecognized option: data_dir) TRANSPOSE(T CC(Transpose convolution layer for tensorflow (was deconvolution)), T CC(Typo in getting started guide)[0, 2, 1]) > [T CC(error in computation graph tutorials)] Tensors of Subgraph CC(未找到相关数据)   T CC(未找到相关数据)(images) shape:[2, 512, 512, 1], type:FLOAT32   T CC(Add support for Python 3.x)(arith.constant) shape:[64], type:FLOAT16 RO 128 bytes, buffer: 2, data:[??, ??, ??, ??, ??, ...]   T CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"")(arith.constant1) shape:[64, 3, 3, 1], type:FLOAT16 RO 1152 bytes, buffer: 3, data:[??, ??, ??, ??, ??, ...]   T CC(JVM, .NET Language Support)(arith.constant2) shape:[2, 1, 512, 1], type:INT64 RO 8192 bytes, buffer: 4, data:[??, ??, ??, ??, ??, ...]   T CC(Installation over pip fails to import with protobuf 2.6.1)(arith.constant3) shape:[], type:INT32 RO 4 bytes, buffer: 5, data:[1]   T CC(Java interface)(arith.constant4) shape:[1], type:INT32 RO 4 bytes, buffer: 6, data:[1]   T CC(Pretrained models)(arith.constant5) shape:[4], type:INT32 RO 16 bytes, buffer: 7, data:[2, 1, 512, 2]   T CC(API docs does not list RNNs)(arith.constant6) shape:[], type:INT32 RO 4 bytes, buffer: 8, data:[0]   T CC(Setting lower gcc version for cuda)(arith.constant7) shape:[], type:FLOAT16 RO 2 bytes, buffer: 9, data:[??, ??]   T CC(Typo in getting started guide)(arith.constant8) shape:[3], type:INT32 RO 12 bytes, buffer: 10, data:[0, 2, 1]   T CC(Go API)(arith.constant9) shape:[], type:FLOAT16 RO 2 bytes, buffer: 11, data:[??, ??]   T CC(0.5.0 wheel install on Mac OS X using Homebrew python broken)(arith.constant10) shape:[1], type:INT32 RO 4 bytes, buffer: 12, data:[3]   T CC(Remote worker configuration)(arith.constant11) shape:[4], type:INT32 RO 16 bytes, buffer: 13, data:[0, 3, 1, 2]   T CC([doc] typo)(arith.constant12) shape:[5], type:INT32 RO 20 bytes, buffer: 14, data:[0, 3, 1, 4, 2]   T CC(g3doc format)(arith.constant13) shape:[1, 1, 2], type:INT64 RO 16 bytes, buffer: 15, data:[??, ??, ??, ??, ??, ...]   T CC(Quantized ops?)(arith.constant14) shape:[], type:FLOAT16 RO 2 bytes, buffer: 16, data:[??, ??]   T CC(iOS Support and Example)(arith.constant15) shape:[2, 64, 64, 256], type:FLOAT16 RO 4194304 bytes, buffer: 17, data:[??, ??, ??, ??, ??, ...]   T CC(Windows Support and Documentation)(arith.constant16) shape:[], type:FLOAT16 RO 2 bytes, buffer: 18, data:[??, ??]   T CC(C api)(arith.constant17) shape:[2], type:FLOAT16 RO 4 bytes, buffer: 19, data:[??, ??, ??, ??]   T CC(Swift API)(arith.constant18) shape:[2, 256, 512], type:FLOAT16 RO 524288 bytes, buffer: 20, data:[??, ??, ??, ??, ??, ...]   T CC(CUDA 7.5 fails with pip install and docker (Ubuntu 14.04))(arith.constant19) shape:[], type:FLOAT16 RO 2 bytes, buffer: 21, data:[??, ??]   T CC(GPU_Base dockerfile image not found)(arith.constant20) shape:[], type:INT32 RO 4 bytes, buffer: 22, data:[512]   T CC(OpenCL support)(arith.constant21) shape:[3], type:INT32 RO 12 bytes, buffer: 23, data:[2, 512, 1]   T CC(Distributed Version)(arith.constant22) shape:[3], type:INT32 RO 12 bytes, buffer: 24, data:[2, 256, 512]   T CC(Problems running the image example (Python 2.7.10, PyEnv, Xubuntu 14.04 64bit))(arith.constant23) shape:[5], type:INT32 RO 20 bytes, buffer: 25, data:[2, 8, 8, 64, 64]   T CC(Cuda 3.0?)(arith.constant24) shape:[2], type:INT32 RO 8 bytes, buffer: 26, data:[2, 262144]   T CC(simplify contributing process)(arith.constant25) shape:[2, 4, 512], type:FLOAT16 RO 8192 bytes, buffer: 27, data:[??, ??, ??, ??, ??, ...]   T CC(Warning while creating Session on Mac OS X: can't determine number of CPU cores)(arith.constant26) shape:[2, 512, 4], type:FLOAT16 RO 8192 bytes, buffer: 27, data:[??, ??, ??, ??, ??, ...]   T CC(Could port to OpenCL?)(arith.constant27) shape:[2, 512, 512, 1], type:FLOAT16 RO 1048576 bytes, buffer: 29, data:[??, ??, ??, ??, ??, ...]   T CC(minimum req: Cuda compute capability 3.5)(arith.constant28) shape:[1, 1, 2], type:FLOAT16 RO 4 bytes, buffer: 30, data:[??, ??, ??, ??]   T CC(Go API)(arith.constant29) shape:[1, 1, 2], type:FLOAT16 RO 4 bytes, buffer: 31, data:[??, ??, ??, ??]   T CC(Slack Channel)(arith.constant30) shape:[64, 3, 3, 64], type:FLOAT16 RO 73728 bytes, buffer: 32, data:[??, ??, ??, ??, ??, ...]   T CC(Connectionist Temporal Classification example)(arith.constant31) shape:[64, 3, 3, 64], type:FLOAT16 RO 73728 bytes, buffer: 33, data:[??, ??, ??, ??, ??, ...]   T CC(Error while installing tensorflow using pip on Ubuntu 14.04 32bit system)(arith.constant32) shape:[64, 3, 3, 64], type:FLOAT16 RO 73728 bytes, buffer: 34, data:[??, ??, ??, ??, ??, ...]   T CC(Error when run docker image on Mac OS X 10.11.1)(arith.constant33) shape:[128, 3, 3, 64], type:FLOAT16 RO 147456 bytes, buffer: 35, data:[??, ??, ??, ??, ??, ...]   T CC(Can't run TensorBoard on El Captain)(arith.constant34) shape:[128, 3, 3, 128], type:FLOAT16 RO 294912 bytes, buffer: 36, data:[??, ??, ??, ??, ??, ...]   T CC(No module named tensorflow.python.platform)(arith.constant35) shape:[128, 3, 3, 128], type:FLOAT16 RO 294912 bytes, buffer: 37, data:[??, ??, ??, ??, ??, ...]   T CC(Node.js (JavaScript) Wrapper API)(arith.constant36) shape:[128, 3, 3, 128], type:FLOAT16 RO 294912 bytes, buffer: 38, data:[??, ??, ??, ??, ??, ...]   T CC(Updated links in documentation.)(arith.constant37) shape:[256, 3, 3, 128], type:FLOAT16 RO 589824 bytes, buffer: 39, data:[??, ??, ??, ??, ??, ...]   T CC(is a Python 3 support coming soon ?)(arith.constant38) shape:[65, 1, 1, 256], type:FLOAT16 RO 33280 bytes, buffer: 40, data:[??, ??, ??, ??, ??, ...]   T CC(Error in the Getting started/Variables section of the website)(arith.constant39) shape:[256, 3, 3, 128], type:FLOAT16 RO 589824 bytes, buffer: 41, data:[??, ??, ??, ??, ??, ...]   T CC(Pretrained models)(arith.constant40) shape:[256, 1, 1, 256], type:FLOAT16 RO 131072 bytes, buffer: 42, data:[??, ??, ??, ??, ??, ...]   T CC(Windows support)(arith.constant41) shape:[4], type:INT32 RO 16 bytes, buffer: 43, data:[0, 0, 0, 0]   T CC(Cannot import after installing with pip)(arith.constant42) shape:[4], type:INT32 RO 16 bytes, buffer: 44, data:[0, 0, 0, 64]   T CC(Can't install on El Capitan  probably python again :()(arith.constant43) shape:[4], type:INT32 RO 16 bytes, buffer: 45, data:[1, 1, 1, 1]   T CC(OSX PIP Install: Setup.py missing)(arith.constant44) shape:[2, 4, 512], type:FLOAT16 RO 8192 bytes, buffer: 46, data:[??, ??, ??, ??, ??, ...]   T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")(arith.constant45) shape:[3], type:INT32 RO 12 bytes, buffer: 47, data:[2, 512, 512]   T CC(Integration with blaze ecosystem numba python to llvm compiler?)(arith.constant46) shape:[2, 4, 2], type:INT32 RO 64 bytes, buffer: 48, data:[0, 0, 0, 1, 0, ...]   T CC(Object Detection)(arith.constant47) shape:[2, 4, 2], type:INT32 RO 64 bytes, buffer: 49, data:[0, 508, 0, 509, 0, ...]   T CC(error __init__() got an unexpected keyword argument 'syntax')(arith.constant48) shape:[2, 512, 4], type:FLOAT16 RO 8192 bytes, buffer: 46, data:[??, ??, ??, ??, ??, ...]   T CC(Ruby API)(arith.constant49) shape:[2, 512, 4, 3], type:INT32 RO 49152 bytes, buffer: 51, data:[0, 0, 0, 0, 0, ...]   T CC(Ubuntu ImportError: No module named core.framework.graph_pb2)(arith.constant50) shape:[2, 512, 4, 3], type:INT32 RO 49152 bytes, buffer: 52, data:[0, 0, 508, 0, 0, ...]   T CC(Problem running RNN example)(arith.constant51) shape:[65], type:FLOAT16 RO 130 bytes, buffer: 53, data:[??, ??, ??, ??, ??, ...]   T CC(Can't install on ubuntu 12.04.5 LTS)(arith.constant52) shape:[256], type:FLOAT16 RO 512 bytes, buffer: 54, data:[??, ??, ??, ??, ??, ...]   T CC(Support cuda 7.5 and cudnn 7.0)(arith.constant53) shape:[4], type:INT32 RO 16 bytes, buffer: 55, data:[2, 512, 512, 1]   T CC(Javascript > JavaScript)(arith.constant54) shape:[64], type:FLOAT16 RO 128 bytes, buffer: 56, data:[??, ??, ??, ??, ??, ...]   T CC(pip error: No such file or directory: '/tmp/pip...build/setup.py')(arith.constant55) shape:[64], type:FLOAT16 RO 128 bytes, buffer: 57, data:[??, ??, ??, ??, ??, ...]   T CC(virtualenv python 2.7.6 import tensorflow. TypeError: __init__() got an unexpected keyword argument 'syntax')(arith.constant56) shape:[64], type:FLOAT16 RO 128 bytes, buffer: 58, data:[??, ??, ??, ??, ??, ...]   T CC(No gradient defined for the Reverse op)(arith.constant57) shape:[128], type:FLOAT16 RO 256 bytes, buffer: 59, data:[??, ??, ??, ??, ??, ...]   T CC(Error in final step of installation)(arith.constant58) shape:[128], type:FLOAT16 RO 256 bytes, buffer: 60, data:[??, ??, ??, ??, ??, ...]   T CC(Try to convert to readable latex)(arith.constant59) shape:[128], type:FLOAT16 RO 256 bytes, buffer: 61, data:[??, ??, ??, ??, ??, ...]   T CC(ImportError: No module named core.framework.graph_pb2)(arith.constant60) shape:[128], type:FLOAT16 RO 256 bytes, buffer: 62, data:[??, ??, ??, ??, ??, ...]   T CC(CUDNN error on ""import tensorflow as tf"" for gpu version)(arith.constant61) shape:[256], type:FLOAT16 RO 512 bytes, buffer: 63, data:[??, ??, ??, ??, ??, ...]   T CC(Specify output tensor for ops from python)(arith.constant62) shape:[256], type:FLOAT16 RO 512 bytes, buffer: 64, data:[??, ??, ??, ??, ??, ...]   T CC(Mac: OSError: [Errno 1] Operation not permitted: '/tmp/pipXcfgD6uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six1.4.1py2.7.egginfo')(tfl.dequantize) shape:[256], type:FLOAT32   T CC(FR: Change TensorBoard image interpolation method)(tfl.dequantize1) shape:[256], type:FLOAT32   T CC(compilation error)(tfl.dequantize2) shape:[128], type:FLOAT32   T CC(no such package '//': Error downloading from ijg.org)(tfl.dequantize3) shape:[128], type:FLOAT32   T CC(Typo)(tfl.dequantize4) shape:[128], type:FLOAT32   T CC(models/rnn/ptb not included in pip package)(tfl.dequantize5) shape:[128], type:FLOAT32   T CC(Human Consumable Language Independent DSL)(tfl.dequantize6) shape:[64], type:FLOAT32   T CC(Can't install from source if I don't have a GPU? )(tfl.dequantize7) shape:[64], type:FLOAT32   T CC(Target //tensorflow/tools/pip_package:build_pip_package failed to build on OSX)(tfl.dequantize8) shape:[64], type:FLOAT32   T CC(No plan for official doc of any other languages than English?)(tfl.dequantize9) shape:[256], type:FLOAT32   T CC(Cannot run the android example on Android 5.1.1)(tfl.dequantize10) shape:[65], type:FLOAT32   T CC(No module named copy_reg  Installation Issue)(tfl.dequantize11) shape:[2, 512, 4], type:FLOAT32   T CC(cuDNN v2 (6.5) not available anymore)(tfl.dequantize12) shape:[2, 4, 512], type:FLOAT32   T CC(1st Class Windows Support)(tfl.dequantize13) shape:[256, 1, 1, 256], type:FLOAT32   T CC(Scalar Equation is incorrect ? Documentation)(tfl.dequantize14) shape:[256, 3, 3, 128], type:FLOAT32   T CC(segmentation fault when running convolutional.py)(tfl.dequantize15) shape:[65, 1, 1, 256], type:FLOAT32   T CC(tensorboard gulp  analytics.js missing)(tfl.dequantize16) shape:[256, 3, 3, 128], type:FLOAT32   T CC(Installed from source; unable to import tensorflow)(tfl.dequantize17) shape:[128, 3, 3, 128], type:FLOAT32   T CC(bazel compile error)(tfl.dequantize18) shape:[128, 3, 3, 128], type:FLOAT32   T CC(can't install on ubuntu 12.04)(tfl.dequantize19) shape:[128, 3, 3, 128], type:FLOAT32   T CC(Septation of Generalised DAG / Data Flow Programming Framework and ML Components)(tfl.dequantize20) shape:[128, 3, 3, 64], type:FLOAT32   T CC(C++ API neural net examples)(tfl.dequantize21) shape:[64, 3, 3, 64], type:FLOAT32   T CC(GPU implementations for more ops)(tfl.dequantize22) shape:[64, 3, 3, 64], type:FLOAT32   T CC(fixed link to tutorial and some typos)(tfl.dequantize23) shape:[64, 3, 3, 64], type:FLOAT32   T CC(Unable to run tensorboard)(tfl.dequantize24) shape:[1, 1, 2], type:FLOAT32   T CC(Typo in `/tutorials/mnist/beginners/index.md`)(tfl.dequantize25) shape:[1, 1, 2], type:FLOAT32   T CC(Fix 89)(tfl.dequantize26) shape:[2, 512, 512, 1], type:FLOAT32   T CC(import six.moves.copyreg as copyreg error)(tfl.dequantize27) shape:[2, 512, 4], type:FLOAT32   T CC(do we have plans for java api?)(tfl.dequantize28) shape:[2, 4, 512], type:FLOAT32   T CC( build failed!  File ""/usr/lib/python2.7/encodings/__init__.py"", line 123       raise CodecRegistryError,\)(tfl.dequantize29) shape:[], type:FLOAT32   T CC(Abandon gerrit and use github for everything)(tfl.dequantize30) shape:[2, 256, 512], type:FLOAT32   T CC(Greedy heuristics may not find the optimal node placement)(tfl.dequantize31) shape:[2], type:FLOAT32   T CC(tensorflow0.5.0cp27nonelinux_x86_64.whl is not a supported wheel on this platform.)(tfl.dequantize32) shape:[], type:FLOAT32   T CC(How to extract predictions)(tfl.dequantize33) shape:[2, 64, 64, 256], type:FLOAT32   T CC(MAC pip install operation not permitted)(tfl.dequantize34) shape:[], type:FLOAT32   T CC(Fix when installation on OSX)(tfl.dequantize35) shape:[], type:FLOAT32   T CC(Installing from source  problem with bazel)(tfl.dequantize36) shape:[], type:FLOAT32   T CC(Neural Translation Model example fails due to missing EN tokens )(tfl.dequantize37) shape:[], type:FLOAT32   T CC(Fix 'Fetches' example in basic_usage)(model_26/tf.nn.convolution/convolution) shape:[64, 3, 3, 1], type:FLOAT32   T CC(Alpine Linux: __isnanf: symbol not found )(model_26/tf.nn.relu/Relu;model_26/tf.math.add/Add;model_26/tf.nn.convolution/convolution) shape:[64], type:FLOAT32   T CC(Support for python 3)(model_26/tf.nn.relu/Relu;model_26/tf.math.add/Add;model_26/tf.nn.convolution/convolution1) shape:[2, 512, 512, 64], type:FLOAT32   T CC(Can't build from source?)(model_26/tf.nn.relu_1/Relu;model_26/tf.math.add_1/Add;model_26/tf.nn.convolution_1/convolution) shape:[2, 512, 512, 64], type:FLOAT32   T CC(Ubuntu installation error using pip)(model_26/tf.nn.max_pool2d/MaxPool2d) shape:[2, 256, 256, 64], type:FLOAT32   T CC(Truncated backprop docs are confusing)(model_26/tf.nn.relu_2/Relu;model_26/tf.math.add_2/Add;model_26/tf.nn.convolution_2/convolution) shape:[2, 256, 256, 64], type:FLOAT32   T CC(Building a shared libary)(model_26/tf.nn.relu_3/Relu;model_26/tf.math.add_3/Add;model_26/tf.nn.convolution_3/convolution) shape:[2, 256, 256, 64], type:FLOAT32   T CC( C++ compilation of rule '//tensorflow/python:tf_session_helper' failed)(model_26/tf.nn.max_pool2d_1/MaxPool2d) shape:[2, 128, 128, 64], type:FLOAT32   T CC(Support for Redhat, Centos and many superclusters)(model_26/tf.nn.relu_4/Relu;model_26/tf.math.add_4/Add;model_26/tf.nn.convolution_4/convolution) shape:[2, 128, 128, 128], type:FLOAT32   T CC(configure script hardcodes location of cuda that makes it fail on OSX)(model_26/tf.nn.relu_5/Relu;model_26/tf.math.add_7/Add;model_26/tf.nn.convolution_7/convolution) shape:[2, 128, 128, 128], type:FLOAT32   T CC(EC2 g2.2xlarge: Ignoring gpu device (GRID K520) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.)(model_26/tf.nn.max_pool2d_2/MaxPool2d) shape:[2, 64, 64, 128], type:FLOAT32   T CC(AlexNet with FC layers: backward is very slow?)(model_26/tf.nn.relu_6/Relu;model_26/tf.math.add_8/Add;model_26/tf.nn.convolution_8/convolution) shape:[2, 64, 64, 128], type:FLOAT32   T CC(Matrix multiplication in softmax documentation carried out incorrectly)(model_26/tf.nn.relu_7/Relu;model_26/tf.math.add_9/Add;model_26/tf.nn.convolution_9/convolution) shape:[2, 64, 64, 128], type:FLOAT32   T CC(Incorrect matrix math in tutorial at:  http://www.tensorflow.org/tutorials/mnist/beginners/index.md)(model_26/tf.nn.relu_8/Relu;model_26/tf.math.add_10/Add;model_26/tf.nn.convolution_10/convolution) shape:[2, 64, 64, 256], type:FLOAT32   T CC(When will you have a version of TensorFlow for Win10/8/7?)(model_26/tf.math.add_12/Add;model_26/tf.nn.convolution_12/convolution) shape:[2, 64, 64, 65], type:FLOAT32   T CC(TensorBoard logdir path, if relative, is relative to $HOME)(model_26/tf.nn.softmax_1/wa/extractor/Softmax) shape:[2, 64, 64, 65], type:FLOAT32   T CC(""help wanted"" I cannot get the TensorBoard working, I am following the given tutorial)(model_26/tf.nn.relu_9/Relu;model_26/tf.math.add_11/Add;model_26/tf.nn.convolution_11/convolution) shape:[2, 64, 64, 256], type:FLOAT32   T CC(Bazel can't build protobuf)(model_26/tf.math.add_13/Add;model_26/tf.nn.convolution_13/convolution) shape:[2, 64, 64, 256], type:FLOAT32   T CC(TensorFlow session.run() overhead for graphs with few flops)(model_26/tf.compat.v1.norm_1/norm/mul) shape:[2, 64, 64, 256], type:FLOAT32   T CC(ImportError: undefined symbol: clock_gettime)(model_26/tf.compat.v1.norm_1/norm/Sum) shape:[2, 64, 64, 1], type:FLOAT32   T CC(Windows Installation?)(model_26/tf.compat.v1.norm_1/norm/Sqrt) shape:[2, 64, 64, 1], type:FLOAT32   T CC(Cant install, Mac  El Capitan  not a supported wheel)(model_26/tf.math.maximum/Maximum) shape:[2, 64, 64, 1], type:FLOAT32   T CC(tools/jdk: BUILD file not found on package path.)(model_26/tf.math.multiply/Mul) shape:[2, 64, 64, 256], type:FLOAT32   T CC(Can anyone install it with cuda7.5 and cudnn 7.0?)(model_26/tf.math.divide/truediv) shape:[2, 64, 64, 256], type:FLOAT32   T CC(cpu version Installed successfully, but cannot import tensorflow in python.)(model_26/tf.strided_slice/StridedSlice) shape:[2, 64, 64, 64], type:FLOAT32   T CC(cannot use bazel to compile tensorflow example codes)(model_26/tf.compat.v1.transpose_3/transpose) shape:[2, 64, 64, 64], type:FLOAT32   T CC(Lots of C++ compiletime warnings)(model_26/tf.reshape_2/Reshape) shape:[2, 8, 8, 64, 64], type:FLOAT32   T CC(Update docs to instruct to use Bazel 0.1.1 installer)(model_26/tf.compat.v1.transpose_6/transpose) shape:[2, 64, 8, 64, 8], type:FLOAT32   T CC(Use Bazel 0.1.1)(model_26/tf.compat.v1.transpose_12/transpose) shape:[2, 512, 512, 1], type:FLOAT32   T CC(CUDA 7.0 is hardcoded in `configure` script for Linux)(model_26/tf.compat.v1.nn.pool/max_pool) shape:[2, 512, 512, 1], type:FLOAT32   T CC(Communicating channels: gitter.im + discourse)(model_26/tf.math.equal/Equal) shape:[2, 512, 512, 1], type:BOOL   T CC(Linux installation problem with VirtualEnv)(model_26/tf.cast/Cast) shape:[2, 512, 512, 1], type:FLOAT32   T CC(Linux installation issue for GPUenabled version)(model_26/tf.compat.v1.nn.pool_1/max_pool) shape:[2, 512, 512, 1], type:FLOAT32   T CC(osx 10.11 installation issues)(model_26/tf.cast_1/Cast) shape:[2, 512, 512, 1], type:BOOL   T CC(ResourceExhaustedError in CNN/MNIST example (with GPU))(model_26/tf.math.logical_not/LogicalNot) shape:[2, 512, 512, 1], type:BOOL   T CC(bazel always redownloaded the dependency libraries)(model_26/tf.where/SelectV2) shape:[2, 512, 512, 1], type:FLOAT32   T CC(Out of Memory in mnist?)(model_26/tf.compat.v1.nn.pool_2/max_pool) shape:[2, 512, 512, 1], type:FLOAT32   T CC(unable to use nn.moments when the dimension of the axis is None)(model_26/tf.math.equal_1/Equal) shape:[2, 512, 512, 1], type:BOOL   T CC(Questions about using LSTM )(model_26/tf.math.logical_and/LogicalAnd) shape:[2, 512, 512, 1], type:BOOL   T CC(typo in decaying the learning rate example)(model_26/tf.math.logical_or/LogicalOr) shape:[2, 512, 512, 1], type:BOOL   T CC(TF not compatible with AWS GPU instances?)(model_26/tf.cast_2/Cast) shape:[2, 512, 512, 1], type:FLOAT32   T CC(Truncated backdrop with max pooling over time)(model_26/tf.compat.v1.nn.pool_3/max_pool) shape:[2, 512, 512, 1], type:FLOAT32   T CC(tutorial GPU issue)(model_26/tf.cast_3/Cast) shape:[2, 512, 512, 1], type:BOOL   T CC(unable to install Inside the virtualenv, install TensorFlow:)(model_26/tf.math.logical_not_1/LogicalNot) shape:[2, 512, 512, 1], type:BOOL   T CC(Wrong multiplication in MNIST beginner tutorial)(model_26/tf.where_1/SelectV2) shape:[2, 512, 512, 1], type:FLOAT32   T CC(add multiplemachine support)(model_26/tf.compat.v1.nn.pool_4/max_pool) shape:[2, 512, 512, 1], type:FLOAT32   T CC(Wrong link in the ""common problems"" docs)(model_26/tf.math.equal_2/Equal) shape:[2, 512, 512, 1], type:BOOL   T CC(Official Tensorflow Docker Image)(model_26/tf.math.logical_and_1/LogicalAnd) shape:[2, 512, 512, 1], type:BOOL   T CC(Is there 3D ConvNets support ? )(model_26/tf.math.logical_or_1/LogicalOr) shape:[2, 512, 512, 1], type:BOOL   T CC(Typo in reshape documentation)(model_26/tf.where_2/SelectV2) shape:[2, 512, 512, 1], type:FLOAT32   T CC(Segmentation fault when GPUs are already used)(model_26/tf.compat.v1.gather/GatherV2;model_26/tf.compat.v1.gather/GatherV2/axis) shape:[2, 512, 512], type:FLOAT32   T CC(Library not loaded: /usr/lib/libc++.1.dylib)(model_26/tf.compat.v1.transpose_17/transpose) shape:[2, 512, 512], type:FLOAT32   T CC(Changes to word2vec_basic.py)(model_26/tf.compat.v1.transpose_17/transpose1) shape:[2, 512, 512], type:FLOAT32   T CC(Complete the loop before returning the words)(model_26/tf.tensor_scatter_nd_update/TensorScatterUpdate) shape:[2, 512, 512], type:FLOAT32   T CC(RuntimeError: Broken toolchain: cannot link a simple C program)(model_26/tf.tensor_scatter_nd_update/TensorScatterUpdate1) shape:[2, 512, 512], type:FLOAT32   T CC(Change test set in mnist demo to use batches to avoid being OOM (>4GB) on gpu.)(model_26/tf.compat.v1.transpose_17/transpose2) shape:[2, 512, 512], type:FLOAT32   T CC(Switch int to uint to remove some warnings)(model_26/tf.tensor_scatter_nd_update/TensorScatterUpdate2) shape:[2, 512, 512], type:FLOAT32   T CC(tensorflow binary image for ARM architecture)(model_26/tf.compat.v1.transpose_18/transpose) shape:[2, 512, 512], type:FLOAT32   T CC(doc for install from source of pip+gpu missing config=cuda and use_gpu)(model_26/tf.compat.v1.transpose_18/transpose1) shape:[2, 512, 512], type:FLOAT32   T CC(Can't install tensorflow on OS X  problem with virtualenv)(model_26/tf.tensor_scatter_nd_update_1/TensorScatterUpdate) shape:[2, 512, 512], type:FLOAT32   T CC(Any Roadmap Available?)(model_26/tf.tensor_scatter_nd_update_1/TensorScatterUpdate1) shape:[2, 512, 512], type:FLOAT32   T CC(Published Roadmap)(model_26/tf.compat.v1.transpose_18/transpose2) shape:[2, 512, 512], type:FLOAT32   T CC(Direct Native Code / LLVM IR generation / JIT Compilation / Staging / Incremental Computing)(model_26/tf.tensor_scatter_nd_update_1/TensorScatterUpdate2) shape:[2, 512, 512], type:FLOAT32   T CC(in_top_k op does not work with int64 labels)(model_26/tf.compat.v1.transpose_19/transpose) shape:[2, 512, 512], type:FLOAT32   T CC(Does TensorFlow support temporal convolution)(model_26/tf.compat.v1.transpose_19/transpose1) shape:[2, 512, 512], type:FLOAT32   T CC(g3doc is not installed when using pip )(model_26/tf.tensor_scatter_nd_update_2/TensorScatterUpdate) shape:[2, 512, 512], type:FLOAT32   T CC(tensor flow does not support operator.__truediv__)(model_26/tf.tensor_scatter_nd_update_2/TensorScatterUpdate1) shape:[2, 512, 512], type:FLOAT32   T CC(Small typo in Deep MNIST Tutorial)(model_26/tf.compat.v1.transpose_19/transpose2) shape:[2, 512, 512], type:FLOAT32   T CC(Not a gzipped file)(model_26/tf.tensor_scatter_nd_update_2/TensorScatterUpdate2) shape:[2, 512, 512], type:FLOAT32   T CC(Tensorboard creates unecessary loops in graph)(model_26/tf.compat.v1.transpose_20/transpose) shape:[2, 512, 512], type:FLOAT32   T CC(Wrong Logistic Loss)(model_26/tf.compat.v1.transpose_20/transpose1) shape:[2, 512, 512], type:FLOAT32   T CC(Unable to restore trained models on the enfr translate model:  tensorflow.python.framework.errors.NotFoundError: Tensor name ""embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Matrix"" not found)(model_26/tf.tensor_scatter_nd_update_3/TensorScatterUpdate) shape:[2, 512, 512], type:FLOAT32   T CC(run from script only instead of compiling with bazel?)(model_26/tf.tensor_scatter_nd_update_3/TensorScatterUpdate1) shape:[2, 512, 512], type:FLOAT32   T CC(einsumlike function?)(model_26/tf.compat.v1.transpose_20/transpose2) shape:[2, 512, 512], type:FLOAT32   T CC(Split a tensor with Tensor 0D int32 as num_split argument)(model_26/tf.tensor_scatter_nd_update_3/TensorScatterUpdate2) shape:[2, 512, 512], type:FLOAT32   T CC(ImportError: /lib64/libc.so.6: version `GLIBC_2.17')(model_26/tf.compat.v1.transpose_23/transpose) shape:[2, 512, 512], type:FLOAT32   T CC(Installation error)(model_26/tf.reshape_9/Reshape) shape:[2, 262144], type:FLOAT32   T CC(Typo in TensorBoard: Visualizing Learning docs)(model_26/tf.math.top_k/TopKV2) shape:[2, 512], type:FLOAT32   T CC('state' is not defined )(model_26/tf.math.top_k/TopKV21) shape:[2, 512], type:INT32   T CC(Typo in tf.Graph.name_scope(name) docs)(model_26/tf.cast_4/Cast) shape:[2, 512], type:INT64   T CC(The icon is assymetric)(model_26/tf.reshape_10/Reshape) shape:[2, 512, 1], type:INT64   T CC(does tensorboard allow for building/editing models?)(model_26/tf.cast_5/Cast) shape:[2, 512, 1], type:FLOAT32   T CC(Dropout Loses Shape Inference Information)(model_26/tf.math.divide_1/truediv) shape:[2, 512, 2], type:FLOAT32   T CC(Incorrect matrix in beginner tutorial on www.tensorflow.org)(model_26/tf.cast_6/Cast) shape:[2, 512, 2], type:INT64   T CC(No documentation for Saver class)(Identity) shape:[2, 512, 2], type:INT64   T CC(Make Python/Numpy include paths configurable)(model_26/tf.cast_7/Cast) shape:[2, 512, 2], type:FLOAT32   T CC(Extra exp in softmax formula?)(model_26/tf.math.subtract_2/Sub) shape:[2, 512, 2], type:FLOAT32   T CC(No model directory)(model_26/tf.math.multiply_10/Mul) shape:[2, 512, 2], type:FLOAT32   T CC(when install from sources, I encounter ERROR: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed)(model_26/tf.math.divide_2/truediv) shape:[2, 512, 2], type:FLOAT32   T CC(Unrecognized option: data_dir)(model_26/tf.math.subtract_5/Sub) shape:[2, 512, 2], type:FLOAT32   T CC(ImportError: No module named copyreg)(model_26/tf.__operators__.add_7/AddV2;model_26/tf.reshape_11/Reshape) shape:[2, 512, 2], type:FLOAT32   T CC(Building from source for BSD)(model_26/tf.math.multiply_22/Mul;model_26/tf.__operators__.add_7/AddV2;model_26/tf.reshape_11/Reshape) shape:[2, 512, 2], type:FLOAT32   T CC(g3doc tutorial mnist.py tf.range missing argument)(model_26/tf.math.multiply_22/Mul;model_26/tf.__operators__.add_7/AddV2;model_26/tf.reshape_11/Reshape1) shape:[2, 1, 512, 2], type:FLOAT32   T CC(Maybe the website in other languages?)(model_26/tf.split_1/split) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Padding type definition is swapped in the documentation.)(model_26/tf.split_1/split1) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Small typo in Beginners MNIST Tutorial)(model_26/tf.math.floor_2/Floor) shape:[2, 1, 512, 1], type:FLOAT32   T CC(cumulative longer epochs)(model_26/tf.clip_by_value_4/clip_by_value/Minimum) shape:[2, 1, 512, 1], type:FLOAT32   T CC(a Makefile would be supremely helpful)(model_26/tf.clip_by_value_4/clip_by_value) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Broken link on beginner's tutorial page)(model_26/tf.__operators__.add_8/AddV2) shape:[2, 1, 512, 1], type:FLOAT32   T CC(How To doc for tensor indexing and assigning ops)(model_26/tf.clip_by_value_6/clip_by_value/Minimum) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Better error message for tf.assign)(model_26/tf.clip_by_value_6/clip_by_value) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Promote usage of docker above all other installation methods)(model_26/tf.math.subtract_12/Sub) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Using 3d Input for seq2seq Models  Word Vector Input)(model_26/tf.math.subtract_15/Sub) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Remove unnecessary null pointer checks)(model_26/tf.math.floor_3/Floor) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Generalize slicing and slice assignment ops (including gather and scatter))(model_26/tf.clip_by_value_5/clip_by_value/Minimum) shape:[2, 1, 512, 1], type:FLOAT32   T CC(confused by device placement on Amazon AWS)(model_26/tf.clip_by_value_5/clip_by_value) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Symbolic loops (like ""scan"" in Theano))(model_26/tf.__operators__.add_9/AddV2) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Yann LeCun's web page is not available  cannot download data from there is there a mirror?)(model_26/tf.clip_by_value_7/clip_by_value/Minimum) shape:[2, 1, 512, 1], type:FLOAT32   T CC(feature request: softmax target axes / multidimensional softmax)(model_26/tf.clip_by_value_7/clip_by_value) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Add gpu support for LRN)(model_26/tf.concat_5/concat) shape:[2, 1, 512, 2], type:FLOAT32   T CC(Multiple models in one session)(model_26/tf.cast_13/Cast) shape:[2, 1, 512, 2], type:INT64   T CC(tensorflow howto for sharing variable is confusing)(model_26/tf.compat.v1.gather_nd_5/BatchGatherND/concat_3) shape:[2, 1, 512, 3], type:INT64   T CC(Beam Search)(model_26/tf.compat.v1.gather_nd_5/BatchGatherND/GatherNd) shape:[2, 1, 512, 256], type:FLOAT32   T CC(Question:  Example on how can TensorFlow be used for Text classification?)(model_26/tf.concat_6/concat) shape:[2, 1, 512, 2], type:FLOAT32   T CC(MatMul Broadcasting / tensordot)(model_26/tf.cast_14/Cast) shape:[2, 1, 512, 2], type:INT64   T CC(Is pep8 compatibility necessary right now for tensorflow?)(model_26/tf.compat.v1.gather_nd_6/BatchGatherND/concat_3) shape:[2, 1, 512, 3], type:INT64   T CC(bazel run error)(model_26/tf.compat.v1.gather_nd_6/BatchGatherND/GatherNd) shape:[2, 1, 512, 256], type:FLOAT32   T CC(ImportError:No module named setuptools after exectued the step for bazelbin)(model_26/tf.concat_4/concat) shape:[2, 1, 512, 2], type:FLOAT32   T CC(how can I export androidtensorflow into my android stuido)(model_26/tf.cast_12/Cast) shape:[2, 1, 512, 2], type:INT64   T CC(tf.matrix_inverse() is slow compared to numpy.linalg.inv)(model_26/tf.compat.v1.gather_nd_4/BatchGatherND/concat_3) shape:[2, 1, 512, 3], type:INT64   T CC(Error in api_docs/images/Gather.png )(model_26/tf.compat.v1.gather_nd_4/BatchGatherND/GatherNd) shape:[2, 1, 512, 256], type:FLOAT32   T CC(extra cpus not recognized when using docker on windows 10)(model_26/tf.concat_7/concat) shape:[2, 1, 512, 2], type:FLOAT32   T CC(TensorFlow for Jetson TK1 (ARM + Cuda))(model_26/tf.cast_15/Cast) shape:[2, 1, 512, 2], type:INT64   T CC(Cannot get TensorBoard example working)(model_26/tf.compat.v1.gather_nd_7/BatchGatherND/concat_3) shape:[2, 1, 512, 3], type:INT64   T CC(`from tensorflow.g3doc...` is Broken)(model_26/tf.compat.v1.gather_nd_7/BatchGatherND/GatherNd) shape:[2, 1, 512, 256], type:FLOAT32   T CC(Compute capability < 3.5)(model_26/tf.math.subtract_13/Sub) shape:[2, 1, 512, 1], type:FLOAT32   T CC(AttributeError in Tensor)(model_26/tf.math.multiply_24/Mul) shape:[2, 1, 512, 1], type:FLOAT32   T CC(from __future__ import division gives error in word2vec_basic when dividing tensors.Just comment out ""from __future__...."")(model_26/tf.math.multiply_28/Mul) shape:[2, 1, 512, 256], type:FLOAT32   T CC(document error)(model_26/tf.math.multiply_25/Mul) shape:[2, 1, 512, 1], type:FLOAT32   T CC(cannot enable peer access from device ordinal 0 to device ordinal 1)(model_26/tf.math.multiply_29/Mul) shape:[2, 1, 512, 256], type:FLOAT32   T CC(NameError: name 'init' is not defined)(model_26/tf.math.subtract_14/Sub) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Need a way to ask users what version of tensorflow they are running)(model_26/tf.math.multiply_23/Mul) shape:[2, 1, 512, 1], type:FLOAT32   T CC(translate example is missing '' in its 'run' command)(model_26/tf.math.multiply_27/Mul) shape:[2, 1, 512, 256], type:FLOAT32   T CC(the tutorial Sequence to Sequence Models has errors)(model_26/tf.__operators__.add_10/AddV2) shape:[2, 1, 512, 256], type:FLOAT32   T CC(Getting Started variable name mismatch)(model_26/tf.__operators__.add_11/AddV2) shape:[2, 1, 512, 256], type:FLOAT32   T CC(Error when running code from seq2seq translate model)(model_26/tf.math.multiply_26/Mul) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Empty input to conv2d causes floating point exception)(model_26/tf.math.multiply_30/Mul) shape:[2, 1, 512, 256], type:FLOAT32   T CC(Basic Usage  Variables: incorrect variable name)(model_26/tf.__operators__.add_12/AddV2) shape:[2, 1, 512, 256], type:FLOAT32   T CC(Mistake in matrix multiplication(http://tensorflow.org/tutorials/mnist/beginners/index.md))(model_26/tf.math.greater_2/Greater) shape:[2, 1, 512, 1], type:BOOL   T CC(Minor change for read consistency. (Highlight MD Fix))(model_26/tf.math.greater_3/Greater) shape:[2, 1, 512, 1], type:BOOL   T CC(Minor change for tutorial.)(model_26/tf.math.less_2/Less) shape:[2, 1, 512, 1], type:BOOL   T CC(Wheel for binary installation is outdated; consider automating release process)(model_26/tf.math.logical_or_5/LogicalOr) shape:[2, 1, 512, 1], type:BOOL   T CC(Example word2vec.py)(model_26/tf.math.less_3/Less) shape:[2, 1, 512, 1], type:BOOL   T CC(word2vec tutorial plot labels are incorrect)(model_26/tf.math.logical_or_6/LogicalOr) shape:[2, 1, 512, 1], type:BOOL   T CC(Examples for loop control flow ops (Enter/Leave/NextIteration))(model_26/tf.math.logical_or_7/LogicalOr) shape:[2, 1, 512, 1], type:BOOL   T CC(translate module not present in binary pip installation)(model_26/tf.where_4/SelectV2) shape:[2, 1, 512, 256], type:FLOAT32   T CC(typo in placeholder docs)(model_26/tf.compat.v1.transpose_34/transpose) shape:[2, 256, 1, 512], type:FLOAT32   T CC(Documentation: 'typo' in softmax explanation image)(model_26/tf.reshape_14/Reshape) shape:[2, 256, 512], type:FLOAT32   T CC(Outofsource builds)(model_26/tf.compat.v1.norm_3/norm/mul;model_26/tf.reshape_14/Reshape) shape:[2, 256, 1, 512], type:FLOAT32   T CC(Install tensorflow from source )(model_26/tf.compat.v1.norm_3/norm/mul;model_26/tf.reshape_14/Reshape1) shape:[2, 256, 512], type:FLOAT32   T CC(Make TensorFlow compatible with PyPy)(model_26/tf.compat.v1.norm_3/norm/Sum) shape:[2, 1, 512], type:FLOAT32   T CC(Need help, how to choose one column of a tensor)(model_26/tf.compat.v1.norm_3/norm/Sqrt) shape:[2, 1, 512], type:FLOAT32   T CC(Tensorflow on Raspberry Pi)(model_26/tf.math.maximum_1/Maximum) shape:[2, 1, 512], type:FLOAT32   T CC(CUDA_ERROR_NO_DEVICE)(model_26/tf.math.multiply_31/Mul) shape:[2, 256, 512], type:FLOAT32   T CC(Transpose convolution layer for tensorflow (was deconvolution))(model_26/tf.math.divide_3/truediv) shape:[2, 256, 512], type:FLOAT32   T CC(error in computation graph tutorials)(Identity_1) shape:[2, 512, 256], type:FLOAT32 Your model looks compatible with GPU delegate on TFLite runtime version 2.18.0. This does not guarantee that your model will work well with GPU delegate because there could still be runtime incompatibililties.                Model size:    8527712 bytes     Nondata buffer size:      35425 bytes (00.42 %)   Total data buffer size:    8492287 bytes (99.58 %)     (Zero value buffers):    1048598 bytes (12.30 %) * Buffers of TFLite model are mostly used for constant tensors.   And zero value buffers are buffers filled with zeros.   (Consider use `converter._experimental_unfold_large_splat_constant` to save the model size.)   Nondata buffers area are used to store operators, subgraphs and etc.   You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs ```","Hi,   Please take a look into this issue. Thank you.","Hi lite24, what is the ideal behavior you are expecting? I'm trying to understand is this a feature request or did you want the error behavior to behave differently?, Or some combination of both.", I am not sure why the entire delegation has to fail and why the op failure is detected much later. If an op cannot be applied shouldn't the graph just be partitioned there. Also if there is any documentation on the exact nature of tensors all ops support that would be pretty useful as well so that I can account for all of them in my model beforehand. So basically yeah it is a combination of a bug fix or a feature request (the way I understand it currently anyway)
tpu,copybara-service[bot],[XLA:GPU] Fix RaggedAllToAllDecomposer when input and output buffers have different sizes.,"[XLA:GPU] Fix RaggedAllToAllDecomposer when input and output buffers have different sizes. We were doubling the size of the buffer to be able to use dynamicupdateslice, because by HLO semantics, if the update goes out of bound of the result, the update is not applied at all. The correct solution is to pad to `input_size + output_size`.",2025-02-28T18:21:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88350
tpu,jasonleekungfu,Could you kindly update CUDA version in official Tensorflow containers?," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.18.0, 2.19.0rc0  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Can you please update official Tensorflow Docker containers starting version 2.18.0? It is supposed to require CUDA > 12.5. However, the official containers still use 12.3 and does not work. We can technically pull the container and modify. But official containers should include the correct dependencies. Thank you!  Standalone code to reproduce the issue ```shell import tensorflow as tf tf.config.list_physical_devices() ```  Relevant log output ```shell WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1740763678.575642 2218885 gpu_device.cc:2340] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')] ```",2025-02-28T17:31:07Z,type:feature comp:gpu TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/88348,Hi! Just checking. Any updates on this issue? Thank you!
xla compile,copybara-service[bot],[XLA:CPU] Move xla compiled function library from tf2xla,[XLA:CPU] Move xla compiled function library from tf2xla,2025-02-28T16:05:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88345
int8,copybara-service[bot],PR #22788: Implement GetCompiledMemoryStats for GPU AOT executables,"PR CC(TFTRT User provided INT8 quantization scales): Implement GetCompiledMemoryStats for GPU AOT executables Imported from GitHub PR https://github.com/openxla/xla/pull/22788 This implements `GetCompiledMemoryStats` for aheadoftime compiled executables. With this patch, one can estimate memory consumption of a JAX function even without access to a GPU. Unfortunately, the patch duplicates code between unloaded and loaded GPU executables and between `GpuThunkAotCompilationResult::GetBufferAssignment()`  and `Compiler::BufferSizeBytesFunction()`+`GpuCompiler::ShapeSizeBytesFunction()`. This could be perhaps improved by exposing the relevant compiler code as static methods, but that does not seem worth the extra complexity. The patch also threads `pointer_size` from `GpuCompiler` to `GpuThunkAotCompilationResult` so that we can get buffer allocation sizes without direct access to the compiler. Another option would be embedding `pointer_size` within `CompilationResultProto`. Also note that this still does not set `generated_code_size_in_bytes` correctly  that would require duplicating some code from `GpuExecutable::SizeOfGeneratedCodeInBytes()`. Copybara import of the project:  39015570fab3e32bae2a54eb4ae7bf428a3c0e3b by Jaroslav Sevcik : Implement GetCompiledMemoryStats for GPU AOT executables  a3e60c7e7ae724d89ae670c14eb0cee8226b66d4 by Jaroslav Sevcik : Check more stats Merging this change closes CC(TFTRT User provided INT8 quantization scales) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22788 from jarosevcik:memstatsforunloadedexecutable a3e60c7e7ae724d89ae670c14eb0cee8226b66d4",2025-02-28T13:25:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88331
opt,copybara-service[bot],Example QNN accelerator options link.,Example QNN accelerator options link. This is distributed as a part of LiteRt (or vendor specific SDK package). **The header needs to be public.** The implementation can be distributed either as the source or as a precompiled binary.  `set_options_example.cc` is an example of C API use for the high level   bindings. End users are not expected to use this API.,2025-02-28T12:54:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88328
opt,copybara-service[bot],[XLA:CPU] NanoRt exposes ExecutionOptions.,[XLA:CPU] NanoRt exposes ExecutionOptions. This can be used to set the thread pool device. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88412 from tensorflow:dependabot/docker/tensorflow/tools/tf_sig_build_dockerfiles/ubuntued1544e 4b6ba37c2667bdfded35b1774ed8ee438bc5fb25,2025-02-28T11:34:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88325
opt,copybara-service[bot],Rename input format flag option from `input_snapshot_proto_binary` to `unoptimized_snapshot_proto_binary` before it's too late!,"Rename input format flag option from `input_snapshot_proto_binary` to `unoptimized_snapshot_proto_binary` before it's too late! Make it consistent and unified with all other places, it used to be called input snapshots on early stages of development.",2025-02-28T11:14:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88324
opt,copybara-service[bot],Templatize C++ options parsing and move to public api.,Templatize C++ options parsing and move to public api.,2025-02-28T04:04:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88290
tpu,amdjebb,error handling in list_devices() not present," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I noticed that there is a lack of error handling in the test_utils.py or device_lib.py to handle improper device query failures.  If _pywrap_device_lib.list_devices() is called when GPU drivers are not updated/missing, it could lead to Tensorflow crashing out without handling the error.  This can also happen for undefined device_lib in gpu_device_name() or if no GPU is detected.   If viable, I would like to be assigned this issue to work on.  Standalone code to reproduce the issue ```shell import tensorflow as tf from tensorflow.python.client import device_lib def list_local_devices():     """"""List the available devices""""""     from tensorflow.core.protobuf import device_attributes_pb2     def _convert(pb_str):         """"""Convert serialized device attribute protobuf to a readable format.""""""         m = device_attributes_pb2.DeviceAttributes()         m.ParseFromString(pb_str)         return m     return [_convert(s) for s in tf.compat.v1.Session().list_devices()]  Might return none def gpu_device_name():     """"""Returns the name of a GPU device""""""     for x in device_lib.list_local_devices():   Might return none         if x.device_type == ""GPU"":             return tf.compat.as_str(x.name)   Might return none     return """" def test_gpu():     """"""Test the GPU functions""""""     devices = list_local_devices()   Where problem might occur     for device in devices:         print(f""Device found: {device}"")     gpu_name = gpu_device_name()   Where problem might occur     print(f""GPU Name: {gpu_name}"") if __name__ == ""__main__"":     test_gpu() ```  Relevant log output ```shell ImportError: cannot import name 'device_attributes_pb2' from 'tensorflow.core.protobuf' ```",2025-02-28T03:11:13Z,type:bug TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/88288,I was able to reproduce the same issue using TensorFlow 2.18 and the nightly version. Please find the gist here for reference. Thank you!
opt,copybara-service[bot],"run_hlo_module, hlo-opt tools : Identify it is a MLIR input based on input filename.","run_hlo_module, hloopt tools : Identify it is a MLIR input based on input filename. fixes issue raised at https://github.com/openxla/xla/pull/22827",2025-02-28T02:04:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88283
opt,copybara-service[bot],Add an option for TfAllocatorAdapter to disallow asynchronous_deallocation,Add an option for TfAllocatorAdapter to disallow asynchronous_deallocation,2025-02-28T02:02:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88282
tpu,copybara-service[bot],Add output_shape() to the xla::Executable base class.,Add output_shape() to the xla::Executable base class.,2025-02-28T01:55:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88281
opt,copybara-service[bot],Add `--verbose_failures` option for building the wheel,Add `verbose_failures` option for building the wheel,2025-02-27T23:25:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88274
tpu,calcuttj,Error regarding nodes colocated with other unknown nodes, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.20  Custom code No  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Can't load model stored in pb file get an error regarding colocated nodes. This happens for some models but not others. Bad file: https://www.dropbox.com/scl/fi/4pmvsihcv4f5eak7bxw5g/cnn_emtrkmichl_pitch_5_wire_48_drift_48_down_6_mean_notes_protoduneBeamAndCosmicsMCC11.pb?rlkey=dj28ktmqmp2kvi9idrnmq0j3e&st=tqft1tdy&dl=0 Good file: https://www.dropbox.com/scl/fi/1q8ace49r2zsbputg3b7q/dune_cvn_hd_2x6_anu_2023_tf26_v03_00_00.pb?rlkey=p3cx01gc0aurs6ok6j8gs1adn&st=2a2up2si&dl=0  Standalone code to reproduce the issue ```shell https://www.dropbox.com/scl/fi/35nt291vp8fnb40is58s8/colocate_error.ipynb?rlkey=2m0tr7tq57lolin8eh61g6ffq&st=x42ktldf&dl=0 ```  Relevant log output ```shell ValueError: Node 'em_trk_none_netout/kernel/read' expects to be colocated with unknown node 'em_trk_none_out/kernel' ```,2025-02-27T19:40:11Z,stat:awaiting response type:bug stale TF 2.18,closed,0,10,https://github.com/tensorflow/tensorflow/issues/88254,"Hi **** , Apologies for the delay, and thanks for raising your concern here. Could you please provide a Colab gist to reproduce the issue? The link you shared is inaccessible to me. Thank you!",My apologies. Here's a colab link https://colab.research.google.com/drive/1FM_B4imfuYHUTlDw1252vnMDaIc3WJpF?usp=sharing please request access,"Hi ****, Could you please provide access to the shared Colab? The link you shared is currently inaccessible to me. To ensure I can access it, please create a Colab gist instead. I am attaching a screenshot for your reference on how to create and share a Colab gist. Thank you!","Apologies, I was unfamiliar with gist. Here's a link to one that should work if you have those files https://gist.github.com/calcuttj/df4c081421e336842fd0f4ffa2283c82 If you're unable to retrieve the input files in the original post, please let me know how I can best provide them",Were you able to use the above gist?,Can I please have an update on this?,"Hi  , Apologies for the delay, and thanks for your patience. I tried running your code on Colab using TensorFlow 2.19.0, but I encountered the following error: ``` NotFoundError: dune_cvn_hd_2x6_anu_2023_tf26_v03_00_00.pb; No such file or directory ``` It seems that the required file is missing or inaccessible. I attempted to download the attached files, but I don't have access to them. Could you please provide access so I can investigate further? Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],Add a test for rewriting BF16 convolutions to OneDNN; remove an `#ifdef`.,"Add a test for rewriting BF16 convolutions to OneDNN; remove an `ifdef`. Allow tests to cover a platformdependent behavior detail regardless of build configuration. In order to support this (and also to improve general code health), replace an `ifdef` with a runtime `if` checking a new member of `AlgebraicSimplifierOptions`.",2025-02-27T19:29:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88251
opt,copybara-service[bot],PR #23019: Bump actions/upload-artifact from 4.6.0 to 4.6.1,"PR CC(`tf.layers.Conv2D`'s padding doesn't allow variable reuse in some cases): Bump actions/uploadartifact from 4.6.0 to 4.6.1 Imported from GitHub PR https://github.com/openxla/xla/pull/23019 Bumps actions/uploadartifact from 4.6.0 to 4.6.1.  Release notes Sourced from actions/uploadartifact's releases.  v4.6.1 What's Changed  Update to use artifact 2.2.2 package by @​yacaovsnc in actions/uploadartifact CC(PoolAlloc: Remove div by zero, demote WARN>INFO)  Full Changelog: https://github.com/actions/uploadartifact/compare/v4...v4.6.1    Commits  4cec3d8 Merge pull request  CC(PoolAlloc: Remove div by zero, demote WARN>INFO) from actions/yacaovsnc/artifact_2.2.2 e9fad96 license cache update for artifact b26fd06 Update to use artifact 2.2.2 package See full diff in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)  Copybara import of the project:  7cb47206e44222d93a2c1efc5525c31190e046b1 by dependabot[bot] : Bump actions/uploadartifact from 4.6.0 to 4.6.1 Bumps actions/uploadartifact from 4.6.0 to 4.6.1.  Release notes  Commits  updateddependencies:  dependencyname: actions/uploadartifact   dependencytype: direct:production   updatetype: versionupdate:semverpatch ... Signedoffby: dependabot[bot]  Merging this change closes CC(`tf.layers.Conv2D`'s padding doesn't allow variable reuse in some cases) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23019 from openxla:dependabot/github_actions/actions/uploadartifact4.6.1 7cb47206e44222d93a2c1efc5525c31190e046b1",2025-02-27T19:24:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88250
gemma,copybara-service[bot],[XLA] Remove extraneous `gemma2_2b_keras_jax.hlo` argument in wget.,[XLA] Remove extraneous `gemma2_2b_keras_jax.hlo` argument in wget.,2025-02-27T19:18:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88249
tensorrt,copybara-service[bot],PR #23165: [XLA:GPU] fix case command buffer operator data corruption issue when index is bool type,"PR CC(TensorRT engine binding error): [XLA:GPU] fix case command buffer operator data corruption issue when index is bool type Imported from GitHub PR https://github.com/openxla/xla/pull/23165 XLA misinterprets that the first branch in conditional_thunk's branch vector is true branch, while the 2nd branch is false branch.  Copybara import of the project:  db66863cf131268af020c2347fb7f90c142163a6 by Shawn Wang : fix case command buffer operator fault when index is bool type Merging this change closes CC(TensorRT engine binding error) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23165 from shawnwang18:shawnw/conditional_test db66863cf131268af020c2347fb7f90c142163a6",2025-02-27T15:41:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88232
opt,copybara-service[bot],"[XLA:GPU/TMA] Add TMA optionality and lowering to triton_xla tile, extract, and insert.","[XLA:GPU/TMA] Add TMA optionality and lowering to triton_xla tile, extract, and insert.",2025-02-27T13:43:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88227
tpu,Unknownuserfrommars,Tensorflow Website Out-of-date," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2  Custom code Yes  OS platform and distribution Windows 11 24H2  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In tensorflow.org/install, everything is just so out of date. It started when i'm reading the chinese version of the install page (https://www.tensorflow.org/install?hl=zhcn): which is:   我们在以下 64 位系统上测试过 TensorFlow 并且这些系统支持 TensorFlow： Python 3.6–3.9     See the problem? it said: TensorFlow is tested and supported on the following 64bit systems: Python 3.63.9 But 3.6 is EOS like millions of years ago, so i scrolled down and saw the page last updated date: 20210825. But this may be that the people are lazy to translate them to chinese, so i changed my language to english. Better but it said TF supports Python 3.83.11, which is also not true. The date last updated for this is 20230324, almost two years ago. So, my question is: why is this so out of date? Like this is supposed to be the official website for tensorflow! In the main page (tensorflow.org), it said: TF 2.18 released (which is true), but the API docs version is actually v2.16.1, with a datelastupdated of 20240930. This is just so annoying. When people use a google search on 'Tensorflow', the first thing that they see is going to be the website, not the Github repo itself, so please update it. (Okay, if you're just too lazy to stay updated just post a message on the screen saying ""This page is no longer maintained so see the repo (link to repo)"" something like that)  Standalone code to reproduce the issue ```shell No code. ```  Relevant log output ```shell ```",2025-02-27T13:07:47Z,type:docs-bug stat:awaiting tensorflower type:bug,open,0,1,https://github.com/tensorflow/tensorflow/issues/88226,"The Chinese installation document translation has not been updated for a long time, see CC(Remove or update zhcn translation from installation instructions). "
opt,weilhuan-quic,Qualcomm AI Engine Direct - Op Builders for 1P Models," WHAT 1. Conv2d 2. DepthwiseConv2d 3. AveragePool 4. MaxPool 5. DepthToSpace 6. SpaceToDepth 7. HardSwish 8. LeakyRelu 9. ResizeBilinear 10. Litert options for these op builders, unit test 11. update qnn_compiler_plugin_test with these op builders  TEST  qnn_compiler_plugin_test ``` [] Global test environment teardown [==========] 115 tests from 5 test suites ran. (3489 ms total) [  PASSED  ] 115 tests. ```  litert_options_test ``` [] Global test environment teardown [==========] 22 tests from 1 test suite ran. (0 ms total) [  PASSED  ] 22 tests ```",2025-02-27T10:49:15Z,awaiting review comp:lite ready to pull size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88221
yi,clickbaron,Fix TOSA bytecode conversion and improve code readability," Bug Fix This PR fixes a critical bug in the TOSA conversion pipeline where `experimental_tflite_to_tosa_bytecode()` was trying to call `ExperimentalTFLiteToTosaBytecode` directly instead of through the `_pywrap_mlir` module.  Code Improvements Additionally, this PR improves code readability and reduces complexity by:  Adding a helper function for string encoding  Simplifying parameter defaults  Making code more consistent across functions  Testing Verified the fix resolves the original NameError by importing and using the function. Fixes CC([TOSA] NameError: name 'ExperimentalTFLiteToTosaBytecode' is not defined) ",2025-02-27T02:38:09Z,size:M comp:lite-tosa,closed,0,6,https://github.com/tensorflow/tensorflow/issues/88193,> [!IMPORTANT] > The terms of service for this installation has not been accepted. Please ask the Organization owners to visit the Gemini Code Assist Admin Console to sign it.,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , Can you please sign CLA, Thank you !","hi  , are you still working on this PR? should we close this? ","Oh you can close it because I didn't know g workspaces couldn't be connected  will contribute later on with my Gmail 👍🏻 Founder of CloudTap.com On Mon, Apr 14, 2025 at 2:43 PM JerryGe ***@***.***> wrote: > hi   , are you still working on > this PR? should we close this? > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> > *JerryGe* left a comment (tensorflow/tensorflow CC(Fix TOSA bytecode conversion and improve code readability)) >  > > hi   , are you still working on > this PR? should we close this? > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","Based on the last comment, I'm closing this PR."
opt,copybara-service[bot],[XLA:GPU] Fix post-optimization pipeline parallelism tests,[XLA:GPU] Fix postoptimization pipeline parallelism tests These tests passed earlier (by chance). The underlying issue is the same as for the test fixed in cl/730568729.,2025-02-27T01:14:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88191
opt,copybara-service[bot],[XLA:GPU] Add copy insertion test for directly shifing data with send/recv,[XLA:GPU] Add copy insertion test for directly shifing data with send/recv This test case caused nondeterministic behavior in pipeline parallelism tests. The root cause was invalid postoptimization HLO in the test case. Adding this test case is an improvement either way.,2025-02-27T01:12:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88189
opt,copybara-service[bot],[XLA:GPU] Fix post-optimization pipeline parallelism tests,"[XLA:GPU] Fix postoptimization pipeline parallelism tests The test does not run HLO passes. In particular, copy insertion does not run on this input. This means the input must guarantee nonconflicting live ranges of all buffers. The new copies and control dependencies enforce this guarantee. With HLO passes enabled, this would be enforced by the copy insertion pass.",2025-02-27T01:12:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88188
sharding,copybara-service[bot],Remove parsed_pspec from NamedSharding constructor,Remove parsed_pspec from NamedSharding constructor,2025-02-27T01:03:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88185
opt,copybara-service[bot],PR #23020: Bump github/codeql-action from 3.24.9 to 3.28.10,"PR CC(Error in C++ Tensorflow to load model trained by python): Bump github/codeqlaction from 3.24.9 to 3.28.10 Imported from GitHub PR https://github.com/openxla/xla/pull/23020 Bumps github/codeqlaction from 3.24.9 to 3.28.10.  Release notes Sourced from github/codeqlaction's releases.  v3.28.10 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.10  21 Feb 2025  Update default CodeQL bundle version to 2.20.5.  CC(Please consider adding flatten) Address an issue where the CodeQL Bundle would occasionally fail to decompress on macOS.  CC(Checkpoint Restore blocked by changed default bias variable name)  See the full CHANGELOG.md for more information. v3.28.9 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.9  07 Feb 2025  Update default CodeQL bundle version to 2.20.4.  CC(C++ compilation of rule '//:sip_hash' failed (Tensorflow serving on Android))  See the full CHANGELOG.md for more information. v3.28.8 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.8  29 Jan 2025  Enable support for Kotlin 2.1.10 when running with CodeQL CLI v2.20.3.  CC(Fix for build issue 2742;)  See the full CHANGELOG.md for more information. v3.28.7 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.7  29 Jan 2025 No user facing changes. See the full CHANGELOG.md for more information. v3.28.6 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs.   ... (truncated)   Changelog Sourced from github/codeqlaction's changelog.  CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. [UNRELEASED] No user facing changes. 3.28.10  21 Feb 2025  Update default CodeQL bundle version to 2.20.5.  CC(Please consider adding flatten) Address an issue where the CodeQL Bundle would occasionally fail to decompress on macOS.  CC(Checkpoint Restore blocked by changed default bias variable name)  3.28.9  07 Feb 2025  Update default CodeQL bundle version to 2.20.4.  CC(C++ compilation of rule '//:sip_hash' failed (Tensorflow serving on Android))  3.28.8  29 Jan 2025  Enable support for Kotlin 2.1.10 when running with CodeQL CLI v2.20.3.  CC(Fix for build issue 2742;)  3.28.7  29 Jan 2025 No user facing changes. 3.28.6  27 Jan 2025  Reenable debug artifact upload for CLI versions 2.20.3 or greater.  CC(Modifying MNIST example to distributed version: could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR)  3.28.5  24 Jan 2025  Update default CodeQL bundle version to 2.20.3.  CC(Branch 124290852)  3.28.4  23 Jan 2025 No user facing changes. 3.28.3  22 Jan 2025  Update default CodeQL bundle version to 2.20.2.  CC(Update roadmap.md) Fix an issue downloading the CodeQL Bundle from a GitHub Enterprise Server instance which occurred when the CodeQL Bundle had been synced to the instance using the CodeQL Action sync tool and the Actions runner did not have Zstandard installed.  CC(Branch 124251558) Uploading debug artifacts for CodeQL analysis is temporarily disabled.  CC(Tensorflow with Pyinstaller)  3.28.2  21 Jan 2025 No user facing changes. 3.28.1  10 Jan 2025  CodeQL Action v2 is now deprecated, and is no longer updated or supported. For better performance, improved security, and new features, upgrade to v3. For more information, see this changelog post.  CC(Import error)    ... (truncated)   Commits  b56ba49 Merge pull request  CC(Isn't current tensorflowgit r0.9? ) from github/updatev3.28.109856c48b1 60c9c77 Update changelog for v3.28.10 9856c48 Merge pull request  CC(Segmentation fault on tensorflow 0.9.0) from github/redsun82/rust 9572e09 Rust: fix log string 1a52936 Rust: special case default setup cf7e909 Merge pull request  CC(Please consider adding flatten) from github/updatebundle/codeqlbundlev2.20.5 b7006aa Merge branch 'main' into updatebundle/codeqlbundlev2.20.5 cfedae7 Rust: throw configuration errors if requested and not correctly enabled 3971ed2 Merge branch 'main' into redsun82/rust d38c6e6 Merge pull request  CC(Bazel fail to resolve submodule tensorflow) from github/angelapwen/bumpoctokit Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)  Copybara import of the project:  62ccec95e0caa348151e04d97d944e4bdee3dd58 by dependabot[bot] : Bump github/codeqlaction from 3.24.9 to 3.28.10 Bumps github/codeqlaction from 3.24.9 to 3.28.10.  Release notes  Changelog  Commits  updateddependencies:  dependencyname: github/codeqlaction   dependencytype: direct:production   updatetype: versionupdate:semverminor ... Signedoffby: dependabot[bot]  Merging this change closes CC(Error in C++ Tensorflow to load model trained by python) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23020 from openxla:dependabot/github_actions/github/codeqlaction3.28.10 62ccec95e0caa348151e04d97d944e4bdee3dd58",2025-02-26T23:40:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88180
opt,copybara-service[bot],[xla] Optimize accidental copies in HloEvaluator::Postprocess,[xla] Optimize accidental copies in HloEvaluator::Postprocess,2025-02-26T23:39:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88179
opt,copybara-service[bot],PR #23018: Bump ossf/scorecard-action from 2.3.1 to 2.4.1,"PR CC(.): Bump ossf/scorecardaction from 2.3.1 to 2.4.1 Imported from GitHub PR https://github.com/openxla/xla/pull/23018 Bumps ossf/scorecardaction from 2.3.1 to 2.4.1.  Release notes Sourced from ossf/scorecardaction's releases.  v2.4.1 What's Changed  This update bumps the Scorecard version to the v5.1.1 release. For a complete list of changes, please refer to the v5.1.0 and v5.1.1 release notes. Publishing results now uses half the API quota as before. The exact savings depends on the repository in question.  use Scorecard library entrypoint instead of Cobra hooking by @​spencerschrock in ossf/scorecardaction CC(Need force_gpu_if_available for tests)   Some errors were made into annotations to make them more visible  Make default branch error more prominent by @​jsoref in ossf/scorecardaction CC(partial_run segfault)   There is now an optional file_mode input which controls how repository files are fetched from GitHub. The default is archive, but git produces the most accurate results for repositories with .gitattributes files at the cost of analysis speed.  add input for specifying filemode by @​spencerschrock in ossf/scorecardaction CC(Fix python3 b)   The underlying container for the action is now hosted on GitHub Container Registry. There should be no functional changes.  :seedling: publish docker images to GitHub Container Registry by @​spencerschrock in ossf/scorecardaction CC(Multidimensional RNN)    Docs  Installation docs update by @​JeremiahAHoward in ossf/scorecardaction CC(ci_build  debian jessie)  New Contributors  @​JeremiahAHoward made their first contribution in ossf/scorecardaction CC(ci_build  debian jessie) @​jsoref made their first contribution in ossf/scorecardaction CC(partial_run segfault) Full Changelog: https://github.com/ossf/scorecardaction/compare/v2.4.0...v2.4.1  v2.4.0 What's Changed This update bumps the Scorecard version to the v5 release. For a complete list of changes, please refer to the v5.0.0 release notes. Of special note to Scorecard Action is the Maintainer Annotation feature, which can be used to suppress some Code Scanning false positives. Alerts will not be generated for any Scorecard Check with an annotation.  :seedling: Bump github.com/ossf/scorecard/v5 from v5.0.0rc2 to v5.0.0 by @​spencerschrock in ossf/scorecardaction CC(tensorflor cond(pred, fn1, fn2) evaluate fn1 and fn2 together regardless of pred) :bug: lower license sarif alert threshold to 9 by @​spencerschrock in ossf/scorecardaction CC(Mac OS;import error ""ImportError: cannot import name _message"")  Documentation  docs: dogfooding badge by @​jkowalleck in ossf/scorecardaction CC(Error using tf.image.random._ : 'numpy.ndarray' object has no attribute 'get_shape')  New Contributors  @​jkowalleck made their first contribution in ossf/scorecardaction CC(Error using tf.image.random._ : 'numpy.ndarray' object has no attribute 'get_shape')  Full Changelog: https://github.com/ossf/scorecardaction/compare/v2.3.3...v2.4.0 v2.3.3  [!NOTE] There is no v2.3.2 release as a step was skipped in the release process. This was fixed and rereleased under the v2.3.3 tag  What's Changed  :seedling: Bump github.com/ossf/scorecard/v4 (v4.13.1) to github.com/ossf/scorecard/v5 (v5.0.0rc1) by @​spencerschrock in ossf/scorecardaction CC(Implement the Special Functions incbet,igam,igamc for CPU+GPU for float & double.) :seedling: Bump github.com/ossf/scorecard/v5 from v5.0.0rc1 to v5.0.0rc2 by @​spencerschrock in ossf/scorecardaction CC(Document trick with slash in scope names) :seedling: Bump github.com/ossf/scorecard/v5 from v5.0.0rc2 to v5.0.0rc2.0.202405091827347ce860946928 by @​spencerschrock in ossf/scorecardaction CC(Tensorflow Website : Missing Mathjax results in broken equations)  For a full changelist of what these include, see the v5.0.0rc1 and v5.0.0rc2 release notes. Documentation  :book: Move token discussion out of main README. by @​spencerschrock in ossf/scorecardaction CC(Arch doesn't support it)    ... (truncated)   Commits  f49aabe bump docker to ghcr v2.4.1 ( CC(Hardcoded bash path)) 30a595b :seedling: Bump github.com/sigstore/cosign/v2 from 2.4.2 to 2.4.3 ( CC(rnn.bidirectional_rnn  cause a problem)) 69ae593 omit vcs info from build ( CC(Bugfix to test/run_and_gather_logs.)) 6a62a1c add input for specifying filemode ( CC(Fix python3 b)) 2722664 :seedling: Bump the githubactions group with 2 updates ( CC(Delete useless directory)) ae0ef31 :seedling: Bump github.com/spf13/cobra from 1.8.1 to 1.9.1 ( CC(some learning decays from Stanford CS231n Karpathy lecture 6)) 3676bbc :seedling: Bump golang from 1.23.6 to 1.24.0 in the dockerimages group ( CC(seems issues with softmax_cross_entropy_with_logits)) ae7548a Limit codeQL push trigger to main branch ( CC(quick python3 fix)) 9165624 upgrade scorecard to v5.1.0 ( CC(Fix python3 breakage (oldstyle exception block))) 620fd28 :seedling: Bump the githubactions group with 2 updates ( CC(typos fix and ign temp files in gitignore)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)  Copybara import of the project:  c459d962aa2a508b6d8c33dafe7cbf7ed8e8a034 by dependabot[bot] : Bump ossf/scorecardaction from 2.3.1 to 2.4.1 Bumps ossf/scorecardaction from 2.3.1 to 2.4.1.  Release notes  Changelog  Commits  updateddependencies:  dependencyname: ossf/scorecardaction   dependencytype: direct:production   updatetype: versionupdate:semverminor ... Signedoffby: dependabot[bot]  Merging this change closes CC(.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23018 from openxla:dependabot/github_actions/ossf/scorecardaction2.4.1 c459d962aa2a508b6d8c33dafe7cbf7ed8e8a034",2025-02-26T23:37:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88178
opt,copybara-service[bot],Apply PostQuantize pass after TFL Optimization pipeline to aid constant folding on some quantized ops.,Apply PostQuantize pass after TFL Optimization pipeline to aid constant folding on some quantized ops.,2025-02-26T23:12:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88174
quantization,copybara-service[bot],Fork StrictQuantizationPattern from QuantizationPattern,Fork StrictQuantizationPattern from QuantizationPattern This will allow us to have tflite specific quantization logic which will only depend on quant annotations.,2025-02-26T22:13:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88168
opt,copybara-service[bot],Enable BatchMatMul->FC optimization for i8.,Enable BatchMatMul>FC optimization for i8.,2025-02-26T21:49:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88166
opt,copybara-service[bot],#tf-data Adds `min_parallelism` option to allow advanced users to force set minimum parallelism for autotuning to reduce warm up time,tfdata Adds `min_parallelism` option to allow advanced users to force set minimum parallelism for autotuning to reduce warm up time,2025-02-26T21:47:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88165
gemma,copybara-service[bot],Update cpu_benchmarks.yml and gpu_benchmarks.yml to run the Gemma2-2B HLO.,Update cpu_benchmarks.yml and gpu_benchmarks.yml to run the Gemma22B HLO.,2025-02-26T21:40:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88164
tpu,copybara-service[bot],Add an AllGatherSimplifier pass that detects and removes unnecessary AllGather Ops.,Add an AllGatherSimplifier pass that detects and removes unnecessary AllGather Ops. Examples: * A trivial allgather where the input and output shapes are compatible will be replaced by its operand. * An allgather with a single consumer that is a dynamicslice such that the output of the dynamicslice is the same as the input of the allgather will be replaced by the operand.,2025-02-26T21:31:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88163
int8,copybara-service[bot],Enable optimize_batch_matmul_pass to convert BMM with int8 RHS to FC.,Enable optimize_batch_matmul_pass to convert BMM with int8 RHS to FC.,2025-02-26T21:22:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88162
llm,copybara-service[bot],[XlaCallModule] Add better error message to computation deserialization failures.,[XlaCallModule] Add better error message to computation deserialization failures.,2025-02-26T20:11:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88158
tpu,copybara-service[bot],Add non-generic functions to HloRunnerPjRt that separate arg load + execution.,Add nongeneric functions to HloRunnerPjRt that separate arg load + execution. In select circumstances we want to use a HloRunnerPjRt implementation but have better control of the lifetime of the inputs and outputs. This functionality is not supported by the HloRunnerInterface. This patch provides some functions to allow the loading of inputs once and multiple execution on the same inputs.,2025-02-26T19:48:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88155
opt,copybara-service[bot],Move PJRT plugin linker scripts to `additional_linker_inputs`,Move PJRT plugin linker scripts to `additional_linker_inputs` Part 2 of https://github.com/openxla/xla/pull/16696 The final change will be the namechange of the target and add the `install_name` option.,2025-02-26T19:31:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88153
quantization,copybara-service[bot],Fix tensor CloneTo() not coping per-channel quantization parameters.,Fix tensor CloneTo() not coping perchannel quantization parameters.,2025-02-26T19:05:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88150
tpu,chunhsue,Qualcomm AI Engine Direct - Cast int8 embedding lookup table to int16 and set vendor logger to INFO, What  Some models contain embedding lookup ops with int8 table and int16 output. Provided a solution.  Set default qualcomm vendor logger to INFO.  Tests Passed `qnn_compiler_plugin_test` ``` [] Global test environment teardown [==========] 97 tests from 5 test suites ran. (3450 ms total) [  PASSED  ] 97 tests. ```,2025-02-26T09:06:22Z,ready to pull size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88128
opt,weilhuan-quic,Qualcomm AI Engine Direct - Graph O3 Optimization,1. Use O3 as default 2. Use relax precision for float as default because QNN tends to always enable it.  Test ```      0.0ms [  INFO ]   QnnContext_getBinary done successfully. context = 0x1 INFO: [tensorflow/lite/experimental/litert/vendors/qualcomm/qnn_manager.cc:247] Serialized a context bin of size (bytes): 644400 INFO: [tensorflow/lite/experimental/litert/vendors/qualcomm/compiler/qnn_compiler_plugin.cc:437] Context binary 0 generated      0.0ms [  INFO ]   QnnContext_free started. context = 0x1      0.0ms [  INFO ]   QnnContext_free done successfully.      0.0ms [  INFO ]   QnnDevice_free started. device = 0x1      0.0ms [  INFO ]   QnnDevice_free done. status 0x0      0.0ms [  INFO ]   QnnBackend_free started. backend = 0x1      0.0ms [  INFO ] Closing environment: 0x55aab477e5f0 [       OK ] SupportedOpsTest/QnnPluginOpCompatibilityTest.SupportedOpsTest/41 (666 ms) [] 42 tests from SupportedOpsTest/QnnPluginOpCompatibilityTest (2995 ms total) [] Global test environment teardown [==========] 97 tests from 5 test suites ran. (3196 ms total) [  PASSED  ] 97 tests. ```,2025-02-26T08:56:08Z,awaiting review comp:lite ready to pull size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88127
opt,copybara-service[bot],PR #23092: [NFC] Switch absl::optional to std::optional and cleanup.,PR CC(add interpreter debug func): [NFC] Switch absl::optional to std::optional and cleanup. Imported from GitHub PR https://github.com/openxla/xla/pull/23092 Copybara import of the project:  5d8333a8df3417e7769eb2b0b1143bb9834a0368 by Ilia Sergachev : [NFC] Cleanup uses of optional.  a2d4ce205ec499e592429ae873eba68baf2b1a9c by Ilia Sergachev : [NFC] Switch most remaining uses of absl::optional to std::optional. Merging this change closes CC(add interpreter debug func) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23092 from openxla:optional_cleanup a2d4ce205ec499e592429ae873eba68baf2b1a9c,2025-02-26T08:50:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88126
tpu,ywangwxd,"tensorflow cannot detect GPU with cuda 12.8, torch can"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.13  Custom code No  OS platform and distribution Linux CentOS 7  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuda 12.8 cudnn 9.7.1  GPU model and memory v100  Current behavior? I have tried python 3.8, 3.11, none of the can detect GPU, but pytorch and tfnightly is working well. Whenever I ask tensorflow to list GPU available, it gave me this message: > [GCC 11.2.0] :: Anaconda, Inc. on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf  "", len(tf.config.list_physical_devices('GPU')))20250226 14:28:49.474758: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used. 20250226 14:28:49.528867: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.  Standalone code to reproduce the issue ```shell import tensorflow as tf print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) ```  Relevant log output ```shell ```",2025-02-26T06:33:16Z,stat:awaiting tensorflower type:support comp:gpu TF 2.13,open,3,5,https://github.com/tensorflow/tensorflow/issues/88122,"I'm having trouble getting TensorFlow 2.13 to detect my GPU, even though PyTorch and tfnightly work just fine. Things I’ve Tried Checked CUDA installation – nvidiasmi confirms the GPU is fine. Verified TensorFlow compatibility – Seems like TF 2.13 only supports CUDA 11.8, but I have CUDA 12.8. Reinstalled TensorFlow with GPU support – No change. Set environment variables correctly – Still not working. Workarounds tfnightly works, so it looks like TensorFlow 2.13 doesn’t support CUDA 12.8. Potential Fixes: Use tfnightly: pip install tfnightly Downgrade to CUDA 11.8 (since TF 2.13 officially supports that). Would be great to know if stable TensorFlow will support CUDA 12.8 soon. Thanks!",">  Issue type >  > Bug >  >  Have you reproduced the bug with TensorFlow Nightly? >  > No >  >  Source >  > binary >  >  TensorFlow version >  > tf 2.13 >  >  Custom code >  > No >  >  OS platform and distribution >  > Linux CentOS 7 >  >  Mobile device >  > _No response_ >  >  Python version >  > 3.8 >  >  Bazel version >  > _No response_ >  >  GCC/compiler version >  > _No response_ >  >  CUDA/cuDNN version >  > cuda 12.8 cudnn 9.7.1 >  >  GPU model and memory >  > v100 >  >  Current behavior? >  > I have tried python 3.8, 3.11, none of the can detect GPU, but pytorch and tfnightly is working well. > Whenever I ask tensorflow to list GPU available, it gave me this message: >  > > [GCC 11.2.0] :: Anaconda, Inc. on linux > Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. > >>> import tensorflow as tf >  "", len(tf.config.list_physical_devices('GPU')))20250226 14:28:49.474758: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used. > 20250226 14:28:49.528867: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used. >  >  Standalone code to reproduce the issue >  > ```shell > import tensorflow as tf > print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) > ``` >  >  Relevant log output >  > ```shell >  > ``` On it","Check GPU Availability – Ensure your system detects the GPU using (nvidiasmi). If not, install/update NVIDIA drivers. Verify TensorFlow's CUDA Requirements – Different TensorFlow versions require specific CUDA and cuDNN versions. Install the correct versions. Install/Update NVIDIA Drivers – If no GPU is detected, install the latest NVIDIA driver. Install CUDA & cuDNN – Download and install the correct versions required by TensorFlow. Set Environment Variables – Ensure CUDA and cuDNN paths are correctly set in the system’s PATH. Restart the System – After installation, reboot to apply changes. Test TensorFlow GPU Support – Run>( tf.config.list_physical_devices('GPU')) to check if TensorFlow detects the GPU.","`import tensorflow as tf print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) if len(tf.config.list_physical_devices('GPU')) > 0:     print(""TensorFlow is using GPU"") else:     print(""TensorFlow is using CPU"")  If still error occurs , reinstall tensorflow pip uninstall  y tensorflow pip install tensorflow==2.10  `","Please support CUDA 12.8 , NVIDIA RTX 5090, Blackwell architecture GPU as PyTorch, at least nightly version of TensorFlow."
tpu,copybara-service[bot],Make external_litert_buffer_context library visible to TPU.,Make external_litert_buffer_context library visible to TPU.,2025-02-26T02:42:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88104
bert,copybara-service[bot],PR #22437: Added frontend attribute handling to explicit_stream_annotation_async_wrapper ,"PR CC(tensorflow1.11.0rc1 fails to build): Added frontend attribute handling to explicit_stream_annotation_async_wrapper  Imported from GitHub PR https://github.com/openxla/xla/pull/22437 This is a small change that ensures the frontend attributes are correctly passed to both the `asyncstart` and `asyncdone` created pairs. This also clears the scheduling attributes that are directly on the call operation and inner ops. The specific goal of this change is to have stable support combining the scheduling group ids with stream annotation in JAX.  ```python with set_xla_metadata(_scheduling_group_id=1):    result = compute_on(""gpu_stream:1"")(jitted_func)(...) ``` Currently, the issue stems from the `set_xla_metadata` context manager, which will apply the frontend attribute to all operations, including the ones within our `jitted_func`. When the same scheduling annotations is found in two `HloComputation`s, an error is raised in `LegalizeSchedulingAnnotations`. This is intended to avoid hitting this check, and cleaning up the annotations on the wrapped streamed computation. Copybara import of the project:  994c2eee3c946102270587681f5c17b994cbb6a9 by chaser : Added frontend attributed handling  9db58b2b988dc2288d42126271223f924aac19f9 by chaser : Added clearing of scheduling annotations  a83e32a34ba5d64a29c7f01b03536f27decd8125 by chaser : Added HloInstruction.erase_frontend_attribute Merging this change closes CC(tensorflow1.11.0rc1 fails to build) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22437 from chaserileyroberts:chase/frontend_forward_async_wrapper a83e32a34ba5d64a29c7f01b03536f27decd8125",2025-02-26T00:25:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88097
opt,copybara-service[bot],PR #23060: Refacting xla/tests/client_library_test_base to remove duplicated codes.,PR CC(Attributes values not inferred by TFE C API (eager mode)): Refacting xla/tests/client_library_test_base to remove duplicated codes. Imported from GitHub PR https://github.com/openxla/xla/pull/23060 Copybara import of the project:  217e312c7f5bd5834f0b4a2d4206aa735b72476a by Shawn Wang : code refacotring  c62b8f587da2886287376246086c9bf550fe7fcb by Shawn Wang : fix  b98044083e6dd6f2a37152e9638b93ac7cee6a06 by Shawn Wang : fix  a291d6e332ab1cc3fd11ff5ff19d414ac9973896 by Shawn Wang : fix: Merging this change closes CC(Attributes values not inferred by TFE C API (eager mode)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23060 from shawnwang18:shawnw/options_iterator a291d6e332ab1cc3fd11ff5ff19d414ac9973896,2025-02-26T00:02:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88094
tpu,copybara-service[bot],Fix graph output tensor not recognized as output in QC plugin.,Fix graph output tensor not recognized as output in QC plugin.,2025-02-25T23:41:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88092
tpu,copybara-service[bot],[easy] [XLA:TPU] [Simplifier] Remove redundant strides for slices,[easy] [XLA:TPU] [Simplifier] Remove redundant strides for slices FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88412 from tensorflow:dependabot/docker/tensorflow/tools/tf_sig_build_dockerfiles/ubuntued1544e 4b6ba37c2667bdfded35b1774ed8ee438bc5fb25,2025-02-25T22:18:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88084
opt,copybara-service[bot],Register TFQuantDialect in tf_quant_opt.cc,Register TFQuantDialect in tf_quant_opt.cc,2025-02-25T22:04:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88082
opt,copybara-service[bot],Add serialization options to the public API for alignment for bytecode.,Add serialization options to the public API for alignment for bytecode.,2025-02-25T22:00:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88081
opt,copybara-service[bot],PR #84932: Qualcomm AI Engine Direct - Add dispatch options for QC,PR CC(Qualcomm AI Engine Direct  Add dispatch options for QC): Qualcomm AI Engine Direct  Add dispatch options for QC Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/84932 Summary:  Add htp runtime options  Add log level settings dispatch_delegate_qualcomm_test !image Copybara import of the project:  15aa2288c9491a5e4a7bbab13c019fea99f11855 by jiunkaiy : Qualcomm AI Engine Direct  Add dispatch options for QC Summary:  Add htp runtime options  Add log level settings Merging this change closes CC(Qualcomm AI Engine Direct  Add dispatch options for QC) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/84932 from jiunkaiy:dev/jiunkaiy/qualcomm_dispatch_config 15aa2288c9491a5e4a7bbab13c019fea99f11855,2025-02-25T21:50:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88078
quantization,copybara-service[bot],Register TFQuantDialect with tf/compiler/mlir/quantization/tensorflow/passes/preprocess_op.cc,Register TFQuantDialect with tf/compiler/mlir/quantization/tensorflow/passes/preprocess_op.cc,2025-02-25T20:39:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88074
opt,copybara-service[bot],[XLA:CPU] Add option to always fold constants,[XLA:CPU] Add option to always fold constants,2025-02-25T20:00:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88073
tpu,copybara-service[bot],[IFRT] Add short form for specifying platform_names for IFRT IR passes.,"[IFRT] Add short form for specifying platform_names for IFRT IR passes. Some IFRT IR passes require a list of platform names to be given as an option. Currently, a platform names list requires an entry for each device, which makes  manually running the passes on modules with many devices tedious. This change introduces a short form for specifying platform names with the format  platform_name:number_of_occurrences. For example, tpu:2,cpu:3 is expanded to tpu,tpu,cpu,cpu,cpu.",2025-02-25T19:48:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88071
opt,copybara-service[bot],litert: Fix broken Dispatch API tests,litert: Fix broken Dispatch API tests Provide valid DispatchOption to LiteRtDispatchInitialize(),2025-02-25T18:20:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88060
tpu,copybara-service[bot],Introduce `TPUDummyInput` as a specialization of `Fill` for ICI weight distribution.,"Introduce `TPUDummyInput` as a specialization of `Fill` for ICI weight distribution. The new op has a few benefits over the previous version: * We can generate a single op instead of three ops for each dummy input. * The new op is marked as `DoNotOptimize` and `TF_NoConstantFold`, so it will never be accidentally constantfolded to a large memory footprint.",2025-02-25T18:07:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88059
tpu,Kuree,[TOSA] NameError: name 'ExperimentalTFLiteToTosaBytecode' is not defined," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.19.0rc  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? This function call should not return a `NameError`. It should either do not exist, or should be removed.  Standalone code to reproduce the issue ```shell from tensorflow.python.pywrap_mlir import experimental_tflite_to_tosa_bytecode experimental_tflite_to_tosa_bytecode("""", """") ```  Relevant log output ```shell Traceback (most recent call last):   File """", line 1, in    File ""/tmp/env/lib/python3.12/sitepackages/tensorflow/python/pywrap_mlir.py"", line 123, in experimental_tflite_to_tosa_bytecode     return ExperimentalTFLiteToTosaBytecode(            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ NameError: name 'ExperimentalTFLiteToTosaBytecode' is not defined. Did you mean: 'experimental_tflite_to_tosa_bytecode'? ```",2025-02-25T16:59:24Z,stat:awaiting tensorflower type:bug comp:lite TF 2.18,open,0,4,https://github.com/tensorflow/tensorflow/issues/88053, on it  putting in a PR ,"Hi, ,   I apologize for the delayed response, I have been able to replicate the same issue with `tensorflow==2.19.0rc0` but when I tried with `tensorflow 2.18.0` it's working as expected for reference please refer this gistfile and I see  has already submitted PR to take care of this issue but CLA has not signed so  could you please sign CLA for PR ? Thank you for your cooperation and patience."," 's PR does not fix the issue  the python binding is deleted by this commit: https://github.com/tensorflow/tensorflow/commit/95def861c766fd78dd673d7b8fdebd3c695dfb50. Based on the commit message, this python API is deprecated and should not exist.",I saw that the original code wasn't clean as well. Is there something better we can do?
opt,copybara-service[bot],PR #22898: [GPU] GEMM fusion autotuner: dump unoptimized fusions before profiling them.,PR CC(Error building on Windows // no such package '): [GPU] GEMM fusion autotuner: dump unoptimized fusions before profiling them. Imported from GitHub PR https://github.com/openxla/xla/pull/22898 This helps debugging failures during profiling. Copybara import of the project:  e63f7865126281a7eb5b410394424826275037a8 by Ilia Sergachev : [GPU] GEMM fusion autotuner: dump unoptimized fusions before profiling them. This helps debugging failures during profiling. Merging this change closes CC(Error building on Windows // no such package ') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22898 from openxla:dump_before_profiling e63f7865126281a7eb5b410394424826275037a8,2025-02-25T15:54:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88048
tpu,shaoyuyoung,[XLA] can't compile `tf.sparse.from_dense-tf.sparse.minimum`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version nightly 20250225  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? pass the compilation  Standalone code to reproduce the issue ```python import os import tensorflow import tensorflow as tf import numpy as np os.environ[""CUDA_VISIBLE_DEVICES""] = ""1"" class ComplexModel(tf.keras.Model):     def __init__(self):         super(ComplexModel, self).__init__()     def call(self, x):         x_sparse = tf.sparse.from_dense(x)         x = tf.sparse.minimum(x_sparse, tf.sparse.from_dense(tf.ones_like(x)))         return x model = ComplexModel() x = tf.constant([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32) inputs = [x] model(*inputs) print(""succeed on eager"") class ComplexModel(tf.keras.Model):     def __init__(self):         super(ComplexModel, self).__init__()     .function(jit_compile=True)     def call(self, x):         x_sparse = tf.sparse.from_dense(x)         x = tf.sparse.minimum(x_sparse, tf.sparse.from_dense(tf.ones_like(x)))         return x model = ComplexModel() model(*inputs) print(""succeed on XLA"") ```  Relevant log output ``` succeed on eager tf2xla conversion failed while converting __inference_call_33[_XlaMustCompile=true,config_proto=13561319589895757934,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_call_33] ```",2025-02-25T15:19:14Z,type:bug comp:xla TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88042,Hey tensorflow team! Just wanted to chime in on this XLA sparse tensor issue. I've been hitting the same problem with `tf.sparse.from_dense` and `tf.sparse.minimum` not working with XLA  I went into the cuda ask and messed around; The operations run perfectly in eager mode but fail when using `jit_compile=True`. This seems to be a known limitation with XLA not fully supporting these sparse tensor operations yet. Workaround until there is a fix  I'll try to add in my end btw: 1. Simplest workaround: Remove `jit_compile=True` when using sparse ops 2. Alternative: Rewrite your code to use dense operations if performance allows 3. Hybrid approach: Split your function into XLA and nonXLA parts Is there anyone currently working on XLA sparse tensor support who could use an extra hand? any guidance on where to start if I wanted to contribute to implementing this functionality? Peace!,"I was able to reproduce the same issue using TensorFlow 2.18 and the nightly version. Please find the gist here for reference. Additionally, I have provided an alternative solution in the gist, which might be helpful. Thank you!",Thank you for your confirmation!,Are you satisfied with the resolution of your issue? Yes No
tpu,shaoyuyoung,[XLA] can't compile `tf.linalg.inv` when input tensor dtype is `tf.complex64`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version nightly 20250223  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? eager can pass the check for `tf.linalg.inv` when input dtype is `tf.complex64`. So XLA should also pass the compilation but it fails  Standalone code to reproduce the issue ```python import os import tensorflow import tensorflow as tf import numpy as np os.environ[""CUDA_VISIBLE_DEVICES""] = ""1"" class FFTInverseModel(tf.keras.Model):     def __init__(self):         super().__init__()     def call(self, x):         inv = tf.linalg.inv(x)         return inv model = FFTInverseModel() input_shape = (2, 2) x = tf.constant([[1.0, 2.0], [3.0, 4.0]], dtype=tf.complex64, shape=input_shape)   tf.complex64 is the trigger condition inputs = [x] model(*inputs) print(""succeed on eager"") class FFTInverseModel(tf.keras.Model):     def __init__(self):         super().__init__()     .function(jit_compile=True)     def call(self, x):         inv = tf.linalg.inv(x)         return inv model = FFTInverseModel() model(*inputs) print(""succeed on XLA"") ```  Relevant log output ``` succeed on eager tf2xla conversion failed while converting __inference_call_7[_XlaMustCompile=true,config_proto=13561319589895757934,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_call_7] ```",2025-02-25T15:04:36Z,type:bug comp:xla TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88040,I was able to reproduce the same issue using TensorFlow 2.18 and the nightly version. Please find the gist here for reference. Thank you!,Thank you for your confirmation!,Are you satisfied with the resolution of your issue? Yes No,I'm having the same issue and not sure how this was resolved.
tpu,shaoyuyoung,[XLA] crashes when compiling `tf.raw_ops.AssignVariableOp`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version nightly 20250223  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? `tf.raw_ops.AssignVariableOp` should pass the compliation  Standalone code to reproduce the issue ```python import os import tensorflow import tensorflow as tf import numpy as np os.environ[""CUDA_VISIBLE_DEVICES""] = ""1"" class VariableModel(tf.keras.Model):     def __init__(self):         super(VariableModel, self).__init__()         self.variable = tf.Variable(initial_value=0.0)     def call(self, x):         return tf.raw_ops.AssignVariableOp(resource=self.variable.handle, value=tf.add(self.variable, x)) model = VariableModel() input_shape = [1] x = tf.constant([1.0], shape=input_shape) inputs = [x] model(*inputs) print(""succeed on eager"") class VariableModel(tf.keras.Model):     def __init__(self):         super(VariableModel, self).__init__()         self.variable = tf.Variable(initial_value=0.0)     .function(jit_compile=True)     def call(self, x):         return tf.raw_ops.AssignVariableOp(resource=self.variable.handle, value=tf.add(self.variable, x)) model = VariableModel() model(*inputs) print(""succeed on XLA"") ```  Relevant log output ``` succeed on eager ValueError: Shapes must be equal rank, but are 0 and 1 for '{{node AssignVariableOp}} = AssignVariableOpdtype=DT_FLOAT, validate_shape=false' with input shapes: [], [1]. ```",2025-02-25T14:54:24Z,type:bug comp:xla TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/88038,"I was able to reproduce the same issue using TensorFlow 2.18 and the nightly version. Please find the gist here for reference. Additionally, I have provided an alternative solution in the gist, which might be helpful. Thank you!",Thank you for your confirmation!,Are you satisfied with the resolution of your issue? Yes No
tpu,johnnynunez,Cuda 12.8 support, Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19.0  Custom code Yes  OS platform and distribution Ubuntu 24.04  Mobile device _No response_  Python version 3.11  Bazel version 0.6.5  GCC/compiler version _No response_  CUDA/cuDNN version 12.8/9.7.0  GPU model and memory 5090  Current behavior? Not supported. Hermetic cuda download 12.5  Standalone code to reproduce the issue ```shell compile with hermetic cuda. XLA is not updated in tensorflow ```  Relevant log output ```shell ```,2025-02-25T12:00:33Z,type:feature comp:gpu TF 2.18,open,3,1,https://github.com/tensorflow/tensorflow/issues/88029,"Hey, I'm also affected by this issue with CUDA 12.8 and TensorFlow 2.19.0 on Ubuntu CLI within the same OS. While waiting for official support, I found a temporary workaround that might help others:  I manually installed CUDA 12.8 and cuDNN 9.7.0 on my system.  I disabled hermetic CUDA during the TensorFlow build process and pointed it to my local CUDA installation.  However, XLA still has compatibility issues, so I had to disable XLA for now. This is not a complete solution, but it allowed me to use TensorFlow with CUDA 12.8 for basic tasks. I'm still hoping for official support and XLA updates. Let me know if anyone else has tried this or found better workarounds.  Oh and one other thing: within the CUDA update, is there documentation which can help fix this compatibility issue regarding the tensorflow commits ? "
opt,copybara-service[bot],[XLA:CPU] Kernel renamer function should be an optional member of the CPU compiler,[XLA:CPU] Kernel renamer function should be an optional member of the CPU compiler,2025-02-25T11:44:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88028
tpu,copybara-service[bot],Let FusionDeduplicationCache handle ProducerConsumer multi-output fusions.,Let FusionDeduplicationCache handle ProducerConsumer multioutput fusions. This will be needed when we want to allow such fusions in PriorityFusion.,2025-02-25T11:20:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88026
tpu,copybara-service[bot],[XLA:GPU] Fix thunk emitter for degenerate ops.,"[XLA:GPU] Fix thunk emitter for degenerate ops. The condition to get index of the output buffer wasn't always correct. It's possible to have an op with 1 operand and result with a tuple of 1 element. For example, a degenerate a2a will look like: ``` a2a = (u32[2]) alltoall(u32[2] a1), replica_groups={{0},{1}} ``` It's better to check that output is a tuple.",2025-02-25T10:07:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88022
tpu,copybara-service[bot],[XLA:GPU] Init output data with -1.,[XLA:GPU] Init output data with 1. Makes it easier to detect cases when we overwrite data out of the update range.,2025-02-25T09:32:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88021
sharding,copybara-service[bot],Update `hlo_sharding_util::CanonicalizeLayoutAfterShardingPropagation` to support partial parameter/result is updated while the other is not allowed to be updated.,"Update `hlo_sharding_util::CanonicalizeLayoutAfterShardingPropagation` to support partial parameter/result is updated while the other is not allowed to be updated. Before this change, this function takes two boolean variables `update_output_layout` and `update_parameters_layout` and applies it to all results/parameters. We need finegrained control on each element of results/parameters. This change replaces them with two `std::vector`.",2025-02-25T04:19:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87996
tpu,copybara-service[bot],PR #22822: Fix ambiguous constructor call in SourceTargetPairs initialization,"PR CC([INTEL MKL] Fix bug in MklSlice op when allocating output tensor.): Fix ambiguous constructor call in SourceTargetPairs initialization Imported from GitHub PR https://github.com/openxla/xla/pull/22822  Description Resolve a build failure (with GCC11) in `collective_permute_cycle_test` caused by an ambiguous constructor call when initializing `SourceTargetPairs` with an empty list (`{{}}`).    Issue   When calling `SourceTargetPairs({{}})`, the compiler could not determine whether to use the `std::vector>` constructor or the default copy/move constructors, leading to an error:   ``` xla/service/collective_permute_cycle_test.cc:130:48: error: call of overloaded 'SourceTargetPairs()' is ambiguous   130 |   EXPECT_EQ(GetCycleType(SourceTargetPairs({{}})), CycleType::kNone); ```  Solution 1. Explicitly define an `initializer_list` constructor for `SourceTargetPairs` to properly handle `{}` and `{{src, tgt}}` initializations.   2. Update the test case to use default ctor `SourceTargetPairs()` instead of `SourceTargetPairs({{}})`, ensuring clarity and correctness.   This fix ensures proper initialization and eliminates ambiguity Tested with GCC11 Copybara import of the project:  f97c38d47c8373ec609fdfbaedff3856f123fc33 by Alexander Pivovarov : Fix ambiguous constructor call in SourceTargetPairs initialization Merging this change closes CC([INTEL MKL] Fix bug in MklSlice op when allocating output tensor.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22822 from apivovarov:fix_source_target_pairs f97c38d47c8373ec609fdfbaedff3856f123fc33",2025-02-24T22:24:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87974
tpu,t-kalinowski,`tf.random.stateless_uniform()` example broken," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.18, 2.19.0rc0  Custom code No  OS platform and distribution macOS  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Simple usage of `tf.random.stateless_uniform()` raises an exception if `tensorflowmetal` is installed and `minval` and `maxval` are supplied. https://www.tensorflow.org/api_docs/python/tf/random/stateless_uniform   Standalone code to reproduce the issue ```shell uv run python 3.11 \  with 'tensorflow==2.19.0rc0' \  with 'tensorflowmetal' \  python c \  'import tensorflow as tf; x=tf.random.stateless_uniform([10], seed = [2, 3], minval=tf.constant(0), maxval=tf.constant(10), dtype=tf.int32); print(x)' ```  Relevant log output ```shell tomaszWQVX ~ % uv run python 3.11 with 'tensorflow==2.19.0rc0' with 'tensorflowmetal' python c 'import tensorflow as tf; x=tf.random.stateless_uniform([10], seed = [2, 3], minval=tf.constant(0), maxval=tf.constant(10), dtype=tf.int32); print(x)' 20250224 16:35:02.793541: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Max 20250224 16:35:02.793566: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 128.00 GB 20250224 16:35:02.793573: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 48.00 GB WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1740432902.793592 6966705 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. I0000 00:00:1740432902.793860 6966705 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) > physical PluggableDevice (device: 0, name: METAL, pci bus id: ) 20250224 16:35:02.800931: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: minval must be 0D, got shape [2] Traceback (most recent call last):   File """", line 1, in    File ""/Users/tomasz/.cache/uv/archivev0/2UOghugvqv1rQRmVCTGy/lib/python3.11/sitepackages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/Users/tomasz/.cache/uv/archivev0/2UOghugvqv1rQRmVCTGy/lib/python3.11/sitepackages/tensorflow/python/framework/ops.py"", line 6006, in raise_from_not_ok_status     raise core._status_to_exception(e) from None   pylint: disable=protectedaccess     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StatelessRandomUniformIntV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} minval must be 0D, got shape [2] [Op:StatelessRandomUniformIntV2] name: ```",2025-02-24T21:36:49Z,stat:awaiting response type:bug stale TF 2.18,closed,0,6,https://github.com/tensorflow/tensorflow/issues/87972,"Hi **kalinowski** , Apologies for the delay, and thanks for raising your concern here. I tried running your code on tensorflowmetal using TensorFlow 2.18.0 and encountered the same issue. I am attaching the results here for your reference. ``` (tfenv) maayaramacbookpro:~ maayara$ python Python 3.11.11 (main, Dec  3 2024, 17:20:40) [Clang 16.0.0 (clang1600.0.26.4)] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf >>> print(tf.__version__) 2.18.0 >>> print(tf.config.list_physical_devices()) [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] >>> import tensorflow as tf; x=tf.random.stateless_uniform([10], seed = [2, 3], minval=tf.constant(0), maxval=tf.constant(10), dtype=tf.int32); print(x) 20250226 12:55:07.301577: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro 20250226 12:55:07.301700: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB 20250226 12:55:07.301729: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1740554707.302295 12433048 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. I0000 00:00:1740554707.302777 12433048 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) > physical PluggableDevice (device: 0, name: METAL, pci bus id: ) 20250226 12:55:07.337793: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: minval must be 0D, got shape [2] Traceback (most recent call last):   File """", line 1, in    File ""/Users/maayara/tfenv/lib/python3.11/sitepackages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/Users/maayara/tfenv/lib/python3.11/sitepackages/tensorflow/python/framework/ops.py"", line 6002, in raise_from_not_ok_status     raise core._status_to_exception(e) from None   pylint: disable=protectedaccess     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StatelessRandomUniformIntV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} minval must be 0D, got shape [2] [Op:StatelessRandomUniformIntV2] name:  ``` Since this issue is not related to the TensorFlow repository, I recommend raising it on the Apple Developer Forums for further assistance. Thank you!","Hi, I am unable to post on the apple forums.  Is there any coordination between the tensorflow and tensorflowmetal projects?","Hi **kalinowski** , Apologies for the delay, To post the issue on apple developer forum you will have to create apple account then only you are able to post the issue so I recommend you to please create apple account if you don't have it to get faster resolution for your issue : Apple Developer  TensorFlow Metal Plugin. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],Flag guard the option to disable embedding pipelining when summary ops are present,Flag guard the option to disable embedding pipelining when summary ops are present,2025-02-24T20:10:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87968
opt,copybara-service[bot],"[hlo-opt] remove convert tool, all conversion options are already supported by hlo-opt.","[hloopt] remove convert tool, all conversion options are already supported by hloopt. hloproto > hlotext hloproto binary > hlotext hlotext > hloproto",2025-02-24T19:14:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87964
opt,copybara-service[bot],Overwrite xla_dump_as_* options in raw_options only if raw_options.xla_dump_to is set. Otherwise keep debug_options settings.,"Overwrite xla_dump_as_* options in raw_options only if raw_options.xla_dump_to is set. Otherwise keep debug_options settings. This is needed to access the flags state in PjRtStreamExecutorLoadedExecutable::Execute. Specifically, I need to access dumping options in order to dump unoptimized hlo module with arguments during execution correctly.",2025-02-24T17:28:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87955
opt,copybara-service[bot],PR #22800: Change the default value of print_operand_shape_ to false and print_large_constants_ to true.,"PR CC(Variance Inflation Factor estimate for each layer as guidance for dropout): Change the default value of print_operand_shape_ to false and print_large_constants_ to true. Imported from GitHub PR https://github.com/openxla/xla/pull/22800 Operand shape in long hlo text adds redundant information, which shouldn't be required. Changing the default value to off. The large constants were also printed earlier by default print options, and it is required for parsability and reproducibility. Turning this on by default. This is still controlled by debug option and the default value of that flag disables the large constants, and that behavior is not changed. Just the default print options change here. Copybara import of the project:  e30dea20489b3fb4d03d373fec0391d69486f4aa by Shraiysh Vaishay : Change the default value of print_operand_shape_ to false and print_large_constants_ to true. Operand shape in long hlo text adds redundant information, which shouldn't be required. Changing the default value to off. The large constants were also printed earlier by default print options, and it is required for parsability and reproducibility. Turning this on by default. This is still controlled by debug option and the default value of that flag disables the large constants, and that behavior is not changed. Just the default print options change here.  7008af0dd0ce342ecbe9475f1d0e277319f1705a by Shraiysh Vaishay : Handle tests  b22d5f95cfb7e15f930a2198279a76c38593cc53 by Shraiysh Vaishay : Fix more tests  d51579cae7359c6426a87ad4a7ff1b4b0c80f74a by Shraiysh Vaishay : Fix more tests Merging this change closes CC(Variance Inflation Factor estimate for each layer as guidance for dropout) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22800 from shraiysh:change_default_print_op_shape d51579cae7359c6426a87ad4a7ff1b4b0c80f74a",2025-02-24T17:06:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87953
bert,copybara-service[bot],PR #22437: Added frontend attribute handling to explicit_stream_annotation_async_wrapper ,"PR CC(tensorflow1.11.0rc1 fails to build): Added frontend attribute handling to explicit_stream_annotation_async_wrapper  Imported from GitHub PR https://github.com/openxla/xla/pull/22437 This is a small change that ensures the frontend attributes are correctly passed to both the `asyncstart` and `asyncdone` created pairs. This also clears the scheduling attributes that are directly on the call operation and inner ops. The specific goal of this change is to have stable support combining the scheduling group ids with stream annotation in JAX.  ```python with set_xla_metadata(_scheduling_group_id=1):    result = compute_on(""gpu_stream:1"")(jitted_func)(...) ``` Currently, the issue stems from the `set_xla_metadata` context manager, which will apply the frontend attribute to all operations, including the ones within our `jitted_func`. When the same scheduling annotations is found in two `HloComputation`s, an error is raised in `LegalizeSchedulingAnnotations`. This is intended to avoid hitting this check, and cleaning up the annotations on the wrapped streamed computation. Copybara import of the project:  994c2eee3c946102270587681f5c17b994cbb6a9 by chaser : Added frontend attributed handling  9db58b2b988dc2288d42126271223f924aac19f9 by chaser : Added clearing of scheduling annotations  a83e32a34ba5d64a29c7f01b03536f27decd8125 by chaser : Added HloInstruction.erase_frontend_attribute Merging this change closes CC(tensorflow1.11.0rc1 fails to build) Reverts 52fc64b538c7291b8caa0de7b0bfdcf7762376e8 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22437 from chaserileyroberts:chase/frontend_forward_async_wrapper a83e32a34ba5d64a29c7f01b03536f27decd8125",2025-02-24T12:36:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87944
opt,copybara-service[bot],Upgrade to Bazel 7.4.1 for TensorFlow,"Upgrade to Bazel 7.4.1 for TensorFlow  Disabled Bzlmod for now before we start the migration  Disabled  `modify_execution_info` due to https://github.com/bazelbuild/bazel/pull/16262  Explicitly added `Wl,undefined,dynamic_lookup` as linkopt on macOS after https://github.com/bazelbuild/bazel/pull/16414  Set `force_no_whole_archive` for host features  Addressed Windows linking issue by adding `//:__subpackages__` in `exports_filter` of `//tensorflow/python:pywrap_tensorflow_internal`   Removed `license` attribute on cc_shared_library  Fixed license checks",2025-02-24T11:23:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87943
tpu,diddlywob,Help with reference," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 13.3.06ubuntu2~24.04) 13.3.0  CUDA/cuDNN version CUDA 12.0  GPU model and memory NVIDIA GeForce RTX 4050  Current behavior? I am expecting my model to run without giving any type of warnings. I know for certain that my data does not have any NaN values in it, so I am not sure why am I getting the warning I am receiving. My output should look something like the below: 2138/14403 ━━━━━━━━━━━━━━━━━━━━ 2:45 14ms/step  accuracy: 0.7590  loss: 1.1440E0000 00:00:1740370043.647457  Standalone code to reproduce the issue ```shell I know the issue has to do with this part of the code. I am just not sure where. import os os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async' import math import pyarrow.parquet as pq import pandas as pd import numpy as np import tensorflow as tf from tensorflow.keras.regularizers import l2 from scipy.sparse import hstack, csr_matrix from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import OneHotEncoder from itertools import combinations   Initialize Encoders & Scaler  onehot_encoder = OneHotEncoder(handle_unknown=""ignore"", sparse_output=False) scaler = StandardScaler()   Stream Train/Test Split in Chunks  def stream_split_data(parquet_files, categorical_columns, numeric_columns):     """"""Splits merged data into training and testing sets dynamically in chunks.""""""     for chunk in stream_merged_data(parquet_files, categorical_columns, numeric_columns, merge_key):         mask = np.random.rand(len(chunk)) = end_idx:                 return   Stop when we reach end index             X_batch = X_chunk[i:i + batch_size]             y_batch = y_chunk[i:i + batch_size]             yield X_batch, y_batch             current_idx += batch_size   Create `tf.data.Dataset` Using the Chunked Generator  train_dataset = tf.data.Dataset.from_generator(     lambda: preprocess_dense_batches(parquet_files, categorical_columns, numeric_columns, batch_size, end_idx=int(train_ratio * total_rows)),     output_signature=(         tf.TensorSpec(shape=(None, one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model)), dtype=tf.float32),         tf.TensorSpec(shape=(None,), dtype=tf.float32)     ) ).repeat().prefetch(buffer_size=tf.data.AUTOTUNE) test_dataset = tf.data.Dataset.from_generator(     lambda: preprocess_dense_batches(parquet_files, categorical_columns, numeric_columns, batch_size, end_idx=total_rows),     output_signature=(         tf.TensorSpec(shape=(None, one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model)), dtype=tf.float32),         tf.TensorSpec(shape=(None,), dtype=tf.float32)     ) ).prefetch(buffer_size=tf.data.AUTOTUNE) class_weight_dict = {     0: 1.0,      1: 2.0,       2: 3.0    } steps_per_epoch = math.ceil((train_ratio * total_rows) / batch_size) test_steps = math.ceil(((1  train_ratio) * total_rows) / batch_size) print(""Datasets created dynamically."") input_all_data = Input(shape=(one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model),), name='all_data_input') print(""Creating the dense layers..."") x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(input_all_data) x = BatchNormalization()(x) x = Dropout(0.3)(x) x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x) x = BatchNormalization()(x) x = Dropout(0.3)(x) output = Dense(3, activation='softmax')(x)   3 neurons, softmax for multiclass  Compile the model model = Model(inputs=input_all_data, outputs=output) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) print(""\nBegin training the model.\n"") history_all_data = model.fit(     train_dataset,                         validation_data=test_dataset,          epochs=1,                         steps_per_epoch=steps_per_epoch,     validation_steps=test_steps,     class_weight=class_weight_dict,   Pass class weights     verbose=1 ) ```  Relevant log output ```shell WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1740371136.641692   17996 service.cc:148] XLA service 0x7f9fc4004850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: I0000 00:00:1740371136.642763   17996 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9 20250223 23:25:36.704967: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. I0000 00:00:1740371136.874435   17996 cuda_dnn.cc:529] Loaded cuDNN version 90300 I0000 00:00:1740371138.351218   17996 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.  1490/14403 ━━━━━━━━━━━━━━━━━━━━ 3:33 17ms/step  accuracy: 0.5462  loss: 2.3473E0000 00:00:1740371164.392105   17996 buffer_comparator.cc:157] Difference at 0: nan, expected 1.65142 E0000 00:00:1740371164.392245   17996 buffer_comparator.cc:157] Difference at 1: nan, expected 1.48098 E0000 00:00:1740371164.392259   17996 buffer_comparator.cc:157] Difference at 2: nan, expected 1.67361 E0000 00:00:1740371164.392283   17996 buffer_comparator.cc:157] Difference at 3: nan, expected 3.16466 E0000 00:00:1740371164.392288   17996 buffer_comparator.cc:157] Difference at 4: nan, expected 2.17621 E0000 00:00:1740371164.392296   17996 buffer_comparator.cc:157] Difference at 5: nan, expected 3.38546 E0000 00:00:1740371164.392319   17996 buffer_comparator.cc:157] Difference at 6: nan, expected 1.67085 E0000 00:00:1740371164.392324   17996 buffer_comparator.cc:157] Difference at 7: nan, expected 1.38643 E0000 00:00:1740371164.392342   17996 buffer_comparator.cc:157] Difference at 8: nan, expected 1.76836 E0000 00:00:1740371164.392347   17996 buffer_comparator.cc:157] Difference at 9: nan, expected 3.48673 20250223 23:26:04.392363: E external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:987] Results do not match the reference. This is likely a bug/unexpected loss of precision.  2967/14403 ━━━━━━━━━━━━━━━━━━━━ 2:58 16ms/step  accuracy: 0.5747  loss: 2.0899E0000 00:00:1740371185.763887   17997 buffer_comparator.cc:157] Difference at 0: nan, expected 3.13696 E0000 00:00:1740371185.764129   17997 buffer_comparator.cc:157] Difference at 1: nan, expected 3.33544 E0000 00:00:1740371185.764148   17997 buffer_comparator.cc:157] Difference at 2: nan, expected 2.92697 E0000 00:00:1740371185.764155   17997 buffer_comparator.cc:157] Difference at 3: nan, expected 2.59307 E0000 00:00:1740371185.764161   17997 buffer_comparator.cc:157] Difference at 4: nan, expected 2.88114 E0000 00:00:1740371185.764169   17997 buffer_comparator.cc:157] Difference at 5: nan, expected 2.56242 E0000 00:00:1740371185.764175   17997 buffer_comparator.cc:157] Difference at 6: nan, expected 2.62881 E0000 00:00:1740371185.764181   17997 buffer_comparator.cc:157] Difference at 7: nan, expected 2.84552 E0000 00:00:1740371185.764234   17997 buffer_comparator.cc:157] Difference at 8: nan, expected 2.49929 E0000 00:00:1740371185.764246   17997 buffer_comparator.cc:157] Difference at 9: nan, expected 2.47989 20250223 23:26:25.764275: E external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:987] Results do not match the reference. This is likely a bug/unexpected loss of precision. ```",2025-02-24T05:00:27Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87925,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change for visibility,Internal change for visibility,2025-02-24T04:42:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87920
tpu,diddlywob,Results Do Not Match Reference," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 13.3.06ubuntu2~24.04) 13.3.0  CUDA/cuDNN version CUDA 12.0  GPU model and memory NVIDIA GeForce RTX 4050  Current behavior? I am expecting my model to run without giving any type of warnings. I know for certain that my data does not have any NaN values in it, so I am not sure why am I getting the warning I am receiving. My output should look something like the below: 2138/14403 ━━━━━━━━━━━━━━━━━━━━ 2:45 14ms/step  accuracy: 0.7590  loss: 1.1440E0000 00:00:1740370043.647457  Standalone code to reproduce the issue ```shell import os os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async' import math import pyarrow.parquet as pq import pandas as pd import numpy as np import tensorflow as tf from tensorflow.keras.regularizers import l2 from scipy.sparse import hstack, csr_matrix from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import OneHotEncoder from itertools import combinations   Initialize Encoders & Scaler  onehot_encoder = OneHotEncoder(handle_unknown=""ignore"", sparse_output=False) scaler = StandardScaler()   Stream Train/Test Split in Chunks  def stream_split_data(parquet_files, categorical_columns, numeric_columns):     """"""Splits merged data into training and testing sets dynamically in chunks.""""""     for chunk in stream_merged_data(parquet_files, categorical_columns, numeric_columns, merge_key):         np.random.seed(42)   Ensures the split is always the same upon restarting         mask = np.random.rand(len(chunk)) = end_idx:                 return   Stop when we reach end index             X_batch = X_chunk[i:i + batch_size]             y_batch = y_chunk[i:i + batch_size]             yield X_batch, y_batch             current_idx += batch_size   Create `tf.data.Dataset` Using the Chunked Generator  train_dataset = tf.data.Dataset.from_generator(     lambda: preprocess_dense_batches(parquet_files, categorical_columns, numeric_columns, batch_size, end_idx=int(train_ratio * total_rows)),     output_signature=(         tf.TensorSpec(shape=(None, one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model)), dtype=tf.float32),         tf.TensorSpec(shape=(None,), dtype=tf.float32)     ) ).repeat().prefetch(buffer_size=tf.data.AUTOTUNE) test_dataset = tf.data.Dataset.from_generator(     lambda: preprocess_dense_batches(parquet_files, categorical_columns, numeric_columns, batch_size, end_idx=total_rows),     output_signature=(         tf.TensorSpec(shape=(None, one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model)), dtype=tf.float32),         tf.TensorSpec(shape=(None,), dtype=tf.float32)     ) ).prefetch(buffer_size=tf.data.AUTOTUNE)   Class Weights Handling  class_weight_dict = {     0: 1.0,   Pushes (0.5)     1: 2.0,   Misses (0)     2: 3.0    Hits (1) }   Calculate Steps per Epoch  steps_per_epoch = math.ceil((train_ratio * total_rows) / batch_size) test_steps = math.ceil(((1  train_ratio) * total_rows) / batch_size) print(""Datasets created dynamically."")   Define Your Neural Network (Unchanged)  input_all_data = Input(shape=(one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model),), name='all_data_input') print(""Creating the dense layers..."")  Create the dense layers x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(input_all_data) x = BatchNormalization()(x) x = Dropout(0.3)(x) x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x) x = BatchNormalization()(x) x = Dropout(0.3)(x) output = Dense(3, activation='softmax')(x)   3 neurons, softmax for multiclass print(""Compiling the model..."")  Compile the model model = Model(inputs=input_all_data, outputs=output) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])   Train Model Using Existing Training Loop  print(""\nBegin training the model.\n"") history_all_nfl_data = model.fit(     train_dataset,                     Training dataset     validation_data=test_dataset,      Testing dataset     epochs=1,                         steps_per_epoch=steps_per_epoch,     validation_steps=test_steps,     class_weight=class_weight_dict,   Pass class weights     verbose=1 ) ```  Relevant log output ```shell WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1740371136.641692   17996 service.cc:148] XLA service 0x7f9fc4004850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: I0000 00:00:1740371136.642763   17996 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9 20250223 23:25:36.704967: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. I0000 00:00:1740371136.874435   17996 cuda_dnn.cc:529] Loaded cuDNN version 90300 I0000 00:00:1740371138.351218   17996 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.  1490/14403 ━━━━━━━━━━━━━━━━━━━━ 3:33 17ms/step  accuracy: 0.5462  loss: 2.3473E0000 00:00:1740371164.392105   17996 buffer_comparator.cc:157] Difference at 0: nan, expected 1.65142 E0000 00:00:1740371164.392245   17996 buffer_comparator.cc:157] Difference at 1: nan, expected 1.48098 E0000 00:00:1740371164.392259   17996 buffer_comparator.cc:157] Difference at 2: nan, expected 1.67361 E0000 00:00:1740371164.392283   17996 buffer_comparator.cc:157] Difference at 3: nan, expected 3.16466 E0000 00:00:1740371164.392288   17996 buffer_comparator.cc:157] Difference at 4: nan, expected 2.17621 E0000 00:00:1740371164.392296   17996 buffer_comparator.cc:157] Difference at 5: nan, expected 3.38546 E0000 00:00:1740371164.392319   17996 buffer_comparator.cc:157] Difference at 6: nan, expected 1.67085 E0000 00:00:1740371164.392324   17996 buffer_comparator.cc:157] Difference at 7: nan, expected 1.38643 E0000 00:00:1740371164.392342   17996 buffer_comparator.cc:157] Difference at 8: nan, expected 1.76836 E0000 00:00:1740371164.392347   17996 buffer_comparator.cc:157] Difference at 9: nan, expected 3.48673 20250223 23:26:04.392363: E external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:987] Results do not match the reference. This is likely a bug/unexpected loss of precision.  2967/14403 ━━━━━━━━━━━━━━━━━━━━ 2:58 16ms/step  accuracy: 0.5747  loss: 2.0899E0000 00:00:1740371185.763887   17997 buffer_comparator.cc:157] Difference at 0: nan, expected 3.13696 E0000 00:00:1740371185.764129   17997 buffer_comparator.cc:157] Difference at 1: nan, expected 3.33544 E0000 00:00:1740371185.764148   17997 buffer_comparator.cc:157] Difference at 2: nan, expected 2.92697 E0000 00:00:1740371185.764155   17997 buffer_comparator.cc:157] Difference at 3: nan, expected 2.59307 E0000 00:00:1740371185.764161   17997 buffer_comparator.cc:157] Difference at 4: nan, expected 2.88114 E0000 00:00:1740371185.764169   17997 buffer_comparator.cc:157] Difference at 5: nan, expected 2.56242 E0000 00:00:1740371185.764175   17997 buffer_comparator.cc:157] Difference at 6: nan, expected 2.62881 E0000 00:00:1740371185.764181   17997 buffer_comparator.cc:157] Difference at 7: nan, expected 2.84552 E0000 00:00:1740371185.764234   17997 buffer_comparator.cc:157] Difference at 8: nan, expected 2.49929 E0000 00:00:1740371185.764246   17997 buffer_comparator.cc:157] Difference at 9: nan, expected 2.47989 20250223 23:26:25.764275: E external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:987] Results do not match the reference. This is likely a bug/unexpected loss of precision. ```",2025-02-24T04:41:34Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87919,Are you satisfied with the resolution of your issue? Yes No
tpu,diddlywob,Results do not match the reference. This is likely a bug/unexpected loss of precision.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 13.3.06ubuntu2~24.04) 13.3.0  CUDA/cuDNN version CUDA 12.0  GPU model and memory NVIDIA GeForce RTX 4050  Current behavior? I am simply expecting my model to run without giving any type of warnings. I know for certain that my data does not have any NaN values in it, so I am not sure why am I getting the warning I am receiving. My output should look something like the below:  2138/14403 ━━━━━━━━━━━━━━━━━━━━ 2:45 14ms/step  accuracy: 0.7590  loss: 1.1440E0000 00:00:1740370043.647457    Standalone code to reproduce the issue ```shell import os os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async' import math import pyarrow.parquet as pq import pandas as pd import numpy as np import tensorflow as tf from tensorflow.keras.regularizers import l2 from scipy.sparse import hstack, csr_matrix from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import OneHotEncoder from itertools import combinations   Initialize Encoders & Scaler  onehot_encoder = OneHotEncoder(handle_unknown=""ignore"", sparse_output=False) scaler = StandardScaler()   Stream Train/Test Split in Chunks  def stream_split_data(parquet_files, categorical_columns, numeric_columns):     """"""Splits merged data into training and testing sets dynamically in chunks.""""""     for chunk in stream_merged_data(parquet_files, categorical_columns, numeric_columns, merge_key):         np.random.seed(42)   Ensures the split is always the same upon restarting         mask = np.random.rand(len(chunk)) = end_idx:                 return   Stop when we reach end index             X_batch = X_chunk[i:i + batch_size]             y_batch = y_chunk[i:i + batch_size]             yield X_batch, y_batch             current_idx += batch_size   Create `tf.data.Dataset` Using the Chunked Generator  train_dataset = tf.data.Dataset.from_generator(     lambda: preprocess_dense_batches(parquet_files, categorical_columns, numeric_columns, batch_size, end_idx=int(train_ratio * total_rows)),     output_signature=(         tf.TensorSpec(shape=(None, one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model)), dtype=tf.float32),         tf.TensorSpec(shape=(None,), dtype=tf.float32)     ) ).repeat().prefetch(buffer_size=tf.data.AUTOTUNE) test_dataset = tf.data.Dataset.from_generator(     lambda: preprocess_dense_batches(parquet_files, categorical_columns, numeric_columns, batch_size, end_idx=total_rows),     output_signature=(         tf.TensorSpec(shape=(None, one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model)), dtype=tf.float32),         tf.TensorSpec(shape=(None,), dtype=tf.float32)     ) ).prefetch(buffer_size=tf.data.AUTOTUNE)   Class Weights Handling  class_weight_dict = {     0: 1.0,   Pushes (0.5)     1: 2.0,   Misses (0)     2: 3.0    Hits (1) }   Calculate Steps per Epoch  steps_per_epoch = math.ceil((train_ratio * total_rows) / batch_size) test_steps = math.ceil(((1  train_ratio) * total_rows) / batch_size) print(""Datasets created dynamically."")   Define Your Neural Network (Unchanged)  input_all_data = Input(shape=(one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model),), name='all_data_input') print(""Creating the dense layers..."")  Create the dense layers x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(input_all_data) x = BatchNormalization()(x) x = Dropout(0.3)(x) x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x) x = BatchNormalization()(x) x = Dropout(0.3)(x) output = Dense(3, activation='softmax')(x)   3 neurons, softmax for multiclass classification (hit, miss, push) print(""Compiling the model..."")  Compile the model model = Model(inputs=input_all_nfl_data, outputs=output) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])   Train Model Using Existing Training Loop  print(""\nBegin training the model.\n"") history_all_nfl_data = model.fit(     train_dataset,                     Training dataset     validation_data=test_dataset,      Testing dataset     epochs=1,                         steps_per_epoch=steps_per_epoch,     validation_steps=test_steps,     class_weight=class_weight_dict,   Pass class weights     verbose=1 ) ```  Relevant log output ```shell WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1740371136.641692   17996 service.cc:148] XLA service 0x7f9fc4004850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: I0000 00:00:1740371136.642763   17996 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9 20250223 23:25:36.704967: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. I0000 00:00:1740371136.874435   17996 cuda_dnn.cc:529] Loaded cuDNN version 90300 I0000 00:00:1740371138.351218   17996 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.  1490/14403 ━━━━━━━━━━━━━━━━━━━━ 3:33 17ms/step  accuracy: 0.5462  loss: 2.3473E0000 00:00:1740371164.392105   17996 buffer_comparator.cc:157] Difference at 0: nan, expected 1.65142 E0000 00:00:1740371164.392245   17996 buffer_comparator.cc:157] Difference at 1: nan, expected 1.48098 E0000 00:00:1740371164.392259   17996 buffer_comparator.cc:157] Difference at 2: nan, expected 1.67361 E0000 00:00:1740371164.392283   17996 buffer_comparator.cc:157] Difference at 3: nan, expected 3.16466 E0000 00:00:1740371164.392288   17996 buffer_comparator.cc:157] Difference at 4: nan, expected 2.17621 E0000 00:00:1740371164.392296   17996 buffer_comparator.cc:157] Difference at 5: nan, expected 3.38546 E0000 00:00:1740371164.392319   17996 buffer_comparator.cc:157] Difference at 6: nan, expected 1.67085 E0000 00:00:1740371164.392324   17996 buffer_comparator.cc:157] Difference at 7: nan, expected 1.38643 E0000 00:00:1740371164.392342   17996 buffer_comparator.cc:157] Difference at 8: nan, expected 1.76836 E0000 00:00:1740371164.392347   17996 buffer_comparator.cc:157] Difference at 9: nan, expected 3.48673 20250223 23:26:04.392363: E external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:987] Results do not match the reference. This is likely a bug/unexpected loss of precision.  2967/14403 ━━━━━━━━━━━━━━━━━━━━ 2:58 16ms/step  accuracy: 0.5747  loss: 2.0899E0000 00:00:1740371185.763887   17997 buffer_comparator.cc:157] Difference at 0: nan, expected 3.13696 E0000 00:00:1740371185.764129   17997 buffer_comparator.cc:157] Difference at 1: nan, expected 3.33544 E0000 00:00:1740371185.764148   17997 buffer_comparator.cc:157] Difference at 2: nan, expected 2.92697 E0000 00:00:1740371185.764155   17997 buffer_comparator.cc:157] Difference at 3: nan, expected 2.59307 E0000 00:00:1740371185.764161   17997 buffer_comparator.cc:157] Difference at 4: nan, expected 2.88114 E0000 00:00:1740371185.764169   17997 buffer_comparator.cc:157] Difference at 5: nan, expected 2.56242 E0000 00:00:1740371185.764175   17997 buffer_comparator.cc:157] Difference at 6: nan, expected 2.62881 E0000 00:00:1740371185.764181   17997 buffer_comparator.cc:157] Difference at 7: nan, expected 2.84552 E0000 00:00:1740371185.764234   17997 buffer_comparator.cc:157] Difference at 8: nan, expected 2.49929 E0000 00:00:1740371185.764246   17997 buffer_comparator.cc:157] Difference at 9: nan, expected 2.47989 20250223 23:26:25.764275: E external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:987] Results do not match the reference. This is likely a bug/unexpected loss of precision. ```",2025-02-24T04:27:53Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87918,Are you satisfied with the resolution of your issue? Yes No
tpu,0gur1,A heap oob write in TensorArray.write," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.18.0  Custom code Yes  OS platform and distribution Ubuntu 22.04.5  Mobile device _No response_  Python version Python 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In a model, if there is a lambda layer containing the TensorArray.write operation, the model service will crash after sending data for inference.  Standalone code to reproduce the issue ```shell In a python code, we can varify the vulnerability quickly with the following poc. import tensorflow as tf .function() def foo():   ta = tf.TensorArray(tf.float32, size=10,dynamic_size=True, clear_after_read=False)   ta = ta.write(tf.cast(0xffffffff, tf.int32),1)   return ta.read(1) a=foo() print(a) ```  Relevant log output ```shell  crash Thread 1 ""python"" received signal SIGSEGV, Segmentation fault. 0x00007fffebfa5560 in tensorflow::TensorListSetItem::Compute(tensorflow::OpKernelContext*) () from /home/test/ai/kerash5/tfvenv/lib/python3.10/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 (gdb) bt CC(未找到相关数据)  0x00007fffebfa5560 in tensorflow::TensorListSetItem::Compute(tensorflow::OpKernelContext*) ()    from /home/test/tfvenv/lib/python3.10/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 CC(Add support for Python 3.x)  0x00007ffff563d0b9 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::SimplePropagatorState::TaggedNode const&, long) ()    from /home/test/tfvenv/lib/python3.10/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"")  0x00007ffff566c844 in std::_Function_handler), tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector, std::allocator >, tensorflow::Tensor>, std::allocator, std::allocator >, tensorflow::Tensor> > > const&, std::vector, std::allocator >, std::allocator, std::allocator > > > const&, std::vector >*)::$_1>::_M_invoke(std::_Any_data const&, std::function&&) ()    from /home/test/tfvenv/lib/python3.10/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 CC(JVM, .NET Language Support)  0x00007ffff563bbd7 in tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(absl::lts_20230802::InlinedVector >*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*) ()    from /home/test/tfvenv/lib/python3.10/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 CC(Installation over pip fails to import with protobuf 2.6.1)  0x00007ffff563eb8e in tensorflow::(anonymous namespace)::ExecutorState::NodeDone(absl::lts_20230802::Status const&, absl::lts_20230802::InlinedVector >*, tensorflow::NodeExecStatsInterface*, tensorflow::SimplePropagatorState::TaggedNodeRType  for more, q to quit, c to continue without pagingq Quit (gdb) x/i $rip => 0x7fffebfa5560 :	lock decq 0x8(%rbx) (gdb) i r rbx rbx            0x151               337 (gdb) x/8gx 0x5555577db8700x20 0x5555577db850:	0x0000555559a09268	0x0000015555d278d8 0x5555577db860:	0x0000000000000001	【0x0000000000000151】>rbx 0x5555577db870:	0x0000000000000000	0x0001003000000003 0x5555577db880:	0x0000000000000000	0x0000000000000000  oob write It will write a Tensor before the vector address(0x5555577db870).  r12 is the start of vector, r15 is the index and r14 is the src Tensor which will be written to the index 0x7fffebfa550b :	lea    (%r12,%r15,1),%rbx 0x7fffebfa550f :	movzbl 0xd(%r14),%r13d (gdb) x/4gx $r14 0x5555599e3170:	0x00005555599ea3f8	0x0000015558132bf8 0x5555599e3180:	0x0000000000000001	0x00005555596e7370 (gdb) i r r12 r12            0x5555577db870      93825028438128 (gdb) i r r15 r15            0xffffffffffffffe0  32 before writing (gdb) x/4gx $rbx 0x5555577db850:	0x0000000000000000	0x0000000000000000 0x5555577db860:	0x00007fffaf2fb1e0	0x0000000000000151 copy contents of r14 to an oob address rbx 0x7fffebfa5514 :	mov    0x10(%r14),%rax 0x7fffebfa5518 :	mov    %rax,0x10(%r12,%r15,1) ... 0x7fffebfa552c :	vmovups (%r14),%xmm0 0x7fffebfa5531 :	vmovups %xmm0,(%rbx) after writing (gdb) x/4gx 0x5555577db8700x20 0x5555577db850:	0x00005555599ea3f8	0x0000015558132bf8 0x5555577db860:	0x0000000000000001	0x0000000000000151 ```",2025-02-24T02:27:14Z,type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/87912,"Hi **** , Thanks for reaching out! The issue is caused by the large index value, which exceeds memory bounds. Using excessively large indices can lead to crashes due to memory allocation failures. Could you try using a reasonable index value? It should work correctly. I tested it on my end, and it worked fine. For your reference, I am attaching a gist with a working example. Thank you!","Hi, Thanks for your reply. I get your point to use a  resonable index value. And I create this issue just to illustrate the potential security issues which may be exploited by attackers. We cannot expect users to always perform reasonable actions. The root cause lies in https://github.com/tensorflow/tensorflow/blob/6550e4bd80223cdb8be6c3afd1f81e86a4d433c3/tensorflow/core/kernels/list_kernels.ccL483 `resize_if_index_out_of_bounds_ comes` from `dynamic_size`. If `dynmic_size` is set to True, it will come to [1]. Since `l>tensors().size()` is an unsigned integer, index will be converted to an unsigned int and match the condition with a value of `0xffffffff`. At [2], it will resize the vector to 0. Finally it writes to index 1 at [3], which will lead to a heap oob write operation. ```     TensorList* output_list = nullptr;     OP_REQUIRES_OK(c, ForwardInputOrCreateNewList(c, 0, 0, *l, &output_list));     int32_t index = c>input(1).scalar()();     if (!resize_if_index_out_of_bounds_) {       OP_REQUIRES(c, index tensors().size(),                   errors::InvalidArgument(""Trying to modify element "", index,                                           "" in a list with "",                                           l>tensors().size(), "" elements.""));     } else if (index >= l>tensors().size()) { //>[1]       output_list>tensors().resize(index + 1, Tensor(DT_INVALID));//>[2]     }     output_list>tensors()[index] = value; //>[3] ``` So a validation for the return value of `resize` may solve the problem in my opinion."
tf-trt,dennys,Should I continue using TF-TRT (TensorRT in TensorFlow) after 2.18?, Issue type Others  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.17.1  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I just upgrade to TensorFlow 2.17.1 and find TensorRT support is disabled in CUDA builds since TensorFlow 2.18. And TFTRT (TensorRT in TensorFlow) is archived on 2025/2/5. I'm not sure should I continue using TFTRT in the future? Or I should use native TensorRT?  Standalone code to reproduce the issue ```shell N/A ```  Relevant log output ```shell ```,2025-02-24T00:59:50Z,stat:awaiting response type:others 2.17,closed,0,3,https://github.com/tensorflow/tensorflow/issues/87911,"Hi **** , Thanks for raising your concern here. As per the official documentation, TensorRT support has been disabled in CUDA builds starting from TensorFlow 2.18.0. Regarding whether to continue using TFTRT or switch to native TensorRT, it depends on your use case: If you are working with TensorFlow models and need TensorRT acceleration, you will need to use TensorFlow 2.17 or earlier, as these versions still support TFTRT. However, this means you would not benefit from newer TensorFlow features. If you prioritize TensorRT optimization and performance, switching to native TensorRT could be a better choice. For more details, please refer to the official documentation, which I have attached for your reference. Thank you!","Got it, I think I should migrate to native TensorRT, thanks for your feedback.",Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Clear out XLA computations from compilation caches after finalizing the TensorFlow session.,"Clear out XLA computations from compilation caches after finalizing the TensorFlow session. After compiling a TF Graph into an XLA HLO program and after compiling the HLO into an executable, we keep around a `std::shared_ptrcomputation`. When the compiled HLO contains many constants, its heap memory consumption is significant and otherwise unreferenced after initialization. This CL adds an entrypoint `DeviceCompilationCache::Finalize`, which is exposed as `DeviceCompiler::Finalize`, which is an implementation of the virtual function `ResourceBase::Finalize`. `ResourceBase::Finalize` returns `absl::AnyInvocable` so that we can defer destruction of finalized objects owned by `ResourceBase` until after we release the lock `ResourceMgr::mu_`.",2025-02-24T00:17:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87909
tpu,JacksonDivakar,Inconsistent Behavior When Using tf.keras.metrics.Accuracy with F1-Score and Precision," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.0  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? !Image !Image !Image  Standalone code to reproduce the issue ```shell 1)Compile a model using tf.keras.metrics.Accuracy() along with custom F1score and precision metrics. 2)Observe the incorrect behavior in the reported accuracy. 3)Compare with behavior when using ""accuracy"" in the metrics list. Link : https://github.com/JacksonDivakarProjects/Others/blob/main/accuracy_issue%20(2).ipynb ```  Relevant log output ```shell ```",2025-02-23T06:34:05Z,stat:awaiting response type:bug stale type:others comp:keras TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/87893,"Hi **** , Please post this issue on kerasteam/keras repo. as this issue is more related to keras Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change of visibility,Internal change of visibility,2025-02-22T20:22:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87879
sharding,copybara-service[bot],"Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `all-gather` with an `all-to-all`.","Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `allgather` with an `alltoall`. Given the following input ``` ENTRY entry {   %param0 = f32[14,257] parameter(0), sharding={devices=[1,2]0,1}   %param1 = f32[14,116] parameter(1), sharding={devices=[1,2]0,1}   ROOT %concatenate = f32[14,373] concatenate(%param0, %param1),     dimensions={1}, sharding={devices=[1,2]0,1} } ``` Previously, we (1) replicate the input along the concat dimension, (2) apply concat, (3) partition the result with dynamicslice. With this change, we (1) use alltoall to move sharding axis from the concat dim to batch dim for operands, (2) apply concat, and then (3) use alltoall to reshard the result. Reverts 81b0a48fcf8618fdab0a03907b05a65413399585",2025-02-22T01:34:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87841
tpu,copybara-service[bot],Expose ExecutableBuildOptions::CompilationEnvironments::CreateFromProto to python,Expose ExecutableBuildOptions::CompilationEnvironments::CreateFromProto to python Add a default TpuCompilationEnvironment to the wiz export,2025-02-22T00:21:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87831
quantization,LaithMustafa,spam," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible). ``` (You can paste links or attach files by dragging & dropping them below)  Provide links to your updated versions of the above two colab notebooks.  Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model. ```  Option B: Paste your code here or provide a link to a custom endtoend colab ``` (You can paste links or attach files by dragging & dropping them below)  Include code to invoke the TFLite Converter Python API and the errors.  Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model. ```  3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2025-02-21T18:02:20Z,invalid TFLiteConverter,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87796,Please don't spam.
opt,copybara-service[bot],Fix invalid pointer in environment_options,Fix invalid pointer in environment_options,2025-02-21T15:51:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87786
opt,copybara-service[bot],"[XLA:Python] Make sure we hold the GIL while writing to the ""view"" passed to a bf_getbuffer method.","[XLA:Python] Make sure we hold the GIL while writing to the ""view"" passed to a bf_getbuffer method. See https://github.com/python/cpython/issues/130409 . We need to prevent concurrent GC traversals at least under freethreading mode to avoid a data race. The same APIs that hold the GIL under nonfreethreading builds also have the effect of preventing concurrent garbage collection under freethreading mode: GC is a stoptheworld process that requires that all active Python threads be suspended. Fixes https://github.com/jaxml/jax/issues/26305",2025-02-21T15:32:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87784
opt,copybara-service[bot],PR #87613: Explicit print options while dumping HLO,"PR CC(Explicit print options while dumping HLO): Explicit print options while dumping HLO Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/87613 We are changing the `HloModule::ToString` function to consider the debug options related to printing like, `xla_dump_hlo_as_long_text`, `xla_dump_large_constants` etc. These options are already embedded into the `HloModule` and are set via the command line and so, the `ToString` function should default to these functions.  We are also changing the default value of HloPrintOptions: `set_print_operand_shape(false)` and `set_print_large_constants(true)`. The related changes in XLA are https://github.com/openxla/xla/pull/22614 and https://github.com/openxla/xla/pull/22800. This breaks some of the tests in tensorflow because it relies on the `ToString` function and the default print options. As a temporary fix, we are making the default print options explicit here. Once the XLA change gets merged, we can remove this and fix the tests (or not, depending on what the TF community decides, but the behavior of the tool will be intentional.) Copybara import of the project:  fe5a22699723a33292c4ebdbe186a41cf47217c0 by Shraiysh Vaishay : Explicit print options while dumping HLO We are changing the `HloModule::ToString` function to consider the debug options related to printing like, `xla_dump_hlo_as_long_text`, `xla_dump_large_constants` etc. These options are already embedded into the `HloModule` and are set via the command line and so, the `ToString` function should default to these functions. We are also changing the default value of HloPrintOptions: `set_print_operand_shape(false)` and `set_print_large_constants(true)`. The related changes in XLA are https://github.com/openxla/xla/pull/22614 and https://github.com/openxla/xla/pull/22800. This breaks some of the tests in tensorflow because it relies on the `ToString` function and the default print options. As a temporary fix, we are making the default print options explicit here. Once the XLA change gets merged, we can remove this and fix the tests (or not, depending on what the TF community decides, but the behavior of the tool will be intentional.) Merging this change closes CC(Explicit print options while dumping HLO) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/87613 from shraiysh:fixxlatests fe5a22699723a33292c4ebdbe186a41cf47217c0",2025-02-21T08:45:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87751
yi,copybara-service[bot],Internal change for visibility,Internal change for visibility,2025-02-21T05:49:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87734
opt,copybara-service[bot],Add a flatbuffer util (python) function for getting the builtin options as a given type.,Add a flatbuffer util (python) function for getting the builtin options as a given type.,2025-02-21T04:58:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87731
int8,copybara-service[bot],litert: Add uint8 support,litert: Add uint8 support,2025-02-21T01:37:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87730
opt,copybara-service[bot],XLA Tooling: Improve and Update the documentation to include new features introduced at the tool. ,"XLA Tooling: Improve and Update the documentation to include new features introduced at the tool.  1. changed Heading format to [`tool name`]  2. Added missing  how to build a binary instruction for each tool.  3. Added hloopt tool new feature documentation.  4. Updated hloopt tool old features documentation to clarify, it is deviceless compilation, corrected paths, links.",2025-02-21T00:41:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87726
tpu,copybara-service[bot],Fix CreateInput/OutputBuffers in compiled model test.,Fix CreateInput/OutputBuffers in compiled model test.,2025-02-21T00:04:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87723
sharding,copybara-service[bot],[HLO->MHLO] Don't validate mhlo.token sharding mismatches.,"[HLO>MHLO] Don't validate mhlo.token sharding mismatches. Legacy programs pass a tuple of multiple sharding values for a single mhlo.token output. When we use HLO>MHLO import with tuple flattening, this check breaks since there are more than one flattened tuple values, and only one function result.",2025-02-20T23:25:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87719
yi,copybara-service[bot],"Adds visibility restriction to some XLA bzl files to prevent them from being used outside of XLA, as they are internal implementation details.","Adds visibility restriction to some XLA bzl files to prevent them from being used outside of XLA, as they are internal implementation details. This CL is not complete. It's the first step that establishes the mechanism. Once I get buyin on the approach, I'll follow up with more CLs to add visibility restriction to the other XLA bazl files.",2025-02-20T23:21:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87718
opt,copybara-service[bot],[tsl:concurrency] Micro optimizations for AsyncValue::AndThen,"[tsl:concurrency] Micro optimizations for AsyncValue::AndThen Instead of relying on absl::AnyInvocable keep Waiter directly in the linked list node, this improves performance by: 1. Avoiding one extra heap allocation for the absl::AnyInvocable 2. Remove one pointer indirection in RunWaiters BEFORE:  Benchmark                      Time             CPU   Iterations  BM_AddAndThenCallback       20.3 ns         20.3 ns     34354892 AFTER:  Benchmark                      Time             CPU   Iterations  BM_AddAndThenCallback       12.2 ns         12.2 ns     57932249",2025-02-20T22:58:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87714
quantization,Schnipper24,Direction Problem after creating model / So after sucsessfully creating my Model and saving it automatic in the folder i need i get this message," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64Bit  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source): abslpy==1.0.0 appdirs==1.4.4 astunparse==1.6.3 attrs==22.1.0 audioread==3.0.0 cachetools==5.2.0 certifi==2022.9.24 cffi==1.15.1 chardet==5.0.0 charsetnormalizer==2.1.1 colorama==0.4.5 contourpy==1.0.5 cycler==0.11.0 Cython==0.29.32 dataclasses==0.6 decorator==5.1.1 dill==0.3.5.1 dmtree==0.1.7 etils==0.8.0 fire==0.4.0 flatbuffers==1.12 fonttools==4.37.4 gast==0.4.0 ginconfig==0.5.0 googleapicore==2.8.2 googleapipythonclient==2.64.0 googleauth==2.12.0 googleauthhttplib2==0.1.0 googleauthoauthlib==0.4.6 googlecloudbigquery==3.3.3 googlecloudbigquerystorage==2.16.0 googlecloudcore==2.3.2 googlecrc32c==1.5.0 googlepasta==0.2.0 googleresumablemedia==2.4.0 googleapiscommonprotos==1.56.4 grpcio==1.48.2 grpciostatus==1.48.2 h5py==3.1.0 httplib2==0.20.4 idna==3.4 importlibmetadata==5.0.0 importlibresources==5.9.0 joblib==1.2.0 kaggle==1.5.12 keras==2.9.0 kerasnightly==2.5.0.dev2021032900 KerasPreprocessing==1.1.2 kiwisolver==1.4.4 labelImg==1.8.6 libclang==14.0.6 librosa==0.8.1 llvmlite==0.36.0 lml==0.1.0 lxml==4.9.1 Markdown==3.4.1 MarkupSafe==2.1.1 matplotlib==3.4.3 neuralstructuredlearning==1.4.0 numba==0.53.0  2. Code Provide code to help us reproduce your issues using one of the following options:  https://www.tensorflow.org/lite/tutorials/model_maker_object_detection  https://github.com/tzutalin/labelImg import sys, getopt, time, pathlib from tflite_model_maker.config import ExportFormat from tflite_model_maker.config import QuantizationConfig from tflite_model_maker import model_spec from tflite_model_maker import object_detector def main(argv):     dir = None     batch_size = 8     epochs = 50     try:         opts, _ = getopt.getopt(argv, ""hd:b:e:"", [""help"",""directory="",""batchSize="",""epochs=""])         for opt, arg in opts:             print(opt + "":"" + arg)             if opt == ""h, help"":                 raise Exception()             elif opt in (""d"", ""directory""):                 dir = str(arg)             elif opt in (""b"", ""batchSize""):                 batch_size = int(arg)             elif opt in (""e"", ""epochs""):                 epochs = int(arg)         if (dir is None):             raise Exception()     except Exception:         print(""Specify a directory that contains your dataset."")         print(""createmodel.py d "")         sys.exit(2)     start = round(time.time() * 1000)      select object recognition model architecture     spec = model_spec.get(""efficientdet_lite0"")     spec.config.var_freeze_expr = ""(efficientnetresample_p6)""     spec.config.tflite_max_detections = 25     print(spec.config)      load input data specific to an ondevice ML app     train_data, validation_data, test_data = object_detector.DataLoader.from_csv(dir + ""/dataset.csv"")      customize the TensorFlow model     model = object_detector.create(         train_data,         model_spec=spec,         batch_size=batch_size,         epochs=epochs,         train_whole_model=False     )     model.summary()      evaluate the model     print(model.evaluate(test_data))      export to Tensorflow Lite model and label file in `export_dir`     path = pathlib.PurePath(dir)     model.export(export_dir=""build/"" + path.name + ""/"")     model.export(export_dir=""build/"" + path.name + ""/"", export_format=ExportFormat.LABEL)      evaluate the tensorflow lite model     print(model.evaluate_tflite(""build/"" + path.name + ""/model.tflite"", test_data))     stop = round(time.time() * 1000)     print(""process image: {} ms"".format(stop  start)) if __name__ == ""__main__"":    main(sys.argv[1:]) **So after sucsessfully creating my Model and saving it automatic in the folder i need i get this message:** Traceback (most recent call last):   File ""C:\Users\Robert\Desktop\IT_FischerTechnik_Klotz\Robert_KI\machinelearning\objectdetection\createmodel.py"", line 75, in      main(sys.argv[1:])   File ""C:\Users\Robert\Desktop\IT_FischerTechnik_Klotz\Robert_KI\machinelearning\objectdetection\createmodel.py"", line 69, in main     print(model.evaluate_tflite(""build/"" + path.name + ""/model.tflite"", test_data))   File ""C:\Users\Robert\AppData\Local\Programs\Python\Python39\lib\sitepackages\tensorflow_examples\lite\model_maker\core\task\object_detector.py"", line 155, in evaluate_tflite     return self.model_spec.evaluate_tflite(tflite_filepath, ds, len(data),   File ""C:\Users\Robert\AppData\Local\Programs\Python\Python39\lib\sitepackages\tensorflow_examples\lite\model_maker\core\task\model_spec\object_detector_spec.py"", line 373, in evaluate_tflite     lite_runner = eval_tflite.LiteRunner(tflite_filepath, only_network=False)   File ""C:\Users\Robert\AppData\Local\Programs\Python\Python39\lib\sitepackages\tensorflow_examples\lite\model_maker\third_party\efficientdet\keras\eval_tflite.py"", line 67, in __init__     self.interpreter = tf.lite.Interpreter(tflite_model_path)   File ""C:\Users\Robert\AppData\Local\Programs\Python\Python39\lib\sitepackages\tensorflow\lite\python\interpreter.py"", line 455, in __init__     _interpreter_wrapper.CreateWrapperFromFile( ValueError: Could not open 'build/ALLE_Klötze/model.tflite'. **I can even see the saved Model but still it says there is no file. I really dont know how to make it. (ROOKIE)** !Image",2025-02-20T22:29:17Z,TFLiteConverter,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87708,I Guess i found the Answer by myself and changed the folder from Klötze to Kloetze and it was done .
yi,copybara-service[bot],#litert Add a automatically added accelerator compilation structure.,litert Add a automatically added accelerator compilation structure. This structure allows passing metadata that is generated during the model compilation onto accelerators when they alter the underlying runtime.,2025-02-20T21:54:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87705
opt,copybara-service[bot],[HLO-OPT] Add `--emit-proto` flag to convert input hlo in text to proto format.,[HLOOPT] Add `emitproto` flag to convert input hlo in text to proto format.,2025-02-20T20:26:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87698
sharding,copybara-service[bot],[XLA:SPMD] Optimize the partitioning for element-wise operations when all operands share the same sharding.,"[XLA:SPMD] Optimize the partitioning for elementwise operations when all operands share the same sharding. Let us take `C with S3 = add(A with S1, B with S2)` as an example, where A, B, C are tensors, S1, S2, S3 are their shardings. Before this change, we always have ``` A with S3 = reshard(A with S1, new_sharding=S3) B with S3 = reshard(B with S2, new_sharding=S3) C with S3 = add(A with S3, B with S3) ``` With this cl, if S1 and S2 are the same, we will have ``` C with S1 = add(A with S1, B with S1) C with S3 = reshard(C with S1) ``` The new partitioning method can reduce the number of reshards.",2025-02-20T19:10:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87692
tpu,copybara-service[bot],[XLA:GPU] only store information about symbol dimension tiling for triton nested fusion,"[XLA:GPU] only store information about symbol dimension tiling for triton nested fusion Previously we were storing tiling sizes for both contracting and noncontracting dimensions for a nested fusion. Unfortunatelly this information is both redundant (as tiling for noncontracting dimension will be set by the outer fusion) and missing information ""which of the dimension is contracting?"" to pick the tile size. Later, in tiling analysis we will have to walk the tree again and find dot for each fusion to understand what tiling parameter we want to set for the new symbol (K). If we store _only_ the contracting dimension then we should have an easier time  tiling for contracting dimension is the only element of the output_tile_sizes of the nested fusion.",2025-02-20T18:29:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87689
opt,copybara-service[bot],Make tensorflow-io-gcs-filesystem an optional dependency.,"Make tensorflowiogcsfilesystem an optional dependency. The dependency has been causing a number of issues, see: https://github.com/tensorflow/tensorflow/pull/82771 The support for the package is currently uncertain, and has been on the low end for a while  no Windows wheels since 0.32.0, for example. It has been capped on <py3.12 for a while now also.",2025-02-20T18:25:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87687
opt,vivkong,Fix compile error in tensorflow/python/tfcompile_wrapper.cc on s390x,This fixes the following error on s390x: ``` ImportError: Unable to import _pywrap_tfcompile; you must build TensorFlow with XLA.  You may need to build tensorflow with flag define=with_xla_support=true.  Original error: /home/test/.cache/bazel/_bazel_test/3a51eda24c560ebddf29b66b8fa0460e/execroot/org_tensorflow/bazelout/s390xoptexec50AE0418/bin/tensorflow/python/tools/saved_model_cli.runfiles/org_tensorflow/tensorflow/python/_pywrap_tfcompile.so: undefined symbol: _ZN4llvm3sys22getDefaultTargetTripleB5cxx11Ev ```,2025-02-20T14:00:04Z,ready to pull size:XS,open,0,9,https://github.com/tensorflow/tensorflow/issues/87676,"Hi, wondering if the checks can be kicked off again?   I ran the tests against master and didn't encounter errors.  Thanks for your help.","So, the error here is that copybara import into the internal system has failed. Can you try to rebase the PR on top of the current master branch? Maybe that would nudge copybara to retry",Ah I see.  Thanks for the info!  I've rebased on top of master  hope it does the trick.,Same type of error.  can you look into why copybara import fails here? I don't have access to those logs.,It doesn't like the Build file: `.../tensorflow/python/BUILD:504:2: syntax error near )` Is there an extra right parentheses there? ,"Nice catch. Yeah, there was a duplicated `)`",Thanks  and  for your help.  Fixed the change.  Please help kick off the checks.  Thanks!, Sorry it looks like there's a failure.  I don't have access to the failure.  Please let me know if I can make a change to help merge this.  Thanks.,"There is an issue with the internal version of the code, given a slightly different syntax between Bazel and the internal equivalent. Nothing for you to do here, we just need to bridge the internal gap on the internal version of this."
tpu,Kakarute,"[WSL2 + Docker + TensorFlow] CUDA failed to initialize in TensorFlow container (RTX 5090, CUDA 12.8)"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.2 LTS (WSL2)  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 12.8, cuDNN 8.x (inside TensorFlow container)  GPU model and memory NVIDIA GeForce RTX 5090, 32GB VRAM  Current behavior? 1. System Information OS: Windows 11 Pro (WSL2) WSL2 Distro: Ubuntu 22.04 GPU: NVIDIA GeForce RTX 5090 (VRAM 32GB) NVIDIA Driver Version: 572.16 CUDA Version: 12.8 NVIDIA Container Toolkit Version: Latest Docker Version: 26.1.3 TensorFlow Container: nvcr.io/nvidia/tensorflow:25.01tf2py3 2. Steps to Reproduce & Issue Description ✅ 1) NVIDIA driver and GPU recognition check in WSL2 I first verified that the NVIDIA GPU is recognized correctly in WSL2 by running: nvidiasmi ✅ Output (GPU recognized properly) Thu Feb 20 17:07:18 2025 ±+  ±±±+ ✅ This confirms that NVIDIA drivers and CUDA work correctly in the WSL2 + Docker environment. ❌ 3) Issue: TensorFlow container fails to initialize CUDA I then launched the TensorFlow container and checked if CUDA was accessible. bash docker run rm gpus all it nvcr.io/nvidia/tensorflow:25.01tf2py3 bash ✅ Successfully entered the TensorFlow container == TensorFlow == NVIDIA Release 25.01tf2 (build 134984172) TensorFlow Version 2.17.0 Container image Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved. … ❌ However, CUDA failed to initialize with the following error: ERROR: The NVIDIA Driver is present, but CUDA failed to initialize. GPU functionality will not be available. [[ Named symbol not found (error 500) ]] Next, I checked if TensorFlow inside the container recognized the GPU by running: python3 c “import tensorflow as tf; print(tf.config.list_physical_devices(‘GPU’))” ❌ Output TensorFlow does not detect any GPUs. 3. Debugging Attempts 1️⃣ Checked for libcuda.so inside the container find /usr name “libcuda.so*” ✅ Output (Files are present) /usr/local/cuda12.8/compat/lib.real/libcuda.so /usr/local/cuda12.8/compat/lib.real/libcuda.so.1 /usr/local/cuda12.8/compat/lib.real/libcuda.so.570.86.10 /usr/lib/x86_64linuxgnu/libcuda.so /usr/lib/x86_64linuxgnu/libcuda.so.1 2️⃣ Checked environment variables export LD_LIBRARY_PATH=/usr/lib/x86_64linuxgnu:/usr/local/cuda12.8/compat/lib.real:$LD_LIBRARY_PATH Still, the same error persists. 3️⃣ Ran TensorFlow container with modified options bash docker run gpus all ipc=host ulimit memlock=1 ulimit stack=67108864 it nvcr.io/nvidia/tensorflow:25.01tf2py3 bash 🚨 Still, CUDA failed to initialize error persists. 4. Summary & Questions Findings so far: NVIDIA driver and CUDA work correctly in WSL2 and Docker. CUDA container (nvidia/cuda:12.3.0baseubuntu22.04) detects GPU properly. However, TensorFlow container (nvcr.io/nvidia/tensorflow:25.01tf2py3) fails to initialize CUDA. TensorFlow does not detect GPU (tf.config.list_physical_devices('GPU') returns []). Questions: Is the combination of NVIDIA RTX 5090 + CUDA 12.8 + TensorFlow 2.17.0 expected to work in WSL2? What could be causing CUDA failed to initialize in the TensorFlow container while it works fine in the CUDA container? Are there any additional configurations required to make TensorFlow detect the GPU inside the container? ✅ Looking for guidance on debugging and fixing this issue. Any insights or suggestions would be greatly appreciated!  Standalone code to reproduce the issue ```shell be included in the above content ```  Relevant log output ```shell ```",2025-02-20T10:28:50Z,stat:awaiting response type:bug stale comp:gpu 2.17,closed,0,8,https://github.com/tensorflow/tensorflow/issues/87669,"Hi **** , Apologies for the delay, and thanks for raising your concern here. This issue might be related to compatibility. I noticed a version mismatch between CUDA and TensorFlow. Could you please verify that you are using compatible versions to avoid errors? I am providing the official documentation for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,I have the same problem. New version for RTX50 series are needed because of CUDA 12.8. Tensorflow 2.18 only support cuda 12.5,"I have a working (buggy as hell) build, it might help you, maybe, full details in my Issue (I'm not using Docker, so it may not solve your issue): https://github.com/tensorflow/tensorflow/issues/89272",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,it worsk on my pc :D chatgpt 4.0 helps me that it work and only with chatgpt it work xD so hard xD i have windows 11 pro then wsl2 ubuntu 24.04 then linux installeiton docker.. tensorflow 2.17.0 with toolkit 2.18.1 ctreaderopenapi etc thje container: et c Kakarute opened on Feb 20 the same solve lkike ghis :D i have chatgpt give his post an schatgpt made it for me that it run xD 5090 master my KI that trade on forex stock runs python tensorflow perfectly stabile and only chatgpt know how it work xD blup greeze insta: stonypiles
tpu,caofx0418,LiteRT delegate for Pixel Phones (Soc Tensor G2/G3/G4) TPUs,"Before Android 15, LiteRT nnapi delegate could accelerate NN models.  But after the release of Android 15, NNAPI will be deprecated. Compare to Qualcomm SOC, it offers QNN TFlite Delegate. Will the Pixel Phones offer TPU delegate ?",2025-02-20T09:25:29Z,type:support comp:lite,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87661,"Hi,   Thank you for reporting this issue. I apologize for the delayed response.  As this appears to be a duplicate of issue https://github.com/googleaiedge/LiteRT/issues/969 in the LiteRT repository (https://github.com/googleaiedge/LiteRT/issues/969), I will close this issue to consolidate the discussion.  Please refer to https://github.com/googleaiedge/LiteRT/issues/969 for updates and further information. Thank you for your understanding."
attention,copybara-service[bot],PR #22826: [XLA:GPU] Only enable cuDNN flash attention segment mask generation on hopper and above,"PR CC(Win10 C++ TF1.9, error LNK2001, build by bazel  !): [XLA:GPU] Only enable cuDNN flash attention segment mask generation on hopper and above Imported from GitHub PR https://github.com/openxla/xla/pull/22826 Copybara import of the project:  8985672fa3a7bb543624a1e5cafbd2975e3c1214 by cjkkkk : init Merging this change closes CC(Win10 C++ TF1.9, error LNK2001, build by bazel  !) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22826 from Cjkkkk:enable_segment_packing_only_on_hopper 8985672fa3a7bb543624a1e5cafbd2975e3c1214",2025-02-20T07:29:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87651
opt,shraiysh,Explicit print options while dumping HLO,"We are changing the `HloModule::ToString` function to consider the debug options related to printing like, `xla_dump_hlo_as_long_text`, `xla_dump_large_constants` etc. These options are already embedded into the `HloModule` and are set via the command line and so, the `ToString` function should default to these functions.  We are also changing the default value of HloPrintOptions: `set_print_operand_shape(false)` and `set_print_large_constants(true)`. The related changes in XLA are https://github.com/openxla/xla/pull/22614 and https://github.com/openxla/xla/pull/22800. This breaks some of the tests in tensorflow because it relies on the `ToString` function and the default print options. As a temporary fix, we are making the default print options explicit here. Once the XLA change gets merged, we can remove this and fix the tests (or not, depending on what the TF community decides, but the behavior of the tool will be intentional.)",2025-02-20T00:27:21Z,awaiting review ready to pull comp:xla size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87613
sharding,copybara-service[bot],xla::ShardingPropagation. Avoid applying sharding constraints if the operand is used by multiple sharding constraints with different shardings.,"xla::ShardingPropagation. Avoid applying sharding constraints if the operand is used by multiple sharding constraints with different shardings. With `B = customcall(A), custom_call_target=""Sharding""`, we can set the sharding for A when all the three conditions are true. 1. Unspecified dims are empty. Otherwise, the sharding is open and can be further modified. 2. A has no sharding. We cannot overwrite the existing one. 3. A does not have other sharding constraints. A can have multiple sharding constraints with the same sharding. The first two conditions are checked before this cl. This cl add the third condition.",2025-02-20T00:20:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87611
opt,copybara-service[bot],Add a test for the `rng-bit-generator-expander` HLO optimization pass.,Add a test for the `rngbitgeneratorexpander` HLO optimization pass. This increases `rngbitgeneratorexpander` coverage from about 4% to about 89%.,2025-02-19T23:59:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87609
sharding,copybara-service[bot],[xla:python] Relax checks in custom call batch partitioner when some inputs are not sharded.,"[xla:python] Relax checks in custom call batch partitioner when some inputs are not sharded. The current behavior is to crash sharding propagation when some inputs to a batch partitionable FFI call don't have an associated sharding, but we really only require that at least on operand is sharded, and should perform the default action otherwise.",2025-02-19T23:45:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87607
tpu,disgoraj,fix-bug : add more datatype,This PR fixes issue CC(tf.raw_ops.Round outputs zeros for any integer tensor) by adding int32 and int64 support for the Round operation.,2025-02-19T22:38:17Z,stat:awaiting response stale size:XS comp:core,closed,0,4,https://github.com/tensorflow/tensorflow/issues/87600,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi  , Can you please sign CLA , thank you",This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,This PR was closed because it has been inactive for 14 days since being marked as stale. Please reopen if you'd like to work on this further.
tensorrt,copybara-service[bot],PR #22829: Add GCC Version Detection and Enforce XNNPACK Compatibility Flags,PR CC(can tensorflow tensorRT (with multiple outputs) be used for object dectection? can ): Add GCC Version Detection and Enforce XNNPACK Compatibility Flags Imported from GitHub PR https://github.com/openxla/xla/pull/22829  Description:    Added `_get_gcc_major_version` function to `configure.py` to determine the major version of GCC.    Adjusted XNNPACK build flags dynamically based on detected Clang and GCC versions:      Disabled `mavxvnniint8` for Clang : Add GCC Version Detection and Enforce XNNPACK Compatibility Flags Merging this change closes CC(can tensorflow tensorRT (with multiple outputs) be used for object dectection? can ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22829 from apivovarov:gcc_ver_flags a44a5d6ebd5fbdbc544f97dc7cc00fbe03c6eba7,2025-02-19T22:01:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87595
opt,copybara-service[bot],"[hlo-opt] Produce fatal message if pass already registered. There are multiple codepath via a HLO pass can be registered to the hlo-opt pass registry. To avoid multiple registrations, produce a fatal error.","[hloopt] Produce fatal message if pass already registered. There are multiple codepath via a HLO pass can be registered to the hloopt pass registry. To avoid multiple registrations, produce a fatal error. also it will help to identify if a user adds new pass with a same name as the existing pass.",2025-02-19T20:49:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87589
tpu,tensorflow-jenkins,"r2.19 cherry-pick: 2b775610a7b ""Update libtpu installation index path""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/2b775610a7b49450f406933e7a96d06ff37e3bd3,2025-02-19T19:54:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87586
tpu,copybara-service[bot],Expose ExecutableBuildOptions::CompileOptions and the method AddEnv to python,Expose ExecutableBuildOptions::CompileOptions and the method AddEnv to python Add a default TpuCompilationEnvironment to the wiz export,2025-02-19T18:10:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87581
opt,copybara-service[bot],Enable `AddOptimizationPasses` when `quant_specs.strict_qdq_mode` is true.,"Enable `AddOptimizationPasses` when `quant_specs.strict_qdq_mode` is true. * This includes ewise optimization, broadcast optimization, batchmatmul optimization and optimize pass. * Add a missing case for int4 in `lower_quant_annotations_helper`.",2025-02-19T16:18:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87576
quantization,copybara-service[bot],"Fix per-axis quantization of TFL::TransposeOp. This is currently hard-coded to I8FP32, making this generic to support any per-axis quant types.","Fix peraxis quantization of TFL::TransposeOp. This is currently hardcoded to I8FP32, making this generic to support any peraxis quant types.",2025-02-19T16:09:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87575
opt,copybara-service[bot],PR #22800: Change the default value of print_operand_shape_ to false and print_large_constants_ to true.,"PR CC(Variance Inflation Factor estimate for each layer as guidance for dropout): Change the default value of print_operand_shape_ to false and print_large_constants_ to true. Imported from GitHub PR https://github.com/openxla/xla/pull/22800 Operand shape in long hlo text adds redundant information, which shouldn't be required. Changing the default value to off. The large constants were also printed earlier by default print options, and it is required for parsability and reproducibility. Turning this on by default. This is still controlled by debug option and the default value of that flag disables the large constants, and that behavior is not changed. Just the default print options change here. Copybara import of the project:  7d2427432c6924becc5df8276fd01991d129a5ed by Shraiysh Vaishay : Change the default value of print_operand_shape_ to false and print_large_constants_ to true. Operand shape in long hlo text adds redundant information, which shouldn't be required. Changing the default value to off. The large constants were also printed earlier by default print options, and it is required for parsability and reproducibility. Turning this on by default. This is still controlled by debug option and the default value of that flag disables the large constants, and that behavior is not changed. Just the default print options change here. Merging this change closes CC(Variance Inflation Factor estimate for each layer as guidance for dropout) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22800 from shraiysh:change_default_print_op_shape 7d2427432c6924becc5df8276fd01991d129a5ed",2025-02-19T15:48:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87572
sharding,copybara-service[bot],Add cache entries for reshape ops in SPMD.,"Add cache entries for reshape ops in SPMD. We may have two compatible sharding pairs when handling reshape. If we have two pairs, we use the first one. We can still use the second one to add a reshard cache. Given the following reshape, ``` p0 = bf16[8,8] parameter(0), sharding={replicated} reshape = bf16[64] reshape(p0), sharding={devices=[4] (bf16[16], bf16[64]) {   %param = bf16[8,8]{1,0} parameter(0), sharding={replicated}   %constant = s32[4]{0} constant({0, 2, 4, 6})   %partitionid = u32[] partitionid()   %dynamicslice = s32[1]{0} dynamicslice(s32[4]{0} %constant, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.1 = s32[] reshape(s32[1]{0} %dynamicslice)   %constant.1 = s32[] constant(0)   %dynamicslice.1 = bf16[2,8]{1,0} dynamicslice(bf16[8,8]{1,0} %param, s32[] %reshape.1, s32[] %constant.1), dynamic_slice_sizes={2,8}   %reshape.2 = bf16[16]{0} reshape(bf16[2,8]{1,0} %dynamicslice.1)   %allgather = bf16[64]{0} allgather(bf16[16]{0} %reshape.2), channel_id=1, replica_groups={{0,1,2,3}}, dimensions={0}, use_global_device_ids=true   %abs.1 = bf16[64]{0} abs(bf16[64]{0} %allgather)   ROOT %tuple.1 = (bf16[16]{0}, bf16[64]{0}) tuple(bf16[16]{0} %reshape.2, bf16[64]{0} %abs.1) } ``` With this change, we replace reshard (allgather) with reshape ``` ENTRY %reshape_spmd (param: bf16[8,8]) > (bf16[16], bf16[64]) {   %param = bf16[8,8]{1,0} parameter(0), sharding={replicated}   %constant = s32[4]{0} constant({0, 2, 4, 6})   %partitionid = u32[] partitionid()   %dynamicslice = s32[1]{0} dynamicslice(s32[4]{0} %constant, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.1 = s32[] reshape(s32[1]{0} %dynamicslice)   %constant.1 = s32[] constant(0)   %dynamicslice.1 = bf16[2,8]{1,0} dynamicslice(bf16[8,8]{1,0} %param, s32[] %reshape.1, s32[] %constant.1), dynamic_slice_sizes={2,8}   %reshape.2 = bf16[16]{0} reshape(bf16[2,8]{1,0} %dynamicslice.1)   %reshape.3 = bf16[64]{0} reshape(bf16[8,8]{1,0} %param)   %abs.1 = bf16[64]{0} abs(bf16[64]{0} %reshape.3)   ROOT %tuple.1 = (bf16[16]{0}, bf16[64]{0}) tuple(bf16[16]{0} %reshape.2, bf16[64]{0} %abs.1) } ```",2025-02-19T08:19:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87533
tpu,copybara-service[bot],Update libtpu installation index path,"Update libtpu installation index path Update the libtpu installation index to https://storage.googleapis.com/libtpuwheels/index.html, which includes both stable and nightly libtpu versions, as per the cloud libtpu team's guidance. Also update the libtpu version in the setup.py. It starts to differ from the TF version to support JAX, and it requires manual updates for new releases for now.",2025-02-19T04:47:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87524
opt,copybara-service[bot],Add a test for the `rng-expander` HLO optimization pass.,Add a test for the `rngexpander` HLO optimization pass.,2025-02-19T01:50:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87521
opt,copybara-service[bot],Simplify heartbeat options for PjRt distributed runtime.,Simplify heartbeat options for PjRt distributed runtime.,2025-02-19T01:10:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87516
int8,copybara-service[bot],"Remove `build --define=xnn_enable_avxvnniint8=false` from top-level .bazelrc, add it to relevant MacOS builds","Remove `build define=xnn_enable_avxvnniint8=false` from toplevel .bazelrc, add it to relevant MacOS builds This was originally an imprecise change from me, adding this in the configure.py generated bazelrc will be handled in https://github.com/openxla/xla/pull/22829.",2025-02-19T00:47:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87514
tensorrt,copybara-service[bot],PR #22830: Fix macOS build.,PR CC(can tensorflow tensorRT be used for  optimization of object detection model): Fix macOS build. Imported from GitHub PR https://github.com/openxla/xla/pull/22830 This fixes builds on macOS without Xcode (with Clang from Homebrew for example). Copybara import of the project:  4b5afd519ff62e8fe9d013030789edfc56ad6110 by Ilia Sergachev : Update apple_support to 1.18.0.  9a9c42f6836ce758add28550cf157552a426ae92 by Ilia Sergachev : Add bazel_features 1.25.0. Merging this change closes CC(can tensorflow tensorRT be used for  optimization of object detection model) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22830 from openxla:fix_osx_build 9a9c42f6836ce758add28550cf157552a426ae92,2025-02-19T00:34:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87512
sharding,copybara-service[bot],Move some strategy generation utilities from auto_sharding_dot_handler.cc to,Move some strategy generation utilities from auto_sharding_dot_handler..h with the intention of using the utilities more broadly throughout the codebase. Reverts 1e0f639a26b2aafd2732ae0e64817b7cdd387c81,2025-02-18T21:25:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87505
opt,copybara-service[bot],[PJRT:CPU] Support per-execution process_index and collectives overrides,"[PJRT:CPU] Support perexecution process_index and collectives overrides This changes extends `xla::ExecuteContext` as a new subclass `xla::CpuExecuteContext` recognized by the PjRt CPU client. `xla::CpuExecuteContext` accepts optional `process_index` and `collectives`. When specified, each configuration will override `process_index` and/or `collectives` instead of using the value specified at CPU client construction time. The goal of these overrides is to allow reconfiguring collectives while endpoints can be dynamically added or removed. Since reconfiguration does not require recreating the CPU client, any user state can be preserved across collectives reconfiguration and reduce the overall system reconfiguration cost by not having to checkpoint the state outside of the CPU client.",2025-02-18T20:56:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87502
opt,copybara-service[bot],PR #22800: Change the default value of print_operand_shape_ to false and print_large_constants_ to true.,"PR CC(Variance Inflation Factor estimate for each layer as guidance for dropout): Change the default value of print_operand_shape_ to false and print_large_constants_ to true. Imported from GitHub PR https://github.com/openxla/xla/pull/22800 Operand shape in long hlo text adds redundant information, which shouldn't be required. Changing the default value to off. The large constants were also printed earlier by default print options, and it is required for parsability and reproducibility. Turning this on by default. This is still controlled by debug option and the default value of that flag disables the large constants, and that behavior is not changed. Just the default print options change here. Copybara import of the project:  e30dea20489b3fb4d03d373fec0391d69486f4aa by Shraiysh Vaishay : Change the default value of print_operand_shape_ to false and print_large_constants_ to true. Operand shape in long hlo text adds redundant information, which shouldn't be required. Changing the default value to off. The large constants were also printed earlier by default print options, and it is required for parsability and reproducibility. Turning this on by default. This is still controlled by debug option and the default value of that flag disables the large constants, and that behavior is not changed. Just the default print options change here.  7008af0dd0ce342ecbe9475f1d0e277319f1705a by Shraiysh Vaishay : Handle tests  b22d5f95cfb7e15f930a2198279a76c38593cc53 by Shraiysh Vaishay : Fix more tests  d51579cae7359c6426a87ad4a7ff1b4b0c80f74a by Shraiysh Vaishay : Fix more tests Merging this change closes CC(Variance Inflation Factor estimate for each layer as guidance for dropout) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22800 from shraiysh:change_default_print_op_shape d51579cae7359c6426a87ad4a7ff1b4b0c80f74a",2025-02-18T19:39:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87498
sharding,copybara-service[bot],[Dtensor] Remove MHLO dependency from DTensor. ,[Dtensor] Remove MHLO dependency from DTensor.  There are uses of mhlo.sharding but it is just a string as a dictionary key. We are not planning to change it for now.,2025-02-18T19:20:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87497
opt,copybara-service[bot],#litert Pass the compilation options to accelerators' `CreateDelegate()`.,litert Pass the compilation options to accelerators' `CreateDelegate()`.,2025-02-18T18:54:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87494
opt,copybara-service[bot],#litert Simplify `LiteRtAppendAcceleratorCompilationOptions()`'s usage.,"litert Simplify `LiteRtAppendAcceleratorCompilationOptions()`'s usage. This change allows passing the head of the list as a `nullptr` instead of having to manually set it. Before: ```cpp if (!options>accelerator_compilation_options) {   options>accelerator_compilation_options = accelerator_compilation_options; } else {   LiteRtAppendAcceleratorCompilationOptions(       options>accelerator_compilation_options,       accelerator_compilation_options); } ``` After ```cpp LiteRtAppendAcceleratorCompilationOptions(     &options>accelerator_compilation_options,     accelerator_compilation_options); ```",2025-02-18T18:53:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87493
opt,copybara-service[bot],[xla:cpu] Optimize non-grouped convolution,[xla:cpu] Optimize nongrouped convolution ``` BM_Conv2D/8/5/5/1/1/1/32/process_time                     1.57µs ± 8%  1.54µs ± 8%   1.84%  (p=0.007 n=40+40) BM_Conv2D/8/5/5/4/1/1/32/process_time                     3.09µs ± 4%  3.09µs ± 4%     ~     (p=0.770 n=40+40) BM_Conv2D/8/128/128/4/1/1/8/process_time                   872µs ±10%   899µs ±14%   +3.09%  (p=0.044 n=38+40) BM_Conv2D/8/32/32/128/1/1/1024/process_time               71.4ms ±11%  70.7ms ±10%     ~     (p=0.382 n=38+35) BM_Conv2D/16/32/32/128/1/1/1024/process_time               187ms ±13%   189ms ±11%     ~     (p=0.622 n=40+40) BM_Conv2D/32/32/32/128/1/1/1024/process_time               347ms ±10%   345ms ±10%     ~     (p=0.642 n=40+40) BM_Conv2D/32/64/64/32/1/1/64/process_time                 43.2ms ± 7%  43.6ms ±13%     ~     (p=0.418 n=40+40) BM_Conv2D/32/256/256/4/1/1/16/process_time                 127ms ± 4%   128ms ± 4%     ~     (p=0.074 n=35+39) BM_Conv2D/32/64/64/4/1/1/16/process_time                  2.46ms ±11%  2.46ms ±15%     ~     (p=0.717 n=38+40) BM_Conv2D/32/32/32/96/1/1/96/process_time                 23.7ms ± 7%  23.7ms ± 6%     ~     (p=0.939 n=40+37) BM_Conv2D/8/5/5/1/3/3/32/process_time                     20.4µs ± 1%  21.3µs ± 2%   +4.27%  (p=0.000 n=39+38) BM_Conv2D/8/5/5/4/3/3/32/process_time                     51.0µs ± 1%  27.0µs ± 2%  47.13%  (p=0.000 n=37+35) BM_Conv2D/8/128/128/4/3/3/8/process_time                  72.3ms ± 3%  21.9ms ± 3%  69.71%  (p=0.000 n=40+39) BM_Conv2D/8/32/32/128/3/3/1024/process_time                761ms ± 5%   637ms ± 7%  16.30%  (p=0.000 n=38+40) BM_Conv2D/16/32/32/128/3/3/1024/process_time               1.28s ± 6%   1.10s ± 5%  14.50%  (p=0.000 n=38+39) BM_Conv2D/32/32/32/128/3/3/1024/process_time               2.55s ± 3%   2.23s ± 5%  12.41%  (p=0.000 n=40+39) BM_Conv2D/32/64/64/32/3/3/64/process_time                  301ms ± 6%   180ms ±10%  40.25%  (p=0.000 n=38+38) BM_Conv2D/32/256/256/4/3/3/16/process_time                 1.46s ± 4%   0.39s ±14%  73.50%  (p=0.000 n=38+37) BM_Conv2D/32/64/64/4/3/3/16/process_time                  80.0ms ± 4%  24.0ms ± 3%  69.97%  (p=0.000 n=40+39) BM_Conv2D/32/32/32/96/3/3/96/process_time                  259ms ± 6%   201ms ± 9%  22.40%  (p=0.000 n=39+38) BM_GroupedConv2D/1/45/45/1024/5/5/1024/1024/process_time        491ms ± 6%   509ms ± 8%   +3.55%  (p=0.001 n=40+40) BM_Conv1DStrided/1/129/process_time                            29.2ms ± 6%  25.7ms ± 9%  11.94%  (p=0.000 n=39+40) BM_Conv1DStrided/3/129/process_time                             136ms ± 4%    76ms ± 8%  44.51%  (p=0.000 n=40+40) BM_Conv1DTransposedStrided/129/1/process_time                  28.1ms ± 5%  28.2ms ± 6%     ~     (p=0.950 n=40+40) BM_Conv1DTransposedStrided/129/3/process_time                  67.8ms ± 7%  66.5ms ± 7%   2.00%  (p=0.010 n=38+40) BM_Conv1DTransposedStridedNonDefaultLayout/129/1/process_time  25.0ms ± 5%  24.8ms ± 6%     ~     (p=0.074 n=39+39) BM_Conv1DTransposedStridedNonDefaultLayout/129/3/process_time  67.1ms ± 4%  65.5ms ± 7%   2.33%  (p=0.001 n=38+40) BM_Conv2DStrided/process_time                                  31.6ms ± 6%  28.0ms ± 5%  11.50%  (p=0.000 n=40+40) BM_Conv2DTransposedStrided/process_time                        28.0ms ± 4%  27.9ms ± 4%     ~     (p=0.555 n=39+40) BM_GroupedConv2DStrided/128/128/128/process_time               64.7ms ± 4%  64.8ms ± 3%     ~     (p=0.429 n=40+40) BM_GroupedConv2DTransposedStrided/128/128/128/process_time      5.58s ± 1%   5.58s ± 1%     ~     (p=0.172 n=39+39) BM_GroupedConv2DStrided/128/128/16/process_time                37.3ms ± 3%  37.6ms ± 2%   +0.72%  (p=0.010 n=40+38) BM_GroupedConv2DTransposedStrided/128/128/16/process_time       1.02s ± 2%   1.02s ± 2%     ~     (p=0.195 n=37+38) ```,2025-02-18T15:23:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87478
opt,copybara-service[bot],[mlir/lite] flatbuffer_export: Improve message for StableHLO ops,"[mlir/lite] flatbuffer_export: Improve message for StableHLO ops To convert StableHLO ops, Translator::Translate must be called with serialize_stablehlo_ops=true.  If using TfLiteConverter, this is done by setting target_spec.suggested_ops={tf.lite.OpsSet.EXPERIMENTAL_STABLEHLO_OPS}. However, this was not obvious from the old error message of ``` error: 'vhlo.reduce_v1' op is not part of the vhlo support yet. ``` which was also misleading, since `vhlo.reduce_v1` is, in fact, supported with the right options. The new error message is: ``` error: 'vhlo.reduce_v1' op is part of the vhlo support, but Translator::Translate was called with serialize_stablehlo_ops=false.  If using TfLiteConverter, set target_spec.suggested_ops={tf.lite.OpsSet.EXPERIMENTAL_STABLEHLO_OPS}. ```",2025-02-18T14:50:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87477
tpu,knebojsa11,[RNN] Conversion of model containing GRU layer to quantized TFLite causes Segmentation Fault," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04  TensorFlow installation (pip package or built from source): both pip package and built from source  TensorFlow library (version, if pip package or github SHA, if built from source): 2.18  2. Code import os os.environ[""TF_USE_LEGACY_KERAS""] = ""1"" import tensorflow as tf import numpy as np import tf_keras as keras def gru_model():     """"""Factory method for gru model.""""""     gru_input = keras.layers.Input(batch_input_shape=(1, 1, 64),                                     name=""gru_input"")     gru_state_in = keras.layers.Input(batch_input_shape=(1, 128),                                        name=""gru_state_in"")     gru_output, gru_state_out = keras.layers.GRU(128,                                                  activation=""tanh"",                                                  recurrent_activation=""sigmoid"",                                                  use_bias=True,                                                  return_sequences=False,                                                  bias_initializer=""random_uniform"",                                                  return_state=True)([gru_input, gru_state_in])     keras_model = keras.Model(inputs=[gru_input, gru_state_in], outputs=[gru_output, gru_state_out])     return keras_model model = gru_model() train_inputs = [tf.random.uniform((1, 1, 64), minval=1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32),                 tf.random.uniform((1, 128), minval=1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32)] train_outputs = tf.random.uniform((1, 128), minval=1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32) model.compile(optimizer='adam', loss='mse', metrics=['mse']) model.fit([train_inputs], train_outputs, batch_size=1, epochs=1) model.summary() model.save(""ticket_gru.keras"") print(""Converting to floating point Tensorflow Lite"") converter_fp = tf.lite.TFLiteConverter.from_keras_model(model) converter_fp.optimizations = [tf.lite.Optimize.DEFAULT] converter_fp.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS] tflite_model_fp = converter_fp.convert() with open(""ticket_gru_fp.tflite"",""wb"") as f: f.write(tflite_model_fp) print(""Success converting to floating point Tensorflow Lite containing WHILE operator in \ subgraph 0 and cell implementation in subgraph 1"") print(""Converting to quantized Tensorflow Lite"") def representative_data():   for _ in range(10):     yield {model.inputs[0].name:tf.random.uniform((1, 1, 64), minval=1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32, seed=None, name=None),            model.inputs[1].name:tf.random.uniform((1, 128), minval=1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32, seed=None, name=None)} converter_q = tf.lite.TFLiteConverter.from_keras_model(model) converter_q.optimizations = [tf.lite.Optimize.DEFAULT] converter_q.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS] converter_q.inference_input_type = tf.int8 converter_q.inference_output_type = tf.int8 converter_q._experimental_disable_per_channel = True converter_q.representative_dataset = representative_data print(""Call to convert method of the converter_q will cause SEGFAULT during model calibration"") tflite_model_q = converter_q.convert() print(""This line is never reached"") with open(""ticket_gru_8_8.tflite"",""wb"") as f: f.write(tflite_model_q)  3. Failure after conversion Converter fails with throwing Segmentation fault. The fault is tracked down to the Calibration phase where the representative dataset is fead to the floating point model inferred by the interpreter instantiated by the converter. The main subgraph processing causes nested subgraph processing as a part of the WHILE operator invoke. All the operator in the nested subgraph are invoked, but the return from the model invoke ends with Segmentation Fault. The segmentation fault can be traced down to Tensorflow 2.14. Tensorflow 2.13 handles the quantized conversion properly. Conversion to floating point TFLite model works fine, and the floating point inference with same data that is used as representative dataset in quantized conversion does complete without problems  4. (optional) RNN conversion support Yes  RNN conversion support. Prefixed in the title as [RNN]  5. (optional) Any other info / logs 20250218 11:45:02.889542: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used. 20250218 11:45:04.329812: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303) 1/1 [==============================]  ETA: 0s  loss: 1.0199  gru_loss: 0.5099  gru_1_loss: 0.5099  gru_mse: 0.5099  gru_1_mse: 1/1 [==============================]  1s 1s/step  loss: 1.0199  gru_loss: 0.5099  gru_1_loss: 0.5099  gru_mse: 0.5099  gru_1_mse: 0.5099 Model: ""model"" __________________________________________________________________________________________________  Layer (type)                Output Shape                 Param    Connected to                   ==================================================================================================  gru_input (InputLayer)      [(1, 1, 64)]                 0         []                              gru_state_in (InputLayer)   [(1, 128)]                   0         []                              gru (GRU)                   [(1, 128),                   74496     ['gru_input[0][0]',                                          (1, 128)]                              'gru_state_in[0][0]']         ================================================================================================== Total params: 74496 (291.00 KB) Trainable params: 74496 (291.00 KB) Nontrainable params: 0 (0.00 Byte) __________________________________________________________________________________________________ Converting to floating point Tensorflow Lite WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1739875508.650404  896323 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format. W0000 00:00:1739875508.650452  896323 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency. 20250218 11:45:08.650993: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp96nzf981 20250218 11:45:08.656672: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve } 20250218 11:45:08.656711: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp96nzf981 I0000 00:00:1739875508.691084  896323 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled 20250218 11:45:08.698338: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle. 20250218 11:45:08.768960: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp96nzf981 20250218 11:45:08.807201: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 156213 microseconds. 20250218 11:45:09.144839: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. 20250218 11:45:09.264954: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 0.174 M  ops, equivalently 0.087 M  MACs Success converting to floating point Tensorflow Lite containing WHILE operator in subgraph 0 and cell implementation in subgraph 1 Converting to quantized Tensorflow Lite Call to convert method of the converter_q will cause SEGFAULT during model calibration /opt/samba/nxf35112/keras2_experiments/venv/lib/python3.10/sitepackages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.   warnings.warn( W0000 00:00:1739875512.496385  896323 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format. W0000 00:00:1739875512.496430  896323 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency. 20250218 11:45:12.496633: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpfx3q8bul 20250218 11:45:12.501629: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve } 20250218 11:45:12.501670: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpfx3q8bul 20250218 11:45:12.542409: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle. 20250218 11:45:12.609623: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpfx3q8bul 20250218 11:45:12.649789: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 153160 microseconds. 20250218 11:45:13.053211: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 0.174 M  ops, equivalently 0.087 M  MACs Segmentation fault",2025-02-18T11:08:09Z,stat:awaiting tensorflower type:support comp:lite TFLiteConverter TF 2.18,open,0,3,https://github.com/tensorflow/tensorflow/issues/87468,"Hi,   I apologize for the delay in my response, I have been able to replicate the similar behavior from my end for reference here is gistfile so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention. Thank you for your cooperation and patience.","Hi,  I apologize for the delay in my response, The error occurs because GRU layers have complex state management that conflicts with full integer quantization so if full integer quantization is not compulsary for your use case/ project you can go with either float16 Quantization or dynamic range quantization which is more compatible with RNN layers.  This is a limitation in the TensorFlow Lite quantization system when dealing with complex recurrent structures. GRU and LSTM layers involve internal states and complex cell implementations that the quantization process sometimes cannot properly handle resulting in segmentation faults during model calibration. I have tried from my end with float16 Quantization and dynamic range quantization it's working as expected for your reference here is gistfile Thank you for your cooperation and patience.",Dear   I really need the model to be fully quantized so I am interested if there are plans to fix the broken feature in the converter. Best Regards
sharding,copybara-service[bot],PR #22723: Fix call of overloaded Tile is ambiguous,"PR CC(bug: tf.train.Saver.save() fails if 2 tf.contrib.cudnn_rnn.cudnnLSTM instances passed to saver are built with build() method): Fix call of overloaded Tile is ambiguous Imported from GitHub PR https://github.com/openxla/xla/pull/22723  Fix GCC13 Build Error in AutoSharding Due to vector vs. absl::Span Ambiguity When building auto_sharding with GCC13, the following build error occurred: ``` xla/hlo/experimental/auto_sharding/auto_sharding.cc:895:37: error: call of overloaded 'Tile(const xla::Shape&, , , const xla::spmd::DeviceMesh&)' is ambiguous   895              ^~~~ ```  Solution: To resolve the ambiguity between `std::vector>` and `absl::Span` in `Tile()`, I introduced an overloaded Tile() function that takes `std::initializer_list mesh_dims`. Now, expressions like the following now compile successfully with GCC13: ``` Tile(shape, {0}, {0}, device_mesh); ```  Additional Changes  Removed the `Tile()` declaration from `auto_sharding.h` since it is already declared in `auto_sharding_util.h`. Copybara import of the project:  a28207f171c086130097d520079648b1548976dc by Alexander Pivovarov : Fix call of overloaded Tile is ambiguous Merging this change closes CC(bug: tf.train.Saver.save() fails if 2 tf.contrib.cudnn_rnn.cudnnLSTM instances passed to saver are built with build() method) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22723 from apivovarov:fix_Tile_init_list a28207f171c086130097d520079648b1548976dc",2025-02-18T11:04:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87467
opt,copybara-service[bot],Use custom HLO serialization for HloUnoptimizedSnapshot.,Use custom HLO serialization for HloUnoptimizedSnapshot. This change makes it possible to dump HloUnoptimizedSnapshot protos that are over 2GiB in size (the proto binary serialization limit).,2025-02-18T09:17:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87460
opt,copybara-service[bot],PR #22614: Fix hlo_opt printing of Hlo module,PR CC(what is operator name for python corresponding to kReducePrecision): Fix hlo_opt printing of Hlo module Imported from GitHub PR https://github.com/openxla/xla/pull/22614 The tool `hloopt` was not honoring the debug options within the HloModule while printing the HloModule. These options should be honored by the default printing of the HloModule as they are a part of the same HloModule. Fixed the print method to do this. This should now be reflected in all the tools using these debug options. Copybara import of the project:  a22584a819a0fc6ee8f41b4c50f4f8d68a6a2184 by Shraiysh Vaishay : Fix hlo_opt printing of Hlo module The tool `hloopt` was not honoring the debug options within the HloModule while printing the HloModule. These options should be honored by the default printing of the HloModule as they are a part of the same HloModule. Fixed the print method to do this. This should now be reflected in all the tools using these debug options.  b42178b4da3fd5f81fc2d50346cb2f9b18153ab5 by Shraiysh Vaishay : Rebase and avoid edits to testcases.  51cdfbfa355efe34936073fd68d4e19191131bb7 by Shraiysh Vaishay : Addressed failing test Merging this change closes CC(what is operator name for python corresponding to kReducePrecision) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22614 from shraiysh:fix_dumping_hlo_opt 51cdfbfa355efe34936073fd68d4e19191131bb7,2025-02-18T08:00:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87458
tpu,jwnhy,[CUDA] Check failed work_element_count >= 0," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Ubuntu 24.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuDNN 90300  GPU model and memory H100 80G  Current behavior? F0000 00:00:1739864447.179429 1679389 gpu_launch_config.h:129] Check failed: work_element_count >= 0 (1881720796 vs. 0) *** Check failure stack trace: ***     @     0x7fffec867bc4  absl::lts_20230802::log_internal::LogMessage::SendToLog()     @     0x7fffec8679c4  absl::lts_20230802::log_internal::LogMessage::Flush()     @     0x7fffec867fe9  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()     @     0x7fffe33017d4  tensorflow::functor::TransformFilter::operator()()     @     0x7fffe1896f56  tensorflow::LaunchConvOpImpl()     @     0x7fffe1a03561  tensorflow::Conv2DOp::Compute()     @     0x7ffff51474d5  tensorflow::BaseGPUDevice::Compute()     @     0x7ffff51f8f48  tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run()     @     0x7ffff51b8724  tensorflow::FunctionLibraryRuntimeImpl::RunSync()     @     0x7ffff51c5730  tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync()     @     0x7ffff51cbb7d  tensorflow::ProcessFunctionLibraryRuntime::RunSync()     @     0x7fffdcd3ddf0  tensorflow::KernelAndDeviceFunc::Run()     @     0x7fffdcce98d6  tensorflow::EagerKernelExecute()     @     0x7fffdccf33b0  tensorflow::ExecuteNode::Run()     @     0x7fffdcd39244  tensorflow::EagerExecutor::SyncExecute()     @     0x7fffdcce925b  tensorflow::(anonymous namespace)::EagerLocalExecute()     @     0x7fffdcce6929  tensorflow::DoEagerExecute()     @     0x7fffdccea660  tensorflow::EagerExecute()     @     0x7fffdc3209f7  tensorflow::EagerOperation::Execute()     @     0x7fffdcd37943  tensorflow::CustomDeviceOpHandler::Execute()     @     0x7fffd9b38cf5  TFE_Execute     @     0x7fffee231efa  TFE_Py_FastPathExecute_C()     @     0x7fffedad6893  pybind11::detail::argument_loader::call()     @     0x7fffedad67cf  pybind11::cpp_function::initialize()::{lambda() CC(Add support for Python 3.x)}::__invoke()     @     0x7fffedab08df  pybind11::cpp_function::dispatcher()     @           0x54d2d4  cfunction_call  Standalone code to reproduce the issue ```shell import tensorflow as tf from keras import layers import os os.environ[""TF_DISABLE_RZ_CHECK""] = ""1"" os.environ[""TF_GPU_ALLOCATOR""] = ""cuda_malloc_async"" tf.keras.backend.set_image_data_format('channels_first') gpus = tf.config.experimental.list_physical_devices('GPU') for gpu in gpus:     tf.config.experimental.set_memory_growth(gpu, True) tensor = tf.random.uniform([100, 100, 100]) model = layers.Conv1D(kernel_size=3, strides=98, filters=8044155, groups=1) model(tensor) ```  Relevant log output ```shell ```",2025-02-18T07:48:37Z,type:bug comp:gpu TF 2.8,open,0,3,https://github.com/tensorflow/tensorflow/issues/87457,"Hi **** , Apologies for the delay, and thanks for raising your concern here. I observed that you are using an older version TensorFlow 2.8. Could you please update to the latest version for better results? I ran your code on Colab using TensorFlow 2.18.0, and it threw the following error message: ` ResourceExhaustedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name:  ` This error occurs because the program is trying to allocate more GPU memory than is available. The issue seems to be caused by the large number of filters used in your model. To resolve this, I reduced the number of filters, and it worked fine for me. I am attaching a gist here for your reference. Thank you!","Oh, sorry, I mistyped the version number. I am using the latest tensorflow 2.18... The memory issue is fine on my machine, probably because I am using the H100 with 80GiB of VRAM.",The following is the environment I am using. ``` nvidiacublascu12        12.5.3.2                 pypi_0    pypi nvidiacudacupticu12    12.5.82                  pypi_0    pypi nvidiacudanvcccu12     12.5.82                  pypi_0    pypi nvidiacudanvrtccu12    12.5.82                  pypi_0    pypi nvidiacudaruntimecu12  12.5.82                  pypi_0    pypi nvidiacudnncu12         9.3.0.75                 pypi_0    pypi nvidiacufftcu12         11.2.3.61                pypi_0    pypi nvidiacurandcu12        10.3.6.82                pypi_0    pypi nvidiacusolvercu12      11.6.3.83                pypi_0    pypi nvidiacusparsecu12      12.5.1.3                 pypi_0    pypi nvidiancclcu12          2.21.5                   pypi_0    pypi nvidianvjitlinkcu12     12.5.82                  pypi_0    pypi openssl                   3.0.15               h5eee18b_0 opteinsum                3.4.0                    pypi_0    pypi optree                    0.14.0                   pypi_0    pypi packaging                 24.2                     pypi_0    pypi pexpect                   4.9.0                    pypi_0    pypi pip                       25.0            py312h06a4308_0 protobuf                  5.29.3                   pypi_0    pypi ptyprocess                0.7.0                    pypi_0    pypi pygments                  2.19.1                   pypi_0    pypi python                    3.12.9               h5148396_0 readline                  8.2                  h5eee18b_0 requests                  2.32.3                   pypi_0    pypi rich                      13.9.4                   pypi_0    pypi setuptools                75.8.0          py312h06a4308_0 six                       1.17.0                   pypi_0    pypi sqlite                    3.45.3               h5eee18b_0 tensorboard               2.18.0                   pypi_0    pypi tensorboarddataserver   0.7.2                    pypi_0    pypi tensorflow                2.18.0                   pypi_0    pypi t ```
opt,copybara-service[bot],Add a field in PluginCompileOptions to specify the input program format.,Add a field in PluginCompileOptions to specify the input program format.,2025-02-18T07:02:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87456
tpu,jwnhy,[CUDA] cudaErrorInvalidConfiguration detected by compute-sanitizer," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.8.1  Custom code Yes  OS platform and distribution Ubuntu 24.04  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 90300  GPU model and memory H100 80G  Current behavior? computesanitizer reports Program hit cudaErrorInvalidConfiguration  Standalone code to reproduce the issue ```shell import tensorflow as tf from keras import layers import os os.environ[""TF_DISABLE_RZ_CHECK""] = ""1"" os.environ[""TF_GPU_ALLOCATOR""] = ""cuda_malloc_async"" tf.keras.backend.set_image_data_format('channels_first') gpus = tf.config.experimental.list_physical_devices('GPU') for gpu in gpus:     tf.config.experimental.set_memory_growth(gpu, True) tensor = tf.random.uniform([2, 388814, 2]) model = layers.Conv1D(filters=2, kernel_size=1, strides=2, groups=2) model(tensor) ```  Relevant log output ```shell ========= COMPUTESANITIZER ========= Program hit cudaErrorInvalidConfiguration (error 9) due to ""invalid configuration argument"" on CUDA API call to cudaLaunchKernelExC. =========     Saved host backtrace up to driver entry point at error =========     Host Frame: [0x4aa4a5] =========                in /lib/x86_64linuxgnu/libcuda.so.1 =========     Host Frame: [0x1d1b509] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0x15f94a1] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0x1461f45] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0x14627e9] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0xfefdb1] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0xff0704] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0x4a33ad] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0x4a3888] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0x4c4a94] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame:cudnn::backend::execute(cudnnContext*, cudnn::backend::ExecutionPlan const&, cudnn::backend::VariantPack&) [0x134c78] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9 =========     Host Frame:cudnnBackendExecute [0x134d8e] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9 =========     Host Frame: [0x8d5cf] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9 =========     Host Frame:cuda_graph_util::CudaGraphInfo::init(void*, cudnn::backend::OperationSetFinalizedMode_t) [0x868e5] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9 =========     Host Frame:cudnn::backend::cudnnBackendGetMetadataFromGraph(void*) [0x125fac] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9 =========     Host Frame: [0x153281] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_heuristic.so.9 =========     Host Frame: [0x1a4291] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_heuristic.so.9 =========     Host Frame: [0x1a593a] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_heuristic.so.9 =========     Host Frame: [0x151dea] ```",2025-02-18T05:21:21Z,type:bug comp:gpu TF 2.8,open,0,2,https://github.com/tensorflow/tensorflow/issues/87455,"Hi **** , Apologies for the delay, and thanks for raising your concern here. I noticed that you are using an older version, TensorFlow 2.8. Could you please update to the latest version for better performance and compatibility? I ran your code on Colab using TensorFlow 2.18.0 and the nightly version with GPU support, and I did not encounter any issues. Please find the gist here for your reference. Thank you!",Sorry I mistyped the version number... I found this issue on the latest tensorflow 2.18. I am using an H100 with 80GiB ram to find this issue. Can you try to reproduce this on a card with more ram?
tpu,jwnhy,[CUDA] illegal memory read in ShuffleInTensor3Simple," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuDNN 90300  GPU model and memory H100 80G  Current behavior? computesanitizer detects invalid memory read.  Standalone code to reproduce the issue ```shell import tensorflow as tf from keras import layers import os os.environ[""TF_DISABLE_RZ_CHECK""] = ""1"" os.environ[""TF_GPU_ALLOCATOR""] = ""cuda_malloc_async"" tf.keras.backend.set_image_data_format('channels_first') gpus = tf.config.experimental.list_physical_devices('GPU') for gpu in gpus:     tf.config.experimental.set_memory_growth(gpu, True) tensor = tf.random.uniform([1, 79768, 2]) model = layers.Conv1D(filters=65470, kernel_size=1, strides=32827, groups=1) model(tensor) ```  Relevant log output ```shell ========= COMPUTESANITIZER ========= Invalid __global__ read of size 4 bytes =========     at void tensorflow::functor::ShuffleInTensor3Simple(int, const T1 *, tensorflow::functor::Dimension, T1 *)+0x630 =========     by thread (64,0,0) in block (84,0,0) =========     Address 0x32f9d8d188 is out of bounds =========     and is 5,845,702,776 bytes before the nearest allocation at 0x3456472a00 of size 20,889,643,840 bytes =========     Saved host backtrace up to driver entry point at kernel launch time =========     Host Frame: [0x37f187] =========                in /lib/x86_64linuxgnu/libcuda.so.1 =========     Host Frame: [0x15a13] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cuda_runtime/lib/libcudart.so.12 =========     Host Frame:cudaLaunchKernel [0x75750] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cuda_runtime/lib/libcudart.so.12 =========     Host Frame:absl::lts_20230802::Status tensorflow::GpuLaunchKernel, float*, int, float const*, tensorflow::functor::Dimension, float*>(void (*)(int, float const*, tensorflow::functor::Dimension, float*), dim3, dim3, unsigned long, CUstream_st*, int, float const*, tensorflow::functor::Dimension, float*) [0x2b5018e4] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::functor::TransformFilter::operator()(Eigen::GpuDevice const&, tensorflow::FilterTensorFormat, Eigen::TensorMap, 16, Eigen::MakePointer>, Eigen::TensorMap, 16, Eigen::MakePointer>) [0x2b5015ff] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:void tensorflow::LaunchConvOpImpl(tensorflow::OpKernelContext*, bool, tensorflow::Tensor const&, tensorflow::Tensor const&, absl::lts_20230802::InlinedVector > const&, absl::lts_20230802::InlinedVector > const&, tensorflow::Padding const&, std::vector > const&, tensorflow::TensorFormat, tensorflow::Tensor*) [0x29a96f55] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::Conv2DOp::Compute(tensorflow::OpKernelContext*) [0x29c03560] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) [0x6d474d4] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) [0x6df8f47] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span, std::vector >*) [0x6db8723] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector, std::allocator > >*, std::function) const [0x6dc572f] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span, std::vector >*) const [0x6dcbb7c] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector, std::allocator > >*, tsl::CancellationManager*, std::optional const&, std::optional const&, tsl::CoordinationServiceAgent*) [0x24f3ddef] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector > const&, std::optional const&, tsl::core::RefCountPtr const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span, std::optional const&) [0x24ee98d5] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::ExecuteNode::Run() [0x24ef33af] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) [0x24f39243] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) [0x24ee925a] ```",2025-02-18T05:14:25Z,type:bug comp:gpu TF 2.8,open,0,3,https://github.com/tensorflow/tensorflow/issues/87454,"Hi **** , Apologies for the delay, and thanks for raising your concern here. I observed that you are using an older version TensorFlow 2.8 which might be causing the issue. Could you please try using the latest version for better results? I ran your code on Colab using TensorFlow 2.18.0 with GPU, and it produced the following proper error message: ` ResourceExhaustedError: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[1,79768,65470] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator gpu_async_0 [Op:StatelessRandomUniformV2] name:  ` This error indicates that the GPU is running out of memory (OOM) due to large tensor allocations. To resolve this, I reduced the memory requirements based on the model, and it worked fine for me. I am providing a gist here for your reference. Thank you!",Sorry I mistyped the version number... I found this issue on the latest tensorflow 2.18. I am using an H100 with 80GiB ram to find this issue. Can you try to reproduce this on a card with more ram?,The following is the environment I am using. ``` nvidiacublascu12        12.5.3.2                 pypi_0    pypi nvidiacudacupticu12    12.5.82                  pypi_0    pypi nvidiacudanvcccu12     12.5.82                  pypi_0    pypi nvidiacudanvrtccu12    12.5.82                  pypi_0    pypi nvidiacudaruntimecu12  12.5.82                  pypi_0    pypi nvidiacudnncu12         9.3.0.75                 pypi_0    pypi nvidiacufftcu12         11.2.3.61                pypi_0    pypi nvidiacurandcu12        10.3.6.82                pypi_0    pypi nvidiacusolvercu12      11.6.3.83                pypi_0    pypi nvidiacusparsecu12      12.5.1.3                 pypi_0    pypi nvidiancclcu12          2.21.5                   pypi_0    pypi nvidianvjitlinkcu12     12.5.82                  pypi_0    pypi openssl                   3.0.15               h5eee18b_0 opteinsum                3.4.0                    pypi_0    pypi optree                    0.14.0                   pypi_0    pypi packaging                 24.2                     pypi_0    pypi pexpect                   4.9.0                    pypi_0    pypi pip                       25.0            py312h06a4308_0 protobuf                  5.29.3                   pypi_0    pypi ptyprocess                0.7.0                    pypi_0    pypi pygments                  2.19.1                   pypi_0    pypi python                    3.12.9               h5148396_0 readline                  8.2                  h5eee18b_0 requests                  2.32.3                   pypi_0    pypi rich                      13.9.4                   pypi_0    pypi setuptools                75.8.0          py312h06a4308_0 six                       1.17.0                   pypi_0    pypi sqlite                    3.45.3               h5eee18b_0 tensorboard               2.18.0                   pypi_0    pypi tensorboarddataserver   0.7.2                    pypi_0    pypi tensorflow                2.18.0                   pypi_0    pypi ```
tpu,johnnkp,Add CUDA plugin registration checking,"This pull request is to fix https://github.com/openxla/xla/issues/20803. Test result is as follows. No error. ``` 20250205 12:19:54.390955: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1738729197.544741    4373 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3315 MB memory:  > device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:29:00.0, compute capability: 6.1 Model: ""functional_4"" ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ ┃ Layer (type)                    ┃ Output Shape           ┃       Param  ┃ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ │ input_layer (InputLayer)        │ (None, None, 128)      │             0 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ encoder (Encoder)               │ (None, None, 128)      │       662,528 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ out_pretraining (Dense)         │ (None, None, 128)      │        16,512 │ └─────────────────────────────────┴────────────────────────┴───────────────┘  Total params: 679,040 (2.59 MB)  Trainable params: 679,040 (2.59 MB)  Nontrainable params: 0 (0.00 B) ```",2025-02-18T05:13:58Z,comp:xla size:M,closed,0,3,https://github.com/tensorflow/tensorflow/issues/87453," I already created as https://github.com/openxla/xla/pull/22391, but they didn't review.",">  I already created as openxla/xla CC(Fixed broken links), but they didn't review. Sorry about your experience. The problem is that it was assigned to someone who is currently out of office. We need to find a different reviewer.",Closed as https://github.com/tensorflow/tensorflow/pull/88512 merged.
tpu,jwnhy,[CUDA] illegal memory write in ShuffleInTensor3Simple," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.8.1  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuDNN: 90300  GPU model and memory H100 80G  Current behavior? computesanitizer reports Illegal Memory Access. Expect: no IMA should occur.  Standalone code to reproduce the issue ```shell import tensorflow as tf from keras import layers import os os.environ[""TF_DISABLE_RZ_CHECK""] = ""1"" tf.keras.backend.set_image_data_format('channels_first') gpus = tf.config.experimental.list_physical_devices('GPU') for gpu in gpus:     tf.config.experimental.set_memory_growth(gpu, True) tensor = tf.random.uniform([2, 1, 392366]) model = layers.Conv1D(filters=2147483647, kernel_size=1, strides=392366, groups=1) model(tensor) ```  Relevant log output ```shell ========= COMPUTESANITIZER ========= Invalid __global__ write of size 4 bytes =========     at void tensorflow::functor::ShuffleInTensor3Simple(int, const T1 *, tensorflow::functor::Dimension, T1 *)+0x660 =========     by thread (128,0,0) in block (8,0,0) =========     Address 0x2e9c2ff600 is out of bounds =========     and is 517 bytes after the nearest allocation at 0x2c9c2ff400 of size 8,589,934,588 bytes =========     Saved host backtrace up to driver entry point at kernel launch time =========     Host Frame: [0x37f187] =========                in /lib/x86_64linuxgnu/libcuda.so.1 =========     Host Frame: [0x15a13] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cuda_runtime/lib/libcudart.so.12 =========     Host Frame:cudaLaunchKernel [0x75750] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cuda_runtime/lib/libcudart.so.12 =========     Host Frame:absl::lts_20230802::Status tensorflow::GpuLaunchKernel, float*, int, float const*, tensorflow::functor::Dimension, float*>(void (*)(int, float const*, tensorflow::functor::Dimension, float*), dim3, dim3, unsigned long, CUstream_st*, int, float const*, tensorflow::functor::Dimension, float*) [0x2b5018e4] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::functor::TransformFilter::operator()(Eigen::GpuDevice const&, tensorflow::FilterTensorFormat, Eigen::TensorMap, 16, Eigen::MakePointer>, Eigen::TensorMap, 16, Eigen::MakePointer>) [0x2b5015ff] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:void tensorflow::LaunchConvOpImpl(tensorflow::OpKernelContext*, bool, tensorflow::Tensor const&, tensorflow::Tensor const&, absl::lts_20230802::InlinedVector > const&, absl::lts_20230802::InlinedVector > const&, tensorflow::Padding const&, std::vector > const&, tensorflow::TensorFormat, tensorflow::Tensor*) [0x29a96f55] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::Conv2DOp::Compute(tensorflow::OpKernelContext*) [0x29c03560] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) [0x6d474d4] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) [0x6df8f47] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span, std::vector >*) [0x6db8723] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector, std::allocator > >*, std::function) const [0x6dc572f] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span, std::vector >*) const [0x6dcbb7c] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector, std::allocator > >*, tsl::CancellationManager*, std::optional const&, std::optional const&, tsl::CoordinationServiceAgent*) [0x24f3ddef] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector > const&, std::optional const&, tsl::core::RefCountPtr const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span, std::optional const&) [0x24ee98d5] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::ExecuteNode::Run() [0x24ef33af] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) [0x24f39243] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) [0x24ee925a] ```",2025-02-18T04:22:19Z,type:bug comp:gpu TF 2.8,open,0,2,https://github.com/tensorflow/tensorflow/issues/87438,"Hi **** , Apologies for the delay, and thanks for your patience. I ran your code on Colab using TensorFlow 2.18.0, and it threw the following error: ``` ResourceExhaustedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name:  ``` This error occurs because the program is trying to allocate more GPU memory than is available. The issue seems to be caused by the large number of filters used in your model. To resolve this, I reduced the number of filters, and it worked fine for me. I am attaching a gist here for your reference. Thank you!",Sorry I mistyped the version number... I found this issue on the latest tensorflow 2.18. I am using an H100 with 80GiB ram to find this issue. Can you try to reproduce this on a card with more ram?
multi-gpu,datasciantle,NaN loss on multi-gpu training," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version v1.12.1122039g784fed5357b 2.20.0dev20250211  Custom code Yes  OS platform and distribution Amazon Linux 2  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 12.3 (Build V12.3.107)  GPU model and memory GPU Model: NVIDIA A10G Memory per GPU: 24 GB  Current behavior? I find that training a distributed model  with either MirroredStrategy or MultiWorkerMirroredStrategy  that the loss jumps to NaN on the 61st batch. The problem vanishes if I train on a single GPU (no distribute strategy), or if I use tf 2.13.1. The problem also vanishes if I set the number of steps per epoch to be less than 61. I've attached some reproducible code but note that I find the exact same finding (the role of batch 61, single GPU, v.2.13) on other datasets, other model architectures, and with different batch sizes.  Standalone code to reproduce the issue ```shell call this test.py  these pass python3 test.py steps_per_epoch 60 strategy None python3 test.py steps_per_epoch 60 strategy MirroredStrategy python3 test.py steps_per_epoch 60 strategy MultiWorkerMirroredStrategy python3 test.py steps_per_epoch 61 strategy None  these fail python3 test.py steps_per_epoch 61 strategy MirroredStrategy python3 test.py steps_per_epoch 61 strategy MultiWorkerMirroredStrategy import tensorflow as tf import numpy as np import argparse parser = argparse.ArgumentParser() parser.add_argument('batch_size', type=int, default=64) parser.add_argument('steps_per_epoch', type=int, default=128) parser.add_argument('strategy', type=str, default='None') args = parser.parse_args()  For reproducibility tf.random.set_seed(42) np.random.seed(42)  Parameters n_string = 256       used for the CLS token value n_features = 256    length of the output label vector dim_model = 64 batch_size = int(args.batch_size) steps_per_epoch = int(args.steps_per_epoch) def random_input(n_features, n_string):     """"""     Generate random input features in the integer domain.     """"""     return tf.random.uniform(shape=(n_features,), minval=0, maxval=n_string, dtype=tf.int32) def random_mask(input_features, batch_masking_rate):     """"""     Randomly mask input features in the integer domain.     Each element is retained with probability (1  batch_masking_rate) and zeroed otherwise.     """"""      Generate random values for each element in the same shape as input_features.     rand_vals = tf.random.uniform(shape=tf.shape(input_features), minval=0.0, maxval=1.0)      Create a binary mask: retain element if random value >= masking_rate, else 0.      This yields 1 with probability (1  batch_masking_rate) and 0 with probability batch_masking_rate.     mask = tf.cast(rand_vals >= batch_masking_rate, dtype=tf.int32)     return input_features * mask def process_example(output_label):     """"""     Given an output_label vector, generate a random masking rate,     apply random masking to produce input_features, and prepend a CLS token.     All operations remain in the integer domain.     """"""      Draw a random masking rate uniformly between 0 and 0.5.      (Note: masking_rate is used only for the masking decision and is not passed on.)     masking_rate = tf.random.uniform([], minval=0.0, maxval=0.5)      output_label is already an integer tensor.     masked_input = random_mask(output_label, masking_rate)      Return a features dictionary and a labels dictionary.     features = {""input_features"": masked_input}     labels = {""the_label"": output_label}     return features, labels def make_dataset(num_examples, n_features, n_string, batch_size):     """"""     Create a tf.data.Dataset of random output labels.     Each output label is a random integer tensor of shape (n_features,).     """"""      Create a dataset of num_examples random output labels.     dummy_examples = [random_input(n_features, n_string) for _ in range(num_examples)]      Build a tf.data.Dataset from the dummy output labels.     dataset = tf.data.Dataset.from_tensor_slices(dummy_examples)     dataset = dataset.map(process_example)     dataset = dataset.batch(batch_size)     return dataset.repeat() def make_the_model(n_features, n_string, dim_model):     """"""     Build a simple model that takes input_features and predicts the_label.     """"""     input_features = tf.keras.layers.Input(shape=(n_features,), dtype=tf.int32, name=""input_features"")     embedding = tf.keras.layers.Embedding(n_string, dim_model)(input_features)     mha = tf.keras.layers.MultiHeadAttention(         num_heads=8,         key_dim=dim_model//8,         dropout=0.,         name='mha_0'     )(embedding, embedding)     mha = tf.keras.layers.MultiHeadAttention(         num_heads=8,         key_dim=dim_model//8,         dropout=0.,         name='mha_1'     )(mha, mha)      Flatten the embedding and apply a dense layer to predict the_label.     the_label = tf.keras.layers.Dense(n_features, activation=None, name=""the_label"")(mha)     model = tf.keras.Model(inputs={'input_features': input_features}, outputs={'the_label': the_label})     optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)     loss_dict = {'the_label': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)}     model.compile(optimizer=optimizer, loss=loss_dict)     return model training_data = make_dataset(1024, n_features, n_string, batch_size) validation_data = make_dataset(256, n_features, n_string, batch_size)  print an example of the training data  for batch in training_data.take(1):      print(batch)  Build the model if args.strategy == 'MirroredStrategy':     print(""Using MirroredStrategy."")     strategy = tf.distribute.MirroredStrategy()     with strategy.scope():         this_model = make_the_model(n_features, n_string, dim_model)         this_model.summary()         print(f'Training with batch size {batch_size} and steps per epoch {steps_per_epoch} with {strategy.num_replicas_in_sync} replicas.') if args.strategy == 'MultiWorkerMirroredStrategy':     print(""Using MultiWorkerMirroredStrategy."")     strategy = tf.distribute.MultiWorkerMirroredStrategy()     with strategy.scope():         this_model = make_the_model(n_features, n_string, dim_model)         this_model.summary()         print(f'Training with batch size {batch_size} and steps per epoch {steps_per_epoch} with {strategy.num_replicas_in_sync} replicas.') if args.strategy == 'None':     print(""No strategy."")     this_model = make_the_model(n_features, n_string, dim_model)     this_model.summary()     print(f'Training with batch size {batch_size} and steps per epoch {steps_per_epoch} with 1 GPU.')  Train the model call_backs = tf.keras.callbacks.TerminateOnNaN() this_model.fit(training_data, validation_data=validation_data, epochs=100, steps_per_epoch=steps_per_epoch, validation_steps=4, callbacks=[call_backs])```  Relevant log output ```shell ```",2025-02-17T21:04:11Z,stat:awaiting response type:bug stale comp:gpu TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/87432,"Hi **** , Apologies for the delay, and thanks for raising your concern here. I tried running your code using TensorFlow 2.18.0 (stable) on VM instances, but I did not encounter any issues. I am attaching the output below for your reference. And I recommend using stable versions for better results. ``` [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')] ``` ``` (tf_env) maayaragpu1:~$ python3 test.py steps_per_epoch 61 strategy MultiWorkerMirroredStrategy 20250305 11:01:28.725763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741172488.746097   33345 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741172488.752394   33345 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250305 11:01:28.774422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. I0000 00:00:1741172491.290716   33345 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  > device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5 I0000 00:00:1741172491.293431   33345 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13764 MB memory:  > device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5 Using MultiWorkerMirroredStrategy. Model: ""functional"" ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ Layer (type)                  ┃ Output Shape              ┃         Param  ┃ Connected to               ┃ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩ │ input_features (InputLayer)   │ (None, 256)               │               0 │                           │ ├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤ │ embedding (Embedding)         │ (None, 256, 64)           │          16,384 │ input_features[0][0]       │ ├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤ │ mha_0 (MultiHeadAttention)    │ (None, 256, 64)           │          16,640 │ embedding[0][0],           │ │                               │                           │                 │ embedding[0][0]            │ ├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤ │ mha_1 (MultiHeadAttention)    │ (None, 256, 64)           │          16,640 │ mha_0[0][0], mha_0[0][0]   │ ├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤ │ the_label (Dense)             │ (None, 256, 256)          │          16,640 │ mha_1[0][0]                │ └───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘  Total params: 66,304 (259.00 KB)  Trainable params: 66,304 (259.00 KB)  Nontrainable params: 0 (0.00 B) Training with batch size 64 and steps per epoch 61 with 2 replicas. Epoch 1/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 10s 48ms/step  loss: 5.5452  val_loss: 5.5454 Epoch 2/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step  loss: 5.5448  val_loss: 5.5454 . . . . Epoch 97/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step  loss: 1.3885  val_loss: 1.3389 Epoch 98/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step  loss: 1.4202  val_loss: 1.3394 Epoch 99/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 3s 44ms/step  loss: 1.3504  val_loss: 1.3385 Epoch 100/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 3s 44ms/step  loss: 1.4391  val_loss: 1.3389 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],Attaching custom options with TAC filters:,"Attaching custom options with TAC filters:  Add google.any.proto to op_filter message which can encapsulate any opaque proto that is parsed & managed using a callback.  Update filter so that it can also ""invert"" match & specify CPU v/s non CPU  Pass callback to filtering pass that annotates matched op with fingerprint of unpacked custom options.  Disable inlining during TFL export, but enable after custom IP export.  Merge compiler options with default options (can be changed).",2025-02-17T16:55:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87425
opt,copybara-service[bot],Optionally support setting options on ShardyXLA pass at constrution time.,Optionally support setting options on ShardyXLA pass at constrution time.,2025-02-17T16:16:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87423
opt,copybara-service[bot],PR #22385: [XLA:GPU] Fix test ConditionalOpTest.SwappedInputsInSequentialConditionals failed when enabling command buffer.,"PR CC(session = tf.Session()): [XLA:GPU] Fix test ConditionalOpTest.SwappedInputsInSequentialConditionals failed when enabling command buffer. Imported from GitHub PR https://github.com/openxla/xla/pull/22385 This PR fix test ConditionalOpTest.SwappedInputsInSequentialConditionals failure when enabling command buffer. The issue is that command buffer lowering process misinterprets the bool predict to int32 predict, so the predict value is corrupted.   the original errors are:  ``` xla/tests/client_library_test_base.cc:466: Failure Value of: LiteralTestUtil::Near(expected, actual, error)   Actual: false ( Mismatches in shape (f32[], f32[]) (2 elements): Array at shape index {0}, Mismatch count 1 (100.0000%) in shape f32[] (1 elements), abs bound 0.001, rel bound 0 Top relative error mismatches:   actual             5.55000019, expected             11.2399998, index {}, rel error    0.506, abs error     5.69 : Array at shape index {1}, Mismatch count 1 (100.0000%) in shape f32[] (1 elements), abs bound 0.001, rel bound 0 Top relative error mismatches:   actual             11.2399998, expected             5.55000019, index {}, rel error     1.03, abs error     5.69 Expected literal: ( f32[] 11.24, f32[] 5.55 ) Actual literal: ( f32[] 5.55, f32[] 11.24 )) ``` Copybara import of the project:  272959fb11a5306414ef924536130e4b3456fa37 by Shawn Wang : fix case command buffer unittest failure  a254286ec86013186380bb0502769bdc01580130 by Shawn Wang : restore default falgs  df91f58d5546edfd625ad3d4825c9bbf5e72971e by Shawn Wang : fix test failure  0f12478c89a85f2d0bbca6fceae4eb7779f84868 by Shawn Wang : fix  33befa603257c76897a250130582bde67049cae5 by Shawn Wang : update ptx code Merging this change closes CC(session = tf.Session()) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22385 from shawnwang18:shawnw/enable_cuda_graph_by_default 33befa603257c76897a250130582bde67049cae5",2025-02-17T15:59:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87422
tpu,cybersupersoap,"`tf.raw_ops.QuantizeAndDequantizeV3` aborts with ""Check failed: d >= 0 (0 vs. -4)"""," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tfnightly 2.19.0  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `aborted issue` in TensorFlow when I used API `tf.raw_ops.QuantizeAndDequantizeV3` . I have confirmed that below code would crash on `tfnightly 2.19.0dev20250207` (nightlybuild). Please find the gist to reproduce the issue.  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np input_data = tf.random.uniform(shape=[1, 2, 3, 2], dtype=tf.dtypes.float32) input_min = tf.constant([0.0], dtype=tf.dtypes.float32) input_max = tf.constant([1.0], dtype=tf.dtypes.float32) output = tf.raw_ops.QuantizeAndDequantizeV3(input=input_data, input_min=input_min, input_max=input_max, num_bits=8, signed_input=True, range_given=True, narrow_range=False, axis=4, name=None) ```  Relevant log output ```shell 20250217 12:20:26.671705: F tensorflow/core/framework/tensor_shape.cc:358] Check failed: d >= 0 (0 vs. 4) Aborted (core dumped) ```",2025-02-17T15:33:19Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/87421,", I request you to take a look at this issue where a similar issue was raised and it is still open. Also I request to follow the similar issues which has been proposed to have the updates on the similar issue. Thank you!","> [](https://github.com/cybersupersoap), I request you to take a look at this issue where a similar issue was raised and it is still open. Also I request to follow the similar issues which has been proposed to have the updates on the similar issue. Thank you! Thank you for your reply!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,cybersupersoap,"`tf.raw_ops.LookupTableExportV2` aborts with ""Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64"""," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tfnightly 2.19.0  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04 LTS  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? encountered an `aborted issue` in TensorFlow when I used API `tf.raw_ops.LookupTableExportV2` . I have confirmed that below code would crash on `tfnightly 2.19.0dev20250207` (nightlybuild). Please find the gist to reproduce the issue.  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np keys = tf.constant([0, 1, 2], dtype=tf.int64) values = tf.constant([1, 2, 3], dtype=tf.float32)   table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(keys, values), 1) export_keys, export_values = tf.raw_ops.LookupTableExportV2(table_handle=table.resource_handle, Tkeys=tf.int64, Tvalues=tf.int64) ```  Relevant log output ```shell 20250217 12:21:39.561442: F tensorflow/core/framework/tensor.cc:857] Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64 Aborted (core dumped) ```",2025-02-17T15:19:44Z,type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/87420,I tried running your code on Colab using TensorFlow v2.18.0 and the nightly version. I faced the same issue. Please find gist here for reference. Thank you!
tpu,cybersupersoap,"`tf.experimental.numpy.swapaxes` aborts with ""Check failed: d >= 0 (0 vs. -2)"""," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tfnightly 2.19.0  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04 LTS  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `aborted issue` in TensorFlow when I used API `tf.experimental.numpy.swapaxes` . I have confirmed that below code would crash on `tfnightly 2.19.0dev20250207` (nightlybuild) Please find the gist to reproduce the issue.  Standalone code to reproduce the issue ```shell import tensorflow import tensorflow as tf import numpy import numpy as np input_data = np.arange(24).reshape(2, 3, 4) output_data = tf.experimental.numpy.swapaxes(input_data, 5, 2) ```  Relevant log output ```shell 20250217 12:30:19.854628: F tensorflow/core/framework/tensor_shape.cc:358] Check failed: d >= 0 (0 vs. 2) Aborted (core dumped) ```",2025-02-17T15:12:11Z,type:bug comp:apis TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/87419,", tf.experimental.numpy.swapaxes is in an experimental stage which is equivalent to numpy.swapaxes. The numpy.swapaxes args are axis1 and 2 which are int. I tried with the int as mentioned in the official document and the output was as expected. Kindly find the gist of it here. https://www.tensorflow.org/api_docs/python/tf/experimental/numpy/swapaxes https://numpy.org/doc/stable/reference/generated/numpy.swapaxes.html Thank you!", Thank you for looking at this. I think this issue can be treated as a bug because the crash could potentially lead to a DOS attack.
tpu,uri-granta,Memory leak when compiling tfp.util.TransformedVariable since TF 2.14 (worked fine before!)," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.14+ (present in 2.14, 2.16, 2.18; not an issue in 2.11, 2.12, 2.13)  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Upgrading a GPflowbased workflow to use TF 2.14 is showing memory leaks where none previously occurred. Preliminary investigations connected this to the compilation of GPflow models: in the code snippet below the memory usage increases over time (by around 1MB per iteration for a total of ~200MB) when run with TF 2.14, 2.16 or 2.18, but not when run with TF 2.11, 2.12 or 2.13. Raising as a TF bug as code that was previously executing fine is now appearing to leak memory, though there is also a chance that this is an issue in GPflow. Any suggestions for helping identify the leaked memory would be very welcome!  Standalone code to reproduce the issue ```shell import gc import gpflow import keras.backend import numpy as np import os import psutil import tensorflow as tf  define simple GPflow model X = np.array([[0.865], [0.666], [0.804], [0.771], [0.147], [0.866], [0.007], [0.026], [0.171], [0.889], [0.243], [0.028]]) Y = np.array([[1.57], [3.48], [3.12], [3.91], [3.07], [1.35], [3.80], [3.82], [3.49], [1.30], [4.00], [3.82]]) model = gpflow.models.GPR((X, Y), kernel=gpflow.kernels.SquaredExponential())  repeatedly compile and evaluate the model's log marginal likelihood for i in range(200):      garbage collect to remove some of the noise     keras.backend.clear_session(); gc.collect()      compile and evaluate closure     tf.function(model.log_marginal_likelihood)()      track memory usage     print(f""[{i+1}] Memory usage: {psutil.Process(os.getpid()).memory_info().rss / 1024 **2} MiB"") ```  Relevant log output ```shell ```",2025-02-17T11:48:58Z,type:bug type:performance TF 2.18,open,0,5,https://github.com/tensorflow/tensorflow/issues/87414,"Hi **granta** , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow 2.13.0 it is working as expected and 2.18.0 faced the same issue like you. The main cause might be version compatibility. When using GPflow, you need to install the GPflow package along with its dependencies TensorFlow and TensorFlow Probability ensuring all versions are compatible. According to the GPflow documentation, compatibility is mentioned only up to TensorFlow 2.12. Therefore, the exact TensorFlow Probability version required for the latest TensorFlow versions is unknown. I recommend first confirming the compatibility versions in the GPflow documentation. It would be best to check with GPflow for the correct compatibility versions for the latest TensorFlow releases. Thank you!","Hi , Thanks for looking into this! **I've since managed to reproduce the issue without using `gpflow` at all, just with a `TransformedVariable` from `tensorflowprobability`.** (Also FYI `gpflow` does actually support tensorflow up to 2.16, though it looks like the docs aren't up to date!) As before, the following script shows continually increasing memory usage when run with TF 2.1418 but not when run with TF 2.1113, though the memory increase is smaller than before (only around 0.1MB per iteration) and takes a few iterations to get going. ```python import gc import os os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' import psutil import keras.backend import tensorflow as tf import tensorflow_probability as tfp class Kernel:     def __init__(self):          note that using tfp.bijectors.Identity() instead doesn't leak (though tfp.bijectors.Exp() does)         self._variance = tfp.util.TransformedVariable(tf.constant(1.), tfp.bijectors.Softplus())     def variance(self):          note that just returning self.variance doesn't leak         return tf.squeeze(self._variance) kernel = Kernel() for i in range(400):      garbage collect to remove noise     keras.backend.clear_session(); gc.collect()      compile and evaluate closure     tf.function(lambda: kernel.variance())()      track memory usage     print(f""[{i+1}] Memory usage: {psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2} MiB"") ```","FYI here's an even simpler class that can be used in the previous example: ```python class Kernel:     def __init__(self):         self._variance = tfp.util.DeferredTensor(tf.Variable(1.), tfp.bijectors.Shift(0.0))     def variance(self):         return tf.convert_to_tensor(self._variance) ``` As before, setting the bijector to `tfp.bijectors.Identity()` removes the leak. However, changing this bijector's `_forward(self, x)` method to return `x + 0` rather than `x` reintroduces it. AFAICT the bijector is never evaluated in the test, so this is presumably all to do with compilation.","So it looks like the issue in this particular example is that in every compilation, a pair of weak references to the input and output `SymbolicTensor`'s aren't being garbage collected, which in turn means that their cleanup callbacks aren't being called to remove them from the global `BijectorCache` (`tfp.bijectors.bijector._cache`). By contrast, both in TF=2.14 both references *are* garbage collected and the callbacks *are* called. Note that the `BijectorCache` code in tfp hasn't changed in the last 4 years, and the `gpflow` example from the beginning continues to show issues even after manually disabling the cache, so I suspect the cache is just a symptom and the underlying cause is somehow connected to why the compiled graphs aren't being garbage collected (or at least why their cleanup callbacks aren't being called).","So I'm pretty sure the `BijectoCache` issue is due to https://github.com/tensorflow/tensorflow/commit/333ee99ecb9bd8f122c683defa74281bb3bd1664. Before this change, when initialising a `Function`, TF used to call `TracingCompiler._get_concrete_function_internal_garbage_collected`, which ended up creating and deleting a `ConcreteFunctionGarbageCollector`, dismantling the function graph at the end. After this change, when initialising a `Function`, TF now calls `tracing_compilation.trace_function`, which similarly creates a `ConcreteFunctionGarbageCollector`. However, because `tracing_options.bind_graph_to_function` is `False` (the default value, unchanged by `Function._generate_tracing_options`) it also calls `release` on the garbage collector before deleting it, preventing it from dismantling the function graph (and therefore from cleaning up the bijector cache). Note though that this probably isn't the whole story. Changing `Function._initialize` to use `dataclasses.replace(self._variable_creation_config, bind_graph_to_function=True)` instead does fix the memory leak in the two bijector examples above. However, it doesn't fix the original leaks discovered in `gpflow` (which based on profiling may now be connected to the `gradient_registry` somehow)."
strategy,copybara-service[bot],PR #22706: Increase the compilation thread stack size to 4MB,"PR CC(CUDA 10): Increase the compilation thread stack size to 4MB Imported from GitHub PR https://github.com/openxla/xla/pull/22706 When JIT compiling modules using a process runner, the default stack size on OS X is too low. Fx this StableHLO module will fail to compile with a stack overflow error using this command: ``` bazel run spawn_strategy=sandboxed //xla/tools:run_hlo_module  platform=CPU input_format=stablehlo stack_overflow.mlir ``` This PR will increase the thread stack size for the compilation thread pool to 4MB per thread. Copybara import of the project:  31d88dbd8cfbb99986202bd450c71fd21b5a4223 by Kasper Nielsen : Increase the compilation thread stack size to 4MB Merging this change closes CC(CUDA 10) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22706 from kasper0406:kn/increasecompilationthreadstack 31d88dbd8cfbb99986202bd450c71fd21b5a4223",2025-02-17T08:46:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87407
tpu,AD-lite24,Warning in XNNPACK: unable to enable JIT: not compiled with JIT enabled," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.1  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version GCC 9 aarch cross compiler  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Upon updating our tflite service to 2.17.1, XNNPACK fails to apply with this cryptic error  `Warning in XNNPACK: unable to enable JIT: not compiled with JIT enabled` I don't understand this error at all, there is really only way to build XNNPACK while building TFLite as described in CMake, so I have no idea what this is referring to. We didn't face this issue with earlier versions (2.7.0)  Standalone code to reproduce the issue ```shell Just applying XNNPACK delegate to any model ```  Relevant log output ```shell ```",2025-02-16T14:38:54Z,stat:awaiting response type:support stale comp:lite comp:lite-xnnpack 2.17,closed,0,11,https://github.com/tensorflow/tensorflow/issues/87385,"Hi, lite24  Thank you for bringing this issue to our attention, I believe you followed this official documentation Build LiteRT with CMake, if possible could you please help me with exact steps before encountering mentioned warning to replicate the same behavior from my end ? Thank you for your cooperation and understanding.","Hi   Please find the CMakeLists.txt we use to build the project (tflite is built alongside it) ```cmake cmake_minimum_required(VERSION 3.10) SET(TARGET voxltfliteserver) set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(TFLITE_ENABLE_GPU ON CACHE BOOL ""Enable GPU delegate support"") set(TFLITE_ENABLE_NNAPI ON CACHE BOOL ""Enable NNAPI delegate support"") set(XNNPACK_ENABLE_ARM_BF16 OFF CACHE BOOL) set(XNNPACK_ENABLE_ARM_I8MM OFF CACHE BOOL) set(TENSORFLOW_SOURCE_DIR ""~/tensorflow_src"" CACHE PATH ""Directory with checkout of tensorflow"") add_subdirectory(""${TENSORFLOW_SOURCE_DIR}/tensorflow/lite"" ""${CMAKE_CURRENT_BINARY_DIR}/tensorflowlite"" EXCLUDE_FROM_ALL) option(BUILD_QRB5165 ""Build the qrb5165 binary"" OFF) if(BUILD_QRB5165)     add_definitions(DBUILD_QRB5165) endif() if(CMAKE_BUILD_TYPE STREQUAL ""DEBUG"")     set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} g3 Wall Wuninitialized Wmaybeuninitialized fnoomitframepointer"") elseif(CMAKE_BUILD_TYPE STREQUAL ""RELEASE"")     set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} O3 fsee fomitframepointer fnosignedzeros fnomatherrno funrollloops"") endif()  Enable compile optimizations  Enable debug flags (use if you want to debug in gdb)  Build from all source files file(GLOB_RECURSE all_src_files *.c*) add_executable(${TARGET} 	${all_src_files} ) include_directories(     ../include/     ../include/model_helper/     /usr/include/opencv4/       apq8096 SPECIFIC      /usr/include/tensorflow/lite/tools/make/downloads/absl/      /usr/include/tensorflow/lite/tools/make/downloads/flatbuffers/include/      qrb5165 SPECIFIC     /usr/include/flatbuffers/include/     /usr/include/abseilcpp/     /usr/include/ruy/ ) set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} Wnoimplicitfallthrough"") if (BUILD_QRB5165) set(LINK_LIBS ""modal_pipe""     ""modal_json""     ""glib2.0""     ""dl""     ""stdc++""     ""pthread""     ""z""     ""cutils""     ""log""     ""sync""     ""gthread2.0""     ""pcre""     ""modal_pipe""     ""modal_json""     ""opencv_core""     ""opencv_highgui""     ""opencv_imgproc""     ""opencv_imgcodecs""     ""opencv_dnn""     ""gsl""     ""llvmqcom""     ""adreno_utils""     ""OpenCL""     ""CB""     ""EGL_adreno""     ""GLESv2_adreno""     ""pthreadpool""     ""rt"") endif() target_link_libraries(${TARGET}     Wl,rpathlink,/usr/lib64/     L/usr/lib64/     ${LINK_LIBS}     tensorflowlite )  make sure everything is installed where we want  LIB_INSTALL_DIR comes from the parent cmake file install( 	TARGETS			${TARGET} 	LIBRARY			DESTINATION ${LIB_INSTALL_DIR} 	RUNTIME			DESTINATION /usr/bin 	PUBLIC_HEADER	DESTINATION /usr/include ) ``` and our build script ```shell echo ""Applying MAI Patches"" patch uN ~/tensorflow_src/tensorflow/lite/CMakeLists.txt i ~/patches/cmake_fix.patch echo ""Done Applying Patches"" mkdir p build cd build   cmake DCMAKE_TOOLCHAIN_FILE=${TOOLCHAIN_QRB5165} DCMAKE_BUILD_TYPE=DEBUG DBUILD_QRB5165=${BUILD_QRB5165} DCMAKE_VERBOSE_MAKEFILE=ON DCMAKE_CXX_FLAGS=""${CMAKE_CXX_FLAGS} std=c++17 march=armv8a"" ${EXTRA_OPTS} ../ make j5 VERBOSE=1 cd ../ ``` with patches ```  tensorflow/tensorflow/lite/CMakeLists.txt.orig	20220209 08:54:18.370750801 0800 +++ tensorflow/tensorflow/lite/CMakeLists.txt	20220209 08:43:51.505795653 0800 @@ 59,13 +59,13 @@    ${CMAKE_PREFIX_PATH}  )  include(CMakeDependentOption) option(TFLITE_ENABLE_RUY ""Enable experimental RUY integration"" OFF) +option(TFLITE_ENABLE_RUY ""Enable experimental RUY integration"" ON)  option(TFLITE_ENABLE_RESOURCE ""Enable experimental support for resources"" ON)  option(TFLITE_ENABLE_NNAPI ""Enable NNAPI (Android only)."" ON) cmake_dependent_option(TFLITE_ENABLE_NNAPI_VERBOSE_VALIDATION ""Enable NNAPI verbose validation."" OFF +cmake_dependent_option(TFLITE_ENABLE_NNAPI_VERBOSE_VALIDATION ""Enable NNAPI verbose validation."" ON                         ""TFLITE_ENABLE_NNAPI"" ON)  option(TFLITE_ENABLE_MMAP ""Enable MMAP (unsupported on Windows)"" ON) option(TFLITE_ENABLE_GPU ""Enable GPU"" OFF) +option(TFLITE_ENABLE_GPU ""Enable GPU"" ON)  option(TFLITE_ENABLE_METAL ""Enable Metal delegate (iOS only)"" OFF)  option(TFLITE_ENABLE_XNNPACK ""Enable XNNPACK backend"" ON) @@ 86,7 +86,7 @@  endif()  set(_TFLITE_ENABLE_NNAPI ""${TFLITE_ENABLE_NNAPI}"")  if(NOT ""${CMAKE_SYSTEM_NAME}"" STREQUAL ""Android"")   set(_TFLITE_ENABLE_NNAPI OFF) +  set(_TFLITE_ENABLE_NNAPI ON)  endif()  set(_TFLITE_ENABLE_MMAP ""${TFLITE_ENABLE_MMAP}"")  if(${CMAKE_SYSTEM_NAME} MATCHES ""Windows"") ``` Once the project is built, the entry point simply invokes the interpreter in a standard manner and tries to apply the delegate. There is a an issue in the cross compilation process with respect to protobuf where the protobuf library built is built for the target but the host tries to use it. We resolved it by manually building protobuf for the host architecture and setting the appropriate system paths.", Was there any update on this? ,"Hi, lite24  If possible could you please add this `set(XNNPACK_ENABLE_JIT ON CACHE BOOL ""Enable JIT in XNNPACK"")` in CMakeLists.txt and see is it resolving your issue or not ? after updating the CMake configuration clean your build directory and recompile to apply the changes. Thank you for your cooperation and patience.","Hi   Thank you for the reply I was not aware that such a flag existed. Yes that error was resolved but a new error has popped up ``` INFO: Created TensorFlow Lite XNNPACK delegate for CPU. INFO: XNNPack weight cache not enabled. VERBOSE: Replacing 94 out of 144 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 21 partitions for the whole graph. ERROR: /home/root/tensorflow_src/tensorflow/lite/core/subgraph.cc:600 tensor>delegate == nullptr  tensor>delegate == delegate was not true. ERROR: Restored original execution plan after delegate application failure. INFO: Error in applying the default TensorFlow Lite delegate indexed at 0, and all previously applied delegates are reverted. Inference time: 115846 ms Output: 6.30584e43 ``` This error also did not used to occur earlier and has only come up once we upgraded tflite. Could you please tell us why this might be occuring","Hi, lite24 Good to hear that initial reported issue got resolved, Troubleshooting TFLite delegate application failures can be challenging.  I recommend the following approaches: Increase logging verbosity to obtain more detailed information about the delegate application process and also verify the model compatibility with the chosen delegate. You can run the model without any delegates to ensure that the model itself is functioning correctly. This helps isolate whether the issue is with the model or the delegate.  Please check if the tensors are already owned by another delegate. The error message `tensor>delegate == nullptr  tensor>delegate == delegate was not true` indicates a conflict please ensure that tensors are not being shared between delegates.You can do this by inspecting the tensor's delegate before applying a new one.  Please check for unsupported operations some operations in your model may not be supported by the delegate. For XNNPACK, you can find the list of supported operations in the XNNPACK documentation and for GPU delegates  Thank you for your cooperation and understanding."," As you said it likely fails due to tensors being owned by two delegates. The issue occurs due to a failure to apply GPU delegate (which is a separate issue), and it then falls back to XNNPACK to run on CPU. If I don't apply GPU delegates XNNPACK works fine.  This I believe is a bug that did not occur in older versions, the fallback to XNNPACK is likely not changing ownership of the tensors.","Hi, lite24  I apologize for the delayed response, if possible could you please give it try with latest stable version of `TensorFlow 2.19.0` and `tfnightly` and see is it resolving your issue or not ? Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,khangtruong2252314,Colab TPU crash on transformer import," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.18  Custom code No  OS platform and distribution Google colab default  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Import transformer module get crash on a colab notebook with TPU   Standalone code to reproduce the issue ```shell from transformers import TFT5ForConditionalGeneration ''' This will crash the TPU session ''' ```  Relevant log output ```shell ""Crashed for unknown reason"" ```",2025-02-16T13:13:48Z,type:bug,closed,0,5,https://github.com/tensorflow/tensorflow/issues/87383,Are there any error messages during the installation process? It could help identify what is causing the issue,"No, sorry. I just open the colab and import, then crash. Perhaps you now can recreate it quickly as it is the default colab session. There are some session log that might be useful.","Is there something in the error message that points to Tensorflow? Or, in other words, should this be opened on Colab repo or on transformers repo?","Ah, sorry, I was working with tensorflow that I didn’t realize it was colab’s fault. ",Are you satisfied with the resolution of your issue? Yes No
tpu,MoritzKronberger,GPU and CPU utilization dropping to 0% during long training runs," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.1 LTS  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 2.14  GPU model and memory _No response_  Current behavior? When running the attached test script for longer, at some point during the training GPU and CPU utilization will both fall to 0%, although neither VRAM nor RAM are exhausted. The training on my NVIDIA L40S vGPU slows from ~35ms/step with batch size 256 to minutes per step. Training speed only recovers on reboot. Larger batch sizes will make the error occurr later during training. I am unsure if this is an issue of my machine/the vGPU configuration.  Standalone code to reproduce the issue ```shell import tensorflow as tf print(""TensorFlow version:"", tf.__version__) print(""CPU devices:"", tf.config.list_physical_devices(""CPU"")) print(""GPU devices:"", tf.config.list_physical_devices(""GPU"")) print(""Using cuDNN:"", tf.test.is_built_with_cuda()) gpus = tf.config.list_physical_devices(""GPU"") for gpu in gpus:     tf.config.experimental.set_memory_growth(gpu, True) with tf.device(""GPU:0""):     num_samples = 18_000     input_length = 480     input_channels = 1     X = tf.random.normal((num_samples, input_length, input_channels), dtype=tf.float32)     Y = tf.random.normal((num_samples, input_length, input_channels), dtype=tf.float32)     input = tf.keras.layers.Input(shape=(input_length, input_channels))     conv = tf.keras.layers.Conv1D(         filters=256,         kernel_size=5,         strides=1,         padding='same',         activation=None,         input_shape=(input_length, input_channels)     )(input)     pool = tf.keras.layers.AveragePooling1D(         pool_size=2,         strides=2     )(conv)     deconv = tf.keras.layers.Conv1DTranspose(         filters=256,         kernel_size=4,         strides=2,         padding='same',         activation=None     )(pool)     dense = tf.keras.layers.Dense(2048*2, activation='tanh')(deconv)     out = tf.keras.layers.Dense(input_channels, activation='linear')(dense)     model = tf.keras.models.Model(         inputs=input,         outputs=out     )   type: ignore     model.summary()     model.compile(optimizer='adam', loss='mse', jit_compile=True)     dataset = tf.data.Dataset.from_tensor_slices((X, Y))     dataset = dataset.batch(256).prefetch(tf.data.AUTOTUNE)     model.fit(dataset, epochs=500, verbose=1) ```  Relevant log output ```shell 20250216 06:35:01.114446: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250216 06:35:01.824571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1739687702.107514    2941 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1739687702.197108    2941 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250216 06:35:02.874809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. TensorFlow version: 2.18.0 CPU devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')] GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] Using cuDNN: True I0000 00:00:1739687707.790282    2941 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5117 MB memory:  > device: 0, name: NVIDIA L40S8C, pci bus id: 0000:03:04.0, compute capability: 8.9 20250216 06:35:07.832778: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:07.838416: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. /home/mokro1/canilm/venv/lib/python3.12/sitepackages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.   super().__init__(activity_regularizer=activity_regularizer, **kwargs) 20250216 06:35:07.899661: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing Cast input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:07.903310: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.228058: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.228711: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.230774: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing StatelessRandomGetKeyCounter input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:CPU:0 but is actually on /job:localhost/replica:0/task:0/device:GPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.249462: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.260795: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing Cast input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.263365: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.264361: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.265277: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing StatelessRandomGetKeyCounter input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:CPU:0 but is actually on /job:localhost/replica:0/task:0/device:GPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.274980: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing Cast input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.276006: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.276518: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.276878: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing StatelessRandomGetKeyCounter input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:CPU:0 but is actually on /job:localhost/replica:0/task:0/device:GPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.285657: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing Cast input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.286505: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.286793: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.287169: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing StatelessRandomGetKeyCounter input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:CPU:0 but is actually on /job:localhost/replica:0/task:0/device:GPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. ```",2025-02-15T21:12:30Z,type:bug TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/87366,"Hi **** , Thanks for raising your concern here. Could you please provide information about the cuDNN version you are using? There might be a mismatch in version compatibility. Please check all the compatible versions to avoid issues. I am providing the documentation for your reference. I also tried with reduced sizes, and it worked fine for me. Please find gist1 and gist2 here for your reference. Thank you!","I forgot to add `tf.config.experimental.set_device_policy(""warn"")` to the standalone code, which produces the log output on my local machine. Trying this in Colab did not produce the same logs, but I am usure if Colab is filtering logs. The crash happens after ~200 epochs on my machine (without reduced sizes), 10 epochs are not an issue. Regarding cuDNN: `nvidiacudnncu12=9.3.0.75` and `nvidiacudnncu11=8.5.0.96` are installed, according to TF logs 9.3.0.75 is used."
tpu,tvortsa,Comeon guys!," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version all  Custom code Yes  OS platform and distribution no  Mobile device no  Python version no  Bazel version no  GCC/compiler version no  CUDA/cuDNN version no  GPU model and memory no  Current behavior? Это конечно все интересно, нейросетки, все дела, но, блин: Вопервых: СUDA только в линукс или я недопонял чегото? Вовторых: на Deno я так и не смог поставить этот ваш tensorflow Хреновато както для мегагуглпроекта не? Я пока на brain.js пошел, а что делать!?  Standalone code to reproduce the issue ```shell meh ```  Relevant log output ```shell ```",2025-02-15T12:41:03Z,type:bug invalid,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87362,Please don't spam
tpu,Raju-07,Request for Python 3.13.2 Compatibility in TensorFlow," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.8 i dont know  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Subject: Request for Python 3.13.2 Compatibility in TensorFlow Dear TensorFlow Team, I hope this message finds you well. I am writing to express my appreciation for the incredible work you have done with TensorFlow. It has been an invaluable tool for my machine learning and data science projects. I recently upgraded to Python 3.13.2 and noticed that TensorFlow is not yet compatible with this version. As an enthusiastic user of both Python and TensorFlow, I kindly request that you consider adding support for Python 3.13.2 in an upcoming release. This would greatly benefit the community and allow us to continue using the latest Python features alongside TensorFlow. Thank you for your attention to this matter. I look forward to your response and appreciate your continuous efforts in improving TensorFlow. Best regards, Raju Yadav  Standalone code to reproduce the issue ```shell Subject: Request for Python 3.13.2 Compatibility in TensorFlow Dear TensorFlow Team, I hope this message finds you well. I am writing to express my appreciation for the incredible work you have done with TensorFlow. It has been an invaluable tool for my machine learning and data science projects. I recently upgraded to Python 3.13.2 and noticed that TensorFlow is not yet compatible with this version. As an enthusiastic user of both Python and TensorFlow, I kindly request that you consider adding support for Python 3.13.2 in an upcoming release. This would greatly benefit the community and allow us to continue using the latest Python features alongside TensorFlow. Thank you for your attention to this matter. I look forward to your response and appreciate your continuous efforts in improving TensorFlow. Best regards, Raju Yadav ```  Relevant log output ```shell ```",2025-02-15T09:54:25Z,type:support,closed,0,3,https://github.com/tensorflow/tensorflow/issues/87361,Please see https://github.com/tensorflow/tensorflow/issues/78774.,Please search for duplicates before opening new issues.,Are you satisfied with the resolution of your issue? Yes No
quantization,copybara-service[bot],Migrate tf/compiler/mlir/quantization/stablehlo:quantization_patterns to use TF's fork of QuantOps Dialect from LiteRT's fork,Migrate tf/compiler/mlir/quantization/stablehlo:quantization_patterns to use TF's fork of QuantOps Dialect from LiteRT's fork,2025-02-14T21:38:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87336
tpu,copybara-service[bot],[RFC] [XLA][TPU][MSA] Use HW Spec instead of hardcoding bandwidth multipliers,"[RFC] [XLA][TPU][MSA] Use HW Spec instead of hardcoding bandwidth multipliers Changes:  Instead of using a constant for bandwidth multiplier for alternative memory vs default memory, use HW spec data to derive the multiplier differentiating read vs write.  While it doesn't affect performance much, this change adds support for providing different read vs write multipliers.",2025-02-14T18:07:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87325
opt,copybara-service[bot],PR #22680: Changing the default value of the flag xla_dump_hlo_as_long_text,"PR CC(1.12rc0 cherrypick request: Pin wheel=0.31.1 to work around issue https://github.com/pypa/auditwheel/issues/102): Changing the default value of the flag xla_dump_hlo_as_long_text Imported from GitHub PR https://github.com/openxla/xla/pull/22680 This stems from the difference in HLO dumps from tools like hloopt, multihost_hlo_runner, and hlo_runner_main. Although the options related to printing HLO are tied to the HLO via DebugOptions, these tools have different behaviors because the tool hloopt uses `ToString()` function from `HloModule` while `functional_hlo_runner` uses a separate dumping utility from `xla/service/dump.cc`. To standardize this behavior, we first are making the default behavior uniform by setting the default value of long text HLO dumps to be true. This ensures that the HLO dumps will be functional by default (for example, backend_config will be printed). Copybara import of the project:  0818c7871688a8e18cefdaea7bd17fd7aba293d9 by Shraiysh Vaishay : Changing the default value of the flag xla_dump_hlo_as_long_text This stems from the difference in HLO dumps from tools like hloopt, multihost_hlo_runner, and hlo_runner_main. Although the options related to printing HLO are tied to the HLO via DebugOptions, these tools have different behaviors because the tool hloopt uses `ToString()` function from `HloModule` while `functional_hlo_runner` uses a separate dumping utility from `xla/service/dump.cc`. To standardize this behavior, we first are making the default behavior uniform by setting the default value of long text HLO dumps to be true. This ensures that the HLO dumps will be functional by default (for example, backend_config will be printed). Merging this change closes CC(1.12rc0 cherrypick request: Pin wheel=0.31.1 to work around issue https://github.com/pypa/auditwheel/issues/102) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22680 from shraiysh:xla_dump_hlo_as_long_text_as_true 0818c7871688a8e18cefdaea7bd17fd7aba293d9",2025-02-14T10:55:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87312
opt,copybara-service[bot],"PR #22645: Fix error in the gpu_specs README. The spec is the TargetConfig, which includes the device description.","PR CC(Problem with CUDA 10 and Tensorflow 1.11): Fix error in the gpu_specs README. The spec is the TargetConfig, which includes the device description. Imported from GitHub PR https://github.com/openxla/xla/pull/22645 Copybara import of the project:  5fefad4bdb1b947844c7c2d8ff1029f5c99aace5 by Dimitris Vardoulakis : Fix error in the specs README. The spec is the TargetConfig, which includes the device description.  39f8e9f343de45b4678a6760b9a9720d48163299 by Dimitris Vardoulakis : Update xla/tools/hlo_opt/gpu_specs/README.md Coauthoredby: Allan Renucci  Merging this change closes CC(Problem with CUDA 10 and Tensorflow 1.11) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22645 from dimvar:addreadmetogpu_specs 39f8e9f343de45b4678a6760b9a9720d48163299",2025-02-14T09:30:47Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/87309,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
tensorrt,copybara-service[bot],PR #22575: [XLA:GPU] Fix triton sparse dot lowering on Blackwell,"PR CC(Preferred way of installing TensorRT with Tensorflow 1.11 on Ubuntu?): [XLA:GPU] Fix triton sparse dot lowering on Blackwell Imported from GitHub PR https://github.com/openxla/xla/pull/22575 Sparse dot is supported for MMA v2 and v3 only, and sm100/sm120 should use MMA v2 (v3 is Hopperonly). Copybara import of the project:  bd4c827db0e4adbff629bf0b02d09ff2860e4fb2 by Sergey Kozub : [XLA:GPU] Fix triton sparse dot lowering on Blackwell Merging this change closes CC(Preferred way of installing TensorRT with Tensorflow 1.11 on Ubuntu?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22575 from openxla:skozub/sm100_sparse bd4c827db0e4adbff629bf0b02d09ff2860e4fb2",2025-02-14T09:05:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87308
opt,copybara-service[bot],PR #22680: Changing the default value of the flag xla_dump_hlo_as_long_text,"PR CC(1.12rc0 cherrypick request: Pin wheel=0.31.1 to work around issue https://github.com/pypa/auditwheel/issues/102): Changing the default value of the flag xla_dump_hlo_as_long_text Imported from GitHub PR https://github.com/openxla/xla/pull/22680 This stems from the difference in HLO dumps from tools like hloopt, multihost_hlo_runner, and hlo_runner_main. Although the options related to printing HLO are tied to the HLO via DebugOptions, these tools have different behaviors because the tool hloopt uses `ToString()` function from `HloModule` while `functional_hlo_runner` uses a separate dumping utility from `xla/service/dump.cc`. To standardize this behavior, we first are making the default behavior uniform by setting the default value of long text HLO dumps to be true. This ensures that the HLO dumps will be functional by default (for example, backend_config will be printed). Copybara import of the project:  0818c7871688a8e18cefdaea7bd17fd7aba293d9 by Shraiysh Vaishay : Changing the default value of the flag xla_dump_hlo_as_long_text This stems from the difference in HLO dumps from tools like hloopt, multihost_hlo_runner, and hlo_runner_main. Although the options related to printing HLO are tied to the HLO via DebugOptions, these tools have different behaviors because the tool hloopt uses `ToString()` function from `HloModule` while `functional_hlo_runner` uses a separate dumping utility from `xla/service/dump.cc`. To standardize this behavior, we first are making the default behavior uniform by setting the default value of long text HLO dumps to be true. This ensures that the HLO dumps will be functional by default (for example, backend_config will be printed). Merging this change closes CC(1.12rc0 cherrypick request: Pin wheel=0.31.1 to work around issue https://github.com/pypa/auditwheel/issues/102) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22680 from shraiysh:xla_dump_hlo_as_long_text_as_true 0818c7871688a8e18cefdaea7bd17fd7aba293d9",2025-02-14T09:00:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87307
tpu,copybara-service[bot],"Improve standalone tool in ""compile"" mode, now able to write byte code to multiple output files.","Improve standalone tool in ""compile"" mode, now able to write byte code to multiple output files. e.g. ./apply_plugin_main compile libs  model  o  o  soc_man  soc_model ",2025-02-14T08:23:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87306
quantization,copybara-service[bot],Support per-axis quantization of TFL::ReshapeOp,Support peraxis quantization of TFL::ReshapeOp,2025-02-14T01:20:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87280
quantization,copybara-service[bot],Fix TFL::ReshapeOp to allow correct per-axis quantization types.,Fix TFL::ReshapeOp to allow correct peraxis quantization types.,2025-02-14T00:32:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87276
quantization,copybara-service[bot],Add conversion from quantization composites to stablehlo native quantized ops.,Add conversion from quantization composites to stablehlo native quantized ops.,2025-02-14T00:20:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87275
tpu,wonjeon,[mlir][tosa] Update Tensorflow to match TOSA v1.0 specification (part 3),"We've been pushing TOSA v1.0 LLVM patches to upstream. This PR includes commits to keep the following operators to be aligned with LLVM: [mlir][tosa] Change 'shape' of RESHAPE from attribute to input shape https://github.com/llvm/llvmproject/pull/125789 [mlir][tosa] Make Convolution Zero Points Inputs https://github.com/llvm/llvmproject/pull/122939 [mlir][tosa] Remove Quantization Attribute https://github.com/llvm/llvmproject/pull/125479 [mlir][tosa] Change ClampOp's min/max attributes https://github.com/llvm/llvmproject/pull/125197 [mlir][tosa] Make TOSA RESIZE's scale, offset, border as Input https://github.com/llvm/llvmproject/pull/124956 This patch including: [mlir][tosa] Change 'shape' attribute of RESHAPE operator to become an input Signedoffby: Won Jeon  ChangeId: I938503349f38b64db5e77a01c3a7b2bb33e8f041 [mlir][tosa] Switch zero point of convolutions to input variable type ChangeId: Ifd3f24779886377735e6694f716f23c5137f9138 [Tosa] Refactor QuantizationAttr changes due to removal of quantization attr in TOSA dialect and due to name changes in while_loop region names Signedoffby: Tai Ly  ChangeId: I09533bffcd8e2179505c7e11e1320b673266585d [Tosa] ClampOp attributes changes This patch implements changes required by Tosa ClampOp's new min_val/max_val attributes Signedoffby: Tai Ly  ChangeId: I25ba0d077fa44d4c384ab094a6070a4743383414 [TOSA] Calculate unknown reshape dimension when input is static This commit updates the reshape legalization to calculate static shape and output type when a static input shape is provided and only one dimension is unknown. ChangeId: I0843549b47131b0852fbf375f00846b1fcbe8bc6 Signedoffby: Luke Hutton  [TOSA] Numerical mismatch on tfl.transpose_conv layer * Legalization now handles cases where the layer has a bias ChangeId: Ie3ba38644d1cf8e5d6f71271e8bb6f1b5636f406 [mlir][tosa] Change resize attrs to inputs This patch implements changes required by Tosa resize op's scale/offset/border changing from attributes to inputs. Signedoffby: Tai Ly  ChangeId: I9a4319ac53298c25568fc651e249528b9a9477fc",2025-02-13T23:59:44Z,size:XL comp:lite-tosa,closed,0,6,https://github.com/tensorflow/tensorflow/issues/87273,"Local testing successfully done: INFO: Build completed successfully, 27 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test        PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test         PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test             PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test      PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test                PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test   PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test           PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test  PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test      PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test         PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 2.9s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test     PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test        PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test   PASSED in 0.3s Executed 17 out of 17 tests: 17 tests pass.",Internal CI with bazel test passing.  Assigning to  . ,Tested and confirmed that this patch works with the following LLVM integration: https://github.com/tensorflow/tensorflow/commit/01197637cb4eee7da2f1645f7367268ee44efa87,"Tested and confirmed that this patch works with the LLVM integration, https://github.com/tensorflow/tensorflow/commit/4d8657f0c13540570c65bb8fc9a17b9d66755f6f."," , thanks for your comments. Changed the PR so that it contains multiple individual patches plus the test file changes. Hopefully this helps for your review.","Local testing successfully done: INFO: Analyzed 33 targets (0 packages loaded, 33384 targets configured). INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 2.061s, Critical Path: 0.25s INFO: 2 processes: 2 local. INFO: Build completed successfully, 2 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test    (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test       (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test  (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test (cached) PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 0.1s Executed 1 out of 17 tests: 17 tests pass."
yi,copybara-service[bot],Use seprate collective resource when scheduling p2p communication,Use seprate collective resource when scheduling p2p communication This is in preparation of removing all the 4 existing p2p resources. We are simplifying the pipeline parallelism implementation here.,2025-02-13T23:09:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87272
opt,copybara-service[bot],[DirectSession] Add experimental option for making `Finalize` clear the function library.,[DirectSession] Add experimental option for making `Finalize` clear the function library. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22588 from Tixxx:tixxx/event_barrier 3d78e81091eb0b6cb1dc22f1ff27c3a8de4cff4f,2025-02-13T22:55:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87271
opt,copybara-service[bot],[MHLO] Use generated constructors for MHLO->StableHLO pass.,"[MHLO] Use generated constructors for MHLO>StableHLO pass. This pass uses an option declared in tablegen, and needs the generated ctors to be able to create a pass that has an option set.",2025-02-13T22:05:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87267
opt,copybara-service[bot],PR #86630: Qualcomm AI Engine Direct - Support DUS and Pack Op in LiteRT,"PR CC(Qualcomm AI Engine Direct  Support DUS and Pack Op in LiteRT): Qualcomm AI Engine Direct  Support DUS and Pack Op in LiteRT Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/86630  WHAT  Pack is supported using QNN Concat to address type support issue.  DUS are supported partially for specific use cases.  Reuse input static tensor.  Use absl::Span instead of std::span.  TESTS ```  build bazel build c opt cxxopt=std=c++17 //tensorflow/lite/experimental/litert/vendors/qualcomm/compiler:qnn_compiler_plugin_test  run ./bazelbin/tensorflow/lite/experimental/litert/vendors/qualcomm/compiler/qnn_compiler_plugin_test ``` ``` [] Global test environment teardown [==========] 100 tests from 4 test suites ran. (4292 ms total) [  PASSED  ] 100 tests. ``` Copybara import of the project:  5fca94baacb2d669dd1d32c26d615b79274b3644 by chunhsue : pack op builder  e5994feec2100044b7994baaecadee69982e0de7 by chunhsue : Support DUS op  33980cbee14c6b55b53f52c29f64537acc8d59d4 by chunhsue : reuse static input tensor  988465882f4e4c2d90e46dd1cf7762c8760b1daf by chunhsue : use absl span, stick to c++17  f6bedf4292218118a9b45c0d18b8b1bf46f68238 by chunhsue : refactor pack op and correct the return value of builder  2a54437cf8f703c1e380125c2aa8a48d636abb70 by chunhsue : comment out failed tests due to QNN regression Merging this change closes CC(Qualcomm AI Engine Direct  Support DUS and Pack Op in LiteRT) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/86630 from jiunkaiy:dev/chunhsue/add_DUS_and_Pack 2a54437cf8f703c1e380125c2aa8a48d636abb70",2025-02-13T21:58:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87266
sharding,copybara-service[bot],Refactor `SpmdPartitioningVisitor::HandleReshape`. No behavior change.,"Refactor `SpmdPartitioningVisitor::HandleReshape`. No behavior change. This change recovers cl/717991433 with modification. The previous one is not a pure refactoring since it assumes that the inference `in_sharding_1 > out_sharding > in_sharding_2` will have `in_sharding_1 == in_sharding_2`. This assumption may be false. In the added test target, we reshape 24 > 6x4, and have the following inferred shardings. ``` in_sharding_1: [4] out_sharding: [2,1,2] last_tile_dim_replicate in_sharding_2: [2,2] last_tile_dim_replicate ``` This change should a pure refactoring without behavior change.",2025-02-13T21:23:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87265
opt,copybara-service[bot],Remake tool for inserting FileCheck directives in HLO optimization-pass tests.,"Remake tool for inserting FileCheck directives in HLO optimizationpass tests. The tool previously required the user to perform most of the steps manually, only automating the replacement of hardcoded symbols with regex captures. It now automatically runs an optimizer on the test file, writes FileCheck directives based on the optimized HLO, replaces symbols with regex captures, and inserts the FileCheck directives above their respective test cases. The step of replacing explicit symbols with regex captures has also been improved to support capturing function names and to only add disambiguation suffixes when necessary.",2025-02-13T19:42:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87258
opt,copybara-service[bot],Make `hlo-opt` return error status when given an invalid `--passes` argument.,Make `hloopt` return error status when given an invalid `passes` argument. `hloopt` previously logged an error in this case but did not indicate an error in its exit status. Note that the program now exits immediately upon encountering an invalid `passes` argument; it previously logged an error and continued executing. Fixing this bug also revealed that the `algebraic_simplifier.hlo` test file wasn't doing anything because the `AlgebraicSimplifier` pass wasn't registered in `hloopt`. This CL therefore also registers that pass and updates its test accordingly.,2025-02-13T19:22:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87257
yi,copybara-service[bot],[xla:python] Remove unused _single_device_array_to_np_array on ArrayImpl.,[xla:python] Remove unused _single_device_array_to_np_array on ArrayImpl.,2025-02-13T19:18:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87256
sharding,copybara-service[bot],Refactor `hlo_sharding_util::ReshapeSharding` by reducing the if-else branches.,"Refactor `hlo_sharding_util::ReshapeSharding` by reducing the ifelse branches. We also highlight a TODO in this cl, which will be revisited later. No behavior change.",2025-02-13T18:38:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87254
opt,copybara-service[bot],"PR #22645: Fix error in the gpu_specs README. The spec is the TargetConfig, which includes the device description.","PR CC(Problem with CUDA 10 and Tensorflow 1.11): Fix error in the gpu_specs README. The spec is the TargetConfig, which includes the device description. Imported from GitHub PR https://github.com/openxla/xla/pull/22645 Copybara import of the project:  5fefad4bdb1b947844c7c2d8ff1029f5c99aace5 by Dimitris Vardoulakis : Fix error in the specs README. The spec is the TargetConfig, which includes the device description.  39f8e9f343de45b4678a6760b9a9720d48163299 by Dimitris Vardoulakis : Update xla/tools/hlo_opt/gpu_specs/README.md Coauthoredby: Allan Renucci  Merging this change closes CC(Problem with CUDA 10 and Tensorflow 1.11) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22645 from dimvar:addreadmetogpu_specs 39f8e9f343de45b4678a6760b9a9720d48163299",2025-02-13T17:17:26Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87251,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
tpu,KeskoPreeti,Tensorflow in pycharm setup issue," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.12.0  Custom code Yes  OS platform and distribution windows 11  Mobile device windows11  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? error while running the file predeiction.py                                                                                                                                                      (c) Microsoft Corporation. All rights reserved. (venv_new) C:\analytics_driven_enginemaster\analytics_driven_enginemaster>python m analysis.prediction [20250213 15:38:26,485] INFO  Starting prediction pipeline... ['Number' 'Opened' 'State' 'Symptom category' 'Short description' 'State'] Traceback (most recent call last):   File ""C:\analytics_driven_enginemaster\analytics_driven_enginemaster\venv_new\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 62, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The paging file is too small for this operation to complete. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File """", line 1, in    File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\multiprocessing\spawn.py"", line 116, in spawn_main     exitcode = _main(fd, parent_sentinel)   File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\multiprocessing\spawn.py"", line 125, in _main     prepare(preparation_data)   File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\multiprocessing\spawn.py"", line 234, in prepare     _fixup_main_from_name(data['init_main_from_name'])   File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\multiprocessing\spawn.py"", line 258, in _fixup_main_from_name     main_content = runpy.run_module(mod_name,   File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 225, in run_module     return _run_module_code(code, init_globals, run_name, mod_spec)   File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 97, in _run_module_code     _run_code(code, mod_globals, init_globals,   File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""C:\analytics_driven_enginemaster\analytics_driven_enginemaster\analysis\prediction.py"", line 3, in      import tensorflow as tf   File ""C:\analytics_driven_enginemaster\analytics_driven_enginemaster\venv_new\lib\sitepackages\tensorflow\__init__.py"", line 37, in      from tensorflow.python.tools import module_util as _module_util   File ""C:\analytics_driven_enginemaster\analytics_driven_enginemaster\venv_new\lib\sitepackages\tensorflow\python\__init__.py"", line 36, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   File ""C:\analytics_driven_enginemaster\analytics_driven_enginemaster\venv_new\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 77, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\analytics_driven_enginemaster\analytics_driven_enginemaster\venv_new\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 62, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The paging file is too small for this operation to complete. expected  to process ticket analytics output files based on input  Standalone code to reproduce the issue ```shell import datetime import logging import tensorflow as tf import numpy as np from analysis.data_helpers import add_analyser, find_keywords, add_translated_data from analysis.save_and_get_models import get_model, load_tokeniser, load_vectorizer, decode_data as decode, get_encoder from config.config import output_col, input_path, unique_col, columns_for_parsing, output_path, output_col_apps, data  from TA_semantic_approach.main import predict_semantic_cti import gc import os import torch torch.set_num_threads(1)   Limits PyTorch parallelism to avoid excessive memory usage torch.cuda.empty_cache()   Clears unused GPU memory (if using CUDA)  Reduce TensorFlow memory usage os.environ[""TF_FORCE_GPU_ALLOW_GROWTH""] = ""true"" os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""   Forces CPU processing tf.config.threading.set_intra_op_parallelism_threads(1) tf.config.threading.set_inter_op_parallelism_threads(1) def predict_output_gartner_cti_infra(df, col=[], name='', path='', flag=''):     models = get_model(flag)      tokenize = load_tokeniser()     vectorizer = load_vectorizer(flag)     df = add_analyser(df, path=path, columns_for_parsing=col)      data = tokenize.texts_to_matrix(df['Analysis'])     value = vectorizer.transform(df['Analysis'].values).toarray()     for key, model in models.items():         input = {}         for col in output_col[key]['input']:             if col != 'Analysis':                 input[col.replace('/ ', '').replace(' ', '_').replace('/', '')] = tf.keras.utils.to_categorical(                     get_encoder(list(df[col]), col.replace('/ ', '').replace(' ', '_').replace('/', '')))             else:                 input[col] = value         data = model.predict([val for x, val in input.items()])         for i, col in enumerate(output_col[key]['output']):             if len(output_col[key]['output']) > 1:                 df[col] = decode([tf.argmax(x) for x in data[i]],                                  col.replace('/ ', '').replace(' ', '_').replace('/', ''))             else:                 df[col] = decode([tf.argmax(x) for x in data],                                  col.replace('/ ', '').replace(' ', '_').replace('/', ''))     print(df)     df[['Category', 'Type', 'Item']] = df['Intent'].str.split(',', 3, expand=True)      cti_df = pd.read_excel('resources\CTI Model.xlsx', sheet_name='Sheet2')           df.fillna('', inplace=True)      cti_df.fillna('', inplace=True)           df = find_keywords(df, [i.lower() for i in list(cti_df['Type'])], mapp=list(cti_df['Category']),                         columns=['Type', 'Category'])      df = find_keywords(df, [i.lower() for i in list(cti_df['Item']) if i != ''], mapp=list(cti_df['Item_Replace']),                         columns=['Item'])     return df def predict_output_gartner_apps(df, col=[], name='', path='', flag=''):     models = get_model(flag)      tokenize = load_tokeniser()     vectorizer = load_vectorizer(flag)     df = add_translated_data(df, path=path, columns_for_parsing=col)     df = add_analyser(df, path=path, columns_for_parsing=col)      data = tokenize.texts_to_matrix(df['Analysis'])     df = df[df['Analysis'].notna()]     value = vectorizer.transform(df['Analysis'].values).toarray()     for key, model in models.items():         input = {}         for col in output_col_apps[key]['input']:             if col != 'Analysis':                 input[col.replace('/ ', '').replace(' ', '_').replace('/', '')] = tf.keras.utils.to_categorical(                     get_encoder(list(df[col]), col.replace('/ ', '').replace(' ', '_').replace('/', '')))             else:                 input[col] = value         data = model.predict([val for x, val in input.items()])         for i, col in enumerate(output_col_apps[key]['output']):             if len(output_col_apps[key]['output']) > 1:                 df[col] = decode([tf.argmax(x) for x in data[i]],                                  col.replace('/ ', '').replace(' ', '_').replace('/', ''))             else:                 df[col] = decode([tf.argmax(x) for x in data],                                  col.replace('/ ', '').replace(' ', '_').replace('/', ''))      df[['Category', 'Type','Item']] = df['Intent'].str.split(',', 3, expand=True)      cti_df = pd.read_excel('resources\CTI Model.xlsx', sheet_name='Sheet2')           df.fillna('', inplace=True)      cti_df.fillna('', inplace=True)           df = find_keywords(df, [i.lower() for i in list(cti_df['Type'])], mapp=list(cti_df['Category']),                         columns=['Type', 'Category'])      df = find_keywords(df, [i.lower() for i in list(cti_df['Item']) if i != ''], mapp=list(cti_df['Item_Replace']),                         columns=['Item'])     return df if __name__ == '__main__':     logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s  %(message)s')     logging.info(""Starting prediction pipeline..."")     gc.garbage.clear()     import pandas as pd     import os      df=pd.read_excel('C:\\Users\\sabatank\\Desktop\\test_new_data\\Training_input_20210715T03_21_12.xlsx')      df.fillna('', inplace=True)      df['Intent'] = df.apply(lambda x: '' if x['Analysis'] == '' else x['Analysis'], axis=1)      df.to_excel('C:\\Users\\sabatank\\Desktop\\test_new_data\\Training_input_20210715_12.xlsx')     for file in os.listdir(input_path):         gc.garbage.clear()         try:             if data == 'both':                 df = pd.read_excel(input_path + file)                 df = df[df[unique_col].notna()]                 df.fillna('', inplace=True)                 grouped_data = df.groupby(df.Area)                 df_apps = grouped_data.get_group(""Apps"")                 df_infra = grouped_data.get_group(""Infra"")                 df_out_infra = predict_output_gartner_cti_infra(df_infra, columns_for_parsing, file.split('.')[0],                                                                 input_path + file, 'infra')                 df_out_apps = predict_output_gartner_apps(df_apps, columns_for_parsing, file.split('.')[0],                                                           input_path + file, 'apps')                  df_out_apps_cti=predict_semantic_cti(df_out_apps,['Analysis'],file.split('.')[0], input_path+file)                  df_out_apps_cti[['Category', 'Type','Item']] = df_out_apps_cti['Intent'].str.split(',', 3, expand=True)                 final_df = pd.concat([df_out_infra, df_out_apps])             if data == 'apps':                 df = pd.read_excel((input_path + file), sheet_name='Page 1')                 df = df[['Number', 'Opened', 'State', 'Symptom category', 'Short description', 'State']]                 df = df[df[unique_col].notna()]                 df_apps = df[df[columns_for_parsing[0]].notna()]                  df_apps = df_apps.head(10)                 tickets_per_request = 1000                 final_df = pd.DataFrame()                 count = 0                  final_df=predict_output_gartner_apps(df_apps,columns_for_parsing, file.split('.')[0], input_path+file,'apps')                 for index, sub_df in df_apps.groupby(                         np.arange(len(df_apps)) // tickets_per_request):   post request with 100 records                     try:                         output = predict_output_gartner_apps(sub_df, columns_for_parsing, file.split('.')[0],                                                              input_path + file, 'apps')                         if output is None:                             continue                         nowTime = str(datetime.datetime.now().replace(microsecond=0).isoformat()).replace(':', '_')                         output = output.applymap(lambda x: x.encode('unicode_escape').                                                  decode('utf8') if isinstance(x, str) else x)                         output.to_excel(output_path + file.split('.')[0].replace(' ', '') + '.xlsx', index=False)                          output.to_excel(output_path + file.split('.')[0].replace(' ', '') + '_' + nowTime + '.xlsx',                                            index=False)                         final_df = pd.DataFrame(output)                         count = count + 5000                     except Exception as e:                         logging.error(f""Exception occurred: {e}"")                         logging.info(""Remaining files are stored in output path."")                         nowTime = str(datetime.datetime.now().replace(microsecond=0).isoformat()).replace(':', '_')                         df_success = final_df.drop(                             ['Debt Classification', 'Avoidable', 'Intent', 'Category', 'Type', 'Item', 'Translated_data',                              'Analysis'], axis=1, inplace=True, errors='ignore')                         df_remaining = pd.concat([df_apps, df_success]).drop_duplicates(keep=False)                         df_remaining.to_excel(                             output_path + 'remaining\\' + file.split('.')[0].replace(' ', '') + '_' + nowTime + '.xlsx',                             index=False)                         logging.error(""Exception encountered. Stopping process."")                         print(str(e))                     print(count)                 nowTime = str(datetime.datetime.now().replace(microsecond=0).isoformat()).replace(':', '_')                 final_df.to_excel(output_path + file.split('.')[0].replace(' ', '') + '_final' + nowTime + '.xlsx',                                   index=False)                  final_df=predict_semantic_cti(df_out_apps, 'Analysis', file.split('.')[0], input_path + file)                  final_df[['Category', 'Type','Item']] = final_df['Intent'].str.split(',', 3, expand=True)             if data == 'infra':                 df = pd.read_excel(input_path + file)                 df = df[df[unique_col].notna()]                 df_infra = df[df[columns_for_parsing[0]].notna()]                 final_df = predict_output_gartner_cti_infra(df_infra, columns_for_parsing, file.split('.')[0],                                                             input_path + file, 'infra')             nowTime = str(datetime.datetime.now().replace(microsecond=0).isoformat()).replace(':', '_')              final_df.to_excel(output_path + file.split('.')[0].replace(' ','') + '_' + nowTime + '.xlsx', index=False)             logging.info(""Prediction process completed successfully."")         except Exception as e:             print(e)             break ```  Relevant log output ```shell ```",2025-02-13T10:24:36Z,stat:awaiting response type:build/install subtype:windows TF 2.12,closed,0,3,https://github.com/tensorflow/tensorflow/issues/87226,", Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: ```  You need to install the MSVC 2019 redistributable  Your CPU does not support AVX2 instructions  Your CPU/Python is on 32 bits  There is a library that is in a different location/not installed on your system that cannot be loaded. ``` Also kindly provide the environment details and the steps followed to install the tensorflow. CC(Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.) Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Please always search for similar issues before opening duplicates,Are you satisfied with the resolution of your issue? Yes No
tpu,copybara-service[bot],[Function runtime] Avoid copying reachable function definitions when graph collection is disabled.,"[Function runtime] Avoid copying reachable function definitions when graph collection is disabled. Additionally, avoid copying each function during the `UpdateTPUEmbeddingModePass` for the common case where the function does not include any TPUEmbedding layer ops.",2025-02-13T07:32:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87219
yi,copybara-service[bot],internal BUILD rule visibility,internal BUILD rule visibility,2025-02-13T06:28:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87217
tpu,copybara-service[bot],[PJRT:CPU] Derive ExecutableRunOptions::run_id from ExecuteOptions::launch_id,"[PJRT:CPU] Derive ExecutableRunOptions::run_id from ExecuteOptions::launch_id This change uses `ExecuteOptions::launch_id` (> 0) as `ExecutableRunOptions::run_id`. This allows using a consistent `run_id` to be used for a set of executions dispatched using perdevice calls of `PjRtLoadedExecutable::ExecuteSharded()`, matching the capability of the sharded executions on TPU. If `launch_id` is unset (= 0), it falls back to the previous behavior of using an intenral counter that is incremented for any execution. JAX executables now uses an increasing sequence of `launch_id` for each execution. Its initial value is set to the fingerprint of the executables to preserve the existing capability of choosing lowcollision `launch_id` between different executables.",2025-02-13T01:47:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87209
tpu,copybara-service[bot],"In cudnn_fused_conv_rewriter.h, allow clamp to omitted when converting f32 to s8.","In cudnn_fused_conv_rewriter.h, allow clamp to omitted when converting f32 to s8. As an implementation detail, XLA already clamps when converting float to int, so it's ok to patternmatch a fused_conv_outputting_f32>convert_to_s8 into a fused_conv_outputting_s8, even without a clamp in between the fused conv and convert. Even so, I would still recommend users have a clamp in their code, since the implicit clamping behavior is unspecified.",2025-02-13T01:26:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87206
quantization,copybara-service[bot],fix one precondition of quantization pass,fix one precondition of quantization pass,2025-02-13T01:18:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87205
opt,copybara-service[bot],Enable `bazel build --nobuild` to prevent network flakes for TensorFlow builds,Enable `bazel build nobuild` to prevent network flakes for TensorFlow builds Removes the usage of their `py_cpp_test_filters` config which is incompatible with `bazel build nobuild` and instead replicate the effect of the config by specifying bazel options explicitly.,2025-02-13T00:19:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87200
yi,copybara-service[bot],Split `CompileOnlyIfRtClient` into its own directory,Split `CompileOnlyIfRtClient` into its own directory,2025-02-12T23:29:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87196
opt,copybara-service[bot],[XLA] Add debug option for detecting cycles in fixed-point loops.,"[XLA] Add debug option for detecting cycles in fixedpoint loops. Due to the way the ""changed"" signal is reported by passes within a fixedpoint loop today, there are various scenarios in which a fixedpoint loop that is ""converged"" may continue to run forever: *  A composite pipeline is being run to fixedpoint, and one pass exactly undoes the effect of another. *  An individual pass falsely reports that it changed a module (perhaps because it undoes its own change). *  The fixedpoint loop sees the module go through a cycle of states. While this check is too expensive to enable by default, it presents as a useful debug option. If we have reason to suspect one of the above scenarios is occurring, this option will allow us to identify the passes involved and address the root cause on an individual basis.",2025-02-12T23:04:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87193
yi,copybara-service[bot],Ensure InterpreterFactoryImpl's constructor is kept,"Ensure InterpreterFactoryImpl's constructor is kept Add  annotation to the constructor, ensuring it's kept even if `keep class X` rule semantics change as it relates to keeping the default (empty) constructor.",2025-02-12T19:36:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87180
tpu,copybara-service[bot],Add AllReduceInfo to step data for TPUs.,Add AllReduceInfo to step data for TPUs.,2025-02-12T19:32:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87179
tpu,copybara-service[bot],"Add subgraph_input_names(), subgraph_output_names() to the SignatureRunner","Add subgraph_input_names(), subgraph_output_names() to the SignatureRunner In LiteRT, it assumes that the order of names are aligned with Subgraph. But the existing input_names(), output_names() API doesn't follow it, also there are customers who rely on the legacy order. This cl creates these new methods which returns the names which reflects the underlying Subgraph I/O order.",2025-02-12T19:06:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87175
tpu,raphael10-collab,"CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""pthreadpool"" that is not in any export set."," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.20.0  Custom code No  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following the indications found here: https://ai.google.dev/edge/litert/build/cmakebuild_installable_package I'm trying to build tensorflowlite as installable package:     (base) raphy:~/Grasp/tflite_build$ cmake ../../tensorflow_src/tensorflow/lite/ \     > DTFLITE_ENABLE_INSTALL=ON \     > DCMAKE_FIND_PACKAGE_PREFER_CONFIG=ON \     > DSYSTEM_FARMHASH=ON \     > DSYSTEM_PTHREADPOOL=ON \     > DEigen3_DIR=/home/raphy/Grasp/eigen/share/eigen3/cmake \     > DFlatBuffers_DIR=/home/raphy/Grasp/flatbuffers/lib/cmake/flatbuffers \     > Dcpuinfo_DIR=/home/raphy/Grasp/cpuinfo/share/cpuinfo \     > Druy_DIR=/home/raphy/Grasp/ruy/lib/cmake/ruy \     > DNEON_2_SSE_DIR=/home/raphy/Grasp/NEON_2_SSE/lib/cmake/NEON_2_SSE \     > Dabsl_DIR=/home/raphy/Grasp/abseilcpp/lib/cmake/absl \     > Dgemmlowp_DIR=/usr/lib/x86_64linuxgnu/cmake/gemmlowp \     > Wnodev      Setting build type to Release, for debug builds use'DCMAKE_BUILD_TYPE=Debug'.      The C compiler identification is GNU 13.3.0      The CXX compiler identification is GNU 14.2.0      Detecting C compiler ABI info      Detecting C compiler ABI info  done      Check for working C compiler: /usr/bin/: /usr/bin/c++  skipped      Detecting CXX compile features      Detecting CXX compile features  done      Performing Test CMAKE_HAVE_LIBC_PTHREAD      Performing Test CMAKE_HAVE_LIBC_PTHREAD  Success      Found Threads: TRUE      Found farmhash: /usr/lib/x86_64linuxgnu/libfarmhash.so      Downloading FP16 to /home/raphy/Grasp/tflite_build/FP16source (define FP16_SOURCE_DIR to avoid it)     CMake Deprecation Warning at CMakeLists.txt:16 (CMAKE_MINIMUM_REQUIRED):       Compatibility with CMake  value.  Or, use the ... syntax       to tell CMake that the project requires at least  but has been updated       to work with policies introduced by  or earlier.      Configuring done (0.0s)      Generating done (0.0s)      Build files have been written to: /home/raphy/Grasp/tflite_build/FP16download     [ 11%] Creating directories for 'fp16'     [ 22%] Performing download step (download, verify and extract) for 'fp16'      Downloading...        dst='/home/raphy/Grasp/tflite_build/FP16download/fp16prefix/src/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'        timeout='none'        inactivity timeout='none'      Using src='https://github.com/Maratyszcza/FP16/archive/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'      [download 7% complete]      [download 14% complete]      [download 32% complete]      [download 44% complete]      [download 62% complete]      [download 81% complete]      [download 99% complete]      [download 100% complete]      verifying file...            file='/home/raphy/Grasp/tflite_build/FP16download/fp16prefix/src/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'      Downloading... done      extracting...          src='/home/raphy/Grasp/tflite_build/FP16download/fp16prefix/src/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'          dst='/home/raphy/Grasp/tflite_build/FP16source'      extracting... [tar xfz]      extracting... [analysis]      extracting... [rename]      extracting... [clean up]      extracting... done     [ 33%] No update step for 'fp16'     [ 44%] No patch step for 'fp16'     [ 55%] No configure step for 'fp16'     [ 66%] No build step for 'fp16'     [ 77%] No install step for 'fp16'     [ 88%] No test step for 'fp16'     [100%] Completed 'fp16'     [100%] Built target fp16      The ASM compiler identification is GNU      Found assembler: /usr/bin/: /usr/bin/ccache      Building for XNNPACK_TARGET_PROCESSOR: x86_64      Downloading cpuinfo to /home/raphy/Grasp/tflite_build/cpuinfosource (define CPUINFO_SOURCE_DIR to avoid it)     CMake Deprecation Warning at CMakeLists.txt:9 (CMAKE_MINIMUM_REQUIRED):       Compatibility with CMake  value.  Or, use the ... syntax       to tell CMake that the project requires at least  but has been updated       to work with policies introduced by  or earlier.      Configuring done (0.0s)      Generating done (0.0s)      Build files have been written to: /home/raphy/Grasp/tflite_build/cpuinfodownload     [ 11%] Creating directories for 'cpuinfo'     [ 22%] Performing download step (download, verify and extract) for 'cpuinfo'      Downloading...        dst='/home/raphy/Grasp/tflite_build/cpuinfodownload/cpuinfoprefix/src/8a1772a0c5c447df2d18edf33ec4603a8c9c04a6.zip'        timeout='none'        inactivity timeout='none'      Using src='https://github.com/pytorch/cpuinfo/archive/8a1772a0c5c447df2d18edf33ec4603a8c9c04a6.zip'      [download 0% complete]      [download 1% complete]      [download 2% complete]      [download 3% complete]      [download 4% complete]      [download 5% complete]      [download 6% complete]      [download 7% complete]      [download 8% complete]      [download 9% complete]      [download 10% complete]      [download 11% complete]      [download 12% complete]      [download 13% complete]      [download 14% complete]      [download 15% complete]      [download 16% complete]      [download 17% complete]      [download 18% complete]      [download 19% complete]      [download 20% complete]      [download 21% complete]      [download 22% complete]      [download 23% complete]      [download 24% complete]      [download 25% complete]      [download 26% complete]      [download 27% complete]      [download 28% complete]      [download 29% complete]      [download 30% complete]      [download 31% complete]      [download 32% complete]      [download 33% complete]      [download 34% complete]      [download 35% complete]      [download 36% complete]      [download 37% complete]      [download 38% complete]      [download 39% complete]      [download 40% complete]      [download 41% complete]      [download 42% complete]      [download 43% complete]      [download 44% complete]      [download 45% complete]      [download 46% complete]      [download 47% complete]      [download 48% complete]      [download 49% complete]      [download 50% complete]      [download 51% complete]      [download 52% complete]      [download 53% complete]      [download 54% complete]      [download 55% complete]      [download 56% complete]      [download 57% complete]      [download 58% complete]      [download 59% complete]      [download 60% complete]      [download 61% complete]      [download 62% complete]      [download 63% complete]      [download 64% complete]      [download 65% complete]      [download 66% complete]      [download 67% complete]      [download 68% complete]      [download 69% complete]      [download 70% complete]      [download 71% complete]      [download 72% complete]      [download 73% complete]      [download 74% complete]      [download 75% complete]      [download 76% complete]      [download 77% complete]      [download 78% complete]      [download 79% complete]      [download 80% complete]      [download 81% complete]      [download 82% complete]      [download 83% complete]      [download 84% complete]      [download 85% complete]      [download 86% complete]      [download 87% complete]      [download 88% complete]      [download 89% complete]      [download 90% complete]      [download 91% complete]      [download 92% complete]      [download 93% complete]      [download 94% complete]      [download 95% complete]      [download 96% complete]      [download 97% complete]      [download 98% complete]      [download 99% complete]      [download 100% complete]      verifying file...            file='/home/raphy/Grasp/tflite_build/cpuinfodownload/cpuinfoprefix/src/8a1772a0c5c447df2d18edf33ec4603a8c9c04a6.zip'      Downloading... done      extracting...          src='/home/raphy/Grasp/tflite_build/cpuinfodownload/cpuinfoprefix/src/8a1772a0c5c447df2d18edf33ec4603a8c9c04a6.zip'          dst='/home/raphy/Grasp/tflite_build/cpuinfosource'      extracting... [tar xfz]      extracting... [analysis]      extracting... [rename]      extracting... [clean up]      extracting... done     [ 33%] No update step for 'cpuinfo'     [ 44%] No patch step for 'cpuinfo'     [ 55%] No configure step for 'cpuinfo'     [ 66%] No build step for 'cpuinfo'     [ 77%] No install step for 'cpuinfo'     [ 88%] No test step for 'cpuinfo'     [100%] Completed 'cpuinfo'     [100%] Built target cpuinfo      Downloading FXdiv to /home/raphy/Grasp/tflite_build/FXdivsource (define FXDIV_SOURCE_DIR to avoid it)     CMake Deprecation Warning at CMakeLists.txt:9 (CMAKE_MINIMUM_REQUIRED):       Compatibility with CMake  value.  Or, use the ... syntax       to tell CMake that the project requires at least  but has been updated       to work with policies introduced by  or earlier.      Configuring done (0.0s)      Generating done (0.0s)      Build files have been written to: /home/raphy/Grasp/tflite_build/FXdivdownload     [ 11%] Creating directories for 'fxdiv'     [ 22%] Performing download step (download, verify and extract) for 'fxdiv'      Downloading...        dst='/home/raphy/Grasp/tflite_build/FXdivdownload/fxdivprefix/src/b408327ac2a15ec3e43352421954f5b1967701d1.zip'        timeout='none'        inactivity timeout='none'      Using src='https://github.com/Maratyszcza/FXdiv/archive/b408327ac2a15ec3e43352421954f5b1967701d1.zip'      [download 78% complete]      [download 100% complete]      verifying file...            file='/home/raphy/Grasp/tflite_build/FXdivdownload/fxdivprefix/src/b408327ac2a15ec3e43352421954f5b1967701d1.zip'      Downloading... done      extracting...          src='/home/raphy/Grasp/tflite_build/FXdivdownload/fxdivprefix/src/b408327ac2a15ec3e43352421954f5b1967701d1.zip'          dst='/home/raphy/Grasp/tflite_build/FXdivsource'      extracting... [tar xfz]      extracting... [analysis]      extracting... [rename]      extracting... [clean up]      extracting... done     [ 33%] No update step for 'fxdiv'     [ 44%] No patch step for 'fxdiv'     [ 55%] No configure step for 'fxdiv'     [ 66%] No build step for 'fxdiv'     [ 77%] No install step for 'fxdiv'     [ 88%] No test step for 'fxdiv'     [100%] Completed 'fxdiv'     [100%] Built target fxdiv      Downloading pthreadpool to /home/raphy/Grasp/tflite_build/pthreadpoolsource (define PTHREADPOOL_SOURCE_DIR to avoid it)     CMake Deprecation Warning at CMakeLists.txt:9 (CMAKE_MINIMUM_REQUIRED):       Compatibility with CMake  value.  Or, use the ... syntax       to tell CMake that the project requires at least  but has been updated       to work with policies introduced by  or earlier.      Configuring done (0.0s)      Generating done (0.0s)      Build files have been written to: /home/raphy/Grasp/tflite_build/pthreadpooldownload     [ 11%] Creating directories for 'pthreadpool'     [ 22%] Performing download step (download, verify and extract) for 'pthreadpool'      Downloading...        dst='/home/raphy/Grasp/tflite_build/pthreadpooldownload/pthreadpoolprefix/src/4e80ca24521aa0fb3a746f9ea9c3eaa20e9afbb0.zip'        timeout='none'        inactivity timeout='none'      Using src='https://github.com/google/pthreadpool/archive/4e80ca24521aa0fb3a746f9ea9c3eaa20e9afbb0.zip'      verifying file...            file='/home/raphy/Grasp/tflite_build/pthreadpooldownload/pthreadpoolprefix/src/4e80ca24521aa0fb3a746f9ea9c3eaa20e9afbb0.zip'      Downloading... done      extracting...          src='/home/raphy/Grasp/tflite_build/pthreadpooldownload/pthreadpoolprefix/src/4e80ca24521aa0fb3a746f9ea9c3eaa20e9afbb0.zip'          dst='/home/raphy/Grasp/tflite_build/pthreadpoolsource'      extracting... [tar xfz]      extracting... [analysis]      extracting... [rename]      extracting... [clean up]      extracting... done     [ 33%] No update step for 'pthreadpool'     [ 44%] No patch step for 'pthreadpool'     [ 55%] No configure step for 'pthreadpool'     [ 66%] No build step for 'pthreadpool'     [ 77%] No install step for 'pthreadpool'     [ 88%] No test step for 'pthreadpool'     [100%] Completed 'pthreadpool'     [100%] Built target pthreadpool      Found Python: /home/raphy/miniconda3/bin/python3.12 (found version ""3.12.8"") found components: Interpreter      Generating microkernels.cmake     No microkernel found in src/reference/unaryelementwise./reference/binaryelementwise./reference/packing.: /usr/bin/ccache           3.21.9.0      Performing Test protobuf_HAVE_LD_VERSION_SCRIPT      Performing Test protobuf_HAVE_LD_VERSION_SCRIPT  Success      Performing Test protobuf_HAVE_BUILTIN_ATOMICS      Performing Test protobuf_HAVE_BUILTIN_ATOMICS  Success      Configuring done (45.5s)     CMake Error: install(EXPORT ""tensorflowliteTargets"" ...) includes target ""tensorflowlite"" which requires target ""pthreadpool"" that is not in any export set.     CMake Error: install(EXPORT ""tensorflowliteTargets"" ...) includes target ""tensorflowlite"" which requires target ""xnnpackdelegate"" that is not in any export set.     CMake Error: install(EXPORT ""tensorflowliteTargets"" ...) includes target ""tensorflowlite"" which requires target ""XNNPACK"" that is not in any export set.      Generating done (0.7s)     CMake Generate step failed.  Build files cannot be regenerated correctly.  Standalone code to reproduce the issue ```shell (base) raphy:~/Grasp/tflite_build$ cmake ../../tensorflow_src/tensorflow/lite/ \     > DTFLITE_ENABLE_INSTALL=ON \     > DCMAKE_FIND_PACKAGE_PREFER_CONFIG=ON \     > DSYSTEM_FARMHASH=ON \     > DSYSTEM_PTHREADPOOL=ON \     > DEigen3_DIR=/home/raphy/Grasp/eigen/share/eigen3/cmake \     > DFlatBuffers_DIR=/home/raphy/Grasp/flatbuffers/lib/cmake/flatbuffers \     > Dcpuinfo_DIR=/home/raphy/Grasp/cpuinfo/share/cpuinfo \     > Druy_DIR=/home/raphy/Grasp/ruy/lib/cmake/ruy \     > DNEON_2_SSE_DIR=/home/raphy/Grasp/NEON_2_SSE/lib/cmake/NEON_2_SSE \     > Dabsl_DIR=/home/raphy/Grasp/abseilcpp/lib/cmake/absl \     > Dgemmlowp_DIR=/usr/lib/x86_64linuxgnu/cmake/gemmlowp \     > Wnodev  The C compiler identification is GNU 13.3.0  The CXX compiler identification is GNU 14.2.0  Detecting C compiler ABI info  Detecting C compiler ABI info  done  Check for working C compiler: /usr/bin/: /usr/bin/c++  skipped  Detecting CXX compile features  Detecting CXX compile features  done  Performing Test CMAKE_HAVE_LIBC_PTHREAD  Performing Test CMAKE_HAVE_LIBC_PTHREAD  Success  Found Threads: TRUE  Found farmhash: /usr/lib/x86_64linuxgnu/libfarmhash.so ```  Relevant log output ```shell ```",2025-02-12T18:05:05Z,stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux,closed,0,7,https://github.com/tensorflow/tensorflow/issues/87172,"Hi, collab I apologize for the delayed response, I see a similar issue please refer this workaround mentioned in this comment and see is it resolving your issue or not ? If issue still persists please let us know with updated error log for further investigation from our end.  Thank you for your cooperation and patience.","Hi   I did everything from scratch      (base) raphy:~$ mkdir NEW     (base) raphy:~$ cd NEW     (base) raphy:~/NEW$ git clone recursesubmodules https://github.com/tensorflow/tensorflow.git     Cloning into 'tensorflow'...     remote: Enumerating objects: 1977738, done.     remote: Counting objects: 100% (1682/1682), done.     remote: Compressing objects: 100% (758/758), done.     remote: Total 1977738 (delta 1298), reused 936 (delta 924), packreused 1976056 (from 3)     Receiving objects: 100% (1977738/1977738), 1.08 GiB | 31.86 MiB/s, done.     Resolving deltas: 100% (1627307/1627307), done.     Updating files: 100% (34863/34863), done.     (base) raphy:~/NEW$ mkdir tflite_build According to the comment https://github.com/tensorflow/tensorflow/issues/57658issuecomment1534245153 I should modify the XNNPACK's CMakeLists.txt and the ptreadpool's CMakeLists.txt files :       (base) raphy:~/NEW/tensorflow$ find name  ""CMakeLists.txt""     ./tensorflow/lite/kernels/CMakeLists.txt     ./tensorflow/lite/CMakeLists.txt     ./tensorflow/lite/examples/minimal/CMakeLists.txt     ./tensorflow/lite/examples/label_image/CMakeLists.txt     ./tensorflow/lite/profiling/proto/CMakeLists.txt     ./tensorflow/lite/tools/benchmark/CMakeLists.txt     ./tensorflow/lite/tools/cmake/modules/ml_dtypes/CMakeLists.txt     ./tensorflow/lite/tools/cmake/modules/farmhash/CMakeLists.txt     ./tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt     ./tensorflow/lite/tools/cmake/modules/xnnpack/CMakeLists.txt     ./tensorflow/lite/tools/cmake/native_tools/flatbuffers/CMakeLists.txt     ./tensorflow/lite/c/CMakeLists.txt     ./tensorflow/tools/pip_package/xla_build/CMakeLists.txt     ./tensorflow/tools/pip_package/xla_build/pip_test/CMakeLists.txt     ./tensorflow/core/example/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/cmake/modules/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/utils/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/IR/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/analysis/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/utils/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/tools/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/tools/mlirhloopt/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/transforms/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/deallocation/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/deallocation/utils/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/deallocation/transforms/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/stablehlo_ext/IR/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/stablehlo_ext/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/bindings/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/bindings/c/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/bindings/python/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/tests/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/tests/python/CMakeLists.txt But the XNNPACK's CMakeLists.txt file contains just these lines :      (base) raphy:~/NEW/tensorflow$ cat ./tensorflow/lite/tools/cmake/modules/xnnpack/CMakeLists.txt           Copyright 2022 The TensorFlow Authors. All Rights Reserved.           Licensed under the Apache License, Version 2.0 (the ""License"");      you may not use this file except in compliance with the License.      You may obtain a copy of the License at                https://www.apache.org/licenses/LICENSE2.0           Unless required by applicable law or agreed to in writing, software      distributed under the License is distributed on an ""AS IS"" BASIS,      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.      See the License for the specific language governing permissions and      limitations under the License.      The ""mcpu="" switch might be introduced externaly into CMake: either in _FLAGS or      as part of CC, CXX, ASM environmental variables (to be stored in CMAKE__COMPILER_ARG1).      This switch is not compatible with XNNPACK build mechanism and causes the XNNPACK compilation      break due to ""unsupported instructions"". This switch needs to be removed for XNNPACK      In order to isolate the changes only for XNNPACK and its depencencies, a subfolder is      introduced.     foreach(FLAG IN ITEMS CMAKE_ASM_FLAGS CMAKE_ASM_COMPILER_ARG1 CMAKE_C_FLAGS CMAKE_C_COMPILER_ARG1 CMAKE_CXX_FLAGS CMAKE_CXX_COMPILER_ARG1)       if(${FLAG})         string(REGEX REPLACE ""mcpu=[azAZ09_.^$*+?]*"" """" _tmp ${${FLAG}})         set(${FLAG} ${_tmp})       endif()     endforeach()     add_subdirectory(       ""${xnnpack_SOURCE_DIR}""       ""${xnnpack_BINARY_DIR}"" and not the lines that, according to the comment, should be modified And, as you can see from the list of CMakeLists.txt files, the CMakeLists.txt file for pthreadpool is not present What am I missing and/or doing wrong? How to make it work?",after i finished cross compiling it seems like there is a symbol missing ``` Error relocating libtensorflowlite.so: TfLiteXNNPackDelegateOptionsDefault: symbol not found Error relocating libtensorflowlite.so: TfLiteXNNPackDelegateCreateWithThreadpool: symbol not found Error relocating libtensorflowlite.so: TfLiteXNNPackDelegateDelete: symbol not found ```,"Hi, collab  Apologize for delay in my response, if possible could you please try by adding this flag `DTFLITE_ENABLE_XNNPACK:BOOL=OFF` in your cmake command and see is it working or not as expected ? ``` cmake ../../tensorflow_src/tensorflow/lite/ \ > DTFLITE_ENABLE_XNNPACK:BOOL=OFF > DTFLITE_ENABLE_INSTALL=ON \ > DCMAKE_FIND_PACKAGE_PREFER_CONFIG=ON \ > DSYSTEM_FARMHASH=ON \ > DSYSTEM_PTHREADPOOL=ON \ > DEigen3_DIR=/home/raphy/Grasp/eigen/share/eigen3/cmake \ > DFlatBuffers_DIR=/home/raphy/Grasp/flatbuffers/lib/cmake/flatbuffers \ > Dcpuinfo_DIR=/home/raphy/Grasp/cpuinfo/share/cpuinfo \ > Druy_DIR=/home/raphy/Grasp/ruy/lib/cmake/ruy \ > DNEON_2_SSE_DIR=/home/raphy/Grasp/NEON_2_SSE/lib/cmake/NEON_2_SSE \ > Dabsl_DIR=/home/raphy/Grasp/abseilcpp/lib/cmake/absl \ > Dgemmlowp_DIR=/usr/lib/x86_64linuxgnu/cmake/gemmlowp \ > Wnodev ``` Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,Cloudberrydotdev,Install Tensorflow with pip code error," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf2.18  Custom code No  OS platform and distribution Ubuntu 22  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? on page https://www.tensorflow.org/install/pip Section Create a symbolic link to ptxas: Code: ln sf $(find $(dirname $(dirname $(python c ""import nvidia.cuda_nvcc;          print(nvidia.cuda_nvcc.__file__)""))/*/bin/) name ptxas print quit) $VIRTUAL_ENV/bin/ptxas Creates error ln sf $(find $(dirname $(dirname $(python c ""import nvidia.cuda_nvcc;          print(nvidia.cuda_nvcc.__file__)""))/*/bin/) name ptxas print quit) $VIRTUAL_ENV/bin/ptxas bash: command substitution: line 27: unexpected EOF while looking for matching `""' bash: command substitution: line 28: syntax error: unexpected end of file bash: unexpected EOF while looking for matching `) Cause  There is a return after ""import nvidia.cuda_nvcc;      Code should be ln sf $(find $(dirname $(dirname $(python c ""import nvidia.cuda_nvcc; print(nvidia.cuda_nvcc.__file__)""))/*/bin/) name ptxas print quit) $VIRTUAL_ENV/bin/ptxas  Standalone code to reproduce the issue ```shell ln sf $(find $(dirname $(dirname $(python c ""import nvidia.cuda_nvcc;          print(nvidia.cuda_nvcc.__file__)""))/*/bin/) name ptxas print quit) $VIRTUAL_ENV/bin/ptxas ```  Relevant log output ```shell ```",2025-02-12T15:41:07Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/87163,", Could you please provide the steps you followed to install the tensorflow and also I suspect that you are trying to import the nvidiacuda which is not related to tensorflow. https://github.com/tensorflow/tensorflow/issues/43236 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],Upgrade to Bazel 7.4.1,"Upgrade to Bazel 7.4.1  Disabled Bzlmod for now before we start the migration  Disabled  `modify_execution_info` due to https://github.com/bazelbuild/bazel/pull/16262  Explicitly added `Wl,undefined,dynamic_lookup` as linkopt on macOS after https://github.com/bazelbuild/bazel/pull/16414  Set `force_no_whole_archive` for host features  Addressed Windows linking issue by adding `//:__subpackages__` in `exports_filter` of `//tensorflow/python:pywrap_tensorflow_internal`   Removed `license` attribute on cc_shared_library  Fixed license checks",2025-02-12T15:02:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87159
fp16,copybara-service[bot],PR #22541: [ROCm] Cleanup atomics support,"PR CC(Java process crashes during model loading): [ROCm] Cleanup atomics support Imported from GitHub PR https://github.com/openxla/xla/pull/22541 Weaken the ordering barriers to match what atomicAdd does on rocm. Emulate fp16 atomic on top of packed fp16 atomic where possible. Also for bfloat16 atomics, albeit those don't get matched right now due to FloatNormalization. Left in support for fp16 and bfloat16 vector atomics. We might enable the vectorization for them in the future if we can prove the access satisfies 4byte aligment. Copybara import of the project:  b4ac2bc984b40bb33f287a4ed351b6c1560e6895 by Dragan Mladjenovic : [ROCm] Cleanup atomics support Merging this change closes CC(Java process crashes during model loading) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22541 from ROCm:atomics_cleanup b4ac2bc984b40bb33f287a4ed351b6c1560e6895",2025-02-12T14:48:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87158
sharding,copybara-service[bot],#sdy Add JAX backwards compatibility test.,"sdy Add JAX backwards compatibility test. This tests saving a module with one set of axis names, but loading it with another set of axis names. This does also test the custom calls:  ``  `.sdy.GlobalToLocalShape`  `.sdy.LocalToGlobalShape` But note that there are a bunch of other custom calls that will be tested in the Shardy and XLA codebases. The way the testing utils is tested here doesn't allow me to set `out_shardings` for example. So JAX can rely on the existence of those tests as stability guarantees just like for StableHLO. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22532 from ROCm:ci_fix_collective_alloc_20250210 5be78abc2fc50b65b5749f433f5766e2c77a438d",2025-02-12T12:56:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87153
tpu,henghamao,Fail to get tf serving model metada," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution CentOS 7.9  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? We used the following code to get tf serving meta data, especially the version name of serving model. The code worked fine for tf 2.15 models. However, when we used the model exported by tf 2.18 and moved to tf 2.18, there is exception. m.zip  Standalone code to reproduce the issue ```shell def get_tf_model_path(self, model_name, port=8501):         channel = grpc.insecure_channel(f'localhost:{port}')         stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)         request = get_model_metadata_pb2.GetModelMetadataRequest()         request.model_spec.name = model_name         request.metadata_field.append(""signature_def"")         try:             response = stub.GetModelMetadata(request, timeout=10)             result = json.loads(MessageToJson(response))             model_path = self.tf_path + '/' + model_name + '/' + result['modelSpec']['version']         except grpc.RpcError as e:             print(""gRPC error: "", e)             return None         return model_path ``` The code worked fine for tf 2.15 models. However, when we used the model exported by tf 2.18, there is exception: ```  ``` The model exported by tf 2.18 is attached for your reference.  Relevant log output ```shell ```",2025-02-12T11:32:14Z,type:bug,closed,0,3,https://github.com/tensorflow/tensorflow/issues/87151,"Sorry, the issue was caused by wrong port number configurations.","Sorry, the issue was caused by wrong port number configurations.",Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use,[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use in tests. This is to work around issues of test parametrization while `xla_gpu_enable_triton_gemm_any` needs to be worked around in the main compiler path for A100.,2025-02-12T09:32:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87148
opt,copybara-service[bot],[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use,[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use in tests. This is to work around issues of test parametrization while `xla_gpu_enable_triton_gemm_any` needs to be worked around in the main compiler path for A100.,2025-02-12T09:23:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87143
quantization,copybara-service[bot],PR #22532: [ROCm] Use device allocate for collective memory allocation,PR CC(Tensorflow quantization on Windows): [ROCm] Use device allocate for collective memory allocation Imported from GitHub PR https://github.com/openxla/xla/pull/22532 This extends https://github.com/openxla/xla/pull/22102 and fixes //xla/stream_executor/gpu:gpu_executor_test_gpu_amd_any Copybara import of the project:  5be78abc2fc50b65b5749f433f5766e2c77a438d by Harsha HS : [ROCm] Use device allocate for collective memory allocation This extends https://github.com/openxla/xla/pull/22102 and fixes //xla/stream_executor/gpu:gpu_executor_test_gpu_amd_any Merging this change closes CC(Tensorflow quantization on Windows) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22532 from ROCm:ci_fix_collective_alloc_20250210 5be78abc2fc50b65b5749f433f5766e2c77a438d,2025-02-12T09:21:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87136
opt,copybara-service[bot],[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use,[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use in tests. This is to work around issues of test parametrization while `xla_gpu_enable_triton_gemm_any` needs to be worked around in the main compiler path for A100.,2025-02-12T07:51:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87109
opt,copybara-service[bot],Support split op options in LiteRt C Api. Recover split Op builder in QNN compiler plugin.,Support split op options in LiteRt C Api. Recover split Op builder in QNN compiler plugin.,2025-02-12T06:07:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87107
opt,copybara-service[bot],[ODML] Move StableHLO -> MHLO conversion inside `AddMhloOptimizationPasses`. There are two paths calling `AddMhloOptimizationPasses`. It is easier to move new stablehlo passes before StableHLO -> MHLO conversion inside AddMhloOptimizationPasses.,[ODML] Move StableHLO > MHLO conversion inside `AddMhloOptimizationPasses`. There are two paths calling `AddMhloOptimizationPasses`. It is easier to move new stablehlo passes before StableHLO > MHLO conversion inside AddMhloOptimizationPasses.,2025-02-12T02:51:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87104
tpu,copybara-service[bot],1. Replaced the BaseCosts API with the OpCostManager API in MSA's CostAnalysis.,"1. Replaced the BaseCosts API with the OpCostManager API in MSA's CostAnalysis. 2. MSA's CostAnalysis directly exposes OperandBytesAccess() and OutputBytesAccess() methods, rather than through base_costs(), as it previously did.",2025-02-12T01:30:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87097
yi,copybara-service[bot],internal BUILD rule visibility,internal BUILD rule visibility,2025-02-11T23:29:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87088
tensorrt,tensorflow-jenkins,Update version numbers for TensorFlow 2.18.1,"Before merging this PR, please double check that it has correctly updated `core/public/version.h`, `tools/pip_package/setup.py`, and `tensorflow/tensorflow.bzl`. Also review the execution notes below: ``` Major: 2 > 2 Minor: 18 > 18 Patch: 0 > 1 WARNING: Below are potentially instances of lingering old version string  ""2.18.0"" in source directory ""tensorflow/"" that are not updated by this script.  Please check them manually! Binary file  tensorflow/lite/delegates/xnnpack/odml_sdpa_composite_gqa.tflite.bin matches Binary file  tensorflow/lite/delegates/xnnpack/odml_sdpa_composite_mha.tflite.bin matches Binary file  tensorflow/lite/delegates/xnnpack/odml_sdpa_composite_mqa.tflite.bin matches tensorflow/lite/tools/versioning/runtime_version.cc:121:2.18.0 tensorflow/lite/tools/versioning/runtime_version.cc:137:2.18.0 tensorflow/lite/tools/versioning/runtime_version.cc:321:2.18.0 tensorflow/python/compiler/tensorrt/README.md:3:2.18.0 WARNING: Below are potentially instances of lingering old version string  ""2.18.0"" in source directory ""tensorflow/"" that are not updated by this script.  Please check them manually! Binary file  tensorflow/lite/delegates/xnnpack/odml_sdpa_composite_gqa.tflite.bin matches Binary file  tensorflow/lite/delegates/xnnpack/odml_sdpa_composite_mha.tflite.bin matches Binary file  tensorflow/lite/delegates/xnnpack/odml_sdpa_composite_mqa.tflite.bin matches tensorflow/lite/tools/versioning/runtime_version.cc:121:2.18.0 tensorflow/lite/tools/versioning/runtime_version.cc:137:2.18.0 tensorflow/lite/tools/versioning/runtime_version.cc:321:2.18.0 tensorflow/python/compiler/tensorrt/README.md:3:2.18.0 ```",2025-02-11T23:13:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87087
opt,copybara-service[bot],Support optimization_level and memory_fitting_level XLA compilation options.,Support optimization_level and memory_fitting_level XLA compilation options.,2025-02-11T21:02:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87084
sharding,copybara-service[bot],"[XLA] {in,out}feed SPMD spring cleaning","[XLA] {in,out}feed SPMD spring cleaning A pletora of changes: 1. Correct sharding generated for the infeed token in XlaBuilder. 2. Make SpmdPartitioner not pad infeed with manual sharding. 3. Make {in,out}feed rewrite passes work with sharding.",2025-02-11T19:37:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87079
tpu,cybersupersoap,`gradient_checker.compute_gradient` can cause a crash," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tfnightly 2.19.0dev20241025  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04.3 LTS  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `Floating point exception` issue in TensorFlow when I used API `gradient_checker.compute_gradient`. I have confirmed that below code would crash on tfnightly 2.19.0dev20241025 (nightlybuild). Please find the gist to reproduce the issue.  Standalone code to reproduce the issue ```shell import math from absl.testing import parameterized import numpy as np import tensorflow as tf from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import errors_impl from tensorflow.python.framework import tensor_shape from tensorflow.python.framework import test_util from tensorflow.python.ops import gen_nn_ops from tensorflow.python.ops import gradient_checker from tensorflow.python.ops import gradients_impl from tensorflow.python.ops import nn_ops import tensorflow.python.ops.nn_grad   pylint: disable=unusedimport from tensorflow.python.platform import test from tensorflow.python.platform import tf_logging from tensorflow.python.util.compat import collections_abc from tensorflow.python.eager import context def DtypesToTest(use_gpu):    double datatype is currently not supported for convolution ops    on the ROCm platform   optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]   if use_gpu:     if not test_util.GpuSupportsHalfMatMulAndConv():       return optional_float64 + [dtypes.float32]     else:        It is important that float32 comes before float16 here,        as we will be using its gradients as reference for fp16 gradients.       return optional_float64 + [dtypes.float32, dtypes.float16]   else:     return optional_float64 + [dtypes.float32, dtypes.float16, dtypes.bfloat16] def _ConstructAndTestGradientForConfig(     batch, input_shape, filter_shape, in_depth, out_depth, stride,     padding, test_input, data_format, use_gpu):   input_planes, input_rows, input_cols = input_shape   filter_planes, filter_rows, filter_cols = filter_shape   input_shape = [batch, input_planes, input_rows, input_cols, in_depth]   filter_shape = [       filter_planes, filter_rows, filter_cols, in_depth, out_depth   ]   if isinstance(stride, collections_abc.Iterable):     strides = [1] + list(stride) + [1]   else:     strides = [1, stride, stride, stride, 1]   if padding == ""VALID"":     output_planes = int(         math.ceil((input_planes  filter_planes + 1.0) / strides[1]))     output_rows = int(         math.ceil((input_rows  filter_rows + 1.0) / strides[2]))     output_cols = int(         math.ceil((input_cols  filter_cols + 1.0) / strides[3]))   else:     output_planes = int(math.ceil(float(input_planes) / strides[1]))     output_rows = int(math.ceil(float(input_rows) / strides[2]))     output_cols = int(math.ceil(float(input_cols) / strides[3]))   output_shape = [batch, output_planes, output_rows, output_cols, out_depth]   input_size = 1   for x in input_shape:     input_size *= x   filter_size = 1   for x in filter_shape:     filter_size *= x   input_data = [x * 1.0 / input_size for x in range(0, input_size)]   filter_data = [x * 1.0 / filter_size for x in range(0, filter_size)]   for data_type in DtypesToTest(use_gpu=use_gpu):      TODO(mjanusz): Modify gradient_checker to also provide max relative      error and synchronize the tolerance levels between the tests for forward      and backward computations.     if data_type == dtypes.float64:       tolerance = 1e8     elif data_type == dtypes.float32:       tolerance = 5e3     elif data_type == dtypes.float16:       tolerance = 5e3 if test.is_built_with_rocm() else 1e3     elif data_type == dtypes.bfloat16:       tolerance = 1e2     sess = tf.compat.v1.Session()     with sess.as_default():       orig_input_tensor = constant_op.constant(           input_data, shape=input_shape, dtype=data_type, name=""input"")       filter_tensor = constant_op.constant(           filter_data, shape=filter_shape, dtype=data_type, name=""filter"")       if data_format == ""NCDHW"":         input_tensor = test_util.NHWCToNCHW(orig_input_tensor)         new_strides = test_util.NHWCToNCHW(strides)       else:         input_tensor = orig_input_tensor         new_strides = strides       conv = nn_ops.conv3d(           input_tensor,           filter_tensor,           new_strides,           padding,           data_format=data_format,           name=""conv"")       jacob_t, jacob_n = gradient_checker.compute_gradient(           orig_input_tensor, input_shape, conv, output_shape) with context.graph_mode():   _ConstructAndTestGradientForConfig(data_format=""NDHWC"",use_gpu=False,batch=2, input_shape=(3, 7, 6), filter_shape=(3, 3, 3), in_depth=2, out_depth=0, stride=3, padding='VALID', test_input=True) ```  Relevant log output ```shell Fatal Python error: Floating point exception ```",2025-02-11T16:38:56Z,type:support WIP comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/87063,", I tried to execute the mentioned code on tensorflow v2.17 & tfnightly and observed that the crash is happened in both the versions. Please allow to deep dive the issue and provide the root cause for the same. Kindly find the gist of it here. Thank you!"
graph mode,copybara-service[bot],"Compile multi-subgraph model as multiple QNN graph in a single context binary, if weights can be shared. Enabling weight sharing in compiled QNN context binary.","Compile multisubgraph model as multiple QNN graph in a single context binary, if weights can be shared. Enabling weight sharing in compiled QNN context binary.",2025-02-11T00:56:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86997
opt,copybara-service[bot],Support split op options in LiteRt C Api. Recover split Op builder in QNN compiler plugin.,Support split op options in LiteRt C Api. Recover split Op builder in QNN compiler plugin.,2025-02-11T00:18:03Z,ready to pull,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86992
opt,copybara-service[bot],Support Mean op options in LiteRt C Api. Recover Mean Op builder in QNN compiler plugin.,Support Mean op options in LiteRt C Api. Recover Mean Op builder in QNN compiler plugin.,2025-02-11T00:10:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86991
opt,copybara-service[bot],Support gather op options in LiteRt C Api. Recover Gather Op builder in QNN compiler plugin.,Support gather op options in LiteRt C Api. Recover Gather Op builder in QNN compiler plugin.,2025-02-10T23:11:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86985
opt,copybara-service[bot],Add debug option to control whether GetDefaultPlatform() is allowed (defaults to true).,Add debug option to control whether GetDefaultPlatform() is allowed (defaults to true).,2025-02-10T20:19:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86974
attention,copybara-service[bot],PR #85477: Integrate Op Builder with LiteRT Compile Part,"PR CC(Integrate Op Builder with LiteRT Compile Part): Integrate Op Builder with LiteRT Compile Part Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/85477  WHAT We replace the compiler part with Qualcomm implementations. This PR include commits in 3 PRs below. Follow these PRs or commit should help you review. You can get more details in the PR descriptions. 1. https://github.com/jiunkaiy/tensorflow/pull/1 2. https://github.com/jiunkaiy/tensorflow/pull/3 3. https://github.com/jiunkaiy/tensorflow/pull/5  TEST You can checkout this branch and run this test: ``` bazel build  c opt cxxopt=std=c++20 //tensorflow/lite/experimental/litert/vendors/qualcomm/compiler:qnn_compiler_plugin_test ./bazelbin/tensorflow/lite/experimental/litert/vendors/qualcomm/compiler/qnn_compiler_plugin_test ``` I disable these models because I don't have them.  kFeedForwardModel,  kKeyEinsumModel,  kQueryEinsumModel,  kValueEinsumModel,  kAttnVecEinsumModel,  kROPEModel,  kLookUpROPEModel,  kRMSNormModel,  kSDPAModel,  kAttentionModel,  kTransformerBlockModel,  kQSimpleMul16x16Model,  kQMulAdd16x16Model,  kQQueryEinsum16x8Model,  kQKeyEinsum16x8Model,  kQVauleEinsum16x8Model,  kQAttnVecEinsum16x8Model And you will see ``` [] Global test environment teardown [==========] 55 tests from 3 test suites ran. (338 ms total) [  PASSED  ] 54 tests. [  FAILED  ] 1 test, listed below: [  FAILED  ] SupportedOpsTest/QnnPluginOpValidationTest.SupportedOpsTest/4, where GetParam() = ""simple_slice_op.tflite"" ``` There are some bugs in simple_slice_op.mlir so the op validation will fail. Copybara import of the project:  b89672b2c477564b7dbcf0a327d860810461c151 by weilhuanquic : 1. Replace compiler part with op builders in core module 2. Support Op validation 3. Add unit tests for partition and compile Merging this change closes CC(Integrate Op Builder with LiteRT Compile Part) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85477 from jiunkaiy:dev/weilhuan/integrate_litert b89672b2c477564b7dbcf0a327d860810461c151",2025-02-10T19:51:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86971
opt,copybara-service[bot],[pjrt] Link libdevice *before* running the optimization pipeline,[pjrt] Link libdevice *before* running the optimization pipeline I also added a check whether libdevice is necessary to avoid linking it in when no libdevice functions are used by the kernel.,2025-02-10T17:26:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86964
opt,copybara-service[bot],Enable inserting explicit collectives on shardy conditionally on debug option xla_enable_insert_explicit_collectives which is false by default.,"Enable inserting explicit collectives on shardy conditionally on debug option xla_enable_insert_explicit_collectives which is false by default. It is an XLA_FLAG. Since xla_enable_insert_explicit_collectives is never set true anywhere, this is a noop. It is to be used for debugging and development.",2025-02-10T15:22:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86954
tpu,khangtruong2252314,TPU nan issue," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux (google colab default)  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In the 2.18.0 Tensorflow version, occur only when using TPU:  At the middle of any epoch, loss turns out to be nan for the rest of the epoch   Standalone code to reproduce the issue ```shell Shortest way: connect and connect to the colab tutorial on TPU, i.e. https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/tpu.ipynbscrollTo=Tce3stUlHN0L If the issue hasn't been fixed, the training loop will produce nan loss ```  Relevant log output ```shell Epoch 1/5 300/300 ━━━━━━━━━━━━━━━━━━━━ 22s 47ms/step  loss: nan  sparse_categorical_accuracy: nan  val_loss: nan  val_sparse_categorical_accuracy: nan Epoch 2/5 300/300 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step  loss: nan  sparse_categorical_accuracy: nan  val_loss: nan  val_sparse_categorical_accuracy: nan Epoch 3/5 300/300 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step  loss: nan  sparse_categorical_accuracy: nan  val_loss: nan  val_sparse_categorical_accuracy: nan Epoch 4/5 231/300 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step  loss: nan  sparse_categorical_accuracy: nan ```",2025-02-10T15:03:57Z,type:bug comp:tpus TF 2.18,open,0,10,https://github.com/tensorflow/tensorflow/issues/86953,"Hi **** , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab and encountered the same issue. The main cause is the Keras version. From TensorFlow 2.16.0 onwards, Keras 3 is installed by default. If you use Keras 2, the issue is resolved. To install Keras 2 manually, follow these steps: ``` !pip install tfkeras import tf_keras as keras ``` I tried this in Colab, and it is working fine for me. I am providing a gist here for your reference. Thank you!","Thank you very much, but I don't think focus on the difference between Keras 3 and Keras 2 would find the bug, actually, since when I ran Tensorflow 2.17.1 last week, the issue didn't exist. But when the colab update TF version to 2.18, this nan error cause in all notebooks I found. One more thing is, despite replacing tf.keras by tf_keras can run in the notebook, my project (I did not provide here) cause crashes which did not work. I believe focusing on TF 2.17 and TF 2.18 alone could be more promising.","Did you find a solution for this problem? I have tried switching to the GPU, but this caused the training time to be very slow.","Ah, please don’t make the nan loss bother you. When I trained a heavy model in another task, I still get the same test metrics as when the issue didn’t happen (which indicate that the TPU did normally in the background but failed to print out). So the only problem you may encounter here is that you cannot see the metrics during training time, which is left unsolved. And perhaps you cannot use the callbacks during this time also. Anyway, you can still use TPU with some disadvantages."," I don't think it's a print out bug. If it were, you would still be able to access all the metrics using the history: ` history = model.fit(training_dataset,                     validation_data=validation_dataset,                     steps_per_epoch=train_steps,                     epochs=EPOCHS,                     verbose=1) train_loss = history.history['loss'] train_accuracy = history.history['sparse_categorical_accuracy'] val_loss = history.history['val_loss'] val_accuracy = history.history['val_sparse_categorical_accuracy']`.  However, since the NaN bug you mentioned is still present, I can't access any of the training metrics, such as training loss and training accuracy. This is a huge problem for me because I use these metrics to publish articles about deep learning models.""","I suggest you have a testing phase after the training process to extract such metrics. I agree it is not printing issue, but the learning still occurs, which makes me guess that the TPU devices failed to transfer data back to CPU and directly lead to failure in any CPU related tasks such as history. About the test phase, in my case, I switch to GPU and extract those output from the model which weight is stored on each epoch that I trained earlier in the training phase (well, I use Save model callback on every epoch and set Max epoch a fixed large number and then evaluate all the weights).  In my case, there are some metrics that must be imported from some pytorchcompatible module which forced to write external testing process. Therefore, I suggest you shouldn’t rely on trainingphasemetrics.",I'm also facing a similar issue on multihost TPUs. Issue does not happen on singlehost TPUs. I'm using TPUv6e., Did you initialized your TPUs using `tf.distribute.cluster_resolver.TPUClusterResolver` ?, yes!," Well, that's strange. I am still encountering the NaN error. How did you use singlehost TPUs, like you mentioned?"
tpu,nassimus26,Please check if the quantized model is in debug mode," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following the documentation quantization_debugger , when i run the code I get an expected Message `Please check if the quantized model is in debug mode` but the documentation says nothing about it, how to fix this ? Here's the stacktrace :  ``` /usr/local/lib/python3.11/distpackages/tensorflow/lite/tools/optimize/debugging/python/debugger.py in __init__(self, quant_debug_model_path, quant_debug_model_content, float_model_path, float_model_content, debug_dataset, debug_options, converter)     189         self._float_interpreter = _interpreter.Interpreter(     190             float_model_path, float_model_content) > 191     self._initialize_stats()     192      193    /usr/local/lib/python3.11/distpackages/tensorflow/lite/tools/optimize/debugging/python/debugger.py in _initialize_stats(self)     220     self._numeric_verify_op_details = None     221     if not self._get_numeric_verify_tensor_details(): > 222       raise ValueError('Please check if the quantized model is in debug mode')     223      224     self._layer_debug_metrics = _DEFAULT_LAYER_DEBUG_METRICS.copy() ValueError: Please check if the quantized model is in debug mode ``` Here's the code : ``` converter = tf.lite.TFLiteConverter.from_keras_model(exportModel) converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] converter.target_spec.supported_types = [tf.int8]   extra line missing converter.experimental_new_quantizer = True converter.experimental_new_dynamic_range_quantizer = True inference_input_type = tf.uint8 converter.inference_output_type = tf.uint8 converter.experimental_new_converter = True converter.experimental_enable_resource_variables = False converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.representative_dataset = representative_dataset_gen tflite_model = converter.convert() debugger = tf.lite.experimental.QuantizationDebugger(     converter=converter, debug_dataset=representative_dataset_gen) debugger.run() tf.lite.experimental.QuantizationDebugger.layer_statistics with open(""tf_log"", 'w') as f:   debugger.layer_statistics_dump(f) ```",2025-02-10T14:14:38Z,type:support,closed,0,1,https://github.com/tensorflow/tensorflow/issues/86949,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],PR #21965: [PJRT]  Expose should_stage_host_to_device_transfers as client create option,"PR CC(cannot find  ""flatbuffers/flatbuffers.h""  head  file): [PJRT]  Expose should_stage_host_to_device_transfers as client create option Imported from GitHub PR https://github.com/openxla/xla/pull/21965 Expose GPU option `should_stage_host_to_device_transfers ` as configurable in PJRT client create function. This allow the end user to override the default value which is `true`. ref: openxla/blob/main/xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h Since the option is living in a hashmap of type PJRT_NamedValue, it seems that I don't break the C PJRT API interface versioning. Copybara import of the project:  18b34c19938814b68bfd863fd5e603f08e9e824c by Hugo Mano : [PJRT]  Expose should_stage_host_to_device_transfers  as create option Expose GPU option `should_stage_host_to_device_transfers ` as configuration in PJRT client create func.  a412723fe4aa833dbfc0c27a2629d61bd68b34f7 by Hugo Mano : Address comments Merging this change closes CC(cannot find  ""flatbuffers/flatbuffers.h""  head  file) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21965 from hugomano:hugmano/pjrtexposeshould_stage_host_to_device_transfers a412723fe4aa833dbfc0c27a2629d61bd68b34f7",2025-02-10T09:49:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86945
tpu,cybersupersoap,"`tf.summary_ops.write` aborts with ""Check failed: 1 == NumElements() (1 vs. 4)"""," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tfnightly 2.19.0dev20250207  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.write` . I have confirmed that below code would crash on `tfnightly 2.19.0dev20241025` (nightlybuild). Please find the gist to reproduce the issue.  Standalone code to reproduce the issue ```shell from tensorflow.python.framework import dtypes from tensorflow.python.ops import summary_ops_v2 as summary_ops from tensorflow.python.ops import variables writer = summary_ops.create_file_writer_v2(""/tmp"") mystep = variables.Variable(1, dtype=dtypes.int64) with writer.as_default(step=[3, 0, 0, 2]):     summary_ops.write('tag', 1.0) ```  Relevant log output ```shell 20250209 04:47:35.482562: F tensorflow/core/framework/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor Aborted (core dumped) ```",2025-02-09T04:49:39Z,type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/86918,I tried running your code on Colab using TensorFlow v2.18.0 and the nightly version. I faced the same issue. Please find gist here for reference. Thank you!
tpu,cybersupersoap,"`tf.summary_ops.run_metadata_graphs` aborts with ""Check failed: 1 == NumElements() (1 vs. 4)"""," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tfnightly 2.19.0dev20250207  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.run_metadata_graphs` . I have confirmed that below code would crash on `tfnightly 2.19.0dev20241025` (nightlybuild). Please find the gist to reproduce the issue.  Standalone code to reproduce the issue ```shell from tensorflow.python.ops import summary_ops_v2 as summary_ops from tensorflow.core.protobuf import config_pb2 writer = summary_ops.create_file_writer_v2(""/tmp"") meta = config_pb2.RunMetadata() with writer.as_default([3, 0, 0, 2]):     summary_ops.run_metadata_graphs(name='my_name', data=meta) ```  Relevant log output ```shell 20250209 04:39:36.666278: F tensorflow/core/framework/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor Aborted (core dumped) ```",2025-02-09T04:42:49Z,type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/86917,I tried running your code on Colab using TensorFlow v2.18.0 and the nightly version. I faced the same issue. Please find gist here for reference. Thank you!
tpu,cybersupersoap,"`io_ops.restore_v2` aborts with ""Check failed: size >= 0 (0 vs. -3) """," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.19.0dev20250207  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `aborted issue` in TensorFlow when I used API `tf.io_ops.restore_v2` . I have confirmed that below code would crash on `tfnightly 2.19.0dev20250207` (nightlybuild). Please find the gist to reproduce the issue.  Standalone code to reproduce the issue ```shell from tensorflow.python.framework import dtypes from tensorflow.python.framework import ops from tensorflow.python.ops import io_ops dtype = dtypes.uint4 with ops.Graph().as_default():     op = io_ops.restore_v2('model', ['var1', 'var2'], ['', '3 4 0,1:'], [dtype, dtype]) ```  Relevant log output ```shell 20250209 04:25:19.857968: F tensorflow/core/framework/tensor_shape.cc:413] Check failed: size >= 0 (0 vs. 3)  Aborted (core dumped) ```",2025-02-09T04:34:07Z,type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/86916,I tried running your code on Colab using TensorFlow v2.18.0 and the nightly version. I faced the same issue. Please find gist here for reference. Thank you!
opt,copybara-service[bot],[XLA] Don't forget to delete dead phis,"[XLA] Don't forget to delete dead phis The current phi graph optimization code assumes the call graph is flat. Unfortunately this is not the case in the presence of multiple execution threads and FCG is only run on the main thread, but HloAliasAnalysis, which invokes HloDataFlowAnalysis, is thread agnostic.",2025-02-09T01:35:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86900
opt,copybara-service[bot],[xla:cpu] Add benchmarks for measuring work queue contention,[xla:cpu] Add benchmarks for measuring work queue contention ```  Benchmark                                        Time             CPU   Iterations UserCounters...  BM_PopTaskMultiThreaded/2/process_time       63937 ns       126931 ns         5908 items_per_second=80.6735M/s BM_PopTaskMultiThreaded/4/process_time       52369 ns       167761 ns         4361 items_per_second=61.0392M/s BM_PopTaskMultiThreaded/8/process_time       65791 ns       329416 ns         2222 items_per_second=31.0854M/s BM_PopTaskMultiThreaded/16/process_time     106901 ns       840539 ns          825 items_per_second=12.1827M/s BM_PopTaskMultiThreaded/32/process_time     182810 ns      2188331 ns          304 items_per_second=4.67936M/s BM_PopTaskMultiThreaded/64/process_time     286746 ns      2654006 ns          288 items_per_second=3.85832M/s ```,2025-02-08T18:26:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86898
multi-gpu,epokrso,Bug sur TensorFlow 2.13 multi-GPU : freeze lors du fitting," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.13.0  Custom code Yes  OS platform and distribution Macos 15.3 (worker 0) Macos 12.7.6(worker 1)  Mobile device _No response_  Python version 3.8.20  Bazel version ...  GCC/compiler version 16.0.0 (apple M3) 14.0.0 (intel iris)  CUDA/cuDNN version _No response_  GPU model and memory Apple M3 and Intel Iris Graphics 6100  Current behavior? When I train a TensorFlow model on several GPUs (with tf.distribute.MultiWorkerMirroredStrategy), the execution is blocked at  Build the model under the strategy No error message is displayed, but the process no longer progresses after •	 I followed the recommendations of the official documentation, but the problem persists.  Standalone code to reproduce the issue ```shell import json import os import numpy as np import tensorflow as tf TF_CONFIG = {     ""cluster"": {         ""worker"": [""192.168.0.68:12345"", ""192.168.0.68:12346""]     },     ""task"": {""type"": ""worker"", ""index"": 0}   Modifier index pour chaque worker } os.environ[""TF_CONFIG""] = json.dumps(TF_CONFIG)  Manually Load the MNIST dataset data = np.load(""mnist.npz"") x_train, y_train = data[""x_train""], data[""y_train""] x_test, y_test = data[""x_test""], data[""y_test""]  Normalize images x_train, x_test = x_train / 255.0, x_test / 255.0  Add a dimension to match TensorFlow's expectations x_train = x_train[..., np.newaxis] x_test = x_test[..., np.newaxis]  Define the distribution strategy strategy = tf.distribute.MultiWorkerMirroredStrategy()  Build the model under the strategy with strategy.scope():     model = tf.keras.Sequential([         tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),         tf.keras.layers.MaxPooling2D((2, 2)),         tf.keras.layers.Flatten(),         tf.keras.layers.Dense(128, activation='relu'),         tf.keras.layers.Dense(10, activation='softmax')     ])     model.compile(optimizer='adam',                   loss='sparse_categorical_crossentropy',                   metrics=['accuracy']) model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test)) test_loss, test_acc = model.evaluate(x_test, y_test) print(f""Test accuracy: {test_acc:.4f}"") ```  Relevant log output ```shell 20250208 12:29:20.630247: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 20250208 12:29:20.630286: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB 20250208 12:29:20.630293: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB 20250208 12:29:20.630324: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. 20250208 12:29:20.630338: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) > physical PluggableDevice (device: 0, name: METAL, pci bus id: ) 20250208 12:29:20.631249: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. 20250208 12:29:20.631259: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 0 MB memory) > physical PluggableDevice (device: 0, name: METAL, pci bus id: ) 20250208 12:29:20.632347: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:449] Started server with target: grpc://192.168.0.68:12345 20250208 12:29:20.637550: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:535] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 14287335759644278642 20250208 12:29:20.637654: I tensorflow/tsl/distributed_runtime/coordination/coordination_service_agent.cc:298] Coordination agent has successfully connected. 20250208 12:29:37.728182: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:535] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 15227140312468372989 ```",2025-02-08T11:40:33Z,stat:awaiting tensorflower type:bug comp:gpu TF 2.13,open,0,1,https://github.com/tensorflow/tensorflow/issues/86897,"I was able to reproduce the same issue with a single GPU using TensorFlow 2.13. Below, I am attaching the output for your reference. ``` python multiworker_train.py TF_CONFIG: {'cluster': {'worker': ['192.168.0.68:12345', '192.168.0.68:12346']}, 'task': {'type': 'worker', 'index': 1}} 20250210 10:59:31.700945: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro 20250210 10:59:31.700969: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB 20250210 10:59:31.700978: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB 20250210 10:59:31.701009: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. 20250210 10:59:31.701023: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) > physical PluggableDevice (device: 0, name: METAL, pci bus id: ) 20250210 10:59:31.702173: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. 20250210 10:59:31.702184: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 0 MB memory) > physical PluggableDevice (device: 0, name: METAL, pci bus id: ) 20250210 10:59:31.704488: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:449] Started server with target: grpc://192.168.0.68:12346 ``` Thank you!"
sharding,copybara-service[bot],"In StableHLO -> HLO, add implicit arg shardings to while/case/if with zero results if the op has a sharding (replicated).","In StableHLO > HLO, add implicit arg shardings to while/case/if with zero results if the op has a sharding (replicated).",2025-02-08T11:33:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86896
tpu,Kush3007,Python 3.12 and further," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code No  OS platform and distribution Windows  Mobile device _No response_  Python version 3.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? So when are you guys going to make Tensorflow Compatible for Python version 3.13? 3.12 has been in the market for more than 1 year still the latest update doesn't work on 3.12, why is it so? Is Python 3.12 not for ML?  Standalone code to reproduce the issue ```shell The PIP won't install Tensorflow on 3.13, for now I had gone back to 3.11 ```  Relevant log output ```shell ```",2025-02-08T07:29:37Z,type:build/install,closed,0,2,https://github.com/tensorflow/tensorflow/issues/86879,See CC(Support Python 3.12) and CC(It doesn't support on python3.13). Please don't open a new issue if one already exists.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],Adds an option of additional alignment to the flatbuffer_exporter for the field `tflite.Buffer.data`.,Adds an option of additional alignment to the flatbuffer_exporter for the field `tflite.Buffer.data`.,2025-02-07T23:26:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86866
opt,copybara-service[bot],Simplify flag parsing in flag_types.cc using fixed_option_set_flag.,Simplify flag parsing in flag_types.. Also extend fixed_option_set_flag to support aliases if desired.,2025-02-07T22:08:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86861
opt,copybara-service[bot],[xla:cpu] Worker/WorkQueue micro-optimizations,[xla:cpu] Worker/WorkQueue microoptimizations,2025-02-07T21:18:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86857
opt,copybara-service[bot],Simplify flag parsing in gather_borg_symbols.cc using fixed_option_set_flag.,Simplify flag parsing in gather_borg_symbols..,2025-02-07T19:49:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86852
sharding,copybara-service[bot],#sdy Add replicated hlo shardings for while/case/if ops with no sdy shardings.,"sdy Add replicated hlo shardings for while/case/if ops with no sdy shardings. This is needed so when free variables are lifted in StableHLO>HLO conversion, the ops will get a sharding with the free variable shardings.",2025-02-07T17:12:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86846
tpu,copybara-service[bot],litert::internal::OpenLib probes multiple shared object paths and shouldn't output error messages for each missed probe attempt.,litert::internal::OpenLib probes multiple shared object paths and shouldn't output error messages for each missed probe attempt.,2025-02-07T14:03:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86837
opt,copybara-service[bot],PR #22123: Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline,PR CC(Incorrect results on GPU for reduce_* methods on large tensors): Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline Imported from GitHub PR https://github.com/openxla/xla/pull/22123 Copybara import of the project:  50788338bb8107c1c9b8fe338e253df594e8cfd0 by Sevin Varoglu : Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline  3af6a0d0af7cb488cb6ef6888b471b4ea72526d6 by Sevin Varoglu : Add hlo_opt test  1c87fad70fbbd9072e0221aa18cf15c352fb3d93 by Sevin Varoglu : Modify test Merging this change closes CC(Incorrect results on GPU for reduce_* methods on large tensors) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22123 from sfvaroglu:sevin/collective_cse 1c87fad70fbbd9072e0221aa18cf15c352fb3d93,2025-02-07T11:56:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86832
tpu,copybara-service[bot],[TPU][Pallas][XLA] Add BUILD time codegen tool that turns a pallas kernel into a parameterized kernel loader header that can be utilized anywhere in C++,[TPU][Pallas][XLA] Add BUILD time codegen tool that turns a pallas kernel into a parameterized kernel loader header that can be utilized anywhere in C++ Next step here is to write a specialization pass that takes the kernel loaded above and binds values to it (already done in prototype/scratch),2025-02-07T07:24:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86811
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-07T01:37:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86804
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-07T01:18:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86801
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-07T01:17:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86800
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-07T00:23:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86796
quantization,copybara-service[bot],Provide lax.composites to express quantization operations.,Provide lax.composites to express quantization operations.,2025-02-06T23:08:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86790
opt,copybara-service[bot],add an option to remove call-start/done to sparse core computations.,add an option to remove callstart/done to sparse core computations. used for hybridsim until it support sparsecore.,2025-02-06T23:04:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86789
gemma,copybara-service[bot],[xla:cpu] Add run and setup scripts for e2e gemma2 pyTorch,[xla:cpu] Add run and setup scripts for e2e gemma2 pyTorch,2025-02-06T21:27:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86785
sharding,copybara-service[bot],Rolling back due to broken tests.,"Rolling back due to broken tests. Original change: Build IFRT shardings with both addressable and nonaddressable devices, instead of only addressable devices (rollforward). Reverts e3b0866fb48abd1d20a074a8361608a72c20ad3f",2025-02-06T21:05:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86781
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-06T20:47:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86780
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-06T20:42:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86778
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-06T20:39:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86777
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-06T20:30:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86776
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-06T20:29:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86775
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-06T20:27:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86774
opt,copybara-service[bot],"[XLA:GPU] Make sharded autotuning use ""more unique"" keys, and allow it to reload results already present in the key-value store.","[XLA:GPU] Make sharded autotuning use ""more unique"" keys, and allow it to reload results already present in the keyvalue store. Previously, reautotuning the same set of fusions on the same host at different points during the lifetime of a given keyvalue store would cause sharded autotuning to crash. This was surfaced more prominently after a recent change started canonicalizing fusion strings before using their hash as a key in the keyvalue store. Autotuning two different modules containing the same set of fusions could therefore result in a collision during the second compilation, and a subsequent crash. Simply allowing to read from the cache did not seem like a good solution, given that different modules may be compiled with different options, and the cache can therefore hold stale results that shouldn't be accessed. There didn't seem to be a very good way to resolve this issue given the current design of (sharded) autotuning. The proper solution would be to remove autotuning from the compilation pipeline. To fix the issue faster, we elected here to also hash the module as part of the cache key, and to allow cache hits. Fetching more runtime information to uniquify the cache key was deemed undesirable: for one thing, the cache key derivation needs to be deterministic across hosts; for a second, passing enough information down from the runtime to truly make results unique would force us to further intertwine levels of abstraction that should be independent. There is still an issue whereby autotuning the same set of fusions within the same module twice within the lifetime of the same PjRt client could result in fetching stale cache dataif the user changes compilation options between both runs. Nevertheless, this issue was already there, and this change makes it more unlikely to occur in common scenarios. The hope is that switching to a better compilation model (single host compilation) and hoisting autotuning out of the compilation pipeline will allow us to definitely resolve this issue in the future.",2025-02-06T18:48:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86765
opt,copybara-service[bot],PR #22123: Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline,PR CC(Incorrect results on GPU for reduce_* methods on large tensors): Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline Imported from GitHub PR https://github.com/openxla/xla/pull/22123 Copybara import of the project:  50788338bb8107c1c9b8fe338e253df594e8cfd0 by Sevin Varoglu : Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline  3af6a0d0af7cb488cb6ef6888b471b4ea72526d6 by Sevin Varoglu : Add hlo_opt test  1c87fad70fbbd9072e0221aa18cf15c352fb3d93 by Sevin Varoglu : Modify test Merging this change closes CC(Incorrect results on GPU for reduce_* methods on large tensors) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22123 from sfvaroglu:sevin/collective_cse 1c87fad70fbbd9072e0221aa18cf15c352fb3d93,2025-02-06T17:39:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86764
yi,copybara-service[bot],[XLA:GPU] Do not regard the 'fusion' op as part of the `HloComputationFusion`.,"[XLA:GPU] Do not regard the 'fusion' op as part of the `HloComputationFusion`. Instead, check for this case in `ResolveUsers` and `ResolveOperand`, by querying whether the `fused_expression_root` is part of the `HloFusionAdaptor`. This prevents us from stepping into nested fusions.",2025-02-06T16:46:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86762
memory leak,copybara-service[bot],Fix race condition in FenceInsertionPass,"Fix race condition in FenceInsertionPass It looks like the idea here was to add some basic memoization so we don't end up with a ton of recursive calls, and potentially deadlock. However, doing that through a static variable is problematic both because it's not threadsafe, and because it's a silent memory leak, since we never free up the set (so a longrunning program would just continue adding stuff to it as we compile new kernels indefinitely). I'm still trying to get a good upstreamable reproducer, but this should fix the issue for now so we don't crash in production.",2025-02-06T16:15:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86760
opt,copybara-service[bot],PR #21965: [PJRT]  Expose should_stage_host_to_device_transfers as client create option,"PR CC(cannot find  ""flatbuffers/flatbuffers.h""  head  file): [PJRT]  Expose should_stage_host_to_device_transfers as client create option Imported from GitHub PR https://github.com/openxla/xla/pull/21965 Expose GPU option `should_stage_host_to_device_transfers ` as configurable in PJRT client create function. This allow the end user to override the default value which is `true`. ref: openxla/blob/main/xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h Since the option is living in a hashmap of type PJRT_NamedValue, it seems that I don't break the C PJRT API interface versioning. Copybara import of the project:  18b34c19938814b68bfd863fd5e603f08e9e824c by Hugo Mano : [PJRT]  Expose should_stage_host_to_device_transfers  as create option Expose GPU option `should_stage_host_to_device_transfers ` as configuration in PJRT client create func.  a412723fe4aa833dbfc0c27a2629d61bd68b34f7 by Hugo Mano : Address comments Merging this change closes CC(cannot find  ""flatbuffers/flatbuffers.h""  head  file) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21965 from hugomano:hugmano/pjrtexposeshould_stage_host_to_device_transfers a412723fe4aa833dbfc0c27a2629d61bd68b34f7",2025-02-06T16:11:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86759
yi,copybara-service[bot],[xla:emitters] fix type mismatch for several passes,[xla:emitters] fix type mismatch for several passes I've seen Windows failures because of these after applying some upcoming changes. Fix them.,2025-02-06T15:36:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86757
tpu,AlanBogarin,unexpected import during stub creation from mypy-protobuf," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version master  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.12.8  Bazel version 6.5.0  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm doing a stub distribution for tensorflow, and when I used mypyprotobuf to create stubs for .proto files, it resulted in an import that I didn't expect The format it should create is: ```python import tensorflow.tsl.protobuf.histogram_pb2 ``` but it does ```python import xla.tsl.protobuf.histogram_pb2 ``` I don't really understand how they structure the configurations to compile from Bazel Is there a setting I'm missing, or should I make the change manually? Is there a way for tensorflow to automatically create the stubs files from the compiled proto files? Here I leave you the code to speed up your analysis. third_party\xla\xla\tsl\protobuf\histogram.proto ```proto syntax = ""proto3""; package tensorflow; option cc_enable_arenas = true; option java_multiple_files = true; option java_package = ""org.tensorflow.framework""; option go_package = ""github.com/google/tsl/tsl/go/core/protobuf/summary_go_proto""; // Serialization format for histogram module in // tsl/lib/histogram/histogram.h message HistogramProto {   double min = 1;   double max = 2;   double num = 3;   double sum = 4;   double sum_squares = 5;   // Parallel arrays encoding the bucket boundaries and the bucket values.   // bucket(i) is the count for the bucket i.  The range for   // a bucket is:   //   i == 0:  DBL_MAX .. bucket_limit(0)   //   i != 0:  bucket_limit(i1) .. bucket_limit(i)   repeated double bucket_limit = 6 [packed = true];   repeated double bucket = 7 [packed = true]; } ``` tensorflow\core\framework\summary.proto ```proto syntax = ""proto3""; package tensorflow; import public ""xla/tsl/protobuf/histogram.proto""; import ""tensorflow/core/framework/tensor.proto""; option cc_enable_arenas = true; option java_outer_classname = ""SummaryProtos""; option java_multiple_files = true; option java_package = ""org.tensorflow.framework""; option go_package = ""github.com/tensorflow/tensorflow/tensorflow/go/core/framework/summary_go_proto""; // Metadata associated with a series of Summary data message SummaryDescription {   // Hint on how plugins should process the data in this series.   // Supported values include ""scalar"", ""histogram"", ""image"", ""audio""   string type_hint = 1; } // A SummaryMetadata encapsulates information on which plugins are able to make // use of a certain summary value. message SummaryMetadata {   message PluginData {     // The name of the plugin this data pertains to.     string plugin_name = 1;     // The content to store for the plugin. The best practice is for this to be     // a binary serialized protocol buffer.     bytes content = 2;   }   // Data that associates a summary with a certain plugin.   PluginData plugin_data = 1;   // Display name for viewing in TensorBoard.   string display_name = 2;   // Longform readable description of the summary sequence. Markdown supported.   string summary_description = 3;   // Class of data stored in this time series. Required for compatibility with   // TensorBoard's generic data facilities (`DataProvider`, et al.). This value   // imposes constraints on the dtype and shape of the corresponding tensor   // values. See `DataClass` docs for details.   DataClass data_class = 4; } enum DataClass {   // Unknown data class, used (implicitly) for legacy data. Will not be   // processed by data ingestion pipelines.   DATA_CLASS_UNKNOWN = 0;   // Scalar time series. Each `Value` for the corresponding tag must have   // `tensor` set to a rank0 tensor of type `DT_FLOAT` (float32).   DATA_CLASS_SCALAR = 1;   // Tensor time series. Each `Value` for the corresponding tag must have   // `tensor` set. The tensor value is arbitrary, but should be small to   // accommodate direct storage in database backends: an upper bound of a few   // kilobytes is a reasonable rule of thumb.   DATA_CLASS_TENSOR = 2;   // Blob sequence time series. Each `Value` for the corresponding tag must   // have `tensor` set to a rank1 tensor of bytestring dtype.   DATA_CLASS_BLOB_SEQUENCE = 3; } // A Summary is a set of named values to be displayed by the // visualizer. // // Summaries are produced regularly during training, as controlled by // the ""summary_interval_secs"" attribute of the training operation. // Summaries are also produced at the end of an evaluation. message Summary {   message Image {     // Dimensions of the image.     int32 height = 1;     int32 width = 2;     // Valid colorspace values are     //   1  grayscale     //   2  grayscale + alpha     //   3  RGB     //   4  RGBA     //   5  DIGITAL_YUV     //   6  BGRA     int32 colorspace = 3;     // Image data in encoded format.  All image formats supported by     // image_codec::CoderUtil can be stored here.     bytes encoded_image_string = 4;   }   message Audio {     // Sample rate of the audio in Hz.     float sample_rate = 1;     // Number of channels of audio.     int64 num_channels = 2;     // Length of the audio in frames (samples per channel).     int64 length_frames = 3;     // Encoded audio data and its associated RFC 2045 content type (e.g.     // ""audio/wav"").     bytes encoded_audio_string = 4;     string content_type = 5;   }   message Value {     // This field is deprecated and will not be set.     string node_name = 7;     // Tag name for the data. Used by TensorBoard plugins to organize data. Tags     // are often organized by scope (which contains slashes to convey     // hierarchy). For example: foo/bar/0     string tag = 1;     // Contains metadata on the summary value such as which plugins may use it.     // Take note that many summary values may lack a metadata field. This is     // because the FileWriter only keeps a metadata object on the first summary     // value with a certain tag for each tag. TensorBoard then remembers which     // tags are associated with which plugins. This saves space.     SummaryMetadata metadata = 9;     // Value associated with the tag.     oneof value {       float simple_value = 2;       bytes obsolete_old_style_histogram = 3;       Image image = 4;       HistogramProto histo = 5;       Audio audio = 6;       TensorProto tensor = 8;     }   }   // Set of values for the summary.   repeated Value value = 1; } ```  Standalone code to reproduce the issue ```shell Commands to recreate >>> git clone https://github.com/tensorflow/tensorflow.git >>> cd tensorflow >>> mkdir out >>> protoc tensorflow/core/framework/summary.proto mypy_out=out/ I=""."" I=""third_party/xla/"" ```  Relevant log output ```shell ```",2025-02-06T14:03:17Z,type:build/install type:support,open,0,4,https://github.com/tensorflow/tensorflow/issues/86752,", Could you please provide the error log which you are facing the issue. It helps to analyse the issue in an effective way. Thank you!","> [](https://github.com/AlanBogarin), Could you please provide the error log which you are facing the issue. It helps to analyse the issue in an effective way. Thank you! ``` PS D:\Alan\github> git clone https://github.com/tensorflow/tensorflow.git Cloning into 'tensorflow'... remote: Enumerating objects: 1972623, done. remote: Counting objects: 100% (428/428), done. remote: Compressing objects: 100% (280/280), done. remote: Total 1972623 (delta 289), reused 169 (delta 148), packreused 1972195 (from 2) Receiving objects: 100% (1972623/1972623), 1.07 GiB  None = ...,     ) > None: ...     def ClearField(self, field_name: typing.Literal[""value"", b""value""]) > None: ... global___Summary = Summary ```","When I install tensorflow with pip, tsl is inside tensorflow ``` tensorflow/     tsl/         profiler/             ...         protobuf/             histogram_pb2.py             ...         ...     ... ```", Is there any progress?
opt,copybara-service[bot],PR #22256: [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell,"PR CC(Option: Use fused_batch_norm instead of batch_norm for layer normalization): [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell Imported from GitHub PR https://github.com/openxla/xla/pull/22256 Update the `BlockScalingRewriter` pass to allow lowering the ""__op$block_scaled_dot"" to a cuDNN graph (instead of HLO graph)  this lowers to a block scaled dot kernel via cuDNN frontend (since v1.10). The only format supported currently is MXFP8 (both dot inputs are quantized to E4M3FN/E5M2 tensors with E8M0 scales using block size 32). MXFP8 docs: https://www.opencompute.org/documents/ocpmicroscalingformatsmxv10specfinalpdf The cuDNN kernel has some requirements for the input shapes, so some padding may be needed. Also, the scale tensor must be swizzled (transposed in a specific manner). The tests are added that verify both the pass transformations and the runtime correctness. The pass is also enabled in the same PR. Copybara import of the project:  145554dea5fec31a44b968f344bcd325e894e930 by Sergey Kozub : [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell Merging this change closes CC(Option: Use fused_batch_norm instead of batch_norm for layer normalization) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22256 from openxla:skozub/block_scaling_cudnn 145554dea5fec31a44b968f344bcd325e894e930",2025-02-06T09:32:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86741
opt,copybara-service[bot],[ODML] StablehloOptimizePass : MHLO -> StableHLO,[ODML] StablehloOptimizePass : MHLO > StableHLO,2025-02-06T06:39:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86716
opt,copybara-service[bot],Simplify flag parsing in hlo_runner.cc using fixed_options_flag.,Simplify flag parsing in hlo_runner..,2025-02-06T06:18:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86714
opt,copybara-service[bot],"Optimize pattern for mul(a, a) -> pow(a, 2)","Optimize pattern for mul(a, a) > pow(a, 2) Also,  mul(pow(a, 2), a)) > pow(a, 3)  mul(a, square(a)) > pow(a, 3)",2025-02-06T05:56:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86712
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-06T04:19:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86711
opt,copybara-service[bot],Simplify flag parsing in functional_hlo_runner.cc using the fixed_options_flag library.,Simplify flag parsing in functional_hlo_runner.. The new implementation is more declarative and less errorprone.,2025-02-06T04:13:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86710
tpu,copybara-service[bot],"lite: Keep signature input, output names order sync with SignatureDef.","lite: Keep signature input, output names order sync with SignatureDef. In LiteRT, it assumes that input_names(), output_names() follows the SignatureDef in the given model.",2025-02-06T01:50:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86706
yi,copybara-service[bot],Fix GatherClientLibraryTest under PjRt.,"Fix GatherClientLibraryTest under PjRt. It was unclear to me what this test was actually trying to achieve because it is not clearly documented. If it is trying to exercise a specific behavior with the old nonPjRt `Client`, then that should probably live elsewhere. I've converted it to something that can just run on top of `HloPjRtTestBase` directly. I added some boilerplate to allow the `XlaBuilder` code to remain, though it might be better to just use a HLO string directly.",2025-02-05T23:49:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86697
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-05T23:20:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86693
quantization,copybara-service[bot],Add quantization dimension verification of stablehlo.uniform_(de)quantize ops.,Add quantization dimension verification of stablehlo.uniform_(de)quantize ops.,2025-02-05T23:14:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86692
opt,copybara-service[bot],Adds a library to make fixed options flags easier and safer to define.,Adds a library to make fixed options flags easier and safer to define. xla defines custom commandline flag parsing logic in several places. These definitions follow a common pattern: there is a fixed set of options for the flag type (e.g. the flag type is an enum). Such definitions are repetitious and errorprone (one has to make sure that the name => value and value => name mappings are consistent and have no duplicate names or values). This library abstracts away the tedious details and will be used in subsequent changes to make the definitions simpler and safer.,2025-02-05T19:06:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86669
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-05T18:00:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86661
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-05T18:00:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86660
sharding,copybara-service[bot],"Build IFRT shardings with both addressable and non-addressable devices, instead of only addressable devices (roll-forward).","Build IFRT shardings with both addressable and nonaddressable devices, instead of only addressable devices (rollforward). Reverts 814b7dcd21487da4be271cbcf58a5d463f5e1a1a",2025-02-05T17:45:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86659
yi,copybara-service[bot],Removing some patches by:,Removing some patches by:  Upstreaming internal testto remove file entirely. Also removing patch with redundant (already upstream) tests.  Verifying an issue is already fixed.  Attempting to upstream 2 changes. Added comments to remove 2 more patches in a followup if they land successfully.,2025-02-05T13:29:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86642
tpu,copybara-service[bot],PR #22334: [ROCm] Fix flaky gpu compiler test when building with rocm,PR CC([tflite] fix calculating of output pixels): [ROCm] Fix flaky gpu compiler test when building with rocm Imported from GitHub PR https://github.com/openxla/xla/pull/22334 This change fixes the flaky gpu compiler test used to run on rocm CI pipeline gate. Triton pipeline was wrongly using the TritonGPUAccelerateMatmul pass which supports cuda only. In rocm there is a different pass which is now used in the rocm pipeline. https://github.com/tritonlang/triton/blob/main/third_party/amd/lib/TritonAMDGPUTransforms/AccelerateAMDMatmul.cpp Copybara import of the project:  c5f600f03aa87d155bb624bedb0584e635af190e by Alexandros Theodoridis : Fix flaky gpu compiler test when building with rocm Merging this change closes CC([tflite] fix calculating of output pixels) Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22334 from ROCm:rocm_fix_flaky_gpu_compiler_test c5f600f03aa87d155bb624bedb0584e635af190e,2025-02-05T12:45:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86641
opt,copybara-service[bot],Use custom HLO deserialization for HloUnoptimizedSnapshot.,Use custom HLO deserialization for HloUnoptimizedSnapshot. This change makes it possible to read HloUnoptimizedSnapshot protos that are over 2GiB in size and were dumped using custom proto serialization.,2025-02-05T12:20:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86640
opt,copybara-service[bot],PR #22091: [ROCM] Try targeting cuBLAS if it's not profitable to fuse dot,PR CC(Failed to load the native TensorFlow runtime.): [ROCM] Try targeting cuBLAS if it's not profitable to fuse dot Imported from GitHub PR https://github.com/openxla/xla/pull/22091 TritonTest.NonstandardLayoutWithManyNonContractingDims TritonTest.NonstandardLayoutWithManyNonContractingDimsReversedLayout If it's not profitable to fuse dot and cuBLAS is not requiring extra padding then targeting custom call would be optimal. Align this check for both ROCm and CUDA here. rotation would you please have a look? Copybara import of the project:  a161c86d14b5c3e5a61d47474142646f141814ae by Jian Li : [ROCM] Try targeting cuBLAS if it's not profitable to fuse dot Merging this change closes CC(Failed to load the native TensorFlow runtime.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22091 from ROCm:ci_fix_rocm_gemm_fusion a161c86d14b5c3e5a61d47474142646f141814ae,2025-02-05T10:18:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86637
opt,copybara-service[bot],PR #22256: [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell,"PR CC(Option: Use fused_batch_norm instead of batch_norm for layer normalization): [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell Imported from GitHub PR https://github.com/openxla/xla/pull/22256 Update the `BlockScalingRewriter` pass to allow lowering the ""__op$block_scaled_dot"" to a cuDNN graph (instead of HLO graph)  this lowers to a block scaled dot kernel via cuDNN frontend (since v1.10). The only format supported currently is MXFP8 (both dot inputs are quantized to E4M3FN/E5M2 tensors with E8M0 scales using block size 32). MXFP8 docs: https://www.opencompute.org/documents/ocpmicroscalingformatsmxv10specfinalpdf The cuDNN kernel has some requirements for the input shapes, so some padding may be needed. Also, the scale tensor must be swizzled (transposed in a specific manner). The tests are added that verify both the pass transformations and the runtime correctness. The pass is also enabled in the same PR. Copybara import of the project:  145554dea5fec31a44b968f344bcd325e894e930 by Sergey Kozub : [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell Merging this change closes CC(Option: Use fused_batch_norm instead of batch_norm for layer normalization) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22256 from openxla:skozub/block_scaling_cudnn 145554dea5fec31a44b968f344bcd325e894e930",2025-02-05T10:18:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86635
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-05T10:10:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86633
opt,chunhsue,Qualcomm AI Engine Direct - Support DUS and Pack Op in LiteRT, WHAT  Pack is supported using QNN Concat to address type support issue.   DUS are supported partially for specific use cases.   Reuse input static tensor.   Use absl::Span instead of std::span.   TESTS ```  build bazel build c opt cxxopt=std=c++17 //tensorflow/lite/experimental/litert/vendors/qualcomm/compiler:qnn_compiler_plugin_test  run ./bazelbin/tensorflow/lite/experimental/litert/vendors/qualcomm/compiler/qnn_compiler_plugin_test ``` ``` [] Global test environment teardown [==========] 100 tests from 4 test suites ran. (4292 ms total) [  PASSED  ] 100 tests. ```,2025-02-05T09:32:58Z,comp:lite size:XL,closed,0,5,https://github.com/tensorflow/tensorflow/issues/86630,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , Can you please resolve the conflicts? Thank you!","Thanks for the reminder, done! > Hi , Can you please resolve the conflicts? Thank you!","Hi , Rebased again, please check.  Thanks!",> please uncomment the dus and pack tests in qnn_compiler_plugin_test and check they work Thanks! Didn't notice https://github.com/tensorflow/tensorflow/commit/ac02f89a538615c506ac15d51fba9abfe90483b5 not being in my base. Fixed in 6d5b5ab8653fd27cf202025b8e912e95e14f9eda. Tested with QNN 2.29.0.
yi,copybara-service[bot],internal change for visibilily,internal change for visibilily,2025-02-05T07:06:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86609
tpu,usmonali4,inconsistent result of ```tf.raw_ops.Rsqrt``` on CPU and GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04 (google colab)  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5/9  GPU model and memory Tesla T4  Current behavior? result of ``tf.raw_ops.Rsqrt``` is inconsistent between CPU and GPU  Standalone code to reproduce the issue ```shell import tensorflow as tf x_0 = tf.constant([[[1.2031, 0.2168], [0.1177, 0.6758]],                      [[1.8203, 0.9688], [0.8828, 0.0767]]], dtype=tf.bfloat16) with tf.device('CPU:0'):   result_cpu = tf.raw_ops.Rsqrt(x=x_0)   print(""Output on CPU:"", result_cpu) with tf.device('GPU:0'):   result_gpu = tf.raw_ops.Rsqrt(x=x_0)   print(""Output on GPU:"", result_gpu) max_abs_diff = tf.reduce_max(tf.abs(result_cpu  result_gpu)).numpy() is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e3,  atol=1e2) print(""Max absolute difference:"", max_abs_diff) print(""Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3:"", is_consistent.numpy()) ```  Relevant log output ```shell Output on CPU: tf.Tensor( [[[0.910156 2.14062]   [2.92188 1.21875]]  [[0.742188 1.01562]   [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16) Output on GPU: tf.Tensor( [[[0.914062 2.15625]   [2.90625 1.21875]]  [[0.738281 1.01562]   [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16) Max absolute difference: 0.015625 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: False ```",2025-02-05T06:12:29Z,stat:awaiting tensorflower type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/86607,"  weird, no issue here: ```bash TensorFlow Version: 2.16.2 Python Version: 3.10.16  (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] Python Implementation: CPython CUDA Version (TensorFlow): 12.0 cuDNN Version (TensorFlow): 8 GPU Name (PyTorch): NVIDIA GeForce RTX 4070 SUPER CUDA Version (PyTorch): 12.0 cuDNN Version (PyTorch): 8907 Driver Version: 550.144.03 Ubuntu Version: PRETTY_NAME=""Ubuntu 24.04.1 LTS"" Model name:                           AMD Ryzen 5 7600 6Core Processor Output on CPU: tf.Tensor( [[[0.914062 2.15625]   [2.90625 1.21875]]  [[0.738281 1.01562]   [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16) Output on GPU: tf.Tensor( [[[0.914062 2.15625]   [2.90625 1.21875]]  [[0.738281 1.01562]   [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16) Max absolute difference: 0.0000000000 Consistency check (CPU vs GPU) with atol=1e6 and rtol=1e7: True ```"
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets. Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T06:02:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86606
yi,copybara-service[bot],"In literal_util.cc, use absl::uniform_int_distribution.","In literal_util.cc, use absl::uniform_int_distribution. absl::uniform_int_distribution is faster than std::uniform_int_distribution. This makes initializing literals in run_hlo_module faster. In particular, I tested the following HLO:     ENTRY f {       arg = s8[2000000000] parameter(0)       ROOT add_result = s8[2000000000] add(arg, arg)     } It takes 7.8 seconds to initialize the input literal with the absl function, and 18.2 with the std function. I had to change several tests, which were sensitive to the exact values randomlyinitialized Literals were initialized to. Literals are initialized to a fixed seed, but this change causes such literals to be initialized to different values than before. Unfortunately the absl version of uniform_real_distribution is not faster. It takes 25.5 seconds with absl and 8.3 with std on the HLO when s8 is replaced with f16. The function does become faster if we use an absl::BitGen instead of an std::minstd_rand0, but this is considerable more work to implement as it will affect many places in the codebase which currently use an std::minstd_rand0. Also, changing the floatingpoint RNG algorithm will likely cause many more tests to fail which are inadvertently reliant on the current specific values literals are initialized to. Maybe I'll do this in the future, maybe not.",2025-02-05T03:37:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86599
opt,copybara-service[bot],add an option to remove call-start/done to sparse core computations.,add an option to remove callstart/done to sparse core computations. used for hybridsim until it support sparsecore.,2025-02-05T01:32:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86595
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-04T22:30:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86589
opt,copybara-service[bot],[XLA] Make the compiler respond to the new optimization_level and memory_fitting_level options when available.,[XLA] Make the compiler respond to the new optimization_level and memory_fitting_level options when available. We temporarily continue to respect the legacy exec_time_optimization_effort option if the new one is not specified. This will be phased out once all users have been migrated away from the old option.,2025-02-04T22:06:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86584
yi,copybara-service[bot],Return arrays from `ArrayImpl._check_and_rearrange`.,"Return arrays from `ArrayImpl._check_and_rearrange`. This is in preparation for a larger change, so that input buffers can be checked before Array creation in XLA and the user gets more helpful JAX error messages instead of XLA errors. Reverts 135a67d02fc6282a323fc4ad42ef7d8a687995e6",2025-02-04T21:54:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86583
tpu,copybara-service[bot],Add HloRunnerPjRt support for output tuples w/ mixed array and non-array shapes.,"Add HloRunnerPjRt support for output tuples w/ mixed array and nonarray shapes. Prior to this change, `HloRunnerPjRt` would fail during execution of `FfiCustomCallTest.Tokens` (on CPU) because the `GenerateExecutionOptions` function called `ComputationLayout::FlattenedResultLayouts`. This function returns an error if any of the subshapes of the tuple are not arrays. While it is possible to adapt the implementation of this particular function to return only array shapes and skip over the nonarray shapes, it would also require changes in how the output buffers are allocated in the CPU compiler.",2025-02-04T21:46:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86582
opt,copybara-service[bot],[XLA] Introduce discrete scheme for execution time optimization and memory fitting effort.,"[XLA] Introduce discrete scheme for execution time optimization and memory fitting effort. This will replace the previously introduced floatingpoint effort levels, which will soon be removed.",2025-02-04T20:55:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86580
yi,copybara-service[bot],Stop modifying the TraceEventsContainer in DoStoreAsLevelDbTable. This behavior,Stop modifying the TraceEventsContainer in DoStoreAsLevelDbTable. This behavior is not intuitive (modifying a const value that was passed in) and unnecessary. Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-04T19:55:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86577
opt,copybara-service[bot],Add `--xnnpack_runtime_flags` option to pass raw XNNPACK flags to XNNPACK,Add `xnnpack_runtime_flags` option to pass raw XNNPACK flags to XNNPACK,2025-02-04T18:51:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86572
tpu,copybara-service[bot],[XLA] tool to print indexing map of operands,"[XLA] tool to print indexing map of operands example output ``` Output 0 operand 0: (d0) > (d0), domain: d0 in [0, 1023] Output 0 operand 1: (d0) > (), domain: d0 in [0, 1023] Output 1 operand 0: (d0) > (d0), domain: d0 in [0, 1023] Output 1 operand 1: (d0) > (), domain: d0 in [0, 1023] Output 1 operand 2: (d0) > (d0), domain: d0 in [0, 1023] ```",2025-02-04T16:56:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86569
attention,copybara-service[bot],PR #19834: [ds-fusion]Add support for async dynamic slice fusion,"PR CC(Attention Wrapper state error.): [dsfusion]Add support for async dynamic slice fusion Imported from GitHub PR https://github.com/openxla/xla/pull/19834 This patch adds async handling to dynamic slice fusion when the hero operation is a collective operation. Currently, only reducescatter is supported as a hero operation in dynamic slice thunk, so this patch also follows the same. Added a test with compute, to ensure that communication and compute overlap in the thunks emitted. Copybara import of the project:  13553a14592ed9e2c3bbabdff0051f18a0f27d2e by Shraiysh Vaishay : Add support for async dynamic slice fusion This patch adds async handling to dynamic slice fusion when the hero operation is a collective operation. Currently, only reducescatter is supported as a hero operation in dynamic slice thunk, so this patch also follows the same. Added a test with compute, to ensure that communication and compute overlap in the thunks emitted.  e2c598694b035e94c0b698b0835caf3f8d47a81c by Shraiysh Vaishay : Addressed comments.  ae73bb0e26227870d74f4aa0dcbc3532e4b012e1 by Shraiysh Vaishay : Rebase and fix build errors  c68b096c87115908013889ee99754cb7c3e0776b by Shraiysh Vaishay : Rebase  7609238f6757e789e1d3d910d0687c60e0a4e36b by Shraiysh Vaishay : Addressed comments  6b47e685e7f333cd49b137497f90c2ea30a9e15a by Shraiysh Vaishay : Address comments  12890a56c29c8383a8a9d5ebefa54db52b64a099 by Shraiysh Vaishay : Address comments  a585534c08c6cfabc259319312025386494abe17 by Shraiysh Vaishay : Fix Executable > OpaueExecutable Merging this change closes CC(Attention Wrapper state error.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19834 from shraiysh:asyncdynamicslicefusion a585534c08c6cfabc259319312025386494abe17",2025-02-04T10:26:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86555
tpu,weilhuan-quic,Support Qualcomm Op Builders for LiteRT," WHAT  Support Op implementations based on the wrappers.  Use `TensorPool` to manage the intermediate tensors, `TensorPool` can be abstract or change to allocator if need.  All builders are functions which receive `TensorPool`, input output `TensorWrapper`, and the opspecific parameters.  All builders are functions which return `std::vector`  The interface of all builders is open to change based on the requirement.  Make these builders independent to LiteRT/tflite, use c++ primitive types as parameters.  Only dependent on QNN and `Wrappers`.  Supported Op  ",2025-02-04T10:09:09Z,size:XL,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86553
tpu,usmonali4,inconsistent result of ```tf.raw_ops.Tan``` on CPU and GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5/9  GPU model and memory Tesla T4  Current behavior? ```tf.raw_ops.Tan``` produces inconsistent results between CPU and GPU  Standalone code to reproduce the issue ```shell import tensorflow as tf real = tf.constant([1.5634333], dtype=tf.float32) imag = tf.constant([0.020735], dtype=tf.float32) complex_tensor = tf.complex(real, imag) with tf.device('/CPU:0'):     result_cpu = tf.raw_ops.Tan(x=complex_tensor)     print(result_cpu) with tf.device('/GPU:0'):     result_gpu = tf.raw_ops.Tan(x=complex_tensor)     print(result_gpu) Comparing whole complex numbers max_abs_diff = tf.reduce_max(tf.abs(result_cpu  result_gpu)).numpy() is_cons = tf.experimental.numpy.allclose(result_cpu, result_gpu, rtol=1e6,  atol=1e5) print(""Max absolute difference:"", max_abs_diff) print(""Consistency check (CPU vs GPU) with atol=1e5 and rtol=1e6:"", is_cons.numpy()) Comparing by parts real_part_cpu = tf.math.real(result_cpu) real_part_gpu = tf.math.real(result_gpu) real_part_diff = tf.reduce_max(tf.abs(real_part_cpu  real_part_gpu)).numpy() real_part_cons = tf.experimental.numpy.allclose(real_part_cpu, real_part_gpu, rtol=1e6,  atol=1e5) imag_part_cpu = tf.math.imag(result_cpu) imag_part_gpu = tf.math.imag(result_gpu) imag_part_diff = tf.reduce_max(tf.abs(imag_part_cpu  imag_part_gpu)).numpy() imag_part_cons = tf.experimental.numpy.allclose(imag_part_cpu, imag_part_gpu, rtol=1e6,  atol=1e5) print(""Real parts absolute difference:"", real_part_diff) print(""Real parts Consistency check with atol=1e5 and rtol=1e6:"", real_part_cons.numpy()) print(""Imag parts absolute difference:"", imag_part_diff) print(""Imag parts Consistency check with atol=1e5 and rtol=1e6:"", imag_part_cons.numpy()) ```  Relevant log output ```shell tf.Tensor([15.205576+42.834145j], shape=(1,), dtype=complex64) tf.Tensor([15.205605+42.834225j], shape=(1,), dtype=complex64) Max absolute difference: 8.5064334e05 Consistency check (CPU vs GPU) with atol=1e5 and rtol=1e6: False Real parts absolute difference: 2.861023e05 Real parts Consistency check with atol=1e5 and rtol=1e6: False Imag parts absolute difference: 8.010864e05 Imag parts Consistency check with atol=1e5 and rtol=1e6: False ```",2025-02-04T03:18:15Z,type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/86506,I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TFnightly on both CPU and GPU. Please find gist1 and gist2 here for your reference. Thank you!,"reproduced on different configuration: ```bash TensorFlow Version: 2.16.2 Python Version: 3.10.16  (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] Python Implementation: CPython CUDA Version (TensorFlow): 12.0 cuDNN Version (TensorFlow): 8 GPU Name (PyTorch): NVIDIA GeForce RTX 4070 SUPER CUDA Version (PyTorch): 12.0 cuDNN Version (PyTorch): 8907 Driver Version: 550.144.03 Ubuntu Version: PRETTY_NAME=""Ubuntu 24.04.1 LTS"" Model name:                           AMD Ryzen 5 7600 6Core Processor tf.Tensor([15.205576+42.834145j], shape=(1,), dtype=complex64) tf.Tensor([15.205498+42.83405j], shape=(1,), dtype=complex64) Max absolute difference: 0.0001233304 Consistency check (CPU vs GPU) with atol=1e5 and rtol=1e6: False Real parts absolute difference: 7.8201294e05 Real parts Consistency check with atol=1e5 and rtol=1e6: False Imag parts absolute difference: 9.536743e05 Imag parts Consistency check with atol=1e5 and rtol=1e6: False ```"
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-04T00:47:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86498
sharding,copybara-service[bot],"[xla:python] Add a mechanism for ""batch partitioning"" of FFI calls.","[xla:python] Add a mechanism for ""batch partitioning"" of FFI calls. This is the first in a series of changes to add a simple API for supporting a set of common sharding and partitioning patterns for FFI calls. The high level motivation is that custom calls (including FFI calls) are opaque to the SPMD partitioner, and the only ways to customize the partitioning behavior is to (a) explicitly register an `xla::CustomCallPartitoner` with XLA, or (b) use the `jax.experimental.custom_partitioning` APIs. Option (a) isn't generally practical for most use cases where the FFI handler lives in an external binary. Option (b) is flexible, and supports all common use cases, but it requires embedding Python callbacks in to the HLO, which can lead to issues including cache misses. Furthermore, `custom_partitioning` is overpowered for many use cases, where only (what I will call) ""batch partitioning"" is supported. In this case, ""batch partitioning"" refers to the behavior of many FFI calls where they can be trivially partitioned on some number of (leading) dimensions, with the same call being executed independently on each shard of data. If the data are sharded on nonbatch dimensions, partitioning will still reshard the data to be replicated on the nonbatch dimensions. This kind of partitioning logic applies to all the LAPACK/cuSOLVER/etc.backed linear algebra functions in jaxlib, as well as some external users of `custom_partitioning`. The approach I'm taking here is to add a new registration function to the XLA client, which let's a user label their FFI call as batch partitionable. Then, when lowering the custom call, the user passes the number of batch dimensions as a frontend attribute, which is then interpreted by the SPMD partitioner. In parallel with this change, shardy has added support for sharding propagation across custom calls using a string representation that is similar in spirit to this approach, but somewhat more general. However, the shardy implementation still requires a Python callback for the partitioning step, so it doesn't (yet!) solve all of the relevant problems with the `custom_partitioning` approach. Ultimately, it should be possible to have the partitioner parse the shardy sharding rule representation, but I wanted to start with the minimal implementation. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22454 from yliu120:create_dump_dir d5bb15865b5601b6082ae871616b9611405e844e",2025-02-03T21:29:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86483
opt,copybara-service[bot],[HLO-OPT] Tool : register HWI passes from hlo/transforms/ directory,[HLOOPT] Tool : register HWI passes from hlo/transforms/ directory,2025-02-03T19:25:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86473
opt,copybara-service[bot],"Move the many options for printing of instructions into separate file,","Move the many options for printing of instructions into separate file, as this header file is too long already.",2025-02-03T18:59:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86472
opt,copybara-service[bot],PR #21375: [ds-fusion] Get While loop analysis with copy fusion,"PR CC(Raspberry Pi install command not properly formatted.): [dsfusion] Get While loop analysis with copy fusion Imported from GitHub PR https://github.com/openxla/xla/pull/21375 In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index. Copybara import of the project:  3147ec926aa1c6fdfa2f4376668434c9a2fbeb87 by Shraiysh Vaishay : [dsfusion] Get While loop analysis with copy fusion In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index.  a435fbd2eadc17269d7bccbe141dcf7a21cc20e8 by Shraiysh Vaishay : Relay control dependencies while converting fusion to call (extractor) Merging this change closes CC(Raspberry Pi install command not properly formatted.) Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21375 from shraiysh:while_loop_analysis a435fbd2eadc17269d7bccbe141dcf7a21cc20e8",2025-02-03T18:39:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86470
tensorrt,copybara-service[bot],PR #21998: Add kCall to GetFusibleComputations,"PR CC(TensorRT INT8 assertion failed when performing calibration): Add kCall to GetFusibleComputations Imported from GitHub PR https://github.com/openxla/xla/pull/21998 Previously, we had issues with the stream annotation being applied to nonGemm operations, and having the compilation fail at IR emitter stage. Operations that normally should've been fused weren't getting fused inside the async computation. This PR is intended to fix this issue by including the call computations when searching for fusible computations. This was suggested by   Copybara import of the project:  939f105c3f053eaa14f119e67b5c92ab9e0b3aca by chaser : Add kCall to GetFusibleComputations  1ef97c83e26cb61dc222bd9982f58a140c5d97a9 by chaser : Add test to priority_fusion, edits based on comments Merging this change closes CC(TensorRT INT8 assertion failed when performing calibration) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21998 from chaserileyroberts:chase/fusions_in_async_computation 1ef97c83e26cb61dc222bd9982f58a140c5d97a9",2025-02-03T13:10:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86456
attention,copybara-service[bot],[XLA:GPU] Delete passes and rewriters related to cuDNN FMHA.,"[XLA:GPU] Delete passes and rewriters related to cuDNN FMHA. The custom call remains registered in the frontend, and remains explicitly callable from input HLO. JAX users can use the dedicated attention API to target cuDNN explicitly. The related flag is also deprecated, but remains part of the commandline options for the time being. A future change will delete that as well.",2025-02-03T11:36:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86450
tpu,copybara-service[bot],PR #22117: [XLA:GPU] Fix FMHA unit tests to not use FMHA rewriter,PR CC(Bad output for sqrt?): [XLA:GPU] Fix FMHA unit tests to not use FMHA rewriter Imported from GitHub PR https://github.com/openxla/xla/pull/22117 * use FMHA custom call directly instead of calling rewriter. Copybara import of the project:  f0a8e4242edaf16df2d42ba490cdad6573aefc95 by cjkkkk : fix  9edd2c7f6047a648e4f83d851dd279042e60c41a by cjkkkk : fix format Merging this change closes CC(Bad output for sqrt?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22117 from Cjkkkk:fix_fmha_rewriter 9edd2c7f6047a648e4f83d851dd279042e60c41a,2025-02-03T11:15:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86447
sharding,copybara-service[bot],PR #22164: [GPU] Fix sharded autotuning compatibility with result caching.,PR CC(Fix broken link in rnn_colorbot): [GPU] Fix sharded autotuning compatibility with result caching. Imported from GitHub PR https://github.com/openxla/xla/pull/22164 The algorithm before this change was:   collect unique noncached fusions   shard them   autotune the shard   publish the shard   read the other shards   merge them into the local cache This did not work when the ranks observed the cache(s) in different states  sharding requires the set of autotuned fusions to be exactly the same. The new algorithm is:   collect unique fusions ignoring the caches   shard them   skip fusions present in the local caches of the rank   autotune the remainder   publish cached + new autotuned results comprising the shard   read the other shards   merge them into the local cache overwriting on conflicts Copybara import of the project:  80e2579255e43fa693c481d3a5775251cffb6335 by Ilia Sergachev : [GPU] Fix sharded autotuning compatibility with result caching. The algorithm before this change was:   collect unique noncached fusions   shard them   autotune the shard   publish the shard   read the other shards   merge them into the local cache This did not work when the ranks observed the cache(s) in different states  sharding requires the set of autotuned fusions to be exactly the same. The new algorithm is:   collect unique fusions ignoring the caches   shard them   skip fusions present in the local caches of the rank   autotune the remainder   publish cached + new autotuned results comprising the shard   read the other shards   merge them into the local cache overwriting on conflicts Merging this change closes CC(Fix broken link in rnn_colorbot) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22164 from openxla:fix_sharded_autotuning_with_caching 80e2579255e43fa693c481d3a5775251cffb6335,2025-02-03T10:01:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86445
opt,copybara-service[bot],Use custom HLO serialization for HloUnoptimizedSnapshot.,Use custom HLO serialization for HloUnoptimizedSnapshot. This change makes it possible to dump HloUnoptimizedSnapshot protos that are over 2GiB in size (the proto binary serialization limit).,2025-02-03T09:21:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86444
yi,copybara-service[bot],[XLA:GPU] move DotDecompose out of simplification pipeline,"[XLA:GPU] move DotDecompose out of simplification pipeline That seems to be a better approach then moving TransposeFold to simplification2 in 961e5c25fbd4082a1ac4f2e0865ad28163d12f7d: 1. There is a report that previous change has resulted in perf degradation https://github.com/openxla/xla/pull/22081 2. I have found another case when DotDecompose is competing with algsimp. Added a test for that. Overall, having an pass that expands operation together with passes that are trying to do the simplification asks for such infinite loops.  For archeologists:   passes DotDimensionSorter and DotDecomposer were added along with GpuAlgebraicSimplifier as it previously could have added multiple contracting dimensions to dot. But cudnn does not support dots with 2+ dimensions, forcing us to use a less efficient loop emitter.  That what ""// AlgebraicSimplifier may add contracting dimensions to a dot."" comment was about. After a while simplifier started to use supports_non_canonical_dots to guard against this case. So it should be safe to remove dot decomposer and friends. Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981",2025-02-03T09:16:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86443
sharding,copybara-service[bot],PR #22164: [GPU] Fix sharded autotuning compatibility with result caching.,PR CC(Fix broken link in rnn_colorbot): [GPU] Fix sharded autotuning compatibility with result caching. Imported from GitHub PR https://github.com/openxla/xla/pull/22164 The algorithm before this change was:   collect unique noncached fusions   shard them   autotune the shard   publish the shard   read the other shards   merge them into the local cache This did not work when the ranks observed the cache(s) in different states  sharding requires the set of autotuned fusions to be exactly the same. The new algorithm is:   collect unique fusions ignoring the caches   shard them   skip fusions present in the local caches of the rank   autotune the remainder   publish cached + new autotuned results comprising the shard   read the other shards   merge them into the local cache overwriting on conflicts Copybara import of the project:  80e2579255e43fa693c481d3a5775251cffb6335 by Ilia Sergachev : [GPU] Fix sharded autotuning compatibility with result caching. The algorithm before this change was:   collect unique noncached fusions   shard them   autotune the shard   publish the shard   read the other shards   merge them into the local cache This did not work when the ranks observed the cache(s) in different states  sharding requires the set of autotuned fusions to be exactly the same. The new algorithm is:   collect unique fusions ignoring the caches   shard them   skip fusions present in the local caches of the rank   autotune the remainder   publish cached + new autotuned results comprising the shard   read the other shards   merge them into the local cache overwriting on conflicts Merging this change closes CC(Fix broken link in rnn_colorbot) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22164 from openxla:fix_sharded_autotuning_with_caching 80e2579255e43fa693c481d3a5775251cffb6335,2025-02-03T08:32:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86441
tpu,copybara-service[bot],Introduce (de)serialisation of `HloUnoptimizedSnapshot`. The snapshot is serialized in the following format:,Introduce (de)serialisation of `HloUnoptimizedSnapshot`. The snapshot is serialized in the following format: * Metadata * Raw arguments The metadata is `HloUnoptimizedSnapshot` with `HloModuleProto` and a descriptor for each argument. The descriptor specifies the size of the argument in bytes. The raw arguments are serialized in the same format as `Literal::Serialize`. I Updated the `HloUnoptimizedSnapshot` proto structure to store the metadata; introduced `CodedStreamInput(/Output)Iterator`  interface for iterators over protobuf CodedStreams. This allows to use already existing `Literal::[De]Serialize` functions.,2025-02-03T06:36:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86437
tpu,usmonali4,inconsistent result of ```tf.raw_ops.LogSoftmax``` on CPU and GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04 (google colab)  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5/9  GPU model and memory Tesla T4  Current behavior? result of ``tf.raw_ops.LogSoftmax``` is inconsistent between CPU and GPU  Standalone code to reproduce the issue ```shell import tensorflow as tf logits = tf.constant([[0.0664, 2.3906]], dtype=tf.bfloat16) with tf.device('CPU:0'):   result_cpu = tf.raw_ops.LogSoftmax(logits=logits)   print(""Output on CPU:"", result_cpu) with tf.device('GPU:0'):   result_gpu = tf.raw_ops.LogSoftmax(logits=logits)   print(""Output on GPU:"", result_gpu) max_abs_diff = tf.reduce_max(tf.abs(result_cpu  result_gpu)).numpy() is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e3,  atol=1e2) print(""Max absolute difference:"", max_abs_diff) print(""Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3:"", is_consistent.numpy()) ```  Relevant log output ```shell Output on CPU: tf.Tensor([[0.0825195 2.53125]], shape=(1, 2), dtype=bfloat16) Output on GPU: tf.Tensor([[0.0825195 2.54688]], shape=(1, 2), dtype=bfloat16) Max absolute difference: 0.015625 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: False ```",2025-02-03T03:30:58Z,type:bug WIP comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/86434,"reproduced on different configuration: ```bash TensorFlow Version: 2.16.2 Python Version: 3.10.16  (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] Python Implementation: CPython CUDA Version (TensorFlow): 12.0 cuDNN Version (TensorFlow): 8 GPU Name (PyTorch): NVIDIA GeForce RTX 4070 SUPER CUDA Version (PyTorch): 12.0 cuDNN Version (PyTorch): 8907 Driver Version: 550.144.03 Ubuntu Version: PRETTY_NAME=""Ubuntu 24.04.1 LTS"" Model name:                           AMD Ryzen 5 7600 6Core Processor Output on CPU: tf.Tensor([[0.0825195 2.53125]], shape=(1, 2), dtype=bfloat16) Output on GPU: tf.Tensor([[0.0825195 2.54688]], shape=(1, 2), dtype=bfloat16) Max absolute difference: 0.015625 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: False ```"
tpu,dicotom,ERROR: An error occurred during the fetch of repository 'local_config_def_file_filter': Auto-Configuration Error: Visual C++ build tools not found on your machine," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution windows 11 pro  Mobile device N/A  Python version 3.12  Bazel version 6.5.0  GCC/compiler version 	CLANG 17.0.6  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? that it would be compiled, I followed the instructions for windows https://www.tensorflow.org/install/source_windows  Standalone code to reproduce the issue ```shell N/A ```  Relevant log output ```shell DEBUG: C:/users/user/_bazel_user/yopxtjri/external/local_xla/third_party/py/python_repo.bzl:107:10: Using hermetic Python 3.12 INFO: Repository local_config_def_file_filter instantiated at:   C:/users/user/tensorflow/WORKSPACE:64:14: in    C:/users/user/tensorflow/tensorflow/workspace2.bzl:936:19: in workspace   C:/users/user/tensorflow/tensorflow/workspace2.bzl:117:30: in _tf_toolchains Repository rule def_file_filter_configure defined at:   C:/users/user/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl:54:44: in  ERROR: An error occurred during the fetch of repository 'local_config_def_file_filter':    Traceback (most recent call last):         File ""C:/users/user/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl"", line 32, column 28, in _def_file_filter_configure_impl                 auto_configure_fail(""Visual C++ build tools not found on your machine"")         File ""C:/users/user/_bazel_user/yopxtjri/external/bazel_tools/tools/cpp/lib_cc_configure.bzl"", line 112, column 9, in auto_configure_fail                 fail(""\n%sAutoConfiguration Error:%s %s\n"" % (red, no_color, msg)) Error in fail: AutoConfiguration Error: Visual C++ build tools not found on your machine ERROR: C:/users/user/tensorflow/WORKSPACE:64:14: fetching def_file_filter_configure rule //external:local_config_def_file_filter: Traceback (most recent call last):         File ""C:/users/user/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl"", line 32, column 28, in _def_file_filter_configure_impl                 auto_configure_fail(""Visual C++ build tools not found on your machine"")         File ""C:/users/user/_bazel_user/yopxtjri/external/bazel_tools/tools/cpp/lib_cc_configure.bzl"", line 112, column 9, in auto_configure_fail                 fail(""\n%sAutoConfiguration Error:%s %s\n"" % (red, no_color, msg)) Error in fail: AutoConfiguration Error: Visual C++ build tools not found on your machine ERROR: C:/users/user/tensorflow/tensorflow/python/BUILD:978:8: //tensorflow/python:pywrap_tensorflow_filtered_def_file depends on //:def_file_filter in repository  which failed to fetch. no such package '//': AutoConfiguration Error: Visual C++ build tools not found on your machine ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted: INFO: Elapsed time: 584.412s INFO: 0 processes. FAILED: Build did NOT complete successfully (453 packages loaded, 9221 targets configured)     currently loading: // ... (6 packages)     Fetching repository ; starting 6s     Fetching ...r/yopxtjri/external/eigen_archive; Extracting eigen33d0937c6bdf5ec999939fb17f2a553183d14a74.tar.gz 6s     Fetching repository ; starting ```",2025-02-02T19:31:16Z,type:build/install,closed,0,2,https://github.com/tensorflow/tensorflow/issues/86410,Are you satisfied with the resolution of your issue? Yes No,my fault.
tpu,Cloudberrydotdev,User Guide: Deprecated Nvidia Docker Link," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version any  Custom code No  OS platform and distribution Linux GPU  Mobile device _No response_  Python version any  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The link to the Nvidia Docker github https://github.com/NVIDIA/nvidiadocker?tab=readmeovfile Reports: This repository has been archived by the owner on Jan 22, 2024. It is now readonly.  and provides a link to: https://github.com/NVIDIA/nvidiacontainertoolkit Titled: Build and run containers leveraging NVIDIA GPUs   Standalone code to reproduce the issue ```shell This is a documentation bug and no code errors are involved. ```  Relevant log output ```shell See above ```",2025-02-02T09:16:37Z,type:docs-bug stat:awaiting response type:build/install stale,closed,0,9,https://github.com/tensorflow/tensorflow/issues/86407,https://www.tensorflow.org/install/docker Sorry I meant to include this url for the documentation page that holds the error link.,The relevant text is: Docker is the easiest way to enable TensorFlow GPU support on Linux since only the NVIDIA® GPU driver is required on the host machine (the NVIDIA® CUDA® Toolkit does not need to be installed). The link is in the text: NVIDIA® GPU driver,  您好，邮件已经收到，我会尽快处理的。谢谢,"Sam, Thanks for picking this up. I'm not sure what happens now. Do I wait for a reply from the Tensorflow documentation team before closing this issue? Regards Ian Berry On Sun, 2 Feb 2025, 16:35 Sam Fletcher, ***@***.***> wrote: > The issue is that the TensorFlow documentation still links to the > nowarchived NVIDIA Docker repository. Instead, it should link to the new, > active repository: > > Solution: > >    1. The TensorFlow documentation team needs to update the incorrect >    link. >    2. The old link: > >    https://github.com/NVIDIA/nvidiadocker?tab=readmeovfile > >    Should be replaced with: > >    https://github.com/NVIDIA/nvidiacontainertoolkit > >    3. If you're relying on the old NVIDIA Docker setup, transition to >    using the *NVIDIA Container Toolkit* as per the new repository. You >    can follow their official setup guide: > > For now, you can manually install TensorFlow with GPU support using the > NVIDIA Container Toolkit instead of the deprecated NVIDIA Docker. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",", Thank you for reporting the issue. I have raised the internal request for the mentioned change and will be updated once it gets submitted. Thank you!",", The raised request was submitted and also the changes are reflected in the official document. https://www.tensorflow.org/install/dockertensorflow_docker_requirements reflecting to https://github.com/NVIDIA/nvidiacontainertoolkit !Image Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,usmonali4,inconsistent result of ```tf.raw_ops.L2Loss``` on CPU and GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04 (google colab)  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5/9  GPU model and memory Tesla T4  Current behavior? result of ```tw.raw_ops.L2Loss``` is inconsistent between CPU and GPU  Standalone code to reproduce the issue ```shell import tensorflow as tf t = tf.constant([     [[0.9922, 1.4922],       [0.0376,  0.1504],       [0.6172,  1.2266]],     [[0.1387,  1.3047],       [0.3535, 0.0471],       [0.0437,  0.2637]] ], dtype=tf.bfloat16) with tf.device('CPU:0'):   result_cpu = tf.raw_ops.L2Loss(t=t)   print(""Output on CPU:"", result_cpu) with tf.device('GPU:0'):   result_gpu = tf.raw_ops.L2Loss(t=t)   print(""Output on GPU:"", result_gpu) max_abs_diff = tf.reduce_max(tf.abs(result_cpu  result_gpu)).numpy() is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e3,  atol=1e2) print(""Max absolute difference:"", max_abs_diff) print(""Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3:"", is_consistent.numpy()) ```  Relevant log output ```shell Output on CPU: tf.Tensor(3.51562, shape=(), dtype=bfloat16) Output on GPU: tf.Tensor(3.53125, shape=(), dtype=bfloat16) Max absolute difference: 0.015625 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: False ```",2025-02-02T07:05:17Z,type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/86406,I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TFnightly on both CPU and GPU. Please find gist1 and gist2 here for your reference. Thank you!,"no issue here: ```bash TensorFlow Version: 2.16.2 Python Version: 3.10.16  (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] Python Implementation: CPython CUDA Version (TensorFlow): 12.0 cuDNN Version (TensorFlow): 8 GPU Name (PyTorch): NVIDIA GeForce RTX 4070 SUPER CUDA Version (PyTorch): 12.0 cuDNN Version (PyTorch): 8907 Driver Version: 550.144.03 Ubuntu Version: PRETTY_NAME=""Ubuntu 24.04.1 LTS"" Model name:                           AMD Ryzen 5 7600 6Core Processor Output on CPU: tf.Tensor(3.53125, shape=(), dtype=bfloat16) Output on GPU: tf.Tensor(3.53125, shape=(), dtype=bfloat16) Max absolute difference: 0 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: True ```"
tpu,regularRandom,TF wheel shouldn't be built with CUDA dependencies," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution CentOS Stream 9  Mobile device _No response_  Python version 3.9  Bazel version 6.5.0  GCC/compiler version 11.5.0  CUDA/cuDNN version 12.6.1/8.9.7.29  GPU model and memory 2080 Ti 11GB  Current behavior? Build fails with following error: Error in fail: TF wheel shouldn't be built with CUDA dependencies. Please provide `config=cuda_wheel` for bazel build command. If you absolutely need to add CUDA dependencies, provide `//cuda:override_include_cuda_libs=true`. Current driver: ` NVIDIASMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8 ` Build command: ` bazel build config=cuda local_cpu_resources=HOST_CPUS*.8 //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow ` What I missed?  Standalone code to reproduce the issue ```shell You have bazel 6.5.0 installed. Please specify the location of python. [Default is /usr/bin/python3]: Found possible Python library paths:   /usr/lib/python3.9/sitepackages   /usr/lib64/python3.9/sitepackages   /usr/local/lib/python3.9/sitepackages   /usr/local/lib64/python3.9/sitepackages Please input the desired Python library path to use.  Default is [/usr/lib/python3.9/sitepackages] Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: y CUDA support will be enabled for TensorFlow. Please specify the hermetic CUDA version you want to use or leave empty to use the default version. 12.6.1 Please specify the hermetic cuDNN version you want to use or leave empty to use the default version. Please specify a list of commaseparated CUDA compute capabilities you want to build with. You can find the compute capability of your device at: https://developer.nvidia.com/cudagpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code. Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.5 Please specify the local CUDA path you want to use or leave empty to use the default version. Please specify the local CUDNN path you want to use or leave empty to use the default version. Please specify the local NCCL path you want to use or leave empty to use the default version. Do you want to use clang as CUDA compiler? [Y/n]: n nvcc will be used as CUDA compiler. Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/lib64/ccache/gcc]: Please specify optimization flags to use during compilation when bazel option ""config=opt"" is specified [Default is Wnosigncompare]: Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""config="" to your build command. See .bazelrc for more details.         config=mkl             Build with MKL support.         config=mkl_aarch64     Build with oneDNN and Compute Library for the Arm Architecture (ACL).         config=monolithic      Config for mostly static monolithic build.         config=numa            Build with NUMA support.         config=dynamic_kernels         (Experimental) Build kernels into separate shared objects.         config=v1              Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:         config=nogcp           Disable GCP support.         config=nonccl          Disable NVIDIA NCCL support. Configuration finished ```  Relevant log output ```shell ERROR: /usr/src/tensorflow/tensorflow/tools/pip_package/BUILD:278:9: in tf_wheel rule //tensorflow/tools/pip_package:wheel: Traceback (most recent call last):         File ""/usr/src/tensorflow/tensorflow/tools/pip_package/utils/tf_wheel.bzl"", line 72, column 13, in _tf_wheel_impl                 fail(""TF wheel shouldn't be built with CUDA dependencies."" + Error in fail: TF wheel shouldn't be built with CUDA dependencies. Please provide `config=cuda_wheel` for bazel build command. If you absolutely need to add CUDA dependencies, provide `//cuda:override_include_cuda_libs=true`. ERROR: /usr/src/tensorflow/tensorflow/tools/pip_package/BUILD:278:9: Analysis of target '//tensorflow/tools/pip_package:wheel' failed ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted: INFO: Elapsed time: 189.679s INFO: 0 processes. FAILED: Build did NOT complete successfully (790 packages loaded, 56723 targets configured) ```",2025-02-02T04:59:19Z,type:build/install subtype: ubuntu/linux TF 2.18,open,0,4,https://github.com/tensorflow/tensorflow/issues/86405,", Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.In this case, Tensorflow v2.18 is compatible with python 3.93.12, compiler(Clang) 17.0.6, Bazel  6.5.0,  cudNN9.3, CUDA12.5 https://www.tensorflow.org/install/sourcegpu Also To build tensorflow GPU package: `bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow config=cuda config=cuda_wheel` https://www.tensorflow.org/install/sourcebuild_the_package Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.," where can I find a list of exact requirements for 2.18 & 2.19? Currently I have CUDA 12.6, libcudnn88.9.7.29, Bazel 6.5.0, clang19.1.7, ccache 4.5.1. Downgraded to CUDA 12.5  it fails with the following message: `ERROR: /root/.cache/bazel/_bazel_md/cbe8b06b94787a6b39e59564d90f2497/external/boringssl/BUILD:133:11: Compiling winaarch64/crypto/test/trampolinearmv8win.S failed: (Exit 1): clang failed: error executing command (from target //:crypto) /usr/lib64/ccache/clang MD MF bazelout/k8opt/bin/external/boringssl/_objs/crypto/trampolinearmv8win.pic.d ... (remaining 59 arguments skipped) clang: error: argument unused during compilation: 'cudapath=external/cuda_nvcc' [Werror,Wunusedcommandlineargument] Target //tensorflow/tools/pip_package:wheel failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 2.910s, Critical Path: 0.11s INFO: 41 processes: 41 internal. FAILED: Build did NOT complete successfully ` What I missed?","I don't understand what is going on. Each try ends with the new error message. I removed bazel & ccache caches (rm rf) & tensorflow sources. Then I recloned it from github and now I am getting following error in origin/r2.18 branch: ` ERROR: /root/.cache/bazel/_bazel_md/cbe8b06b94787a6b39e59564d90f2497/external/local_xla/xla/service/gpu/BUILD:2261:13: Compiling xla/service/gpu/stream_executor_util_kernel.cu.: (Exit 1): clang failed: error executing command (from target //xla/service/gpu:stream_executor_util_kernel) /usr/lib64/ccache/clang MD MF bazelout/k8opt/bin/external/local_xla/xla/service/gpu/_objs/stream_executor_util_kernel/stream_executor_util_kernel.cu.pic.d ... (remaining 168 arguments skipped) clang: error: unknown argument: 'Xcudafatbinary=compressall' clang: error: unknown argument: 'nvcc_options=exptrelaxedconstexpr' clang: warning: CUDA version is newer than the latest partially supported version 12.5 [Wunknowncudaversion] clang: error: GPU arch sm_35 is supported by CUDA versions between 7.0 and 11.8 (inclusive), but installation at external/cuda_nvcc is ; use 'cudapath' to specify a different CUDA install, pass a different GPU arch with 'cudagpuarch', or pass 'nocudaversioncheck' Target //tensorflow/tools/pip_package:wheel failed to build Use verbose_failures to see the command lines of failed build steps. ERROR: /usr/src/tensorflow/tensorflow/tools/pip_package/BUILD:266:9 Action tensorflow/tools/pip_package/wheel_house failed: (Exit 1): clang failed: error executing command (from target //xla/service/gpu:stream_executor_util_kernel) /usr/lib64/ccache/clang MD MF bazelout/k8opt/bin/external/local_xla/xla/service/gpu/_objs/stream_executor_util_kernel/stream_executor_util_kernel.cu.pic.d ... (remaining 168 arguments skipped) ` I just removed CUDA 12.6 and installed 12.5  what is wrong with that? `cudatoolkit12512.5.11.x86_64`"
int8,copybara-service[bot],"Remove unused/rarely used functions from HloOpCode. Parametarize test for HloOpCode. Change `HloOpcodeArity` return type to `int8_t`, since `kHloOpcodeMaxArity` is 31.","Remove unused/rarely used functions from HloOpCode. Parametarize test for HloOpCode. Change `HloOpcodeArity` return type to `int8_t`, since `kHloOpcodeMaxArity` is 31.",2025-02-02T01:11:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86383
tpu,usmonali4,inconsistent result of ```tf.raw_ops.BiasAddGrad``` on CPU and GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5/9  GPU model and memory Tesla T4  Current behavior? ```tf.raw_ops.BiasAddGrad``` produces inconsistent results between CPU and GPU  Standalone code to reproduce the issue ```shell import tensorflow as tf out_backprop = tf.constant([     [         [             [[ 0.2207,  2.1094], [0.3730, 1.0625], [ 1.7031,  0.7148]],              [[ 1.5078, 0.6719], [0.6367,  0.5039], [2.3281,  0.5078]]         ],         [             [[0.3574,  0.0461], [ 2.3750, 2.9688], [0.5703, 2.0156]],             [[ 0.8125,  1.7656], [0.9570,  0.6250], [0.6914, 0.4746]]         ],         [             [[0.3750, 0.7383], [ 0.3691,  0.4570], [ 1.1641,  0.2715]],             [[1.2969, 0.9844], [0.4863,  1.0938], [1.4297,  0.8086]]         ]     ],     [         [             [[ 0.3730,  0.8477], [0.3887,  1.2266], [ 0.0859, 0.5742]],             [[0.7383, 0.2432], [0.7578, 0.8281], [0.1660, 0.9336]]         ],         [             [[ 1.4297,  0.6797], [1.6172,  0.4941], [0.3047, 0.3711]],             [[0.6250, 0.7617], [ 0.9453,  0.1064], [ 1.4062, 2.9531]]         ],         [             [[1.4297, 0.1387], [ 0.0625,  1.0469], [0.1953,  1.6406]],             [[0.3047,  0.5117], [ 1.8125,  1.1797], [0.8789, 0.4688]]         ]     ] ], dtype=tf.bfloat16) with tf.device('CPU:0'):   result_cpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=""NCHW"")   print(""BiasAddGrad Output on CPU:"", result_cpu) with tf.device('GPU:0'):   result_gpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=""NCHW"")   print(""BiasAddGrad Output on GPU:"", result_gpu) max_abs_diff = tf.reduce_max(tf.abs(result_cpu  result_gpu)).numpy() is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e3,  atol=1e2) print(""Max absolute difference:"", max_abs_diff) print(""Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3:"", is_consistent.numpy()) ```  Relevant log output ```shell BiasAddGrad Output on CPU: tf.Tensor([0.09375 3.96875 1.70312], shape=(3,), dtype=bfloat16) BiasAddGrad Output on GPU: tf.Tensor([0.078125 4 1.70312], shape=(3,), dtype=bfloat16) Max absolute difference: 0.03125 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: False ```",2025-02-01T10:43:06Z,stat:awaiting tensorflower type:bug comp:ops TF 2.18,open,0,5,https://github.com/tensorflow/tensorflow/issues/86378,", I think the issue is specific to GPU, I was able to reproduce the issue on colab with GPU runtime. But, when the inputs are changed to float64 precision, the results are as expected. The reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors. The same behavior is not observed on Apple M1, using numpy or in CPU. https://github.com/tensorflow/tensorflow/issues/58479 Thank you!",", similar reasoning as in this comment  CC(inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU)/comment, but the acceptable tolerance thresholds have been set to atol=1e2 and rtol=1e3 considering that the dtypes of results are bfloat16 (16‑bit floating‐point types). Glad to hear your thoughts on this; thanks for your time.",", Looks like the other issues which were created are similar to CC(inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU) . Could you please close the other issues which were raised which helps to track the issue in a better way. Thank you!",", if you take for example this report and CC(inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU), I believe that the core issue is different since they are targeting different api's with different types of tensors","Reproduced here: ```bash TensorFlow Version: 2.16.2 Python Version: 3.10.16  (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] Python Implementation: CPython CUDA Version (TensorFlow): 12.0 cuDNN Version (TensorFlow): 8 GPU Name (PyTorch): NVIDIA GeForce RTX 4070 SUPER CUDA Version (PyTorch): 12.0 cuDNN Version (PyTorch): 8907 Driver Version: 550.144.03 Ubuntu Version: PRETTY_NAME=""Ubuntu 24.04.1 LTS"" Model name:                           AMD Ryzen 5 7600 6Core Processor BiasAddGrad Output on CPU: tf.Tensor([0.0625 3.96875 1.70312], shape=(3,), dtype=bfloat16) BiasAddGrad Output on GPU: tf.Tensor([0.078125 4 1.70312], shape=(3,), dtype=bfloat16) Max absolute difference: 0.03125 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: False ```"
tpu,dependabot[bot],Bump the github-actions group with 8 updates,"Bumps the githubactions group with 8 updates:  Updates `google/osvscanneraction` from 1.9.0 to 1.9.2  Release notes Sourced from google/osvscanneraction's releases.  v1.9.2 What's Changed  Update to v1.9.2 by @​hogo6002 in google/osvscanneraction CC(Can't install on ubuntu 12.04.5 LTS)  Full Changelog: https://github.com/google/osvscanneraction/compare/v1.9.1...v1.9.2 v1.9.1 What's Changed  Update to use osvscanner v1.9.1 chore(deps): update workflows by @​renovatebot in google/osvscanneraction CC(Integration with blaze ecosystem numba python to llvm compiler?) Update to v1.9.1 by @​anotherrex in google/osvscanneraction CC(error __init__() got an unexpected keyword argument 'syntax') chore(deps): update workflows by @​renovatebot in google/osvscanneraction CC(Object Detection)  Full Changelog: https://github.com/google/osvscanneraction/compare/v1.9.0...v1.9.1    Commits  764c918 Merge pull request  CC(Can't install on ubuntu 12.04.5 LTS) from google/updatetov1.9.2 af3118a Update unified workflow example to point to v1.9.2 reusable workflows e994fd8 Update reusable workflows to point to v1.9.2 actions f8115f2 Update actions to use v1.9.2 osvscanner image daa2c68 Merge pull request  CC(Object Detection) from renovatebot/renovate/workflows af00d40 chore(deps): update workflows c411404 Merge pull request  CC(error __init__() got an unexpected keyword argument 'syntax') from google/updatetov1.9.1 1ab2a61 Update unified workflow example to point to v1.9.1 reusable workflows 8bd1ce1 Update reusable workflows to point to v1.9.1 actions cbb0295 Update actions to use v1.9.1 osvscanner image Additional commits viewable in compare view    Updates `actions/setuppython` from 5.3.0 to 5.4.0  Release notes Sourced from actions/setuppython's releases.  v5.4.0 What's Changed Enhancements:  Update cache error message by @​aparnajyothiy in actions/setuppython CC(tensorflow.bzl.bzl doesn't exist, unsurprisingly) Enhance Workflows: Add Ubuntu24, Remove Python 3.8  by @​priyakinthali in actions/setuppython CC(Wrong file dimension when calling maybe_download function) Configure Dependabot settings by @​HarithaVattikuti in actions/setuppython CC(Minor comment spelling fix.)  Documentation changes:  Readme update  recommended permissions by @​benwells in actions/setuppython CC(Example code for SummaryIterator using `tf.train.summary_iterator`.) Improve Advanced Usage examples by @​lrq3000 in actions/setuppython CC(Casting int64 to int8 is not supported)  Dependency updates:  Upgrade undici from 5.28.4 to 5.28.5 by @​dependabot in actions/setuppython CC(pip installation failing on fedora) Upgrade urllib3 from 1.25.9 to 1.26.19 in /tests/data by @​dependabot in actions/setuppython CC(Udacity example 1: import urllib for download) Upgrade actions/publishimmutableaction from 0.0.3 to 0.0.4 by @​dependabot in actions/setuppython CC(Fix merge gone wrong) Upgrade /httpclient from 2.2.1 to 2.2.3 by @​dependabot in actions/setuppython CC(Fixed tensorboard installation for pip) Upgrade requests from 2.24.0 to 2.32.2 in /tests/data by @​dependabot in actions/setuppython CC(Tensorflow Android not building for nonarm architectures.) Upgrade /cache to ^4.0.0 by @​priyagupta108 in actions/setuppython CC(TensorFlow: fix word2vec timeouts on GPU by pinning model on CPU)  New Contributors  @​benwells made their first contribution in actions/setuppython CC(Example code for SummaryIterator using `tf.train.summary_iterator`.) @​HarithaVattikuti made their first contribution in actions/setuppython CC(Minor comment spelling fix.) @​lrq3000 made their first contribution in actions/setuppython CC(Casting int64 to int8 is not supported)  Full Changelog: https://github.com/actions/setuppython/compare/v5...v5.4.0    Commits  4237552 Improve Advanced Usage examples ( CC(Casting int64 to int8 is not supported)) 709bfa5 Bump requests from 2.24.0 to 2.32.2 in /tests/data ( CC(Tensorflow Android not building for nonarm architectures.)) ceb20b2 Bump @​actions/httpclient from 2.2.1 to 2.2.3 ( CC(Fixed tensorboard installation for pip)) 0dc2d2c Bump actions/publishimmutableaction from 0.0.3 to 0.0.4 ( CC(Fix merge gone wrong)) feb9c6e Bump urllib3 from 1.25.9 to 1.26.19 in /tests/data ( CC(Udacity example 1: import urllib for download)) d0b4fc4 Bump undici from 5.28.4 to 5.28.5 ( CC(pip installation failing on fedora)) e3dfaac Configure Dependabot settings ( CC(Minor comment spelling fix.)) b8cf3eb Use the new cache service: upgrade /cache to ^4.0.0 ( CC(TensorFlow: fix word2vec timeouts on GPU by pinning model on CPU)) 1928ae6 Update README.md ( CC(Example code for SummaryIterator using `tf.train.summary_iterator`.)) 3fddbee Enhance Workflows: Add Ubuntu24, Remove Python 3.8  ( CC(Wrong file dimension when calling maybe_download function)) Additional commits viewable in compare view    Updates `peterevans/createpullrequest` from 7.0.5 to 7.0.6  Release notes Sourced from peterevans/createpullrequest's releases.  Create Pull Request v7.0.6 ⚙️ Fixes an issue with commit signing where unicode characters in file paths were not preserved. What's Changed  build(depsdev): bump @​vercel/ncc from 0.38.1 to 0.38.2 by @​dependabot in peterevans/createpullrequest CC(Remove read_analogies() from word2vec class initialization) Update distribution by @​actionsbot in peterevans/createpullrequest CC(Tests for examples/learn) build(deps): bump @​octokit/pluginrestendpointmethods from 13.2.4 to 13.2.5 by @​dependabot in peterevans/createpullrequest CC(wrong use of sequence_loss_by_example in ptb_word_lm.py?) build(depsdev): bump @​types/node from 18.19.50 to 18.19.54 by @​dependabot in peterevans/createpullrequest CC(Elementwise tf.cond (like theano switch)) build(deps): bump @​octokit/pluginpaginaterest from 11.3.3 to 11.3.5 by @​dependabot in peterevans/createpullrequest CC(Moving data from CPU to GPU is slow ) Update distribution by @​actionsbot in peterevans/createpullrequest CC(Bug: Exception ignored in BaseSession.__del__) build(depsdev): bump @​types/node from 18.19.54 to 18.19.55 by @​dependabot in peterevans/createpullrequest CC(new stacks) build(deps): bump @​actions/core from 1.10.1 to 1.11.1 by @​dependabot in peterevans/createpullrequest CC(Run a TensorFlow demo model : IOError: CRC check failed) build(deps): bump @​octokit/pluginrestendpointmethods from 13.2.5 to 13.2.6 by @​dependabot in peterevans/createpullrequest CC(A link in tensorflow's website is ineffective) build(depsdev): bump eslintpluginimport from 2.30.0 to 2.31.0 by @​dependabot in peterevans/createpullrequest CC(Given one input tensor, why tf.nn.max_pool generate two tensors?) build(deps): bump @​octokit/pluginthrottling from 9.3.1 to 9.3.2 by @​dependabot in peterevans/createpullrequest CC(Can't create placeholder with partially defined shape e.g. (1, 10)) Update distribution by @​actionsbot in peterevans/createpullrequest CC(Fix few warnings that I see on macosxclang compilation) build(depsdev): bump typescript from 5.6.2 to 5.6.3 by @​dependabot in peterevans/createpullrequest CC(Documentation for 'Adding a new op') build(deps): bump undici from 6.19.8 to 6.20.1 by @​dependabot in peterevans/createpullrequest CC(Error malloc(): memory corruption) Update distribution by @​actionsbot in peterevans/createpullrequest CC(retrain.py validation and testing evaluation seems incorrect) build(depsdev): bump @​types/node from 18.19.55 to 18.19.58 by @​dependabot in peterevans/createpullrequest CC(Problem in distributed tensorflow demo?) build(depsdev): bump @​types/jest from 29.5.13 to 29.5.14 by @​dependabot in peterevans/createpullrequest CC(Correction in Dequantize comments) build(depsdev): bump @​types/node from 18.19.58 to 18.19.60 by @​dependabot in peterevans/createpullrequest CC(Dev request  LSTM RNN) chore: don't bundle undici by @​benmccann in peterevans/createpullrequest CC(Device placement error while using multi gpus on single machine by distributed version) Update distribution by @​actionsbot in peterevans/createpullrequest CC(How to restore a distributed model and continue to train it with more workers?) chore: use nodefetchnative support for proxy env vars by @​peterevans in peterevans/createpullrequest CC(Slicing error: Using a `tf.Tensor` as a Python `bool` is not allowed) build(depsdev): bump @​types/node from 18.19.60 to 18.19.64 by @​dependabot in peterevans/createpullrequest CC(minor typo fix) build(depsdev): bump undici from 6.20.1 to 6.21.0 by @​dependabot in peterevans/createpullrequest CC(Placeholder names are inconsistent when reentering a scope) build(depsdev): bump @​vercel/ncc from 0.38.2 to 0.38.3 by @​dependabot in peterevans/createpullrequest CC(Replaced _logger.warn with _logger.warning) docs: note pushtorepo classic PAT workflow scope requirement by @​scop in peterevans/createpullrequest CC(Branch 128485842) docs: spelling fixes by @​scop in peterevans/createpullrequest CC(Branch 128492931) build(depsdev): bump typescript from 5.6.3 to 5.7.2 by @​dependabot in peterevans/createpullrequest CC(Fix an error message.) build(depsdev): bump prettier from 3.3.3 to 3.4.0 by @​dependabot in peterevans/createpullrequest CC(Error when running distributed MNIST example) build(depsdev): bump @​types/node from 18.19.64 to 18.19.66 by @​dependabot in peterevans/createpullrequest CC(Update version string to 0.10.0rc0) docs(README): clarify that an existing open PR is managed by @​caugner in peterevans/createpullrequest CC('tensorflow.contrib.learn' has no attribute 'infer_real_valued_columns_from_input') Update distribution by @​actionsbot in peterevans/createpullrequest CC(Fixed typo:) build(deps): bump @​octokit/pluginpaginaterest from 11.3.5 to 11.3.6 by @​dependabot in peterevans/createpullrequest CC(Fix go build errors) build(depsdev): bump @​types/node from 18.19.66 to 18.19.67 by @​dependabot in peterevans/createpullrequest CC(boolean_mask failed in iOS: Running model failed:Invalid argument: No OpKernel was registered to support Op 'Gather' with these attrs) build(depsdev): bump prettier from 3.4.0 to 3.4.1 by @​dependabot in peterevans/createpullrequest CC(Should *.pb.h files be generated in crosscompile cases?) build(depsdev): bump eslintimportresolvertypescript from 3.6.3 to 3.7.0 by @​dependabot in peterevans/createpullrequest CC(Can I add a py_func to a queue?) build(depsdev): bump prettier from 3.4.1 to 3.4.2 by @​dependabot in peterevans/createpullrequest CC(Inception retraining / transfer learning fails when running with GPU) build(depsdev): bump @​types/node from 18.19.67 to 18.19.68 by @​dependabot in peterevans/createpullrequest CC(word2vec_basic.py : to prevent possible issue with libpng) build(deps): bump plimit from 6.1.0 to 6.2.0 by @​dependabot in peterevans/createpullrequest CC(Branch 128859117) Update distribution by @​actionsbot in peterevans/createpullrequest CC(Dying Threads?) fix: preserve unicode in filepaths when commit signing by @​peterevans in peterevans/createpullrequest CC(Branch 128894163)  New Contributors  @​benmccann made their first contribution in peterevans/createpullrequest CC(Device placement error while using multi gpus on single machine by distributed version) @​scop made their first contribution in peterevans/createpullrequest CC(Branch 128485842) @​caugner made their first contribution in peterevans/createpullrequest CC('tensorflow.contrib.learn' has no attribute 'infer_real_valued_columns_from_input')    ... (truncated)   Commits  67ccf78 fix: preserve unicode in filepaths when commit signing ( CC(Branch 128894163)) bb88e27 build: update distribution ( CC(Dying Threads?)) b378ed5 build(deps): bump plimit from 6.1.0 to 6.2.0 ( CC(Branch 128859117)) fa9200e build(depsdev): bump @​types/node from 18.19.67 to 18.19.68 ( CC(word2vec_basic.py : to prevent possible issue with libpng)) 16e0059 build(depsdev): bump prettier from 3.4.1 to 3.4.2 ( CC(Inception retraining / transfer learning fails when running with GPU)) 5bffd5a build(depsdev): bump eslintimportresolvertypescript ( CC(Can I add a py_func to a queue?)) a22a0dd build(depsdev): bump prettier from 3.4.0 to 3.4.1 ( CC(Should *.pb.h files be generated in crosscompile cases?)) b27ce37 build(depsdev): bump @​types/node from 18.19.66 to 18.19.67 ( CC(boolean_mask failed in iOS: Running model failed:Invalid argument: No OpKernel was registered to support Op 'Gather' with these attrs)) 4e0cc19 build(deps): bump @​octokit/pluginpaginaterest from 11.3.5 to 11.3.6 ( CC(Fix go build errors)) 25b6871 docs: update scopes for pushtofork Additional commits viewable in compare view    Updates `actions/uploadartifact` from 4.4.3 to 4.6.0  Release notes Sourced from actions/uploadartifact's releases.  v4.6.0 What's Changed  Expose env vars to control concurrency and timeout by @​yacaovsnc in actions/uploadartifact CC(optimize slice_input_producer)  Full Changelog: https://github.com/actions/uploadartifact/compare/v4...v4.6.0 v4.5.0 What's Changed  fix: deprecated Node.js version in action by @​hamirmahal in actions/uploadartifact CC(Fixed random_contrast link in the Deep CNN tutorial) Add new artifactdigest output by @​bdehamer in actions/uploadartifact CC(Tiny fix to API docs)  New Contributors  @​hamirmahal made their first contribution in actions/uploadartifact CC(Fixed random_contrast link in the Deep CNN tutorial) @​bdehamer made their first contribution in actions/uploadartifact CC(Tiny fix to API docs)  Full Changelog: https://github.com/actions/uploadartifact/compare/v4.4.3...v4.5.0    Commits  65c4c4a Merge pull request  CC(optimize slice_input_producer) from actions/yacaovsnc/add_variable_for_concurrency_a... 0207619 move files back to satisfy licensed ci 1ecca81 licensed cache updates 9742269 Expose env vars to controll concurrency and timeout 6f51ac0 Merge pull request  CC(Tiny fix to API docs) from bdehamer/bdehamer/artifactdigest c40c16d add new artifactdigest output 735efb4 bump @​actions/artifact from 2.1.11 to 2.2.0 184d73b Merge pull request  CC(Fixed random_contrast link in the Deep CNN tutorial) from hamirmahal/fix/deprecatednodejsusageinaction b4a0a98 Merge branch 'main' into fix/deprecatednodejsusageinaction See full diff in compare view    Updates `github/codeqlaction` from 3.27.5 to 3.28.8  Release notes Sourced from github/codeqlaction's releases.  v3.28.8 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.8  29 Jan 2025  Enable support for Kotlin 2.1.10 when running with CodeQL CLI v2.20.3.  CC(Fix for build issue 2742;)  See the full CHANGELOG.md for more information. v3.28.7 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.7  29 Jan 2025 No user facing changes. See the full CHANGELOG.md for more information. v3.28.6 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.6  27 Jan 2025  Reenable debug artifact upload for CLI versions 2.20.3 or greater.  CC(Modifying MNIST example to distributed version: could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR)  See the full CHANGELOG.md for more information. v3.28.5 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.5  24 Jan 2025  Update default CodeQL bundle version to 2.20.3.  CC(Branch 124290852)  See the full CHANGELOG.md for more information. v3.28.4 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.4  23 Jan 2025   ... (truncated)   Changelog Sourced from github/codeqlaction's changelog.  CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. [UNRELEASED] No user facing changes. 3.28.8  29 Jan 2025  Enable support for Kotlin 2.1.10 when running with CodeQL CLI v2.20.3.  CC(Fix for build issue 2742;)  3.28.7  29 Jan 2025 No user facing changes. 3.28.6  27 Jan 2025  Reenable debug artifact upload for CLI versions 2.20.3 or greater.  CC(Modifying MNIST example to distributed version: could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR)  3.28.5  24 Jan 2025  Update default CodeQL bundle version to 2.20.3.  CC(Branch 124290852)  3.28.4  23 Jan 2025 No user facing changes. 3.28.3  22 Jan 2025  Update default CodeQL bundle version to 2.20.2.  CC(Update roadmap.md) Fix an issue downloading the CodeQL Bundle from a GitHub Enterprise Server instance which occurred when the CodeQL Bundle had been synced to the instance using the CodeQL Action sync tool and the Actions runner did not have Zstandard installed.  CC(Branch 124251558) Uploading debug artifacts for CodeQL analysis is temporarily disabled.  CC(Tensorflow with Pyinstaller)  3.28.2  21 Jan 2025 No user facing changes. 3.28.1  10 Jan 2025  CodeQL Action v2 is now deprecated, and is no longer updated or supported. For better performance, improved security, and new features, upgrade to v3. For more information, see this changelog post.  CC(Import error) Update default CodeQL bundle version to 2.20.1.  CC(Fix broken link to Anaconda installation)  3.28.0  20 Dec 2024  Bump the minimum CodeQL bundle version to 2.15.5.  CC(Feature Request: Support for YARN cluster manager for Distributed TensorFlow) Don't fail in the unusual case that a file is on the search path.  CC(Max Pooling NCHW).  3.27.9  12 Dec 2024   ... (truncated)   Commits  dd74661 Merge pull request  CC(Missing pywrap_tensorflow) from github/updatev3.28.8a91a3f767 3210a3c Fix Kotlin version in changelog 72f9d02 Update changelog for v3.28.8 a91a3f7 Merge pull request  CC(Fix for build issue 2742;) from github/igfoo/kot2.1.10 c520fb5 Merge pull request  CC(running digits.py) from github/mergeback/v3.28.7tomain6e545590 3879c57 Add changelog entry 0c21937 Run &quot;npm run build&quot; 5a61bf0 Kotlin: The 2.20.3 release supports Kotlin 2.1.10. 163d119 Update checkedin dependencies bcf5cec Update changelog and version after v3.28.7 Additional commits viewable in compare view    Updates `docker/setupbuildxaction` from 3.7.1 to 3.8.0  Release notes Sourced from docker/setupbuildxaction's releases.  v3.8.0  Make cloud prefix optional to download buildx if driver is cloud by @​crazymax in docker/setupbuildxaction CC(parse_example can be _much_ faster than parse_single_example) Bump @​actions/core from 1.10.1 to 1.11.1 in docker/setupbuildxaction CC(convert_to_records.py don't write all values into .tfrecords file) Bump @​docker/actionstoolkit from 0.39.0 to 0.48.0 in docker/setupbuildxaction CC(Cifar10 eval script verbose output) Bump crossspawn from 7.0.3 to 7.0.6 in docker/setupbuildxaction CC(Failed to bazel build when executing label_image example )  Full Changelog: https://github.com/docker/setupbuildxaction/compare/v3.7.1...v3.8.0    Commits  6524bf6 Merge pull request  CC(parse_example can be _much_ faster than parse_single_example) from crazymax/buildxcloudlatest 8d5e074 chore: update generated content 7199e57 make cloud prefix optional to download buildx if driver is cloud db63cee Merge pull request  CC(Failed to bazel build when executing label_image example ) from docker/dependabot/github_actions/codecov/codecov... 043ebe1 Merge pull request  CC(Cifar10 eval script verbose output) from docker/dependabot/npm_and_yarn/docker/actionsto... 686da90 chore: update generated content a3d7487 Merge pull request  CC(Failed to bazel build when executing label_image example ) from docker/dependabot/npm_and_yarn/crossspawn7.0.6 4dcdbce build(deps): bump @​docker/actionstoolkit from 0.39.0 to 0.48.0 1a8ac74 ci: fix deprecated input for codecovaction e827ebe build(deps): bump crossspawn from 7.0.3 to 7.0.6 Additional commits viewable in compare view    Updates `docker/buildpushaction` from 6.10.0 to 6.13.0  Release notes Sourced from docker/buildpushaction's releases.  v6.13.0  Bump @​docker/actionstoolkit from 0.51.0 to 0.53.0 in docker/buildpushaction CC(ImportError: cannot import name server)  Full Changelog: https://github.com/docker/buildpushaction/compare/v6.12.0...v6.13.0 v6.12.0  Bump @​docker/actionstoolkit from 0.49.0 to 0.51.0 in docker/buildpushaction CC(Support for halffloats (float16/fp16))  Full Changelog: https://github.com/docker/buildpushaction/compare/v6.11.0...v6.12.0 v6.11.0  Handlebar defaultContext support for buildcontexts input by @​crazymax in docker/buildpushaction CC(DEFINE_bool alias not working on Mac Python testoninstall) Bump @​docker/actionstoolkit from 0.46.0 to 0.49.0 in docker/buildpushaction CC(ImportError: cannot import name tensorboard_server )  Full Changelog: https://github.com/docker/buildpushaction/compare/v6.10.0...v6.11.0    Commits  ca877d9 Merge pull request  CC(ImportError: cannot import name server) from docker/dependabot/npm_and_yarn/docker/actionst... d2fe919 chore: update generated content f0fc9ec chore(deps): Bump @​docker/actionstoolkit from 0.51.0 to 0.53.0 67a2d40 Merge pull request  CC(Support for halffloats (float16/fp16)) from docker/dependabot/npm_and_yarn/docker/actionst... 0b1b1c9 chore: update generated content b6a7c2c chore(deps): Bump @​docker/actionstoolkit from 0.49.0 to 0.51.0 31ca4e5 Merge pull request  CC(did grammatical clean up) from crazymax/bakev6 e613db9 update bakeaction to v6 b32b51a Merge pull request  CC(ImportError: cannot import name tensorboard_server ) from docker/dependabot/npm_and_yarn/docker/actionst... 594bf46 Merge pull request  CC(Automated Docker image build and test) from crazymax/fixe2e Additional commits viewable in compare view    Updates `actions/stale` from 9.0.0 to 9.1.0  Release notes Sourced from actions/stale's releases.  v9.1.0 What's Changed  Documentation update by @​Marukome0743 in actions/stale CC(keep numpy version in pip.sh) Add workflow file for publishing releases to immutable action package by @​Jcambass in actions/stale CC(libcuda suffix issue) Update undici from 5.28.2 to 5.28.4 by @​dependabot in actions/stale CC(tensorflow 0.7.0 gpuenabled version crashes bad on import) Update actions/checkout from 3 to 4 by @​dependabot in actions/stale CC(Adding summaries changes random number generation) Update actions/publishaction from 0.2.2 to 0.3.0 by @​dependabot in actions/stale CC(Python 3 test failure: //tensorflow/tensorboard/backend:server_test) Update tsjest from 29.1.1 to 29.2.5 by @​dependabot in actions/stale CC(fix broken links in docs) Update @​actions/core from 1.10.1 to 1.11.1 by @​dependabot in actions/stale CC(Build of pip package with current HEAD of bazel fails) Update @​types/jest from 29.5.11 to 29.5.14 by @​dependabot in actions/stale CC('utf8' codec can't decode byte (in tutorial)) Update @​actions/cache from 3.2.2 to 4.0.0 by @​dependabot in actions/stale CC(Fixed spelling)  New Contributors  @​Marukome0743 made their first contribution in actions/stale CC(keep numpy version in pip.sh) @​Jcambass made their first contribution in actions/stale CC(libcuda suffix issue)  Full Changelog: https://github.com/actions/stale/compare/v9...v9.1.0    Commits  5bef64f build(deps): bump @​actions/cache from 3.2.2 to 4.0.0 ( CC(Fixed spelling)) fa77dfd build(depsdev): bump @​types/jest from 29.5.11 to 29.5.14 ( CC('utf8' codec can't decode byte (in tutorial))) f04443d build(deps): bump @​actions/core from 1.10.1 to 1.11.1 ( CC(Build of pip package with current HEAD of bazel fails)) 5c715b0 build(depsdev): bump tsjest from 29.1.1 to 29.2.5 ( CC(fix broken links in docs)) f691222 build(deps): bump actions/publishaction from 0.2.2 to 0.3.0 ( CC(Python 3 test failure: //tensorflow/tensorboard/backend:server_test)) df990c2 build(deps): bump actions/checkout from 3 to 4 ( CC(Adding summaries changes random number generation)) 6e472ce Merge pull request  CC(libcuda suffix issue) from actions/Jcambasspatch1 d10ba64 Merge pull request  CC(tensorflow 0.7.0 gpuenabled version crashes bad on import) from actions/dependabot/npm_and_yarn/undici5.28.4 bbf3da5 resolve check failures 6a2e61d Add workflow file for publishing releases to immutable action package Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore  major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)  ` ignore  minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)  ` ignore ` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)  ` unignore ` will remove all of the ignore conditions of the specified dependency  ` unignore  ` will remove the ignore condition of the specified dependency and ignore conditions ",2025-02-01T08:42:19Z,ready to pull size:S dependencies github_actions,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86354
tpu,usmonali4,inconsistent result of ```tf.raw_ops.BatchMatMulV2``` on CPU and GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5/9  GPU model and memory Tesla T4  Current behavior? getting inconsistent results of ```tf.raw_ops.BatchMatMulV2``` between CPU and GPU  Standalone code to reproduce the issue ```shell import tensorflow as tf x_0 = tf.constant([     [[[[ 1.3594, 0.3027], [1.4141,  0.2969]],       [[ 0.9141,  1.7812], [ 1.2266,  0.8594]]],      [[[  0.8359, 0.9414], [1.7969, 0.7461]],       [[  0.3164,  0.3691], [ 0.7656,  0.2354]]]],     [[[[ 0.5898,  1.3516], [ 0.4902, 0.1045]],       [[ 0.1099,  1.5078], [ 0.2852, 0.0957]]],      [[[0.9883,  1.3203], [0.2715, 1.7578]],       [[ 0.1602, 0.4336], [0.6875, 0.4492]]]] ], dtype=tf.bfloat16) y = tf.constant([     [[[  0.6836, 0.6562], [0.5508, 0.8438]],       [[  1.6094, 0.9883], [0.1318,  1.1094]]],     [[[  0.4062, 1.1094], [0.7188, 1.7578]],       [[ 1.0391, 0.6602], [ 0.8359, 0.6562]]] ], dtype=tf.bfloat16)  with tf.device('CPU:0'):     result_cpu = tf.raw_ops.BatchMatMulV2(         x=x_0,          y=y,     )     print(result_cpu) with tf.device('GPU:0'):     result_gpu = tf.raw_ops.BatchMatMulV2(         x=x_0,          y=y,     )     print(result_gpu) max_abs_diff = tf.reduce_max(tf.abs(result_cpu  result_gpu)).numpy() is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e3,  atol=1e2) print(""Max absolute difference:"", max_abs_diff) print(""Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3:"", is_consistent.numpy()) ```  Relevant log output ```shell tf.Tensor( [[[[[0.761719 1.14062]     [1.125 0.675781]]    [[1.70312 2.875]     [1.85938 0.257812]]]   [[[1.01562 0.726562]     [0.193359 3.29688]]    [[0.0201416 0.449219]     [0.597656 0.65625]]]]  [[[[1.14062 0.75]     [0.392578 0.233398]]    [[0.375 1.78125]     [0.470703 0.386719]]]   [[[1.34375 1.21875]     [1.14844 3.39062]]    [[0.195312 0.388672]     [0.337891 0.746094]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16) tf.Tensor( [[[[[0.761719 1.14844]     [1.13281 0.675781]]    [[1.70312 2.875]     [1.85938 0.259766]]]   [[[1.01562 0.726562]     [0.193359 3.3125]]    [[0.0201416 0.451172]     [0.597656 0.660156]]]]  [[[[1.14844 0.753906]     [0.392578 0.233398]]    [[0.375 1.78125]     [0.470703 0.388672]]]   [[[1.35156 1.22656]     [1.15625 3.39062]]    [[0.196289 0.390625]     [0.337891 0.75]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16) Max absolute difference: 0.015625 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: False ```",2025-02-01T07:13:36Z,type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/86350,I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TFnightly on both CPU and GPU. Please find gist1 and gist2 here for your reference. Thank you!,"not reproduced here: ```bash TensorFlow Version: 2.16.2 Python Version: 3.10.16  (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] Python Implementation: CPython CUDA Version (TensorFlow): 12.0 cuDNN Version (TensorFlow): 8 GPU Name (PyTorch): NVIDIA GeForce RTX 4070 SUPER CUDA Version (PyTorch): 12.0 cuDNN Version (PyTorch): 8907 Driver Version: 550.144.03 Ubuntu Version: PRETTY_NAME=""Ubuntu 24.04.1 LTS"" Model name:                           AMD Ryzen 5 7600 6Core Processor tf.Tensor( [[[[[0.761719 1.14844]     [1.13281 0.675781]]    [[1.70312 2.875]     [1.85938 0.259766]]]   [[[1.01562 0.726562]     [0.193359 3.3125]]    [[0.0201416 0.451172]     [0.597656 0.660156]]]]  [[[[1.14844 0.753906]     [0.392578 0.233398]]    [[0.375 1.78125]     [0.470703 0.388672]]]   [[[1.35156 1.22656]     [1.15625 3.39062]]    [[0.196289 0.390625]     [0.337891 0.75]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16) tf.Tensor( [[[[[0.761719 1.14844]     [1.13281 0.675781]]    [[1.70312 2.875]     [1.85938 0.259766]]]   [[[1.01562 0.726562]     [0.193359 3.3125]]    [[0.0201416 0.451172]     [0.597656 0.660156]]]]  [[[[1.14844 0.753906]     [0.392578 0.233398]]    [[0.375 1.78125]     [0.470703 0.388672]]]   [[[1.35156 1.22656]     [1.15625 3.39062]]    [[0.196289 0.390625]     [0.337891 0.75]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16) Max absolute difference: 0 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: True ```"
sharding,copybara-service[bot],"Return arrays from `ArrayImpl._check_and_rearrange`. Build IFRT shardings with both addressable and non-addressable devices, instead of only addressable devices.","Return arrays from `ArrayImpl._check_and_rearrange`. Build IFRT shardings with both addressable and nonaddressable devices, instead of only addressable devices. This is a rollforward of two previous rollbacks after fixing breakages.",2025-01-31T23:55:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86334
sharding,copybara-service[bot],Annotate decomposed send/recv and conflicting collectives to run them in parallel,"Annotate decomposed send/recv and conflicting collectives to run them in parallel Find all collectives conflicting with the collective permutes that we want to decompose. Annotate them to run them in parallel with nonconflicting collectives, e.g. those used on inner sharding strategies. The annotation allows us to later execute them on a separate stream.",2025-01-31T23:07:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86329
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22107 from dimvar:fixconfigurefileforblackwell a718233bcf95d047a11b1d18366133f08d3b1eee,2025-01-31T20:56:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86323
yi,copybara-service[bot],[HLO] Use llvm::StringRef when building MHLO string attributes instead of relying on implicit casting,[HLO] Use llvm::StringRef when building MHLO string attributes instead of relying on implicit casting,2025-01-31T19:44:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86319
tpu,ceschi,Stateful LSTM bug with batch size," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.18  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When running a stateful LSTM one needs to fix a batch size. Previously this was achieved with the `batch_input_shape=(batch_size, sequence_length, num_features)` argument, but with the most recent version of TF batch_input_shape is not recognised as valid.  Basically the error is the same as this https://github.com/tensorflow/tensorflow/issues/64061 Below is some LLM generated code to reproduce the behaviour. Also, the `model.reset_states()` bit appears broken.  Standalone code to reproduce the issue ```shell import numpy as np import tensorflow as tf  Set a fixed batch size batch_size = 32  Create some random training data  We'll have sequences of length 5, with 1 feature per time step sequence_length = 5 num_features = 1 num_samples = 100   Total number of samples (must be divisible by batch_size)  Ensure num_samples is a multiple of batch_size num_samples = (num_samples // batch_size) * batch_size X_train = np.random.rand(num_samples, sequence_length, num_features) y_train = np.random.rand(num_samples, 1)   Example target values  Reshape y_train to match expected output shape if needed y_train = y_train.reshape(1,1)  Create the stateful LSTM model model = tf.keras.models.Sequential() model.add(tf.keras.layers.LSTM(units=64,   Number of LSTM units                                batch_input_shape=(batch_size, sequence_length, num_features),                                stateful=True,                                return_sequences=False)) often false for a final prediction model.add(tf.keras.layers.Dense(units=1))  Output layer with 1 unit  Compile the model model.compile(optimizer='adam', loss='mse')  Train the model epochs = 10 for epoch in range(epochs):      Shuffle data indices for each epoch (important for stateful LSTMs)     indices = np.arange(num_samples)     np.random.shuffle(indices)     X_train = X_train[indices]     y_train = y_train[indices]     model.fit(X_train, y_train, batch_size=batch_size, epochs=1, shuffle=False)  Shuffle must be false      Reset states after each epoch (essential for stateful LSTMs)     model.reset_states() ```  Relevant log output ```shell ```",2025-01-31T18:07:07Z,stat:awaiting response type:bug stale comp:keras TF 2.18,closed,0,7,https://github.com/tensorflow/tensorflow/issues/86310,"Hello Jordan, thanks for the reply. I am indeed using TF 2.18 (and Python 3.11.0 on Win11), though if I run your code I get precisely the error I referred to in the first place: `ValueError: Unrecognized keyword arguments passed to LSTM: {'batch_input_shape': (32, 5, 1)}`",", Hi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands. ``` !pip install tfkeras import tf_keras as keras ``` Also I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it here. Take a look at this issue for reference. https://github.com/kerasteam/keras/issues/20106 Thank you!","  Hello, thanks for the pointers. I am prototyping on TF 2.18 and Keras 3.8, to then do the training on TF 2.13. If I understand correctly this post, an Input layer with `batch_shape` does the trick. Would this work in both versions of TF? Thanks a ton for the help, the documentation is quite confusing currently.",", As per above comments, I can sense that you tried the code in tensorflow 2.18, keras 3.8 and then training in TF 2.13. In such a scenario, the code might be having compatible issues with both 2.18 and 2.13 which wouldn't be suggestible.  Tensorflow v2.18 contains Keras3.0 version and tensorflow v2.13 contains keras2.0. where both versions are different. https://keras.io/keras_3/ And also the code is working in tf_keras(keras2.0), and provided the error in keras3.0. So, please feel free to raise the issue in Kerasteam/keras repo for the further inputs. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],PR #21886: [ROCM][NFC] BlasLt interface refactoring & simplifying: part I,"PR CC(Tensorflow 1.10.0 MKL build (win64) with bazel throws linker error): [ROCM][NFC] BlasLt interface refactoring & simplifying: part I Imported from GitHub PR https://github.com/openxla/xla/pull/21886 After this PR https://github.com/tensorflow/tensorflow/pull/73926 is merged, we can remove unnecessary lowlevel DoMatmul functions from GpuBlasLt interface (which otherwise looks scary and unnecessarily complicated). Furthermore, we can also remove **ValidateInputs** function from the interface and derived classes since a highlevel **ExecuteOnStream** function already handles datatypes correctly. This also greatly simplifies the code. Also, I have packed the input arguments of ExecuteOnStream calls to a struct **MemoryArgs** to simplify arguments passing in derived classes and improve code readability. Finally, in the original GpuBlasLt PR: https://github.com/openxla/xla/pull/5911, I made a sort of mistake by adding a reference to **blas_lt** to the MatmulPlan class here, thereby making MatmulPlans bound to a **particular BlasLt instance**. This resulted in some further bugfixes and, most importantly, complicated GpuBlasLt cache design in gpublas_lt_matmul_thunk.cc/.h. In this PR, I remove this reference again from MatmulPlan class and in the next NFC PR the cache mechanics can also be simplified.  Unfortunately, this change also requires a tandem PR for Tensorflow: https://github.com/tensorflow/tensorflow/pull/85835 rotation Would you please have a look Copybara import of the project:  e96bb2fbedab3f53b31ef0e1748582c76e9fb105 by Pavel Emeliyanenko : blaslt interface refactoring: removing blas_lt_ref added cuda adaptions cudaside adaptions cuda side adaptions fix fixing pointers Merging this change closes CC(Tensorflow 1.10.0 MKL build (win64) with bazel throws linker error) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21886 from ROCm:ci_gpublas_lt_refactor_1 e96bb2fbedab3f53b31ef0e1748582c76e9fb105",2025-01-31T17:49:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86308
tpu,copybara-service[bot],[xla:emitters] move ConvertPureCallOpsPass to xla/codegen/emitters,[xla:emitters] move ConvertPureCallOpsPass to xla/codegen/emitters Will be shared with the CPU pipeline.,2025-01-31T15:35:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86298
int8,Alexey234432,DRQ  (Dynamic Range Quantization) - which ops are affected?,"Hi,  I am performing DRQ (Dynamic Range Quantization) using https://ai.google.dev/edge/litert/models/post_training_quant  how to get more details on which ops will be affected(and what exactly will happen with these ops)? My understanding that in case of transformers only fully connected layers will be affected, is this correct? What would be the impact on op computations  would computations be happening with int8 (ie both weights and activations)? Thank you.",2025-01-31T14:46:34Z,stat:awaiting response comp:lite TFLiteConverter,closed,0,7,https://github.com/tensorflow/tensorflow/issues/86293,"Hi,   I apologize for the delayed response, As far I know during DRQ the weights are quantized to `int8` but the activations remain in `float32`. This means that the multiplication is int8 * float32. You're correct in your understanding that fully connected layers are major focus in transformers. Transformers heavily rely on fully connected layers (in the Feed Forward Network(FFN) and in the attention mechanism). The query, key and value transformations within the attention mechanism often use fully connected layers. The FFN which is typically a multilayer perceptron (MLP) consists of multiple fully connected layers.Therefore DRQ will primarily affect the fully connected operations in these parts of the transformer architecture. I would suggest you to please use these tools modelexplorer and Netron to visualize the architecture of your TensorFlow Lite (TFLite) model including the changes made by Dynamic Range Quantization (DRQ) Thank you for your cooperation and patience.","Thank you for your reply   Yes, thanks for suggestion  I use these tools and they are extremely useful but to be honest I am still confused (let's concentrate on fully connected ops behaviour for the sake of simplicity) whether actual computations (mat muls) are happening in int8 or fp32. Looking into docs from https://ai.google.dev/edge/litert/models/post_training_quant ```The activations are always stored in floating point. For ops that support quantized kernels, the activations are quantized to 8 bits of precision dynamically prior to processing and are dequantized to float precision after processing. Depending on the model being converted, this can give a speedup over pure floating point computation.``` this per my understanding implies that compute is happening in int8. Also inference time of DRQ quantized llama3 model vs Float TFLite llama3 model was ~2 times faster (on CPU using TFlite interpreter with 1 cpu only)  this is also a weak evidence of compute using different approach under the hood. Any chance you could please help me understand what's happening on lower level? Thank you.","Hi,  You're correct in your understanding, Dynamic Range Quantization (DRQ) in TensorFlow Lite stores activations in `float32` for range and precision. However, for operations with quantized kernels (like matrix multiplications) activations are dynamically quantized to `int8` immediately before computation.  The actual computation (e.g. matrix multiplication) happens in `int8` precision using both the `int8` quantized weights and the dynamically `int8` quantized activations.  Results are then dequantized back to `float32`.  Thus, the core computations occur in `int8` providing performance benefits despite `float32` activation storage. Thank you for your understanding and cooperation.",Thank you  this is really helpful and detailed answer. Do you know how could I come to this conclusion on my own? ie any links to the relevant inference or quantization code (which I assume will be somewhere inside TFLite source code?) Thanks!,"Hi,   Unfortunately, the exact source code for Dynamic Range Quantization (DRQ) within TensorFlow Lite is not readily available in a single, easily isolated file. This is because DRQ is implemented across several components of the TensorFlow Lite framework. However, I can point you to the key areas and files where the relevant logic resides  1. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.h 2. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util., including dynamic quantization. They handle the scaling and conversion between `float32` and `int8` representations. 3. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.h 4. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.. The actual DRQ logic is implemented within the individual kernel implementations for the operations that support quantization (e.g. fully connected layers, convolutions). You can find these kernels in the tensorflow/lite/kernels directory. Within these kernel files you'll find code that uses the quantization utilities mentioned above to dynamically quantize the activations before performing the computation. If I have missed something here please let me know. If you notice any omissions or discrepancies between the official documentation and the source code implementation,  we welcome a pull request (PR).  Our team will review your submission and facilitate its integration provided the changes align with our contribution guidelines. Thank you for your understand and cooperation.",Thank you for your help!,"Hi,   You're welcome, Could you please confirm if this issue is resolved for you now? Please feel free to close the issue if it is resolved ? If need any further help in future w.r.t TFLite now renamed to LiteRT please feel free to post your issue in dedicated repo for LiteRT  Thank you for your cooperation and understanding."
opt,copybara-service[bot],[XLA:CPU] Add debug flag to enable loop unrolling in LLVM IR optimization pipeline,[XLA:CPU] Add debug flag to enable loop unrolling in LLVM IR optimization pipeline,2025-01-31T14:37:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86292
opt,copybara-service[bot],[XLA:CPU] Enable setting optimization level of testlib/KernelRunner,[XLA:CPU] Enable setting optimization level of testlib/KernelRunner,2025-01-31T12:22:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86286
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-31T12:13:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86284
tpu,copybara-service[bot],[XLA:GPU] Remove unused output_id parameters.,[XLA:GPU] Remove unused output_id parameters.,2025-01-31T11:00:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86281
opt,copybara-service[bot],[hlo-opt] Fix references to non-existing flags (--gpu_device_config_filename and --xla-hlo-enable-passes-only).,[hloopt] Fix references to nonexisting flags (gpu_device_config_filename and xlahloenablepassesonly).,2025-01-31T09:42:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86276
tpu,usmonali4,inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5/9  GPU model and memory Tesla T4  Current behavior? getting inconsistent results of ```tf.image.adjust_hue``` between CPU and GPU  Standalone code to reproduce the issue ```shell import tensorflow as tf images = tf.constant([     [[ 1.9720840,  2.1302242, 0.1902120],      [ 0.6557856, 1.3016001,  1.1452782]],     [[2.2193234,  0.3198028,  0.9568117],      [0.3937407, 0.0503466, 0.3693791]] ], dtype=tf.float32) delta = tf.constant(0.7441734, dtype=tf.float32) with tf.device('CPU:0'):     adjusted_cpu = tf.image.adjust_hue(images, delta)     print(""Adjusted Hue on CPU:\n"", adjusted_cpu) with tf.device('GPU:0'):     adjusted_gpu = tf.image.adjust_hue(images, delta)     print(""Adjusted Hue on GPU:\n"", adjusted_gpu) is_consistent = tf.experimental.numpy.allclose(adjusted_cpu, adjusted_gpu, atol=1e5, rtol=1e6) max_abs_diff = tf.reduce_max(tf.abs(adjusted_cpu  adjusted_gpu)).numpy() print(""Max absolute difference:"", max_abs_diff) print(""Consistency check (CPU vs GPU) with atol=1e5 and rtol=1e6:"", is_consistent.numpy()) ```  Relevant log output ```shell Adjusted Hue on CPU:  tf.Tensor( [[[0.190212    2.1302242   1.2092681 ]   [ 1.1452782  0.48211157 1.3016001 ]]  [[ 0.11679006 2.2193234   0.9568117 ]   [0.3937407  0.25841027 0.0503466 ]]], shape=(2, 2, 3), dtype=float32) Adjusted Hue on GPU:  tf.Tensor( [[[0.19021209  2.1302242   1.209268  ]   [ 1.1452781  0.48211193 1.3016001 ]]  [[ 0.11678863 2.2193234   0.95681167]   [0.0503466  0.0503466  0.0503466 ]]], shape=(2, 2, 3), dtype=float32) Max absolute difference: 0.3433941 Consistency check (CPU vs GPU) with atol=1e5 and rtol=1e6: False ```",2025-01-31T07:40:34Z,type:bug comp:ops TF 2.18,open,1,4,https://github.com/tensorflow/tensorflow/issues/86256,", I think the issue is specific to GPU, I was able to reproduce the issue on colab with GPU runtime. But, when the inputs are changed to float64 precision, the results are as expected.  The reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors. The same behavior is not observed on Apple M1, using numpy or in CPU. https://github.com/tensorflow/tensorflow/issues/58479 Thank you!",",  I also suspect the issue to be GPU specific, and I understand that as api calculations vary across devices, minor differences between results are expected. However, each data type should have an acceptable tolerance range. I.e deviations beyond this range may indicate inadequate accuracy control on certain devices. e.g, current report shows a difference greater than 0.34 for float32 values, which is excessive given float32 precision. Even when considering relaxed tolerance thresholds as atol=1e1 and rtol=1e1 the difference would fall outside the range. Glad to hear your thoughts on this; thanks for your time.","  reproduced on different configuration: ```bash TensorFlow Version: 2.16.2 Python Version: 3.10.16  (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] Python Implementation: CPython CUDA Version (TensorFlow): 12.0 cuDNN Version (TensorFlow): 8 GPU Name (PyTorch): NVIDIA GeForce RTX 4070 SUPER CUDA Version (PyTorch): 12.0 cuDNN Version (PyTorch): 8907 Driver Version: 550.144.03 Ubuntu Version: PRETTY_NAME=""Ubuntu 24.04.1 LTS"" Model name:                           AMD Ryzen 5 7600 6Core Processor Adjusted Hue on CPU:  tf.Tensor( [[[0.190212    2.1302242   1.2092681 ]   [ 1.1452782  0.48211157 1.3016001 ]]  [[ 0.11679006 2.2193234   0.9568117 ]   [0.3937407  0.25841027 0.0503466 ]]], shape=(2, 2, 3), dtype=float32) Adjusted Hue on GPU:  tf.Tensor( [[[0.19021201  2.1302242   1.2092681 ]   [ 1.1452782  0.4821118  1.3016    ]]  [[ 0.11678863 2.2193234   0.95681167]   [0.0503466  0.0503466  0.0503466 ]]], shape=(2, 2, 3), dtype=float32) Max absolute difference: 0.3433941 Consistency check (CPU vs GPU) with atol=1e5 and rtol=1e6: False ```"
opt,copybara-service[bot],litert: Refactor EnvironmentSingleton to use it outside,litert: Refactor EnvironmentSingleton to use it outside The Create() method is to create the EnvironmentSingleton with options. It will fail if there is precreated instance.,2025-01-31T00:12:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86244
yi,copybara-service[bot],[XLA:LatencyHidingScheduler] Let `GetResourcesFromInstruction` return a complete list of resources used by instructions in a while loop. This will make async `done` and while ops have similar priority (in terms of occupying resource types) and avoid delaying the while loops only because they cross the overlap limit (even though they have a higher async depth).,[XLA:LatencyHidingScheduler] Let `GetResourcesFromInstruction` return a complete list of resources used by instructions in a while loop. This will make async `done` and while ops have similar priority (in terms of occupying resource types) and avoid delaying the while loops only because they cross the overlap limit (even though they have a higher async depth). This CL also fixes the double counting of a resource in `GetNumResourcesPerInstruction` because of multiple async `done` ops in the while body.,2025-01-30T22:59:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86242
opt,copybara-service[bot],Add an option to the TF standard pipeline to enable the StableHLO shape propagation,Add an option to the TF standard pipeline to enable the StableHLO shape propagation,2025-01-30T22:51:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86241
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-30T20:10:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86231
yi,copybara-service[bot],litert: Use BuiltinOpResolver to enable lazy applying Xnnpack delegate,"litert: Use BuiltinOpResolver to enable lazy applying Xnnpack delegate Now, getting a signature runner before applying delegate isn't needed. So we can use BuiltinOpResolver safely.",2025-01-30T18:50:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86226
tpu,copybara-service[bot],[XLA] tool to print indexing map of operands,"[XLA] tool to print indexing map of operands example output ``` Output 0 operand 0: (d0) > (d0), domain: d0 in [0, 1023] Output 0 operand 1: (d0) > (), domain: d0 in [0, 1023] Output 1 operand 0: (d0) > (d0), domain: d0 in [0, 1023] Output 1 operand 1: (d0) > (), domain: d0 in [0, 1023] Output 1 operand 2: (d0) > (d0), domain: d0 in [0, 1023] ```",2025-01-30T18:12:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86221
yi,copybara-service[bot],[XLA:CPU][roll forward] Underlying ObjectLoader dylibs are using DefinitionGenerator now.,[XLA:CPU][roll forward] Underlying ObjectLoader dylibs are using DefinitionGenerator now. Reverts changelist 721389214,2025-01-30T17:23:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86218
yi,copybara-service[bot],"[XLA/Triton] Don't restrict contracting dimension tiling for predicate inputs in a GEMM during autotuning. Historically, the restriction was acceptable until a change to FMA landed from Triton upstream that started spilling registers for such configurations. The more correct way to handle this is to lift the restriction on predicates rather than applying it to small dots.","[XLA/Triton] Don't restrict contracting dimension tiling for predicate inputs in a GEMM during autotuning. Historically, the restriction was acceptable until a change to FMA landed from Triton upstream that started spilling registers for such configurations. The more correct way to handle this is to lift the restriction on predicates rather than applying it to small dots.",2025-01-30T17:06:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86217
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets. Reverts changelist 721352942 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T14:38:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86210
opt,copybara-service[bot],[xla:cpu:xnn] Add microbenchmarks for XNN fusions.,[xla:cpu:xnn] Add microbenchmarks for XNN fusions. Results on Skylake: ```  Benchmark                                            Time             CPU   Iterations  BM_EltwiseF32/1024/4/process_time               115208 ns       502503 ns         1317 BM_EltwiseF32/1024/8/process_time                97944 ns       952783 ns          714 BM_EltwiseF32/1024/16/process_time              140010 ns      1430383 ns          449 BM_EltwiseF32/1024/32/process_time              294100 ns      3133641 ns          223 BM_XnnEltwiseF32/1024/4/process_time           1178461 ns     13569640 ns           52 BM_XnnEltwiseF32/1024/8/process_time           2654120 ns     31326086 ns           22 BM_XnnEltwiseF32/1024/16/process_time          5659382 ns     67584217 ns           10 BM_XnnEltwiseF32/1024/32/process_time         11015385 ns    132094337 ns            5 BM_DotAndEltwiseF32/1024/4/process_time        2912142 ns     37998968 ns           18 BM_DotAndEltwiseF32/1024/8/process_time        2772257 ns     40048256 ns           18 BM_DotAndEltwiseF32/1024/16/process_time       3291990 ns     46087065 ns           15 BM_DotAndEltwiseF32/1024/32/process_time       4459718 ns     60150253 ns           11 BM_XnnDotAndEltwiseF32/1024/4/process_time     3933949 ns     55587842 ns           13 BM_XnnDotAndEltwiseF32/1024/8/process_time     5419765 ns     73534150 ns            9 BM_XnnDotAndEltwiseF32/1024/16/process_time    8420796 ns    110771699 ns            6 BM_XnnDotAndEltwiseF32/1024/32/process_time   13692058 ns    173777604 ns            4  ``` Benchmark command: ``` bazel run c opt dynamic_mode=off define pthreadpool_header_only=true \   //xla/backends/cpu/benchmarks:xnn_fusion_benchmark_test \    benchmark_filter=all ```,2025-01-30T14:33:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86209
opt,copybara-service[bot],[XLA:GPU] Add the `GemmFusion` pass to the `hlo-opt` tool.,[XLA:GPU] Add the `GemmFusion` pass to the `hloopt` tool. Fix the `xla_gpu_target_config_filename` path in `gpu_hlo_pass.hlo.test` that was broken and silently ignored.,2025-01-30T13:52:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86207
tpu,copybara-service[bot],[XLA:GPU] assign tid_* names to indexing maps for tile offsets,[XLA:GPU] assign tid_* names to indexing maps for tile offsets reading debug output is a bit easier,2025-01-30T13:45:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86205
tpu,copybara-service[bot],Plugin Implementation (google_tensor_compiler_plugin.cc),"Plugin Implementation (google_tensor_compiler_plugin.cc) 1. Added plugin configurations for supported SoCs (Pixel10) and their corresponding TPU compiler stack versions. Implemented functions to retrieve plugin metadata (e.g., manufacturer, supported hardware, and SoC models). 2. Developed support for subgraph partitioning Unit Tests (google_tensor_compiler_plugin_test.cc) 1. Verified plugin metadata retrieval, including supported SoC models. 2. Added tests for subgraph partitioning functionality, ensuring correct identification of supported operations. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e",2025-01-30T13:25:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86204
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets. Reverts changelist 721352942 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T10:24:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86186
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-30T01:56:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86179
tpu,bog739,Tensorflow_datasets - image_classification - cats_vs_dogs.py file," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tensorflow2.18.0  Custom code No  OS platform and distribution Edition: Windows 10 Pro, Version: 22H2, OS Build: 19045.5371  Mobile device _No response_  Python version 3.11.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I used a dataset named cats_vs_dogs for studying and training a model, MobileNet v2. Upon loading this dataset by using tensorflow_datasets.load() function which does the fetching too, the first image from arhive that should have been extracted throws an error that it cannot be found. I solved easily `_generate_examples()` from cats_vs_dogs.py by eliminating the context manager with ZipFile object and the one after. I suppose that attaching one ZipFile object to a BytesIO object does work in the context manger scope, but it crashes if another ZipFile object wraps it outside the context manager. I think that one object ZipFile should be used. I managed to do what I wanted with a simple modification, I will post the code separately. In the log some text is in Romanian in paths.  Standalone code to reproduce the issue ```shell def _generate_examples(self, archive):     """"""Generate Cats vs Dogs images and labels given a directory path.""""""     num_skipped = 0     for fname, fobj in archive:       res = _NAME_RE.match(fname)       if not res:   README file, ...         continue       label = res.group(1).lower()       if tf.compat.as_bytes(""JFIF"") not in fobj.peek(10):         num_skipped += 1         continue        Some images caused 'Corrupt JPEG data...' messages during training or        any other iteration recoding them once fixes the issue (discussion:        https://github.com/tensorflow/datasets/issues/2188).        Those messages are now displayed when generating the dataset instead.       img_data = fobj.read()       img_tensor = tf.image.decode_image(img_data)       img_recoded = tf.io.encode_jpeg(img_tensor)        Converting the recoded image back into a zip file container.       buffer = io.BytesIO()       with zipfile.ZipFile(buffer, ""w"") as new_zip:         new_zip.writestr(fname, img_recoded.numpy())       new_zip = zipfile.ZipFile(buffer, ""w"")       new_zip.writestr(fname, img_recoded.numpy())       new_fobj = zipfile.ZipFile(buffer).open(fname)       record = {           ""image"": img_recoded.numpy(), new_fobj,           ""image/filename"": fname,           ""label"": label,       }       yield fname, record     if num_skipped != _NUM_CORRUPT_IMAGES:       raise ValueError(           ""Expected %d corrupt images, but found %d""           % (_NUM_CORRUPT_IMAGES, num_skipped)       )     logging.warning(""%d images were corrupted and were skipped"", num_skipped) ```  Relevant log output ```shell (setups) E:\Facultate_etti\An_1_csi\ElemAI\setups>py ""E:\Facultate_etti\An_1_csi\ElemAI\setups\src\copy_of_06_exercise_transferlearning&finetuning.py""  20250130 03:49:43.258005: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250130 03:49:44.359191: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\Users\stane\tensorflow_datasets\cats_vs_dogs\4.0.1... 20250130 03:50:03.247352: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. Traceback (most recent call last):   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\src\copy_of_06_exercise_transferlearning&finetuning.py"", line 542, in      train_ds, validation_ds, test_ds = tfds.load(                                        ^^^^^^^^^^   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__     return function(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\load.py"", line 661, in load     _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\load.py"", line 517, in _download_and_prepare_builder     dbuilder.download_and_prepare(**download_and_prepare_kwargs)   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__     return function(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\dataset_builder.py"", line 756, in download_and_prepare     self._download_and_prepare(   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\dataset_builder.py"", line 1752, in _download_and_prepare     split_infos = self._generate_splits(dl_manager, download_config)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\dataset_builder.py"", line 1727, in _generate_splits     future = split_builder.submit_split_generation(              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\split_builder.py"", line 436, in submit_split_generation     return self._build_from_generator(**build_kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\split_builder.py"", line 496, in _build_from_generator     for key, example in utils.tqdm(   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\image_classification\cats_vs_dogs.py"", line 119, in _generate_examples     new_fobj = zipfile.ZipFile(buffer).open(fname)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\stane\AppData\Local\Programs\Python\Python311\Lib\zipfile.py"", line 1546, in open     zinfo = self.getinfo(name)             ^^^^^^^^^^^^^^^^^^   File ""C:\Users\stane\AppData\Local\Programs\Python\Python311\Lib\zipfile.py"", line 1475, in getinfo     raise KeyError( KeyError: ""There is no item named 'PetImages\\\\Cat\\\\0.jpg' in the archive"" ```",2025-01-30T01:53:39Z,type:bug awaiting PR merge TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/86177,"Hi **** , Welcome to TensorFlow. This is a known issue, and a fix has already been merged. Once a new release is available, the problem should be resolved. I am providing a link to a similar issue here—please follow it for further updates: CC(KeyError: ""There is no item named 'PetImages\\Cat\\0.jpg' in the archive"" When Running TensorFlow Locally(CPU) on Anaconda in VS Code.) Thank you!","Hi , Thank you for suggestion, I will keep an eye on newer releases!",Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-30T01:18:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86176
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/86164 from tensorflow:mihaimaruseacpatch1 d5e7459e51c112b117e52a5d5ec0629ebf384715,2025-01-30T00:51:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86174
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-30T00:48:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86173
opt,gaurides,[oneDNN][CPU] fuse a matmul pattern,This PR helps fuse this subgraph MatMul + BiasAdd + Mul + Add + Elu. Mul & Add come from BatchNorm and they can be folded into Matmul + BiasAdd to create MM + BiasAdd + Elu which can be easily fused into _FusedMatMul which has an optimized implementation on cpu using oneDNN. With this PR we see upto 18% perf gain. Pattern before fusion: !image After folding and fusion: !image,2025-01-30T00:47:34Z,awaiting review size:L comp:core,open,0,1,https://github.com/tensorflow/tensorflow/issues/86172,"Hi  , Can you please review this PR? Thank you ! "
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-29T23:54:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86168
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-29T23:44:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86167
opt,copybara-service[bot],Add some optional verbosity to the dynamic loading lib,Add some optional verbosity to the dynamic loading lib,2025-01-29T21:36:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86157
tpu,copybara-service[bot],[XLA:CPU] Add CPU client support for layout modes.,"[XLA:CPU] Add CPU client support for layout modes. The main motivation for this change is to support userspecified input and output layouts for JAX interoperability with other libraries. For example, https://github.com/jaxml/jax/issues/25066. The logic is moreorless a direct copy of the implementation in `PjRtStreamExecutorClient`.",2025-01-29T19:36:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86124
tpu,zainZayam,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.," Issue type Others  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version  2.18.0  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version Python version: 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an error when trying to import TensorFlow. The error message is: ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Multiple times reinstalled it Downgraded my Python, still no sucess.  Standalone code to reproduce the issue ```shell Traceback (most recent call last):   File ""C:\Users\SAM\anaconda3\envs\tf_env\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File """", line 1, in    File ""C:\Users\SAM\anaconda3\envs\tf_env\lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport   File ""C:\Users\SAM\anaconda3\envs\tf_env\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 85, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\SAM\anaconda3\envs\tf_env\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```  Relevant log output ```shell ```",2025-01-29T19:28:01Z,stat:awaiting response type:build/install type:others TF 2.18,closed,0,2,https://github.com/tensorflow/tensorflow/issues/86111,", Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: ```  You need to install the MSVC 2019 redistributable  Your CPU does not support AVX2 instructions  Your CPU/Python is on 32 bits  There is a library that is in a different location/not installed on your system that cannot be loaded. ``` Also kindly provide the environment details and the steps followed to install the tensorflow. CC(Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.) Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Are you satisfied with the resolution of your issue? Yes No
tpu,rsemihkoca,"Mac Air M2, TfLiteGpuDelegate, Cpp, C++ Building from source"," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.19.0  Custom code No  OS platform and distribution Macbook air m2 Macos Sequia   Mobile device _No response_  Python version 3.12  Bazel version 8.0.1  GCC/compiler version Apple clang version 16.0.0 (clang1600.0.26.6)  CUDA/cuDNN version _No response_  GPU model and memory Appledesigned integrated GPU 8GB  Current behavior? I want to be able to inference using tflite gpu delegate on m2. First of all, I wonder if this is possible. https://www.tensorflow.org/install/source?hl=trinstall_gpu_support_optional_linux_only says tf do not have officail gpu support and apple give us metal plugin. My ultimate goal is to first debug on macbook and then use tflite gpu delegate for inference on iphone 6s. I am writing an sdk for this. But I could not build it for macbook. here is the build script i use for. ```sh bazel clean expunge bazel build config=macos_arm64 \     c opt \     config=nogcp  config=nonccl \     repo_env=HERMETIC_PYTHON_VERSION=3.12 \     define tflite_with_gpu=true \     define tflite_with_metal=true \     cxxopt=std=c++17 \     cxxopt=stdlib=libc++ \     host_cxxopt=std=c++17 \     linkopt=stdlib=libc++ \     copt=std=c++17 \     copt=stdlib=libc++ \     //tensorflow/lite/delegates/gpu:metal_delegate \     //tensorflow/lite:libtensorflowlite.dylib \     //tensorflow/lite/delegates/gpu:metal_delegate.h ``` i have added logs this happend because of  copt=std=c++17 \ but when i remove it then i get ``` ERROR: /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/tensorflow/lite/delegates/gpu/metal/BUILD:68:13: Compiling tensorflow/lite/delegates/gpu/metal/common.mm failed: (Exit 1): wrapped_clang_pp failed: error executing command (from target //tensorflow/lite/delegates/gpu/metal:common) external/local_config_cc/wrapped_clang_pp target arm64applemacosx11.0 'stdlib=libc++' 'std=gnu++11' 'D_FORTIFY_SOURCE=1' fstackprotector fcolordiagnostics Wall Wthreadsafety Wselfassign ... (remaining 50 arguments skipped) In file included from tensorflow/lite/delegates/gpu/metal/common.mm:16: In file included from ./tensorflow/lite/delegates/gpu/metal/common.h:25: In file included from ./tensorflow/lite/delegates/gpu/common/status.h:19: In file included from external/com_google_absl/absl/status/status.h:58: In file included from external/com_google_absl/absl/functional/function_ref.h:53: In file included from external/com_google_absl/absl/base/attributes.h:37: In file included from external/com_google_absl/absl/base/config.h:86: external/com_google_absl/absl/base/policy_checks.h:79:2: error: ""C++ versions less than C++14 are not supported.""    79  185.00 KiB/s, done. remote: Total 9 (delta 7), reused 9 (delta 7), packreused 0 (from 0) From https://github.com/tensorflow/tensorflow  * [new branch]              exported_pr_714978844 > origin/exported_pr_714978844 Already up to date. Initializing and updating TensorFlow submodules... Building TensorFlow Lite for macOS (ARM64)... INFO: Reading 'startup' options from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=88 INFO: Reading rc options for 'clean' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'clean' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:   Inherited 'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Found applicable config definition build:short_logs in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:macos in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: apple_platform_type=macos copt=DGRPC_BAZEL_BUILD features=archive_param_file copt=w define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=no_tfrt INFO: Found applicable config definition build:no_tfrt in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils INFO: Starting clean (this may take a while). Consider using async if the clean takes more than several minutes. Starting local Bazel server and connecting to it... INFO: Reading 'startup' options from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=88 INFO: Reading rc options for 'build' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:   'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Found applicable config definition build:short_logs in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:macos_arm64 in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: cpu=darwin_arm64 macos_minimum_os=11.0 INFO: Found applicable config definition build:nogcp in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: define=no_gcp_support=true INFO: Found applicable config definition build:nonccl in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: define=no_nccl_support=true INFO: Found applicable config definition build:macos in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: apple_platform_type=macos copt=DGRPC_BAZEL_BUILD features=archive_param_file copt=w define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=no_tfrt INFO: Found applicable config definition build:no_tfrt in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils DEBUG: /private/var/tmp/_bazel_rsemihkoca/2f937d813e0974bf03f44421dae0324c/external/local_tsl/third_party/py/python_repo.bzl:87:10:  ============================= Hermetic Python configuration: Version: ""3.12"" Kind: """" Interpreter: ""default"" (provided by rules_python) Requirements_lock label: ""//:requirements_lock_3_12.txt"" ===================================== INFO: Analyzed 3 targets (157 packages loaded, 6973 targets configured). INFO: Found 3 targets... ERROR: /private/var/tmp/_bazel_rsemihkoca/2f937d813e0974bf03f44421dae0324c/external/fft2d/BUILD.bazel:27:11: Compiling fftsg2d.c failed: (Exit 1): wrapped_clang failed: error executing command (from target //:fft2d) external/local_config_cc/wrapped_clang 'D_FORTIFY_SOURCE=1' fstackprotector fcolordiagnostics Wall Wthreadsafety Wselfassign fnoomitframepointer g0 O2 DNDEBUG 'DNS_BLOCK_ASSERTIONS=1' ... (remaining 31 arguments skipped) error: invalid argument 'std=c++17' not allowed with 'C' Error in child process '/usr/bin/xcrun'. 1 INFO: Elapsed time: 69.199s, Critical Path: 2.88s INFO: 316 processes: 296 internal, 20 local. FAILED: Build did NOT complete successfully Locating macOS build output... INFO: Reading 'startup' options from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'info' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'info' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:   Inherited 'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Found applicable config definition build:short_logs in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:macos in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: apple_platform_type=macos copt=DGRPC_BAZEL_BUILD features=archive_param_file copt=w define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=no_tfrt INFO: Found applicable config definition build:no_tfrt in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils DEBUG: /private/var/tmp/_bazel_rsemihkoca/2f937d813e0974bf03f44421dae0324c/external/local_tsl/third_party/py/python_repo.bzl:87:10:  ============================= Hermetic Python configuration: Version: ""3.12"" Kind: """" Interpreter: ""default"" (provided by rules_python) Requirements_lock label: ""//:requirements_lock_3_12.txt"" ===================================== Could not find macOS build output. Searching for alternatives... /private/var/tmp/_bazel_rsemihkoca/2f937d813e0974bf03f44421dae0324c/execroot/org_tensorflow/bazelout/darwin_arm64opt/bin/tensorflow/lite/libtensorflowlite.dylib.runfiles/org_tensorflow/tensorflow/lite/libtensorflowlite.dylib Error: Failed to locate macOS build output. ```",2025-01-29T18:21:23Z,stat:awaiting response type:support stale comp:lite TF 2.18,closed,0,8,https://github.com/tensorflow/tensorflow/issues/86075, ,"Hi,   I apologize for the delayed response, As far I know it is possible to use the TFLite GPU delegate on macOS with M2 chip.  You are correct that TensorFlow itself doesn't have official GPU support in the same way it does on Linux with CUDA.  However, Apple provides the Metal framework and the `tensorflowmetal` plugin (if needed building from source) allows TensorFlow Lite to leverage Metal for GPU acceleration please refer this official documentation of Get started with tensorflowmetal so I believe you're using `tensorflowmetal `plugin Could you please give it try with below command and see is it working as expected or not ? if not please help me with complete steps which you followed before encountering the mentioned errors to investigate this issue further from our end ? ``` bazel clean expunge bazel build config=macos_arm64 \     c opt \     config=nogcp config=nonccl \     repo_env=HERMETIC_PYTHON_VERSION=3.12 \   Or your preferred Python version     cxxopt=""std=c++17"" \     host_cxxopt=""std=c++17"" \     linkopt=""stdlib=libc++"" \  Only if you get linker errors related to stdlib     //tensorflow/lite:libtensorflowlite.dylib ``` Thank you for your cooperation and patience.","According to the file below: ``` //tensorflow/lite/delegates/gpu:tensorflow_lite_gpu_framework \ ``` needs to be added. However, when I add it this way, it is not sufficient. I get undefined symbols error but I understand that I also need to install `tensorflowmetal`. Is that correct? [TensorFlow GitHub  BUILD file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/BUILD) ```bazel  bazel build c opt cpu ios_arm64 copt Os copt DTFLITE_GPU_BINARY_RELEASE copt fvisibility=hidden linkopt s strip always cxxopt=std=c++14 :libtensorflowlite_gpu_metal apple_platform_type=ios ios_static_framework(     name = ""tensorflow_lite_gpu_framework"",     hdrs = [         ""metal_delegate.h"",         ""metal_delegate_internal.h"",     ],     minimum_os_version = ""12.0"",     deps = ["":metal_delegate""], ) ```",also we want to use with cpp but tensorflowmetal is only available for python. in python package there is   inflating: tensorflowplugins/libmetal_plugin.dylib   can we use that or will that be enough in order to use gpu on macos ?,"Hi,   I apologize for the delay in my response, Just to confirm by using below bazel command your build completed successfully or not ? No, you do not need to install the `tensorflowmetal` Python package for your C++ TensorFlow Lite build. Your understanding that it's only for Python is correct in this context. As far I know `tensorflow_lite_gpu_framework` target for` iOS` not for `macos_arm64` ``` 1. Clean previous attempts bazel clean expunge 2. Build TFLite core and Metal delegate for macOS ARM64 bazel build config=macos_arm64 \     c opt \     config=nogcp config=nonccl \     repo_env=HERMETIC_PYTHON_VERSION=3.12 \     define tflite_with_metal=true \     cxxopt=std=c++17 \     host_cxxopt=std=c++17 \     linkopt=stdlib=libc++ \     //tensorflow/lite:libtensorflowlite.dylib \     //tensorflow/lite/delegates/gpu:metal_delegate ``` If above command completes successfully you should find the necessary library files inside your **bazelbin** directory (the exact path will be shown in the build output often under **bazelbin/tensorflow/lite/** and **bazelbin/tensorflow/lite/delegates/gpu**). Look for `libtensorflowlite.dylib` (The core TFLite library) a library for the Metal delegate. Its name might be `metal_delegate.dylib`, `libmetal_delegate.dylib` or similar. You need to explicitly tell the linker to include both `libtensorflowlite.dylib` and the Metal delegate library when building your final `libFaceBlurCameraSDK.dylib` depends on how you are building your SDK If I have missed something here please let me know. Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],PR #21825: Exclude the usage of CPU memory from the GPU memory scheduler,"PR CC(TensorFlow lite android example simply does not sync or build.): Exclude the usage of CPU memory from the GPU memory scheduler Imported from GitHub PR https://github.com/openxla/xla/pull/21825 In the MaxText optimizer state offloading, we observed no memory savings when switching from f16 to f32. The root cause is that the GPU memory scheduler does not distinguish between CPU memory and GPU memory. This commit modifies the scheduler to exclude CPU memory. Copybara import of the project:  c77eefa1b4e31724dbfa40f4ab2aa7aff16e0840 by Jane Liu : Exclude the usage of CPU memory from the GPU memory scheduler  1c5720711c6a7d8173e132922b67eee6e2e8b9dd by Jane Liu : Add the explicit return type for the closure Merging this change closes CC(TensorFlow lite android example simply does not sync or build.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21825 from zhenyingliu:scheduler 1c5720711c6a7d8173e132922b67eee6e2e8b9dd",2025-01-29T18:09:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86070
yi,copybara-service[bot],[xla:gpu] [cleanup] Pull out some logic into IterableInput,"[xla:gpu] [cleanup] Pull out some logic into IterableInput This both simplifies the giant EmitMatmul function & makes it more generic, simplifying the TMA change (see CL chain).",2025-01-29T13:47:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86053
tpu,space2,Buffer allocation error in Tensorflow Lite with OpenCL backend on certain platforms," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.13  Custom code No  OS platform and distribution macOS  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I noticed a memory allocation error in clCreateBuffer. The issue seems to be caused by this: 1) TFlite tries to alloca 0xa2000000 bytes of memory (value stored as size_t) 2) The call ends up in this function (tensorflow/lite/experimental/litert/runtime/opencl/buffer.cc): ``` absl::Status CreateClBuffer(cl_context context, int size_in_bytes,                             bool read_only, void* data, cl_mem* result)  ``` where the size is now int (i.e. 32 bit signed integer... so 0xa2000000 is interpreted as a negative value). 3) This function then calls clCreateBuffer, which takes the size argument as size_t again, and thus receives 0xffffffffa2000000, i.e. the signed 32 bit integer first sign extended to 64bit and then interpreted as unsigned, and thus resulting in a huge size. The issue doesn't seem to appear with the same model on Android, probably because: The max buffer allocation size on macOS (M1) seems to be 9GB (according to clinfo), but on that android device it's only 1GB (so on android tflite never tries to allocate such a huge chunk of memory).  Standalone code to reproduce the issue ```shell Unfortunately I'm not allowed to share the code/model, but looking at the function signatures one can see the issue. ```  Relevant log output ```shell ERROR: Failed to allocate device memory (clCreateBuffer): Invalid buffer size ```",2025-01-29T12:11:28Z,stat:awaiting response type:bug comp:lite TF 2.13,closed,0,8,https://github.com/tensorflow/tensorflow/issues/86048,"Hi,   Please take a look into this issue. Thank you.","Hi , have you tried upgrading/updating to LiteRT? 2.13 is quite old at this point and there is a high chance your issue is already resolved by updating to the latest version."," I haven't tried upgrading, because unfortunately in our current project that's a bit complicated. But looking at the master branch on github, in file tensorflow/lite/experimental/litert/runtime/opencl/buffer.cc:31 in function CreateClBuffer the size_in_bytes argument is int, so there will be issues with >2GB allocations. In tag v2.18.0 the code seems to be moved to tensorflow/lite/delegates/gpu/cl/util.cc:180, but the argument type is still int.",I believe this PR will fix your issue: https://github.com/tensorflow/tensorflow/pull/87584. Thanks for your help.,"Hi , the change is now merged, please feel free to test and let us know if it works.","Thank you! Will take a while to test it (we need to rebase tflite, but due to local changes that is complicated). But the fix looks good, so I think the issue can be closed."," thanks for the confirmation, please feel free to close if there is no open items left as we generally cannot close the issues ourselves. Thanks.",Are you satisfied with the resolution of your issue? Yes No
tpu,enthusiast666,Building static c library for xtensa DSP processors, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.8  Custom code No  OS platform and distribution ubuntu 20.04  Mobile device ubuntu 20.04  Python version 3.10.10  Bazel version 3.1.0  GCC/compiler version 9.4.0  CUDA/cuDNN version   GPU model and memory   Current behavior? could someone suggest on how to build c libraries(.so or .a) for xtensa based processors or DSPs for linking with the applications ??  Standalone code to reproduce the issue ```shell compiler xtensacore=xyz Wall pedantic Werror Wextra Wnounusedparameter *.c L xtensaspecificlibs I /tensorflow/third_party/xla/ I /tensorflow/ I /tensorflow/third_party/xla/third_party/tsl/ nostdlib I /tensorflow/lite/ I /tensorflow/lite/core/api/ I /tensorflow/ lc lxmem ```  Relevant log output ```shell : dangerous relocation: cannot decode instruction opcode when building library.  undefined reference to `TF_NewGraph'... when try running the application without tf library ```,2025-01-29T11:46:06Z,stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/86045,"Hi,   I apologize for the delayed response, I understand you're looking for a concrete example, tutorial or Github Repo.  Unfortunately, complete publicly available Github repo of highly optimized DSP kernels for Xtensa HiFi processors integrated with TFLM are rare. This type of code is often considered proprietary and a key competitive advantage. However, you can refer this blog : Accelerating TensorFlow Lite Micro on Cadence Audio Digital Signal Processors and this example. As you've already created a corresponding issue in the tensorflow/tflitemicro repository (https://github.com/tensorflow/tflitemicro/issues/3045) which is the most appropriate repo for this type of query, we recommend closing this issue here and continuing the discussion there. This will help consolidate the discussion and ensure your query reaches the appropriate experts within the tensorflow/tflitemicro repository, where you are most likely to receive further assistance. Please refer this https://github.com/tensorflow/tflitemicro/issues/3045issuecomment2699709459 if you've any further questions please feel free to post in this issue thread https://github.com/tensorflow/tflitemicro/issues/3045 Thank you for your understanding and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,AD-lite24,Tensorflow lite cross compilation to aarch64 failing," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.1  Custom code No  OS platform and distribution Host: x86 Ubuntu 20.04  Mobile device target: aarch64  Python version N/A  Bazel version N/A  GCC/compiler version gcc 9  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? Build fails due to an exec format error with `protoc` ``` [ 97%] Built target protoc /usr/bin/make  f profiling/proto/CMakeFiles/profiling_info_proto.dir/build.make profiling/proto/CMakeFiles/profiling_info_proto.dir/depend make[2]: Entering directory '/home/root/build64' [ 98%] Generating profiling_info.pb.cc, profiling_info.pb.h cd /home/root/build64/profiling/proto && protoc cpp_out=/home/root/build64/profiling/proto proto_path=/home/root/tensorflow/tensorflow/lite/profiling/proto /home/root/tensorflow/tensorflow/lite/profiling/proto/profiling_info.proto /bin/sh: 1: protoc: Exec format error make[2]: *** [profiling/proto/CMakeFiles/profiling_info_proto.dir/build.make:75: profiling/proto/profiling_info.pb.cc] Error 2 make[2]: Leaving directory '/home/root/build64' make[1]: *** [CMakeFiles/Makefile2:7385: profiling/proto/CMakeFiles/profiling_info_proto.dir/all] Error 2 make[1]: Leaving directory '/home/root/build64' make: *** [Makefile:136: all] Error 2 ``` The only possible explanation for this seems to be that that the cmake build process is attempting to use `protoc` for this step  ``` [ 98%] Generating profiling_info.pb.cc, profiling_info.pb.h cd /home/root/build64/profiling/proto && protoc cpp_out=/home/root/build64/profiling/proto proto_path=/home/root/tensorflow/tensorflow/lite/profiling/proto /home/root/tensorflow/tensorflow/lite/profiling/proto/profiling_info.proto ``` But since `protoc` was built using the cross compiler tool chain it is meant for aarch64 while my host machine is trying to run it. This is likely a bug with the cross compilation process and in that case, please suggest a fix. I am not sure to what extent `protoc` is used in the build so any fix I would make cannot be completely correct.  Edit: I installed the x86 version for `protoc` separately, specifically the version 3.21.x which is the exact version that the tflite build process creates (3.21.9) and I am still facing version incompatibility issues  ``` /home/root/build64/profiling/proto/profiling_info.pb.h:17:2: error: error This file was generated by an older version of protoc which is    17   ^~~~~ ```  Standalone code to reproduce the issue ```shell `cmake DCMAKE_TOOLCHAIN_FILE=/opt/cross_toolchain/aarch64gnu9.toolchain.cmake DTFLITE_ENABLE_GPU=ON DTFLITE_ENABLE_NNAPI=ON DXNNPACK_ENABLE_ARM_BF16=OFF DXNNPACK_ENABLE_ARM_I8MM=OFF DCMAKE_CXX_FL<CXX_FLAGS=""${CMAKE_CXX_FLAGS} std=c++11"" ../tensorflow/tensorflow/lite` Using the default Cmake build process. ```  Relevant log output ```shell ```",2025-01-29T11:42:26Z,type:build/install comp:lite subtype: ubuntu/linux 2.17,closed,0,2,https://github.com/tensorflow/tensorflow/issues/86044,"Figured it out. Apparently protobuf 3.21.12 is significantly different from 3.21.9 and there is no release for 3.21.9 so need to build it from source. Still there is a bug with with the cross compilation process that should be resolved. I will try to create a PR if I end up writing a seamless solution, but for now the workaround of manually building protobuf works. Closing the issue for now but the issue has not been resolved.",Are you satisfied with the resolution of your issue? Yes No
tpu,DamarXCV,Input pipeline with RaggedTensor no longer working in +2.16 - No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.1 LTS  Mobile device _No response_  Python version 3.11 & 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In version 2.15 and earlier i was able to have a batched `tf.data.Dataset` with `tf.RaggedTensor` as input for `model.fit` and `model.predict`, with `tf.keras.layers.Resizing` as the first layer of the model. This is no longer works in +2.16 and Keras 3 (Edit: not Keras 2). The error log contains `RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible`, which implies a missing implementation.  Standalone code to reproduce the issue ```shell import tensorflow as tf ds = tf.data.Dataset.from_tensor_slices(range(10)) \     .map(lambda x: (         tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),         0,     )) \     .batch(batch_size=4) \     .prefetch(tf.data.AUTOTUNE) model = tf.keras.Sequential([     tf.keras.layers.Resizing(height=10, width=10),     tf.keras.layers.GlobalMaxPool2D(),     tf.keras.layers.Softmax(), ]) model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy()) model.fit(ds) ```  Relevant log output ```shell I0000 00:00:1738149672.657535   10590 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7070 MB memory:  > device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:0b:00.0, compute capability: 6.1 [...]/.venv/lib/python3.12/sitepackages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis 1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?   warnings.warn( [...]/.venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py:82: UserWarning: The model does not have any trainable weights.   warnings.warn(""The model does not have any trainable weights."") WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1738149672.931281   10665 service.cc:148] XLA service 0x79b50c0038a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: I0000 00:00:1738149672.931302   10665 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1 20250129 12:21:12.940706: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at if_op.cc:291 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[] on XLA_GPU_JIT: RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible with node {{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}){{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}} The op is created at:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 	tf2xla conversion failed while converting sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. Stack trace for op definition:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 20250129 12:21:12.940972: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[] on XLA_GPU_JIT: RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible with node {{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}){{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}} The op is created at:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 	tf2xla conversion failed while converting sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. Stack trace for op definition:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 	 [[sequential_1/resizing_1/RaggedResizeImages/cond]] 	tf2xla conversion failed while converting __inference_one_step_on_data_290[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. 20250129 12:21:12.941007: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[] on XLA_GPU_JIT: RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible with node {{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}){{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}} The op is created at:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 	tf2xla conversion failed while converting sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. Stack trace for op definition:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 	 [[sequential_1/resizing_1/RaggedResizeImages/cond]] 	tf2xla conversion failed while converting __inference_one_step_on_data_290[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. 	 [[StatefulPartitionedCall]] Traceback (most recent call last):   File ""[...]/test.py"", line 43, in      model.fit(ds)   File ""[...]/.venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""[...]/.venv/lib/python3.12/sitepackages/tensorflow/python/eager/execute.py"", line 53, in quick_execute     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error: Detected at node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant defined at (most recent call last):  Detected at node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant defined at (most recent call last):  Detected unsupported operations when trying to compile graph sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[] on XLA_GPU_JIT: RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible with node {{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}){{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}} The op is created at:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 	tf2xla conversion failed while converting sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. Stack trace for op definition:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 	 [[sequential_1/resizing_1/RaggedResizeImages/cond]] 	tf2xla conversion failed while converting __inference_one_step_on_data_290[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. 	 [[StatefulPartitionedCall]] [Op:__inference_multi_step_on_iterator_298] ```",2025-01-29T11:40:28Z,type:bug comp:keras TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/86043,"Hi **** , Apologies for the delay, and thank you for raising your concern here. The main cause of your issue is the Keras installation. Starting from TensorFlow version 2.16.0, it defaults to Keras 3. If you want to use Keras 2, you need to install it manually. This is also mentioned in the documentation. I installed everything as required, and it is working fine for me. Here, I am providing a gist for your reference. Thank you!"," Thank you for your reply. Sorry, i meant Keras 3 not Keras 2, i corrected it in my original post. If i console log the version it prints `3.8.0` for Keras with the following code ``` import tensorflow as tf print(tf.__version__) print(tf.keras.__version__) ds = tf.data.Dataset.from_tensor_slices(range(10)) \     .map(lambda x: (         tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),         0,     )) \     .batch(batch_size=4) \     .prefetch(tf.data.AUTOTUNE) model = tf.keras.Sequential([     tf.keras.layers.Resizing(height=10, width=10),     tf.keras.layers.GlobalMaxPool2D(),     tf.keras.layers.Softmax(), ]) model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy()) model.fit(ds) ``` I know i should use the standalone keras import, as documented in the migration guide, but even i do so it still crashes with the error from my initial post ``` import keras import tensorflow as tf print(tf.__version__) print(keras.__version__) ds = tf.data.Dataset.from_tensor_slices(range(10)) \     .map(lambda x: (         tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),         0,     )) \     .batch(batch_size=4) \     .prefetch(tf.data.AUTOTUNE) model = keras.Sequential([     keras.layers.Resizing(height=10, width=10),     keras.layers.GlobalMaxPool2D(),     keras.layers.Softmax(), ]) model.compile(loss=keras.losses.SparseCategoricalCrossentropy()) model.fit(ds) ``` The printed versions are in both code snipets `2.18.0` and `3.8.0`. If i install tfkeras (aka Keras 2) with `pip install tfkeras` the following code works ``` import tf_keras import tensorflow as tf print(tf.__version__) print(tf_keras.__version__) ds = tf.data.Dataset.from_tensor_slices(range(10)) \     .map(lambda x: (         tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),         0,     )) \     .batch(batch_size=4) \     .prefetch(tf.data.AUTOTUNE) model = tf_keras.Sequential([     tf_keras.layers.Resizing(height=10, width=10),     tf_keras.layers.GlobalMaxPool2D(),     tf_keras.layers.Softmax(), ]) model.compile(loss=tf_keras.losses.SparseCategoricalCrossentropy()) model.fit(ds) ``` But i would like to use Keras 3 and not the outdated Keras 2, which seems to not support `tf.RaggedTensor` as input for `keras.layers.Resizing`",I found kerasteam/keras CC(Add missing semicolon) which mentions  > No RaggedTensor support. We may add it back later. I guess that means that my input pipeline is not supported in Keras 3 for now.,Are you satisfied with the resolution of your issue? Yes No
tpu,oscar066,Title: ValueError when adding TensorFlow Hub KerasLayer to Sequential model," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.1  Custom code No  OS platform and distribution Google colab  Mobile device _No response_  Python version Python 3.11.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When attempting to add hub.KerasLayer to a tf.keras.Sequential model, TensorFlow raises a ValueError, stating that only instances of keras.Layer can be added. However, hub.KerasLayer is a subclass of keras.Layer, so this behavior seems unexpected. I expected hub.KerasLayer to be accepted as a valid layer in the tf.keras.Sequential model, as per the TensorFlow documentation.  Standalone code to reproduce the issue ```shell import tensorflow as tf import tensorflow_hub as hub mobilenet_v2 = ""https://tfhub.dev/google/tf2preview/mobilenet_v2/classification/4"" inception_v3 = ""https://tfhub.dev/google/imagenet/inception_v3/classification/5"" classifier_model = mobilenet_v2    [""mobilenet_v2"", ""inception_v3""] {type:""raw""} IMAGE_SHAPE = (224, 224) classifier = tf.keras.Sequential([     hub.KerasLayer(classifier_model, input_shape=IMAGE_SHAPE + (3,)) ]) link to notebook: ""https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning_with_hub.ipynb"" ```  Relevant log output ```shell  ValueError                                Traceback (most recent call last)  in ()       1 IMAGE_SHAPE = (224, 224)       2  > 3 classifier = tf.keras.Sequential([       4     hub.KerasLayer(classifier_model, input_shape=IMAGE_SHAPE+(3,))       5 ]) 1 frames /usr/local/lib/python3.11/distpackages/keras/src/models/sequential.py in add(self, layer, rebuild)      94                 layer = origin_layer      95         if not isinstance(layer, Layer): > 96             raise ValueError(      97                 ""Only instances of `keras.Layer` can be ""      98                 f""added to a Sequential model. Received: {layer} "" ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received:  (of type ) ```",2025-01-29T11:01:56Z,type:bug comp:keras 2.17,closed,0,7,https://github.com/tensorflow/tensorflow/issues/86041,", Hi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands. ```python !pip install tfkeras import tf_keras as keras ``` Also I have changed some steps like  modifying **tf_keras/keras.Sequential** instead of **tf.keras.Sequential** and the code was executed without error/fail. Kindly find the gist of it here. Thank you!","Thank you. On Thu, 30 Jan 2025, 3:26 pm Tilak, ***@***.***> wrote: >  , > 48 , > Hi, By default the colab notebook is using tensorflow v2.17 which contains > keras3.0 which was causing the error. Could you please try to import > keras2.0 with the below commands. > > !pip install tfkeras > import tf_keras as keras > > Also I have changed some steps like modifying *tf_keras/keras.Sequential* > instead of *tf.keras.Sequential* and the code was executed without > error/fail. Kindly find the gist of it here >  > . > > Thank you! > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","The solution where I installed tf_keras worked for that section, but I’m encountering a similar error in the ""Attach a classification head"" section of the same notebook. However, the previous solution does not seem to work in this case. Here is the code block : feature_extractor_layer = hub.KerasLayer(     feature_extractor_model,     input_shape=(224, 224, 3),     trainable=False) num_classes = len(class_names) model = tf.keras.Sequential([   feature_extractor_layer,   tf.keras.layers.Dense(num_classes) ]) model.summary() the error :  ValueError                                Traceback (most recent call last) [](https://localhost:8080/) in ()       1 num_classes = len(class_names)       2  > 3 model = tf.keras.Sequential(       4   feature_extractor_layer,       5   tf.keras.layers.Dense(num_classes) 1 frames [/usr/local/lib/python3.11/distpackages/keras/src/models/sequential.py in add(self, layer, rebuild)      95                 layer = origin_layer      96         if not isinstance(layer, Layer): > 97             raise ValueError(      98                 ""Only instances of `keras.Layer` can be ""      99                 f""added to a Sequential model. Received: {layer} "" ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received:  (of type ) Error after trying using tf_keras:  TypeError                                 Traceback (most recent call last) [](https://localhost:8080/) in ()       1 num_classes = len(class_names)       2  > 3 model = keras.Sequential(       4   feature_extractor_layer,       5   tf.keras.layers.Dense(num_classes) 2 frames [/usr/local/lib/python3.11/distpackages/tf_keras/src/engine/sequential.py in add(self, layer)     175                 layer = functional.ModuleWrapper(layer)     176         else: > 177             raise TypeError(     178                 ""The added layer must be an instance of class Layer. ""     179                 f""Received: layer={layer} of type {type(layer)}."" TypeError: The added layer must be an instance of class Layer. Received: layer= of type . Link to notebook: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning_with_hub.ipynb",", Could you try to modify the tf.keras to keras and execute the code.  I have changed some steps like modifying tf_keras/keras.Sequential instead of tf.keras.Sequential and the code was executed without error/fail. Kindly find the gist of it here. Thank you!","the gist notebook executed successfully however am still getting the error on this machine : here is the code : (raw_train, raw_validation, raw_test) , metadata = tfds.load(     'cats_vs_dogs',     split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],     with_info=True,     as_supervised=True, ) num_examples = metadata.splits['train'].num_examples num_classes = metadata.features['label'].num_classes print(num_examples) print(num_classes) BATCH_SIZE = 32 train_batches = raw_train.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE) validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1) test_batches = raw_test.map(format_image).batch(1) for  image_batch, label_batch in train_batches.take(1):     pass image_batch.shape module_selection = (""mobilenet_v2_100_224"", 224, 1280)   Updated module name handle_base, pixels, FV_SIZE = module_selection  Use the correct nonpreview URL MODULE_HANDLE = ""https://tfhub.dev/google/tf2preview/mobilenet_v2/feature_vector/4"".format(handle_base) IMAGE_SIZE = (pixels, pixels) feature_extractor = hub.KerasLayer(     MODULE_HANDLE,     input_shape=(224, 224, 3),     trainable=False ) model = keras.Sequential([     feature_extractor,     tf.keras.layers.Dense(num_classes, activation='softmax') ]) and here is the error:  TypeError                                 Traceback (most recent call last) Cell In[5], line 1 > 1 model = keras.Sequential(       [2     feature_extractor,       3     tf.keras.layers.Dense(num_classes, activation='softmax')       4 ])       6 model.summary() File /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/sitepackages/tensorflow/python/trackable/base.py:204, in no_automatic_dependency_tracking.._method_wrapper(self, *args, **kwargs)     202 self._self_setattr_tracking = False   pylint: disable=protectedaccess     203 try: > 204   result = method(self, *args, **kwargs)     205 finally:     206   self._self_setattr_tracking = previous_value   pylint: disable=protectedaccess File /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/sitepackages/tf_keras/src/utils/traceback_utils.py:70, in filter_traceback..error_handler(*args, **kwargs)      67     filtered_tb = _process_traceback_frames(e.__traceback__)      68      To get the full stack trace, call:      69      `tf.debugging.disable_traceback_filtering()` > 70     raise e.with_traceback(filtered_tb) from None      71 finally:      72     del filtered_tb File /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/sitepackages/tf_keras/src/engine/sequential.py:177, in Sequential.add(self, layer) ...     180     )     182 tf_utils.assert_no_legacy_layers([layer])     183 if not self._is_layer_name_unique(layer): TypeError: The added layer must be an instance of class Layer. Received: layer= of type .",The issue has been resolved. Installing tfkeras and using keras from it instead of tf.keras fixed the problem. Thank you!,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],[XLA:GPU] Introduce a separate option which would control falling back to the default layout in HLO parser just for entry_computation_layout.,[XLA:GPU] Introduce a separate option which would control falling back to the default layout in HLO parser just for entry_computation_layout.,2025-01-29T10:58:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86040
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-29T10:41:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86039
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-29T10:34:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86038
tpu,copybara-service[bot],PR #21800: [XLA:GPU] Add block scaling rewriter pass,"PR CC(Custom tensorflow op with output shape determined by the input tensor): [XLA:GPU] Add block scaling rewriter pass Imported from GitHub PR https://github.com/openxla/xla/pull/21800 This PR adds a transformation pass that supports custom calls to block quantize/dequantize/dot ops. Such calls are replaced by an equivalent sequence of HLO operations. This pass is supposed to support MX scaling formats, such as MXFP8, but is not limited to those and can be used with any data types and block sizes. The quantization op sequence matches the one described in the section 6.3 of the MX spec: https://www.opencompute.org/documents/ocpmicroscalingformatsmxv10specfinalpdf Once cuDNN frontend 1.10 is released, a lowering to a cuDNN graph will be enabled for the hardware that supports block scaled dot natively (i.e. Blackwell). This pass will stay disabled until then. I also plan on introducing a new HLO op, ""blockscaleddot"", which will be more generic than a custom call  for example, will have configurable dimensions numbers akin to the general dot op. This will follow in a separate PR, once that is approved, I'll replace the custom call ""__op$block_scaled_dot"" with it. Copybara import of the project:  5dcc610e804e7aaad9b79369f714a63f9f096ad8 by Sergey Kozub : Add block scaling rewriter pass Merging this change closes CC(Custom tensorflow op with output shape determined by the input tensor) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21800 from openxla:skozub/block_scaling 5dcc610e804e7aaad9b79369f714a63f9f096ad8",2025-01-29T08:27:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86011
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets. Reverts 8b90c7755e298136842b0a952ace340e5b535d23,2025-01-29T05:53:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86007
yi,copybara-service[bot],Refactor XLA's common.bara.sky to make copying of top level files and dirs more terse,"Refactor XLA's common.bara.sky to make copying of top level files and dirs more terse This is in preparation for introducing the concept of a ""moveonly"" file explicitly",2025-01-29T00:58:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85996
opt,copybara-service[bot],Set CPU as the default acceleration option for a Compiled Model,Set CPU as the default acceleration option for a Compiled Model,2025-01-28T23:59:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85992
yi,copybara-service[bot],Use efficient packed flatbuffer api to handle underlying tfl models.,Use efficient packed flatbuffer api to handle underlying tfl models.,2025-01-28T23:43:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85991
yi,copybara-service[bot],Return arrays from `ArrayImpl._check_and_rearrange`.,"Return arrays from `ArrayImpl._check_and_rearrange`. This is in preparation for a larger change, so that `_check_arrays` can be called before Array creation in XLA and the user gets more helpful JAX error messages instead of XLA errors. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e",2025-01-28T22:27:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85985
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-28T21:40:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85978
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets. Reverts a47a28e840cf97148669ba3483cd72e87f0efa5b,2025-01-28T21:31:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85977
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-28T20:20:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85973
opt,matteosal,Status of C/C++ APIs?,"Do they fully support running operators and models, gradients and optimizers? What's the relationship between them? Are there uptodate resources, tutorials etc on how to use them?",2025-01-28T17:07:43Z,stat:awaiting response type:support,closed,3,9,https://github.com/tensorflow/tensorflow/issues/85958,"Hi **** , Welcome to TensorFlow! Yes, TensorFlow fully supports running models, computing gradients, and using optimizers. These are fundamental building blocks for building and training machine learning models in TensorFlow. Here, I am providing the Doc1, Doc2 for your reference. If you have any further queries, please fill out all the required templates. Thank you!"," you have linked a page about Javascript, I asked for C/C++. I am asking this because it's hard to find clear information about them.  This document states at the bottom that the C API does not support gradients, but maybe that's for TF 1 so maybe it doesn't apply anymore. Then I found this C++ API reference page which just lists a bunch of symbols with no contextual explanation or introduction. Looking around there only seem to be some isolated gradient primitives but I couldn't find anything analogous to a generic method of running backpropagation through an arbitrary set of operators.","The C API is very limited, it's used to create bindings to other languages. The C++ API is used mostly for inference, I am not 100% it 100% covers training needs.","  > The C API is very limited, it's used to create bindings to other languages. Then it seems natural to expect it supports gradients, right? It's a core feature anyone wanting to create a binding for another language would need.",This is the entire C API. In particular it supports gradients.,"  Yes I noticed that function in the header. It has a note which states that not all gradients are supported. The link is broken but it's clear it's meant to lead here. This document states that implemented gradients should be declared as `Grad(...)`, and looking for this template in the entire directory produces these matches: ``` [Abs, Acos, Acosh, Add, AddN, Angle, Asin, Asinh, Atan, Atan2, Atanh, BaseFusedBatchNorm, BatchMatMul, BatchMatMulV2, BatchToSpace, BatchToSpaceND, BroadcastTo, Cast, CheckNumerics, ClipByValue, Complex, ConcatV2, Conj, Conv2D, Conv2DBackpropInput, Cos, Cosh, Cumsum, DepthToSpace, DepthwiseConv2dNative, Diag, DiagPart, Div, DivNoNan, DynamicPartition, DynamicStitch, Einsum, Erf, Erfinv, Exp, ExpandDims, Expm1, Fill, FusedBatchNormV3, GatherNd, GatherV2, Identity, Imag, Inv, L2Loss, Lgamma, Log, Log1p, LogSoftmax, MatMul, MatrixBandPart, MatrixDiag, Maximum, Mean, Minimum, MinOrMax, MirrorPad, MirrorPadGrad, Mul, Ndtri, Neg, Pack, Pad, PartitionedCall, Pow, Prod, QuantizeAndDequantize, QuantizeAndDequantizeV3, ReadVariableOp, Real, RealDiv, RefIdentity, Reshape, Reverse, ReverseSequence, Roll, Rsqrt, ScatterNd, ScatterNdNonAliasingAdd, SegmentSum, Select, SelectV2, Sigmoid, Sign, Sin, Sinh, Slice, Softmax, SoftmaxCrossEntropyWithLogits, SpaceToBatch, SpaceToBatchND, SpaceToDepth, Split, SplitV, Sqrt, Square, SquaredDifference, Squeeze, Sub, Sum, Tan, Tanh, Tile, Transpose, Unpack, UnsortedSegmentMinOrMax, UnsortedSegmentSum] ``` Several things are missing here, e.g. 1D and 3D Convolutions, ConvTranspose, Pooling, spatial resampling, RNNs. Can anyone confirm this is correct and the above gradients are the only ones available?","That is correct. PRs welcome to add more support if needed, though I think JAX is a better replacement.",Thank you,Are you satisfied with the resolution of your issue? Yes No
tpu,copybara-service[bot],PR #21800: [XLA:GPU] Add block scaling rewriter pass,"PR CC(Custom tensorflow op with output shape determined by the input tensor): [XLA:GPU] Add block scaling rewriter pass Imported from GitHub PR https://github.com/openxla/xla/pull/21800 This PR adds a transformation pass that supports custom calls to block quantize/dequantize/dot ops. Such calls are replaced by an equivalent sequence of HLO operations. This pass is supposed to support MX scaling formats, such as MXFP8, but is not limited to those and can be used with any data types and block sizes. The quantization op sequence matches the one described in the section 6.3 of the MX spec: https://www.opencompute.org/documents/ocpmicroscalingformatsmxv10specfinalpdf Once cuDNN frontend 1.10 is released, a lowering to a cuDNN graph will be enabled for the hardware that supports block scaled dot natively (i.e. Blackwell). This pass will stay disabled until then. I also plan on introducing a new HLO op, ""blockscaleddot"", which will be more generic than a custom call  for example, will have configurable dimensions numbers akin to the general dot op. This will follow in a separate PR, once that is approved, I'll replace the custom call ""__op$block_scaled_dot"" with it. Copybara import of the project:  5dcc610e804e7aaad9b79369f714a63f9f096ad8 by Sergey Kozub : Add block scaling rewriter pass Merging this change closes CC(Custom tensorflow op with output shape determined by the input tensor) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21800 from openxla:skozub/block_scaling bba9d3f711bf3b18ecdd45a4de4e96d422a2122a",2025-01-28T15:55:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85954
yi,copybara-service[bot],PR #21886: [ROCM][NFC] BlasLt interface refactoring & simplifying: part I,"PR CC(Tensorflow 1.10.0 MKL build (win64) with bazel throws linker error): [ROCM][NFC] BlasLt interface refactoring & simplifying: part I Imported from GitHub PR https://github.com/openxla/xla/pull/21886 After this PR https://github.com/tensorflow/tensorflow/pull/73926 is merged, we can remove unnecessary lowlevel DoMatmul functions from GpuBlasLt interface (which otherwise looks scary and unnecessarily complicated). Furthermore, we can also remove **ValidateInputs** function from the interface and derived classes since a highlevel **ExecuteOnStream** function already handles datatypes correctly. This also greatly simplifies the code. Also, I have packed the input arguments of ExecuteOnStream calls to a struct **MemoryArgs** to simplify arguments passing in derived classes and improve code readability. Finally, in the original GpuBlasLt PR: https://github.com/openxla/xla/pull/5911, I made a sort of mistake by adding a reference to **blas_lt** to the MatmulPlan class here, thereby making MatmulPlans bound to a **particular BlasLt instance**. This resulted in some further bugfixes and, most importantly, complicated GpuBlasLt cache design in gpublas_lt_matmul_thunk.cc/.h. In this PR, I remove this reference again from MatmulPlan class and in the next NFC PR the cache mechanics can also be simplified.  Unfortunately, this change also requires a tandem PR for Tensorflow: https://github.com/tensorflow/tensorflow/pull/85835 rotation Would you please have a look Copybara import of the project:  e96bb2fbedab3f53b31ef0e1748582c76e9fb105 by Pavel Emeliyanenko : blaslt interface refactoring: removing blas_lt_ref added cuda adaptions cudaside adaptions cuda side adaptions fix fixing pointers Merging this change closes CC(Tensorflow 1.10.0 MKL build (win64) with bazel throws linker error) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21886 from ROCm:ci_gpublas_lt_refactor_1 e96bb2fbedab3f53b31ef0e1748582c76e9fb105",2025-01-28T14:45:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85948
tpu,copybara-service[bot],[XLA:GPU] Triton support test - minor fixes,"[XLA:GPU] Triton support test  minor fixes * stop calling IsTritonSupportedInstruction explicitly, RunSupportTest calls it and verify that the output is in sync with actual behavior,  * call RunSupportTest for all ops when multiple are tested in a single test. Reverts a47a28e840cf97148669ba3483cd72e87f0efa5b",2025-01-28T13:14:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85945
opt,copybara-service[bot],[xla:cpu] Add more sort keywords to correctly order XLA:CPU debug options.,[xla:cpu] Add more sort keywords to correctly order XLA:CPU debug options.,2025-01-28T11:02:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85941
tpu,copybara-service[bot],Use NMS output to reduce output tensor sizes so invalid entries are never accessed,Use NMS output to reduce output tensor sizes so invalid entries are never accessed,2025-01-28T00:08:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85900
opt,copybara-service[bot],Introduce pipeline parallelism optimization level,Introduce pipeline parallelism optimization level Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-27T21:42:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85890
sharding,copybara-service[bot],"Build IFRT shardings with both addressable and non-addressable devices, instead of only addressable devices.","Build IFRT shardings with both addressable and nonaddressable devices, instead of only addressable devices.",2025-01-27T21:08:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85885
tpu,copybara-service[bot],Replace output_path with output_file in xla_compile_lib.,Replace output_path with output_file in xla_compile_lib. This way the error messages align with the flag the user passes to xla_compile.,2025-01-27T18:35:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85873
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-27T18:27:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85871
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-27T18:17:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85869
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-27T18:16:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85868
int8,copybara-service[bot],PR #21845: [ROCM] Add missing triton MLIR int4 -> int8 rewrite pass for ROCM,PR CC(未找到相关数据): [ROCM] Add missing triton MLIR int4 > int8 rewrite pass for ROCM Imported from GitHub PR https://github.com/openxla/xla/pull/21845 ``` TritonTest.DotWithInt4WeightsOnLhsFusedWithMultiplyByChannelScales TritonTest.NonstandardLayoutInt4 TritonTest.DotWithI4WeightsOnLhsWithBitcastTo3dTensor TritonTest.DotWithI4WeightsOnLhsWithNonStandardLayoutAndMultplyInEpilogue TritonTest.LHSWithMinorDimEqualTo1 TritonTest.RHSWithMinorDimEqualTo1 TritonTest.LHSNonMinorContractingDim TritonTest.LHSNonMinorContractingDimWithBatchDim0 TritonTest.LHSMinorContractingDim TritonTest.ConvertPlusNegate TritonTest.LHSMinorContractingDimWithBatchDim0 TritonTest.RHSTestWithNotMinorContractingDim TritonTest.RHSTestWithMinorContractingDim TritonTest.RHSTestWithMinorContractingDimWithBatchDim TritonTest.RHSTestWithNotMinorContractingDimWithBatchDim0 ParametrizedTritonTest.Int4WeightsOnTheLhs ParametrizedTritonTest.Int4WeightsOnTheLhsWithBatchDim ParametrizedTritonTest.Int4WeightsOnTheRhs ``` Tests above are failing on ROCm side after int4 rewriting was moved from legacy matmul emitter to MLIR pass. This MLIR pass is now missing in ROCm triton pipeline and I'm adding it in the place. rotation: would you please take a look?  Copybara import of the project:  75e78ad365a9d55f6e299c7b64400447ceebb26d by Jian Li : [ROCM] Add missing triton MLIR int4 > int8 rewrite pass for ROCM Merging this change closes CC(未找到相关数据) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21845 from ROCm:ci_fix_rocm_triton_test 75e78ad365a9d55f6e299c7b64400447ceebb26d,2025-01-27T18:14:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85867
tpu,copybara-service[bot],Open source TPU-specific input pipeline analysis.,Open source TPUspecific input pipeline analysis.,2025-01-27T17:36:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85860
opt,copybara-service[bot],PR #21375: [ds-fusion] Get While loop analysis with copy fusion,"PR CC(Raspberry Pi install command not properly formatted.): [dsfusion] Get While loop analysis with copy fusion Imported from GitHub PR https://github.com/openxla/xla/pull/21375 In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index. Copybara import of the project:  3147ec926aa1c6fdfa2f4376668434c9a2fbeb87 by Shraiysh Vaishay : [dsfusion] Get While loop analysis with copy fusion In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index.  a435fbd2eadc17269d7bccbe141dcf7a21cc20e8 by Shraiysh Vaishay : Relay control dependencies while converting fusion to call (extractor) Merging this change closes CC(Raspberry Pi install command not properly formatted.) Reverts a47a28e840cf97148669ba3483cd72e87f0efa5b FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21375 from shraiysh:while_loop_analysis a435fbd2eadc17269d7bccbe141dcf7a21cc20e8",2025-01-27T15:22:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85847
opt,copybara-service[bot],#litert Add accelerator options API.,litert Add accelerator options API.,2025-01-27T12:23:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85834
tpu,copybara-service[bot],[XLA:GPU][Emitters] Add TransposeSpec.,"[XLA:GPU][Emitters] Add TransposeSpec. TransposeSpec keeps the input, output shapes and permutation of the transpose + the canonical shapes and permutations, i.e. transpose from     into . Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e",2025-01-27T12:08:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85833
tpu,copybara-service[bot],Allow simple multi-output fusion in the generic Triton emitter.,"Allow simple multioutput fusion in the generic Triton emitter. By simple we mean that we only allow one fusion root without users. We also only allow cases where we can just reuse an existing tiling for the extra fusion roots that is derived from the tiling of the fusion root without users. This already covers quite a few cases, see the new test cases in fusion_emitter_device_test.cc",2025-01-27T09:28:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85829
bert,copybara-service[bot],PR #20155: Add support for emitting a kCall from an async instruction,"PR CC(Find NCCL2 debians in configure.py): Add support for emitting a kCall from an async instruction Imported from GitHub PR https://github.com/openxla/xla/pull/20155 This is the last PR needed to enable the explicit stream annotation feature. This PR enables the IR emitter for kCall subroutines that are wrapped in async{start,done} pairs. For start, the instructions in the kCall are converted to a `SequentialThunk`. This thunk and the inner thunks are then annotated with their respective `execution_stream_ids`. (These ids come from CC([Intel MKL] Finished support for bad usernames in the CI build scripts.))  Copybara import of the project:  41bb90e33eb751953712f0188ef931310a4c58cf by chaserileyroberts : Added support for emitting a kCall from an async instruction  bd8da7b1f3a644544d0050950fa3f08431f3d3ad by chaser : Add compiler test and a WaitFor thunk on main stream.  2d730d148da474fc7f2ddcb977a13ec582a304ec by chaser : Updates based on comments  9b7a8c161e13c5357115b8b6ecdad69033112fc8 by chaser : Updates based on jreiffers comments Merging this change closes CC(Find NCCL2 debians in configure.py) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20155 from chaserileyroberts:chase/async_kcall_emitter 9b7a8c161e13c5357115b8b6ecdad69033112fc8",2025-01-27T08:22:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85825
graph mode,copybara-service[bot],PR #21805: Fix typo in cli flag help message,PR CC(Unable to convert frozen graph model to required fromat): Fix typo in cli flag help message Imported from GitHub PR https://github.com/openxla/xla/pull/21805 Copybara import of the project:  ec366967d45ac5e781feb35b4d550341bfb1a194 by Zentrik : Fix typo in cli flag help message Merging this change closes CC(Unable to convert frozen graph model to required fromat) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21805 from Zentrik:patch1 ec366967d45ac5e781feb35b4d550341bfb1a194,2025-01-27T07:14:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85823
bert,copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20155 from chaserileyroberts:chase/async_kcall_emitter 9b7a8c161e13c5357115b8b6ecdad69033112fc8,2025-01-27T03:18:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85800
tpu,copybara-service[bot],Add an option to disable predict request output_filter.,Add an option to disable predict request output_filter.,2025-01-27T01:08:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85778
tpu,copybara-service[bot],Add an option to forcing skipping output_filter,Add an option to forcing skipping output_filter,2025-01-27T00:01:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85777
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-26T19:17:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85774
opt,copybara-service[bot],[xla:cpu] Remove code for computing optimal number of workers at run time,"[xla:cpu] Remove code for computing optimal number of workers at run time Instead of trying to figure out optimal number of workers at run time, we'd better have a cost model that can make this decision at compile time based on the XNNPACK fusion.",2025-01-26T18:21:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85772
opt,copybara-service[bot],[xla:cpu] Disable concurrency-optimized schedule.,[xla:cpu] Disable concurrencyoptimized schedule. Reverts f16f97e1f86c5b3ce04c644339b4aa17775f8d52,2025-01-25T01:21:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85705
sharding,copybara-service[bot],Reject invalid None in jax.NamedSharding(spec=None).,Reject invalid None in jax.NamedSharding(spec=None).,2025-01-24T23:25:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85698
sharding,copybara-service[bot],Support `mhlo.sharding` attr inside `backend_config` of ragged_all_to_all,Support `mhlo.sharding` attr inside `backend_config` of ragged_all_to_all,2025-01-24T21:11:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85692
tpu,LinuxPersonEC,"TF 2.18 with GPU does not detect GPU, Cannot dlopen some GPU libraries, in a container"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code Yes  OS platform and distribution Linux Centos 7.9, RHEL 8, RHEL 9  Mobile device _No response_  Python version 3.11.0rc1  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 550.90.07  GPU model and memory _No response_  Current behavior? After discussing this on the Apptainer Git we determined the latest TFGPU running 2.18.0 does not register any GPUs. Older versions like 2.7.1gpu work just fine. `apptainer run nv  /apps/Miniforge/lib/python3.12/sitepackages/containers/tensorflow/tensorflow/latestgpu/tensorflowtensorflowlatestgpusha256\:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif python` ``` import tensorflow as tf 20250124 15:03:27.629215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1737749008.639844   35316 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1737749008.847756   35316 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250124 15:03:31.499335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. ``` ``` print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) W0000 00:00:1737749068.599039   35316 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... Num GPUs Available:  0 ``` ``` >>> print(tf.__version__) 2.18.0 ```  Standalone code to reproduce the issue ```shell shpc install tensorflow/tensorflow:latestgpu or apptainer pull docker://tensorflow/tensorflow:latestgpu apptainer run nv  /apps/Miniforge/lib/python3.12/sitepackages/containers/tensorflow/tensorflow/latestgpu/tensorflowtensorflowlatestgpusha256\:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif python python import tensorflow as tf print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) ```  Relevant log output ```shell apptainer debug exec nv  /apps/Miniforge/lib/python3.12/sitepackages/containers/tensorflow/tensorflow/latestgpu/tensorflowtensorflowlatestgpusha256:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif python DEBUG   [U=0,P=1355208]    persistentPreRun()            Apptainer version: 1.3.61 DEBUG   [U=0,P=1355208]    persistentPreRun()            Parsing configuration file /etc/apptainer/apptainer.conf DEBUG   [U=0,P=1355208]    SetBinaryPath()               Setting binary path to /usr/libexec/apptainer/bin:/usr/share/Modules/bin:/usr/local/sbin:/sbin:/bin:/usr/sbin:/usr/bin:/opt/TurboVNC/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin DEBUG   [U=0,P=1355208]    SetBinaryPath()               Using that path for all binaries DEBUG   [U=0,P=1355208]    handleConfDir()               /root/.apptainer already exists. Not creating. DEBUG   [U=0,P=1355208]    handleRemoteConf()            Ensuring file permission of 0600 on /root/.apptainer/remote.yaml DEBUG   [U=0,P=1355208]    setUmask()                    Saving umask 0002 for propagation into container DEBUG   [U=0,P=1355208]    checkEncryptionKey()          Checking for encrypted system partition DEBUG   [U=0,P=1355208]    Init()                        Image format detection DEBUG   [U=0,P=1355208]    Init()                        Check for sandbox image format DEBUG   [U=0,P=1355208]    Init()                        sandbox format initializer returned: not a directory image DEBUG   [U=0,P=1355208]    Init()                        Check for sif image format DEBUG   [U=0,P=1355208]    Init()                        sif image format detected VERBOSE [U=0,P=1355208]    SetGPUConfig()                'always use nv = yes' found in apptainer.conf DEBUG   [U=0,P=1355208]    setNVLegacyConfig()           Using legacy binds for nv GPU setup VERBOSE [U=0,P=1355208]    NvidiaIpcsPath()              persistenced socket /var/run/nvidiapersistenced/socket not found DEBUG   [U=0,P=1355208]    findOnPath()                  Found ""ldconfig"" at ""/sbin/ldconfig"" DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SHELL environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_GID environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HISTCONTROL environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding no_proxy environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HOSTNAME environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HISTSIZE environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SBATCH_PARTITION environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_OUTPUT environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SLURM_PARTITION environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_COMMAND environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_USER environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_DIR environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding PWD environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LOGNAME environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULESHOME environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MANPATH environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_RESTORE environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding __MODULES_SHARE_MANPATH environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SSH_ASKPASS environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LANG environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LS_COLORS environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_SETTARG_FULL_SUPPORT environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_PS1 environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding https_proxy environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_VERSION environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULEPATH_ROOT environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_PKG environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding TERM environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LESSOPEN environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding USER environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding NO_PROXY environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULES_RUN_QUARANTINE environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LOADEDMODULES environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SHLVL environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_ENV environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_sys environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HTTPS_PROXY environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_INIT environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HTTP_PROXY environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding http_proxy environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding S_COLORS environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding __MODULES_LMINIT environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding which_declare environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding XDG_DATA_DIRS environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULEPATH environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_UID environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_CMD environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MAIL environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULES_CMD environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_ml%% environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_which%% environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_module%% environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_scl%% environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC__module_raw%% environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding _ environment variable VERBOSE [U=0,P=1355208]    SetContainerEnv()             Not forwarding APPTAINER_DEBUG environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding USER_PATH environment variable VERBOSE [U=0,P=1355208]    SetContainerEnv()             Setting HOME=/root VERBOSE [U=0,P=1355208]    SetContainerEnv()             Setting PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin DEBUG   [U=0,P=1355208]    InitImageDrivers()            Skipping installing fuseapps image driver because running as root DEBUG   [U=0,P=1355208]    SetuidMountAllowed()          Kernel squashfs mount allowed because running as root DEBUG   [U=0,P=1355208]    init()                        Use starter binary /usr/libexec/apptainer/bin/starter VERBOSE [U=0,P=1355208]    print()                       Set messagelevel to: 5 VERBOSE [U=0,P=1355208]    init()                        Starter initialization VERBOSE [U=0,P=1355208]    is_suid()                     Check if we are running as setuid: 0 DEBUG   [U=0,P=1355208]    read_engine_config()          Read engine configuration DEBUG   [U=0,P=1355208]    init()                        Wait completion of stage1 DEBUG   [U=0,P=1355224]    set_parent_death_signal()     Set parent death signal to 9 VERBOSE [U=0,P=1355224]    init()                        Spawn stage 1 DEBUG   [U=0,P=1355224]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter DEBUG   [U=0,P=1355224]    func1()                       starter was not relocated from /usr/libexec DEBUG   [U=0,P=1355224]    func1()                       Install prefix is /usr DEBUG   [U=0,P=1355224]    startup()                     apptainer runtime engine selected VERBOSE [U=0,P=1355224]    startup()                     Execute stage 1 DEBUG   [U=0,P=1355224]    StageOne()                    Entering stage 1 DEBUG   [U=0,P=1355224]    InitImageDrivers()            Skipping installing fuseapps image driver because running as root DEBUG   [U=0,P=1355224]    prepareRootCaps()             Root full capabilities DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/proc/sys/fs/binfmt_misc"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/home"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/share"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/misc"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/net"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/mnt/smb/locker"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/mnt/smb/labshare"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/mnt/smb/staging"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for bind path /etc/localtime: no mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for bind path /etc/hosts: no mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for home directory /root: no mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for current working directory /root: no mount point DEBUG   [U=0,P=1355224]    Init()                        Image format detection DEBUG   [U=0,P=1355224]    Init()                        Check for sandbox image format DEBUG   [U=0,P=1355224]    Init()                        sandbox format initializer returned: not a directory image DEBUG   [U=0,P=1355224]    Init()                        Check for sif image format DEBUG   [U=0,P=1355224]    Init()                        sif image format detected DEBUG   [U=0,P=1355224]    setSessionLayer()             Using overlay because it is not disabled DEBUG   [U=0,P=1355224]    PrepareConfig()               image driver is  VERBOSE [U=0,P=1355208]    wait_child()                  stage 1 exited with status 0 DEBUG   [U=0,P=1355208]    cleanup_fd()                  Close file descriptor 4 DEBUG   [U=0,P=1355208]    cleanup_fd()                  Close file descriptor 5 DEBUG   [U=0,P=1355208]    cleanup_fd()                  Close file descriptor 6 DEBUG   [U=0,P=1355208]    init()                        Set child signal mask DEBUG   [U=0,P=1355208]    init()                        Create socketpair for master communication channel DEBUG   [U=0,P=1355208]    init()                        Create RPC socketpair for communication between stage 2 and RPC server VERBOSE [U=0,P=1355208]    init()                        Spawn master process DEBUG   [U=0,P=1355230]    set_parent_death_signal()     Set parent death signal to 9 VERBOSE [U=0,P=1355230]    create_namespace()            Create mount namespace VERBOSE [U=0,P=1355208]    enter_namespace()             Entering in mount namespace DEBUG   [U=0,P=1355208]    enter_namespace()             Opening namespace file ns/mnt VERBOSE [U=0,P=1355230]    create_namespace()            Create mount namespace VERBOSE [U=0,P=1355231]    init()                        Spawn RPC server DEBUG   [U=0,P=1355208]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter DEBUG   [U=0,P=1355208]    func1()                       starter was not relocated from /usr/libexec DEBUG   [U=0,P=1355208]    func1()                       Install prefix is /usr DEBUG   [U=0,P=1355231]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter DEBUG   [U=0,P=1355231]    func1()                       starter was not relocated from /usr/libexec DEBUG   [U=0,P=1355231]    func1()                       Install prefix is /usr DEBUG   [U=0,P=1355208]    startup()                     apptainer runtime engine selected VERBOSE [U=0,P=1355208]    startup()                     Execute master process DEBUG   [U=0,P=1355231]    startup()                     apptainer runtime engine selected VERBOSE [U=0,P=1355231]    startup()                     Serve RPC requests DEBUG   [U=0,P=1355208]    InitImageDrivers()            Skipping installing fuseapps image driver because running as root DEBUG   [U=0,P=1355208]    setupSessionLayout()          Using Layer system: overlay DEBUG   [U=0,P=1355208]    setupOverlayLayout()          Creating overlay SESSIONDIR layout DEBUG   [U=0,P=1355208]    addRootfsMount()              Mount rootfs in readonly mode DEBUG   [U=0,P=1355208]    addRootfsMount()              Image type is 4096 DEBUG   [U=0,P=1355208]    addRootfsMount()              Mounting block [squashfs] image: /share/apps/Miniforge/lib/python3.12/sitepackages/containers/tensorflow/tensorflow/latestgpu/tensorflowtensorflowlatestgpusha256:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif DEBUG   [U=0,P=1355208]    addKernelMount()              Checking configuration file for 'mount proc' DEBUG   [U=0,P=1355208]    addKernelMount()              Adding proc to mount list VERBOSE [U=0,P=1355208]    addKernelMount()              Default mount: /proc:/proc DEBUG   [U=0,P=1355208]    addKernelMount()              Checking configuration file for 'mount sys' DEBUG   [U=0,P=1355208]    addKernelMount()              Adding sysfs to mount list VERBOSE [U=0,P=1355208]    addKernelMount()              Default mount: /sys:/sys DEBUG   [U=0,P=1355208]    addDevMount()                 Checking configuration file for 'mount dev' DEBUG   [U=0,P=1355208]    addDevMount()                 Adding dev to mount list VERBOSE [U=0,P=1355208]    addDevMount()                 Default mount: /dev:/dev DEBUG   [U=0,P=1355208]    addHostMount()                Not mounting host file systems per configuration VERBOSE [U=0,P=1355208]    addBindsMount()               Found 'bind path' = /etc/localtime, /etc/localtime VERBOSE [U=0,P=1355208]    addBindsMount()               Found 'bind path' = /etc/hosts, /etc/hosts DEBUG   [U=0,P=1355208]    addHomeStagingDir()           Staging home directory (/root) at /var/lib/apptainer/mnt/session/root DEBUG   [U=0,P=1355208]    addHomeMount()                Adding home directory mount [/var/lib/apptainer/mnt/session/root:/root] to list using layer: overlay DEBUG   [U=0,P=1355208]    addTmpMount()                 Checking for 'mount tmp' in configuration file DEBUG   [U=0,P=1355208]    addScratchMount()             Not mounting scratch directory: Not requested DEBUG   [U=0,P=1355208]    addLibsMount()                Checking for 'user bind control' in configuration file DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenCL.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenGL.so.0 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiacfg.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libEGL.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaeglcore.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaml.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvcuvid.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiagtk3.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaglvkspirv.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libcudadebugger.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv2.so.2 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaptxjitcompiler.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv2.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGL.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLX_nvidia.so.0 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv1_CM.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiatls.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLdispatch.so.0 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaencode.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenCL.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaptxjitcompiler.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaencode.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidianvvm.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaglsi.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaopticalflow.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaopencl.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaeglwayland.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLX.so.0 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvoptix.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiagpucomp.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGL.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv2_nvidia.so.2 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv1_CM_nvidia.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidianvvm.so.4 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libEGL.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiagtk2.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaml.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiafbc.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiafbc.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvcuvid.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaopticalflow.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLX.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv1_CM.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiacfg.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libcuda.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libvdpau_nvidia.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiartcore.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libcuda.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenGL.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libEGL_nvidia.so.0 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaglcore.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLdispatch.so to mount list DEBUG   [U=0,P=1355208]    addFilesMount()               Checking for 'user bind control' in configuration file DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidiapersistenced:/usr/bin/nvidiapersistenced to mount list DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidiacudampscontrol:/usr/bin/nvidiacudampscontrol to mount list DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidiacudampsserver:/usr/bin/nvidiacudampsserver to mount list DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidiasmi:/usr/bin/nvidiasmi to mount list DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidiadebugdump:/usr/bin/nvidiadebugdump to mount list DEBUG   [U=0,P=1355208]    addResolvConfMount()          Adding /etc/resolv.conf to mount list VERBOSE [U=0,P=1355208]    addResolvConfMount()          Default mount: /etc/resolv.conf:/etc/resolv.conf DEBUG   [U=0,P=1355208]    addHostnameMount()            Skipping hostname mount, not virtualizing UTS namespace on user request DEBUG   [U=0,P=1355208]    create()                      Mount all DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting tmpfs to /var/lib/apptainer/mnt/session DEBUG   [U=0,P=1355208]    mountImage()                  Mounting loop device /dev/loop0 to /var/lib/apptainer/mnt/session/rootfs of type squashfs DEBUG   [U=0,P=1355208]    createCwdDir()                Using /root as current working directory DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting overlay to /var/lib/apptainer/mnt/session/final DEBUG   [U=0,P=1355208]    mountGeneric()                Unmounting and remounting overlay DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final DEBUG   [U=0,P=1355208]    setPropagationMount()         Set RPC mount propagation flag to SLAVE VERBOSE [U=0,P=1355208]    Passwd()                      Checking for template passwd file: /var/lib/apptainer/mnt/session/rootfs/etc/passwd VERBOSE [U=0,P=1355208]    Passwd()                      Creating passwd content VERBOSE [U=0,P=1355208]    Passwd()                      Creating template passwd file and injecting user data: /var/lib/apptainer/mnt/session/rootfs/etc/passwd DEBUG   [U=0,P=1355208]    addIdentityMount()            Adding /etc/passwd to mount list VERBOSE [U=0,P=1355208]    addIdentityMount()            Default mount: /etc/passwd:/etc/passwd VERBOSE [U=0,P=1355208]    Group()                       Checking for template group file: /var/lib/apptainer/mnt/session/rootfs/etc/group VERBOSE [U=0,P=1355208]    Group()                       Creating group content DEBUG   [U=0,P=1355208]    addIdentityMount()            Adding /etc/group to mount list VERBOSE [U=0,P=1355208]    addIdentityMount()            Default mount: /etc/group:/etc/group DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /dev to /var/lib/apptainer/mnt/session/final/dev DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /etc/localtime to /var/lib/apptainer/mnt/session/final/etc/localtime DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/etc/localtime DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /etc/hosts to /var/lib/apptainer/mnt/session/final/etc/hosts DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/etc/hosts DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /proc to /var/lib/apptainer/mnt/session/final/proc DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/proc DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting sysfs to /var/lib/apptainer/mnt/session/final/sys DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /root to /var/lib/apptainer/mnt/session/root DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/root DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/root to /var/lib/apptainer/mnt/session/final/root DEBUG   [U=0,P=1355208]    func1()                       Container /tmp resolves to ""/tmp"" DEBUG   [U=0,P=1355208]    func1()                       Container /var/tmp resolves to ""/var/tmp"" VERBOSE [U=0,P=1355208]    func1()                       Default mount: /tmp:/tmp VERBOSE [U=0,P=1355208]    func1()                       Default mount: /var/tmp:/var/tmp DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /tmp to /var/lib/apptainer/mnt/session/final/tmp DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/tmp DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/tmp to /var/lib/apptainer/mnt/session/final/var/tmp DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/var/tmp DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenCL.so to /var/lib/apptainer/mnt/session/libs/libOpenCL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenCL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenGL.so.0 to /var/lib/apptainer/mnt/session/libs/libOpenGL.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenGL.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiacfg.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiacfg.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiacfg.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libEGL.so.1 to /var/lib/apptainer/mnt/session/libs/libEGL.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libEGL.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaeglcore.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiaeglcore.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaeglcore.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaml.so to /var/lib/apptainer/mnt/session/libs/libnvidiaml.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaml.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvcuvid.so.1 to /var/lib/apptainer/mnt/session/libs/libnvcuvid.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvcuvid.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiagtk3.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiagtk3.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiagtk3.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaglvkspirv.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiaglvkspirv.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaglvkspirv.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libcudadebugger.so.1 to /var/lib/apptainer/mnt/session/libs/libcudadebugger.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libcudadebugger.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv2.so.2 to /var/lib/apptainer/mnt/session/libs/libGLESv2.so.2 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv2.so.2 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaptxjitcompiler.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiaptxjitcompiler.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaptxjitcompiler.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv2.so to /var/lib/apptainer/mnt/session/libs/libGLESv2.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv2.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGL.so to /var/lib/apptainer/mnt/session/libs/libGL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLX_nvidia.so.0 to /var/lib/apptainer/mnt/session/libs/libGLX_nvidia.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLX_nvidia.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv1_CM.so.1 to /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiatls.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiatls.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiatls.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLdispatch.so.0 to /var/lib/apptainer/mnt/session/libs/libGLdispatch.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLdispatch.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaencode.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiaencode.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaencode.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenCL.so.1 to /var/lib/apptainer/mnt/session/libs/libOpenCL.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenCL.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaptxjitcompiler.so to /var/lib/apptainer/mnt/session/libs/libnvidiaptxjitcompiler.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaptxjitcompiler.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaencode.so to /var/lib/apptainer/mnt/session/libs/libnvidiaencode.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaencode.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidianvvm.so to /var/lib/apptainer/mnt/session/libs/libnvidianvvm.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidianvvm.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaglsi.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiaglsi.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaglsi.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaopticalflow.so to /var/lib/apptainer/mnt/session/libs/libnvidiaopticalflow.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaopticalflow.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaopencl.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiaopencl.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaopencl.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaeglwayland.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiaeglwayland.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaeglwayland.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLX.so.0 to /var/lib/apptainer/mnt/session/libs/libGLX.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLX.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvoptix.so.1 to /var/lib/apptainer/mnt/session/libs/libnvoptix.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvoptix.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiagpucomp.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiagpucomp.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiagpucomp.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGL.so.1 to /var/lib/apptainer/mnt/session/libs/libGL.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGL.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv2_nvidia.so.2 to /var/lib/apptainer/mnt/session/libs/libGLESv2_nvidia.so.2 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv2_nvidia.so.2 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv1_CM_nvidia.so.1 to /var/lib/apptainer/mnt/session/libs/libGLESv1_CM_nvidia.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv1_CM_nvidia.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidianvvm.so.4 to /var/lib/apptainer/mnt/session/libs/libnvidianvvm.so.4 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidianvvm.so.4 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libEGL.so to /var/lib/apptainer/mnt/session/libs/libEGL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libEGL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiagtk2.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiagtk2.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiagtk2.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaml.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiaml.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaml.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiafbc.so to /var/lib/apptainer/mnt/session/libs/libnvidiafbc.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiafbc.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiafbc.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiafbc.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiafbc.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvcuvid.so to /var/lib/apptainer/mnt/session/libs/libnvcuvid.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvcuvid.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaopticalflow.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiaopticalflow.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaopticalflow.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLX.so to /var/lib/apptainer/mnt/session/libs/libGLX.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLX.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv1_CM.so to /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiacfg.so to /var/lib/apptainer/mnt/session/libs/libnvidiacfg.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiacfg.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libcuda.so to /var/lib/apptainer/mnt/session/libs/libcuda.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libcuda.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libvdpau_nvidia.so to /var/lib/apptainer/mnt/session/libs/libvdpau_nvidia.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libvdpau_nvidia.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiartcore.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiartcore.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiartcore.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libcuda.so.1 to /var/lib/apptainer/mnt/session/libs/libcuda.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libcuda.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenGL.so to /var/lib/apptainer/mnt/session/libs/libOpenGL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenGL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libEGL_nvidia.so.0 to /var/lib/apptainer/mnt/session/libs/libEGL_nvidia.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libEGL_nvidia.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaglcore.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiaglcore.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaglcore.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLdispatch.so to /var/lib/apptainer/mnt/session/libs/libGLdispatch.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLdispatch.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/libs to /var/lib/apptainer/mnt/session/final/.singularity.d/libs DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/.singularity.d/libs DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidiapersistenced to /var/lib/apptainer/mnt/session/final/usr/bin/nvidiapersistenced DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidiapersistenced DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidiacudampscontrol to /var/lib/apptainer/mnt/session/final/usr/bin/nvidiacudampscontrol DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidiacudampscontrol DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidiacudampsserver to /var/lib/apptainer/mnt/session/final/usr/bin/nvidiacudampsserver DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidiacudampsserver DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidiasmi to /var/lib/apptainer/mnt/session/final/usr/bin/nvidiasmi DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidiasmi DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidiadebugdump to /var/lib/apptainer/mnt/session/final/usr/bin/nvidiadebugdump DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidiadebugdump DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/etc/resolv.conf to /var/lib/apptainer/mnt/session/final/etc/resolv.conf DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/etc/passwd to /var/lib/apptainer/mnt/session/final/etc/passwd DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/etc/group to /var/lib/apptainer/mnt/session/final/etc/group VERBOSE [U=0,P=1355208]    addCwdMount()                 /root found within container DEBUG   [U=0,P=1355208]    create()                      Chroot into /var/lib/apptainer/mnt/session/final DEBUG   [U=0,P=1355231]    Chroot()                      Hold reference to host / directory DEBUG   [U=0,P=1355231]    Chroot()                      Called pivot_root on /var/lib/apptainer/mnt/session/final DEBUG   [U=0,P=1355231]    Chroot()                      Change current directory to host / directory DEBUG   [U=0,P=1355231]    Chroot()                      Apply slave mount propagation for host / directory DEBUG   [U=0,P=1355231]    Chroot()                      Called unmount(/, syscall.MNT_DETACH) DEBUG   [U=0,P=1355231]    Chroot()                      Changing directory to / to avoid getpwd issues DEBUG   [U=0,P=1355208]    create()                      Chdir into / to avoid errors VERBOSE [U=0,P=1355230]    wait_child()                  rpc server exited with status 0 DEBUG   [U=0,P=1355230]    init()                        Set container privileges DEBUG   [U=0,P=1355230]    apply_privileges()            Effective capabilities:   0x000001ffffffffff DEBUG   [U=0,P=1355230]    apply_privileges()            Permitted capabilities:   0x000001ffffffffff DEBUG   [U=0,P=1355230]    apply_privileges()            Bounding capabilities:    0x000001ffffffffff DEBUG   [U=0,P=1355230]    apply_privileges()            Inheritable capabilities: 0x000001ffffffffff DEBUG   [U=0,P=1355230]    apply_privileges()            Ambient capabilities:     0x000001ffffffffff DEBUG   [U=0,P=1355230]    apply_privileges()            Set user ID to 0 DEBUG   [U=0,P=1355230]    set_parent_death_signal()     Set parent death signal to 9 DEBUG   [U=0,P=1355230]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter DEBUG   [U=0,P=1355230]    func1()                       executablePath does not exist, assuming default prefix DEBUG   [U=0,P=1355230]    startup()                     apptainer runtime engine selected VERBOSE [U=0,P=1355230]    startup()                     Execute stage 2 DEBUG   [U=0,P=1355230]    StageTwo()                    Entering stage 2 DEBUG   [U=0,P=1355230]    StartProcess()                Setting umask in container to 0002 DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC__module_raw%%"" to container environment: invalid key DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_ml%%"" to container environment: invalid key DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_module%%"" to container environment: invalid key DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_scl%%"" to container environment: invalid key DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_which%%"" to container environment: invalid key DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/01base.sh DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/10docker2singularity.sh DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/90environment.sh DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/94appsbase.sh DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/95apps.sh DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/99base.sh DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/99runtimevars.sh DEBUG   [U=0,P=1355230]    sylogBuiltin()                Running action command exec DEBUG   [U=0,P=1355208]    PostStartProcess()            Post start process Python 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. ```",2025-01-24T20:16:31Z,stat:awaiting tensorflower type:build/install comp:gpu TF 2.18,open,0,3,https://github.com/tensorflow/tensorflow/issues/85689,The docker  TF container tries to load libcudnn.so.9 However the container has only been built with libcudnn.so.8 More detail here: tensorflow 2.18 requires libcudnn.so.9,"> The docker TF container tries to load libcudnn.so.9 However the container has only been built with libcudnn.so.8 >  > More detail here: tensorflow 2.18 requires libcudnn.so.9 Thanks, how do we get the maintained to flx if?",", I request you to take a look at this issue where a similar feature has been proposed and it is still open. Also I request to follow the similar feature which has been proposed to have the updates on the similar issue. Thank you!"
sharding,copybara-service[bot],"Make `DynamicShape`, `Sharding`, and `ArraySpec` hashable","Make `DynamicShape`, `Sharding`, and `ArraySpec` hashable",2025-01-24T20:00:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85685
opt,copybara-service[bot],[xla:cpu] Add XnnFusionThunk options to be able to run without a thread pool,[xla:cpu] Add XnnFusionThunk options to be able to run without a thread pool,2025-01-24T18:40:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85680
tpu,Chiebuka-Chibuike2024,difficulty installing tensorflow," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.12.2  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I expected to import TensorFlow, but I keep getting the error message below: Traceback (most recent call last):   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\Users\user\Desktop\Ban6440Milestone2assignment.py"", line 77, in      import tensorflow as tf   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 85, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.  Standalone code to reproduce the issue ```shell import tensorflow as tf ```  Relevant log output ```shell Traceback (most recent call last):   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\Users\user\Desktop\Ban6440Milestone2assignment.py"", line 77, in      import tensorflow as tf   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 85, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```",2025-01-24T14:00:16Z,type:build/install,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85668,Are you satisfied with the resolution of your issue? Yes No,Please perform a search for similar issues.
tpu,Chuan1937,tensorflow takes a long time to prepare before the first iteration," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version TF 2.10.0  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 11.4/8.9.1  GPU model and memory Nvidia Tesla K20m  Current behavior? tensorflow takes a long time to prepare before the first iteration.I used my custom model for training, but it took 4060 minutes from the time the data was ready to the first iteration. This was true even for a very small dataset. And my model only had 835,620 parameters. This model is used to pick up the phase of seismic data. If an experiment is conducted, the data can be fabricated by itself.  Standalone code to reproduce the issue ```shell import numpy as np import matplotlib matplotlib.use('agg') import os os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'   Suppress TensorFlow logs from tensorflow.python import keras from tensorflow.keras import backend as K from tensorflow.keras.layers import (     Add, Activation, LSTM, Conv1D, MaxPooling1D, UpSampling1D,     Cropping1D, SpatialDropout1D, Bidirectional, BatchNormalization,add,InputSpec, LayerNormalization,Layer, Dense, Dropout,Layer ) from tensorflow.keras.optimizers import Adam from tensorflow import keras import tensorflow as tf from tensorflow.keras import initializers, regularizers, constraints, activations def f1(y_true, y_pred):     def recall(y_true, y_pred):         '''Recall metric. Computes the recall, a metric for multilabel classification.'''         true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))         possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))         recall = true_positives / (possible_positives + tf.keras.backend.epsilon())         return recall     def precision(y_true, y_pred):         '''Precision metric. Computes the precision, a metric for multilabel classification.'''         true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))         predicted_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_pred, 0, 1)))         precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())         return precision     precision_val = precision(y_true, y_pred)     recall_val = recall(y_true, y_pred)      F1 score calculation     return 2 * (precision_val * recall_val) / (precision_val + recall_val + tf.keras.backend.epsilon()) class LayerNormalization(keras.layers.Layer):     def __init__(self,                  center=True,                  scale=True,                  epsilon=None,                  gamma_initializer='ones',                  beta_initializer='zeros',                  **kwargs):         super(LayerNormalization, self).__init__(**kwargs)         self.supports_masking = True         self.center = center         self.scale = scale         if epsilon is None:             epsilon = K.epsilon() * K.epsilon()         self.epsilon = epsilon         self.gamma_initializer = keras.initializers.get(gamma_initializer)         self.beta_initializer = keras.initializers.get(beta_initializer)     def get_config(self):         config = {             'center': self.center,             'scale': self.scale,             'epsilon': self.epsilon,             'gamma_initializer': keras.initializers.serialize(self.gamma_initializer),             'beta_initializer': keras.initializers.serialize(self.beta_initializer),         }         base_config = super(LayerNormalization, self).get_config()         return dict(list(base_config.items()) + list(config.items()))     def compute_output_shape(self, input_shape):         return input_shape     def compute_mask(self, inputs, input_mask=None):         return input_mask     def build(self, input_shape):         self.input_spec = InputSpec(shape=input_shape)         shape = input_shape[1:]         if self.scale:             self.gamma = self.add_weight(                 shape=shape,                 initializer=self.gamma_initializer,                 name='gamma',             )         if self.center:             self.beta = self.add_weight(                 shape=shape,                 initializer=self.beta_initializer,                 name='beta',             )         super(LayerNormalization, self).build(input_shape)     def call(self, inputs, training=None):         mean = K.mean(inputs, axis=1, keepdims=True)         variance = K.mean(K.square(inputs  mean), axis=1, keepdims=True)         std = K.sqrt(variance + self.epsilon)         outputs = (inputs  mean) / std         if self.scale:             outputs *= self.gamma         if self.center:             outputs += self.beta         return outputs class FeedForward(keras.layers.Layer):     def __init__(self,                  units,                  activation='relu',                  use_bias=True,                  kernel_initializer='glorot_normal',                  bias_initializer='zeros',                  dropout_rate=0.0,                  **kwargs):         self.supports_masking = True         self.units = units         self.activation = keras.activations.get(activation)         self.use_bias = use_bias         self.kernel_initializer = keras.initializers.get(kernel_initializer)         self.bias_initializer = keras.initializers.get(bias_initializer)         self.dropout_rate = dropout_rate         self.W1, self.b1 = None, None         self.W2, self.b2 = None, None         super(FeedForward, self).__init__(**kwargs)     def get_config(self):         config = {             'units': self.units,             'activation': keras.activations.serialize(self.activation),             'use_bias': self.use_bias,             'kernel_initializer': keras.initializers.serialize(self.kernel_initializer),             'bias_initializer': keras.initializers.serialize(self.bias_initializer),             'dropout_rate': self.dropout_rate,         }         base_config = super(FeedForward, self).get_config()         return dict(list(base_config.items()) + list(config.items()))     def compute_output_shape(self, input_shape):         return input_shape     def compute_mask(self, inputs, input_mask=None):         return input_mask     def build(self, input_shape):         feature_dim = int(input_shape[1])         self.W1 = self.add_weight(             shape=(feature_dim, self.units),             initializer=self.kernel_initializer,             name='{}_W1'.format(self.name),         )         if self.use_bias:             self.b1 = self.add_weight(                 shape=(self.units,),                 initializer=self.bias_initializer,                 name='{}_b1'.format(self.name),             )         self.W2 = self.add_weight(             shape=(self.units, feature_dim),             initializer=self.kernel_initializer,             name='{}_W2'.format(self.name),         )         if self.use_bias:             self.b2 = self.add_weight(                 shape=(feature_dim,),                 initializer=self.bias_initializer,                 name='{}_b2'.format(self.name),             )         super(FeedForward, self).build(input_shape)     def call(self, x, mask=None, training=None):         h = K.dot(x, self.W1)         if self.use_bias:             h = K.bias_add(h, self.b1)         if self.activation is not None:             h = self.activation(h)         if 0.0  0.0:             self.add_loss(self._attention_regularizer(a))         if self.return_attention:             return [v, a]         return v     def _call_additive_emission(self, inputs):         input_shape = K.shape(inputs)         batch_size = input_shape[0]         input_len = inputs.get_shape().as_list()[1]          h_{t, t'} = \tanh(x_t^T W_t + x_{t'}^T W_x + b_h)         q = K.expand_dims(K.dot(inputs, self.Wt), 2)         k = K.expand_dims(K.dot(inputs, self.Wx), 1)         if self.use_additive_bias:             h = K.tanh(q + k + self.bh)         else:             h = K.tanh(q + k)          e_{t, t'} = W_a h_{t, t'} + b_a         if self.use_attention_bias:             e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))         else:             e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))         return e     def _call_multiplicative_emission(self, inputs):          e_{t, t'} = x_t^T W_a x_{t'} + b_a         e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))         if self.use_attention_bias:             e += self.ba[0]         return e     def compute_output_shape(self, input_shape):         output_shape = input_shape         if self.return_attention:             attention_shape = (input_shape[0], output_shape[1], input_shape[1])             return [output_shape, attention_shape]         return output_shape     def compute_mask(self, inputs, mask=None):         if self.return_attention:             return [mask, None]         return mask     def _attention_regularizer(self, attention):         batch_size = K.cast(K.shape(attention)[0], K.floatx())         input_len = K.shape(attention)[1]         indices = K.expand_dims(K.arange(0, input_len), axis=0)         diagonal = K.expand_dims(K.arange(0, input_len), axis=1)         eye = K.cast(K.equal(indices, diagonal), K.floatx())         return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(             attention,             K.permute_dimensions(attention, (0, 2, 1)))  eye)) / batch_size          def get_custom_objects():         return {'SeqSelfAttention': SeqSelfAttention} def _block_BiLSTM(filters, drop_rate, padding, inpR):     'Returns LSTM residual block'         prev = inpR      x_rnn = Bidirectional(LSTM(filters, return_sequences=True, dropout=drop_rate, recurrent_dropout=drop_rate))(prev)     符合使用cudnn核心的LSTM     x_rnn = Bidirectional(LSTM(filters, return_sequences=True, dropout=drop_rate, recurrent_dropout=0, activation='tanh', recurrent_activation='sigmoid', use_bias=True, unroll=False))(prev)     NiN = Conv1D(filters, 1, padding = padding)(x_rnn)          res_out = BatchNormalization()(NiN)     return res_out def _block_CNN_1(filters, ker, drop_rate, activation, padding, inpC):      ' Returns CNN residual blocks '     prev = inpC     layer_1 = BatchNormalization()(prev)      act_1 = Activation(activation)(layer_1)      act_1 = SpatialDropout1D(drop_rate)(act_1, training=True)     conv_1 = Conv1D(filters, ker, padding = padding)(act_1)      layer_2 = BatchNormalization()(conv_1)      act_2 = Activation(activation)(layer_2)      act_2 = SpatialDropout1D(drop_rate)(act_2, training=True)     conv_2 = Conv1D(filters, ker, padding = padding)(act_2)     res_out = add([prev, conv_2])     return res_out  def _transformer(drop_rate, width, name, inpC):      ' Returns a transformer block containing one addetive attention and one feed  forward layer with residual connections '     x = inpC     att_layer, weight = SeqSelfAttention(return_attention =True,                                                                                 attention_width = width,                                          name=name)(x)   att_layer = Dropout(drop_rate)(att_layer, training=True)         att_layer2 = add([x, att_layer])         norm_layer = LayerNormalization()(att_layer2)     FF = FeedForward(units=128, dropout_rate=drop_rate)(norm_layer)     FF_add = add([norm_layer, FF])         norm_out = LayerNormalization()(FF_add)     return norm_out, weight  def _encoder(filter_number, filter_size, depth, drop_rate, ker_regul, bias_regul, activation, padding, inpC):     ' Returns the encoder that is a combination of residual blocks and maxpooling.'             e = inpC     for dp in range(depth):         e = Conv1D(filter_number[dp],                     filter_size[dp],                     padding = padding,                     activation = activation,                    kernel_regularizer = ker_regul,                    bias_regularizer = bias_regul,                    )(e)                      e = MaxPooling1D(2, padding = padding)(e)                 return(e)  def _decoder(filter_number, filter_size, depth, drop_rate, ker_regul, bias_regul, activation, padding, inpC):     ' Returns the dencoder that is a combination of residual blocks and upsampling. '                d = inpC     for dp in range(depth):                 d = UpSampling1D(2)(d)          if dp == 2:             d = Cropping1D(cropping=(1, 1))(d)                    d = Conv1D(filter_number[dp],                     filter_size[dp],                     padding = padding,                     activation = activation,                    kernel_regularizer = ker_regul,                    bias_regularizer = bias_regul,                    )(d)             return(d)   def _lr_schedule(epoch):     ' Learning rate is scheduled to be reduced after 40, 60, 80, 90 epochs.'     lr = 1e3     if epoch > 90:         lr *= 0.5e3     elif epoch > 60:         lr *= 1e3     elif epoch > 40:         lr *= 1e2     elif epoch > 20:         lr *= 1e1     print('Learning rate: ', lr)     return lr class cred2():     def __init__(self,                  nb_filters=[8, 16, 16, 32, 32, 96, 96, 128],                  kernel_size=[11, 9, 7, 7, 5, 5, 3, 3],                  padding='same',                  activationf='relu',                  endcoder_depth=7,                  decoder_depth=7,                  cnn_blocks=5,                  BiLSTM_blocks=3,                  drop_rate=0.1,                  loss_weights=[0.2, 0.3, 0.5],                  loss_types=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy'],                                                   kernel_regularizer=keras.regularizers.l1(1e4),                  bias_regularizer=keras.regularizers.l1(1e4),                  ):         self.kernel_size = kernel_size         self.nb_filters = nb_filters         self.padding = padding         self.activationf = activationf         self.endcoder_depth= endcoder_depth         self.decoder_depth= decoder_depth         self.cnn_blocks= cnn_blocks         self.BiLSTM_blocks= BiLSTM_blocks              self.drop_rate= drop_rate         self.loss_weights= loss_weights           self.loss_types = loss_types                self.kernel_regularizer = kernel_regularizer              self.bias_regularizer = bias_regularizer      def __call__(self, inp):         x = inp         x = _encoder(self.nb_filters,                      self.kernel_size,                      self.endcoder_depth,                      self.drop_rate,                      self.kernel_regularizer,                      self.bias_regularizer,                     self.activationf,                      self.padding,                     x)             for cb in range(self.cnn_blocks):             x = _block_CNN_1(self.nb_filters[6], 3, self.drop_rate, self.activationf, self.padding, x)             if cb > 2:                 x = _block_CNN_1(self.nb_filters[6], 2, self.drop_rate, self.activationf, self.padding, x)         for bb in range(self.BiLSTM_blocks):             x = _block_BiLSTM(self.nb_filters[1], self.drop_rate, self.padding, x)         x, weightdD0 = _transformer(self.drop_rate, None, 'attentionD0', x)                      encoded, weightdD = _transformer(self.drop_rate, None, 'attentionD', x)                      decoder_D = _decoder([i for i in reversed(self.nb_filters)],                               [i for i in reversed(self.kernel_size)],                               self.decoder_depth,                               self.drop_rate,                               self.kernel_regularizer,                               self.bias_regularizer,                              self.activationf,                               self.padding,                                                           encoded)         d = Conv1D(1, 11, padding = self.padding, activation='sigmoid', name='detector')(decoder_D)         '''         The requirements to use the cuDNN implementation are:         activation == tanh         recurrent_activation == sigmoid         recurrent_dropout == 0         unroll is False         use_bias is True         Inputs, if use masking, are strictly rightpadded.         Eager execution is enabled in the outermost context.         '''          PLSTM = LSTM(self.nb_filters[1], return_sequences=True, dropout=self.drop_rate, recurrent_dropout=self.drop_rate)(encoded)          假设 self.nb_filters 和 self.drop_rate 已经定义         PLSTM = LSTM(self.nb_filters[1],                      return_sequences=True,                      dropout=self.drop_rate,                      recurrent_dropout=0,                      activation='tanh',                      recurrent_activation='sigmoid',                      use_bias=True,                      unroll=False)(encoded)         norm_layerP, weightdP = SeqSelfAttention(return_attention=True,                                                  attention_width= 3,                                                  name='attentionP')(PLSTM)         decoder_P = _decoder([i for i in reversed(self.nb_filters)],                              [i for i in reversed(self.kernel_size)],                              self.decoder_depth,                              self.drop_rate,                              self.kernel_regularizer,                              self.bias_regularizer,                             self.activationf,                              self.padding,                                                         norm_layerP)         P = Conv1D(1, 11, padding = self.padding, activation='sigmoid', name='picker_P')(decoder_P)          SLSTM = LSTM(self.nb_filters[1], return_sequences=True, dropout=self.drop_rate, recurrent_dropout=self.drop_rate)(encoded)          SLSTM = LSTM(self.nb_filters[1],               return_sequences=True,               dropout=self.drop_rate,               recurrent_dropout=0,               activation='tanh',               recurrent_activation='sigmoid',               use_bias=True,               unroll=False)(encoded)         norm_layerS, weightdS = SeqSelfAttention(return_attention=True,                                                  attention_width= 3,                                                  name='attentionS')(SLSTM)         decoder_S = _decoder([i for i in reversed(self.nb_filters)],                              [i for i in reversed(self.kernel_size)],                             self.decoder_depth,                              self.drop_rate,                              self.kernel_regularizer,                              self.bias_regularizer,                             self.activationf,                              self.padding,                                                         norm_layerS)          S = Conv1D(1, 11, padding = self.padding, activation='sigmoid', name='picker_S')(decoder_S)         model = keras.models.Model(inputs=inp, outputs=[d, P, S])         model.compile(loss=self.loss_types, loss_weights=self.loss_weights,                 optimizer=Adam(lr=_lr_schedule(0)), metrics=[f1])         return model  input=keras.layers.Input(shape=(12000,3))  model=cred2()(input)  model.summary() ```  Relevant log output ```shell ```",2025-01-24T13:47:24Z,stat:awaiting response stale type:performance TF 2.10,closed,4,7,https://github.com/tensorflow/tensorflow/issues/85667,This is likely the time spent JITing the Python imperative code to the graph representation that TF uses.,"How can I speed it up? Otherwise, I have to wait for nearly 60 minutes every time.","I think that that's too much. You could try writing TF code in graph mode directly (or switching to JAX for even more performance gains  since you use Keras, use Keras 3 and switch to Jax backend).","It is normal for the first epoch to take longer to train than subsequent epochs, as TensorFlow needs to compile the model and allocate resources. However, an hour does seem like a long time.  **Here are a few things that could be causing the slow training time:** * Larger models with more parameters will take longer to train. * Training on a larger dataset will take longer. * The speed of your CPU, GPU, and available RAM will all affect training speed. * If you are using custom code in your training loop, it could be slowing things down. **Here are some tips to improve training speed:** * If you are just starting out, try using a smaller model with fewer parameters. * If you have a large dataset, try using a smaller subset for training. * If you have a GPU, you can use it to accelerate training. * Use a profiler to identify bottlenecks in your training code. * A smaller batch size can sometimes improve training speed.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,copybara-service[bot],[XLA:GPU] Add matmul perf table generator.,"[XLA:GPU] Add matmul perf table generator. ``` This tool runs specified matrix shapes and datatypes (HLO dots) on given hardware and saves clock cycles for each. Matrix shapes can be specified by defining a search space. Assume matrix multiplication dims: [n,k] @ [k,m] > [n,m]. The specification has a format {m,n,k}_spec='start=,stop=,step=' Which means for a particular spec we will generate a set   { + n *  | n *     for every n w/ {0}} Program expects a spec for every dim. The generated matrix multplication shapes are a cartesian product of these three specs (+ specified data types). Usage: 1. Run cartesian product for   shape:{256x256x256} x dtype:{bf16,bf16>bf16 and bf16,bf16>f32} and dump to proto. bazel run matmul_perf_table_gen_main config=cuda  \   alsologtostderr \   m_spec='start=256,stop=256,step=1' \   n_spec='start=256,stop=256,step=1' \   k_spec='start=256,stop=256,step=1' \   dtypes_spec='lhs=bf16,rhs=bf16,out=bf16;lhs=bf16,rhs=bf16,out=f32' \   output=/tmp/proto.pbtxt cat /tmp/proto.pbtxt entries {   key: ""sm_86""   value {     entries {       instruction {         name: ""_""         opcode: ""dot""         shape {           element_type: BF16           dimensions: 256           dimensions: 256         }         ...       }       clock_cycles: 10022     }     entries {       instruction {         name: ""_""         opcode: ""dot""         shape {           element_type: F32           dimensions: 256           dimensions: 256        }        ...       clock_cycles: 10137     }   } } 2. Run cartesian product for   shape:{8x16x16 and 16x16x16 and 24x16x16} x dtype:{bf16,bf16>bf16} and print to stdout. bazel run matmul_perf_table_gen_main config=cuda  \   alsologtostderr \   m_spec='start=8,stop=24,step=8' \   n_spec='start=16,stop=16,step=1' \   k_spec='start=16,stop=16,step=1' \   dtypes_spec='lhs=bf16,rhs=bf16,out=bf16 \ entries {   key: ""sm_90""   value {     entries {       instruction {         name: ""_""         opcode: ""dot""         shape {           element_type: BF16           dimensions: 24           dimensions: 16         }       }       ...       clock_cycles: 10961     }     entries {       instruction {         name: ""_""         opcode: ""dot""         shape {           element_type: BF16           dimensions: 16           dimensions: 16         }       }       ...       clock_cycles: 9440     }     entries {       instruction {         name: ""_""         opcode: ""dot""         shape {           element_type: BF16           dimensions: 8           dimensions: 16         }       }       ...       clock_cycles: 9440     }   } } ```",2025-01-24T10:38:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85659
opt,copybara-service[bot],Protect CompileModuleToLlvmIr calls by LLVMCommandLineOptionsLock,Protect CompileModuleToLlvmIr calls by LLVMCommandLineOptionsLock XLA needs to set LLVM's global command line option flags in a some places to influence settings in the compilation pipeline. Since writing or reading these command line flags is not threadsafe and global we have a processwide lock `LLVMCommandLineOptionsLock` which manages multiple LLVM users with different flags in the same process. Turns out the lock was not correctly in place for our LLVM IR emitter code path. This led to a race condition which potentially affected users that compile multiple modules in parallel from different threads. So this change fixes that and also adds a test that should light up when running under TSAN if the compilation pipeline becomes nonthreadsafe in the future.,2025-01-24T07:07:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85636
tpu,intelav,LiteRT build for Android failing," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version v2.17.0  Custom code No  OS platform and distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version bazel 6.5.0  GCC/compiler version NDK26,NDK28  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following this link https://ai.google.dev/edge/litert/build/android for building libtensorflowlite.so for my android JNI project, but its failing all  the time with below error snapshot **Initial Steps** git clone https://github.com/tensorflow/tensorflow.git cd tensorflow git checkout v2.17.0 ./configure ( For Android Environment) **Below is the content of .tf_configure.bazelrc in my build environment** build action_env PYTHON_BIN_PATH=""/usr/bin/python3"" build action_env PYTHON_LIB_PATH=""/usr/lib/python3.10/distpackages"" build python_path=""/usr/bin/python3"" build action_env CLANG_COMPILER_PATH=""/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linuxx86_64/bin/clang17"" build repo_env=CC=/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linuxx86_64/bin/clang17 build repo_env=BAZEL_COMPILER=/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linuxx86_64/bin/clang17 build copt=Wnognuoffsetofextensions build:opt copt=Wnosigncompare build:opt host_copt=Wnosigncompare build action_env ANDROID_NDK_HOME=""/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/"" build action_env ANDROID_NDK_VERSION=""26"" build action_env ANDROID_NDK_API_LEVEL=""21"" build action_env ANDROID_BUILD_TOOLS_VERSION=""35.0.0"" build action_env ANDROID_SDK_API_LEVEL=""35"" build action_env ANDROID_SDK_HOME=""/media/avaish/aiwork/Androidsdk/"" test test_size_filters=small,medium test:v1 test_tag_filters=benchmarktest,no_oss,oss_excluded,gpu,oss_serial test:v1 build_tag_filters=benchmarktest,no_oss,oss_excluded,gpu test:v2 test_tag_filters=benchmarktest,no_oss,oss_excluded,gpu,oss_serial,v1only test:v2 build_tag_filters=benchmarktest,no_oss,oss_excluded,gpu,v1only **Expected Output**  libtensorflowlite.so should get built successfully.   Standalone code to reproduce the issue ```shell **Build Command** avaishdekstop:/media/avaish/linuxgames/litebuild/tensorflow$ bazel build c opt cxxopt=std=c++17 config=android_arm64   fat_apk_cpu=x86,x86_64,arm64v8a,armeabiv7a   define=android_dexmerger_tool=d8_dexmerger   define=android_incremental_dexing_tool=d8_dexbuilder   //tensorflow/lite/java:tensorflowlite ```  Relevant log output ```shell INFO: Reading 'startup' options from /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=153 INFO: Reading rc options for 'build' from /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc:   'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Reading rc options for 'build' from /media/avaish/linuxgames/litebuild/tensorflow/.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=/usr/bin/python3 action_env PYTHON_LIB_PATH=/usr/lib/python3.10/distpackages python_path=/usr/bin/python3 action_env CLANG_COMPILER_PATH=/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linuxx86_64/bin/clang17 repo_env=CC=/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linuxx86_64/bin/clang17 repo_env=BAZEL_COMPILER=/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linuxx86_64/bin/clang17 copt=Wnognuoffsetofextensions action_env ANDROID_NDK_HOME=/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/ action_env ANDROID_NDK_VERSION=26 action_env ANDROID_NDK_API_LEVEL=21 action_env ANDROID_BUILD_TOOLS_VERSION=35.0.0 action_env ANDROID_SDK_API_LEVEL=35 action_env ANDROID_SDK_HOME=/media/avaish/aiwork/Androidsdk/ INFO: Found applicable config definition build:short_logs in file /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:android_arm64 in file /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc: config=android cpu=arm64v8a fat_apk_cpu=arm64v8a INFO: Found applicable config definition build:android in file /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc: crosstool_top=//external:android/crosstool host_crosstool_top=//tools/cpp:toolchain dynamic_mode=off define=xnn_enable_avxvnniint8=false noenable_platform_specific_config copt=w cxxopt=std=c++17 host_cxxopt=std=c++17 define=with_xla_support=false config=no_tfrt INFO: Found applicable config definition build:no_tfrt in file /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils INFO: Analyzed target //tensorflow/lite/java:tensorflowlite (2 packages loaded, 8416 targets configured). INFO: Found 1 target... ERROR: /media/avaish/linuxgames/litebuild/tensorflow/tensorflow/lite/c/jni/BUILD:12:43: Compiling tensorflow/lite/c/jni/jni_utils.: undeclared inclusion(s) in rule '//tensorflow/lite/c/jni:jni_utils': this rule is missing dependency declarations for the following files included by 'tensorflow/lite/c/jni/jni_utils.cc':   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stdarg.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stdint.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stddef.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/__stddef_max_align_t.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stdbool.h' Target //tensorflow/lite/java:tensorflowlite failed to build Use verbose_failures to see the command lines of failed build steps. ```",2025-01-24T05:07:38Z,stat:awaiting response type:build/install stale comp:lite 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85631,"If I run the same build command again , similiar errors apperas from compiling other files ERROR: /home/avaish/.cache/bazel/_bazel_avaish/d98cf14fd195122b7f9fe191efe765ef/external/ruy/ruy/BUILD:423:11: Compiling ruy/denormal.: undeclared inclusion(s) in rule '//ruy:denormal': this rule is missing dependency declarations for the following files included by 'ruy/denormal.cc':   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stdint.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stddef.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/__stddef_max_align_t.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/xmmintrin.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/mmintrin.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/mm_malloc.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stdarg.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/emmintrin.h' Target //tensorflow/lite/java:tensorflowlite failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 0.903s, Critical Path: 0.12s INFO: 12 processes: 12 internal. FAILED: Build did NOT complete successfully","Hi,  I apologize for the delay in my response, I was trying to replicate the same behavior from my end but I'm getting different error and Build did NOT complete successfully so I have added error log below for reference please let me know if Am I missing something here to replicate same behavior which you reported here ? ``` (base) gaikwadrahuln1standard1gput4x1tfliteubuntu22:~/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow$ bazel build c opt cxxopt=std=c++17 config=android_arm64   fat_apk_cpu=x86,x86_64,arm64v8a,armeabiv7a   define=android_dexmerger_tool=d8_dexmerger   define=android_incremental_dexing_tool=d8_dexbuilder   //tensorflow/lite/java:tensorflowlite Extracting Bazel installation... Starting local Bazel server and connecting to it... INFO: Reading 'startup' options from /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=190 INFO: Reading rc options for 'build' from /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc:   'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Reading rc options for 'build' from /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=/home/gaikwadrahul/miniconda3/bin/python3 action_env PYTHON_LIB_PATH=/home/gaikwadrahul/miniconda3/lib/python3.12/sitepackages python_path=/home/gaikwadrahul/miniconda3/bin/python3 action_env CLANG_COMPILER_PATH=/usr/bin/clang17 repo_env=CC=/usr/bin/clang17 repo_env=BAZEL_COMPILER=/usr/bin/clang17 copt=Wnognuoffsetofextensions action_env ANDROID_NDK_HOME=/home/gaikwadrahul/androidndkr25b action_env ANDROID_NDK_VERSION=25 action_env ANDROID_NDK_API_LEVEL=21 action_env ANDROID_BUILD_TOOLS_VERSION=34.0.0 action_env ANDROID_SDK_API_LEVEL=33 action_env ANDROID_SDK_HOME=/home/gaikwadrahul/Android/Sdk INFO: Found applicable config definition build:short_logs in file /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:android_arm64 in file /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc: config=android cpu=arm64v8a fat_apk_cpu=arm64v8a INFO: Found applicable config definition build:android in file /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc: crosstool_top=//external:android/crosstool host_crosstool_top=//tools/cpp:toolchain dynamic_mode=off noenable_platform_specific_config copt=w cxxopt=std=c++17 host_cxxopt=std=c++17 define=with_xla_support=false config=no_tfrt INFO: Found applicable config definition build:no_tfrt in file /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils DEBUG: /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/tensorflow/tools/toolchains/python/python_repo.bzl:32:14:  TF_PYTHON_VERSION environment variable was not set correctly; using Python 3.11. To set Python version, run: export TF_PYTHON_VERSION=3.11 INFO: Analyzed target //tensorflow/lite/java:tensorflowlite (146 packages loaded, 15447 targets configured). INFO: Found 1 target... ERROR: /home/gaikwadrahul/.cache/bazel/_bazel_gaikwadrahul/17e9d465a5125118a485d68d85fbf68a/external/flatbuffers/src/BUILD.bazel:19:11: Compiling src/code_generators.cpp [for tool] failed: (Exit 1): clang17 failed: error executing command (from target //src:code_generators) /usr/bin/clang17 U_FORTIFY_SOURCE fstackprotector Wall Wthreadsafety Wselfassign Wunusedbutsetparameter Wnofreenonheapobject fcolordiagnostics fnoomitframepointer g0 O2 ... (remaining 27 arguments skipped) In file included from external/flatbuffers/src/code_generators.cpp:17: bazelout/k8optexec50AE0418/bin/external/flatbuffers/src/_virtual_includes/code_generators/flatbuffers/code_generators.h:20:10: fatal error: 'map' file not found    20           ^~~~~ 1 error generated. Target //tensorflow/lite/java:tensorflowlite failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 126.844s, Critical Path: 12.54s INFO: 84 processes: 61 internal, 23 local. FAILED: Build did NOT complete successfully ``` Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
llm,copybara-service[bot],"Make the TF shape inference update the XlaCallModule's StableHLO module when the shapes are refined, if enabled by `enable_stablehlo_propagation`. It is disabled by default for now.","Make the TF shape inference update the XlaCallModule's StableHLO module when the shapes are refined, if enabled by `enable_stablehlo_propagation`. It is disabled by default for now.",2025-01-24T02:42:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85630
tpu,ghinaaraf,Can not import tensorflow as tf," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11.5  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense  Standalone code to reproduce the issue ```shell i always use python with VS Code, and yesterday my code finally success. but today when I run these code ""import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense"" . it always become error, even I had uninstall and install the tensorflow again, it not helped ```  Relevant log output ```shell ImportError                               Traceback (most recent call last) File c:\Users\Ghinaa\AppData\Local\Programs\Python\Python311\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[52], line 1 > 1 import tensorflow as tf       2 from tensorflow.keras.models import Sequential       3 from tensorflow.keras.layers import Dense File c:\Users\Ghinaa\AppData\Local\Programs\Python\Python311\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 ... Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```",2025-01-24T02:39:30Z,type:performance,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85629,Please search for duplicates before opening a new issue,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],Enable p2p pipelining when `xla_gpu_experimental_enable_pipeline_parallelism_opt` is enabled,Enable p2p pipelining when `xla_gpu_experimental_enable_pipeline_parallelism_opt` is enabled,2025-01-24T00:30:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85626
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-24T00:04:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85624
sharding,copybara-service[bot],#sdy add sharding rules for ragged_all_to_all custom call,sdy add sharding rules for ragged_all_to_all custom call,2025-01-23T23:56:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85623
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:45:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85621
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:42:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85620
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:25:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85617
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:17:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85616
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:13:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85615
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:10:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85614
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:10:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85613
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:09:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85612
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:07:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85611
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T22:52:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85609
yi,copybara-service[bot],[ODML] Pass expand-tuple : Migrate from MHLO to StableHLO,[ODML] Pass expandtuple : Migrate from MHLO to StableHLO FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21825 from zhenyingliu:scheduler 1c5720711c6a7d8173e132922b67eee6e2e8b9dd,2025-01-23T22:38:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85606
tpu,MahmoudBahar,TensorFlow warning shows whenever importing it," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version v2.18.0rc24g6550e4bd802  Custom code No  OS platform and distribution Linux Ubuntu 24.10 x86_64  Mobile device _No response_  Python version 3.12.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA:12.6  GPU model and memory _No response_  Current behavior? `cuFFT`, `cuDNN` and `cuBLAS` warnings showing after importing `TensorFlow`  OS: Ubuntu 24.10 x86_64  Host: G5 5590  Kernel: 6.11.013generic  CPU: Intel i79750H (12) @ 4.500GHz  GPU: NVIDIA GeForce GTX 1650 Mobile / MaxQ  GPU: Intel CoffeeLakeH GT2 [UHD Graphics 630] > whenever running the following code it gives that warning also it outputs the predicted output but after the warning: ```python import tensorflow as tf print(tf.config.list_physical_devices('GPU')) ``` > output: ``` 20250123 21:08:06.468437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1737659286.484845  763412 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1737659286.489647  763412 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250123 21:08:06.505984: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] ``` > also i know that for that warning (`cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`) i must rebuild the tensorflow from binaries enabling the AVX2 and FMA instructions but what about the others?  Standalone code to reproduce the issue ```shell import tensorflow as tf print(tf.config.list_physical_devices('GPU')) ```  Relevant log output ```shell ```",2025-01-23T21:56:39Z,type:bug 2.18.rc,open,1,3,https://github.com/tensorflow/tensorflow/issues/85604,"Hi **** , Apologies for the delay, and thank you for raising your concern here. It seems there might be a version mismatch. Could you please verify the compatibility of all the versions you are using? For your reference, I have provided relevant documentation below. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Hello , sorry for replying lately. I checked the versions as in the documentation you sent me as following: I am working in conda enviroment with the following configs: python=3.12.8 running the following code to check the version of `TensorFlow`: ```python import tensorflow as tf print(tf.__version__) ``` would get the following output: ```bash 20250205 05:42:43.524743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1738726963.539270   17574 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1738726963.543483   17574 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250205 05:42:43.558701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 2.18.0 ``` showing that `TensorFlow` installed with version `2.18.0` running the following code in terminal to check compiler and build tools would get that output: ```bash (tf) username:~$ python c ""import tensorflow as tf; print(tf.sysconfig.get_compile_flags())"" 20250205 05:47:22.629352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1738727242.644411   17974 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1738727242.648762   17974 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250205 05:47:22.664596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. ['I/home/mahmoudbahar/anaconda3/envs/tf/lib/python3.12/sitepackages/tensorflow/include', 'D_GLIBCXX_USE_CXX11_ABI=1', 'std=c++17', 'DEIGEN_MAX_ALIGN_BYTES=64'] (tf) username:~$ python c ""import tensorflow as tf; print(tf.sysconfig.get_build_info())"" 20250205 05:47:39.771357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1738727259.787632   18027 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1738727259.792173   18027 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250205 05:47:39.808482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. OrderedDict({'cpu_compiler': '/usr/lib/llvm18/bin/clang', 'cuda_compute_capabilities': ['sm_60', 'sm_70', 'sm_80', 'sm_89', 'compute_90'], 'cuda_version': '12.5.1', 'cudnn_version': '9', 'is_cuda_build': True, 'is_rocm_build': False, 'is_tensorrt_build': False}) ``` running the following code to check if `TensorFlow` can recognize the cudnn version: ```python import tensorflow as tf print(tf.sysconfig.get_build_info()['cudnn_version']) ``` outputs the following: ```bash 20250205 05:31:31.865306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1738726291.880059   16607 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1738726291.884535   16607 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250205 05:31:31.900147: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 9 ``` showing that `cuDNN` installed with the package is version is `9` running the following code to check if `TensorFlow` can recognize the cuda version: ```python import tensorflow as tf print(tf.sysconfig.get_build_info()['cuda_version']) ``` outputs the following: ```bash 20250205 05:33:28.844110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1738726408.859348   16754 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1738726408.863713   16754 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250205 05:33:28.880097: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 12.5.1 ``` showing that that `CUDA` installed with the package is `12.5.1` `nvidiacudatoolkit` is not installed globally in my machine. so the following would be expected: ```bash (tf) username:~$ nvcc version Command 'nvcc' not found, but can be installed with: sudo apt install nvidiacudatoolkit ``` There is a CUDA version installed globally which is 12.6 so the following output is as expected: ```bash (tf) username:~$ nvidiasmi Wed Feb  5 05:39:56 2025        ++  ++ ``` > [!IMPORTANT] > running the following commands in terminal would show that they are not installed globally > cmake version > bazel version"
tpu,copybara-service[bot],Fix output shape of slice op test.,Fix output shape of slice op test.,2025-01-23T20:06:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85598
opt,copybara-service[bot],PR #21391: [NVIDIA GPU] Add an option to xla_gpu_disable_async_collectives to disable all async collectives using a single op,"PR CC(tf.linspace doesn't except integers for start/stop and floats for num): [NVIDIA GPU] Add an option to xla_gpu_disable_async_collectives to disable all async collectives using a single op Imported from GitHub PR https://github.com/openxla/xla/pull/21391 xla_gpu_disable_async_collectives accepts a list of collective names, to be more efficient, we add a ""ALLCOLLECTIVE"" option to the enum so we can disable all ops with a single element. Copybara import of the project:  d561e610c170fac3a6a92436fc0983e383ed93eb by TJ Xu : add an option to disable all async collectives using a single op  e30cee65f20468eb5332a747382eaf78c64a99d5 by TJ Xu : add option in the test locally Merging this change closes CC(tf.linspace doesn't except integers for start/stop and floats for num) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21391 from Tixxx:tixxx/add_disable_all e30cee65f20468eb5332a747382eaf78c64a99d5",2025-01-23T19:06:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85594
tpu,copybara-service[bot],"[pjrt] Removed `PjRtRawDeviceBuffer`, `CreateRawDeviceBuffer` and `GetOnDeviceSizeInBytes` from `PjRtClient`","[pjrt] Removed `PjRtRawDeviceBuffer`, `CreateRawDeviceBuffer` and `GetOnDeviceSizeInBytes` from `PjRtClient` The class and both methods were only used in TPU client tests.",2025-01-23T18:45:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85592
sharding,copybara-service[bot],PR #21639: [GPU] Fix sharded autotuning test.,PR CC(Unnecessary character in logging.h): [GPU] Fix sharded autotuning test. Imported from GitHub PR https://github.com/openxla/xla/pull/21639 Autotuning relies on device assignment (replicas / partitions) to calculate sharding. Copybara import of the project:  bca2fc3edbb456fa420a1bbee356e8dc3f6ddeb9 by Ilia Sergachev : [GPU] Fix sharded autotuning test. Autotuning relies on device assignment (replicas / partitions) to calculate sharding. Merging this change closes CC(Unnecessary character in logging.h) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21639 from openxla:fix_sharded_autotuning_test bca2fc3edbb456fa420a1bbee356e8dc3f6ddeb9,2025-01-23T12:40:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85574
tpu,copybara-service[bot],[XLA:GPU] Always run under profiles in the multihost runner,[XLA:GPU] Always run under profiles in the multihost runner And output nsprecision execution duration to log.,2025-01-23T10:32:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85564
yi,Redoxon33,Compatibility table for TensorFlow 2.18 with CUDA and cuDNN is missing on the official website,"In previous versions of TensorFlow, the official documentation included a clear compatibility table specifying which versions of TensorFlow worked with specific versions of CUDA and cuDNN. However, upon reviewing the documentation for TensorFlow 2.18, I noticed this information is no longer available. These tables were  helpful for users, as they prevented installation and compatibility issues when setting up the development environment. It would be great if this compatibility table could be brought back to the official documentation. If it was removed due to any updates or errors that have been identified, it would also be helpful to notify users of such changes. If you're reading this issue, I kindly ask that you notify users if any updates or changes regarding this topic are implemented.",2025-01-23T10:09:06Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/85560,"After further investigation, I discovered that the issue is specific to the Spanish versions of the documentation (both Latin American and European Spanish), which appear to be outdated compared to the English version. The compatibility table is available in the English documentation, but it is not present in the Spanish translations. It would be helpful if an indicator could be added to show that the Spanish documentation is outdated or missing specific information, to prevent confusion for users relying on these versions."
tpu,cj401-amd,[RFC] rocprof insights for rocprof data," RFC for rocprof insights  Introduction `rocprof`, `rocprofv2`, and `rocprofv3` (rocprofilersdk) are the profiling tools that can be used to collect AMD hardware performance data when running applications with ROCm/HIP. The collected timeline trace data, which are JSON format for `rocprof` and `rocprofv2` and `pftrace` format for `rocprofv3`, can be visualized via `https://ui.perfetto.dev/` to guide the loop of profiling, analysis and optimization. To gain deep insights into specific running kernels, API launch and memory copy, it is necessary to obtain more statistics about them. rocprof insights, which is developed as a Python package, aims to provide the following functionalities: 1. Data loading, including loading CSV and JSON files saved from `rocprof v1/v2/v3` 2. Data analysis, providing:     Total running time     Number of calls (instances)     Average time     Median time     Min/max time     StdDev time    For each:     Kernel     HIP/HSA API calls     H2D, D2H, D2D memcopy     Checking private/group segment size (scratch/local memory, register spillage, shared local memory) 3. Data visualization, providing:     Pie chart plot of latency (running time)     Histogram of latency     Bar plot for kernels, API calls, etc. Other features we would like to explore are: 1. Can we overlay the latency on top of the original operators in the computational graph (latency per op/node)? 2. Can we trace the input/output values of every node in the computational graph for checking accuracy (mismatch) per node?",2025-01-23T10:00:14Z,size:XL,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85559
tpu,copybara-service[bot],PR #20794: [gpu][ds-fusion] Add handling for offset module in ds-fusion thunk,PR CC(Add all keep nodes to output lists): [gpu][dsfusion] Add handling for offset module in dsfusion thunk Imported from GitHub PR https://github.com/openxla/xla/pull/20794 This patch adds support for offset modules in ds fusion thunk. It also moves the `ResourceRequests` structure from `gpu_executable.cc` to `gpu_executable.h` because a valid implementation of the abstract class `Thunk::ResourceRequests` is required for calling `Thunk::Prepare()`. This is split from CC(An attempt at Tensorflow and Bazel CPU/GPU build (v1.8.0)) as per request. Copybara import of the project:  53226e9428dff816819c192735b9569ef3d309ea by Shraiysh Vaishay : [gpu][dsfusion] Add handling for offset module in dsfusion thunk This patch adds support for offset modules in ds fusion thunk. It also moves the `ResourceRequests` structure from `gpu_executable.cc` to `gpu_executable.h` because a valid implementation of the abstract class `Thunk::ResourceRequests` is required for calling `Thunk::Prepare()`.  e554ac6b09b5023082210c5100a1f1a86c1c6605 by Shraiysh Vaishay : Address comments and rebase  4ac8efa0c9885b0f21f526072f821ace40be4043 by Shraiysh Vaishay : Address comments  8c23e2a4e2ff66fb7361c48c9d988ebb2261bc41 by Shraiysh Vaishay : Addressed comments.  efbe6bfa6f10f1ee998683161e398ae3adc0f183 by Shraiysh Vaishay : Addressed comments Merging this change closes CC(Add all keep nodes to output lists) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20794 from shraiysh:ds_fusion_thunk_changes efbe6bfa6f10f1ee998683161e398ae3adc0f183,2025-01-23T09:08:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85539
opt,copybara-service[bot],Allow customization of HloParserOptions when using ParseAndReturnVerifiedModule.,Allow customization of HloParserOptions when using ParseAndReturnVerifiedModule.,2025-01-23T05:54:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85531
opt,copybara-service[bot],[xla:cpu] Micro-optimizations for XLA:CPU kernels that use only x dimension,[xla:cpu] Microoptimizations for XLA:CPU kernels that use only x dimension ``` name                                   old cpu/op   new cpu/op   delta BM_KernelAsyncLaunch/1/process_time    10.0ns ± 2%  10.5ns ± 2%  +4.92%  (p=0.000 n=37+37) BM_KernelAsyncLaunch/4/process_time    32.5µs ± 8%  32.7µs ±16%    ~     (p=0.483 n=38+40) BM_KernelAsyncLaunch/8/process_time    51.7µs ± 7%  51.0µs ± 7%  1.46%  (p=0.011 n=39+34) BM_KernelAsyncLaunch/16/process_time   73.0µs ± 7%  71.5µs ± 6%  2.03%  (p=0.007 n=39+39) BM_KernelAsyncLaunch/32/process_time   74.8µs ± 9%  73.0µs ± 6%  2.45%  (p=0.001 n=39+35) BM_KernelAsyncLaunch/64/process_time   76.9µs ± 8%  74.4µs ± 6%  3.29%  (p=0.000 n=38+36) BM_KernelAsyncLaunch/128/process_time  85.8µs ±12%  80.9µs ±13%  5.68%  (p=0.000 n=40+40) BM_KernelAsyncLaunch/256/process_time   104µs ± 8%    95µs ± 7%  8.80%  (p=0.000 n=38+37) name                                   old time/op          new time/op          delta BM_KernelAsyncLaunch/1/process_time    10.0ns ± 1%          10.5ns ± 1%  +5.01%  (p=0.000 n=39+38) BM_KernelAsyncLaunch/4/process_time    13.4µs ±10%          13.3µs ±19%    ~     (p=0.747 n=38+40) BM_KernelAsyncLaunch/8/process_time    17.0µs ± 6%          16.7µs ± 6%  1.25%  (p=0.025 n=39+34) BM_KernelAsyncLaunch/16/process_time   20.4µs ± 9%          20.1µs ± 6%  1.68%  (p=0.043 n=39+37) BM_KernelAsyncLaunch/32/process_time   20.9µs ± 7%          20.6µs ± 6%    ~     (p=0.102 n=39+35) BM_KernelAsyncLaunch/64/process_time   21.4µs ± 8%          20.7µs ±10%  3.19%  (p=0.001 n=39+39) BM_KernelAsyncLaunch/128/process_time  23.6µs ± 9%          22.3µs ±12%  5.54%  (p=0.000 n=40+40) BM_KernelAsyncLaunch/256/process_time  27.7µs ± 7%          25.9µs ±10%  6.42%  (p=0.000 n=39+39) ```,2025-01-23T01:06:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85522
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T00:49:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85521
opt,copybara-service[bot],[XLA:MSA] Add an option in WindowPrefetch to switch between different modes for window prefetch,"[XLA:MSA] Add an option in WindowPrefetch to switch between different modes for window prefetch In our current implementation of window prefetch, we don't actually perform prefetching. We only expose the window buffers from the reserved scoped memory. Because of that, we should distinguish these two different implementations. The one with prefetch is window prefetch, and the one without can be called window exposure. For window exposure, we don't need to call Prefetch, which allocates the buffer and prefetches the window. So in this change, we added an option to allow us to switch between window exposure and window prefetch and set window exposure as the default mode. Once we have the prefetch logic checked in, we can switch to use the window prefetch mode.",2025-01-23T00:43:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85520
sharding,copybara-service[bot],"Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `all-gather` with an `all-to-all`.","Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `allgather` with an `alltoall`. Given the following input ``` ENTRY entry {   %param0 = f32[14,257] parameter(0), sharding={devices=[1,2]0,1}   %param1 = f32[14,116] parameter(1), sharding={devices=[1,2]0,1}   ROOT %concatenate = f32[14,373] concatenate(%param0, %param1),     dimensions={1}, sharding={devices=[1,2]0,1} } ``` The partitioner generates allgather before this change ``` ENTRY %entry_spmd (param: f32[14,129], param.1: f32[14,58]) > f32[14,187] {   %param = f32[14,129]{1,0} parameter(0), sharding={devices=[1,2] f32[14,187] {   %param = f32[14,129]{1,0} parameter(0), sharding={devices=[1,2]<=[2]}   %reshape.1 = f32[2,7,129]{2,1,0} reshape(f32[14,129]{1,0} %param)   %alltoall = f32[2,7,129]{2,1,0} alltoall(f32[2,7,129]{2,1,0} %reshape.1), channel_id=1, replica_groups={{0,1}}, dimensions={0}   %transpose = f32[7,2,129]{2,0,1} transpose(f32[2,7,129]{2,1,0} %alltoall), dimensions={1,0,2}   %reshape.2 = f32[7,258]{1,0} reshape(f32[7,2,129]{2,0,1} %transpose)   %slice = f32[7,257]{1,0} slice(f32[7,258]{1,0} %reshape.2), slice={[0:7], [0:257]}   %param.1 = f32[14,58]{1,0} parameter(1), sharding={devices=[1,2]<=[2]}   %reshape.5 = f32[2,7,58]{2,1,0} reshape(f32[14,58]{1,0} %param.1)   %alltoall.1 = f32[2,7,58]{2,1,0} alltoall(f32[2,7,58]{2,1,0} %reshape.5), channel_id=2, replica_groups={{0,1}}, dimensions={0}   %transpose.1 = f32[7,2,58]{2,0,1} transpose(f32[2,7,58]{2,1,0} %alltoall.1), dimensions={1,0,2}   %reshape.6 = f32[7,116]{1,0} reshape(f32[7,2,58]{2,0,1} %transpose.1)   %concatenate.1 = f32[7,373]{1,0} concatenate(f32[7,257]{1,0} %slice, f32[7,116]{1,0} %reshape.6), dimensions={1}   %constant.20 = f32[] constant(0)   %pad = f32[7,374]{1,0} pad(f32[7,373]{1,0} %concatenate.1, f32[] %constant.20), padding=0_0x0_1   %reshape.9 = f32[7,2,187]{2,1,0} reshape(f32[7,374]{1,0} %pad)   %alltoall.2 = f32[7,2,187]{2,1,0} alltoall(f32[7,2,187]{2,1,0} %reshape.9), channel_id=3, replica_groups={{0,1}}, dimensions={1}   %transpose.2 = f32[2,7,187]{2,0,1} transpose(f32[7,2,187]{2,1,0} %alltoall.2), dimensions={1,0,2}   ROOT %reshape.10 = f32[14,187]{1,0} reshape(f32[2,7,187]{2,0,1} %transpose.2) } ```",2025-01-22T23:48:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85519
opt,copybara-service[bot],[JAX] Optimize array shard reordering,[JAX] Optimize array shard reordering This change adds a C++ implementation that uses `xla::ifrt::RemapArrays` to reorder shards of an array. This avoids creating intermediate singledevice arrays and accelerates reordering shards within `jax.device_put()` implementation.,2025-01-22T20:55:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85509
opt,copybara-service[bot],[xla:cpu] Enable concurrency-optimized schedule by default,[xla:cpu] Enable concurrencyoptimized schedule by default Improve wall time at the cost of CPU time. ``` name                                                                    old cpu/op   new cpu/op   delta BM_HloModule/shorts_ranking_v2.b333429386.20240606/process_time         25.5ms ± 6%  32.3ms ± 9%  +26.71%  (p=0.000 n=140+140) BM_HloModule/torax_sparc_prd_theta_method_block_residual/process_time    973µs ± 5%  1093µs ± 3%  +12.28%  (p=0.000 n=136+138) BM_HloModule/torax_sparc_prd_theta_method_block_jacobian/process_time    972µs ± 4%  1091µs ± 3%  +12.19%  (p=0.000 n=135+140) BM_HloModule/diffrax.b380012920/process_time                            17.2ms ± 2%  17.3ms ± 2%   +0.29%  (p=0.000 n=127+134) name                                                                    old time/op          new time/op          delta BM_HloModule/shorts_ranking_v2.b333429386.20240606/process_time         8.39ms ± 6%          6.67ms ± 8%  20.52%  (p=0.000 n=140+140) BM_HloModule/torax_sparc_prd_theta_method_block_residual/process_time    867µs ± 4%           856µs ± 3%   1.31%  (p=0.000 n=136+138) BM_HloModule/torax_sparc_prd_theta_method_block_jacobian/process_time    868µs ± 3%           855µs ± 2%   1.48%  (p=0.000 n=132+140) BM_HloModule/diffrax.b380012920/process_time                            17.2ms ± 2%          17.3ms ± 2%   +0.28%  (p=0.001 n=127+134) ```,2025-01-22T19:46:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85508
tpu,copybara-service[bot],[XLA:TPU] Document the computational cost of sorting the computations within a module.,[XLA:TPU] Document the computational cost of sorting the computations within a module.,2025-01-22T18:50:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85504
opt,copybara-service[bot],#litert Change `LiteRtCompilationOptions` to an opaque type.,"litert Change `LiteRtCompilationOptions` to an opaque type.  Add a C++ wrapper type for easier management.  Make the compile options mandatory in `CompiledModel::Create`. This aligns   the default value for hardware accelerator selection (depending on how the   options were specified, you would either get `None` or `Cpu`).",2025-01-22T16:32:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85496
attention,weilhuan-quic,Integrate Op Builder with LiteRT Compile Part," WHAT We replace the compiler part with Qualcomm implementations. This PR include commits in 3 PRs below. Follow these PRs or commit should help you review. You can get more details in the PR descriptions. 1. https://github.com/jiunkaiy/tensorflow/pull/1 2. https://github.com/jiunkaiy/tensorflow/pull/3 3. https://github.com/jiunkaiy/tensorflow/pull/5  TEST You can checkout this branch and run this test: ``` bazel build  c opt cxxopt=std=c++20 //tensorflow/lite/experimental/litert/vendors/qualcomm/compiler:qnn_compiler_plugin_test ./bazelbin/tensorflow/lite/experimental/litert/vendors/qualcomm/compiler/qnn_compiler_plugin_test ``` I disable these models because I don't have them.  kFeedForwardModel,  kKeyEinsumModel,  kQueryEinsumModel,  kValueEinsumModel,  kAttnVecEinsumModel,  kROPEModel,  kLookUpROPEModel,  kRMSNormModel,  kSDPAModel,  kAttentionModel,  kTransformerBlockModel,  kQSimpleMul16x16Model,  kQMulAdd16x16Model,  kQQueryEinsum16x8Model,  kQKeyEinsum16x8Model,  kQVauleEinsum16x8Model,  kQAttnVecEinsum16x8Model And you will see ``` [] Global test environment teardown [==========] 55 tests from 3 test suites ran. (338 ms total) [  PASSED  ] 54 tests. [  FAILED  ] 1 test, listed below: [  FAILED  ] SupportedOpsTest/QnnPluginOpValidationTest.SupportedOpsTest/4, where GetParam() = ""simple_slice_op.tflite"" ``` There are some bugs in simple_slice_op.mlir so the op validation will fail.",2025-01-22T10:19:36Z,comp:lite size:XL,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85477
tpu,weilhuan-quic,Support Qnn Wrappers for LiteRt," WHAT  Basic wrapper for QNN types, handle dynamic resources along with wrapper instances.  Make these wrappers independent to LiteRT/tflite  Only depend on QNN and STL   `ScalarParamWrapper`  Wrap `Qnn_Param_t` with `QNN_PARAMTYPE_SCALAR` for `paramType`   Choose correct `QNN_DATATYPE` based on the data type  `TensorParamWrapper`  Wrap `Qnn_Param_t` with `QNN_PARAMTYPE_TENSOR` for `paramType`  `UndefinedQuantizeParamsWrapper`  Wrap `Qnn_QuantizeParams_t`  Default for quantization parameter  `ScaleOffsetQuantizeParamsWrapper`  Wrap `Qnn_QuantizeParams_t` for pertensor quantization   `AxisScaleOffsetQuantizeParamsWrapper`  Wrap `Qnn_QuantizeParams_t`  for peraxis quantization  `TensorWrapper`  Wrap `Qnn_TensorType_t`  Handle dynamic resource, e.g. name, dimensions, weight data.  `OpWrapper`  Wrap `Qnn_OpConfig_t`  Handle dynamic resource, e.g. name, input output tensors, params",2025-01-22T09:57:06Z,awaiting review comp:lite size:L,closed,0,1,https://github.com/tensorflow/tensorflow/issues/85476,> Can this be developed outside of TF? As a plugin or a repo that uses TF as a dependency? >  > We're trying to reduce the amount of code that lands into the repo to not get (for at least the third time) into a situation where code becomes unmaintained because the original submitters abandoned the project Hi  let's chat offline
strategy,copybara-service[bot],PR #21412: [GPU] Use single cuDNN handle for graph deserialization.,"PR CC(Error in Distribution Strategy with train_and_evaluate): [GPU] Use single cuDNN handle for graph deserialization. Imported from GitHub PR https://github.com/openxla/xla/pull/21412 There are two ways to get a cuDNN handle in cuda_dnn.cc. Execution uses a single mutexlocked handle (GetHandle()); compilation uses disposable temporary handles for efficient parallelism (GetLocalHandle()). Deserialization, which happens during initialization of the GPU executable and is serial, can use either way, but is slightly more efficient when uses the single mutexlocked handle. Copybara import of the project:  4986c6bc200eb12de7b6ffef859800d2a9da3ff5 by Ilia Sergachev : [GPU] Use single cuDNN handle for graph deserialization. There are two ways to get a cuDNN handle in cuda_dnn.cc. Execution uses a single mutexlocked handle (GetHandle()); compilation uses disposable temporary handles for efficient parallelism (GetLocalHandle()). Deserialization, which happens during initialization of the GPU executable and is serial, can use either way, but is slightly more efficient when uses the single mutexlocked handle. Merging this change closes CC(Error in Distribution Strategy with train_and_evaluate) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21412 from openxla:cudnn_handle_deserialization 4986c6bc200eb12de7b6ffef859800d2a9da3ff5",2025-01-22T09:49:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85475
tpu,copybara-service[bot],[tpu/kernels] _pywrap_sparse_core_layout_header_only: Fix deps,"[tpu/kernels] _pywrap_sparse_core_layout_header_only: Fix deps Add missing :btree dep, remove unneeded :log dep.",2025-01-22T09:40:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85473
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-22T08:43:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85452
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-22T08:00:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85450
tpu,rizkyy702,issue, Issue type Others  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? import tensorflow as tf from google.colab import files uploaded_zip_file = list(files.upload().keys())[0] !unzip $uploaded_zip_file tf_model = tf.keras.models.load_model(uploaded_zip_file[:4])  Standalone code to reproduce the issue ```shell import tensorflow as tf from google.colab import files uploaded_zip_file = list(files.upload().keys())[0] !unzip $uploaded_zip_file tf_model = tf.keras.models.load_model(uploaded_zip_file[:4]) ```  Relevant log output ```shell ```,2025-01-22T07:52:23Z,type:others,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85449,"Closing as there's no error message, no informative title and the template doesn't seem to be filled correctly (TF 2.0 and nightly surely have diverged)",Are you satisfied with the resolution of your issue? Yes No
tpu,copybara-service[bot],PR #21616: [ROCM] Avoiding lazy initialization of blas handles on ROCM,PR CC([Java] Render secondary factory for default output types): [ROCM] Avoiding lazy initialization of blas handles on ROCM Imported from GitHub PR https://github.com/openxla/xla/pull/21616 Calling AsBlas() under stream capture causes runtime errors on ROCM side: ``` Hip error: 'operation would make the legacy stream depend on a capturing blocking stream'(906) at /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/hipBLASLt/library/src/amd_detail/hipblaslt.cpp:135 E0000 00:00:1737366478.217495  154310 rocm_blas.cc:130] failed to create rocBLAS handle: rocblas_status_internal_error ``` We avoid this by initializing Blas handles earlier in GpuExecutor::Init() function. Also I added a shardedcomputation test which verifies this issue.  rotation: would you please have a look? Copybara import of the project:  e10e5cd62f5c8ec1b3429391dc2e05e4e123cf07 by Pavel Emeliyanenko : Avoiding lazy initialization of blas handles on ROCM  645b83c1b07be74fba7fa994a2b764b3549335ac by Pavel Emeliyanenko : added missing hlo file  4d4ef9129bfe7afd8946e355b74d0600aaa3a130 by Pavel Emeliyanenko : added mutexes to InitBlas and AsBlas functions Merging this change closes CC([Java] Render secondary factory for default output types) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21616 from ROCm:ci_blas_stream_capture_fix 4d4ef9129bfe7afd8946e355b74d0600aaa3a130,2025-01-22T07:03:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85444
tpu,copybara-service[bot],[MHLO] Handle dynamic dimensions in HLO<->MHLO,"[MHLO] Handle dynamic dimensions in HLOMHLO  Fix creating constant zero for ConvertOp HLO>MHLO translation  Fix broadcast in dim bounded lowering from MHLO>HLO  Don't use StableHLO verification methods on MHLO ReshapeOp with bounded dynamic outputs ``` $ cat /tmp/t.hlo  HloModule main, entry_computation_layout={(pred[pred[ pred[>) > tensor> {   %0 = mhlo.constant dense : tensor   %1 = ""mhlo.get_dimension_size""(%arg0)  : (tensor>) > tensor   %2 = ""mhlo.set_dimension_size""(%0, %1)  : (tensor, tensor) > tensor>   %3 = mhlo.compare  NE, %arg0, %2 : (tensor>, tensor>) > tensor>   return %3 : tensor> } ``` Currently this fails when trying to create the `mhlo.constant dense` that gets fed into compare since constants cannot have a bounded size.",2025-01-22T03:54:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85441
opt,copybara-service[bot],Fix a segmentation fault issue when the -c opt is turned on. Keep the function object around for absl::FunctionRef,Fix a segmentation fault issue when the c opt is turned on. Keep the function object around for absl::FunctionRef,2025-01-21T22:08:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85429
opt,copybara-service[bot],[MSA] Microoptimizations in `AsynchronousCopyResource`.,"[MSA] Microoptimizations in `AsynchronousCopyResource`. Based on a profiling the memoryspace assignment algorithm, this change makes two small optimizations to `AsynchronousCopyResource`: * Pass a prereserved `std::vector>` instead of an `absl::flat_hash_map` to capture the changes to `delays`, because we do not need random access to the map, and a vector is faster to resize than a hash map. * Cache the raw data pointers from `std::vector` to avoid the overhead of bounds and null checking in the hardened `std::vector` implementation. * Replace the simple functions in `time_utils.cc` with inline implementations in `time_utils.h`: since these boil down to adding or subtracting `1`, the resulting code will be smaller and more efficient (and less likely to spill FP registers to the stack). * Refactor the innerloop that writes `delay_changes` so that the floatingpoint operations are not separated by a datadependent call, and we can keep more `float`s in registers.",2025-01-21T22:06:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85428
opt,copybara-service[bot],Remove `use_parameter_layout_on_device`.,"Remove `use_parameter_layout_on_device`. With the removal of calls to `UpdateEntryComputationLayout`, it turns out this functionality is not necessary (and potentially harmful). Instead we opt to always respect the ECLprovided layout if the client supports it, or fall back to using the implementationdefined buffer ondevice layout otherwise. This patch also removes the remaining HloRunnerPjRt calls to `UpdateEntryComputationLayout`.",2025-01-21T21:35:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85425
tpu,copybara-service[bot],xla_compile_main: Make --output_file optional.,"xla_compile_main: Make output_file optional. Some compileonly use cases want the optimized binary or even just to verify that compilation succeeded, and the final binary can be large.",2025-01-21T21:34:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85424
quantization,copybara-service[bot],Remove tf/compiler/mlir/lite/quantization/ir:QuantizationOpsTdFiles from tf/compiler/mlir/tfr:tfr_ops_td_files,Remove tf/compiler/mlir/lite/quantization/ir:QuantizationOpsTdFiles from tf/compiler/mlir/tfr:tfr_ops_td_files,2025-01-21T19:50:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85416
opt,copybara-service[bot],Add mask as optional input for converter,Add mask as optional input for converter,2025-01-21T19:01:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85412
opt,copybara-service[bot],Fix a segmentation fault issue when the `-c opt` is turned on. Keep the function object around for absl::FunctionRef,Fix a segmentation fault issue when the `c opt` is turned on. Keep the function object around for absl::FunctionRef,2025-01-21T17:24:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85406
sharding,copybara-service[bot],[XLA:GPU] Add matmul perf table gen sharding support via GNU parallel.,"[XLA:GPU] Add matmul perf table gen sharding support via GNU parallel. We support the partitioning granularity at HLO level, but in theory we can squeeze the most parallelism if we have granularity at HLO op level. This would probably require serialization of `ExplicitSpec` abstraction and introducing a twostep process in table generation. For now the implemented approach is good enough, we can revisit potential improvements later.",2025-01-21T15:26:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85400
opt,copybara-service[bot],[XLA:GPU] Rename `xla_gpu_enable_experimental_pipeline_parallelism_opt` to `xla_gpu_experimental_enable_pipeline_parallelism_opt`.,[XLA:GPU] Rename `xla_gpu_enable_experimental_pipeline_parallelism_opt` to `xla_gpu_experimental_enable_pipeline_parallelism_opt`. This is to follow the agreed upon flag nomenclature.,2025-01-21T14:10:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85396
opt,copybara-service[bot],[XLA:GPU] Remove the no-op `xla_gpu_triton_fusion_level` from the debug options.,[XLA:GPU] Remove the noop `xla_gpu_triton_fusion_level` from the debug options.,2025-01-21T13:58:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85395
tpu,alvinwong64,Tensorflow 2.14.0 installation/run on C++ in visual studio code," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.14  Custom code Yes  OS platform and distribution macos 14.4  Mobile device _No response_  Python version _No response_  Bazel version 6.1  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi I would like to try to run a custom model on C++, how should I setup the tensorflow so that i could call and use function like i did in python?  Standalone code to reproduce the issue ```shell tried bazel build //tensorflow/tools/pip_package, yet it pops up error  Skipping '//tensorflow/tools/pip_package': no such target '//tensorflow/tools/pip_package:pip_package': target 'pip_package' not declared in package 'tensorflow/tools/pip_package' defined by /Users/XXXX/Documents/testing/tensorflow/tensorflow/tools/pip_package/BUILD (Tip: use `query ""//tensorflow/tools/pip_package:*""` to see all the targets in that package) WARNING: Target pattern parsing failed. ERROR: no such target '//tensorflow/tools/pip_package:pip_package': target 'pip_package' not declared in package 'tensorflow/tools/pip_package' defined by /Users/XXXX/Documents/testing/tensorflow/tensorflow/tools/pip_package/BUILD (Tip: use `query ""//tensorflow/tools/pip_package:*""` to see all the targets in that package) ```  Relevant log output ```shell ```",2025-01-21T09:26:43Z,stat:awaiting response type:build/install stale awaiting PR merge TF2.14,closed,0,8,https://github.com/tensorflow/tensorflow/issues/85385,"you need to set up TensorFlow for C++ development, the target //tensorflow/tools/pip_package:pip_package is specifically for creating Python pip packages, not for compiling TensorFlow for C++ use.  Install TensorFlow C++ Library,Build TensorFlow C++ API,Configure Build Settings,Build TensorFlow Shared Libraries then load and run your model",Could you guide me on how to install tensorflow c++ library and build it?,"yes ofc !  System Requirements: Linuxbased system,C++ compiler ,Python installed,Bazel  Install C++ Build Dependencies:  sudo aptget install buildessential Install Bazelisk  curl LO https://github.com/bazelbuild/bazelisk/releases/download/v1.10.0/bazelisklinuxamd64 chmod +x bazelisklinuxamd64 mv bazelisklinuxamd64 /usr/local/bin/bazel StepbyStep Installation: Clone the TensorFlow GitHub Repository git clone https://github.com/tensorflow/tensorflow.git cd tensorflow Configure the Build ./configure You’ll be prompted with several configuration options, including whether to enable CUDA (GPU support) and whether to build TensorFlow for specific systems or features. For building with GPU support, you’ll need to have the CUDA toolkit and cuDNN installed. Build the C++ Library bazel build //tensorflow:libtensorflow_cc.so This will build the TensorFlow C++ library as a shared object (.so) file. If you want to build a static library, you can modify the build command accordingly. For debugging and testing, you want to build the tensorflow_cc library with the c dbg flag: bazel build c dbg //tensorflow:libtensorflow_cc.so This will take some time, depending on your system's hardware. Include TensorFlow C++ Headers and Libraries located in the bazelbin/ directory.  bazelbin/ tensorflow","Hi while building, the build failed, with the use of verbose_failures ERROR: /Users/001522/Documents/testing/tensorflow/tensorflow/lite/acceleration/configuration/BUILD:41:8: Executing genrule //tensorflow/lite/acceleration/configuration:configuration_schema failed: (Exit 127): bash failed: error executing command (from target //tensorflow/lite/acceleration/configuration:configuration_schema)    (cd /private/var/tmp/_bazel_001522/497fb1666e8c0e255ebeec937d6e9115/execroot/org_tensorflow && \   exec env  \     PATH=/Users/001522/Library/Caches/bazelisk/downloads/sha256/c6b6dc17efcdf13fba484c6fe0b6c3361b888ae7b9573bc25a2dbe8c502448eb/bin:/Users/001522/anaconda3/bin:/Users/001522/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/usr/local/share/dotnet:~/.dotnet/tools \     PYTHON_BIN_PATH=/Users/001522/anaconda3/bin/python3 \     PYTHON_LIB_PATH=/Users/001522/anaconda3/lib/python3.10/sitepackages \     TF2_BEHAVIOR=1 \   /bin/bash c 'source external/bazel_tools/tools/genrule/genrulesetup.sh;          bazelout/darwin_arm64optexec50AE0418/bin/external/flatbuffers/flatc proto o bazelout/darwin_arm64opt/bin/tensorflow/lite/acceleration/configuration tensorflow/lite/acceleration/configuration/configuration.proto         perl p i e '\''s/tflite.proto/tflite/'\'' bazelout/darwin_arm64opt/bin/tensorflow/lite/acceleration/configuration/configuration.fbs     ')   Configuration: 156e18005636972e4bcf95905ead517f4bd9575d761ba24552404ce2ea964aec    Execution platform: //:platform : command not found Target //tensorflow:libtensorflow_cc.so failed to build",",  The build targets have changed. Please try the below command. `bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow_cpu` And also it might fail if you don't have  dependencies (the instructions assume Ubuntu OSes, for other operating systems you have to find your own dependencies and install them). Also tensorflow 2.14 is pretty old, please try to install the tensorflow v2.18 and also try to follow the steps mentioned in the below document. https://www.tensorflow.org/install/sourcemacos Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,Reyadeyat,How to enable compiler flags for avx2 avx512f fma,"I've compiled Tensorflow on Debian 12 successfully but when use jupyter I get this message **To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild Tensorflow with the appropriate compiler flags.** ``` the compilation is like this: $ python ./configure.py Please specify optimization flags to use during compilation when bazel option ""config=opt"" is specified [Default is Wnosigncompare]: Wnosigncompare march=native mavx2 mavx512f mfma msse4.1 msse4.2 $ bazelisk build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow_cpu local_ram_resources=2048 config=monolithic verbose_failures c opt config=opt copt=Wnognuoffsetofextensions copt=march=native copt=mavx copt=mavx2 copt=mfma copt=msse4.1 copt=msse4.2 ``` How to correctly compile tensorflow with AVX2 AVX512F FMA flags",2025-01-20T21:35:22Z,stat:awaiting response stale,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85365,"Verify Hardware Support,Configure TensorFlow,Compile with Bazel,Verify Build Success,Test the Build with print(tf.config.list_physical_devices('CPU'))",", In order to expedite the troubleshooting process, could you please provide the CUDA/cuDNN version, GPU model and also provide the tensorflow version which you are trying. Along with the information try to provide the error log which helps to analyse the issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
opt,copybara-service[bot],[xla:cpu] Use Eigen::ThreadPoolDevice in XLA CPU Kernel,"[xla:cpu] Use Eigen::ThreadPoolDevice in XLA CPU Kernel We always use intraop device to run XLA:CPU kernels, stop pretending that we might have some other option (until we have a real alternative).",2025-01-20T18:11:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85359
tpu,Chadster766,"Tutorial ""Multi-worker training with Keras"" fails to complete"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version v1.12.1120353gc5bd67bc56f 2.19.0dev20250107  Custom code No  OS platform and distribution Debian 6.1.1231 (20250102) x86_64 GNU/Linux  Mobile device _No response_  Python version Python 3.12.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following the tutorial everything goes well until you start the second worker. Then the below failure occures. 20250120 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250120 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250120 07:19:35.307721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250120 07:19:36.510476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDAcapable device is detected 20250120 07:19:36.510494: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""1"" 20250120 07:19:36.510499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to 1  this hides all GPUs from CUDA 20250120 07:19:36.510501: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually v=1 or vmodule=cuda_diagnostics=1) to get more diagnostic output from this module 20250120 07:19:36.510505: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael 20250120 07:19:36.510507: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: michael 20250120 07:19:36.510562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0 20250120 07:19:36.510572: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0 20250120 07:19:36.510574: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0 20250120 07:19:36.519175: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:637] Initializing CoordinationService WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc://localhost:12345 20250120 07:19:36.524874: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4677280066871850635 20250120 07:19:36.524894: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 1/2 tasks to connect. 20250120 07:19:36.524898: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:819] Example stragglers: /job:worker/replica:0/task:1 I0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected. 20250120 07:22:27.996664: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 13530699364709055870 20250120 07:22:27.996686: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 0/2 tasks to connect. /home/chad/anaconda3/lib/python3.12/sitepackages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.   warnings.warn( 20250120 07:22:28.461733: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations. Traceback (most recent call last):   File ""/home/chad/Documents/McCueFiles/NeuralNetworks/TensorFlowProject/TensorFlowDocExample/main.py"", line 21, in      multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)   File ""/home/chad/anaconda3/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/home/chad/anaconda3/lib/python3.12/sitepackages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor     return ops.EagerTensor(value, ctx.device_name, dtype)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ValueError: Attempt to convert a value (PerReplica:{   0:  }) with an unsupported type () to a Tensor.  Standalone code to reproduce the issue ```shell python main.py &> job_1.log ```  Relevant log output ```shell 20250120 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250120 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250120 07:19:35.307721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250120 07:19:36.510476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDAcapable device is detected 20250120 07:19:36.510494: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""1"" 20250120 07:19:36.510499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to 1  this hides all GPUs from CUDA 20250120 07:19:36.510501: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually v=1 or vmodule=cuda_diagnostics=1) to get more diagnostic output from this module 20250120 07:19:36.510505: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael 20250120 07:19:36.510507: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: michael 20250120 07:19:36.510562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0 20250120 07:19:36.510572: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0 20250120 07:19:36.510574: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0 20250120 07:19:36.519175: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:637] Initializing CoordinationService WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc://localhost:12345 20250120 07:19:36.524874: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4677280066871850635 20250120 07:19:36.524894: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 1/2 tasks to connect. 20250120 07:19:36.524898: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:819] Example stragglers: /job:worker/replica:0/task:1 I0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected. 20250120 07:22:27.996664: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 13530699364709055870 20250120 07:22:27.996686: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 0/2 tasks to connect. /home/chad/anaconda3/lib/python3.12/sitepackages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.   warnings.warn( 20250120 07:22:28.461733: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations. Traceback (most recent call last):   File ""/home/chad/Documents/McCueFiles/NeuralNetworks/TensorFlowProject/TensorFlowDocExample/main.py"", line 21, in      multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)   File ""/home/chad/anaconda3/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/home/chad/anaconda3/lib/python3.12/sitepackages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor     return ops.EagerTensor(value, ctx.device_name, dtype)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ValueError: Attempt to convert a value (PerReplica:{   0:  }) with an unsupported type () to a Tensor. ```",2025-01-20T14:03:18Z,type:bug TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/85351,"Hi **** , Apologies for the delay, and thank you for raising your concern here. Could you please provide more details, such as the versions of TensorFlow and any other relevant libraries you are using? Additionally, sharing your code would make it easier for us to troubleshoot the issue effectively. In the meantime, please ensure that all compatibility requirements are met. For your reference, here is the relevant documentation. Thank you!","Hi  , I think I've provided the info regarding versions of TensorFlow and any other relevant libraries in the issue creation. I'm not running any of my code I'm just using the jupyter notebook of the tutorial."
opt,wokron,Fix Bug in XSpace to StepStats Conversion for GPU Tracing to Keep Compatibility with RunMetadata,"In TensorFlow v2, we use `ConvertGpuXSpaceToStepStats` to convert `XSpace` data to `StepStats` used in v1. However, the timestamps in `XSpace` are relative and were not converted to the absolute time used in `RunMetadata` during the conversion. This discrepancy affected the correctness of analysis tools like `timeline.Timeline` in v1. See CC(TF timeline timestamp was shifted. The resultant timeline cannot be shown correctly in chrome tracing viewer) and tensorflow/profiler issue CC(Empty input to conv2d causes floating point exception). The following code can reproduce this issue: ```py import tensorflow as tf from tensorflow.python.client import timeline tf.compat.v1.disable_eager_execution() matrix1 = tf.constant([[3.0, 3.0]]) matrix2 = tf.constant([[2.0], [2.0]]) matrix3 = tf.constant([[1.0, 2.0], [3.0, 4.0]]) matrix4 = tf.constant([[2.0, 0.0], [1.0, 2.0]]) product1 = tf.matmul(matrix1, matrix2) product2 = tf.matmul(matrix3, matrix4) sum_product = tf.add(product1, product2) run_options = tf.compat.v1.RunOptions(     trace_level=tf.compat.v1.RunOptions.HARDWARE_TRACE ) run_metadata = tf.compat.v1.RunMetadata() with tf.compat.v1.Session() as sess:     result = sess.run(sum_product, options=run_options, run_metadata=run_metadata)     print(result) tl = timeline.Timeline(run_metadata.step_stats) ctf = tl.generate_chrome_trace_format() with open(""timeline.json"", ""w"") as f:     f.write(ctf) ``` Here is the result timeline_error.json, where some timestamps are in relative time, such as `13427`, while others are in absolute time, such as `1737358838975242`. This pull request addresses the issue by converting relative timestamps to absolute timestamps in the `SetNodeTimes` function.",2025-01-20T08:44:22Z,awaiting review ready to pull size:S comp:core,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85339
yi,NLLAPPS,Cannot use GpuDelegate - java.lang.IllegalArgumentException: Internal error: Cannot create interpreter,"Hi, I get ""Cannot use GpuDelegate  java.lang.IllegalArgumentException: Internal error: Cannot create interpreter"" when attempting to use GpuDelegate I have seen a couple of issue related to this but all seems to be abandoned. I have created a repo replicating the issue.  You can see the config at https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.javaL114 **System information**  Android Device information: Samsung S23  TensorFlow Lite in Play Services SDK version : 16.4.0  Google Play Services version: 24.50.34 **Standalone code to reproduce the issue** Clone and run project from https://github.com/NLLAPPS/WhisperOffline/ **Any other info / logs** `Created TensorFlow Lite delegate for GPU. Created interpreter. Created interpreter. java.lang.IllegalArgumentException: Internal error: Cannot create interpreter:  at com.google.android.gms.tflite.NativeInterpreterWrapper.createInterpreter(Native Method) at com.google.android.gms.tflite.NativeInterpreterWrapper.zzs(com.google.android.gms:playservicestflitejava@.4.0:34) at com.google.android.gms.tflite.NativeInterpreterWrapper.(com.google.android.gms:playservicestflitejava@.4.0:14) at com.google.android.gms.tflite.zzd.(com.google.android.gms:playservicestflitejava@.4.0:3) at com.google.android.gms.tflite.InterpreterFactoryImpl.create(com.google.android.gms:playservicestflitejava@.4.0:4) at org.tensorflow.lite.InterpreterApi.create(InterpreterApi.java:373) at com.whispertflite.engine.WhisperEngineJava.loadModel(WhisperEngineJava.java:131) at com.whispertflite.engine.WhisperEngineJava.initialize(WhisperEngineJava.java:44) at com.whispertflite.asr.WhisperJava.loadModel(WhisperJava.java:72) at com.whispertflite.asr.WhisperJava.loadModel(WhisperJava.java:67) at com.whispertflite.MainActivity.initModel(MainActivity.java:247) at com.whispertflite.MainActivity.lambda$onCreate$3$comwhispertfliteMainActivity(MainActivity.java:155) at com.whispertflite.MainActivity$$ExternalSyntheticLambda0.run(D8$$SyntheticClass:0) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:644) at java.lang.Thread.run(Thread.java:1012)`",2025-01-19T22:01:13Z,comp:lite TFLiteGpuDelegate,open,0,12,https://github.com/tensorflow/tensorflow/issues/85313,"there is an issue with the configuration of the GpuDelegate or its compatibility with your environment, Test with CPUonly execution to confirm whether the issue is specific to the GPU Delegate","Hi and thank you. I have tested CPU only it works fine. What is the ""configuration of the GpuDelegate""? My configuration can be seen at https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.javaL114","'GpuDelegateFactory.Options' in your code uses generic settings. For better control, you can configure options like precision or inference preference. try this or Run a compatibility check  GpuDelegateFactory.Options gpuOptions = new GpuDelegateFactory.Options(); gpuOptions.setInferencePreference(GpuDelegateFactory.Options.INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER); gpuOptions.setPrecisionLossAllowed(true); ","Thanks, regarding ""compatibility check"". Is it possible to provide link to documentation for compatibility checking?",check out these links : https://stackoverflow.com/questions/50622525/whichtensorflowandcudaversioncombinationsarecompatible https://docs.nvidia.com/cuda/cudatoolkitreleasenotes/index.html https://docs.nvidia.com/deeplearning/cudnn/latest/reference/supportmatrix.html,"Your links seem to be related to PCs. Have I misunderstood what GpuDelegateFactory does. I thought it would be for using GPU on the phone since the artifact is ""playservicestflitegpu""",I also just noticed you are not related to this project. Do you have experience on implementing tflite on Android?,"you are correct in assuming that GpuDelegateFactory is intended for mobile devices to leverage the GPU for TensorFlow Lite inference, especially in Android using the playservicestflitegpu artifact. If you're targeting mobile platforms, this is the correct path to enable GPU acceleration dependencies {     implementation 'org.tensorflow:tensorflowlite:2.x.x'     implementation 'org.tensorflow:tensorflowlitegpu:2.x.x' } GpuDelegate delegate = new GpuDelegate(); Interpreter.Options options = new Interpreter.Options().addDelegate(delegate); Interpreter interpreter = new Interpreter(modelFile, options);","yes i am not a part of this project yet , im trying to contribute as much possible to be recognized by the organization before gsoc 2025","> yes i am not a part of this project yet , i'm trying to contribute as much possible to be recognized by the organization before gsoc 2025 I don't think you will be able to help me in this case. Issue seems to be related to actual SDK/API. Hopefully  will have a look at it.","Hi,   I apologize for the delayed response, I was trying to reproduce the similar issue from my end after cloning your provided repo but I'm getting below error message, if possible could you please help me to replicate the same issue from my end which you reported in the issue template to investigate this issue further from our end ? **Here is error log for reference :** ``` FAILURE: Build failed with an exception. * What went wrong: A problem occurred configuring root project 'WhisperOffline'. > Could not resolve all files for configuration ':classpath'.    > Could not find com.android.tools.build:gradle:8.10.2.      Searched in the following locations:         https://dl.google.com/dl/android/maven2/com/android/tools/build/gradle/8.10.2/gradle8.10.2.pom         https://repo.maven.apache.org/maven2/com/android/tools/build/gradle/8.10.2/gradle8.10.2.pom      Required by:          project : * Try: > Run with stacktrace option to get the stack trace. > Run with info or debug option to get more log output. > Run with scan to get full insights. > Get more help at https://help.gradle.org. BUILD FAILED in 829ms ``` Thank you for your cooperation and patience.","Hi, project is using com.android.tools.build:gradle:8.8.0 there is no com.android.tools.build:gradle:8.10.2. 8.10.2 is a Gradle version, not Android build tools version. Have you changed anything? This stack overflow post seems to suggest it may be related to Android Studio Gradle Settings.  Here is mine attached for the project !Image"
tpu,metal3d,LD_LIBRARY_PATH to set when installing tensorflow[and-cuda] with pip or poetry," Issue type Documentation Feature Request  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code No  OS platform and distribution Linux Fedora 41  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory Nvidia RTX 3060  Current behavior? Installing `tensorflow[andcuda]` is very easy and avoid installing cuda packages on our computers. It's then very confortable to be able to drop virtual envs. But there is a bad behavior. From scratch: ```bash rm rf .venv mkdir .venv poetry add ""tensorflow[andcuda]""  check poetry run python mytest.py W0000 00:00:1737317404.258825   10111 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. ``` As you can see, no CUDA shared library is loaded. The solution is to generate the `LD_LIBRARY_PATH` finding directories name of any "".so"" files related to ""nvidia"": ```bash export LD_LIBRARY_PATH=$(find .venv name ""*.so*""  paste d: s ) echo $LD_LIBRARY_PATH .venv/lib/python3.12/sitepackages/nvidia/cublas/lib:.venv/lib/python3.12/sitepackages/nvidia/cuda_cupti/lib:.venv/lib/python3.12/sitepackages/nvidia/cuda_nvcc/nvvm/lib64:.venv/lib/python3.12/sitepackages/nvidia/cuda_nvrtc/lib:.venv/lib/python3.12/sitepackages/nvidia/cuda_runtime/lib:.venv/lib/python3.12/sitepackages/nvidia/cudnn/lib:.venv/lib/python3.12/sitepackages/nvidia/cufft/lib:.venv/lib/python3.12/sitepackages/nvidia/curand/lib:.venv/lib/python3.12/sitepackages/nvidia/cusolver/lib:.venv/lib/python3.12/sitepackages/nvidia/cusparse/lib:.venv/lib/python3.12/sitepackages/nvidia/nccl/lib:.venv/lib/python3.12/sitepackages/nvidia/nvjitlink/lib  and now: poetry run python mytest.py I0000 00:00:1737317342.317060    9825 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4169 MB memory:  > device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6 ``` I'm not sur if I have to create the issue here or to Nvidia (where?), but it could be interesting to propose this solution for who has the same problem.  Standalone code to reproduce the issue ```shell poetry run python c ""import tensorflow;tensorflow.keras.Sequential().compile()"" ```  Relevant log output ```shell ```",2025-01-19T20:13:12Z,stat:awaiting response type:feature type:build/install stale type:docs-feature,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85311,"TensorFlow installed with tensorflow[andcuda] doesn't automatically set up the LD_LIBRARY_PATH environment variable to include the locations of the NVIDIA shared libraries packaged with the installation. Your solution is an effective way to resolve the problem by dynamically setting LD_LIBRARY_PATH to include the necessary directories containing the .so files.  et LD_LIBRARY_PATH dynamically to include the directories containing the NVIDIA .so files within the virtual environment use this command  export LD_LIBRARY_PATH=$(find .venv name ""*.so*""  paste d: s ) then verify echo $LD_LIBRARY_PATH",", Thank you for reporting the issue. This is a known issue where other issues are still open and developers are working on the same. I request you to take a look at this https://github.com/tensorflow/tensorflow/issues/62075 and where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. https://github.com/tensorflow/tensorflow/issues/70947 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],[xla:cpu:xnn] Extract WorkQueue into a separate target,[xla:cpu:xnn] Extract WorkQueue into a separate target Added a benchmark for popping a task from a queue  Benchmark           Time             CPU   Iterations  BM_PopTask       2.63 ns         2.63 ns    265317289,2025-01-19T19:58:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85310
out of memory,maxima120,Force TF to log GPU memory allocation, Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.18.0  Custom code No  OS platform and distribution Debian 12  Mobile device _No response_  Python version 3.11.2  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.2  GPU model and memory RTX 3060 12Gb  Current behavior? If I ran out of GPU memory I get error saying `Allocator (GPU_0_bfc) ran out of memory trying to allocate 9.78GiB` But if allocation succeeded there is no real way to know what the allocator did/doing.. I know that you can use nvidiasmi etc but this is an indirect way to estimate very crudely whats going on. What I believe would be very beneficial (because I dont know about you guys but I run out of memory so very often and I cant affort 128Gb card)  is to be able to force TF to log every and single one GPU memory allocation.  So I can see whats going on my healthy models and whats in the failing.,2025-01-19T13:50:42Z,stat:awaiting tensorflower type:feature comp:gpu,open,0,0,https://github.com/tensorflow/tensorflow/issues/85303
tpu,Animemchik,Unable to install TensorFlow: No matching distribution found for TensorFlow!," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.8  Custom code Yes  OS platform and distribution Windows 10  Mobile device _No response_  Python version 3.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I expected to install TensorFlow  Standalone code to reproduce the issue Can't install TensorFlow with pip ```shell > pip install tensorflow ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) [notice] A new release of pip is available: 24.2 > 24.3.1       [notice] To update, run: python.exe m pip install upgrade pip ERROR: No matching distribution found for tensorflow ``` Pls help  Relevant log output ```shell ```",2025-01-19T07:18:59Z,stat:awaiting response type:build/install type:support stale TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/85298,"There was no python 2.13 at the time of tensorflow 2.8 release. If you want to use TF 2.8 you need to use a Python version that is supported there. If you want to use Python 3.13, please see CC(It doesn't support on python3.13) ",> There was no python 2.13 at the time of tensorflow 2.8 release. >  > If you want to use TF 2.8 you need to use a Python version that is supported there. I also tried 2.7 and 2.6 but still can't do it,"TF 2.7 and TF 2.6 are even older, so of course they won't work with Python 3.13 either",TensorFlow requires specific Python versions depending on the TensorFlow version. try Python 3.8 to 3.11,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,LongZE666,Aborted in `tensorflow.nn.depthwise_conv2d_backprop_filter`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.nn.depthwise_conv2d_backprop_filterr` triggers crash. The crash occurs in the  operator DepthwiseConv2dNativeBackpropFilter  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow filter_sizes = np.ones((13, 2, 5, 4), dtype=np.uint32) input = 26262.2175547925 out_backprop = 14557.552005335412 strides = [] padding = 'VALID' tensorflow.nn.depthwise_conv2d_backprop_filter(input=input, filter_sizes=filter_sizes, out_backprop=out_backprop, strides=strides, padding=padding) ```  Relevant log output ```shell 20250118 12:47:13.553183: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 0, 0, N Aborted (core dumped) ```",2025-01-18T12:49:02Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85258,"Hi **** , Apologies for the delay, and thank you for raising your concern here. I tested your code on Colab using TensorFlow 2.17.0, 2.18.0, and the nightly versions. The code is throwing a proper error message instead of aborting. Please find the gist here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,LongZE666,Aborted in `tensorflow.compat.v1.nn.conv2d_backprop_filter`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.compat.v1.nn.conv2d_backprop_filter` triggers crash. The crash occurs in the  operator Conv2DBackpropFilter  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow input = np.array([1.2807603e+38], dtype=np.float32) filter_sizes = np.ones((20, 6, 5, 3), dtype=np.uint16) strides = [4538426903074820908] padding = 'VALID' out_backprop = np.ones((19, 5, 11, 4), dtype=np.float32) tensorflow.compat.v1.nn.conv2d_backprop_filter(input=input, filter_sizes=filter_sizes, out_backprop=out_backprop, strides=strides, padding=padding) ```  Relevant log output ```shell 20250118 12:34:39.686592: F tensorflow/core/common_runtime/mkl_layout_pass.cc:2754] NonOKstatus: GetNodeAttr(orig_node>def(), ""strides"", &strides) status: INVALID_ARGUMENT: Attr strides has value 4538426903074820908 out of range for an int32 Aborted (core dumped) ```",2025-01-18T12:35:54Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85257,", I tried to execute the mentioned code and observed that the attribute error which might be due to a value is too large to fit into a 32bit signed integer variable. Could you try to provide the input which fits the stride and test the code.  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,LongZE666,Aborted in `tensorflow.raw_ops.ScatterNdNonAliasingAdd`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.raw_ops.ScatterNdNonAliasingAdd` triggers crash. The crash occurs in the  operator ScatterNdNonAliasingAdd  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow input = np.array([False,  True,  True, False,  True, False,  True, False, False,      False, False,  True,  True, False, False,  True,  True]) indices = 1547981256 updates = np.array([ True, False,  True, False,  True]) tensorflow.raw_ops.ScatterNdNonAliasingAdd(input=input, indices=indices, updates=updates) ```  Relevant log output ```shell 20250118 12:28:50.452498: F ./tensorflow/core/framework/tensor.h:847] Check failed: new_num_elements == NumElements() (1 vs. 5) Aborted (core dumped) ```",2025-01-18T12:30:20Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85256,"Hi **** , Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly versions and initially faced the same issue. Upon investigating, the main cause seems to be a mismatch in the dimensions and values. After making the necessary adjustments, it worked fine for me. I have detailed the findings and corrections in a Colab gist. Please find the gist here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,LongZE666,Aborted in `tensorflow.compat.v1.nn.depthwise_conv2d_native`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.compat.v1.nn.depthwise_conv2d_native` triggers crash. The crash occurs in the  operator DepthwiseConv2DNative  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow input = 18261.482131447112 strides = [13762484222233278712, 17895496137899471100] padding = 'VALID' dilations = [11529923499597944069, 6104546329631715214, 16719022861480294065, 15641088296656225516, 5831796171324899039, 2019887057379389965, 6603155403475823832, 128243198633280011, 1962111378253501566, 12787126510677041415, 10754291262680183180] filter = np.array([[ 36832. ,  8776. ,    255.2, 31200. ,  6408. , 12656. , 31696. , 65024. , 50688. ]], dtype=np.float16) tensorflow.compat.v1.nn.depthwise_conv2d_native(input=input, filter=filter, strides=strides, padding=padding, dilations=dilations) ```  Relevant log output ```shell 20250118 12:25:34.220155: F tensorflow/core/common_runtime/mkl_layout_pass.cc:2755] NonOKstatus: GetNodeAttr(orig_node>def(), ""dilations"", &dilations) status: INVALID_ARGUMENT: Attr dilations has value 6104546329631715214 out of range for an int32 Aborted (core dumped) ```",2025-01-18T12:26:48Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85255," Explanation of the Error: `Attr dilations has value [...] out of range for an int32` The error **`Attr dilations has value [...] out of range for an int32`** occurs because the `dilations` attribute passed to the TensorFlow operation contains values that exceed the range of a 32bit integer.   Why This Happens  The `dilations` parameter defines how the convolution kernel is spaced for height and width dimensions.  TensorFlow expects `dilations` to be a **list of small positive integers**, typically `[1, dilation_height, dilation_width, 1]`.  In your code, `dilations` contains excessively large values, such as:   ```python   dilations = [11529923499597944069, 6104546329631715214, ...] Another error I ran into:  Explanation of the Error: `Sliding window strides field must specify 4 dimensions` The error **`Sliding window strides field must specify 4 dimensions`** means that TensorFlow expects the `strides` parameter to be a list of exactly **4 integers**, corresponding to the following dimensions: 1. **Batch size**: Stride for the batch dimension (usually set to `1`). 2. **Height**: Stride for the height dimension. 3. **Width**: Stride for the width dimension. 4. **Channels**: Stride for the channels dimension (usually set to `1`).",  I tried to execute the mentioned code and observed that the attribute error which might be due to a value is too large to fit into a 32bit signed integer variable. Could you try to provide the input which fits the stride and test the code. Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,LongZE666,Aborted in `tensorflow.raw_ops.RecordInput`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.raw_ops.RecordInput` triggers crash. The crash occurs in the  operator RecordInput  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow file_pattern = '@\x0chnFvGe CC(can't install on ubuntu 12.04)FxIIw.\';$`E?8YC2p>\x0bl\\B*;z;]a4lrH{gWK}rAY{;T]2k&jEB)pQu{JLy1K{Q zGnevJS7G=N_{c}\x0cS4p\'] CC(Add support for Python 3.x)`T?FAep' file_random_seed = 0 batch_size =  8494458287779632258 tensorflow.raw_ops.RecordInput(file_pattern=file_pattern, file_random_seed=file_random_seed, batch_size=batch_size) ```  Relevant log output ```shell 20250118 12:21:05.765578: F tensorflow/core/framework/tensor_shape.cc:201] NonOKstatus: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be nonnegative, got 8494458287779632258 Aborted (core dumped) ```",2025-01-18T12:21:59Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85254,"Hi **** , Apologies for the delay, and thank you for raising your concern here. I tested your code on Colab using TensorFlow 2.17.0, 2.18.0, and the nightly versions. The code is throwing a proper error message instead of aborting. Please find the gist here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,LongZE666,Aborted in `tensorflow.nn.max_pool3d`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.nn.max_pool3d` triggers crash. The crash occurs in the  operator MaxPool3D  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow input = np.ones((20, 19, 20, 5, 0), dtype=np.float32) ksize = [3505357736] strides = [2230364274603370470]  padding = 'SAME' tensorflow.nn.max_pool3d(input=input, ksize=ksize, strides=strides, padding=padding) ```  Relevant log output ```shell 20250118 12:15:49.067137: F tensorflow/core/common_runtime/mkl_layout_pass.cc:1609] NonOKstatus: GetNodeAttr(n>def(), ""ksize"", &ksize) status: INVALID_ARGUMENT: Attr ksize has value 3505357736 out of range for an int32 Aborted (core dumped) ```",2025-01-18T12:16:46Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85253,", According to the official document for the tensorflow.nn.max_pool3d, the input for the strides should be **An int or list of ints that has length 1, 3 or 5. The stride of the sliding window for each dimension of the input tensor.** https://www.tensorflow.org/api_docs/python/tf/nn/max_pool3dargs Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,LongZE666,Floating point exception in `tensorflow.nn.max_pool1d`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.nn.max_pool1d` triggers crash. The crash occurs in the  operator MaxPool  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow input = np.ones((16,19,18), dtype=np.float32) ksize = [58486] strides = [49657] padding = 'VALID' tensorflow.nn.max_pool1d(input=input, ksize=ksize, strides=strides, padding=padding) ```  Relevant log output ```shell Floating point exception (core dumped) ```",2025-01-18T12:14:05Z,stat:awaiting response type:bug stale comp:apis comp:ops 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85252,"The crash is caused by invalid inputs, not a bug in TensorFlow.  To prevent crashes: Validate ksize and strides to ensure they align with the input dimensions. Use reasonable values for these parameters, adhering to the intended size and stride of your pooling operation.","Hi **** , Apologies for the delay, and thank you for raising your concern here. I tested your code on colab using TensorFlow 2.17.0, 2.18.0, and the nightly versions, and I did not encounter any issues. Please find the results in gist1 and gist2 for reference. If you are still facing issues, the main cause might be related to the kernel size and strides. Ensure that the kernel size is less than or equal to the input's second dimension, and the same applies to strides. Adjust these values accordingly, and the code should run smoothly. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,LongZE666,Aborted (core dumped) in `tensorflow.nn.max_pool1d`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.nn.max_pool1d` triggers crash. The crash occurs in the  operator MaxPool  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow input = np.ones((17,7,14), dtype=np.float32) ksize = [3134755050008138495] strides = [48732] padding = 'SAME' tensorflow.nn.max_pool1d(input=input, ksize=ksize, strides=strides, padding=padding) ```  Relevant log output ```shell 20250118 12:09:37.053592: F tensorflow/core/common_runtime/mkl_layout_pass.cc:1609] NonOKstatus: GetNodeAttr(n>def(), ""ksize"", &ksize) status: INVALID_ARGUMENT: Attr ksize has value 3134755050008138495 out of range for an int32 Aborted (core dumped) ```",2025-01-18T12:10:43Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85251,", This indicates the problem is due to Memory issue where OS crashed in allocating required memory which is expected.Please refer to the developer https://github.com/tensorflow/tensorflow/issues/59168issuecomment1405633596 related to malloc with High input size which will eventually lead to OS crash. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,LongZE666,Abort in `tensorflow.keras.backend.conv2d`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.keras.backend.conv2d` triggers crash. The crash occurs in the  operator Conv2D  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow x = np.array([1.5841993e+38, 4.7480855e+36, 1.8916006e+38], dtype=np.float32) kernel = np.ones((13, 8, 5, 14), dtype=np.float32) tensorflow.keras.backend.conv2d(x=x, kernel=kernel) ```  Relevant log output ```shell 20250118 12:02:15.375499: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C Aborted (core dumped) ```",2025-01-18T12:04:15Z,stat:awaiting response type:bug stale comp:apis 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85250,"Hi **** , Apologies for the delay, and thank you for raising your concern here. It seems the issue might be caused by using the deprecated API `tensorflow.keras.backend.conv2d`. I recommend switching to the latest APIs for better compatibility and performance. Please find the documentation here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,LongZE666,Aborted in ` tf.nn.conv3d_transpose`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.1  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tf.nn.conv3d_transpose` triggers crash.   Standalone code to reproduce the issue ```shell https://colab.research.google.com/drive/1wA5pUfm4HpNpHxPawrZt4CKS2YCUR4?usp=sharing ```  Relevant log output ```shell 20250118 11:31:33.696896: F tensorflow/core/common_runtime/mkl_layout_pass.cc:2754] NonOKstatus: GetNodeAttr(orig_node>def(), ""strides"", &strides) status: INVALID_ARGUMENT: Attr strides has value 5717580618211388939 out of range for an int32 Aborted (core dumped) ```",2025-01-18T11:34:53Z,stat:awaiting response type:bug stale comp:apis comp:ops TF 2.16,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85246,"code ``` import pickle import numpy as np import tensorflow input = np.ones((9, 12, 18, 6), dtype=np.float32) filters = np.ones((14, 1, 9, 10, 0), dtype=np.float32) strides = [5717580618211388939] padding = 'SAME' output_shape = np.array([], dtype=np.complex128) tensorflow.nn.conv3d_transpose(input=input, filters=filters, output_shape=output_shape, strides=strides, padding=padding) ``` result ``` 20250118 13:18:01.541522: F tensorflow/core/common_runtime/mkl_layout_pass.cc:2754] NonOKstatus: GetNodeAttr(orig_node>def(), ""strides"", &strides) status: INVALID_ARGUMENT: Attr strides has value 5717580618211388939 out of range for an int32 Aborted (core dumped) ``` The colab link given is inconsistent with the results on my machine. _","Hi **** , Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions. However, instead of aborting, the code is throwing an error. I have provided an example with all the necessary steps, which is working fine. Please find the gist here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,LongZE666,Aborted  in `tf.raw_ops.RaggedGather`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tf.raw_ops.RaggedGather` triggers crash.  Standalone code to reproduce the issue ```shell params_nested_splits = tf.constant(0, shape=[3], dtype=tf.int64) params_dense_values = tf.constant(1, shape=[0], dtype=tf.float32) indices = tf.constant(0, shape=[], dtype=tf.int64) OUTPUT_RAGGED_RANK = 1 PARAMS_RAGGED_RANK = 1 tf.raw_ops.RaggedGather(     params_nested_splits=[params_nested_splits],     params_dense_values=params_dense_values,     indices=indices,     OUTPUT_RAGGED_RANK=1,     name=None ) ```  Relevant log output ```shell 20250118 09:30:00.549762: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64 Aborted (core dumped) ```",2025-01-18T09:32:16Z,type:bug comp:ops 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/85242,I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TFnightly. Please find the gist here for your reference. Thank you!
tpu,LongZE666,Segmentation fault (core dumped) in `RaggedTensorToTensor`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tf.raw_ops.RaggedTensorToTensor` triggers crash.  Standalone code to reproduce the issue ```shell import tensorflow as tf shape = tf.constant(1, shape=[], dtype=tf.int64) values = tf.constant(0, shape=[0], dtype=tf.int32) default_value = tf.constant(0, shape=[], dtype=tf.int32) row_partition_tensors = tf.constant([0, 1, 6], shape=[3], dtype=tf.int64) row_partition_types = [""ROW_SPLITS""] tf.raw_ops.RaggedTensorToTensor(     shape=shape,     values=values,     default_value=default_value,     row_partition_tensors=[row_partition_tensors],     row_partition_types=row_partition_types) ```  Relevant log output ```shell Segmentation fault (core dumped) ```",2025-01-18T09:27:19Z,type:bug comp:ops 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/85240,I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TFnightly. Please find the gist here for your reference. Thank you!
opt,copybara-service[bot],[xla:cpu:xnn] Measure execution time of parallel task to decide the optimal number of workers,[xla:cpu:xnn] Measure execution time of parallel task to decide the optimal number of workers ``` ```,2025-01-18T04:09:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85203
sharding,copybara-service[bot],Add cache entries for reshape ops in SPMD.,"Add cache entries for reshape ops in SPMD. We may have two compatible sharding pairs when handling reshape. If we have two pairs, we use the first one. We can still use the second one to add as a sharding cache. Given the following reshape, ``` p0 = bf16[8,8] parameter(0), sharding={replicated} reshape = bf16[64] reshape(p0), sharding={devices=[4] (bf16[16], bf16[64]) {   %param = bf16[8,8]{1,0} parameter(0), sharding={replicated}   %constant = s32[4]{0} constant({0, 2, 4, 6})   %partitionid = u32[] partitionid()   %dynamicslice = s32[1]{0} dynamicslice(s32[4]{0} %constant, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.1 = s32[] reshape(s32[1]{0} %dynamicslice)   %constant.1 = s32[] constant(0)   %dynamicslice.1 = bf16[2,8]{1,0} dynamicslice(bf16[8,8]{1,0} %param, s32[] %reshape.1, s32[] %constant.1), dynamic_slice_sizes={2,8}   %reshape.2 = bf16[16]{0} reshape(bf16[2,8]{1,0} %dynamicslice.1)   %allgather = bf16[64]{0} allgather(bf16[16]{0} %reshape.2), channel_id=1, replica_groups={{0,1,2,3}}, dimensions={0}, use_global_device_ids=true   %abs.1 = bf16[64]{0} abs(bf16[64]{0} %allgather)   ROOT %tuple.1 = (bf16[16]{0}, bf16[64]{0}) tuple(bf16[16]{0} %reshape.2, bf16[64]{0} %abs.1) } ``` With this change, we replace reshard with reshape ``` ENTRY %reshape_spmd (param: bf16[8,8]) > (bf16[16], bf16[64]) {   %param = bf16[8,8]{1,0} parameter(0), sharding={replicated}   %constant = s32[4]{0} constant({0, 2, 4, 6})   %partitionid = u32[] partitionid()   %dynamicslice = s32[1]{0} dynamicslice(s32[4]{0} %constant, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.1 = s32[] reshape(s32[1]{0} %dynamicslice)   %constant.1 = s32[] constant(0)   %dynamicslice.1 = bf16[2,8]{1,0} dynamicslice(bf16[8,8]{1,0} %param, s32[] %reshape.1, s32[] %constant.1), dynamic_slice_sizes={2,8}   %reshape.2 = bf16[16]{0} reshape(bf16[2,8]{1,0} %dynamicslice.1)   %reshape.3 = bf16[64]{0} reshape(bf16[8,8]{1,0} %param)   %abs.1 = bf16[64]{0} abs(bf16[64]{0} %reshape.3)   ROOT %tuple.1 = (bf16[16]{0}, bf16[64]{0}) tuple(bf16[16]{0} %reshape.2, bf16[64]{0} %abs.1) } ```",2025-01-18T01:52:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85202
opt,copybara-service[bot],`TfrtCpuClient` should respect `run_backend_only` option.,`TfrtCpuClient` should respect `run_backend_only` option.,2025-01-18T01:44:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85201
sharding,copybara-service[bot],[JAX] Include the memory kind when converting JAX/IFRT sharding types,[JAX] Include the memory kind when converting JAX/IFRT sharding types,2025-01-18T01:20:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85200
tpu,copybara-service[bot],Add argument to fix a random seed when generating random arguments for HLO runner. Also add OutputFormat so that literal dumps can be saved as a pb file.,Add argument to fix a random seed when generating random arguments for HLO runner. Also add OutputFormat so that literal dumps can be saved as a pb file.,2025-01-18T00:43:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85195
tpu,copybara-service[bot],Add argument to fix a random seed when generating random arguments for HLO runner. Also add OutputFormat so that literal dumps can be saved as a pb file.,Add argument to fix a random seed when generating random arguments for HLO runner. Also add OutputFormat so that literal dumps can be saved as a pb file.,2025-01-17T22:15:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85186
yi,copybara-service[bot],internal change only,internal change only,2025-01-17T17:32:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85178
yi,copybara-service[bot],internal change only,internal change only,2025-01-17T15:18:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85175
tpu,henghamao,Could not get sample weight from customized loss," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.13.1  Custom code Yes  OS platform and distribution CentOS 7.9  Mobile device _No response_  Python version 3.8.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? We used customized loss for the model training, and would like to get sample weight to calculate the loss. However, sample weight does not pass to loss function as expected.  Standalone code to reproduce the issue ```shell Here are the code to reproduce the issue. import tensorflow as tf from tensorflow.keras.layers import Dense import numpy as np def weighted_zero_mean_r2_loss(y_true, y_pred, sample_weight=None):     y_true = tf.cast(y_true, tf.float32)     y_pred = tf.cast(y_pred, tf.float32)     sample_weight = tf.cast(sample_weight, tf.float32)     weighted_squared_error = sample_weight * (y_true  y_pred) ** 2     weighted_true_squared = sample_weight * y_true ** 2     numerator = tf.reduce_sum(weighted_squared_error)     denominator = tf.reduce_sum(weighted_true_squared)     r2_score = 1  numerator / denominator     return r2_score def build_model():     metrics = 'mae'     loss = weighted_zero_mean_r2_loss 'mse'     output_num = 1     inputs = tf.keras.layers.Input(shape=(40, 36))     lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)     output = Dense(output_num, activation='linear')(lstm_out)     model = tf.keras.Model(inputs, output)     model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)     model.summary()     return model def data_gen(class_weights):     while True:         x_batch = np.random.rand(128, 40, 36)         y_batch = np.random.randint(0, 3, (128, 1))          Apply class weights to the labels         sample_weights = np.vectorize(lambda x: class_weights[x])(y_batch)         yield x_batch, y_batch, sample_weights model = build_model() cw = {0: 0.3, 1: 2.5, 2: 3.2} model.fit(data_gen(cw), epochs=2, steps_per_epoch=10) ```  Relevant log output ```shell Error messages: File ""/root/.virtualenvs/infinity_stock/lib/python3.8/sitepackages/keras/src/engine/training.py"", line 1338, in train_function  *         return step_function(self, iterator)     File ""/data/release/kagglejanestreet/scripts/python/test.py"", line 8, in weighted_zero_mean_r2_loss  *         sample_weight = tf.cast(sample_weight, tf.float32)     ValueError: None values not supported. ```",2025-01-17T15:16:39Z,type:feature comp:keras TF 2.13,closed,0,9,https://github.com/tensorflow/tensorflow/issues/85174,"I was able to reproduce the issue on Colab using TensorFlow v2.13 and TFnightly. Please find the gist1, gist2 here for your reference. Thank you!","Hi   When you are building a custom loss function, the loss function should have a signature of **```custom_loss_fn(y_true, y_pred)```** (source). When doing this, you are essentially trying to override the `call()` function in `class Loss` in keras/src/losses/loss/py.  You can still pass in a `sample_weight` parameter to the `model.fit()` call with the custom loss function, but sample_weight will be multiplied elementwise with the loss terms in the reduce_weighted_values() call. So it won't work the way you want it to for the r2 loss. One observation about your code is that sample_weight only depends on y_true, so you could move the sample_weight computation inside your function.  The following code would work: ``` def weighted_zero_mean_r2_loss(y_true, y_pred):     y_pred = tf.cast(y_pred, tf.float32)     y_true = tf.cast(y_true, tf.float32)     table = tf.lookup.StaticHashTable(         tf.lookup.KeyValueTensorInitializer(             keys=tf.constant([0, 1, 2], dtype=tf.int32),              values=tf.constant([0.3, 2.5, 3.2], dtype=tf.float32)),         default_value=1)     sample_weight = table.lookup(tf.cast(y_true, tf.int32))     weighted_squared_error = sample_weight * (y_true  y_pred) ** 2     weighted_true_squared = sample_weight * y_true ** 2     numerator = tf.reduce_sum(weighted_squared_error)     denominator = tf.reduce_sum(weighted_true_squared)     r2_score = 1  numerator / denominator     return r2_score ``` If you want to use a custom logic for applying sample_weights, there's another way to do it by subclassing the `keras.losses.Loss` class. You would have to override the `__init__` and `call` functions. You can pass in a custom function during `__init__` and use it during `call()`. For example: ``` class WeightedZeroMeanR2Loss((keras.losses.Loss):   def __init__(self, custom_weight_multiplier_fn, name='weighted_zero_mean_r2_loss'):     super().__init__(name=name)     self._custom_weight_multiplier_fn = custom_weight_multiplier_fn   def call(self, y_true, y_pred):      sample_weight = self._custom_weight_multiplier_fn(y_true)     weighted_squared_error = sample_weight * (y_true  y_pred) ** 2     weighted_true_squared = sample_weight * y_true ** 2     numerator = tf.reduce_sum(weighted_squared_error)     denominator = tf.reduce_sum(weighted_true_squared)     r2_score = 1  numerator / denominator     return r2_score ``` Note that you would probably need to use tensorflow operations and not python operations, since the loss function gets converted to a tf.function during the graph execution. You can initialize the model with the loss class above as follows: ```     model.compile(loss=WeightedZeroMeanR2Loss(custom_fn), optimizer=tf.keras.optimizers.Adam(), metrics=metrics) ``` I hope this explanation helps. Please let me know if you have any questions. ","Hi  , Thanks for the reply. The code is to reproduce the issue. And in real scenario, the sample weight could not refer by y_ture value.  The problme is the regression for multiple categories of data. The sample weight is to apply for different categories. y_ture is the data point at time stamp t. If it is a classify problem, we could refer sample weight by category label of y_ture value. However, for regression problems, we could not do that. In torch, we could easily use customize loss to calcualte weighted_r2_loss. Example code as below ``` class WeightedR2Loss(nn.Module):     """"""PyTorch loss function for weighted R².""""""     def __init__(self, epsilon: float = 1e38) > None:         """"""         Initialize the WeightedR2Loss class.         Args:             epsilon (float, optional): Small constant added to the denominator                  for numerical stability. Defaults to 1e38.         """"""         super(WeightedR2Loss, self).__init__()         self.epsilon = epsilon     def forward(         self,         y_pred: torch.Tensor,         y_true: torch.Tensor,         weights: torch.Tensor) > torch.Tensor:         """"""Compute the weighted R² loss.         Args:             y_true (torch.Tensor): Ground truth tensor.             y_pred (torch.Tensor): Predicted tensor.             weights (torch.Tensor): Weights for each observation (same shape as y_true).         Returns:             torch.Tensor: Computed weighted R² loss.         """"""         numerator = torch.sum(weights * (y_pred  y_true) ** 2)         denominator = torch.sum(weights * (y_true) ** 2) + 1e38         loss = numerator / denominator         return loss ``` Hope tf could provide similar solution to solve the problem.","  Thank you for going over your use case in detail. You can do this in Tensorflow by subclassing `keras.Model` and overriding the `compute_loss()` function. ``` class CustomModel(keras.Model):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)     def compute_loss(self, x, y_true, y_pred, sample_weight):         y_true = tf.cast(y_true, tf.float32)         y_pred = tf.cast(y_pred, tf.float32)         sample_weight = tf.cast(sample_weight, tf.float32)         weighted_squared_error = sample_weight * (y_true  y_pred) ** 2         weighted_true_squared = sample_weight * y_true ** 2         numerator = tf.reduce_sum(weighted_squared_error)         denominator = tf.reduce_sum(weighted_true_squared)         r2_score = 1  numerator / denominator         return r2_score def build_model():     metrics = ['mae']     output_num = 1     inputs = tf.keras.layers.Input(shape=(40, 36))     lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)     output = Dense(output_num, activation='linear')(lstm_out)     model = CustomModel(inputs, output)     class_weights = {0: 0.3, 1: 2.5, 2: 3.2}     model.compile(optimizer=tf.keras.optimizers.Adam(), metrics=metrics)     model.summary()     return model def data_gen(class_weights):     while True:         x_batch = np.random.rand(128, 40, 36)         y_batch = np.random.randint(0, 3, (128, 1))          Apply class weights to the labels         sample_weights = np.vectorize(lambda x: class_weights[x])(y_batch)         yield x_batch, y_batch, sample_weights model = build_model() cw = {0: 0.3, 1: 2.5, 2: 3.2} model.fit(data_gen(cw), epochs=2, steps_per_epoch=10) ```","  Great thanks for providing the solution. It works for our problems. BTW, there is another issue about class weight and sample weight with the similar code to reproduce the issue. https://github.com/tensorflow/tensorflow/issues/77958 We submited the issue a few months ago, and it did not get any further updates.",Happy to help!  Let me take a look at https://github.com/tensorflow/tensorflow/issues/77958.,"  Hi SanjaySG, We found a problem with the code. By using comput_loss(), the model tend to maximize the loss, rather than minimize the loss. Here is the code to repoduce the issue: ``` import tensorflow as tf from tensorflow.keras.layers import Dense import numpy as np class CustomModel(tf.keras.Model):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)     def compute_loss(self, x, y_true, y_pred, sample_weight):         y_true = tf.cast(y_true, tf.float32)         y_pred = tf.cast(y_pred, tf.float32)         sample_weight = tf.cast(sample_weight, tf.float32)         weighted_squared_error = sample_weight * (y_true  y_pred) ** 2         weighted_true_squared = sample_weight * y_true ** 2         numerator = tf.reduce_sum(weighted_squared_error)         denominator = tf.reduce_sum(weighted_true_squared)         r2_score = 1  numerator / denominator         return r2_score def build_model():     metrics = ['mae']     output_num = 1     inputs = tf.keras.layers.Input(shape=(40, 36))     lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)     output = Dense(output_num, activation='linear')(lstm_out)     model = CustomModel(inputs, output)     class_weights = {0: 0.3, 1: 2.5, 2: 3.2}     model.compile(optimizer=tf.keras.optimizers.Adam(), metrics=metrics)     model.summary()     return model def data_gen(class_weights):     while True:         x_batch = np.random.rand(128, 40, 36)         y_batch = np.random.randint(0, 3, (128, 1))          Apply class weights to the labels         sample_weights = np.vectorize(lambda x: class_weights[x])(y_batch)         yield x_batch, y_batch, sample_weights model = build_model() cw = {0: 0.3, 1: 2.5, 2: 3.2} model.fit(data_gen(cw), epochs=50, steps_per_epoch=10, verbose=2) ``` We observed the metric 'mae' keep growing, and the loss grows as well.","  This is down to the objective chosen for optimization. The model.fit() call tries to **minimize** the loss. r2 on the other hand should increase when the model performs better. A naive way to do this is by providing the negative of r2.  More importantly, r2 is not differentiable. So ideally, it shouldn't be used as a loss function for gradient descent, but it can be used as a metric. Something like MSE or MAE would be a better loss function. ",Great thanks for the explanations.
yi,copybara-service[bot],internal change only,internal change only,2025-01-17T15:11:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85173
yi,copybara-service[bot],Add some clarifying comments for Dockerfiles.,Add some clarifying comments for Dockerfiles.,2025-01-17T14:59:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85172
tpu,copybara-service[bot],[XLA:GPU] Allow fusing multiply into triton fusion.,[XLA:GPU] Allow fusing multiply into triton fusion. The pr enables multiply fusion for the cases when its arguments have positive effect on the input sizes. I.e. when total size of multiply args is less than the output size of the multiply. It is the case when we multiply s4 by a small number of scales and pass the result to the dot.,2025-01-17T14:51:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85171
tpu,copybara-service[bot],[XLA:GPU][NFC] Optimize `TritonEmitterLongDeviceTest.FusionWithOutputContainingMoreThanInt32MaxElementsExecutesCorrectly` to run in reasonable time.,"[XLA:GPU][NFC] Optimize `TritonEmitterLongDeviceTest.FusionWithOutputContainingMoreThanInt32MaxElementsExecutesCorrectly` to run in reasonable time. The test now requires much fewer resources than it used toand runs more than `50x` faster. As a result, a dedicated test target is no longer necessary.",2025-01-17T13:48:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85165
yi,copybara-service[bot],internal changes only,internal changes only,2025-01-17T13:07:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85161
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-17T11:45:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85144
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-17T10:52:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85137
tpu,copybara-service[bot],Adapt HloFusionAdaptor GetRoots() method,"Adapt HloFusionAdaptor GetRoots() method For ProducerConsumer fusions, we can have the case that a multioutput fusion is created even if the producer fusion was not a multioutput fusion. This happens if the fusion root of the producer is also used outside of the created ProducerConsumer fusion, and we don't want to duplicate the producer. This change adds the possibility to create a HloFusionAdaptor for a ProducerConsumer fusion with extra outputs created for producer roots that are used outside the ProducerConsumer fusion.",2025-01-17T10:14:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85133
opt,copybara-service[bot],Make creation of CompilationProvider depend on DebugOptions,"Make creation of CompilationProvider depend on DebugOptions  Backstory With the introduction of the CompilationProvider framework I created an instance of the CompilationProvider in the constructor of NVPTXCompiler. That meant individual HLO compilations couldn't influence the PTX compilation pipeline anymore, they would all go through the same compilation provider. That was very intentional because the PTX compilation path can influence the resulting binary but the caching logic that was inplace didn't account for that and happily returned a cache hit that was generated through a via a different path. This has led to subtle bugs and weird behaviour (compilation that should fail don't because they get served from the cache) in the past. But as it turns out there are downstream users of XLA that depend on the ability to influence the PTX compilation pipeline through the DebugOptions that come with an HLO module. They set `xla_gpu_cuda_data_dir` programmatically there and don't specify it through XLA_FLAGS (which was still working fine).  What's changing So in this change I make NVPTXCompiler handle multiple compilation providers based on the flags in the debug options. In almost all cases there will ever be one compilation provider, but since NVPTXCompiler can now handle different pipelines we also avoid the caching bug from before.  Implementation details 1. There is a new type `CompilationProviderOptions`. It holds all the parameters from `DebugOptions` that can influence how the PTX compilation pipeline gets constructed. It's hashable, so it can act as a key in a hash map. There is also a conversion function from debug options. 2. `AssembleCompilationProvider` now takes a `CompilationProviderOptions` value instead of a `DebugOptions` value. This way we make sure it only depends on parameters defined in the former. 3. `NVPTXCompiler` holds a mutexguarded map from `CompilationProviderOptions` to `CompilerProvider`. When an `HloModule` gets compiled we construct the `CompilationProviderOptions` from the debug options and retrieve the correct compilation provider from the map (assemble a new one, if none exists already).",2025-01-17T09:58:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85130
opt,copybara-service[bot],[XLA:GPU] Fix comment for `xla_gpu_analytical_latency_estimator_options` usage.,[XLA:GPU] Fix comment for `xla_gpu_analytical_latency_estimator_options` usage. s/ms/us.,2025-01-17T09:33:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85127
llm,copybara-service[bot],Add LLM inference engine based on CompiledModel APIs,Add LLM inference engine based on CompiledModel APIs The new pipeline is only enabled with `use_compiled_model` flag to the script. It will define `USE_LITERT_COMPILED_MODEL` for the executor builds. The KV Cache management logic is implemented in LlmLiteRtCompiledModelExecutor with TensorBuffers.,2025-01-16T21:22:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85088
tpu,copybara-service[bot],Refactor mechanisms of building TF wheel and storing TF project version.,"Refactor mechanisms of building TF wheel and storing TF project version. This change introduces a uniform way of building the TF wheel and controlling the filename version suffixes. A new repository rule `python_wheel_version_suffix_repository` provides information about project and wheel version suffixes. The final value depends on environment variables passed to Bazel command: `_ML_WHEEL_WHEEL_TYPE, _ML_WHEEL_BUILD_DATE, _ML_WHEEL_GIT_HASH, _ML_WHEEL_VERSION_SUFFIX` `tf_version.bzl` defines the TF project version and loads the version suffix information calculated by `python_wheel_version_suffix_repository`. The targets `//tensorflow/core/public:release_version, //tensorflow:tensorflow_bzl //tensorflow/tools/pip_package:setup_py` use the version chunks defined above. The version of the wheel in the build rule output depends on the environment variables. Environment variables combinations for creating wheels with different versions:   * snapshot (default build rule behavior): `repo_env=ML_WHEEL_TYPE=snapshot`   * release: `repo_env=ML_WHEEL_TYPE=release`   * release candidate: `repo_env=ML_WHEEL_TYPE=release repo_env=ML_WHEEL_VERSION_SUFFIX=rc1`   * nightly build with date as version suffix: `repo_env=ML_WHEEL_TYPE=nightly repo_env=ML_WHEEL_BUILD_DATE=`   * build with git data as version suffix: `repo_env=ML_WHEEL_TYPE=custom repo_env=ML_WHEEL_BUILD_DATE=$(git show s format=%as HEAD) repo_env=ML_WHEEL_GIT_HASH=$(git revparse HEAD)`",2025-01-16T21:16:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85086
tpu,copybara-service[bot],LiteRT: Update CompiledModel API,LiteRT: Update CompiledModel API  Added CreateInputBuffer / CreateOutputBuffer APIs with tensor names.  Use unmanaged litert::Model to keep the original Model instead of pointer   which is unstable.  Added `const` to const methods.,2025-01-16T19:08:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85079
int8,copybara-service[bot],Create `xla.bazelrc` in preparation for XLA having a completely independent .bazelrc,Create `xla.bazelrc` in preparation for XLA having a completely independent .bazelrc Starting with disabling `mavxvnniint8` from XNNPACK. In the future we can refactor more XLA specific bits into this file. Eventually we can stop using the TF bazelrc entirely.,2025-01-16T18:35:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85077
tpu,cg-tester,Urgent help needed.,"Hello, hope anybody help me please! I am going to train a model for information extraction from photo. I am using Windows 10. And am using python 3.10.10. Actually I used python 3.12 but I couldn't install tensorflowio package with python 3.12. Not sure reason yet. So I use python 3.10 now. I have prepared dataset and downloaded predefined model from Model Zoo. Now I need to train. Steps that I did: 1. git clone https://github.com/tensorflow/models.git     cd models 2. pip install tensorflowtext (It will automatically install tensorflow==2.10.1)     pip install tensorflowio 3. cd official     pip install r requirements.txt 4. cd ..     cd research     in object_detection/packages/tf2/setup.py file     change version of tfmodelsofficial into 2.10.1 or 2.10.0     python object_detection/packages/tf2/setup.py install 5. $env:PYTHONPATH = ""$(GetLocation):$(GetLocation)\slim""  in Powershell or     set PYTHONPATH=%cd%;%cd%\slim  in Cmd 6. protoc object_detection/protos/*.proto python_out=.     protoc and protobuf version are same. 3.19.6 7. Run train command     python object_detection/model_main_tf2.py pipeline_config_path=../../model/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu8/pipeline.config      model_dir=../../model/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu8/checkpoint num_train_steps=5000       sample_1_of_n_eval_examples=1 alsologtostderr I have not succeded in here. For not below error I have. venv\lib\sitepackages\tensorflow\python\framework\dtypes.py"", line 34, in      _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type() TypeError: Unable to convert function return value to a Python type! The signature was         () > handle But I tried with many other python versions, tensorflow versions, but every time I met version conflict issues and bugs inside packages. Hope anybody help me run this process to train. I need help urgently. Thanks.",2025-01-16T17:50:21Z,stat:awaiting response type:build/install stale subtype:windows TF 2.10,closed,0,6,https://github.com/tensorflow/tensorflow/issues/85074,"> Actually I used python 3.12 but I couldn't install tensorflowio package with python 3.12. Not sure reason yet. There is no support for Python 3.12 on Windows on tensorflowio. See https://pypi.org/project/tensorflowio/files, there is no Windows file there. You can install on Python 3.10 (or 3.11 too) because an older version supports Windows: https://pypi.org/project/tensorflowio/0.31.0/files. But that version, 0.31.0, was released nearly 2 years ago, so it is not compatible with Tensorflow 2.10 which was released in 2022. You need to have everything at compatible versions. Ideally, you'd also use versions that are in the support window (last release and 1 or 2 releases behind that).","Thanks for your reply   Well, could you let me know versions that are compatible? I use python 3.10 now, because I am using Windows OS. I need to train model on Windows OS absolutely. So could you give me list of compatible versions of packages like tensorflow, tensorflowtext, tensorflowio, protobuf, tfmodelsofficial and so on. Highest available version of tensorflowtext is 2.10, so I am using tensorflow 2.10 but still have many issues inside tensorflow package files. Not sure reason. Hope you to provide me good set of packages and usage experience, if possible. Thanks very much. Hope to build friendship!!!","TensorFlow is the main repo in the ecosystem. We guarantee support for the last released version and 1 or 2 behind it. You can use https://pypi.org/ to locate all of the packages you need and make sure they were released at around the same time. For a package, if you go to the files tab you can see all combinations that are supported. Or you can try `pip install $package==$version` (`$package` and `$version` are placeholders here) and see if they get installed on your system. If you cannot install one, then most likely your system is not supported / too old (either the system or the software stack you are using).",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-16T17:05:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85071
tpu,cg-tester,Unable to convert function return value to a Python type! Error Fix Solution Needed," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.10.1  Custom code Yes  OS platform and distribution Windows 10  Mobile device _No response_  Python version 3.10.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am using Windows 10 I use python 3.10.10, because when I use python 3.12, I couldn't install tensorflowio I installed tensorflow 2.10.1, because only tensorflowtext 2.10.0 is available on Windows 10. Actually I am going to train model. So I cloned tensorflow models repository and made dataset, ... I want help to run models/research/object_detection/model_main_tf2.py file without error. Please help me.  Standalone code to reproduce the issue ```shell in tensorflow/python/framework/dtypes.py line 29 _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type() Here I have error TypeError: Unable to convert function return value to a Python type! The signature was         () > handle ```  Relevant log output ```shell ```",2025-01-16T16:40:25Z,type:bug,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85070,"This looks like a duplicate. Please don't open multiple issues for the same thing. Also, please don't ask for urgent help, this is an OSS community.",Are you satisfied with the resolution of your issue? Yes No
int8,copybara-service[bot],[xla:cpu] Disable AVX_VNNI in XNNPACK by default.,"[xla:cpu] Disable AVX_VNNI in XNNPACK by default. The feature invokes compiler flag mavxvnniint8 which isn't available in many compilers, e.g., not in clang older than version 16.",2025-01-16T16:18:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85069
tpu,gmjw,tf.math.bincount no longer broadcasts over weights," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18.0 (behaviour started 2.15)  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.5 LTS  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version Could not find cuda drivers on your machine  GPU model and memory _No response_  Current behavior? The `tf.math.bincount` implementation used to broadcast nicely when `values` was a flat tensor, allowing the same `values` to be used for all rows of the `weights` passed in.  However, this is no longer the case. The new behaviour seems to make it impossible to broadcast `values` at all, even if reshaping them to match the shape of `weights`. If you run the script below using tf 2.14, it runs fine and produces sensible output, two columns bincounted. However, if run in 2.15 or later, it throws `weights must be the same shape as arr or a length0 Tensor`. I have tried reshaping the values to match the shape of weights (as per the commented out line of code in the snippet below), but this still does not return the same output as in 2.14  the output is a single column, rather than sensibly bincounted data for two columns. This does not seem to be solvable by using the other arguments (e.g. axis=1).  Standalone code to reproduce the issue ```shell import numpy as np import tensorflow as tf values = tf.constant(     [1, 2, 3, 1, 2, 3], )  Alternative: Try repeating values to be same shape as weights  values = tf.repeat(tf.reshape(values, (1, 1)), 2, axis=1) weights = tf.constant([     [0.1,  0.5],   corresponds to 1, as per `values` above     [0.2,  0.1],   corresponds to 2     [0.3,  0.03],   etc     [0.01, 0.05],     [0.02, 0.01],     [0.03, 0.0], ]) print('values.shape', values.shape) print('weights.shape', weights.shape) out = tf.math.bincount(values, weights=weights) print() print(out) print()  The output describes the amount of weight associated  with each value, per column of `weights`. expected = [     [0.0, 0.0],   weight associated with 0     [0.11, 0.55],   with 1     [0.22, 0.11],   etc     [0.33, 0.03], ] np.testing.assert_allclose(out, expected) ```  Relevant log output ```shell ```",2025-01-16T15:32:01Z,type:support,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85065,"Apologies, I've realised that things work properly if the inputs to bincount are transposed. So, the example above works fine if the `Alternative:` approach is taken, repeating the values, then everything is transposed. Marking this issue as closed.",Are you satisfied with the resolution of your issue? Yes No
tpu,SankuriJeyaSanjana,Tensorflow related issue," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Collecting tensorflowNote: you may need to restart the kernel to use updated packages.   Using cached tensorflow2.18.0cp310cp310win_amd64.whl.metadata (3.3 kB) Requirement already satisfied: tensorflowintel==2.18.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflow) (2.18.0) Requirement already satisfied: abslpy>=1.0.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (2.1.0) Requirement already satisfied: astunparse>=1.6.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (1.6.3) Requirement already satisfied: flatbuffers>=24.3.25 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (24.12.23) Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (0.4.0) Requirement already satisfied: googlepasta>=0.1.1 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (0.2.0) Requirement already satisfied: libclang>=13.0.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (18.1.1) Requirement already satisfied: opteinsum>=2.3.2 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (3.4.0) Requirement already satisfied: packaging in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (24.2) Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,=3.20.3 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (5.29.3) Requirement already satisfied: requests=2.21.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (2.32.3) Requirement already satisfied: setuptools in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (57.4.0) Requirement already satisfied: six>=1.12.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (1.17.0) Requirement already satisfied: termcolor>=1.1.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (2.5.0) Requirement already satisfied: typingextensions>=3.6.6 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (4.12.2) Requirement already satisfied: wrapt>=1.11.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (1.17.2) Requirement already satisfied: grpcio=1.24.3 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (1.69.0) Requirement already satisfied: tensorboard=2.18 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (2.18.0) Requirement already satisfied: keras>=3.5.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (3.8.0) Requirement already satisfied: numpy=1.26.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (2.0.2) Requirement already satisfied: h5py>=3.11.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (3.12.1) Requirement already satisfied: mldtypes=0.4.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (0.4.1) ... Requirement already satisfied: mdurl~=0.1 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from markdownitpy>=2.2.0>rich>keras>=3.5.0>tensorflowintel==2.18.0>tensorflow) (0.1.2) Using cached tensorflow2.18.0cp310cp310win_amd64.whl (7.5 kB) Installing collected packages: tensorflow Successfully installed tensorflow2.18.0 Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... WARNING: Error parsing dependencies of tensorflowgpu: [Errno 2] No such file or directory: 'c:\\users\\avs mani\\desktop\\project\\venv\\lib\\sitepackages\\tensorflow_gpu2.10.1.distinfo\\METADATA'  ImportError                               Traceback (most recent call last) File c:\Users\AVS MANI\Desktop\Project\venv\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The operation completed successfully. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[1], line 5       1  Install tensorflow package       3 get_ipython().run_line_magic('pip', 'install tensorflow') > 5 import tensorflow as tf   type: ignore       7 from tensorflow.keras.models import Sequential  type: ignore       9 from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout  type: ignore File c:\Users\AVS MANI\Desktop\Project\venv\lib\sitepackages\tensorflow\__init__.py:40 ... Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...  Standalone code to reproduce the issue ```shell import tensorflow as tf   type: ignore from tensorflow.keras.models import Sequential  type: ignore from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout  type: ignore from tensorflow.keras.datasets import mnist  type: ignore ```  Relevant log output ```shell ```",2025-01-16T15:24:33Z,type:build/install,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85064,"Please use \`\`\` to quote error messages to make them more readable. Also, please search previous issues, this issue has been discussed multiple times.",Are you satisfied with the resolution of your issue? Yes No
tensorrt,user-redans,Issues on trying to compile TensorFlow C API for JETSON AGX Xavier using Bazel,"On my JETSON AGX Xavier, with: cuda: 11.4.315 cuDNN: 8.6.0 tensorrt: 8.5.2.2 jetpack: 5.1.3 python3 c “import tensorflow as tf; print(‘TensorFlow version:’, tf.version)” TensorFlow version: 2.11.0 I can’t compile tf with bazel ( bazel version: bazel 5.3.0 ) , error: ~/tensorflow$ bazel build config=opt config=cuda //tensorflow:libtensorflow.so Starting local Bazel server and connecting to it… WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Reading ‘startup’ options from /home/redans/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client: Inherited ‘common’ options: isatty=1 terminal_columns=237 INFO: Reading rc options for ‘build’ from /home/redans/tensorflow/.bazelrc: Inherited ‘common’ options: experimental_repo_remote_exec INFO: Reading rc options for ‘build’ from /home/redans/tensorflow/.bazelrc: ‘build’ options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Reading rc options for ‘build’ from /home/redans/tensorflow/.tf_configure.bazelrc: ‘build’ options: action_env PYTHON_BIN_PATH=/usr/bin/python3.9 action_env PYTHON_LIB_PATH=/usr/local/lib/python3.9/distpackages python_path=/usr/bin/python3.9 action_env PYTHONPATH=/usr/local/lib/python3.9/distpackages:/usr/local/lib/python3.9/distpackages:/home/redans/ros2_ws/install/yolov8_ros/lib/python3.9/sitepackages:/home/redans/ros2_ws/install/yolov8_msgs/lib/python3.9/sitepackages:/home/redans/ros2_ws/install/realsense2_camera_msgs/lib/python3.9/sitepackages:/opt/ros/humble/lib/python3.9/sitepackages action_env LD_LIBRARY_PATH=/usr/local/cuda11.4/lib64:/home/redans/local/lib/python3.8/distpackages/tensorflow:/home/redans/ros2_ws/install/yolov8_msgs/lib:/home/redans/ros2_ws/install/realsense2_camera/lib:/home/redans/ros2_ws/install/realsense2_camera_msgs/lib:/opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/aarch64linuxgnu:/opt/ros/humble/lib:/usr/local/cuda11.4/lib64: action_env GCC_HOST_COMPILER_PATH=/usr/bin/aarch64linuxgnugcc9 config=cuda INFO: Found applicable config definition build:short_logs in file /home/redans/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /home/redans/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda repo_env=HERMETIC_CUDA_VERSION=12.5.1 repo_env=HERMETIC_CUDNN_VERSION=9.3.0 //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.tf_configure.bazelrc: repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=7.2 INFO: Found applicable config definition build:opt in file /home/redans/tensorflow/.tf_configure.bazelrc: copt=Wnosigncompare host_copt=Wnosigncompare INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda repo_env=HERMETIC_CUDA_VERSION=12.5.1 repo_env=HERMETIC_CUDNN_VERSION=9.3.0 //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.tf_configure.bazelrc: repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=7.2 INFO: Found applicable config definition build:linux in file /home/redans/tensorflow/.bazelrc: host_copt=w copt=Wnoall copt=Wnoextra copt=Wnodeprecated copt=Wnodeprecateddeclarations copt=Wnoignoredattributes copt=Wnoarraybounds copt=Wunusedresult copt=Werror=unusedresult copt=Wswitch copt=Werror=switch define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=dynamic_kernels experimental_guard_against_concurrent_changes INFO: Found applicable config definition build:dynamic_kernels in file /home/redans/tensorflow/.bazelrc: define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS ERROR: Traceback (most recent call last): File “/home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/rules_python/python/versions.bzl”, line 734, column 32, in PLATFORMS = _generate_platforms() File “/home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/rules_python/python/versions.bzl”, line 723, column 15, in _generate_platforms }  dict INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda repo_env=HERMETIC_CUDA_VERSION=12.5.1 repo_env=HERMETIC_CUDNN_VERSION=9.3.0 //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda repo_env=HERMETIC_CUDA_VERSION=12.5.1 repo_env=HERMETIC_CUDNN_VERSION=9.3.0 //cuda:include_cuda_libs=true WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. ERROR: //:enable_cuda :: Error loading option //:enable_cuda: error loading package ‘’: at /home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/local_tsl/third_party/py/python_init_repositories.bzl:3:6: at /home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/rules_python/python/repositories.bzl:24:6: at /home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/rules_python/python/private/python_register_multi_toolchains.bzl:22:6: initialization of module ‘python/versions.bzl’ failed Do you have any suggestions?",2025-01-16T10:26:30Z,stat:awaiting response type:build/install stale subtype:bazel TF 2.11,closed,0,3,https://github.com/tensorflow/tensorflow/issues/85034,"redans, Tensorflow v2.11 is a pretty older version which is not actively supported. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.In this case, can you please try installing TensorFlow v2.11 which the respective configurations. https://www.tensorflow.org/install/source Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Are you satisfied with the resolution of your issue? Yes No
tf-trt,copybara-service[bot],[TF-TRT] Added an example to Xlogy documentation.,[TFTRT] Added an example to Xlogy documentation.,2025-01-16T09:46:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85030
opt,copybara-service[bot],[XLA:GPU] Fix MacOS build,"[XLA:GPU] Fix MacOS build The standard format for the linker option is `rpath,`.   The `rpath=` is a GNU extension that MacOS linker doesn't recognize.",2025-01-16T08:08:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85027
tpu,rogerci91,tf.config.LogicalDeviceConfiguration() not able to set the memory limit but tf.config.experimental.VirtualDeviceConfiguration() is able to," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 22.04.4 LTS  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.2  GPU model and memory RTX A5000 24Gb  Current behavior? I was trying to set the memory limit of 10Gb on the virtual device using tf.config.LogicalDeviceConfiguration(), but when I trained the model it was taking way more than 10Gb of memory. Eventually I was able to set the memory limit using tf.config.experimental.VirtualDeviceConfiguration() but I'm not sure why  Standalone code to reproduce the issue ```shell  this was not able to set the memory limit gpus = tf.config.list_physical_devices('GPU') if gpus:     try:         tf.config.set_visible_devices(gpus[0], 'GPU')         tf.config.set_logical_device_configuration(gpus[0],         [tf.config.LogicalDeviceConfiguration(memory_limit=10*1024)])         logical_gpus = tf.config.list_logical_devices('GPU')     except RuntimeError as e:         print(e)   this was able to set the memory limit gpus = tf.config.list_physical_devices('GPU') if gpus:     try:         tf.config.experimental.set_virtual_device_configuration(             gpus[0],             [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10*1024)]         )     except RuntimeError as e:         print(e) ```  Relevant log output ```shell ```",2025-01-16T07:37:49Z,stat:awaiting response type:bug stale 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85026,"TensorFlow's tf.config.experimental.set_virtual_device_configuration is more focused on managing device resources at a lower level, such as limiting memory and managing the allocation between multiple virtual devices. This is why it worked for you while the tf.config.LogicalDeviceConfiguration did not.If you need to limit memory usage, you should use tf.config.experimental.set_virtual_device_configuration() to ensure the desired memory cap is respected.","Hi **** , Hi **** Thank you for your pointers. Apologies for the delay, and thank you for raising your concern here. As  mentioned, `tf.config.experimental.set_virtual_device_configuration()` works by managing resource allocation at a lower level, including limiting memory usage and handling allocation across multiple virtual devices. Therefore, `tf.config.LogicalDeviceConfiguration()` will not achieve the desired results in this scenario. It is recommended to use `tf.config.experimental.set_virtual_device_configuration()` for better outcomes. Here is the relevant TensorFlow documentation for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-16T06:40:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85023
tpu,copybara-service[bot],LiteRT: Update Model API,"LiteRT: Update Model API  Added access to Input / Output Tensor.  When a model is loaded with signature, update tensor names of I/O tensors   with signature I/O names.",2025-01-16T03:58:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85019
tpu,ThomasHughesIV,4080 RTX not detected on windows 11 24H2," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Windows 11 24h2  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version Cuda compilation tools, release 12.6, V12.6.85; cuDNN: Driver Version: 561.17   GPU model and memory 4080 RTX  Current behavior? physical_devices = tf.config.list_physical_devices() to return GPU information.  Instead, all I got was this: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]  Standalone code to reproduce the issue ```shell import tensorflow as tf  physical_devices = tf.config.list_physical_devices()  print(physical_devices) ```  Relevant log output ```shell ```",2025-01-16T00:58:02Z,type:build/install subtype:windows TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85006,"I've tried different python versions, down to 3.10.  I cannot get tensor to detect my GPU so I can play around with building models and learn how to do this in vs2022",Are you satisfied with the resolution of your issue? Yes No,Resolving as duplicate and pointing to a ticket that has no resolution isn't a very good resolution.   At the very least I should get a ETA or some effort to explain this getting fixed.,That is discussed in the duplicated ticket.
opt,copybara-service[bot],[XLA][hlo-opt] Refactor opt tooling to make it easier to add new options.,[XLA][hloopt] Refactor opt tooling to make it easier to add new options. Also adds a test for the listpasses feature.,2025-01-16T00:37:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85004
quantization,copybara-service[bot],Remove tf/compiler/mlir/lite/quantization/ir:QuantizationOpsTdFiles from tf/compiler/mlir/quantization/common/quantization_lib:quantization_td_files,Remove tf/compiler/mlir/lite/quantization/ir:QuantizationOpsTdFiles from tf/compiler/mlir/quantization/common/quantization_lib:quantization_td_files,2025-01-15T23:44:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85001
quantization,copybara-service[bot],Remove //third_party/tensorflow/compiler/mlir/lite/quantization:quantization_passes from //third_party/tensorflow/compiler/mlir:passes,Remove //third_party/tensorflow/compiler/mlir/lite/quantization:quantization_passes from //third_party/tensorflow/compiler/mlir:passes,2025-01-15T23:32:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84996
quantization,copybara-service[bot],Remove //third_party/tensorflow/compiler/mlir/lite/quantization/tensorflow:tf_quantization_passes from //third_party/tensorflow/compiler/mlir:passes,Remove //third_party/tensorflow/compiler/mlir/lite/quantization/tensorflow:tf_quantization_passes from //third_party/tensorflow/compiler/mlir:passes,2025-01-15T23:30:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84995
opt,copybara-service[bot],Remove //third_party/tensorflow/compiler/mlir/lite:tensorflow_lite_optimize from //third_party/tensorflow/compiler/mlir:passes,Remove //third_party/tensorflow/compiler/mlir/lite:tensorflow_lite_optimize from //third_party/tensorflow/compiler/mlir:passes,2025-01-15T23:23:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84993
quantization,copybara-service[bot],Remove //third_party/tensorflow/compiler/mlir/lite/core/c:tflite_common from //third_party/tensorflow/compiler/mlir/quantization/tensorflow/utils:tf_to_xla_attribute_utils,Remove //third_party/tensorflow/compiler/mlir/lite/core/c:tflite_common from //third_party/tensorflow/compiler/mlir/quantization/tensorflow/utils:tf_to_xla_attribute_utils,2025-01-15T23:11:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84991
quantization,copybara-service[bot],Remove from tf/compiler/mlir/lite:tensorflow_lite from tf/compiler/mlir/quantization/tensorflow:passes,Remove from tf/compiler/mlir/lite:tensorflow_lite from tf/compiler/mlir/quantization/tensorflow:passes,2025-01-15T22:29:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84988
tpu,codinglover222,target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize fail to build," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.19  Custom code No  OS platform and distribution Linux Debian 6.1.1191 (20241122) x86_64 x86_64 x86_64 GNU/Linux  Mobile device _No response_  Python version 3.10  Bazel version bazel 6.5.0  GCC/compiler version gcc version 13.1.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior?  Background I follow the guidance here to run unit test locally. I use a google cloud compute engine. gcc version 13.1.0 I run the follow command to docker image tensorflow/build:2.19python3.10. docker run it v $PWD:/tmp w /tmp tensorflow/build:2.19python3.10 bash c ""bazel build experimental_action_cache_store_output_metadata disk_cache=~/.cache/bazel jobs=3 config=linux //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize""  The build failed and the error message is: /usr/include/c++/13/bits/unique_ptr.h:1070:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous 1070  ```  Relevant log output ```shell ```",2025-01-15T17:34:09Z,type:build/install comp:lite TF 2.18,closed,1,26,https://github.com/tensorflow/tensorflow/issues/84977, ,I also tested on new docker SIG image the the same error occurs. tensorflow/build:2.19python3.12 /usr/include/c++/11/bits/unique_ptr.h:962:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous   962 |     { return unique_ptr(new _Tp(std::forward(__args)...)); },"We are also seeing the same error with gcc 12, python 3.11 and tensorflow master branch. Could someone please provide any pointers on this issue?","Hi,   I apologize for the delayed response, thank you for bringing this issue to our attention I'll try to replicate the same behavior from my end and will update you.  Thank you for your cooperation and patience.","Hi,   I apologize for the delayed response, I've been attempting to reproduce the behavior you described using the provided command within the issue template.  Unfortunately, I've encountered a different problem the build process is freezing on my end and preventing completion.  This is occurring on a GCP VM instance.  I've included the output log below for your review.  To confirm, Did you face similar issue from your end ? If possible could you please help us with exact steps which you followed before encountering the reported error in the issue template which will help us to investigate this issue further from our end ? ``` (base) gaikwadrahuln1standard1gput4x1tfliteubuntu22:~/tensorflow$ sudo docker run it v $PWD:/tmp w /tmp tensorflow/build:2.19python3.10 bash c ""bazel build experimental_action_cache_store_output_metadata disk_cache=~/.cache/bazel jobs=3 config=linux //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize"" 2025/02/03 13:17:27 Downloading https://releases.bazel.build/6.5.0/release/bazel6.5.0linuxx86_64... Extracting Bazel installation... Starting local Bazel server and connecting to it... WARNING: The following configs were expanded more than once: [dynamic_kernels]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Invocation ID: c5c3346d1cbd4a23aa602aa5d8cd318a INFO: Reading 'startup' options from /tmp/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=190 INFO: Reading rc options for 'build' from /tmp/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:   'build' options: action_env=DOCKER_CACHEBUSTER=1738455273945121626 host_action_env=DOCKER_HOST_CACHEBUSTER=1738455274013979537 INFO: Reading rc options for 'build' from /tmp/.bazelrc:   'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Found applicable config definition build:short_logs in file /tmp/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /tmp/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:linux in file /tmp/.bazelrc: host_copt=w copt=Wnoall copt=Wnoextra copt=Wnodeprecated copt=Wnodeprecateddeclarations copt=Wnoignoredattributes copt=Wnoarraybounds copt=Wunusedresult copt=Werror=unusedresult copt=Wswitch copt=Werror=switch define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=dynamic_kernels experimental_guard_against_concurrent_changes INFO: Found applicable config definition build:dynamic_kernels in file /tmp/.bazelrc: define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS INFO: Found applicable config definition build:linux in file /tmp/.bazelrc: host_copt=w copt=Wnoall copt=Wnoextra copt=Wnodeprecated copt=Wnodeprecateddeclarations copt=Wnoignoredattributes copt=Wnoarraybounds copt=Wunusedresult copt=Werror=unusedresult copt=Wswitch copt=Werror=switch define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=dynamic_kernels experimental_guard_against_concurrent_changes INFO: Found applicable config definition build:dynamic_kernels in file /tmp/.bazelrc: define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS DEBUG: /root/.cache/bazel/_bazel_root/d42b9c57d24cf5db3bd8d332dc35437f/external/local_tsl/third_party/py/python_repo.bzl:154:14:  HERMETIC_PYTHON_VERSION variable was not set correctly, using default version. Python 3.10 will be used. To select Python version, either set HERMETIC_PYTHON_VERSION env variable in your shell:   export HERMETIC_PYTHON_VERSION=3.12 OR pass it as an argument to bazel command directly or inside your .bazelrc file:   repo_env=HERMETIC_PYTHON_VERSION=3.12 DEBUG: /root/.cache/bazel/_bazel_root/d42b9c57d24cf5db3bd8d332dc35437f/external/local_tsl/third_party/py/python_repo.bzl:87:10:  ============================= Hermetic Python configuration: Version: ""3.10"" Kind: """" Interpreter: ""default"" (provided by rules_python) Requirements_lock label: ""//:requirements_lock_3_10.txt"" ===================================== WARNING: The following configs were expanded more than once: [dynamic_kernels]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Analyzed target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize (185 packages loaded, 8165 targets configured). INFO: Found 1 target... [2,289 / 2,432] 6 actions, 4 running     Compiling mlir/lib/Analysis/FlatLinearValueConstraints.cpp; 64s remotecache     Compiling tensorflow/compiler/mlir/tensorflow/ir/tf_ops_a_m.cc; 40s local, remotecache     Compiling mlir/lib/Dialect/Affine/IR/AffineValueMap.cpp; 40s remotecache ``` Thank you for your cooperation and patience.",Hi    Did you post the entire error log? I don't see any error in above log you shared.,"Hi,   I have provided the complete build log above. However, the build process stalled at step [2,289 / 2,432] and did not complete.  After waiting for over 30 minutes, there was no further progress.  Could you please share the precise steps you took prior to encountering this issue?  Your assistance in replicating the same issue from our end would be greatly appreciated. Thank you.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Hi  , I will share the steps soon.","Hi , We are encountering the same issue while building TensorFlow on a ppc64le machine. I have attached the build script and the patch we applied. Please review them and let me know if you need any additional details. fixubi.patch Steps_To_Build_TF.txt",Same issue here on x86_64 and aarch64 with GCC 13.2.0: * x86_64 env * x86_64 log * aarch64 env * aarch64 log,We are also encountering the same issue trying to build TF at tipoftree for x86_64 (albeit using hermetic Clang).,"Hi,   Please take a look into this issue. Thank you","Hey All, , , I'm trying to understand the workflow we are building for here. I.e. what shared/static libraries and/or packages do you need to do what you want? I have tested TF 2.19 and it builds fine. We should be building against the LiteRT instructions moving forward for TFLite Workflows. Of course maybe there is a shared/static library and/or package that is not currently available that way  in which case please let me know so we can move forward on this.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"I don't think this issue has anything to do with LiteRT, I encounter this bug anytime I try to build TensorFlow with GCC.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Issue is not stale, it's just being ignored by the TF developers. ",", what's your workflow which requires this target? i.e. what are you trying to do that builds this target? You can also use https://github.com/googleaiedge/aiedgetorch to quantize pytorch models."," GCC is complaining that an inherited constructor is creating an ambiguity. I do not know whether this is a GCC bug, or if GCC is correct that the call is ambiguous. But you can fix the problem by getting rid of the inherited constructor. The following patch allows my GCC 13 build to finish: ``` diff git a/tensorflow/compiler/mlir/lite/transforms/default_quant_params./tensorflow/compiler/mlir/lite/transforms/default_quant_params...800a3fb0603 100644  a/tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc +++ b/tensorflow/compiler/mlir/lite/transforms/default_quant_params.,7 +54,9 @@ namespace {  class DefaultQuantParamsPass      : public impl::DefaultQuantParamsPassBase {   public:   using DefaultQuantParamsPassBase::DefaultQuantParamsPassBase; +  DefaultQuantParamsPass() +  { +  }    explicit DefaultQuantParamsPass(double default_min, double default_max,                                    bool is_signed) { ```"," my target is `//tensorflow/tools/pip_package:wheel`. I have no interest in quantized models.  thanks for the patch! Let me try that and get back to you. I'm guessing this isn't something we can merge into main, I wonder if there is a better way to unambiguously inherit the constructor."," I think this is fine to merge; it's a trivial change that breaks nothing. I will submit a PR. Please do let me know if this fixes the issue for you. The title of this issue should probably be ""TensorFlow build failure with GCC 13"". It has nothing to do with the build target. Also, GCC might be correct that this is an actual ambiguity; I am not sure. I will see if I can create a small reproducer.",Created a small example and asked on StackOverflow (https://stackoverflow.com/q/79553477/),"Sounds like this is actually a tensorflow issue then , can you please reroute appropriately? Apologies for the confusion.",I also opened https://github.com/llvm/llvmproject/issues/134287 OK time for me to get back to work.,Are you satisfied with the resolution of your issue? Yes No
opt,chandu464,Failed to load native TensorFlow Lite methods,"Hi, I'm trying to use tensorflow lite version 2.15.0 to run my tflite model and I'm getting an error when initializing the interpreter. I'm adding the tensor flow libraries in the gradle file  `implementation('org.tensorflow:tensorflowlite') { version { strictly(""2.15.0"") } }` Code:  ``` isLibraryLoaded = false private fun initInterpreter(): Interpreter? {         val tfliteOptions = Interpreter.Options()         tfliteOptions.setNumThreads(2)         if (!isLibraryLoaded) {             System.loadLibrary(""tensorflowlite_jni"")             ARLog.d(""OmniSenseMLDepthImageProcessor"",""Tensor flow lite Library load successful"")             isLibraryLoaded = true         }         return try {             Log.d(""InterpreterDelegate"", ""Thread id getInterpreter ==  ${Thread.currentThread().name}"")             val interpreter = org.tensorflow.lite.Interpreter(loadModelFile(resolveModelFilePath()), tfliteOptions)             Log.d(""InterpreterDelegate"", ""Interpreter initialized successfully"")             return interpreter         } catch (e: Exception) {             Log.e(""InterpreterDelegate"", ""Error: Could not initialize $tfLiteModel interpreter!: ${e.message}"")             e.printStackTrace()             null         }     } ``` TFLite version: 2.15.0 Device: Samsung S20 Error:  ```  E  FATAL EXCEPTION: pool140thread1                                                                                                     Process: com.amazon.mShop.android.shopping, PID: 14755                                                                                                     java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():                                                                                                       java.lang.UnsatisfiedLinkError: dlopen failed: library ""libtensorflowlite_jni_gms_client.so"" not found                                                                                                     	at org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:137)                                                                                                     	at org.tensorflow.lite.NativeInterpreterWrapper.(NativeInterpreterWrapper.java:62)                                                                                                     	at org.tensorflow.lite.NativeInterpreterWrapperExperimental.(NativeInterpreterWrapperExperimental.java:36)                                                                                                     	at org.tensorflow.lite.Interpreter.(Interpreter.java:232)                                                                                                     	at com.a9.fez.tflite.TFLiteInterpreterDelegate.initInterpreter(TFLiteInterpreterDelegate.kt:50)                                                                                                     	at com.a9.fez.tflite.TFLiteInterpreterDelegate.getValue(TFLiteInterpreterDelegate.kt:29)  Caused by: java.lang.UnsatisfiedLinkError: No implementation found for void org.tensorflow.lite.TensorFlowLite.nativeDoNothing() (tried Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing and Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing__)  is the library loaded, e.g. System.loadLibrary?                                                                                                     	at org.tensorflow.lite.TensorFlowLite.nativeDoNothing(Native Method)                                                                                                     	at org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:132)                                                                                                     	... 14 more ```",2025-01-15T17:27:48Z,stat:awaiting response type:support stale comp:lite TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84976,"Hi,   I apologize for the delayed response, if possible could you please help us with your Github repo along with TFLite model and complete steps to replicate the same behavior from our end to investigate this issue further from our end ? Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Pass in pointer instead of Shape object to InstructionValueSet constructor,Pass in pointer instead of Shape object to InstructionValueSet constructor Avoids the copying of the shape object.,2025-01-15T15:24:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84972
tpu,copybara-service[bot],[XLA:GPU] Make LayoutAssignment aware of Cub Radix Sort custom calls.,[XLA:GPU] Make LayoutAssignment aware of Cub Radix Sort custom calls. Ensure that all operands and outputs have the same layout.,2025-01-15T15:23:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84971
int8,copybara-service[bot],[XLA:GPU] Enable Triton MLIR int4 -> int8 rewrite,[XLA:GPU] Enable Triton MLIR int4 > int8 rewrite Roll forward the flag flip once more,2025-01-15T15:22:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84970
tensorrt,user-redans,TensorRT ( C++ ) inference strange behavior on Jetson AGX Xavier,"I developed 2 distinct models, for 2 use cases, to analyzed some vibration patterns: one of them when system is turn on and second when system is shut down (so there are no any vibration detected ) The entire training process uses TensorFlow 2.7.0 (an auto encoder in python) to create .h5 models, which are converted to .onnx models files and then to .engine files for the Jetson platform (Jetson AGX Xavier CUDA ). Jetson AGX Xavier specs: cuda: 11.4.315 cuDNN: 8.6.0 tensorRT: 8.5.2.2 jetpack: 5.1.3 python3 c ""import tensorflow as tf; print('TensorFlow version:', tf.__version__)"" TensorFlow version: 2.11.0 Auto encoder trainig script in python ( sample) : ``` input_img = tf.keras.layers.Input(shape=(2000, lines))    Encoder   x = tf.keras.layers.Conv1D(12, 128, padding='same')(input_img)   x = tf.keras.layers.MaxPooling1D(4)(x)   Downsample: 2000 > 500   x = tf.keras.layers.Conv1D(12, 64, padding='same')(x)   x = tf.keras.layers.MaxPooling1D(2)(x)   Downsample: 500 > 250   x = tf.keras.layers.Conv1D(12, 16, padding='same')(x)   x = tf.keras.layers.MaxPooling1D(2)(x)   Downsample: 250 > 125    Bottleneck   x = tf.keras.layers.Flatten()(x)   x = tf.keras.layers.Dense(self.__config['MODEL']['ENCODED_STATE_SIZE'])(x)    Decoder   x = tf.keras.layers.Dense(125 * 12)(x)   Expand to match last encoder feature size   x = tf.keras.layers.Reshape((125, 12))(x)   x = tf.keras.layers.UpSampling1D(2)(x)   Upsample: 125 > 250   x = tf.keras.layers.Conv1D(12, 16, padding='same')(x)   x = tf.keras.layers.UpSampling1D(2)(x)   Upsample: 250 > 500   x = tf.keras.layers.Conv1D(12, 64, padding='same')(x)   x = tf.keras.layers.UpSampling1D(4)(x)   Upsample: 500 > 2000   x = tf.keras.layers.Conv1D(lines, 128, padding='same')(x)   Correct Final Layer    Model definition   self.__model = tf.keras.models.Model(input_img, x) ``` It doesn't matter which model I use, inference result values are the SAME, exactly the same values, as if the neural network learned nothing...... You can see below 2 comparative charts with the inference values !Image Don't assume that the data might be corrupted, I have collected enough data to train for both cases and I've checked their validity The confusing part is that inference works in python, using TensorFlow 2.7.0 with GPU, an Ubuntu Focal x86_64...I mean, I saw different values between 2 charts In Jetson I've made a py script to convert .h5 model file into .onnx and then into .engine format: ``` import tf2onnx import tensorflow as tf import argparse import subprocess def convert_h5_to_onnx(h5_model_path, onnx_model_path):     print(""Converting .h5 model to ONNX..."")     model = tf.keras.models.load_model(h5_model_path)     model_proto, _ = tf2onnx.convert.from_keras(model, opset=13)     with open(onnx_model_path, ""wb"") as f:         f.write(model_proto.SerializeToString())     print(f""ONNX model saved at {onnx_model_path}"") def convert_onnx_to_trt(onnx_model_path, engine_model_path, trt_precision_mode):     print(""Converting ONNX model to TensorRT Engine..."")     fp_precision_flag = 'fp16' if trt_precision_mode.upper() == 'FP16' else ''     trtexec_path = ""/usr/src/tensorrt/bin/trtexec""     command = f""{trtexec_path} onnx={onnx_model_path} saveEngine={engine_model_path} {fp_precision_flag}""     process = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)     if process.returncode != 0:         print(f""Error in converting to TensorRT engine:\n{process.stderr.decode('utf8')}"")     else:         print(f""TensorRT engine saved at {engine_model_path}"")  Main if __name__ == ""__main__"":     parser = argparse.ArgumentParser(description=""Convert a .h5 model to ONNX and TensorRT engine format"")     parser.add_argument(""h5_model_path"", type=str, required=True, help=""Path to the .h5 model file"")     parser.add_argument(""onnx_model_path"", type=str, required=True, help=""Path to save the converted ONNX model"")     parser.add_argument(""engine_model_path"", type=str, required=True, help=""Path to save the converted TensorRT engine"")     parser.add_argument(""trt_precision_mode"", type=str, choices=['FP32', 'FP16'], default=""FP16"", help=""Precision mode for TensorRT engine (FP32 or FP16)"")     args = parser.parse_args()     convert_h5_to_onnx(args.h5_model_path, args.onnx_model_path)     convert_onnx_to_trt(args.onnx_model_path, args.engine_model_path, args.trt_precision_mode) ``` ""RunInference"" is my C/C++ inference function using TensorRT ( as input data , I used FFT s  of the raw values ) ``` void RunInference(ICudaEngine* engine, IExecutionContext* context, int input_index, int output_index, kiss_fft_cpx* x_fft, kiss_fft_cpx* y_fft, kiss_fft_cpx* z_fft, float* predicted_output, int g_code, const char* clientName) {     int batchSize = 1;     int input_size = batchSize * 2000 * 3 * sizeof(float);  // [1, 2000, 3]     int output_size = batchSize * 3 * sizeof(float);        // [1, 3]     // Prepare normalized input data and set DC component to zero     float input_data[2000 * 3];     const int MN = 4000;     for (int i = 0; i enqueueV2(buffers, stream, nullptr);     cudaStreamSynchronize(stream);     // Copy the output data from GPU to CPU     cudaMemcpy(predicted_output, buffers[output_index], output_size, cudaMemcpyDeviceToHost);     // Free GPU memory     cudaFree(buffers[input_index]);     cudaFree(buffers[output_index]);     cudaStreamDestroy(stream); } ``` This is how I load one model in app and how I call inference function: ``` 	IRuntime* runtime = createInferRuntime(gLogger); 	if (!runtime) { 		write_log(LOG_ERROR, ""client_handler: Failed to create runtime for client %s"", client.ClientName); 		return (void*)1; 	} 	std::vector engine_data = loadEngine(client.ModelPath, client.ClientName);	           ICudaEngine* engine = runtime>deserializeCudaEngine(engine_data.data(), engine_data.size(), nullptr);           if (!engine) {           	write_log(LOG_ERROR, ""client_handler: Failed to create engine for thread %s"", client.ClientName);           	return (void*)1;           }             IExecutionContext* context = engine>createExecutionContext();             if (!context) {             write_log(LOG_ERROR, ""client_handler: Failed to create execution context for thread %s"", client.ClientName);							             engine>destroy();             return (void*)1;             }             int input_index = engine>getBindingIndex(client.ModelInputBindingName) ;//get from config file             int output_index = engine>getBindingIndex(client.ModelOutputBindingName); //get from config file       	   RunInference(engine, context, input_index, output_index, x_fft, y_fft, z_fft, predicted_output, client.G_code, client.ClientName);       	// Synchronize the GPU to ensure all operations are completed       	cudaDeviceSynchronize();       	// Check for CUDA errors after synchronization       	cudaError_t err = cudaGetLastError();       	if (err != cudaSuccess) {       		write_log(LOG_ERROR, ""CUDA error after synchronization in thread '%s': %s"", client.ClientName, cudaGetErrorString(err));       	} else {       		write_log(LOG_INFO, ""GPU synchronized successfully for thread '%s'"", client.ClientName);       	}       	 context>destroy();       	 engine>destroy();          runtime>destroy(); ``` I want to point out that the vibrations are detected by the application, but I don’t understand why the range of values doesn’t change depending on the trained model from the two scenarios. I suspect the problem might be with the model conversion or the inference process / function in TensorRT using C/C++. Do you have any suggestions?",2025-01-15T14:39:02Z,stat:awaiting response TF 2.11,closed,0,2,https://github.com/tensorflow/tensorflow/issues/84963,"Hi **redans** , Apologies for the delay, and thank you for raising your concern here. I reproduced the code you shared but encountered a different error. Could you please share the Colab gist with all the dependencies so I can analyze it further? Also, it seems there might be some compatibility issues, so please check those for a smoother run. I have attached the documentation for your reference. Thank you!","I solved it! In py traning model code, I 've changed FFT calculation, I've added kissFFT lib, like in CPP inference code."
tpu,tboby,Windows libtensorflow size increased 4x with 2.17," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.17+  Custom code No  OS platform and distribution Windows x86_64  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Libtensorflow.dll for windows was 238MB with 2.16.2, is 909MB with 2.17.0, and is 931MB with 2.18.0. At the same time the linux versions haven't changed significantly. I would not expect the tensorflow binaries on windows to be over twice the size of linux, nor for the size to increase so much without any notice in the release notes. I've tried to look through the bazel configs but there's nothing obvious to me which would cause this!  2.16.2  versions_2.16.2_libtensorflowcpuwindowsx86_64 238MB versions_2.16.2_libtensorflowcpulinuxx86_64 422MB  2.17.0  versions_2.17.0_libtensorflowcpuwindowsx86_64 909MB versions_2.17.0_libtensorflowcpulinuxx86_64 412MB  2.18.0 versions_2.18.0_libtensorflowcpuwindowsx86_64 931MB  Standalone code to reproduce the issue ```shell Libtensorflow is provided by google via the GCS buckets documented here https://www.tensorflow.org/install/lang_c ```  Relevant log output ```shell ```",2025-01-15T14:25:26Z,type:build/install subtype:windows 2.17,open,0,0,https://github.com/tensorflow/tensorflow/issues/84962
opt,copybara-service[bot],[XLA:GPU] Fix comment for `xla_gpu_analytical_latency_estimator_options` usage.,[XLA:GPU] Fix comment for `xla_gpu_analytical_latency_estimator_options` usage. s/ms/us.,2025-01-15T11:40:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84949
memory leak,copybara-service[bot],PR #21410: [ROCm] Register gfx12xx,PR CC(Memory leak with tf.py_func): [ROCm] Register gfx12xx Imported from GitHub PR https://github.com/openxla/xla/pull/21410 This PR registers new arch to xla repo. Copybara import of the project:  64abffc2aa18daddcc2678b32f75d7ca122b01a2 by scxfjiang : register gfx12xx Merging this change closes CC(Memory leak with tf.py_func) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21410 from ROCm:dev_register_gfx12xx 64abffc2aa18daddcc2678b32f75d7ca122b01a2,2025-01-15T10:51:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84942
opt,jiunkaiy,Qualcomm AI Engine Direct - Add dispatch options for QC,Summary:  Add htp runtime options  Add log level settings dispatch_delegate_qualcomm_test !image,2025-01-15T08:54:11Z,awaiting review comp:lite size:L,open,0,0,https://github.com/tensorflow/tensorflow/issues/84932
t5,copybara-service[bot],PR #21437: [ds-fusion] Fix algebraic simplifier error in debug mode.,"PR CC(Resnet50 applying last pooling layer regardless pooling parameter): [dsfusion] Fix algebraic simplifier error in debug mode. Imported from GitHub PR https://github.com/openxla/xla/pull/21437 This error was observed while trying to land CC(Raspberry Pi install command not properly formatted.) (which is needed for the dsfusion work). This error occurs when there is a constant operation that can be converted into a scalar broadcast, but some other operation is a successor for the constant operation (via control dependency). Such a dependency is not relayed and so the operation is not converted even after the `ReplaceWithNewInstruction` function call. This causes a runtime error in debug mode testing. Fixing this by relaying this control dependency. Copybara import of the project:  9601c96d468a0d56d9f3ed0a925186ab49a4341b by Shraiysh Vaishay : [dsfusion] Fix algebraic simplifier error in debug mode. This error was observed while trying to land CC(Raspberry Pi install command not properly formatted.) (which is needed for the dsfusion work). This error occurs when there is a constant operation that can be converted into a scalar broadcast, but some other operation is a successor for the constant operation (via control dependency). Such a dependency is not relayed and so the operation is not converted even after the `ReplaceWithNewInstruction` function call. This causes a runtime error in debug mode testing. Fixing this by relaying this control dependency.  3a7ab58814f35a8c7f22cb46248cead3c5cdca50 by Shraiysh Vaishay : Change the function in dfs_hlo_visitor to always relay control deps. Merging this change closes CC(Resnet50 applying last pooling layer regardless pooling parameter) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21437 from shraiysh:algebraic_simplifier_fix 3a7ab58814f35a8c7f22cb46248cead3c5cdca50",2025-01-15T07:03:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84926
opt,copybara-service[bot],Add functions for working with dispatch op custom options using the flex buffer api.,Add functions for working with dispatch op custom options using the flex buffer api.,2025-01-15T06:44:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84925
tpu,Jaswanth28,"tensorflow cuda Unable to register cuDNN factory error in wsl2 with tf 2.17,18"," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.18  Custom code No  OS platform and distribution wsl2 ubuntu 24.04lts  Mobile device windows 11 x86  Python version 3.12  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 13.3.06ubuntu2~24.04) 13.3.0  CUDA/cuDNN version 12.5/9.3  GPU model and memory rtx 4060 laptop gpu/8gb vram/16gb ram  Current behavior? python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"" 20250115 05:03:52.570454: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250115 05:03:52.577748: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1736917432.586305     920 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1736917432.588797     920 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250115 05:03:52.597990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. v2.18.0rc24g6550e4bd802 2.18.0  Standalone code to reproduce the issue steps: https://docs.nvidia.com/cuda/cudainstallationguidelinux/index.htmlverifyyouhaveacudacapablegpu use wsl method ```shell python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"" ```  Relevant log output ```shell 20250115 05:03:52.570454: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250115 05:03:52.577748: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1736917432.586305     920 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1736917432.588797     920 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250115 05:03:52.597990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. v2.18.0rc24g6550e4bd802 2.18.0 ```",2025-01-15T05:05:58Z,stat:awaiting response type:build/install stale wsl2 TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84921,"Hi **** , Apologies for the delay, and thank you for reporting the issue. This is a known issue, and there are other related open issues that developers are actively working on. I would recommend taking a look at CC(cuDNN, cuFFT, and cuBLAS Errors), where a similar issue has been reported and is still under review. Additionally, please follow CC(Unable to register cuDNN factory, cuFFT factory, and cuBLAS factory), which tracks updates on a related issue for more information and progress. Thank you for your patience and understanding!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,copybara-service[bot],Canonicalize inputs of conditionals into tuples in `ConditionalCanonicalizer`,"Canonicalize inputs of conditionals into tuples in `ConditionalCanonicalizer` `DynamicDimensionInference` expects all conditional inputs/outputs to be tuplized so that it can easily add more inputs and `RET_CHECK`fails otherwise, but `ConditionalCanonicalizer` only canonicalizes the outputs. This CL changes the canonicalizer to tuplize the inputs of conditionals as well.",2025-01-15T02:23:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84916
sharding,copybara-service[bot],"In SPMD partitioner, preprocess the sharding on singleton dimensions (dimensions whose size is 1).","In SPMD partitioner, preprocess the sharding on singleton dimensions (dimensions whose size is 1). It is meaningless to partition a dimension whose size is 1. Redundant padding and unpadding may be inserted. To avoid this, we replicate the sharding on these dimensions as a preprocessing. Take the following input as example ``` ENTRY entry {   %constant.785 = f32[1,8] constant({{0,1,2,3,4,5,6,7}}), sharding={devices=[1,8] f32[] {   %constant.8 = u32[8]{0} constant({0, 1, 2, 3, 4, 5, 6, 7})   %partitionid = u32[] partitionid()   %dynamicslice.3 = u32[1]{0} dynamicslice(u32[8]{0} %constant.8, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.2 = u32[] reshape(u32[1]{0} %dynamicslice.3)   %constant.9 = u32[] constant(0)   %compare = pred[] compare(u32[] %reshape.2, u32[] %constant.9), direction=EQ   %broadcast = pred[1,1]{1,0} broadcast(pred[] %compare), dimensions={}   %constant.0 = f32[1,8]{1,0} constant({ { 0, 1, 2, 3, 4, 5, 6, 7 } })   %constant.1 = s32[] constant(0)   %constant.2 = s32[8]{0} constant({0, 1, 2, 3, 4, 5, 6, 7})   %dynamicslice = s32[1]{0} dynamicslice(s32[8]{0} %constant.2, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape = s32[] reshape(s32[1]{0} %dynamicslice)   %dynamicslice.1 = f32[1,1]{1,0} dynamicslice(f32[1,8]{1,0} %constant.0, s32[] %constant.1, s32[] %reshape), dynamic_slice_sizes={1,1}   %copy = f32[1,1]{1,0} copy(f32[1,1]{1,0} %dynamicslice.1)   %constant.10 = f32[] constant(0)   %broadcast.1 = f32[1,1]{1,0} broadcast(f32[] %constant.10), dimensions={}   %select = f32[1,1]{1,0} select(pred[1,1]{1,0} %broadcast, f32[1,1]{1,0} %copy, f32[1,1]{1,0} %broadcast.1)   %allreduce = f32[1,1]{1,0} allreduce(f32[1,1]{1,0} %select), channel_id=1, replica_groups={{0,1,2,3,4,5,6,7}}, use_global_device_ids=true, to_apply=%add.clone   ROOT %reshape.3 = f32[] reshape(f32[1,1]{1,0} %allreduce) } ``` Result with this improvement ``` ENTRY %entry_spmd () > f32[] {   %constant.0 = f32[1,8]{1,0} constant({ { 0, 1, 2, 3, 4, 5, 6, 7 } })   %slice.0 = f32[1,1]{1,0} slice(f32[1,8]{1,0} %constant.0), slice={[0:1], [0:1]}   ROOT %reshape.1 = f32[] reshape(f32[1,1]{1,0} %slice.0) } ``` Reverts a7703e73d050fe62fd59ffd4435a8f9249dfc99c FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21273 from nvcastet:ncclCommInitRankScalable dd6362af36a1f4d22532ad15b2007527898b5fa1",2025-01-15T00:31:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84908
sharding,copybara-service[bot],Remove a check for the sharding in `FindPadWithWrapPattern`.,"Remove a check for the sharding in `FindPadWithWrapPattern`. It is unnecessary to have the same sharding since `FindPadWithWrapPattern` is only used to rewrite the graph with full shape. Even if the shardings are different, we can still rewrite the graph. The partitioner will handle the different sharding.",2025-01-15T00:04:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84905
tpu,copybara-service[bot],add changelist number to tfrt pjrt impl,"add changelist number to tfrt pjrt impl plumb through cl as ""cl_number"" in attributes. This attribute can be used by direct callers of the tpu library to determine the build version.",2025-01-14T22:39:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84900
int8,Yadan-Wei,Seg Fault when iterate dataset created from data service," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Segfault when trying to iterate dataset get from data service.  Standalone code to reproduce the issue ```shell  start the data service file start_dataservice.py import tensorflow as tf dispatcher = tf.data.experimental.service.DispatchServer(     tf.data.experimental.service.DispatcherConfig(port=50050), start=True ) dispatcher_address = dispatcher.target.split(""://"")[1] worker = tf.data.experimental.service.WorkerServer(     tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True ) print(""Starting Worker"") worker.join()  test file test_dataset_service.py import tensorflow as tf import numpy as np flags = tf.compat.v1.app.flags flags.DEFINE_bool(""local"", False, ""Run data service in process"") flags.DEFINE_bool(""distribute"", False, ""Run data service in distributed_epoch mode"") FLAGS = flags.FLAGS def local_service():     print(""Starting Local Service"")     dispatcher = tf.data.experimental.service.DispatchServer(         tf.data.experimental.service.DispatcherConfig(port=50050), start=True     )     dispatcher_address = dispatcher.target.split(""://"")[1]     worker = tf.data.experimental.service.WorkerServer(         tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True     )     print(""Dispatcher target is "", dispatcher.target)     return dispatcher, worker, dispatcher.target def apply_transformations(ds_train):     ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)     ds_train = ds_train.cache()     ds_train = ds_train.shuffle(60000)     ds_train = ds_train.batch(128)     ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)     return ds_train (x_train, y_train), _ = tf.keras.datasets.mnist.load_data() x_train = x_train / np.float32(255) y_train = y_train.astype(np.int64) ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)) def normalize_img(image, label):     """"""Normalizes images: `uint8` > `float32`.""""""     return tf.cast(image, tf.float32) / 255.0, label ds_train = apply_transformations(ds_train)  Create dataset however you were before using the tf.data service. dataset = ds_train if FLAGS.local:     dispatcher, worker, service = local_service() else:     dispatcher_address = ""localhost""     dispatcher_port = ""50050""     service = ""grpc://{}:{}"".format(dispatcher_address, dispatcher_port) if FLAGS.distribute:     processing_mode = ""distributed_epoch"" else:     processing_mode = ""parallel_epochs""  This will register the dataset with the tf.data service cluster so that  tf.data workers can run the dataset to produce elements. The dataset returned  from applying `distribute` will fetch elements produced by tf.data workers. dataset = dataset.apply(     tf.data.experimental.service.distribute(processing_mode=processing_mode, service=service) ) for (x1, y1), (x2, y2) in zip(dataset, ds_train):     np.allclose(x1, x2)     np.allclose(y1, y2) print(""verified mnist dataset locally vs over service"")  script to run  python m pip install upgrade pip python m pip install tensorflow==2.18.0 python m pip install 'protobuf device: 0, name: NVIDIA A100SXM440GB, pci bus id: 0000:10:1c.0, compute capability: 8.0 I0000 00:00:1736891783.520395    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 37945 MB memory:  > device: 1, name: NVIDIA A100SXM440GB, pci bus id: 0000:10:1d.0, compute capability: 8.0 I0000 00:00:1736891783.522012    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 37945 MB memory:  > device: 2, name: NVIDIA A100SXM440GB, pci bus id: 0000:20:1c.0, compute capability: 8.0 I0000 00:00:1736891783.523626    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 37945 MB memory:  > device: 3, name: NVIDIA A100SXM440GB, pci bus id: 0000:20:1d.0, compute capability: 8.0 I0000 00:00:1736891783.525222    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 37945 MB memory:  > device: 4, name: NVIDIA A100SXM440GB, pci bus id: 0000:90:1c.0, compute capability: 8.0 I0000 00:00:1736891783.526807    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 37945 MB memory:  > device: 5, name: NVIDIA A100SXM440GB, pci bus id: 0000:90:1d.0, compute capability: 8.0 I0000 00:00:1736891783.528377    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 37945 MB memory:  > device: 6, name: NVIDIA A100SXM440GB, pci bus id: 0000:a0:1c.0, compute capability: 8.0 I0000 00:00:1736891783.529933    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 37945 MB memory:  > device: 7, name: NVIDIA A100SXM440GB, pci bus id: 0000:a0:1d.0, compute capability: 8.0 /test/bin/testDataservice: line 5:  9168 Segmentation fault      (core dumped) python ${BIN_DIR}/test_dataset_service.py local=False ```",2025-01-14T21:57:20Z,type:bug comp:ops TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/84897
yi,copybara-service[bot],[XLA:GPU] Add `RewritePattern`s for binary elementwise ops in `SimplifyAffinePass`.,"[XLA:GPU] Add `RewritePattern`s for binary elementwise ops in `SimplifyAffinePass`. The rewrites allow us to concretize `index` computations into computations on integers with fixed widths. Triton forces `index`es to concretize to 32bitwide integers, forcing us to concretize early in order to work around an integer overflow when we use `ApplyIndexingOp`s to compute a linear offset into an array with more than `2^32` elements. Eventually, the concretization should be made into a proper passbut we start with a set of `RewritePattern`s to fix the existing integer overflow.",2025-01-14T19:52:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84891
tpu,copybara-service[bot],[PJRT:CPU] Improve handling of input buffers with errors and last execution error,"[PJRT:CPU] Improve handling of input buffers with errors and last execution error * When the CPU client has an input buffer's definition event available, do not ignore it if the event has an error. Instead, take such as definition event as an input dependency, which will poison output buffers. This behavior is consistent with how input error buffers poison output buffers. * If an execution is enqueued before the previous execution has not finished, and the first execution fails with an error, the second execution does not fail accidentally. This is done by clearing any error set in `GetLastEnqueueEvent()` because this last enqueue event is used for force sequential execution, not to propagate errors across independently enqueued computations. * An error buffer is created a dummy internal buffer in `TfrtCpuClient::CreateErrorBuffer`, which meets internal invariants that even an error `TfrtCpuBuffer` has a buffer and can handle metadata operations including `TfrtCpuBuffer::BufferSizes()`.",2025-01-14T18:40:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84879
fp16,gaurides,[oneDNN] Add Infer after last allow to be added to Allow list,"In the current implementation of automixed precision pass for FP16_CPU, the infer node is added to allow set only if both its upstream and downstream nodes are in the allow list. This could cause cast node being inserted in between fuseable nodes. To address this problem, a subpass is added to place an infer node to allow set if its direct upstream is in allow set. This feature was enabled for BF16, and now is being enabled for FP16_CPU to keep the bf16/fp16 optimizations same on CPU. eg: if we have this sequence MatMul > BiasAdd > Relu > Identity Then for FP16_CPU with current implementation, Cast op will be inserted between MM & BiasAdd and the fusion won't happen MatMul (FP16_CPU) > Cast(Fp16 >FP32) > BiasAdd > Relu > Identity With implementation in this PR, Cast op will be inserted after the Infer nodes so that the last fusion is possible MatMul > BiasAdd > Relu > Cast (FP16_CPU > FP32) > Identity FusedMatMul > Cast > Identity",2025-01-14T17:59:57Z,awaiting review size:M comp:core,open,0,1,https://github.com/tensorflow/tensorflow/issues/84874,"Hi , Can you please review this PR? Thank you ! "
oom,copybara-service[bot],[XLA:GPU] Set --set_xla_gpu_experimental_pack_dot_operands_along_k_dimension to true by default.,"[XLA:GPU] Set set_xla_gpu_experimental_pack_dot_operands_along_k_dimension to true by default. If your model started to OOM as a result of this, do this in order of preference. 1. (most preferred)  Layout (subbyte typed) weights of your model in such a way that the contracting (K) dimension of dots they are used in is minor. 2. Use AUTO layout (from JAX side) 3. Set set_xla_gpu_experimental_pack_dot_operands_along_k_dimension=false",2025-01-14T13:58:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84866
tpu,copybara-service[bot],[XLA:TPU] Use `MakeComputationPostOrder()` instead of `MakeComputationSorted()` in `HloDataflowAnalysis::InitializeInstructionValueSets()`.,[XLA:TPU] Use `MakeComputationPostOrder()` instead of `MakeComputationSorted()` in `HloDataflowAnalysis::InitializeInstructionValueSets()`.,2025-01-14T13:45:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84865
opt,jiunkaiy,Qualcomm AI Engine Direct - Add dispatch options for QC,Summary:  Add htp runtime options  Add log level settings  Fix event BUILD,2025-01-14T10:00:09Z,size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84843
tpu,fsamekl,Yolov8-seg.pt segmentation model is deployed on Android after training,"**System information**  Android Device information (use `adb shell getprop ro.build.fingerprint`   if possible): ”“” vivo/PD2020/PD2020:10/QP1A.190711.020/compiler10141555:user/releasekeys “”“  TensorFlow Lite in Play Services SDK version (found in `build.gradle`): ”“”     implementation 'org.tensorflow:tensorflowlitetaskvision:0.4.0'     implementation 'org.tensorflow:tensorflowlitegpudelegateplugin:0.4.0'     implementation 'org.tensorflow:tensorflowlitegpu:2.9.0' “”“  Google Play Services version   (`Settings` > `Apps` > `Google Play Services` > `App details`): **Standalone code to reproduce the issue** ”“Here is the full code of the program”“ """""" public class MainActivity extends AppCompatActivity {     private String MODEL = ""best_float32_metadata.tflite"";          protected void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);         setContentView(R.layout.activity_main);         Bitmap bitmap = BitmapFactory.decodeResource(getResources(), R.drawable.a3);         bitmap = imageScale(bitmap, 640, 640);         TensorImage tensorImage = TensorImage.fromBitmap(bitmap);         Log.e(""HENG"", String.valueOf(tensorImage.getBuffer()));         Log.e(""HENG"", String.valueOf(tensorImage.getTensorBuffer()));         Log.e(""HENG"", String.valueOf(tensorImage.getDataType()));         Log.e(""HENG"", String.valueOf(tensorImage.getColorSpaceType()));         ImageSegmenter.ImageSegmenterOptions options = ImageSegmenter.ImageSegmenterOptions.builder()                 .setBaseOptions(BaseOptions.builder().build())                 .setOutputType(OutputType.CONFIDENCE_MASK)                 .build();         ImageSegmenter imageSegmenter = null;         try {             imageSegmenter = ImageSegmenter.createFromFileAndOptions(this, MODEL, options);         } catch (IOException e) {             throw new RuntimeException(e);         }         List results = imageSegmenter.segment(tensorImage);         Log.e(""HENG"", ""HELLO: ""+results.toString());     }     // 图像缩放方法     public static Bitmap imageScale(Bitmap bitmap, int new_w, int new_h) {         Bitmap scaledBitmap = Bitmap.createScaledBitmap(bitmap, new_w, new_h, true);         return scaledBitmap;     } } """""" **Any other info / logs** Please allow me to repeat my question. Thank you, First, (I may have solved the first problem, but I am not sure) I trained my data set with yolov8seg.pt to get a model. I converted it to tflite format, copied the 32bit model generated by best_float32.tflite into asssets in Android, and then modified the path of model to run the following original code. I got two error messages: ""1 Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.2、java.lang.IllegalStateException: Error getting native address of native library: task_vision_jni”, After I searched, I found“ https://stackoverflow.com/questions/66727627/failedtoinitializedetectorinputtensorhastypektflitefloat32mlkit ”I got a copy of the code in this link. After I tried to run it, I put the model I got into assets again. After running, it was not the above error message (the error message is the second problem). Second, (from the followup to the first question), I also got two errors after running the modified model ""best_float32_metadata.tflite"". The first one is ""java.lang.illegalargumentexception: error occurred when initializing imagesegment: image segmentation models are expected to have only 1 output, found 2"". It says that the model actually returns two outputs, which is a very important question. The second one seems to be the same as the first one, ""java.lang.runtimeexception: unable to start action""activity componentinfo{com.example.yoloseg_android/com.example.yoloseg_android.mainactivity}: java.lang.illegalstateexception: error getting native address of native library: task_vision_jni "", I don't understand this. These are my two problems. I think the second problem should be solved.  Note: I can use the deeplabv3.tflite model officially provided by tensorflow to get the output smoothly",2025-01-14T08:50:42Z,type:support comp:lite Android,open,0,3,https://github.com/tensorflow/tensorflow/issues/84829,I uploaded the project to GitHub. Thank you for your help“https://github.com/fsamekl/Yolov8segAndroidtflite/tree/master”,"Hi,   I apologize for the delayed response, The first issue, `Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images` it requires specifying `NormalizationOptions` metadata to preprocess input images."", was due to lack of metadata. To be more specific, floating models require metadata information of NormalizationOptions. LiteRT Metadata Writer API provides an easytouse API to create Model Metadata for popular ML tasks supported by the TFLite Task Library. You can add metadata using Image segmenters, Please refer TensorFlow Lite Image Segmentation Demo example which may help you to solve your issue. Thank you for your understanding and patience.","> Hi, [](https://github.com/fsamekl) I apologize for the delayed response, The first issue, `Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images` it requires specifying `NormalizationOptions` metadata to preprocess input images."", was due to lack of metadata. To be more specific, floating models require metadata information of NormalizationOptions. >  > LiteRT Metadata Writer API provides an easytouse API to create Model Metadata for popular ML tasks supported by the TFLite Task Library. You can add metadata using Image segmenters, Please refer TensorFlow Lite Image Segmentation Demo example which may help you to solve your issue. >  > Thank you for your understanding and patience. The second question you didn't answer me. I used the segmentation model trained by yolov8. It has two outputs, but imagesegment=imagesegment.createfromfileandoptions (this, model, options); This can only accept one output. What should I do? I uploaded the code to GitHub. I hope you can help solve it"
opt,copybara-service[bot],PR #21375: [ds-fusion] Get While loop analysis with copy fusion,"PR CC(Raspberry Pi install command not properly formatted.): [dsfusion] Get While loop analysis with copy fusion Imported from GitHub PR https://github.com/openxla/xla/pull/21375 In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index. Copybara import of the project:  3147ec926aa1c6fdfa2f4376668434c9a2fbeb87 by Shraiysh Vaishay : [dsfusion] Get While loop analysis with copy fusion In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index.  a435fbd2eadc17269d7bccbe141dcf7a21cc20e8 by Shraiysh Vaishay : Relay control dependencies while converting fusion to call (extractor) Merging this change closes CC(Raspberry Pi install command not properly formatted.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21375 from shraiysh:while_loop_analysis a435fbd2eadc17269d7bccbe141dcf7a21cc20e8",2025-01-14T06:30:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84815
yi,copybara-service[bot],PR #21104: [NVIDIA GPU] Preserve backend config when folding transpose,PR CC(Feature Request: 5D rot90 (for voxel grid rotations)): [NVIDIA GPU] Preserve backend config when folding transpose Imported from GitHub PR https://github.com/openxla/xla/pull/21104 Transpose folding pass doesn't preserve backend config when creating the new dot with transpose folded. Changing the behavior to copy the old dot's config to the new dot. Copybara import of the project:  d2d6b628af1cab777a210e4ac62184e52fe9f4a9 by TJ Xu : Preserve backend config when folding transpose  6b5fa3a1cb70a790803e3ac57ff8329690e88e5e by TJ Xu : use SetupDerivedInstruction instead of just copying the backend config Merging this change closes CC(Feature Request: 5D rot90 (for voxel grid rotations)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21104 from Tixxx:tixxx/transpose_folding 6b5fa3a1cb70a790803e3ac57ff8329690e88e5e,2025-01-14T02:30:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84811
yi,copybara-service[bot],PR #21380: Add F4E2M1FN and F8E8M0FNU types,"PR CC(tf.GradientTape.gradient raise error with tf.nn.relu6): Add F4E2M1FN and F8E8M0FNU types Imported from GitHub PR https://github.com/openxla/xla/pull/21380 Previous PR https://github.com/openxla/xla/pull/19096 was rolled back, retrying. This PR adds F4E2M1FN primitive type (4bit float with 2 bits exponent and 1 bit mantissa), F8E8M0FNU primitive type (8bit float with 8 bits exponent, no mantissa and no sign) and enables loads/stores in the same way S4/U4 type is implemented. This will enable using microscaling (MX) formats (RFC), such as MXFP4. ```c F4E2M1FN  Exponent bias: 1  Maximum stored exponent value: 3 (binary 11)  Maximum unbiased exponent value: 3  1 = 2  Minimum stored exponent value: 1 (binary 01)  Minimum unbiased exponent value: 1 − 1 = 0  Has Positive and Negative zero  Doesn't have infinity  Doesn't have NaNs Additional details:  Zeros (+/): S.00.0  Max normal number: S.11.1 = ±2^(2) x (1 + 0.5) = ±6.0  Min normal number: S.01.0 = ±2^(0) = ±1.0  Min subnormal number: S.00.1 = ±2^(0) x 0.5 = ±0.5 F8E8M0FNU  Exponent bias: 127  Maximum stored exponent value: 254 (binary 1111'1110)  Maximum unbiased exponent value: 254  127 = 127  Minimum stored exponent value: 0 (binary 0000'0000)  Minimum unbiased exponent value: 0 − 127 = 127  Doesn't have zero  Doesn't have infinity  NaN is encoded as binary 1111'1111 Additional details:  Zeros cannot be represented  Negative values cannot be represented  Mantissa is always 1 ``` Related PRs:  https://github.com/openxla/stablehlo/pull/2582  https://github.com/jaxml/ml_dtypes/pull/181  https://github.com/llvm/llvmproject/pull/95392  https://github.com/llvm/llvmproject/pull/108877  https://github.com/jaxml/ml_dtypes/pull/166  https://github.com/llvm/llvmproject/pull/107127  https://github.com/llvm/llvmproject/pull/111028 Copybara import of the project:  d7e00c49a4b4f26c06266d6bb941275e67464c01 by Sergey Kozub : Add F4E2M1FN and F8E8M0FNU types Merging this change closes CC(tf.GradientTape.gradient raise error with tf.nn.relu6) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21380 from openxla:skozub/e2m1_e8m0 d7e00c49a4b4f26c06266d6bb941275e67464c01",2025-01-14T01:51:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84809
yi,copybara-service[bot],Create copy if the operands of gather/scatter instructions overlap.,"Create copy if the operands of gather/scatter instructions overlap. A gather has two operands, input and indices. If they point to the same instruction, create a copy for indices. A scatter has n inputs, 1 indices, and n updates (2n+1 operands in total). We allow overlap between n inputs. We also allow overlap between n updates. We need to create a copy if * indices overlap with any input or update * update overlap with any input The added copy will be removed if it is redundant in the following memory related passes (e.g., CopyInsertion).",2025-01-13T23:26:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84805
sharding,copybara-service[bot],"In preparation for the upcoming JAX support for the `StringDType`, this CL makes two minor tweaks to the `BasicStringArray` class (the string array implementation in the PjRt-IFRT backend):  (1) `CopyToHostBuffer` now supports the host buffer semantics of `kImmutableUntilTransferCompletes`. (2) `FullyReplicated` now works with `ConcreteSharding`.","In preparation for the upcoming JAX support for the `StringDType`, this CL makes two minor tweaks to the `BasicStringArray` class (the string array implementation in the PjRtIFRT backend):  (1) `CopyToHostBuffer` now supports the host buffer semantics of `kImmutableUntilTransferCompletes`. (2) `FullyReplicated` now works with `ConcreteSharding`.",2025-01-13T21:04:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84794
tpu,copybara-service[bot],[xla:cpu:nanort] Suppress msan warnings from uninitialized output buffers,[xla:cpu:nanort] Suppress msan warnings from uninitialized output buffers,2025-01-13T20:00:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84788
opt,copybara-service[bot],PR #21375: [ds-fusion] Get While loop analysis with copy fusion,"PR CC(Raspberry Pi install command not properly formatted.): [dsfusion] Get While loop analysis with copy fusion Imported from GitHub PR https://github.com/openxla/xla/pull/21375 In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index. Copybara import of the project:  ae85690876a106c4d74715fed299779e29e8e641 by Shraiysh Vaishay : [dsfusion] Get While loop analysis with copy fusion In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index. Merging this change closes CC(Raspberry Pi install command not properly formatted.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21375 from shraiysh:while_loop_analysis ae85690876a106c4d74715fed299779e29e8e641",2025-01-13T19:47:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84786
yi,copybara-service[bot],Regenerate pyi stubs with absl::Span imports included,Regenerate pyi stubs with absl::Span imports included,2025-01-13T18:24:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84782
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-13T17:37:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84777
agent,copybara-service[bot],Perform Set key operation first in the Exchange Topology to keep existing behavior unchanged.,"Perform Set key operation first in the Exchange Topology to keep existing behavior unchanged. Only when coordination_agent_recoverable is set, it tries to reconnect to the cluster and would lead to AlreadyExists error. In this case the already_existing error can be handled by checking the existing topology is same as the new one.",2025-01-13T16:22:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84769
bert,copybara-service[bot],PR #19699: Explicit stream annotation: Set ExecutionStreamId based on frontend attribute,PR CC([Intel MKL] Finished support for bad usernames in the CI build scripts.): Explicit stream annotation: Set ExecutionStreamId based on frontend attribute Imported from GitHub PR https://github.com/openxla/xla/pull/19699 This PR picks up the stream annotation frontend attribute on async methods and assigns the matching ExecutionStreamId. This PR is another part of breaking up PR CC(Add parallel implementation of CTC greedy decoder). Copybara import of the project:  db0c310c9ab2df25ce6537618a15fbb2ec3122e8 by chaserileyroberts : Explicit streams are picked up in stream assignment. Merging this change closes CC([Intel MKL] Finished support for bad usernames in the CI build scripts.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19699 from chaserileyroberts:chase/runtime_explicit_streams db0c310c9ab2df25ce6537618a15fbb2ec3122e8,2025-01-13T13:59:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84765
out of memory,copybara-service[bot],PR #20633: Improve the error message of the host out-of-memory,"PR CC(Delete 3_datasets.ipynb): Improve the error message of the host outofmemory Imported from GitHub PR https://github.com/openxla/xla/pull/20633 When working on weight offloading and activation offloading for MaxText Llama27B on a GH200, a host memory Out of Memory (OOM) error occurred as a large amount of memory was offloaded from the device to host memory. This CL clarifies that it was a host OOM, not a device OOM, and suggests using the environment variable XLA_PJRT_GPU_HOST_MEMORY_LIMIT_GB to increase the host memory limit. Copybara import of the project:  e38fac1d00c13185cbb96972814ddc45b0508cd8 by Jane Liu : Improve the error message of the host outofmemory. Merging this change closes CC(Delete 3_datasets.ipynb) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20633 from zhenyingliu:hostOOM e38fac1d00c13185cbb96972814ddc45b0508cd8",2025-01-13T13:56:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84764
opt,copybara-service[bot],[XLA:GPU] Dumping unoptimized HLO snapshots should not trigger dumping of all available information for the HLO module.,[XLA:GPU] Dumping unoptimized HLO snapshots should not trigger dumping of all available information for the HLO module.,2025-01-13T12:43:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84762
opt,copybara-service[bot],Support loading unoptimized HLO snapshot with arguments.,"Support loading unoptimized HLO snapshot with arguments. This should allow us to reproduce hlo runs with the same arguments and, specifically, run benchamkrs that are arguments dependent.",2025-01-13T12:41:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84761
bert,copybara-service[bot],[XLA:GPU] Delete no-op logic for constructing collective combiner keys.,[XLA:GPU] Delete noop logic for constructing collective combiner keys. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19699 from chaserileyroberts:chase/runtime_explicit_streams db0c310c9ab2df25ce6537618a15fbb2ec3122e8,2025-01-13T11:48:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84754
out of memory,copybara-service[bot],PR #20633: Improve the error message of the host out-of-memory,"PR CC(Delete 3_datasets.ipynb): Improve the error message of the host outofmemory Imported from GitHub PR https://github.com/openxla/xla/pull/20633 When working on weight offloading and activation offloading for MaxText Llama27B on a GH200, a host memory Out of Memory (OOM) error occurred as a large amount of memory was offloaded from the device to host memory. This CL clarifies that it was a host OOM, not a device OOM, and suggests using the environment variable XLA_PJRT_GPU_HOST_MEMORY_LIMIT_GB to increase the host memory limit. Copybara import of the project:  e38fac1d00c13185cbb96972814ddc45b0508cd8 by Jane Liu : Improve the error message of the host outofmemory. Merging this change closes CC(Delete 3_datasets.ipynb) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20633 from zhenyingliu:hostOOM e38fac1d00c13185cbb96972814ddc45b0508cd8",2025-01-13T10:53:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84751
bert,copybara-service[bot],PR #19699: Explicit stream annotation: Set ExecutionStreamId based on frontend attribute,PR CC([Intel MKL] Finished support for bad usernames in the CI build scripts.): Explicit stream annotation: Set ExecutionStreamId based on frontend attribute Imported from GitHub PR https://github.com/openxla/xla/pull/19699 This PR picks up the stream annotation frontend attribute on async methods and assigns the matching ExecutionStreamId. This PR is another part of breaking up PR CC(Add parallel implementation of CTC greedy decoder). Copybara import of the project:  db0c310c9ab2df25ce6537618a15fbb2ec3122e8 by chaserileyroberts : Explicit streams are picked up in stream assignment. Merging this change closes CC([Intel MKL] Finished support for bad usernames in the CI build scripts.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19699 from chaserileyroberts:chase/runtime_explicit_streams db0c310c9ab2df25ce6537618a15fbb2ec3122e8,2025-01-13T10:31:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84746
bert,copybara-service[bot],PR #19462: Add ExplicitStreamAnnotationAsyncWrapper pass,"PR CC(Fix discrepancies between doc and implementation for math_ops): Add ExplicitStreamAnnotationAsyncWrapper pass Imported from GitHub PR https://github.com/openxla/xla/pull/19462 This PR introduces a new pass `ExplicitStreamAnnotationAsyncWrapper`. This pass takes `kCall` instructions that are annotated with the frontend attribute `xla_gpu_experimental_stream_annotation`, and wraps the call with an async startdone pair. Initially, I tried to integrate this with the existing `StreamAttributeAnnotator` and `StreamAttributeAsyncWrapper` passes, but much of that code is specifically just for the `windowedeinsum` logic, and users wont necessarily want both enabled at the same time. Thus, I found it cleaner to instead just make a new pass. Copybara import of the project:  4873de9e28551ace7be0ab7e15f5d6e31e17a6cb by chaserileyroberts : Added new async wrap pass Merging this change closes CC(Fix discrepancies between doc and implementation for math_ops) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19462 from chaserileyroberts:chase/stream_async_wrap 4873de9e28551ace7be0ab7e15f5d6e31e17a6cb",2025-01-13T10:18:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84745
tpu,codinglover222,target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize fail to build ,"docker run it v $PWD:/tmp w /tmp tensorflow/build:2.19python3.10  bash c ""bazel build experimental_action_cache_store_output_metadata disk_cache=~/.cache/bazel  jobs=3 config=linux   //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize""  Error message is  /usr/include/c++/13/bits/unique_ptr.h:1070:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous  1070        ^~~~~~~~~~~~~~~~~~~~~~ tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:54:7: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(mlir::TFL::{anonymous}::DefaultQuantParamsPass&&)' (deleted) Target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 407.917s, Critical Path: 206.23s INFO: 43 processes: 7 internal, 36 local. FAILED: Build did NOT complete successfully",2025-01-13T09:25:23Z,stat:awaiting response comp:lite,closed,0,3,https://github.com/tensorflow/tensorflow/issues/84739,"Hi **** , Can you please fill the issue template.Also can you please elaborate about your Feature and please specify the Use Cases for this feature. Thank you!",Thanks  I created Issue CC(target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize fail to build) should I close this one?,"Hi **** , Thank you for providing all the information. Yes, please feel free to close this issue as it is already being tracked in another issue opened by you. Thank you!"
opt,copybara-service[bot],PR #19451: Setting xla_gpu_multi_streamed_windowed_einsum to true by default,PR CC(Feature Request: evaluate both train_data and test_data using tf.contrib.learn.Experiment?): Setting xla_gpu_multi_streamed_windowed_einsum to true by default Imported from GitHub PR https://github.com/openxla/xla/pull/19451 We are trying to deprecate xla_gpu_multi_streamed_windowed_einsum  since we always have better perf with it enabled. This is the first pr to enable it by default to test for stability. Copybara import of the project:  808a9cc0af8901d36a3c219bdf19f38323d01bf3 by Tj Xu : Turn xla_gpu_multi_streamed_windowed_einsum on by default  8221fc4481773f457f5e0235625be22f255fe75b by TJ Xu : Add an option to StreamAttributeAnnotator to skip annotating copystart and async DUS Don't annotate copystart and async DUS when the pass is run before remat  352c1c593b9dcd895f123dea4f7c38e44a787ae6 by TJ Xu : Remove the option to skip annotating copy start and inpect if the module has schedule  257ff6768b59fc7c47c04fa5faa524399f74c80e by TJ Xu : Address rollback by disabling a2a rewrite by default  d3bafebdc0961d61384a49616c29cb9bb6c59db9 by TJ Xu : reverted new flag changes Merging this change closes CC(Feature Request: evaluate both train_data and test_data using tf.contrib.learn.Experiment?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19451 from Tixxx:tixxx/remove_multi_stream_flag d3bafebdc0961d61384a49616c29cb9bb6c59db9,2025-01-13T09:13:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84738
quantization,copybara-service[bot],PR #63959: Typos are fixed in quantization_debugger.ipynb,PR CC(Typos are fixed in quantization_debugger.ipynb): Typos are fixed in quantization_debugger.ipynb Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/63959 Typos and Grammatical errors are fixed Copybara import of the project:  bc8a6549529194af0ed5d5e86d93fe8d0652c10e by LakshmiKalaKadali : Typos are fixed in quantization_debugger.ipynb Typos and Grammatical errors are fixed Merging this change closes CC(Typos are fixed in quantization_debugger.ipynb) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63959 from tensorflow:LakshmiKalaKadalipatch3 bc8a6549529194af0ed5d5e86d93fe8d0652c10e,2025-01-13T07:23:41Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/84721,Check out this pull request on&nbsp;    See visual diffs & provide feedback on Jupyter Notebooks.    Powered by ReviewNB
quantization,copybara-service[bot],PR #20739: Arch specific FP8 SDPA test,"PR CC(Tensorflow lite, invalid quantization ranges): Arch specific FP8 SDPA test Imported from GitHub PR https://github.com/openxla/xla/pull/20739 The workspace size required by CuDNN are different on Hopper and Blackwell. To make the HLO string arch agnostic, pin the workspace size to 0. Copybara import of the project:  f9eafadfdb69d3e68ba6c186979f65dd40e45000 by shuw : Fix fp8 SDPA test workspace to 0 Merging this change closes CC(Tensorflow lite, invalid quantization ranges) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20739 from wenscarl:arch_spec_fp8_sdpa_test f9eafadfdb69d3e68ba6c186979f65dd40e45000",2025-01-13T06:56:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84714
opt,axlrommel,enhancement: add sonargit pr metrics,"This PR introduces a GitHub workflow that leverages the SonarGit Action to collect pull request metrics such as open times, merge rates, and change failure rates. These metrics provide actionable insights to help improve the repository's development workflow. Currently, the workflow logs PR information to the console. Optionally, SonarGit offers a dashboard to visualize the data for deeper analysis—and it’s completely free for life! I’m doing this to help the developer community and to get my name out there. If you’d like more information or assistance in setting this up, feel free to reach out.",2025-01-12T22:41:52Z,size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84696
tpu,Cyprian-igban,tensorflow," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.8  Custom code Yes  OS platform and distribution windows  Mobile device windows  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? unable to load tensorflow as tf  Standalone code to reproduce the issue ```shell  Import necessary libraries import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense from tensorflow.keras.datasets import fashion_mnist from tensorflow.keras.utils import to_categorical  Load and preprocess the dataset (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255 x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255 y_train = to_categorical(y_train, 10) y_test = to_categorical(y_test, 10)  Define the CNN model model = Sequential([     Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),     MaxPooling2D((2, 2)),     Conv2D(64, (3, 3), activation='relu'),     MaxPooling2D((2, 2)),     Flatten(),     Dense(128, activation='relu'),     Dense(10, activation='softmax') ])  Compile the model model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  Train the model model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=32)  Evaluate the model test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2) print(f""Test Accuracy: {test_acc:.2f}"") ```  Relevant log output ```shell ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[1], line 1 > 1 import tensorflow as tf       3 a = tf.constant(2)       4 b = tf.constant(3) File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:85      83     sys.setdlopenflags(_default_dlopen_flags)      84 except ImportError: > 85   raise ImportError(      86       f'{traceback.format_exc()}'      87       f'\n\nFailed to load the native TensorFlow runtime.\n'      88       f'See https://www.tensorflow.org/install/errors '      89       f'for some common causes and solutions.\n'      90       f'If you need help, create an issue '      91       f'at https://github.com/tensorflow/tensorflow/issues '      92       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\lenovo\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. ```",2025-01-12T10:37:11Z,type:build/install,closed,0,3,https://github.com/tensorflow/tensorflow/issues/84692,hey igban check out CC(ImportError: DLL load failed while importing _pywrap_tensorflow_internal:),Duplicate of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.),Are you satisfied with the resolution of your issue? Yes No
opt,advaitpatel,"optimizing layers utilities by improving runtime, memory and readability","This PR introduces several optimizations and enhancements to the layer utilities module, improving runtime performance, memory efficiency, and overall code readability. These changes aim to streamline operations, reduce redundancy, and ensure better maintainability. The code has not been updated in the past 2 years. List of the changes:  introduced VALID_DATA_FORMATS and VALID_PADDING constants to replace repetitive string comparisons. Used set membership checks for faster validation.  combined tuple validation and conversion into a single step.  improved error handling and error messages for clarity.  added lazy evaluation for `true_fn` and `false_fn` using lambdas, ensuring functions are only executed when needed.  simplified checks for boolean and integer inputs to avoid unnecessary conditions.",2025-01-12T00:10:46Z,size:M,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84686,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",I have signed the CLA,refactored code based on the above PR review feedback  refactored convert_data_format to use matchcase for better readability and maintainability.  avoided unnecessary dictionary creation for a single get operation.  enhanced error messages to provide more clarity to users.,closing this for now. and will raise a new one
yi,copybara-service[bot],"Makes keyword arguments of functions loaded from TF1 SavedModels be treated as `POSITIONAL_OR_KEYWROD` (instead of `KEYWORD_ONLY`) arguments by `FunctionType`, so that `FunctionType` won't mistakenly change their order (which can lead to an order mismatch with the underlying TF Graph when calling or re-saving the function).","Makes keyword arguments of functions loaded from TF1 SavedModels be treated as `POSITIONAL_OR_KEYWROD` (instead of `KEYWORD_ONLY`) arguments by `FunctionType`, so that `FunctionType` won't mistakenly change their order (which can lead to an order mismatch with the underlying TF Graph when calling or resaving the function).",2025-01-11T20:44:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84685
opt,copybara-service[bot],[xla:cpu:nanort] Use XLA/TSL utils when possible and add a few micro optimizations,[xla:cpu:nanort] Use XLA/TSL utils when possible and add a few micro optimizations Sprinkle std::move when appropriate and add ABSL_PREDICT_FALSE on unlikely branches. ``` name                old cpu/op   new cpu/op   delta BM_IfRtAddScalars   372ns ± 2%   355ns ± 3%  4.75%  (p=0.000 n=38+35) ```,2025-01-11T19:09:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84684
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-11T09:39:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84675
opt,copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-11T06:13:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84652
opt,copybara-service[bot],[xla:pjrt] Add support for forwarding FFI context to C API client,"[xla:pjrt] Add support for forwarding FFI context to C API client + Correctly (zero/value)initialize PJRT_ExecuteOptions in tests and pjrt_c_api_client ``` If the number of initializer clauses is less than the number of members or initializer list is completely empty, the remaining members are valueinitialized ``` Context: https://github.com/openxla/xla/pull/20429",2025-01-11T05:13:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84650
yi,copybara-service[bot],Internal relative changes only,Internal relative changes only,2025-01-11T04:54:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84649
opt,codinglover222,Adjust the build config to an existing value defined in .bazelrc,"In .bazelrc, there is no build configuration called opt. Update to avoid build failure. ",2025-01-11T04:51:38Z,ready to pull size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84648
opt,copybara-service[bot],[xla:pjrt] Add PJRT_ExecuteContext pointer to PJRT_ExecuteOptions,[xla:pjrt] Add PJRT_ExecuteContext pointer to PJRT_ExecuteOptions For consistency with C++ API pass execute context in ExecuteOptions.,2025-01-11T04:15:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84645
sharding,copybara-service[bot],PR #20808: [GSPMD] Partitions collective permute instructions in manual sharding group.,"PR CC(Feature requested by issue 18354: No gradient defined for operation DepthwiseConv2dNativeBackpropFilter ): [GSPMD] Partitions collective permute instructions in manual sharding group. Imported from GitHub PR https://github.com/openxla/xla/pull/20808 This is a small fix in GSPMD partitioning for partitioning collective permutes instructions added in manual sharding group. In JAX, we can add `ppermute` instruction in shard_map. In cases where we have shard_map with auto axes specified, collective permuting an operand even with the same sharding will end up with an `allgather` and then collective permute, which leads to inefficient collectives. The correct and efficient way is to partition the collective permute as an elementwise op. The unit test added provides a repro. Also, the JAX unit test in https://github.com/jaxml/jax/blob/fa9c7edf736516052df6eab22947bc627d0deca3/tests/shard_map_test.pyL2167 gives a realworld JAX example. Copybara import of the project:  8ee6ecd51f6e4aae8e3d92a6a439a60f53ab02ae by Yunlong Liu : A hacky fix on partitioning collective permute.  e50e87696defb290f7561a7808ee42ebbc11e144 by Yunlong Liu : Local change.  84eb38597c783a4488774823c2c464296a8c54c7 by Yunlong Liu : Simplifies sharding in tests. Merging this change closes CC(Feature requested by issue 18354: No gradient defined for operation DepthwiseConv2dNativeBackpropFilter ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20808 from yliu120:cp_sharding_2 84eb38597c783a4488774823c2c464296a8c54c7",2025-01-11T02:02:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84637
opt,copybara-service[bot],PR #20340: Fix missing template value,"PR CC(reg. tensorflowgpu in amd windows): Fix missing template value Imported from GitHub PR https://github.com/openxla/xla/pull/20340 Fixes a bug introduced in this change: https://github.com/google/tsl/pull/2944 The change makes use of a template variable `%{compiler}`, that is not defined for this file. This causes the `fnocanonicalsystemheaders` option to be set for Clang builds, and Clang will fail with an error about that command line flag not being defined. Copybara import of the project:  75a3d3fbcf2ead55df3872aa80ff21ac3dd9336c by Charles Hofer : Fix missing template value  e08537b09200b0037db7a05780dea0d525399376 by Charles Hofer : Change flag to compiler_is_clang  373f359cbd8d02ee850d98fed92a7bbca4a09c1b by Charles Hofer : Fix typo  2be3c309d05f93a48dd9fdd06af8159108920516 by Harsha HS : [ROCm] Add cudaonly tags for nvidia profiler test Merging this change closes CC(reg. tensorflowgpu in amd windows) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20340 from ROCm:fixmissingtemplatevalue 2be3c309d05f93a48dd9fdd06af8159108920516",2025-01-11T01:55:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84636
opt,copybara-service[bot],PR #20494: Update slop_factor flag desc in debug_options_flags.cc,PR CC(Why dense layer cannot be speed up in tf.contrib.trt ?): Update slop_factor flag desc in debug_options_flags.://github.com/openxla/xla/pull/20494 Copybara import of the project:  04a8e94d73c04e7ffcf3674698a5ad3063918703 by Sevin Varoglu : Update slop_factor flag desc in debug_options_flags. : Fix error  0347b54cdc337e6239f89baf69ba3f6d6c8f160c by Sevin Varoglu : Add default value Merging this change closes CC(Why dense layer cannot be speed up in tf.contrib.trt ?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20494 from sfvaroglu:sevin/update_comment 0347b54cdc337e6239f89baf69ba3f6d6c8f160c,2025-01-11T01:29:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84635
opt,copybara-service[bot],Allows suboptimal solutions for partial mesh shapes when given a *hard* memory budget constraint.,Allows suboptimal solutions for partial mesh shapes when given a *hard* memory budget constraint.,2025-01-11T00:16:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84631
tpu,copybara-service[bot],Add idle and busy time for TPUs to OpStats.,Add idle and busy time for TPUs to OpStats. Add DutyCycleCombiner for handling intra and inter chip duty cycle aggregation. Fix DutyCycleTracker bugs with idleness and duplicate active times.,2025-01-11T00:08:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84630
yi,copybara-service[bot],internal change only to update dependency visibility,internal change only to update dependency visibility FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19066 from Inteltensorflow:mabuzain/handleonednnscalar 576e244530ce0698de0b7137d8e93965fef9d528,2025-01-10T22:45:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84613
yi,copybara-service[bot],[XLA:Python] Make sure we hold the lock on cache_ when destroying executables_ in PjitFunction.,"[XLA:Python] Make sure we hold the lock on cache_ when destroying executables_ in PjitFunction. cache_'s object lock protects executables_ under freethreading mode, so we have to hold the lock.",2025-01-10T22:14:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84611
opt,copybara-service[bot],[xla:cpu] Micro-optimizations for ThunkExecutor,[xla:cpu] Microoptimizations for ThunkExecutor Keep inedges and outedges in a dense container to optimize data locality on a hot path. For smallish thunk sequences all out edges should fit into L1 cache. name                                   old cpu/op   new cpu/op   delta BM_SyncThunkExecutor/1/process_time    29.4ns ± 2%  29.6ns ± 2%  +0.81%   BM_SyncThunkExecutor/2/process_time     103ns ± 2%   101ns ± 3%  1.63%   BM_SyncThunkExecutor/4/process_time     173ns ± 3%   171ns ± 2%  1.10%   BM_SyncThunkExecutor/8/process_time     320ns ± 2%   317ns ± 2%  0.95%   BM_SyncThunkExecutor/16/process_time    652ns ± 2%   638ns ± 2%  2.21%   BM_SyncThunkExecutor/32/process_time   1.28µs ± 3%  1.25µs ± 5%  2.03%   BM_SyncThunkExecutor/64/process_time   2.71µs ± 6%  2.61µs ± 6%  3.73%   BM_SyncThunkExecutor/128/process_time  5.73µs ± 4%  5.41µs ± 3%  5.46%   BM_SyncThunkExecutor/256/process_time  12.0µs ± 3%  11.1µs ± 2%  6.81%   BM_SyncThunkExecutor/512/process_time  25.1µs ± 4%  23.1µs ± 3%  7.93%,2025-01-10T19:59:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84603
tpu,copybara-service[bot],[TF:TPU] Enable cast tests for recently added FP8 types.,"[TF:TPU] Enable cast tests for recently added FP8 types. This registers `float8_e4m3fnuz`, `float8_e4m3b11fnuz` and `float8_e5m2fnuz` as supported types for TF TPU devices.",2025-01-10T18:13:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84601
yi,copybara-service[bot],[XLA:GPU][Emitters] Allow unrolling loops that yield values defined above.,[XLA:GPU][Emitters] Allow unrolling loops that yield values defined above. The change upstream has been integrated.,2025-01-10T17:32:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84599
sharding,copybara-service[bot],#sdy fix bug due to tensor dialect being introduced,"sdy fix bug due to tensor dialect being introduced When investigating a bug, I discovered this fails in JAX: ```py NS = jax.sharding.NamedSharding P = jax.sharding.PartitionSpec mesh = jax.sharding.Mesh(         np.reshape(np.array(jax.devices()), (4,2)), ('data', 'model')) in_avals = (jax.ShapeDtypeStruct((4, 8), jnp.float32),) shardings = (NS(mesh, P('data',)),) (jax.jit, out_shardings=shardings) def gen_dummy_inputs():   return tuple(       jax.random.normal(           jax.random.key(42), shape=in_aval.shape       ).astype(in_aval.dtype)       for in_aval in in_avals   ) gen_dummy_inputs() ``` with the error ``` LLVM ERROR: Building op `tensor.cast` but it isn't known in this MLIRContext: the dialect may not be loaded or this operation hasn't been added by the dialect. See also https://mlir.llvm.org/getting_started/Faq/registeredloadeddependentwhatsupwithdialectsmanagement ``` This was because the sdyroundtripimport introduces the tensor dialect. I'm unsure which pass adds it, but overall what I see is it is actually undone. The details shouldn't matter as long as the pass doesn't crash and the dialect doesn't show up during propagation.",2025-01-10T15:51:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84593
tensorrt,yangjingyuan000804,Tensorflow.math.floormod()," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The operation tf.math.floormod supports float types, but when performing the operation on two floattype tensors with GPU, an internal error occurs. `import tensorflow as tf x = tf.constant([10, 15, 7.5], dtype=tf.float32) y = tf.constant([3, 4, 2.5], dtype=tf.float32) name = ""random_floormod_operation"" result_code = tf.math.floormod(x,y,name) print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"") print(result_code)` **20250110 09:34:03.279577: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable TF_ENABLE_ONEDNN_OPTS=0. 20250110 09:34:03.294385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20250110 09:34:03.312397: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20250110 09:34:03.317811: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250110 09:34:03.330916: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250110 09:34:04.370985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20250110 09:34:05.819237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 513 MB memory: > device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6 20250110 09:34:05.819757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory: > device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6 20250110 09:34:05.820176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory: > device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1736501646.122604 70797 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.124987 70795 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.127360 70789 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.129708 70796 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.132058 70788 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.135845 70801 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.137479 70799 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.139073 70793 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.140713 70787 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.142340 70802 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.143637 70786 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.144931 70783 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.159628 70796 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. 20250110 09:34:06.630804: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_UNSUPPORTED_PTX_VERSION' 20250110 09:34:06.630843: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250110 09:34:06.630870: W tensorflow/core/framework/op_kernel.cc:1828] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250110 09:34:06.630894: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' Traceback (most recent call last): File ""/root/myFuzzer/outputs/test5/code/tensorflow.math.floormod/tensorflow.math.floormod38.py"", line 5, in result_code = tf.math.floormod(x,y,name) File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/sitepackages/tensorflow/python/ops/weak_tensor_ops.py"", line 142, in wrapper return op(*args, kwargs) File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/sitepackages/tensorflow/python/ops/gen_math_ops.py"", line 4177, in floor_mod _ops.raise_from_not_ok_status(e, name) File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/sitepackages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status raise core._status_to_exception(e) from None  pylint: disable=protectedaccess tensorflow.python.framework.errors_impl.InternalError: {{function_node _wrapped__FloorMod_device/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:FloorMod] name: random_floormod_operation The operation runs normally under integer types with GPU. `import tensorflow as tf x = tf.constant([10, 15, 7], dtype=tf.int32) y = tf.constant([3, 4, 2], dtype=tf.int32) name = ""random_floormod_operation"" result_code = tf.math.floormod(x,y,name) print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"") print(result_code)` 20250110 09:38:07.541149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20250110 09:38:07.559247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20250110 09:38:07.564692: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250110 09:38:07.577837: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250110 09:38:08.635137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20250110 09:38:10.256193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 513 MB memory: > device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6 20250110 09:38:10.256732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory: > device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6 20250110 09:38:10.257179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory: > device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6 !!!!!!!!!!!!!!!!!!!!!!!!!!! tf.Tensor([ 1 3 1], shape=(3,), dtype=int32) The float type is correct for the CPU as well. `import tensorflow as tf x = tf.constant([10, 15, 7.8], dtype=tf.float32) y = tf.constant([3, 4, 2.5], dtype=tf.float32) name = ""random_floormod_operation"" with tf.device('/CPU:0'):     result_code = tf.math.floormod(x,y,name) print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"") print(result_code)`20250110 12:57:31.435249: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250110 12:57:31.449893: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20250110 12:57:31.467647: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20250110 12:57:31.472965: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250110 12:57:31.486020: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250110 12:57:32.528966: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20250110 12:57:34.007683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 447 MB memory:  > device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6 20250110 12:57:34.008219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory:  > device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6 20250110 12:57:34.008642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory:  > device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6 !!!!!!!!!!!!!!!!!!!!!!!!!!! tf.Tensor([ 1.        3.         0.3000002], shape=(3,), dtype=float32)  Standalone code to reproduce the issue ```shell import tensorflow as tf x = tf.constant([10, 15, 7.5], dtype=tf.float32) y = tf.constant([3, 4, 2.5], dtype=tf.float32) name = ""random_floormod_operation"" result_code = tf.math.floormod(x,y,name) ```  Relevant log output _No response_",2025-01-10T12:58:11Z,stat:awaiting response type:bug stale comp:apis 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84585,"Hi **** , Apologies for the delay, and welcome to TensorFlow! I tried running your code on Colab using TensorFlow 2.17.0 and 2.18.0 versions with GPU, and I did not encounter any issues. Please find the gist attached here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],Don't set the promotion state explicetly.,Don't set the promotion state explicetly. The method `_set_promotion_state` was removed in numpy 2.2 and the promotion state is set to weak by default: https://numpy.org/devdocs/release/2.2.0notes.htmlnep50promotionstateoptionremoved,2025-01-10T12:30:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84584
tensorrt,yangjingyuan000804,Tensorflow.math.floormod()," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The operation tf.math.floormod supports float types, but when performing the operation on two floattype tensors, an internal error occurs. `import tensorflow as tf x = tf.constant([10, 15, 7.5], dtype=tf.float32) y = tf.constant([3, 4, 2.5], dtype=tf.float32) name = ""random_floormod_operation"" result_code = tf.math.floormod(x,y,name) print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"") print(result_code)` **20250110 09:34:03.279577: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250110 09:34:03.294385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20250110 09:34:03.312397: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20250110 09:34:03.317811: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250110 09:34:03.330916: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250110 09:34:04.370985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20250110 09:34:05.819237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 513 MB memory:  > device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6 20250110 09:34:05.819757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory:  > device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6 20250110 09:34:05.820176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory:  > device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1736501646.122604   70797 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.124987   70795 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.127360   70789 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.129708   70796 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.132058   70788 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.135845   70801 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.137479   70799 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.139073   70793 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.140713   70787 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.142340   70802 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.143637   70786 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.144931   70783 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.159628   70796 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. 20250110 09:34:06.630804: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_UNSUPPORTED_PTX_VERSION' 20250110 09:34:06.630843: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250110 09:34:06.630870: W tensorflow/core/framework/op_kernel.cc:1828] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250110 09:34:06.630894: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' Traceback (most recent call last):   File ""/root/myFuzzer/outputs/test5/code/tensorflow.math.floormod/tensorflow.math.floormod38.py"", line 5, in      result_code = tf.math.floormod(x,y,name)   File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/sitepackages/tensorflow/python/ops/weak_tensor_ops.py"", line 142, in wrapper     return op(*args, **kwargs)   File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/sitepackages/tensorflow/python/ops/gen_math_ops.py"", line 4177, in floor_mod     _ops.raise_from_not_ok_status(e, name)   File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/sitepackages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status     raise core._status_to_exception(e) from None   pylint: disable=protectedaccess tensorflow.python.framework.errors_impl.InternalError: {{function_node __wrapped__FloorMod_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:FloorMod] name: random_floormod_operation** The operation runs normally under integer types. `import tensorflow as tf x = tf.constant([10, 15, 7], dtype=tf.int32) y = tf.constant([3, 4, 2], dtype=tf.int32) name = ""random_floormod_operation"" result_code = tf.math.floormod(x,y,name) print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"") print(result_code)` **20250110 09:38:07.541149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20250110 09:38:07.559247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20250110 09:38:07.564692: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250110 09:38:07.577837: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250110 09:38:08.635137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20250110 09:38:10.256193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 513 MB memory:  > device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6 20250110 09:38:10.256732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory:  > device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6 20250110 09:38:10.257179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory:  > device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6 !!!!!!!!!!!!!!!!!!!!!!!!!!! tf.Tensor([ 1 3  1], shape=(3,), dtype=int32)**  Standalone code to reproduce the issue ```shell import tensorflow as tf x = tf.constant([10, 15, 7.5], dtype=tf.float32) y = tf.constant([3, 4, 2.5], dtype=tf.float32) name = ""random_floormod_operation"" result_code = tf.math.floormod(x,y,name) ```  Relevant log output _No response_",2025-01-10T09:40:55Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/84577,Are you satisfied with the resolution of your issue? Yes No
tpu,copybara-service[bot],PR #21191: [xla:cpu] Fix missing header in oneDNN ACL build,"PR CC(Use correct hash_bucket_size parameter): [xla:cpu] Fix missing header in oneDNN ACL build Imported from GitHub PR https://github.com/openxla/xla/pull/21191 Fixes build error  ``` Compiling src/cpu/jit_utils/jit_utils.cpp failed: (Exit 1): clang failed: error executing command (from target //:mkl_dnn_acl) /usr/lib/llvm14/bin/clang U_FORTIFY_SOURCE fstackprotector Wall Wthreadsafety Wselfassign Wunusedbutsetparameter Wnofreenonheapobject fcolordiagnostics fnoomitframepointer g0 ... (remaining 126 arguments skipped) Use sandbox_debug to see verbose messages from the sandbox and retain the sandbox build root for debugging external/mkl_dnn_acl_compatible/src/cpu/jit_utils/jit_utils.cpp:34:10: fatal error: 'common/ittnotify/jitprofiling.h' file not found include ""common/ittnotify/jitprofiling.h""          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 1 error generated. INFO: Elapsed time: 524.121s, Critical Path: 452.78s INFO: 543 processes: 45 internal, 498 linuxsandbox. FAILED: Build did NOT complete successfully ``` Build step: bazel build config=mkl_aarch64_threadpool test_output=all spawn_strategy=sandboxed //xla/... Copybara import of the project:  23e8fadc3e88208219e685115435c40674efec43 by Crefeda Rodrigues : [xla:cpu] Fix missing headers in oneDNN ACL build Signedoffby: Crefeda Rodrigues  Merging this change closes CC(Use correct hash_bucket_size parameter) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21191 from cfRod:aarch64xlabuild 23e8fadc3e88208219e685115435c40674efec43",2025-01-10T08:47:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84564
tpu,johnnkp,gen_quantized_function_library: clang-cl compilation file path error," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19 nightly  Custom code Yes  OS platform and distribution Windows 11 24H2  Mobile device _No response_  Python version Anaconda 2024.101  Bazel version 6.5.0  GCC/compiler version Visual Studio 2022 (build tools 14.42) + LLVM 19.1.6 + msys2x86_6420241208  CUDA/cuDNN version CUDA 12.6.3 + CUDNN 9.6.0  GPU model and memory GTX 1050 Ti 4GB  Current behavior? `gen_quantized_function_library` is trying to read `'C:\\msys64\\home\\*\\_bazel_*\\*\\execroot\\org_tensorflow\\bazelout\\x64_windowsoptexec*\\bin\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.exe.runfiles\\org_tensorflow\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.py'` on Windows. However, `os.path.exists()` cannot resolve `\\` symbol. Correct path is just like: `'C:/msys64/home/AMD/_bazel_amd/2oea4ayg/execroot/org_tensorflow/bazelout/x64_windowsoptexecBB41B15F/bin/tensorflow/compiler/mlir/quantization/tensorflow/gen_quantized_function_library'`  Standalone code to reproduce the issue ```shell 1. download https://github.com/johnnkp/tensorflowwgputest/archive/refs/heads/default_memory_space_description.zip and extract 2. run `python configure.py` to configure Windows CUDA build 3. run `bazel build config=win_clang config=cuda_wheel config=opt define=no_tensorflow_py_deps=true repo_env=TF_PYTHON_VERSION=3.12 //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow_gpu` ```  Relevant log output ```shell ERROR: C:/users/amd/downloads/tensorflowwgputest/tensorflow/compiler/mlir/quantization/tensorflow/BUILD:38:8: Executing genrule //tensorflow/compiler/mlir/quantization/tensorflow:quantized_function_library failed: (Exit 1): bash.exe failed: error executing command (from target //tensorflow/compiler/mlir/quantization/tensorflow:quantized_function_library)   cd /d C:/msys64/home/amd/_bazel_amd/2oea4ayg/execroot/org_tensorflow   SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Windows;C:\Windows\System32;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6\extras\CUPTI\lib64;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6\bin;C:\Program Files\LLVM\bin;C:\Users\AMD\anaconda3\Scripts;C:\Users\AMD\anaconda3;C:\msys64\usr\local\bin;C:\msys64\usr\bin;C:\msys64\usr\bin;C:\msys64\opt\bin;C:\Windows\System32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\msys64\usr\bin\site_perl;C:\msys64\usr\bin\vendor_perl;C:\msys64\usr\bin\core_perl     SET PYTHON_BIN_PATH=C:/Users/AMD/anaconda3/python.exe     SET PYTHON_LIB_PATH=C:/Users/AMD/anaconda3/Lib/sitepackages     SET TF2_BEHAVIOR=1   C:\msys64\usr\bin\bash.exe c source external/bazel_tools/tools/genrule/genrulesetup.sh; bazelout/x64_windowsoptexecBB41B15F/bin/tensorflow/compiler/mlir/quantization/tensorflow/gen_quantized_function_library.exe output_file bazelout/x64_windowsopt/bin/tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library.h src 'tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library_uniform_quantized.mlir tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library.mlir tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library_uniform_quantized_drq.mlir tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library_tf_drq.mlir tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library_xla_weight_only.mlir'  Configuration: bb43855b4d3a8365141a732926c882a21d79a321eb57928ec09cdaf313f3403c  Execution platform: //tensorflow/tools/toolchains/win:x64_windowsclangcl Traceback (most recent call last):   File ""C:\msys64\home\AMD\_bazel_amd\2oea4ayg\execroot\org_tensorflow\bazelout\x64_windowsoptexecBB41B15F\bin\tensorflow\compiler\mlir\quantization\tensorflow\gen_quantized_function_library"", line 559, in      Main()   File ""C:\msys64\home\AMD\_bazel_amd\2oea4ayg\execroot\org_tensorflow\bazelout\x64_windowsoptexecBB41B15F\bin\tensorflow\compiler\mlir\quantization\tensorflow\gen_quantized_function_library"", line 490, in Main     assert os.path.exists(main_filename), \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ AssertionError: Cannot exec() 'C:\\msys64\\home\\AMD\\_bazel_amd\\2oea4ayg\\execroot\\org_tensorflow\\bazelout\\x64_windowsoptexecBB41B15F\\bin\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.exe.runfiles\\org_tensorflow\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.py': file not found. Target //tensorflow/tools/pip_package:wheel failed to build INFO: Elapsed time: 65.631s, Critical Path: 5.13s INFO: 30 processes: 13 disk cache hit, 8 internal, 9 local. FAILED: Build did NOT complete successfully ```",2025-01-10T07:01:31Z,stat:awaiting response type:build/install subtype:windows TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84558,"I found out `rules_python` provide the python file template, but modify `python/private/python_bootstrap_template.txt` will not affect the generated `.py`.","Hi **** , Apologies for the delay, and thank you for raising your issue here. The main cause appears to be related to your file path. On Windows, path handling is different, and the error suggests that os.path.exists() cannot correctly resolve the path due to the use of backslashes (`\`). These are standard in Windows paths but need to be properly managed in Python. To resolve this, configure your setup to use forward slashes (`/`) instead of backslashes (`\`) for Windows paths. Python, especially when running in environments like MSYS2 or Git Bash, often handles forward slashes more consistently. If you have already tried this, please rebuild the Bazel target that generates the `.py` file. If the issue persists, let us know so we can further assist you. Thank you!","Although I already found out a fix for this issue, I am not going to create a pull request because my compilation failed with nvcc error in windows. And my fix seems unnecessary for other builds.",Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],[xla:cpu] Micro-optimizations for ThunkExecutor,[xla:cpu] Microoptimizations for ThunkExecutor Keep inedges and outedges in a dense container to optimize data locality on a hot path. For smallish thunk sequences all out edges should fit into L1 cache. name                                   old cpu/op   new cpu/op   delta BM_SyncThunkExecutor/1/process_time    29.4ns ± 2%  29.6ns ± 2%  +0.81%   BM_SyncThunkExecutor/2/process_time     103ns ± 2%   101ns ± 3%  1.63%   BM_SyncThunkExecutor/4/process_time     173ns ± 3%   171ns ± 2%  1.10%   BM_SyncThunkExecutor/8/process_time     320ns ± 2%   317ns ± 2%  0.95%   BM_SyncThunkExecutor/16/process_time    652ns ± 2%   638ns ± 2%  2.21%   BM_SyncThunkExecutor/32/process_time   1.28µs ± 3%  1.25µs ± 5%  2.03%   BM_SyncThunkExecutor/64/process_time   2.71µs ± 6%  2.61µs ± 6%  3.73%   BM_SyncThunkExecutor/128/process_time  5.73µs ± 4%  5.41µs ± 3%  5.46%   BM_SyncThunkExecutor/256/process_time  12.0µs ± 3%  11.1µs ± 2%  6.81%   BM_SyncThunkExecutor/512/process_time  25.1µs ± 4%  23.1µs ± 3%  7.93%,2025-01-10T02:10:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84531
yi,copybara-service[bot],Internal change only,Internal change only,2025-01-10T00:59:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84525
tpu,Catakang,Memory Allocation Issues," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.1 LTS on WSL2  Mobile device _No response_  Python version 3.12.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 90300  GPU model and memory RTX 4070 12GB  Current behavior? I create a virtual gpu with a hard limit of 10GB. I start training the network and it works for bit but then says out of memory and tries to allocate more than the set limit. What I expect to happen is that is stays within the 10GB limit and can train the network successfully.  Standalone code to reproduce the issue ```shell import numpy as np import keras from keras import layers import tensorflow as tf import tensorflow_datasets as tfds import matplotlib.pyplot as plt %matplotlib inline tfds.disable_progress_bar() gpus = tf.config.list_physical_devices('GPU') if gpus:    Restrict TensorFlow to only allocate 1GB of memory on the first GPU   try:     tf.config.set_logical_device_configuration(         gpus[0],         [tf.config.LogicalDeviceConfiguration(memory_limit=10240)])     logical_gpus = tf.config.list_logical_devices('GPU')     print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")   except RuntimeError as e:      Virtual devices must be set before GPUs have been initialized     print(e) train_ds, validation_ds, test_ds = tfds.load(     ""cats_vs_dogs"",      Reserve 10% for validation and 10% for test     split=[""train[:40%]"", ""train[40%:50%]"", ""train[50%:60%]""],     as_supervised=True,   Include labels ) resize_fn = keras.layers.Resizing(150, 150) train_ds = train_ds.map(lambda x, y: (resize_fn(x), y)) validation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y)) test_ds = test_ds.map(lambda x, y: (resize_fn(x), y)) augmentation_layers = [     layers.RandomFlip(""horizontal""),     layers.RandomRotation(0.1), ] def data_augmentation(x):     for layer in augmentation_layers:         x = layer(x)     return x train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y)) from tensorflow import data as tf_data batch_size = 16 train_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache() validation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache() test_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache() base_model = keras.applications.Xception(     weights=""imagenet"",   Load weights pretrained on ImageNet.     input_shape=(150, 150, 3),     include_top=False, )   Do not include the ImageNet classifier at the top.  Freeze the base_model base_model.trainable = False  Create new model on top inputs = keras.Input(shape=(150, 150, 3))  Pretrained Xception weights requires that input be scaled  from (0, 255) to a range of (1., +1.), the rescaling layer  outputs: `(inputs * scale) + offset` scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=1) x = scale_layer(inputs)  The base model contains batchnorm layers. We want to keep them in inference mode  when we unfreeze the base model for finetuning, so we make sure that the  base_model is running in inference mode here. x = base_model(x, training=False) x = keras.layers.GlobalAveragePooling2D()(x) x = keras.layers.Dropout(0.2)(x)   Regularize with dropout outputs = keras.layers.Dense(1)(x) model = keras.Model(inputs, outputs) model.summary(show_trainable=True) model.compile(     optimizer=keras.optimizers.Adam(),     loss=keras.losses.BinaryCrossentropy(from_logits=True),     metrics=[keras.metrics.BinaryAccuracy()], ) epochs = 2 print(""Fitting the top layer of the model"") model.fit(train_ds, epochs=epochs, validation_data=validation_ds) ```  Relevant log output ```shell 20250109 19:39:57.953074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1736469597.967544   14431 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1736469597.971752   14431 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250109 19:39:57.986195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 1 Physical GPUs, 1 Logical GPUs I0000 00:00:1736469600.052169   14431 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10240 MB memory:  > device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:0a:00.0, compute capability: 8.9 Fitting the top layer of the model Epoch 1/2 20250109 19:40:04.479339: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1736469604.583055   14486 service.cc:148] XLA service 0x7f8df8002230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: I0000 00:00:1736469604.583109   14486 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4070, Compute Capability 8.9 20250109 19:40:04.722034: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. I0000 00:00:1736469605.234339   14486 cuda_dnn.cc:529] Loaded cuDNN version 90300   7/582 ━━━━━━━━━━━━━━━━━━━━ 12s 22ms/step  binary_accuracy: 0.5658  loss: 0.6950  I0000 00:00:1736469608.599510   14486 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process. 243/582 ━━━━━━━━━━━━━━━━━━━━ 16s 48ms/step  binary_accuracy: 0.8715  loss: 0.2755 20250109 19:40:20.475756: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 1073741824 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469620.475821   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 1073741824 20250109 19:40:20.615636: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 966367744 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469620.615700   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 966367744 20250109 19:40:20.769033: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 869731072 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469620.769095   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 869731072 20250109 19:40:20.906909: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 782758144 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469620.906973   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 782758144 20250109 19:40:21.048863: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 704482304 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469621.048940   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 704482304 20250109 19:40:21.229614: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 634034176 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469621.229682   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 634034176 20250109 19:40:21.371940: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 570630912 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469621.372000   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 570630912 20250109 19:40:21.510751: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 513568000 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469621.510817   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 513568000 20250109 19:40:21.650945: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 462211328 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469621.651034   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 462211328 20250109 19:40:21.814945: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 415990272 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469621.815035   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 415990272 20250109 19:40:21.954790: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 374391296 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469621.954851   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 374391296 20250109 19:40:22.094150: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 336952320 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469622.094219   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 336952320 20250109 19:40:22.267664: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 303257088 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory ... 20250109 19:40:23.128022: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 161164032 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469623.128090   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 161164032 20250109 19:40:23.296856: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 145047808 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469623.296940   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 145047808 Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ```",2025-01-10T00:49:17Z,stat:awaiting response type:bug stale TF 2.18,closed,1,7,https://github.com/tensorflow/tensorflow/issues/84523,"I am facing exactly the same issue for a while now on two slightly different systems:  It even shows the warnings when setting `os.environ['TF_CPP_MIN_LOG_LEVEL'] = ""3""`. In my case, my code runs through despite these error messages, but on the one hand they are annoying and also a bit worrying and on the other hand I have the feeling that they affect the execution time, because I often observe strange behaviour regarding the execution times.","It does affect execution time and my code does not run to completion with the errors as I let it run for an hour to see what happened and it finished with a message saying '0 successful operations'. I don't know why I am running into this error as I am just trying to follow a tutorial on the keras website. I may not have a multi GPU setup to train a bunch of networks in an optimized fashion but surely 10gb on my 4070 should be plenty to run 2 epochs with a batch size of 10. This is ridiculous and from what I am reading from other GitHub issues, this has been an issue for years on certain systems that they have simply not fixed(if I understand everything right) and I really just want to figure out tensorflow for my science fair project.","I will mention I have tried growing the memory, I tried the malloc cuda async flag, I tried slowly reducing the batch size smaller and smaller, this really shouldn't be that complicated.","Hi **** , Apologies for the delay, and thank you for raising your concern here. I attempted to run your code on Colab using the TensorFlow nightly version but encountered a different issue. I have attached a gist for your review—could you please check and let me know if I made any mistakes while executing your code? Additionally, in your setup, consider disabling XLA to potentially reduce memory usage. Let us know if you are still encountering the same issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
sharding,copybara-service[bot],Add `SpmdPartitioningVisitor::HandleBitcastConvert`.,"Add `SpmdPartitioningVisitor::HandleBitcastConvert`. Before this change, we use the default action for BitcastConvert operations. If the input and output has the same rank, it is recognized as an elementwise operations and is handled by `HandleElementwise`. However, if the input and output has different rank, we will always replicate the input, which is inefficient. With this cl, we can handle cases with different rank smartly. We keep the sharding in batch dims and only replicate the extra dims. Given the following input ``` ENTRY entry {   p0 = s64[4] parameter(0), sharding={devices=[2,2] f32[2,1] {   %param = s64[2]{0} parameter(0), sharding={devices=[2,2] f32[2,1] {   %param = s64[2]{0} parameter(0), sharding={devices=[2,2]0,2,1,3 last_tile_dim_replicate}   %collectivepermute = s64[2]{0} collectivepermute(s64[2]{0} %param), channel_id=1, source_target_pairs={{0,0},{2,1},{1,2},{3,3}}   %result.1 = f32[2,2]{1,0} bitcastconvert(s64[2]{0} %collectivepermute)   %constant.3 = s32[4]{0} constant({0, 0, 2, 2})   %partitionid = u32[] partitionid()   %dynamicslice.1 = s32[1]{0} dynamicslice(s32[4]{0} %constant.3, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.1 = s32[] reshape(s32[1]{0} %dynamicslice.1)   %subtract = s32[] subtract(s32[] %reshape.1, s32[] %reshape.1)   %constant.4 = s32[4]{0} constant({0, 1, 0, 1})   %dynamicslice.2 = s32[1]{0} dynamicslice(s32[4]{0} %constant.4, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.2 = s32[] reshape(s32[1]{0} %dynamicslice.2)   %constant.6 = s32[] constant(0)   %subtract.1 = s32[] subtract(s32[] %reshape.2, s32[] %constant.6)   ROOT %dynamicslice.4 = f32[2,1]{1,0} dynamicslice(f32[2,2]{1,0} %result.1, s32[] %subtract, s32[] %subtract.1), dynamic_slice_sizes={2,1} } ```",2025-01-09T23:24:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84516
sharding,copybara-service[bot],Refactor GetIfrtHloSharding and GetIfrtConcreteEvenSharding,Refactor GetIfrtHloSharding and GetIfrtConcreteEvenSharding to be available in jaxlib. These will be useful for implementing c++ device_put.,2025-01-09T23:11:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84513
yi,copybara-service[bot],This is the first step in unifying the various StreamExecutor::Allocate and ::Deallocate methods.  (e.g. HostMemoryAllocate & HostMemoryDeallocate),This is the first step in unifying the various StreamExecutor::Allocate and ::Deallocate methods.  (e.g. HostMemoryAllocate & HostMemoryDeallocate) Future CLs will make use of these new classes to eliminate the need for adding bespoke Allocate/Deallocate pairs as new MemoryTypes are added.,2025-01-09T22:42:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84508
tpu,copybara-service[bot],[XLA:TPU] Avoid unnecessary and potentially expensive computation sorting during instruction fusion.,[XLA:TPU] Avoid unnecessary and potentially expensive computation sorting during instruction fusion. The computations are not being sorted in a semantically meaningful order; they are sorted by instruction count with ties being broken consistently but arbitrarily (based on a hash of the string representation of the computation). There is therefore no reason why these passes need to traverse the computations in this specific order.,2025-01-09T22:09:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84505
opt,copybara-service[bot],Speed-up DepthwiseInputCopyOp,"Speedup DepthwiseInputCopyOp Optimize this by replacing multiplication with advancing the pointer every iteration. Also avoid reloading depth/etc. from args every time. Fixing benchmark for depthwise conv and running them I get a lot of noise, but it seems positive overall. name                                                                                   old cpu/op   new cpu/op   delta BM_ConvFloatDepthwiseFwdCPU1_conv0_float/real_time  [32_112_112_3_8_24_3_3_1_2_cpu1 ]  33.4µs ±16%  34.7µs ±28%     ~     (p=0.284 n=38+39) BM_ConvFloatDepthwiseFwdCPU4_conv0_float/real_time  [32_112_112_3_8_24_3_3_1_2_cpu4 ]  27.3µs ±57%  26.6µs ±52%     ~     (p=0.556 n=40+40) BM_ConvFloatDepthwiseFwdCPU1_conv1_float/real_time  [32_112_112_64_1_64_3_3_1_2_cpu1]  35.6µs ±24%  36.3µs ±27%     ~     (p=0.283 n=35+40) BM_ConvFloatDepthwiseFwdCPU4_conv1_float/real_time  [32_112_112_64_1_64_3_3_1_2_cpu4]  30.0µs ±27%  31.1µs ±33%     ~     (p=0.377 n=36+34) BM_ConvFloatDepthwiseFwdCPU1_conv2_float/real_time  [32_56_56_128_1_128_3_3_1_2_cpu1]  32.8µs ±14%  33.1µs ±18%     ~     (p=0.761 n=33+38) BM_ConvFloatDepthwiseFwdCPU4_conv2_float/real_time  [32_56_56_128_1_128_3_3_1_2_cpu4]  25.7µs ±57%  26.4µs ±55%     ~     (p=0.609 n=40+40) BM_ConvFloatDepthwiseFwdCPU1_conv3_float/real_time  [32_56_56_128_1_128_3_3_2_2_cpu1]  32.2µs ±17%  31.7µs ±12%     ~     (p=0.204 n=37+35) BM_ConvFloatDepthwiseFwdCPU4_conv3_float/real_time  [32_56_56_128_1_128_3_3_2_2_cpu4]  27.8µs ±32%  27.0µs ±24%     ~     (p=0.341 n=34+39) BM_ConvFloatDepthwiseFwdCPU1_conv4_float/real_time  [32_28_28_128_1_128_3_3_1_2_cpu1]  32.1µs ±13%  31.9µs ±12%     ~     (p=0.470 n=39+36) BM_ConvFloatDepthwiseFwdCPU4_conv4_float/real_time  [32_28_28_128_1_128_3_3_1_2_cpu4]  26.2µs ±30%  25.5µs ±44%     ~     (p=0.677 n=38+37) BM_ConvFloatDepthwiseFwdCPU1_conv5_float/real_time  [32_14_14_512_1_512_3_3_1_2_cpu1]  31.5µs ±18%  31.7µs ±17%     ~     (p=0.742 n=38+39) BM_ConvFloatDepthwiseFwdCPU4_conv5_float/real_time  [32_14_14_512_1_512_3_3_1_2_cpu4]  28.5µs ±28%  27.3µs ±29%     ~     (p=0.208 n=35+37) BM_ConvFloatDepthwiseFwdCPU1_conv6_float/real_time  [32_7_7_1024_1_1024_3_3_1_2_cpu1]  29.3µs ±16%  28.9µs ±21%     ~     (p=0.334 n=39+31) BM_ConvFloatDepthwiseFwdCPU4_conv6_float/real_time  [32_7_7_1024_1_1024_3_3_1_2_cpu4]  8.35µs ±62%  7.08µs ±46%  15.24%  (p=0.026 n=40+37) BM_ConvFloatDepthwiseFwdCPU1_conv7_float/real_time  [32_112_112_3_8_24_3_3_2_2_cpu1 ]  31.2µs ±17%  31.4µs ±22%     ~     (p=0.987 n=35+38) BM_ConvFloatDepthwiseFwdCPU4_conv7_float/real_time  [32_112_112_3_8_24_3_3_2_2_cpu4 ]  25.9µs ±45%  26.5µs ±32%     ~     (p=0.859 n=39+38) BM_ConvFloatDepthwiseFwdCPU1_conv8_float/real_time  [32_112_112_3_8_24_3_3_2_1_cpu1 ]  30.0µs ±16%  30.5µs ±18%     ~     (p=0.228 n=34+33) BM_ConvFloatDepthwiseFwdCPU4_conv8_float/real_time  [32_112_112_3_8_24_3_3_2_1_cpu4 ]  26.2µs ±41%  24.4µs ±53%     ~     (p=0.288 n=36+40) BM_ConvFloatDepthwiseFwdCPU1_conv9_float/real_time  [1_100_100_72_1_72_3_3_1_2_cpu1 ]  26.5µs ±16%  25.6µs ±15%     ~     (p=0.051 n=34+37) BM_ConvFloatDepthwiseFwdCPU4_conv9_float/real_time  [1_100_100_72_1_72_3_3_1_2_cpu4 ]  6.33µs ±37%  5.60µs ±36%  11.46%  (p=0.011 n=40+35) BM_ConvFloatDepthwiseFwdCPU1_conv10_float/real_time [1_100_100_72_1_72_5_5_1_2_cpu1 ]  26.4µs ±13%  27.8µs ±20%     ~     (p=0.140 n=33+40) BM_ConvFloatDepthwiseFwdCPU4_conv10_float/real_time [1_100_100_72_1_72_5_5_1_2_cpu4 ]  14.6µs ±79%   9.2µs ±90%  36.75%  (p=0.000 n=40+40)",2025-01-09T21:30:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84499
int8,mahmoud-abuzaina,Replacement PR for #76210 Add support for quint8 type for uniform_quantize and uniform_ dequantize ops,This PR is a replacement for https://github.com/tensorflow/tensorflow/pull/76210,2025-01-09T20:55:07Z,awaiting review ready to pull size:L,closed,0,3,https://github.com/tensorflow/tensorflow/issues/84497,abuzaina Can you please rebase and try again.  The internal error fixed on sync. , I just rebased it. Thanks!,>> Presubmit  Linux x86 CPU  Py+CPP Test Suite `//tensorflow/python/kernel_tests:collective_ops_multi_worker_test` timeout. Looking into it.  
tpu,copybara-service[bot],[XLA:TPU] Disable memory space assignment pass via `exec_time_optimization_effort`.,[XLA:TPU] Disable memory space assignment pass via `exec_time_optimization_effort`.,2025-01-09T17:51:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84486
attention,copybara-service[bot],PR #20911: [XLA:GPU] Update cudnn frontend version to 1.9,"PR CC(Edit  a link to notebooks directory): [XLA:GPU] Update cudnn frontend version to 1.9 Imported from GitHub PR https://github.com/openxla/xla/pull/20911 cudnn frontend 1.9 is released, there are some new features that cudnn flash attention will incorporate, hence this PR. * flex attention with arbitrary pointwise operations after softmax in cudnn flash attention graph. * sequence packing enhancement with reduced workspace size. Release note: https://github.com/NVIDIA/cudnnfrontend/releases/tag/v1.9.0 Copybara import of the project:  07a0d7a6cdff107b3c69dd2a66fc2b247de056e7 by cjkkkk : update Merging this change closes CC(Edit  a link to notebooks directory) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20911 from Cjkkkk:update_cudnn_fe_1.9 07a0d7a6cdff107b3c69dd2a66fc2b247de056e7",2025-01-09T16:34:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84479
tpu,copybara-service[bot],[XLA:GPU] Migrate collective bytes transfered to `BytesTransferred`.,[XLA:GPU] Migrate collective bytes transfered to `BytesTransferred`. `output_bytes_accessed` is not necessarily transferred by the network (for example reduce scatter breaks the rule),2025-01-09T15:30:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84477
opt,dewa-ai,Update README.md,"This code provides the implementation of a custom version of the Adam optimizer called NonFusedAdam for TensorFlow. It inherits from its base optimizer class and implements the Adam algorithm with or without AMSGrad and other hyperparameters including learning rate, betas, and epsilon. To use this optimizer: 1. Make sure TensorFlow is installed. 2. Save the optimizer code in a Python file (like custom_adam.py). 3. Import the optimizer in your script. 4. Build a model, compile it using NonFusedAdam, and train it with your dataset (such as MNIST). 5. Run the script to train the model and check its performance.",2025-01-09T14:54:44Z,size:M,closed,0,1,https://github.com/tensorflow/tensorflow/issues/84475,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
tpu,copybara-service[bot],[XLA:GPU] move TransposeFolding after simplifier pipeline,"[XLA:GPU] move TransposeFolding after simplifier pipeline Combination of DotDecompose, AlgebraicSimplifier, and TransposeFolding might never reach a fixed point and stuck in a rewrite loop. Moving TransposeFolding in simplification2 pipeline should result in a similar output as we keep the relative order. Also:  Added a few tests to detect / record cases when we run into rewrite loop  Log warning when HloPassFix reaches iteraction limit as this is likely a similar bug",2025-01-09T14:47:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84474
tpu,fighting300,Build iOS tensorflowLite error with iOS library," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.8  Custom code Yes  OS platform and distribution iOS   Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When i build tensorflow lite with library, i encounter error below，the error show：Undefined symbols for architecture arm64:   ""std::__1::basic_string, std::__1::allocator>::find(char, unsigned long) const"",   Standalone code to reproduce the issue ```shell Undefined symbols for architecture arm64:   ""std::__1::basic_string, std::__1::allocator>::find(char, unsigned long) const"", referenced from:       l001 in libNeuralnet_a.a32       l001 in libNeuralnet_a.a32       l001 in libNeuralnet_a.a32       l001 in libNeuralnet_a.a32       l001 in libNeuralnet_a.a32       l001 in libNeuralnet_a.a32   ""std::__1::basic_string, std::__1::allocator>::rfind(char, unsigned long) const"", referenced from:       l001 in libNeuralnet_a.a32       l001 in libNeuralnet_a.a32       l001 in libNeuralnet_a.a32   ""std::__1::basic_string, std::__1::allocator>::compare(unsigned long, unsigned long, char const*) const"", referenced from:       l001 in libNeuralnet_a.a32   ""std::__1::basic_string, std::__1::allocator>::compare(unsigned long, unsigned long, char const*, unsigned long) const"", referenced from: ```  Relevant log output _No response_",2025-01-09T10:59:27Z,stat:awaiting response type:bug stale comp:lite TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84466,"Hi,   I apologize for the delayed response, if possible could you please help us with exact steps which you followed before encountering mentioned error in the issue template or if you followed any official documentation please help us with it to replicate the same behavior from our end ? **EDIT :** I believe you're following this official documentation and I see you mentioned TensorFlow version 2.8 so could you please give it try with latest version of TensorFlow and see is it working as expected or not ? If issue still persists please let us know with error log for further investigation Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
gemma,copybara-service[bot],[xla:cpu:benchmarks] Add scripts to run Gemma2 Keras model.,[xla:cpu:benchmarks] Add scripts to run Gemma2 Keras model.,2025-01-09T10:29:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84465
tpu,MoFHeka,GPU Profiling: MemoryProfile do not contain memory events when profile remote worker.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version nightly  Custom code No  OS platform and distribution Ubuntu 22.04  Python version Python 3.12  CUDA/cuDNN version CUDA 12.4  GPU model and memory A100 80GB  Current behavior? Start a simple any collective training with Tensorflow cluster config. And then use RPC client capture_profile in Tensorboard or tf.profiler.experimental.client.trace.  No any memory profile events or OP profiler, but only trace view.  Standalone code to reproduce the issue **tf_allreduce.py** ```python import tensorflow as tf from tensorflow.python.ops.collective_ops import all_reduce, all_reduce_v2 from tensorflow.python.eager import context from tensorflow.core.protobuf import config_pb2 from tensorflow.python.distribute import cluster_resolver as cluster_resolver_lib cluster_resolver = cluster_resolver_lib.TFConfigClusterResolver() cluster = cluster_resolver.cluster_spec() task_type = cluster_resolver.task_type task_id = cluster_resolver.task_id experimental_config = config_pb2.ConfigProto.Experimental(     share_cluster_devices_in_session=False,     share_session_state_in_clusterspec_propagation=False ) config = config_pb2.ConfigProto(experimental=experimental_config) config.experimental.collective_group_leader = '/job:worker/replica:0/task:0' server = tf.distribute.Server(cluster,                               job_name=task_type,                               task_index=task_id,                               protocol=""grpc"",  ""grpc+verbs""                               config=config) run_options = config_pb2.RunOptions() with tf.compat.v1.Session(target=server.target, config=config) as sess:     tensor = tf.Variable(tf.ones([2, 2]), dtype=tf.float32)     init = tf.compat.v1.global_variables_initializer()     sess.run(init)     sess.run(tf.print([""tensor:"",tensor]))     reduced_tensor = all_reduce(tensor, group_size=2, group_key=4321, instance_key=1234, merge_op='Add', final_op='Id', communication_hint='auto')     run_options.experimental.collective_graph_key = 6     while True:         sess.run(tf.print([""reduced_tensor:"",reduced_tensor]), options=run_options) ``` Run script to start server. ```bash CUDA_VISIBLE_DEVICES=0 TF_CONFIG='{""cluster"":{""worker"":[""localhost:2223"",""localhost:2224""]},""task"":{""type"":""worker"",""index"":0}}' python tf_allreduce.py& CUDA_VISIBLE_DEVICES=1 TF_CONFIG='{""cluster"":{""worker"":[""localhost:2223"",""localhost:2224""]},""task"":{""type"":""worker"",""index"":1}}' python tf_allreduce.py& ```  use capture_profile in Tensorboard or tf.profiler.experimental.client.trace. ```python tf.profiler.experimental.client.trace(   'grpc://localhost:2223,grpc://localhost:2224',    '/tmp/my_tb_dir',    2000, ) ``` Try to convert xplane.pb to memory_profile, nothing show. ```python from tensorflow.python.profiler.internal import _pywrap_profiler as profiler_wrapper json = profiler_wrapper.xspace_to_tools_data([""xxx.xplane""], ""memory_profile"") ``` **Relevant log output** ``` {""memoryProfilePerAllocator"":{},""numHosts"":1,""memoryIds"":[]} ``` Relative issue: CC(GPU Profiling: MemoryProfile do not contain memory events.) ",2025-01-09T09:26:20Z,stat:awaiting tensorflower type:bug comp:core TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/84460
sharding,copybara-service[bot],Generalize `GetFirstMergeableDimForSortOperand` and rename it as `GetFirstTargetDimToMoveShardingTiles`.,Generalize `GetFirstMergeableDimForSortOperand` and rename it as `GetFirstTargetDimToMoveShardingTiles`. `GetFirstTargetDimToMoveShardingTiles` can be used for moving the sharding tiles from a source dimension to a target dimension when the source dimension and target dimension are different and the size of target dimension is divisible by the merged tile size. This util function will be used in the dimensions that need replication in the partitioner. This cl has no behavior change. We will use this util function to support 1. Concat dimension in concat operations 2. Slice dimensions in dynamicslice operations,2025-01-09T07:39:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84447
opt,copybara-service[bot],PR #20340: Fix missing template value,"PR CC(reg. tensorflowgpu in amd windows): Fix missing template value Imported from GitHub PR https://github.com/openxla/xla/pull/20340 Fixes a bug introduced in this change: https://github.com/google/tsl/pull/2944 The change makes use of a template variable `%{compiler}`, that is not defined for this file. This causes the `fnocanonicalsystemheaders` option to be set for Clang builds, and Clang will fail with an error about that command line flag not being defined. Copybara import of the project:  75a3d3fbcf2ead55df3872aa80ff21ac3dd9336c by Charles Hofer : Fix missing template value  e08537b09200b0037db7a05780dea0d525399376 by Charles Hofer : Change flag to compiler_is_clang  373f359cbd8d02ee850d98fed92a7bbca4a09c1b by Charles Hofer : Fix typo  2be3c309d05f93a48dd9fdd06af8159108920516 by Harsha HS : [ROCm] Add cudaonly tags for nvidia profiler test Merging this change closes CC(reg. tensorflowgpu in amd windows) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20340 from ROCm:fixmissingtemplatevalue 2be3c309d05f93a48dd9fdd06af8159108920516",2025-01-09T07:13:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84445
tpu,15001366252, expect the compilation to pass but there are errors， how to fix it," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15.1  Custom code Yes  OS platform and distribution LINUX Ubuntu 18.04  Mobile device _No response_  Python version 3.8  Bazel version 6.1.0  GCC/compiler version 9.5.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior?  expect the compilation to pass but there are errors ERROR: /home/aist/.cache/bazel/_bazel_aist/572037a80443c6142f9def570fe3ea55/external/XNNPACK/BUILD.bazel:3050:19: Compiling src/tensor.c failed: (Exit 1): gcc failed: error executing command (from target //:subgraph) /usr/local/bin/gcc U_FORTIFY_SOURCE fstackprotector Wall Wunusedbutsetparameter Wnofreenonheapobject fnoomitframepointer g0 O2 'D_FORTIFY_SOURCE=1' DNDEBUG ffunctionsections ... (remaining 98 arguments skipped) In file included from external/XNNPACK/src/tensor.c:19: external/XNNPACK/src/xnnpack/subgraph.h:431:17: error: array type has incomplete element type 'xnn_timestamp' {aka 'struct timespec'}   431                  ^~~~~~~~ Target //tensorflow:libtensorflow_cc.so failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 969.727s, Critical Path: 11.17s INFO: 2534 processes: 701 internal, 1833 local. FAILED: Build did NOT complete successfully  Standalone code to reproduce the issue ```shell bazel build config=opt //tensorflow:libtensorflow_cc.so ```  Relevant log output _No response_",2025-01-09T06:47:58Z,type:build/install,closed,0,1,https://github.com/tensorflow/tensorflow/issues/84443,Are you satisfied with the resolution of your issue? Yes No
yi,copybara-service[bot],Internal change only,Internal change only,2025-01-09T03:02:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84428
tpu,nathom,Unable to connect to TPU through Cloud VM (metadata issue?)," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version v2.18.0rc24g6550e4bd802 2.18.0  Custom code Yes  OS platform and distribution tpuubuntu2204base  Mobile device _No response_  Python version 3.11.2  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am on a VM instance trying to connect to a tpu v432 using a test script. I installed tensorflowtpu on both the VM (in venv) and TPU (globally) as per the instructions from the google website. It seems like there is an issue with getting TPU metadata. It is able to connect to the metadata server when I request manually from the VM: ``` $ curl http://169.254.169.254/computeMetadata/v1/ H ""MetadataFlavor: Google"" instance/ oslogin/ project/ ``` Any help would be appreciated!  Standalone code to reproduce the issue ```shell resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_name) tf.config.experimental_connect_to_cluster(resolver) try:     tf.tpu.experimental.initialize_tpu_system(resolver)     print(""TPU initialized:"", resolver.master()) except Exception as e:     print(""Failed to initialize TPU:"", e) ```  Relevant log output ```shell $ python hello.py 20250108 23:49:33.189260: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250108 23:49:33.221197: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:95] Opening library: /home/ucsdwanglab/test_tpu/.venv/lib/python3.11/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 20250108 23:49:33.221290: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:121] Libtpu path is: /home/ucsdwanglab/test_tpu/.venv/lib/python3.11/sitepackages/libtpu/libtpu.so Failed to get TPU metadata (tpuenv) from instance metadata for variable CHIPS_PER_HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error === Source Location Trace: ===  learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93 learning/45eac/tfrc/runtime/env_var_utils.cc:50 Failed to get TPU metadata (tpuenv) from instance metadata for variable HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error === Source Location Trace: ===  learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93 learning/45eac/tfrc/runtime/env_var_utils.cc:50 Failed to get TPU metadata (tpuenv) from instance metadata for variable ALT: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error === Source Location Trace: ===  learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93 learning/45eac/tfrc/runtime/env_var_utils.cc:50 Failed to get TPU metadata (tpuenv) from instance metadata for variable WRAP: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error === Source Location Trace: ===  learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93 learning/45eac/tfrc/runtime/env_var_utils.cc:50 Failed to get TPU metadata (acceleratortype) from instance metadata for variable TPU_ACCELERATOR_TYPE: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error === Source Location Trace: ===  learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93 Failed to find host bounds for accelerator type: WARNING: could not determine TPU accelerator type, please set env var `TPU_ACCELERATOR_TYPE` manually, otherwise libtpu.so may not properly initialize. Failed to get TPU metadata (agentworkernumber) from instance metadata for variable TPU_WORKER_ID: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error === Source Location Trace: ===  learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93 Failed to get TPU metadata (workernetworkendpoints) from instance metadata for variable TPU_WORKER_HOSTNAMES: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error === Source Location Trace: ===  learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93 WARNING: Logging before InitGoogle() is written to STDERR E0000 00:00:1736380405.363400    3192 common_lib.cc:511] INVALID_ARGUMENT: Error: unexpected worker hostname 'WARNING: could not determine TPU worker hostnames or IP addresses' from env var TPU_WORKER_HOSTNAMES. Expecting a valid hostname or IP address without port number. (Full TPU workers' addr string: WARNING: could not determine TPU worker hostnames or IP addresses, please set env var `TPU_WORKER_HOSTNAMES` manually, otherwise libtpu.so may not properly initialize.) === Source Location Trace: ===  learning/45eac/tfrc/runtime/libtpu_init_utils.cc:173 20250108 23:56:48.526584: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1736380609.730442    3192 context_distributed_manager.cc:762] unknown service tensorflow.WorkerService Additional GRPC error information from remote target /job:worker/replica:0/task:0 while calling /tensorflow.WorkerService/GetStatus: :{""created"":"".730372913"",""description"":""Error received from peer ipv4:10.130.0.3:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""unknown service tensorflow.WorkerService"",""grpc_status"":12} E0108 23:56:49.730822322    3192 completion_queue.cc:244]    assertion failed: queue.num_items() == 0 https://symbolize.stripped_domain/r/?trace=7f1ccaf5cebc,7f1ccaf0e04f&map=  *** SIGABRT received by PID 3192 (TID 3192) on cpu 4 from PID 3192; stack trace: *** PC: @     0x7f1ccaf5cebc  (unknown)  (unknown)     @     0x7f1caa302841       1888  (unknown)     @     0x7f1ccaf0e050   18460496  (unknown)     @     0x7f1ccaed1c60  (unknown)  (unknown) https://symbolize.stripped_domain/r/?trace=7f1ccaf5cebc,7f1caa302840,7f1ccaf0e04f,7f1ccaed1c5f&map=  E0108 23:56:49.732558    3192 coredump_hook.cc:316] RAW: Remote crash data gathering hook invoked. E0108 23:56:49.732569    3192 coredump_hook.cc:355] RAW: Skipping coredump since rlimit was 0 at process start. E0108 23:56:49.732575    3192 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec. E0108 23:56:49.732580    3192 coredump_hook.cc:411] RAW: Sending fingerprint to remote end. E0108 23:56:49.732595    3192 coredump_hook.cc:420] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory E0108 23:56:49.732601    3192 coredump_hook.cc:472] RAW: Dumping core locally. E0108 23:56:49.745981    3192 process_state.cc:805] RAW: Raising signal 6 with default behavior Aborted ```",2025-01-09T00:04:51Z,stat:awaiting response type:bug comp:tpus TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84413,", Could you please provide more information and also steps you have followed to use the TPU's which helps to debug the issue in an effective way. Thank you!","I think this was my mistake. I was using a Google Cloud VM and trying to connect to the TPU pods from there. I was able to resolve the issue by connecting directly to one of the TPU hosts, and running commands on all workers using `gcloud`. Maybe the error messages could be made more helpful, though.",", Glad the issue was resolved by connecting the TPU hosts. Could you please feel free to move this issue to closed status. Thank you!",Are you satisfied with the resolution of your issue? Yes No
quantization,rizkyy702,bug," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible). ``` (You can paste links or attach files by dragging & dropping them below)  Provide links to your updated versions of the above two colab notebooks.  Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model. ```  Option B: Paste your code here or provide a link to a custom endtoend colab ``` (You can paste links or attach files by dragging & dropping them below)  Include code to invoke the TFLite Converter Python API and the errors.  Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model. ```  3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2025-01-09T00:02:56Z,TFLiteConverter,closed,0,1,https://github.com/tensorflow/tensorflow/issues/84412,Nothing provided in the template.
sharding,copybara-service[bot],PR #20808: [GSPMD] Partitions collective permute instructions in manual sharding group.,"PR CC(Feature requested by issue 18354: No gradient defined for operation DepthwiseConv2dNativeBackpropFilter ): [GSPMD] Partitions collective permute instructions in manual sharding group. Imported from GitHub PR https://github.com/openxla/xla/pull/20808 This is a small fix in GSPMD partitioning for partitioning collective permutes instructions added in manual sharding group. In JAX, we can add `ppermute` instruction in shard_map. In cases where we have shard_map with auto axes specified, collective permuting an operand even with the same sharding will end up with an `allgather` and then collective permute, which leads to inefficient collectives. The correct and efficient way is to partition the collective permute as an elementwise op. The unit test added provides a repro. Also, the JAX unit test in https://github.com/jaxml/jax/blob/fa9c7edf736516052df6eab22947bc627d0deca3/tests/shard_map_test.pyL2167 gives a realworld JAX example. Copybara import of the project:  8ee6ecd51f6e4aae8e3d92a6a439a60f53ab02ae by Yunlong Liu : A hacky fix on partitioning collective permute.  e50e87696defb290f7561a7808ee42ebbc11e144 by Yunlong Liu : Local change.  84eb38597c783a4488774823c2c464296a8c54c7 by Yunlong Liu : Simplifies sharding in tests. Merging this change closes CC(Feature requested by issue 18354: No gradient defined for operation DepthwiseConv2dNativeBackpropFilter ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20808 from yliu120:cp_sharding_2 84eb38597c783a4488774823c2c464296a8c54c7",2025-01-08T20:45:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84401
tpu,copybara-service[bot],PR #19066: [XLA:CPU][oneDNN] Handle oneDNN scalar,PR CC(Segfault Custom Op using KenLM): [XLA:CPU][oneDNN] Handle oneDNN scalar Imported from GitHub PR https://github.com/openxla/xla/pull/19066 This PR makes sure oneDNN handles the scalar properly. Copybara import of the project:  2fb157a16c0ea3ff29a39363ef83510edabf3a13 by Mahmoud Abuzaina : Handle oneDNN scalar  77a39b6c047a797bb7db1ed6361440e8e6a6345a by Mahmoud Abuzaina : Addressed review comments  32b5aba9ee009b3fc025825e705fa0dae49af9d6 by Mahmoud Abuzaina : Return output instead of having parameter  576e244530ce0698de0b7137d8e93965fef9d528 by Mahmoud Abuzaina : Unpack the pair return Merging this change closes CC(Segfault Custom Op using KenLM) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19066 from Inteltensorflow:mabuzain/handleonednnscalar 576e244530ce0698de0b7137d8e93965fef9d528,2025-01-08T20:22:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84400
sharding,copybara-service[bot],Rewrite `Reshard(HloSharding::Replicate())` as `Replicate()` for `PartitionedHlo`.,Rewrite `Reshard(HloSharding::Replicate())` as `Replicate()` for `PartitionedHlo`.,2025-01-08T18:57:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84393
opt,copybara-service[bot],Split `RunAndCompare` with reference backend functionality into a mixin.,"Split `RunAndCompare` with reference backend functionality into a mixin. Many users don't require `RunAndCompare` functionality, but are forced to select and initialize a reference backend anyway. With this change, users can opt to extend their specific `HloRunnerAgnosticTestBase` implementation to add `RunAndCompare` functionality. The mixin acts as a wrapper around any `HloRunnerAgnosticTestBase` implementation, allowing a high degree of customization.",2025-01-08T17:34:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84390
tpu,copybara-service[bot],[XLA:GPU] Model output_bytes_accessed for collectives.,[XLA:GPU] Model output_bytes_accessed for collectives.,2025-01-08T17:15:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84388
tpu,copybara-service[bot],[XLA:GPU] Use output_bytes_accessed in SoL latency estimator.,[XLA:GPU] Use output_bytes_accessed in SoL latency estimator.,2025-01-08T17:14:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84387
tpu,copybara-service[bot],[XLA:GPU] Fix sorted scatter with imperfectly tiled indices.,"[XLA:GPU] Fix sorted scatter with imperfectly tiled indices. The algorithm was checking whether to write to the output or not by comparing the current slice index with the number of indices per warp. It works only when we have perfectly tiled indices, e.g. 50 indices per warp with a total of 2000 indices. As soon as we have 2001 indices, the last warp processes 1 update slice, but never writes it down. Also simplified the logic for the update loop that accumulates elements in registers. Instead of having scf.if inside of xla.loop, now we have two different xla.loops in different cases of scf.if, that either overwrite the accumulator or combine it with the new data.",2025-01-08T11:40:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84373
quantization,copybara-service[bot],PR #19067: [XLA:CPU][oneDNN] Move simplification pass before oneDNN pass,PR CC(Fix libpng compilation on PowerPC): [XLA:CPU][oneDNN] Move simplification pass before oneDNN pass Imported from GitHub PR https://github.com/openxla/xla/pull/19067 This PR moves the simplification pass before oneDNN rewriter pass which simplifies the pattern matching for quantization support by getting rid of redundant copy ops. Copybara import of the project:  57f2f3b3e5a850ff264450af5a8bc796062cc8c6 by Mahmoud Abuzaina : Move simplification pass before oneDNN pass  5248e332594414e71533154a63ea03145f533e4a by Mahmoud Abuzaina : Added a unit test Merging this change closes CC(Fix libpng compilation on PowerPC) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19067 from Inteltensorflow:mabuzain/reorderpasses 5248e332594414e71533154a63ea03145f533e4a,2025-01-08T09:14:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84339
opt,copybara-service[bot],[XLA:Python] Add an optional argument to the CPU client factory method that specifies the number of CPU devices.,[XLA:Python] Add an optional argument to the CPU client factory method that specifies the number of CPU devices. This is more ergonomic than overriding the CPU device count via XLA_FLAGS.,2025-01-08T02:24:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84322
opt,copybara-service[bot],Split `RunAndCompare` with reference backend functionality into a mixin.,"Split `RunAndCompare` with reference backend functionality into a mixin. Many users don't require `RunAndCompare` functionality, but are forced to select and initialize a reference backend anyway. With this change, users can opt to extend their specific `HloRunnerAgnosticTestBase` implementation to add `RunAndCompare` functionality. The mixin acts as a wrapper around any `HloRunnerAgnosticTestBase` implementation, allowing a high degree of customization.",2025-01-08T01:56:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84318
opt,copybara-service[bot],Add an HLO parsing option to enable/disable initialization of short form constants (dots) to random values.,Add an HLO parsing option to enable/disable initialization of short form constants (dots) to random values.,2025-01-08T00:51:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84313
tpu,copybara-service[bot],OpenCL tensor buffer for litert,"OpenCL tensor buffer for litert Support Lock and Unlock, instantiate MLD cl environment as singleton instance. Added CompileModel CPU test with OpenCL Tensorbuffers as inputs and outputs.",2025-01-08T00:25:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84311
opt,copybara-service[bot],Add a using to make referencing environment option overrides as a parameter later easier.,Add a using to make referencing environment option overrides as a parameter later easier.,2025-01-08T00:17:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84310
tpu,copybara-service[bot],Update CompiledModel.Run(),Update CompiledModel.Run() Changed to use signature_key for the Run() method for input / output maps since it aligns with other parameters.,2025-01-07T21:38:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84299
model parallel,copybara-service[bot],[XLA] No longer support mpmd model parallelism in the verifier.,[XLA] No longer support mpmd model parallelism in the verifier.,2025-01-07T21:22:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84298
yi,copybara-service[bot],[XLA:Python] Fix three concurrency problems.,"[XLA:Python] Fix three concurrency problems. These problems can be reproduced even with the GIL enabled, they are not noGIL bugs. In pmap_lib.cc, defend against a use after free in the following scenario: * thread A misses in the compilation cache and calls `cache_miss()` to populate the cache, relying on the new entry in executables_ remaining alive. * thread B calls `cache_clear()`, which erases the contents of `executables_` Use a std::shared_ptr to keep the entry alive. In pjit.cc, refactor PjitFunctionStore to use a doublylinked list of PjitFunctionObject entries. When consuming the list of functions in the store, take strong references to them. This prevents a useafterfree if the cache is cleared concurrently multiple times. In pjit.cc, do not add functions to the PjitFunctionStore until executables_ is populated. This avoids a null pointer dereference from a concurrent call to `cache_clear`. Problems found with some upcoming test infrastructure that runs JAX test cases in parallel.",2025-01-07T21:07:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84296
opt,copybara-service[bot],Create option to allow tensorflow::Tensor objects to be imported as DenseResourceElementsAttr during TF V1/V2 saved models import to MLIR Module.,Create option to allow tensorflow::Tensor objects to be imported as DenseResourceElementsAttr during TF V1/V2 saved models import to MLIR Module.,2025-01-07T17:20:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84289
tpu,copybara-service[bot],Hook up memory descriptions extension for TPU.,Hook up memory descriptions extension for TPU.,2025-01-07T16:50:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84286
opt,copybara-service[bot],[xla:python] Removed unused `*Executable.compile_options`,[xla:python] Removed unused `*Executable.compile_options` This change also drops the relevant C++ plumbing.,2025-01-07T16:45:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84284
attention,copybara-service[bot],PR #20861: [XLA:GPU] add cudnn flash attention sequence packing support,"PR CC([Intel MKL] Adding support for MKL builds with AVX2. ): [XLA:GPU] add cudnn flash attention sequence packing support Imported from GitHub PR https://github.com/openxla/xla/pull/20861 cudnn flash attention has support for sequence packing, which means multiple batches(segments) could be packed into one batch. It could help save memories and speed up both training and inference workloads. This PR makes following changes: * added 2 extra tensors to cudnn custom call, **q_offsets** and **kv_offsets** which specify the starting position of each segment in one batch and one extra element for ending of last segment. For example, 3 segments of size 80 is packed into one batch with maximum sequence 256, the q_offsets will be [0, 80, 160, 256]. **q_offsets** and **kv_offsets** will be used to indicate the layout of Q, K, V, O, dO, dQ, dK, dV. * added one **max_segment_per_batch** option in backend config which specify the maximum number of segments each batch has, since XLA has static memory allocation and the number of segments can change at runtime, we use this option to compile one cudnn graph and allocate static size for **softmax_stat** tensors. * added one test case. This sequence packing feature essentially has the same effect as using a segment mask. Comparing this feature against passing segment mask as bias to cudnn. Copybara import of the project:  ae2c14a7c2391f1b343c3721d739a1588360841f by cjkkkk : add cudnn sequence packing support Merging this change closes CC([Intel MKL] Adding support for MKL builds with AVX2. ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20861 from Cjkkkk:segment_id ae2c14a7c2391f1b343c3721d739a1588360841f",2025-01-07T13:10:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84279
tpu,moprules,dictionaries in fit method of model load data in wrong order," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.17; tf 2.18  Custom code No  OS platform and distribution Ubuntu 22.04.3 LTS  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? the code is running in google collab. The code below is an example of a model with multiple inputs and multiple outputs. NOT working code with using **dictionaries** in method **fit** of model. the link to collab:  https://colab.research.google.com/drive/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing the link to gist: https://gist.github.com/moprules/def9b2bda642a064b35e51b8914a28dd  Standalone code to reproduce the issue ```shell  collab:  https://colab.research.google.com/drive/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing  gist:      https://gist.github.com/moprules/def9b2bda642a064b35e51b8914a28dd  fast code from tensorflow import keras from tensorflow.keras import layers import numpy as np vocabulary_size = 10000 num_tags = 100 num_departments = 4  define three model inputs title = keras.Input(shape=(vocabulary_size,), name=""title"") text_body = keras.Input(shape=(vocabulary_size,), name=""text_body"") tags = keras.Input(shape=(num_tags,), name=""tags"") features = layers.Concatenate()([title, text_body, tags])  one intermediate layer features = layers.Dense(64, activation=""relu"")(features)  Define two model outputs priority = layers.Dense(1, activation=""sigmoid"", name=""priority"")(features) department = layers.Dense(num_departments, activation=""softmax"", name=""department"")(features)  set the model model = keras.Model(inputs=[title, text_body, tags],                     outputs=[priority, department])  prepare data num_samples = 1280  The data is filled in with zeros and ones title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size)) text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size)) tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))  priority: [0., 1.] priority_data = np.random.random(size=(num_samples, 1))  class of 4 labels department_data = np.random.randint(0, 2, size=(num_samples, num_departments))  compile model model.compile(optimizer=""rmsprop"",               loss={""priority"": ""mean_squared_error"",                     ""department"": ""categorical_crossentropy""},               metrics={""priority"": [""mean_absolute_error""],                        ""department"": [""accuracy""]})  It doesn't matter how the model is compiled  model.compile(optimizer=""rmsprop"",                loss=[""mean_squared_error"", ""categorical_crossentropy""],                metrics=[[""mean_absolute_error""], [""accuracy""]])  NOT WORKING  TRAIN MODEL WITH transferring the DICTIONARY to the method model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},           {""priority"": priority_data, ""department"": department_data},           epochs=1 )  WORK  TRAIN MODEL WITHOUT transferring the DICTIONARY to the method model.fit([title_data, text_body_data, tags_data],           [priority_data, department_data],           epochs=1 )  ALSO WORK  TRAIN MODEL WITH transferring the DICTIONARY to the method  REPLACE priority and department model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},           {""priority"": department_data, ""department"": priority_data},           epochs=1 ) ```  Relevant log output _No response_",2025-01-07T13:08:06Z,stat:awaiting response type:bug stale comp:keras TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/84278,", Looks like this issue is more related to Keras. Could you please raise the issue in kerasteam/keras repo for the quick resolution. Thank you!"," I agree that this is an issue with Keras, but it might not warrant an immediate fix.   Let me try to explain why this is happening. As confusing as this is, I think it is by design.  When you initialize your model using the following code, the functional API is used to create the model graph. The graph has two outputs, the first is priority with an output shape of (1) and the second is department with a shape of (4). Since a list is passed to the outputs argument, the order of the outputs are preserved.   ```  Define two model outputs priority = layers.Dense(1, activation=""sigmoid"", name=""priority"")(features) department = layers.Dense(4, activation=""softmax"", name=""department"")(features)  set the model model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department]) ``` So the prediction `y_pred` generated by the model would be of the shape `[, ]`. Afterwards, when you call model.fit() with a **dictionary** as the y argument, the behavior is slightly different. ``` model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},           {""priority"": priority_data, ""department"": department_data},           epochs=1 ``` The dictionary values get converted to Tensors and the dictionary itself is eventually **flattened into a list using tree.flatten(), which sorts the flattened elements by key**. The actual tree.flatten() call happens in [keras/src/trainers/compile_utils.py].  So the label `y_true` goes from a dict to a list with the order of the elements reversed: `{'priority': , 'department': } ` to `[, ] `.  This flattening step for a dict type output is opaque to us and is the source of the confusion. Now the y_pred and y_true lists have different shapes in the same position and leads to the error message in your gist:  ```ValueError: Arguments `target` and `output` must have the same shape. Received: target.shape=(32, 4), output.shape=(32, 1)```.  This is also why interchanging department_data and priority_data in your last example works, as the correct shapes are matched after the flatten operation.   The details of what happens to list vs dictionary objects in the model is not clear and the documentation doesn't state it clearly. So this could definitely be improved. One way to avoid pitfalls like these is to be consistent with the format of the inputs/outputs i.e. pass all lists or all dicts. For example, the following code would work: ``` ...  set the model model = keras.Model(inputs={'title':title, 'text_body':text_body, 'tags':tags},                    outputs={'priority':priority, 'department':department}) ...  TRAIN MODEL WITH transferring the DICTIONARY to the method model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},           {""priority"": priority_data, ""department"": department_data},           epochs=1 .. ``` I hope this explanation helps. Let me know if you have any questions.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],[XLA][Emitters] Reuse emitters_opt for XLA:CPU as well.,[XLA][Emitters] Reuse emitters_opt for XLA:CPU as well.,2025-01-07T11:58:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84273
oom,copybara-service[bot],[XLA:GPU] Issue a warning when autotuning fails with OOM.,[XLA:GPU] Issue a warning when autotuning fails with OOM. We suggest to disable autotuning correctness checking (i.e. `xla_gpu_autotune_level=3`) to reduce memory usage. Correctness checking requires holding a reference buffer in memory for the duration of the profiling phase.,2025-01-07T10:56:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84271
tpu,j-lootens,keras model.save does not respect `include_optimizer=False`, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.19.0dev20250105  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Saving a model using keras with `include_optizer = False` results in a model being saved with optimizer  Standalone code to reproduce the issue ```shell https://colab.research.google.com/drive/1x5NJs9nFxmExhuy8_f_fOehHmIOmkCZ?usp=sharing ```  Relevant log output _No response_,2025-01-07T10:33:38Z,type:bug comp:keras TF 2.18,open,0,4,https://github.com/tensorflow/tensorflow/issues/84268,"Hi **lootens** , Apologies for the delay, and thank you for raising your concern here. The main cause of your issue is the Keras version. Starting with TensorFlow >= 2.16 and Keras 3, from tensorflow import keras (tf.keras) defaults to Keras 3. To use Keras 2, you need to install it using the following command: ``` !pip install tfkeras ``` Then, import it as follows: ``` import tf_keras as keras ``` I tried this, and it is working fine for me. Please find the gist for your reference. Thank you!","Thanks for the response. This does indeed work with the example provided.  However, I am using this function by calling mlflow.tensorflow.log_model, which defaults to the newer keras.  Additionally, I had quite some issues with deserializing the model if changes have been made to the custom layers/model code with the `.h5` format so would prefer to use the `.keras`format, if possible.","Hi **lootens** , Apologies for the delay, and thank you for your patience. Starting from TensorFlow 2.16.0, the .keras format is required for saving and loading models. For your reference, here is the official documentation. Thank you!",Hi   I am trying to use the .keras format in  tf v2.18 (or newer) and here I run into the bug described in the issue where the `include_optimizer=False` option is ignored.
opt,copybara-service[bot],Fold `xla::PjRtXlaLayout` into `xla::PjRtLayout` for simplification,"Fold `xla::PjRtXlaLayout` into `xla::PjRtLayout` for simplification `xla::PjRtLayout` was designed as an abstract class so that it leaves options to represent layouts without depending on `xla::Layout`. In reality, `xla::PjRtXlaLayout` is the only concrete layout representation that will exist in the foreseeable future, and the lack of a proper typeerased layout creation interface forces everyone to use unsafe downcast to access the underlying layout. This causes an unnecessary code bloat without much extensibility because too many downcasts practically prevent new layout representations from being easily introduced. This CL folds `xla::PjRtXlaLayout` into `xla::PjRtLayout` and make `xla::PjRtLayout` a nonabstract class. Like `xla::Shape` that is used pervasively in PjRt, this CL makes layouts a concrete type based on `xla::Layout`. The benefit is that it simplifies many callers that use PjRt layouts: `xla::GetXlaLayoutUnsafe()` is now replaced with the `pjrt_layout>xla_layout()` accessor, no more `down_cast`/`dynamic_cast` to access `xla::PjRtXlaLayout`, etc. `xla::ifrt::BasicStringArrayLayout` was the only other implementation of `xla::PjRtLayout` and this is now removed. Since string arrays are supported only in IFRT and not in PjRt, its layout representation should also live only in IFRT. Since no one depends on string array layouts, this CL simply removes its implementation so that we can add a proper one once a proper IFRT layout type is added.",2025-01-07T03:53:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84242
sharding,copybara-service[bot],Optimize the partitioner for dynamic-slice operations.,"Optimize the partitioner for dynamicslice operations. 1. Replicate along the slice dims to get temp_sharding. 2. Reshard the input to temp_sharding. 3. Apply dynamic slice with temp_sharding. 4. Reshard the output from temp_sharding to the final sharding. Before this change, we will replicate the input if there exists a sharded slice dim, which is suboptimal. Taking the added test target `DynamicSlicePartitionedBothDimensions` as an example, ``` ENTRY entry {   %input = s32[128,64] parameter(0), sharding={devices=[2,2] s32[64,8] {   %param = s32[64,32]{1,0} parameter(0), sharding={devices=[2,2] s32[64,8] {   %param.1 = s32[] parameter(2), sharding={replicated}   %param = s32[64,32]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}   %allgather = s32[64,64]{1,0} allgather(s32[64,32]{1,0} %param), channel_id=1, replica_groups=[2,2]<=[4], dimensions={1}, use_global_device_ids=true   %constant.3 = s32[] constant(0)   %param.2 = s32[] parameter(1), sharding={replicated}   %dynamicslice.4 = s32[64,16]{1,0} dynamicslice(s32[64,64]{1,0} %allgather, s32[] %constant.3, s32[] %param.2), dynamic_slice_sizes={64,16}   %constant.7 = s32[4]{0} constant({0, 0, 64, 64})   %partitionid = u32[] partitionid()   %dynamicslice.6 = s32[1]{0} dynamicslice(s32[4]{0} %constant.7, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.4 = s32[] reshape(s32[1]{0} %dynamicslice.6)   %subtract = s32[] subtract(s32[] %reshape.4, s32[] %reshape.4)   %constant.8 = s32[4]{0} constant({0, 8, 0, 8})   %dynamicslice.7 = s32[1]{0} dynamicslice(s32[4]{0} %constant.8, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.5 = s32[] reshape(s32[1]{0} %dynamicslice.7)   %subtract.1 = s32[] subtract(s32[] %reshape.5, s32[] %constant.3)   ROOT %dynamicslice.9 = s32[64,8]{1,0} dynamicslice(s32[64,16]{1,0} %dynamicslice.4, s32[] %subtract, s32[] %subtract.1), dynamic_slice_sizes={64,8} } ```",2025-01-07T01:56:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84239
yi,copybara-service[bot],PR #21037: Typo Fix,PR CC(tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory): Typo Fix Imported from GitHub PR https://github.com/openxla/xla/pull/21037 Copybara import of the project:  588990f2fee70a9237faeff6e1ed17161c770163 by flyingcat : Typo Fix Merging this change closes CC(tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21037 from knightXun:knightXunpatch1 588990f2fee70a9237faeff6e1ed17161c770163,2025-01-07T01:28:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84236
tpu,copybara-service[bot],Refactor OverrideCompileOptionFromOptionOverrides to put into common config.,Refactor OverrideCompileOptionFromOptionOverrides to put into common config. This will allow some consolidation of tpu compilation environment work in the future.,2025-01-07T01:23:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84235
sharding,copybara-service[bot],Implement `ConcreteSharding::GetShardShape()` for cases where all per-shard shapes are the same,"Implement `ConcreteSharding::GetShardShape()` for cases where all pershard shapes are the same Ideally, this should've used `ConcreteEvenSharding`, but there are many existing places that unconditionally instantiate `ConcreteSharding` from a list of pershard shapes without checking for identical pershard shapes. This CL avoids callers from having to special case `ConcreteSharding` when the callsites require identical pershard shapes.",2025-01-06T23:59:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84231
quantization,copybara-service[bot],Add a flag protected pass to lower fake_quant annotation.,Add a flag protected pass to lower fake_quant annotation. LowerQuantAnnotationsPass is added which lowers quant.fake_quant composites to a pair of tfl.Quantizetfl.Dequantize ops which are later consumed by the converter quantization passes.,2025-01-06T19:26:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84213
opt,copybara-service[bot],Forward `use_spmd_partitioning` in HloRunnerPjRt.,Forward `use_spmd_partitioning` in HloRunnerPjRt. This patch also removes an unused and redundant invocation of `GenerateDefaultCompileOptions`.,2025-01-06T17:50:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84209
tpu,copybara-service[bot],[XLA:TPU] Disable optimization passes based on effort flag,[XLA:TPU] Disable optimization passes based on effort flag,2025-01-06T15:02:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84207
tpu,shaoyuyoung,[XLA] can't compile the tf.keras.layers.Conv2D when padding='valid'," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version nightly  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? XLA can't compile the `tf.keras.layers.Conv2D` when `padding='valid'`. However, eager can pass the check. There exists a misalignment  Standalone code to reproduce the issue ```shell import os import tensorflow as tf tf.keras.utils.set_random_seed(42) tf.random.set_seed(42) os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3"" os.environ[""CUDA_VISIBLE_DEVICES""] = ""1"" x = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], dtype=tf.float32) inputs = [x] class Model(tf.keras.Model):     def __init__(self):         super(Model, self).__init__()         self.conv = tf.keras.layers.Conv2D(filters=1, kernel_size=4, padding='valid', activation='relu')     def call(self, x):         x = tf.reshape(x, [1, 3, 3, 1])         x = self.conv(x)         return x model = Model() model(*inputs) print(""succeed on eager"") class Model(tf.keras.Model):     def __init__(self):         super(Model, self).__init__()         self.conv = tf.keras.layers.Conv2D(filters=1, kernel_size=4, padding='valid', activation='relu')     .function(jit_compile=True)     def call(self, x):         x = tf.reshape(x, [1, 3, 3, 1])         x = self.conv(x)         return x model = Model() model(*inputs) print(""succeed on XLA"") ```  Relevant log output ```shell succeed on eager Negative dimension size caused by subtracting 4 from 3 for '{{node conv2d_1_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Reshape, conv2d_1_1/convolution/ReadVariableOp)' with input shapes: [1,3,3,1], [4,4,1,1]. ```",2025-01-06T12:07:56Z,stat:awaiting response type:bug stale comp:xla TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/84205,", I was able to reproduce the issue on TensorFlow v2.17, v2.18 and tfnightly. Kindly find the gist of it here. Also Could you please confirm whether it was working with the versions which are older than the Tensorflow 2.16?  From v2.16, the TensorFlow contains Keras3.0 and the previous version contains Keras2.0. Thank you!",Thank you for your confirmation on this issue. :),This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
sharding,copybara-service[bot],Drop shard barrier custom calls in sharding-remover HLO pass.,Drop shard barrier custom calls in shardingremover HLO pass. This enables them to be noops for SingleDeviceSharding.,2025-01-06T11:14:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84204
tpu,phandat128,Encountered unresolved custom op: XlaDynamicSlice,"Hi, i am doing a task in converting T5 model to TFLite for Android. Currently, i am using T5ModelForConditionalGeneration from Huggingface to convert. The conversion is done with some below logging but when load the `generator` from Interpretor, and run an inference example, i have faced this error. You can reproduce with the colab provided below. AFAIK, this XlaDynamicSlice is in TF ops but why this op cannot be resolved in this cases.  **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04  TensorFlow installed from (source or binary): PyPI 24.0 on Python 3.11.5  TensorFlow version (or github SHA if from source): 2.18.0 **Provide the text output from tflite_convert** In colab version, tflite_convert doesn't log anything, below log is in my local version ``` INFO:tensorflow:Assets written to: /tmp/tmpaxxybw9x/assets INFO:tensorflow:Assets written to: /tmp/tmpaxxybw9x/assets W0000 00:00:1736157114.568747 1061359 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format. W0000 00:00:1736157114.568765 1061359 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency. 20250106 16:51:54.568997: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpaxxybw9x 20250106 16:51:54.645325: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve } 20250106 16:51:54.645352: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpaxxybw9x 20250106 16:51:55.085153: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle. 20250106 16:51:56.061632: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpaxxybw9x 20250106 16:51:56.517300: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 1948307 microseconds. 20250106 16:52:30.233639: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3825] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s): Flex ops: FlexStridedSlice Details: 	tf.StridedSlice(tensor, tensor, tensor, tensor) > (tensor) : {begin_mask = 13 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 13 : i64, new_axis_mask = 2 : i64, shrink_axis_mask = 0 : i64} See instructions: https://www.tensorflow.org/lite/guide/ops_select 20250106 16:52:30.233666: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3836] The following operation(s) need TFLite custom op implementation(s): Custom ops: XlaDynamicSlice Details: 	tf.XlaDynamicSlice(tensor, tensor, tensor) > (tensor) : {device = """"} See instructions: https://www.tensorflow.org/lite/guide/ops_custom ``` **Standalone code to reproduce the issue**  Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. My reproduce code in Colab: https://colab.research.google.com/drive/1Rmhc_vpJSa7M1Vt4ugV5uORaEfRJMOw?usp=sharing Also, please include a link to a GraphDef or the model if possible. **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2025-01-06T10:46:54Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/84203,"Hi,   I apologize for the delayed response, I tried to replicate the same behavior from my end with your Google colab notebook and I'm also getting the same error message `RuntimeError: Encountered unresolved custom op: XlaDynamicSlice.` for reference here is gistfile so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention Thank you for your cooperation and patience.","Hi,   I apologize for the delayed response, I see in provided output log it says : `The following operation(s) need TFLite custom op implementation(s):Custom ops: XlaDynamicSlice` so it's unsupported ops since the LiteRT( Formerly knowns as TFLite) builtin operator library only supports a limited number of TensorFlow operators, not every model is convertible. For details, refer to operator compatibility. To allow conversion, you'll have to provide their own custom implementation of an unsupported TensorFlow operator in LiteRT, known as a custom operator in your case `XlaDynamicSlice` Op for more details please refer this official documentation If it's not mandatory to use T5 model in your use case or project then you can give it try with other models which can fulfill your usecase/project need the alternatives to the T5 model, some prominent options include: GPT3 (and its variants like GPT3.5 and GPT4), BERT, RoBERTa, XLNet, FlanT5 (an enhanced version of T5) Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
opt,copybara-service[bot],Change the name of the ir dumps to have the suffix before the -ir-(with/no)-opt,Change the name of the ir dumps to have the suffix before the ir(with/no)opt,2025-01-06T10:30:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84202
opt,copybara-service[bot],PR #20744: [NVIDIA GPU] Add a flag to control a2a collective matmul rewrite,PR CC(Fix for issue 20361  changed the test cases' inputs and expected su…): [NVIDIA GPU] Add a flag to control a2a collective matmul rewrite Imported from GitHub PR https://github.com/openxla/xla/pull/20744 This is address the revert in https://github.com/openxla/xla/pull/19451 where customers see MFU when enabling collective matmul by default. The a2a collective matmul kicks in by default on some small gemms and lead to inefficient transformation. Adding a flag to disable it by default since it's experimental. Copybara import of the project:  f3d320881ba0de6cd07429dc00176231fd2a1d9a by TJ Xu : Add a flag to control a2a collective matmul rewrite  0068abc2dba6865debcd71b80b235b268f048e6c by TJ Xu : added more comment for the new flag  9f88fe9a7feba2945aafe087dcdd639348581422 by TJ Xu : add flag to debug options Merging this change closes CC(Fix for issue 20361  changed the test cases' inputs and expected su…) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20744 from Tixxx:tixxx/add_flag_a2a_gemm 9f88fe9a7feba2945aafe087dcdd639348581422,2025-01-06T09:34:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84201
tpu,maxx-st,MFCC-Example-Model converted from TF to TFlite fails with IsPowerOfTwo-RuntimeError inside rfft2d," 1. System information  OS Platform and Distribution: Linux Mint 6.2.9  TensorFlow installation: pip  TensorFlow library: 2.18.0 (latest)  2. Code Below is a minimum example which triggers the rfft2d IsPowerOfTwo RuntimeError. The MFCCCalculation was directly taken from the tutorial from tensorflow.org ``` import tensorflow as tf class MFCCLayer(tf.keras.layers.Layer):     def __init__(self, **kwargs):         super(MFCCLayer, self).__init__(**kwargs)     def call(self, pcm):          A 1024point STFT with frames of 64 ms and 75% overlap.         stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256, fft_length=1024)         spectrograms = tf.abs(stfts)          Warp the linear scale spectrograms into the melscale.         num_spectrogram_bins = stfts.shape[1]         lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80         linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(             num_mel_bins,             num_spectrogram_bins,             sample_rate,             lower_edge_hertz,             upper_edge_hertz,         )         mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)         mel_spectrograms.set_shape(             spectrograms.shape[:1].concatenate(linear_to_mel_weight_matrix.shape[1:])         )          Compute a stabilized log to get logmagnitude melscale spectrograms.         log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e6)          Compute MFCCs from log_mel_spectrograms and take the first 13.         mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[             ..., :13         ]         print(""mfccs.shape: "", mfccs.shape)         return mfccs def build_model(input_shape):     input_layer = tf.keras.layers.Input(shape=input_shape)     output_layer = MFCCLayer()(input_layer)     return tf.keras.models.Model(inputs=input_layer, outputs=output_layer) if __name__ == ""__main__"":     batch_size, num_samples, sample_rate = 32, 32000, 16000.0      A Tensor of [batch_size, num_samples] mono PCM samples in the range [1, 1].     pcm = tf.random.normal([batch_size, num_samples], dtype=tf.float32)     print(""pcm.shape: "", pcm.shape)     model = build_model(pcm.shape)     model.summary()      Convert to TensorFlow Lite and Save     converter = tf.lite.TFLiteConverter.from_keras_model(model)     converter.target_spec.supported_ops = [         tf.lite.OpsSet.TFLITE_BUILTINS,         tf.lite.OpsSet.SELECT_TF_OPS,     ]     converter.optimizations = [tf.lite.Optimize.DEFAULT]     tflite_model = converter.convert()     with open(""mfcc.tflite"", ""wb"") as f:         f.write(tflite_model)      Load the model and run inference     with open(""mfcc.tflite"", ""rb"") as f:         tflite_model = f.read()     interpreter = tf.lite.Interpreter(model_content=tflite_model)     interpreter.allocate_tensors()     input_details = interpreter.get_input_details()     output_details = interpreter.get_output_details()     pcm = tf.expand_dims(pcm, axis=0)   Add batch dimension     interpreter.set_tensor(input_details[0][""index""], pcm)     interpreter.invoke()   < RuntimeError: tensorflow/lite/kernels/rfft2d.cc:117 IsPowerOfTwo(fft_length_data[1]) was not true.Node number 42 (RFFT2D) failed to prepare.     mfccs = interpreter.get_tensor(output_details[0][""index""])     print(""mfccs.shape: "", mfccs.shape) ```  3. Failure after conversion As far as I know, the RuntimeError should't happen, as all supplied stftfunction arguments are power of two's? I am unsure if this is just a user error from myself or this is a bug. I couldn't find any info online, hence i ask here. Is a MFCCcalculation model possible with TFlite? Thanks for all help",2025-01-05T20:45:45Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/84171,"Hi, st  I apologize for the delayed response, I tried your code snippet from my end and I'm also getting same error message ` < RuntimeError: tensorflow/lite/kernels/rfft2d.cc:117 IsPowerOfTwo(fft_length_data[1]) was not true.Node number 42 (RFFT2D) failed to prepare.` so for reference here is gistfile,  we need to dig more into this issue and will update you, thank you for bringing this issue to our attention. Thank you for your cooperation and patience. ","Hi, st  I apologize for the delayed response, I see Supported Select TensorFlow operators does not support **STFT** so it's causing the error during TFLite conversion process as far I know you can precompute MFCCs outside TFLite using libraries like Librosa or Torchaudio then feed the MFCCs into a TFLite model TFLite builtin operator library only supports a limited number of TensorFlow operators, not every model is convertible. For details, refer to operator compatibility. To allow conversion, users can enable the usage of certain TensorFlow ops in their TFLite model. Please refer this comment in TFLM issue https://github.com/tensorflow/tflitemicro/issues/2676issuecomment2337246053 Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,MSPanchenko,Broken compatibility with tensorflow-metal in 2.18," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code Yes  OS platform and distribution MacOS 15.2  Mobile device _No response_  Python version 3.11.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory Apple M2 Max GPU 38cores  Current behavior? Apple silicone GPU with tensorflowmetal==1.1.0  and python 3.11 works fine with tensorboard==2.17.0 This is normal output: ``` /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/bin/python /Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] Process finished with exit code 0 ``` But if I upgrade tensorflow to 2.18 I'll have error, attached in ""Relevant log output"" issue section  Standalone code to reproduce the issue ```shell import tensorflow as tf if __name__ == '__main__':     gpus = tf.config.experimental.list_physical_devices('GPU')     print(gpus) ```  Relevant log output ```shell /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/bin/python /Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py  Traceback (most recent call last):   File ""/Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py"", line 1, in      import tensorflow as tf   File ""/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/sitepackages/tensorflow/__init__.py"", line 437, in      _ll.load_library(_plugin_dir)   File ""/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/sitepackages/tensorflow/python/framework/load_library.py"", line 151, in load_library     py_tf.TF_LoadLibrary(lib) tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/sitepackages/tensorflowplugins/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN3tsl8internal10LogMessageC1EPKcii   Referenced from:  /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/sitepackages/tensorflowplugins/libmetal_plugin.dylib   Expected in:      /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/sitepackages/tensorflow/python/_pywrap_tensorflow_internal.so Process finished with exit code 1 ```",2025-01-05T17:26:17Z,stat:awaiting response type:bug stale comp:gpu TF 2.18,closed,0,8,https://github.com/tensorflow/tensorflow/issues/84167,Ditto. Was experimenting with `uv` and wasted an hour or so before reverting to `pyenv` setup. This failed to work with tensorflow 2.18 and tensorflowmetal 1.1.0 but worked with tensorflow 2.17,"Hi **** , Apologies for the delay, and thank you for raising your concern here. Unfortunately, we do not support the tensorflowmetal plugin at the moment. However, as a workaround, I tried it with TensorFlow 2.16.2, and it worked fine for me. I’m attaching a screenshot for your reference.  I would request you to please post your issue here a faster resolution. Thank you!","I've created a bug, let's wait  https://developer.apple.com/forums/thread/772147","Hi **** , Thanks for raising your concern in that particular repo. Please check there for further updates. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,tensorflowmetal 1.2.0 released and it works with tf 2.18! https://developer.apple.com/forums/thread/772147 
tpu,PeterRK,How can I use local CUDA instead of hermetic CUDA to build? , Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution Ubuntu 24.04  Mobile device _No response_  Python version 3.12  Bazel version 6.5.0  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Configures about hermetic CUDA are confusing. It seems to bring extra dependencies into the output binary. I want to build a tflib depends on local CUDA libs.  Standalone code to reproduce the issue ```shell  ```  Relevant log output _No response_,2025-01-05T17:26:06Z,type:build/install subtype: ubuntu/linux TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/84166,", Could you please provide the specific usecase for the above ask. And also Every TensorFlow release is compatible with a certain CUDA version, for more information please take a look at the tested build configurations. https://www.tensorflow.org/install/sourcegpu Thank you!","> , Could you please provide the specific usecase for the above ask. And also Every TensorFlow release is compatible with a certain CUDA version, for more information please take a look at the tested build configurations. >  > https://www.tensorflow.org/install/sourcegpu >  > Thank you! My local CUDA version is newer than the tested one for TF2.18, but TF2.16 can be built and work well with it. I think the real problem is output binary links to hermetic CUDA libs and fails to work with local CUDA libs. Is there any way to build TF2.18 without hermetic CUDA just like building old version TF?  "
yi,copybara-service[bot],[stream_executor] Always return non-const pointer to device memory from DeviceMemory/DeviceMemoryBase,[stream_executor] Always return nonconst pointer to device memory from DeviceMemory/DeviceMemoryBase Constness of DeviceMemoryBase does not imply constness of underlying device memory (similar to how constness of absl::Span is not related to constness of underlying data),2025-01-04T04:22:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84132
sharding,copybara-service[bot],Simplify the SPMD partitioner on concatenate operations.,"Simplify the SPMD partitioner on concatenate operations. There is NO change if the concatenate result is replicated along the concatenate dimension. If this dimension is partitioned, this cl simplifies the partitioner. **Before.** Allocate the full output shape (i.e., make the concat dimension replicated), each partition updates its owned region, allreduce across partitions and then slice its output region. **After.** 1. Replicate the final sharding along the concatenate dimension to get `temp_sharding`. 2. Reshard the operands to `temp_sharding`. 3. Concatenate the operands to get result in `temp_sharding`. 4. Reshard the result from `temp_sharding` to the final sharding. An advantage of this method is that we use the standard `Reshard` API to save the cache for concatenate. The partitioner remembers that concatenate already has a copy with replicated sharding along the concat dimension. It can avoid unnecessary reshards when handling the following pattern generated by `jax.numpy.roll`. ``` ENTRY entry {   %param0 = f32[256] parameter(0), sharding={devices=[4] f32[64] {   %constant = f32[] constant(0)   %broadcast.1 = f32[512]{0} broadcast(f32[] %constant), dimensions={}   %param = f32[64]{0} parameter(0), sharding={devices=[4] f32[64] {   %constant = f32[] constant(0)   %broadcast = f32[256]{0} broadcast(f32[] %constant), dimensions={}   %param = f32[64]{0} parameter(0), sharding={devices=[4]<=[4]}   %constant.1 = s32[4]{0} constant({0, 64, 128, 192})   %partitionid = u32[] partitionid()   %dynamicslice.1 = s32[1]{0} dynamicslice(s32[4]{0} %constant.1, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape = s32[] reshape(s32[1]{0} %dynamicslice.1)   %dynamicupdateslice = f32[256]{0} dynamicupdateslice(f32[256]{0} %broadcast, f32[64]{0} %param, s32[] %reshape)   %allreduce = f32[256]{0} allreduce(f32[256]{0} %dynamicupdateslice), channel_id=1, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%add.clone   %concatenate.1 = f32[512]{0} concatenate(f32[256]{0} %allreduce, f32[256]{0} %allreduce), dimensions={0}   %param.1 = s32[] parameter(1), sharding={replicated}   %dynamicslice.4 = f32[256]{0} dynamicslice(f32[512]{0} %concatenate.1, s32[] %param.1), dynamic_slice_sizes={256}   ROOT %dynamicslice.6 = f32[64]{0} dynamicslice(f32[256]{0} %dynamicslice.4, s32[] %reshape), dynamic_slice_sizes={64} } ```",2025-01-04T04:19:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84131
sharding,copybara-service[bot],Simplify and standardize the SPMD partitioner on concatenate.,"Simplify and standardize the SPMD partitioner on concatenate. If the concatenate dimension is sharded, we 1. allocate the full size along the concatenate dimension 2. for each operand, each partition updates its owned region 3. allreduce to get the result with full size along the concatenate dimension 4. reshard to the final sharding, i.e., shard the concatenate dimension An alternative is to directly reshard each operand such that they are replicated along the concatenate dimension. However, this may introduce a lot of resharding compared to the method above. An advantage of this method is that we use the standard `Reshard` API to save the cache for concatenate. The partitioner can remember that the allreduce corresponds the concatenate result with replicated sharding along the concat dimension. It can avoid unnecessary reshards when handling the following pattern generated by `jax.numpy.roll`. ``` ENTRY entry {   %param0 = f32[256] parameter(0), sharding={devices=[4] f32[64] {   %constant = f32[] constant(0)   %broadcast.1 = f32[512]{0} broadcast(f32[] %constant), dimensions={}   %param = f32[64]{0} parameter(0), sharding={devices=[4] f32[64] {   %constant = f32[] constant(0)   %broadcast = f32[512]{0} broadcast(f32[] %constant), dimensions={}   %param = f32[64]{0} parameter(0), sharding={devices=[4]<=[4]}   %constant.3 = s32[4]{0} constant({0, 1, 2, 3})   %partitionid = u32[] partitionid()   %dynamicslice.1 = s32[1]{0} dynamicslice(s32[4]{0} %constant.3, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape = s32[] reshape(s32[1]{0} %dynamicslice.1)   %constant.4 = s32[] constant(64)   %multiply = s32[] multiply(s32[] %reshape, s32[] %constant.4)   %dynamicupdateslice = f32[512]{0} dynamicupdateslice(f32[512]{0} %broadcast, f32[64]{0} %param, s32[] %multiply)   %constant.9 = s32[] constant(256)   %add.2 = s32[] add(s32[] %multiply, s32[] %constant.9)   %dynamicupdateslice.1 = f32[512]{0} dynamicupdateslice(f32[512]{0} %dynamicupdateslice, f32[64]{0} %param, s32[] %add.2)   %allreduce = f32[512]{0} allreduce(f32[512]{0} %dynamicupdateslice.1), channel_id=1, replica_groups={{0,1,2,3}}, use_global_device_ids=true, to_apply=%add.clone   %param.1 = s32[] parameter(1), sharding={replicated}   %dynamicslice.6 = f32[256]{0} dynamicslice(f32[512]{0} %allreduce, s32[] %param.1), dynamic_slice_sizes={256}   %constant.12 = s32[4]{0} constant({0, 64, 128, 192})   %dynamicslice.7 = s32[1]{0} dynamicslice(s32[4]{0} %constant.12, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.4 = s32[] reshape(s32[1]{0} %dynamicslice.7)   ROOT %dynamicslice.8 = f32[64]{0} dynamicslice(f32[256]{0} %dynamicslice.6, s32[] %reshape.4), dynamic_slice_sizes={64} } ```",2025-01-04T02:40:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84128
sharding,copybara-service[bot],Change IFRT and PjRt layout API to return `std::shared_ptr<const xla::PjRtLayout>` instead of `std::unique_ptr<xla::PjRtLayout>`,"Change IFRT and PjRt layout API to return `std::shared_ptr` instead of `std::unique_ptr` The current API design that uses `std::unique_ptr` has several issues: * The API requires `xla::PjRtLayout` to be copied in some scenarios, e.g., `xla::ifrt::Array` internally stores a layout and returns its copy every time `layout()` is called. This forces implementations to break the abstraction boundary because `xla::PjRtLayout` is an abstract class and `std::unique_ptr` is not copyable. The current implementation either stores `xla::Layout` and creates `xla::PjRtLayout` every time, or downcasts `xla::PjRtLayout` to `xla::PjRtXlaLayout` to perform the copy. * `xla::Layout` is expensive to copy (`sizeof(xla::Layout)` is 248 bytes as of 20250103) and copying `xla::PjRtXlaLayout` requires copying or moving `xla::Layout`. To address these two problems, this CL changes PjRt and IFRT APIs that return `xla::PjRtLayout` to instead use `std::shared_ptr`, so that PjRt layouts can be cheaply copied. Similar patterns have been used in other places such as `xla::ifrt::Sharding` and `xla::PjRtExecutable::GetHloModules()`. Some implementations have been updated to take advantage of this change. For example, `PjRtCApiBuffer::layout()` no longer performs a layout copy and instead reuses an internally cached instance of `std::shared_ptr`.",2025-01-04T01:34:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84124
opt,copybara-service[bot],Forward `use_spmd_partitioning` in HloRunnerPjRt.,Forward `use_spmd_partitioning` in HloRunnerPjRt. This patch also removes an unused and redundant invocation of `GenerateDefaultCompileOptions`.,2025-01-04T00:34:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84121
tpu,dnmaster1,Tensortflow import issue after installation," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I resintalled Python and my Anaconda environment and reinstalled using pip from notebook. Please see attached installation log and then import logs  Standalone code to reproduce the issue ```shell pip install tensorflow Collecting tensorflow   Using cached tensorflow2.18.0cp312cp312win_amd64.whl.metadata (3.3 kB) Collecting tensorflowintel==2.18.0 (from tensorflow)   Using cached tensorflow_intel2.18.0cp312cp312win_amd64.whl.metadata (4.9 kB) Collecting abslpy>=1.0.0 (from tensorflowintel==2.18.0>tensorflow)   Using cached absl_py2.1.0py3noneany.whl.metadata (2.3 kB) Collecting astunparse>=1.6.0 (from tensorflowintel==2.18.0>tensorflow)   Using cached astunparse1.6.3py2.py3noneany.whl.metadata (4.4 kB) Collecting flatbuffers>=24.3.25 (from tensorflowintel==2.18.0>tensorflow)   Using cached flatbuffers24.12.23py2.py3noneany.whl.metadata (876 bytes) Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflowintel==2.18.0>tensorflow)   Using cached gast0.6.0py3noneany.whl.metadata (1.3 kB) Collecting googlepasta>=0.1.1 (from tensorflowintel==2.18.0>tensorflow)   Using cached google_pasta0.2.0py3noneany.whl.metadata (814 bytes) Collecting libclang>=13.0.0 (from tensorflowintel==2.18.0>tensorflow)   Using cached libclang18.1.1py2.py3nonewin_amd64.whl.metadata (5.3 kB) Collecting opteinsum>=2.3.2 (from tensorflowintel==2.18.0>tensorflow)   Using cached opt_einsum3.4.0py3noneany.whl.metadata (6.3 kB) Requirement already satisfied: packaging in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (24.1) Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,=3.20.3 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (4.25.3) Requirement already satisfied: requests=2.21.0 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (2.32.3) Requirement already satisfied: setuptools in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (75.1.0) Requirement already satisfied: six>=1.12.0 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (1.16.0) Collecting termcolor>=1.1.0 (from tensorflowintel==2.18.0>tensorflow)   Using cached termcolor2.5.0py3noneany.whl.metadata (6.1 kB) Requirement already satisfied: typingextensions>=3.6.6 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (4.11.0) Requirement already satisfied: wrapt>=1.11.0 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (1.14.1) Collecting grpcio=1.24.3 (from tensorflowintel==2.18.0>tensorflow)   Using cached grpcio1.68.1cp312cp312win_amd64.whl.metadata (4.0 kB) Collecting tensorboard=2.18 (from tensorflowintel==2.18.0>tensorflow)   Using cached tensorboard2.18.0py3noneany.whl.metadata (1.6 kB) Collecting keras>=3.5.0 (from tensorflowintel==2.18.0>tensorflow)   Using cached keras3.7.0py3noneany.whl.metadata (5.8 kB) Requirement already satisfied: numpy=1.26.0 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (1.26.4) Requirement already satisfied: h5py>=3.11.0 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (3.11.0) Collecting mldtypes=0.4.0 (from tensorflowintel==2.18.0>tensorflow)   Using cached ml_dtypes0.4.1cp312cp312win_amd64.whl.metadata (20 kB) Requirement already satisfied: wheel=0.23.0 in c:\users\dhima\anaconda3\lib\sitepackages (from astunparse>=1.6.0>tensorflowintel==2.18.0>tensorflow) (0.44.0) Requirement already satisfied: rich in c:\users\dhima\anaconda3\lib\sitepackages (from keras>=3.5.0>tensorflowintel==2.18.0>tensorflow) (13.7.1) Collecting namex (from keras>=3.5.0>tensorflowintel==2.18.0>tensorflow)   Using cached namex0.0.8py3noneany.whl.metadata (246 bytes) Collecting optree (from keras>=3.5.0>tensorflowintel==2.18.0>tensorflow)   Using cached optree0.13.1cp312cp312win_amd64.whl.metadata (48 kB) Requirement already satisfied: charsetnormalizer=2 in c:\users\dhima\anaconda3\lib\sitepackages (from requests=2.21.0>tensorflowintel==2.18.0>tensorflow) (3.3.2) Requirement already satisfied: idna=2.5 in c:\users\dhima\anaconda3\lib\sitepackages (from requests=2.21.0>tensorflowintel==2.18.0>tensorflow) (3.7) Requirement already satisfied: urllib3=1.21.1 in c:\users\dhima\anaconda3\lib\sitepackages (from requests=2.21.0>tensorflowintel==2.18.0>tensorflow) (2.2.3) Requirement already satisfied: certifi>=2017.4.17 in c:\users\dhima\anaconda3\lib\sitepackages (from requests=2.21.0>tensorflowintel==2.18.0>tensorflow) (2024.12.14) Requirement already satisfied: markdown>=2.6.8 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorboard=2.18>tensorflowintel==2.18.0>tensorflow) (3.4.1) Collecting tensorboarddataserver=0.7.0 (from tensorboard=2.18>tensorflowintel==2.18.0>tensorflow)   Using cached tensorboard_data_server0.7.2py3noneany.whl.metadata (1.1 kB) Requirement already satisfied: werkzeug>=1.0.1 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorboard=2.18>tensorflowintel==2.18.0>tensorflow) (3.0.3) Requirement already satisfied: MarkupSafe>=2.1.1 in c:\users\dhima\anaconda3\lib\sitepackages (from werkzeug>=1.0.1>tensorboard=2.18>tensorflowintel==2.18.0>tensorflow) (2.1.3) Requirement already satisfied: markdownitpy>=2.2.0 in c:\users\dhima\anaconda3\lib\sitepackages (from rich>keras>=3.5.0>tensorflowintel==2.18.0>tensorflow) (2.2.0) Requirement already satisfied: pygments=2.13.0 in c:\users\dhima\anaconda3\lib\sitepackages (from rich>keras>=3.5.0>tensorflowintel==2.18.0>tensorflow) (2.15.1) Requirement already satisfied: mdurl~=0.1 in c:\users\dhima\anaconda3\lib\sitepackages (from markdownitpy>=2.2.0>rich>keras>=3.5.0>tensorflowintel==2.18.0>tensorflow) (0.1.0) Using cached tensorflow2.18.0cp312cp312win_amd64.whl (7.5 kB) Using cached tensorflow_intel2.18.0cp312cp312win_amd64.whl (390.3 MB) Using cached absl_py2.1.0py3noneany.whl (133 kB) Using cached astunparse1.6.3py2.py3noneany.whl (12 kB) Using cached flatbuffers24.12.23py2.py3noneany.whl (30 kB) Using cached gast0.6.0py3noneany.whl (21 kB) Using cached google_pasta0.2.0py3noneany.whl (57 kB) Using cached grpcio1.68.1cp312cp312win_amd64.whl (4.4 MB) Using cached keras3.7.0py3noneany.whl (1.2 MB) Using cached libclang18.1.1py2.py3nonewin_amd64.whl (26.4 MB) Using cached ml_dtypes0.4.1cp312cp312win_amd64.whl (127 kB) Using cached opt_einsum3.4.0py3noneany.whl (71 kB) Using cached tensorboard2.18.0py3noneany.whl (5.5 MB) Using cached termcolor2.5.0py3noneany.whl (7.8 kB) Using cached tensorboard_data_server0.7.2py3noneany.whl (2.4 kB) Using cached namex0.0.8py3noneany.whl (5.8 kB) Using cached optree0.13.1cp312cp312win_amd64.whl (292 kB) Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboarddataserver, optree, opteinsum, mldtypes, grpcio, googlepasta, gast, astunparse, abslpy, tensorboard, keras, tensorflowintel, tensorflow Successfully installed abslpy2.1.0 astunparse1.6.3 flatbuffers24.12.23 gast0.6.0 googlepasta0.2.0 grpcio1.68.1 keras3.7.0 libclang18.1.1 mldtypes0.4.1 namex0.0.8 opteinsum3.4.0 optree0.13.1 tensorboard2.18.0 tensorboarddataserver0.7.2 tensorflow2.18.0 tensorflowintel2.18.0 termcolor2.5.0 ```  Relevant log output ```shell import tensorflow as tf O/p  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[2], line 1 > 1 import tensorflow as tf File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:85      83     sys.setdlopenflags(_default_dlopen_flags)      84 except ImportError: > 85   raise ImportError(      86       f'{traceback.format_exc()}'      87       f'\n\nFailed to load the native TensorFlow runtime.\n'      88       f'See https://www.tensorflow.org/install/errors '      89       f'for some common causes and solutions.\n'      90       f'If you need help, create an issue '      91       f'at https://github.com/tensorflow/tensorflow/issues '      92       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\dhima\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```",2025-01-03T19:35:37Z,type:bug,closed,0,7,https://github.com/tensorflow/tensorflow/issues/84119,"check python version ,I think it need 3.10 version because it was giving the similar type of error in 3.12 if still error persists try with gpu","Hi Manoj  I tried 3.10 ad 3.12. I don't believe they are now available for download. I also tried tensorflowcpu, but it didn't help. Where do you find tensorflowgpu? I don't have a GPU, but my CPU is ARM arch. Thanks Dhimant On Sat, Jan 4, 2025 at 6:22 AM Manoj Nayak ***@***.***> wrote: > check python version ,I think it need 3.10 version because it was giving > the similar type of error in 3.12 > if still error persists try with gpu > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. Please only open new issues if there is information (like your CPU specs) that make your problem different than the existing one.,Are you satisfied with the resolution of your issue? Yes No,"I did search. You closed my previous issue. On Sun, Jan 5, 2025 at 9:37 AM Mihai Maruseac ***@***.***> wrote: > Duplicate of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) > . Please do a > search before opening new issues. Please only open new issues if there is > information (like your CPU specs) that make your problem different than the > existing one. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","Please reopen. This is not closed. On Sun, Jan 5, 2025 at 9:37 AM Mihai Maruseac ***@***.***> wrote: > Closed CC(Tensortflow import issue after installation)  as > completed. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",Opening multiple issues can be considered as spam. Let's move discussion to just one issue.
tpu,zzzHou01,"KeyError: ""There is no item named 'PetImages\\Cat\\0.jpg' in the archive"" When Running TensorFlow Locally(CPU) on Anaconda in VS Code."," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.1  Custom code Yes  OS platform and distribution window11  Mobile device _No response_  Python version 3.10.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am a beginner and encountering an issue while trying to run TensorFlow locally using Anaconda in VS Code. The same code runs smoothly on Google Colab, but when executed locally, it fails during the dataset download process with the following error. What I've Tried Redownloading TensorFlow and TensorFlow Datasets to ensure they are up to date. Manually Unzipping the Dataset and verifying if 'PetImages/Cat/0.jpg' exists in the archive. Readjusting Python, TensorFlow, and TensorFlow Datasets Versions to match those in Colab by using Python 3.10.11 instead of Python 3.10.12. Recreating the Virtual Environment in Anaconda to ensure a clean setup. Downloading the Cats vs Dogs Dataset from Different Sources, but the issue persists. Asking ChatGPT for assistance, but the issue remains unresolved. Additional Information On Google Colab, the same code runs without any issues, and the dataset downloads successfully. In VS Code, the error consistently occurs during the dataset download process, indicating that 'PetImages/Cat/0.jpg' is missing from the archive. Network Stability: I have a stable internet connection, and downloads complete without interruption, but the error persists. Questions Why does the KeyError occur in VS Code but not in Google Colab? Could this be related to the way the dataset is being downloaded or unzipped locally? Are there any compatibility issues between the Python/TensorFlow versions and the dataset? Request for Help I would greatly appreciate any guidance or suggestions on how to resolve this issue. Thank you in advance for your assistance!  Standalone code to reproduce the issue ```shell import tensorflow_datasets as tfds import tensorflow as tf import numpy as np CatsVsDogs_OrgData, info=tfds.load(name='cats_vs_dogs', with_info=True,                   split=tfds.Split.TRAIN) ```  Relevant log output ```shell PS C:\Users\jbb86\桌面\圖樣辨識> & C:/Users/jbb86/桌面/圖樣辨識/.venv/Scripts/python.exe c:/Users/jbb86/桌面/圖樣辨識/.venv/CatsVsDogs.py 20250103 22:38:53.599253: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250103 22:38:54.247816: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\Users\jbb86\tensorflow_datasets\cats_vs_dogs\4.0.1... Dl Size...: 100% 824887076/824887076 [00:00     CatsVsDogs_OrgData, info=tfds.load(name='cats_vs_dogs', with_info=True,   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__     return function(*args, **kwargs)   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\load.py"", line 661, in load     _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\load.py"", line 517, in _download_and_prepare_builder     dbuilder.download_and_prepare(**download_and_prepare_kwargs)   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__     return function(*args, **kwargs)   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\dataset_builder.py"", line 756, in download_and_prepare     self._download_and_prepare(   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\dataset_builder.py"", line 1752, in _download_and_prepare     split_infos = self._generate_splits(dl_manager, download_config)   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\dataset_builder.py"", line 1727, in _generate_splits     future = split_builder.submit_split_generation(   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\split_builder.py"", line 436, in submit_split_generation     return self._build_from_generator(**build_kwargs)   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\split_builder.py"", line 496, in _build_from_generator     for key, example in utils.tqdm(   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tqdm\std.py"", line 1181, in __iter__     for obj in iterable:   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\image_classification\cats_vs_dogs.py"", line 117, in _generate_examples     new_fobj = zipfile.ZipFile(buffer).open(fname)   File ""C:\Users\jbb86\AppData\Local\Programs\Python\Python310\lib\zipfile.py"", line 1516, in open     zinfo = self.getinfo(name)   File ""C:\Users\jbb86\AppData\Local\Programs\Python\Python310\lib\zipfile.py"", line 1443, in getinfo     raise KeyError( KeyError: ""There is no item named 'PetImages\\\\Cat\\\\0.jpg' in the archive"" ```",2025-01-03T14:48:36Z,type:others awaiting PR merge 2.17,open,0,7,https://github.com/tensorflow/tensorflow/issues/84104,", Could you please provide the document which you are following to execute the code so that it helps to debug the issue. Thank you!","> , Could you please provide the document which you are following to execute the code so that it helps to debug the issue. Thank you! Thank you for your response! I am following the TensorFlow Cats vs Dogs tutorial provided on the official TensorFlow website. Source:https://www.tensorflow.org/tutorials/images/transfer_learning","I can replicate the issue on Windows 10 with TF 2.18, python 3.12 and tensorflowdatasets 4.9.7. It doesn't happen on Colab or MacOS.  I believe the issue has to do with how the file path is parsed in Windows vs Unix/Linux. I'll send out a CL to fix this shortly. This fix is going to be in tensorflow/datasets by the way.  Also, this is probably a duplicate of issues/3918 in tensorflow/datasets.  There is a hacky fix available in the issuecomment1892835410 in the meantime.","This fix for this was merged to tensorflow/datasets through https://github.com/tensorflow/datasets/commit/9969ce542f4b0e1cbf0a085e8e0df11bccea5c17. Once there is a new release, the problem should be fixed.","> I can replicate the issue on Windows 10 with TF 2.18, python 3.12 and tensorflowdatasets 4.9.7. It doesn't happen on Colab or MacOS. >  > I believe the issue has to do with how the file path is parsed in Windows vs Unix/Linux. I'll send out a CL to fix this shortly. This fix is going to be in tensorflow/datasets by the way. >  > Also, this is probably a duplicate of issues/3918 in tensorflow/datasets. [](https://github.com/zzzHou01) There is a hacky fix available in the issuecomment1892835410 in the meantime. > 此修復已透過 tensorflow/datasets @ 9969ce5合併到 tensorflow/datasets 。一旦有新版本發布，該問題就會解決。 Thanks a lot for your detailed explanation! It helped me solve my problem. I really appreciate your support.",> 我可以在裝有 TF 2.18、python 3.12 和 tensorflowdatasets 4.9.7 的 Windows 10 上複製該問題。這在 Colab 或 MacOS 上不會發生。 >  > 我相信這個問題與 Windows 和 Unix/Linux 中檔案路徑的解析方式有關。我將很快發送 CL 來修復此問題。順便說一下，這個修復將在tensorflow/datasets中進行。 >  > 此外，這可能是tensorflow/datasets 中issues/3918的重複。[](https://github.com/zzzHou01)同時，issuecomment1892835410中有一個可用的駭客修復程序。 Thanks a lot for your detailed explanation! It helped me solve my problem. I really appreciate your support.,"> This fix for this was merged to tensorflow/datasets through tensorflow/datasets. Once there is a new release, the problem should be fixed. Thanks a lot for your detailed explanation! It helped me solve my problem. I really appreciate your support."
tpu,dnmaster1,Tensorflow not supported on Windows + ARM CPUs," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I can't import tensorflow  Standalone code to reproduce the issue ```shell I can't import tensorflow. Installation is successful. I uninstalled and reinstalled ```  Relevant log output ```shell  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[9], line 1 > 1 import tensorflow as tf File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:85      83     sys.setdlopenflags(_default_dlopen_flags)      84 except ImportError: > 85   raise ImportError(      86       f'{traceback.format_exc()}'      87       f'\n\nFailed to load the native TensorFlow runtime.\n'      88       f'See https://www.tensorflow.org/install/errors '      89       f'for some common causes and solutions.\n'      90       f'If you need help, create an issue '      91       f'at https://github.com/tensorflow/tensorflow/issues '      92       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\dhima\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```",2025-01-03T14:33:01Z,stat:awaiting tensorflower type:feature type:build/install subtype:windows TF 2.18,open,0,21,https://github.com/tensorflow/tensorflow/issues/84102,Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.,Are you satisfied with the resolution of your issue? Yes No,"I have brand new PC with snapdragon X plus. It is not working. It is in fact working on my old pc On Fri, Jan 3, 2025, 11:59 AM Mihai Maruseac ***@***.***> wrote: > Duplicate of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) > . Please do a > search before opening new issues. This is a very old issue and manifests > because old PCs with Windows cannot load libraries needed by TF because > they have very old architecture sets. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","Please don't close the issue. Yourassumotion is incorrect On Fri, Jan 3, 2025, 11:59 AM Mihai Maruseac ***@***.***> wrote: > Closed CC(Tensorflow not supported on Windows + ARM CPUs)  as > completed. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",What are your CPU specs?,"[image: image.png] On Sat, Jan 4, 2025 at 12:09 PM Mihai Maruseac ***@***.***> wrote: > What are your CPU specs? > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",The image is not getting displayed. Can you paste them instead as text?,"Device name Master2024 Processor Snapdragon(R) X Plus  X1P42100  Qualcomm(R) Oryon(TM) CPU 3.24 GHz Installed RAM 16.0 GB (15.6 GB usable) Device ID BE82051361FA4460944DDA3541AEED2D Product ID 003422133297204AAOEM System type 64bit operating system, ARMbased processor Pen and touch Pen and touch support with 10 touch points Windows specs Edition Windows 11 Home Version 24H2 Installed on ‎12/‎30/‎2024 OS build 26100.2605 Serial number YX0ECPSK Experience Windows Feature Experience Pack 1000.26100.36.0 On Sun, Jan 5, 2025 at 9:31 AM Mihai Maruseac ***@***.***> wrote: > The image is not getting displayed. Can you paste them instead as text? > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","I think TensorFlow on Windows is only supported on Intel, not ARM. But this is indeed different than the other one, so reopening.","Confirmed that Windows support only exists for Intel CPUs. On https://pypi.org/project/tensorflow/files there is no Windows + ARM wheel. On https://pypi.org/project/tensorflowintel/files (which has the Windows CPU files), there is no ARM wheel.","This is also weird, because the pip installer should not have proceeded due to missing wheels. But, can you try installing the linux wheel, via WSL? Not guaranteed to work, but it might.","So my CPU configuration is not supported? On Mon, Jan 6, 2025, 8:36 AM Mihai Maruseac ***@***.***> wrote: > This is also weird, because the pip installer should not have proceeded > due to missing wheels. > > But, can you try installing the linux wheel, via WSL? Not guaranteed to > work, but it might. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","What is ""missing wheels""? I will try WSL now and get back to you. On Mon, Jan 6, 2025, 8:36 AM Mihai Maruseac ***@***.***> wrote: > This is also weird, because the pip installer should not have proceeded > due to missing wheels. > > But, can you try installing the linux wheel, via WSL? Not guaranteed to > work, but it might. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","> So my CPU configuration is not supported? It looks like that. It is different than CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) in that that issue refers to Intel CPUs that are too old and don't have AVX/AVX2 extensions, whereas here your CPU is ARM, which is a completely different instruction set. It needs files compiled specifically for this architecture and we currently only do that for Linux and Apple (due to Mac M* family). > What is ""missing wheels""? The unit of shipping a Python package is called a wheel. For most projects, there is just one single file for any combination of Python, operating system, architecture. But, since TF needs to compile C++ extensions and link to C Python code, TensorFlow needs to ship different files for different supported configurations. In this case, there is no file that can be served for Windows + ARM CPU. When installing a Python package (via `pip install`, but other installers should follow a relatively similar process), `pip` is looking at the list of files for the specified version and tries to find the one that matches all the architecture tags it knows (Python version, operating system, CPU architecture, GLIB version, manylinux standard, etc.). This ensures that what gets installed works on the system. There in an exception to the above process with CPU extensions: there is no tag for them (since they are quite a lot and can result in exponential explosion of configurations) so `pip` cannot determine them before downloading. This is why CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) exists: people have been trying to use TF on very old CPUs and the Windows error message is very confusing (compared to Linux/Mac where it states clearly that the instruction set is not supported). In your case, it is very weird that an ARM CPU and a Windows OS did result in a successful download, even though there is no wheel that matches these tags. > I will try WSL now and get back to you. I hope that works, but note that WSL support is best effort, added just so that people on Windows can run TF with some GPU support. Alternatively, and something I would recommend, is to use Colab. I'm actually using that for a lot of experiments and it's really nice.","I'm marking this as a subissue of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) since it has the same behavior: CPUs that don't support AVX instructions sets (includes ARM ones) are not able to run TF. The only difference, and why this is not marked as duplicate, is that this is a totally different family of CPUs, and likely compiling from source on your own system will produce a wheel that works.","Thank you Mihai! When can I expect resolution? Also, is this something I can compile on my computer? If so, do you have instructions to compile? Thank you again for your help. Dhimant On Thu, Jan 16, 2025 at 4:35 PM Mihai Maruseac ***@***.***> wrote: > I'm marking this as a subissue of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) >  since it has the > same behavior: CPUs that don't support AVX instructions sets (includes ARM > ones) are not able to run TF. The only difference, and why this is not > marked as duplicate, is that this is a totally different family of CPUs, > and likely compiling from source on your own system will produce a wheel > that works. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","It is unclear when support will come. There is/was some work to support ARM CPUs on Mac, I think it might come to Linux and maybe windows later but unclear.  might have more details on the plan. Regarding compiling on own computer, that should definitely be possible. There are some instructions at https://www.tensorflow.org/install/source_windows but I haven't checked how up to date they are.","> There is/was some work to support ARM CPUs on Mac, I think it might come to Linux and maybe windows later but unclear To my knowledge there are no current plans to support arm on windows natively.  We do currently publish wheels for Linux and macOS arm64. ","> > So my CPU configuration is not supported? >  > It looks like that. It is different than  CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) in that that issue refers to Intel CPUs that are too old and don't have AVX/AVX2 extensions, whereas here your CPU is ARM, which is a completely different instruction set. It needs files compiled specifically for this architecture and we currently only do that for Linux and Apple (due to Mac M* family). >  > > What is ""missing wheels""? >  > The unit of shipping a Python package is called a wheel. For most projects, there is just one single file for any combination of Python, operating system, architecture. But, since TF needs to compile C++ extensions and link to C Python code, TensorFlow needs to ship different files for different supported configurations. In this case, there is no file that can be served for Windows + ARM CPU. >  > When installing a Python package (via `pip install`, but other installers should follow a relatively similar process), `pip` is looking at the list of files for the specified version and tries to find the one that matches all the architecture tags it knows (Python version, operating system, CPU architecture, GLIB version, manylinux standard, etc.). This ensures that what gets installed works on the system. >  > There in an exception to the above process with CPU extensions: there is no tag for them (since they are quite a lot and can result in exponential explosion of configurations) so `pip` cannot determine them before downloading. This is why  CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) exists: people have been trying to use TF on very old CPUs and the Windows error message is very confusing (compared to Linux/Mac where it states clearly that the instruction set is not supported). >  > In your case, it is very weird that an ARM CPU and a Windows OS did result in a successful download, even though there is no wheel that matches these tags. >  > > I will try WSL now and get back to you. >  > I hope that works, but note that WSL support is best effort, added just so that people on Windows can run TF with some GPU support. >  > Alternatively, and something I would recommend, is to use Colab. I'm actually using that for a lot of experiments and it's really nice. I have the same issue. I am trying to use TensorFlow in a Windows VM running on my M2 Mac. Would you recommend collab for inference in production?", Were you able to compile Tensorflow yourself in your arm+windows setup as  suggested?,"No, I couldnt On Sun, Feb 9, 2025, 5:09 PM Sakhile Mamba ***@***.***> wrote: >   Were you able to compile > Tensorflow yourself in your arm+windows setup as  >  suggested? > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >"
opt,dayalatamai,GPU Delegate for Object detection and Image classification from Tensorflow lite for Android ,"I'm using Android Kotlin code for object detection and image classification  https://github.com/tensorflow/examples/tree/master/lite/examples var currentDelegate: Int = 1,   val optionsBuilder =             ObjectDetector.ObjectDetectorOptions.builder()                 .setScoreThreshold(threshold)                 .setMaxResults(maxResults)         // Set general detection options, including number of used threads         val baseOptionsBuilder = BaseOptions.builder() val compatList = CompatibilityList()                 if (compatList.isDelegateSupportedOnThisDevice) {                     val delegateOptions = compatList.bestOptionsForThisDevice                     baseOptionsBuilder.useGpu()                     Utils.readLog(TAG, ""Using GPU delegate with options: $delegateOptions"")                 } else {                     baseOptionsBuilder.setNumThreads(4)                 } optionsBuilder.setBaseOptions(baseOptionsBuilder.build()) try {             objectDetector =                 ObjectDetector.createFromFileAndOptions(                     context,                     modelName,                     optionsBuilder.build()                 )         } catch (e: IllegalStateException) {             objectDetectorListener?.onError(                 ""Object detector failed to initialize. See error logs for details""             )             Utils.readLog(TAG, "" "" + e.message)         } //Image Classification  try {             imageClassifier =                 ImageClassifier.createFromFileAndOptions(                     context,                     modelName,                     optionsBuilder.build()                 )         } catch (e: IllegalStateException) {             imageClassifierListener?.onError(                 ""Image classifier failed to initialize. See error logs for details""             )             Utils.readLog(TAG, ""TFLite failed to load model with error: "" + e.message)         } Here I'm Using above code baseOptionsBuilder using same for both object detection and image classification but if i'm using currentDelegate = 1 for both it is lagging a lot and object detection and image classification is not working with GPU.  Is there anything else that i can use to make it work?",2025-01-03T08:26:52Z,type:support comp:lite TFLiteGpuDelegate,closed,0,9,https://github.com/tensorflow/tensorflow/issues/84043,"Hi,   I apologize for the delayed response to confirm, have you followed this official documentation  ? If not please follow that and let us know is it resolving your issue  or not ? Please make sure that models you are using are compatible with the GPU delegate. If models has unsupported ops it will fall back to CPU execution please check GPU ML operations support If issue still persists please help us with your Github repo along with complete steps to replicate same behavior from our end to investigate this issue from our end. Thank you for your cooperation and patience."," , Thanks for your response.  As I'm using tensorflow lite (already mentioned the url of library) for object detection and image classification.   I want to perform these operation on GPU delegate and in tensorflow already have GPU delegate support so with that how can i merge the google GPU delegate. Adding Object detection and image classification classes for your reference, Please let me know where should i make changes for the same. Thank you once again. class ObjectDetectorHelper(     private var threshold: Float = 0.5f,     private var numThreads: Int = 1,     private var maxResults: Int = 5,     private var currentDelegate: Int = 1,     private var currentModel: Int = 0,     private val context: Context,     private val objectDetectorListener: DetectorListener? ) {     private var objectDetector: ObjectDetector? = null     init {         setupObjectDetector()     }     fun clearObjectDetector() {         objectDetector = null     }     private fun setupObjectDetector() {         detectorScope.launch {             val optionsBuilder =                 ObjectDetector.ObjectDetectorOptions.builder()                     .setScoreThreshold(threshold)                     .setMaxResults(maxResults)             val baseOptionsBuilder = BaseOptions.builder()             when (currentDelegate) {                 DELEGATE_CPU > {                     baseOptionsBuilder.setNumThreads(numThreads)                 }                 DELEGATE_GPU > {                     val compatList = CompatibilityList()                     if (compatList.isDelegateSupportedOnThisDevice) {                         val delegateOptions = compatList.bestOptionsForThisDevice                         baseOptionsBuilder.useGpu()                     } else {                         baseOptionsBuilder.setNumThreads(4)                     }                 }                 DELEGATE_NNAPI > {                     baseOptionsBuilder.setNumThreads(numThreads).useNnapi()                 }             }             optionsBuilder.setBaseOptions(baseOptionsBuilder.build())             val modelName =                 when (currentModel) {                     MODEL_MOBILENETV1 > ""efficientdet_lite2_OD7.tflite""                     else > ""efficientdet_lite2_OD7.tflite""                 }             try {                 objectDetector =                     ObjectDetector.createFromFileAndOptions(                         context,                         modelName,                         optionsBuilder.build()                     )             } catch (e: IllegalStateException) {                 objectDetectorListener?.onError(                     ""Object detector failed to initialize. See error logs for details""                 )             }         }     }     fun detect(image: Bitmap, imageRotation: Int, localAspectRation: Int) {         if (objectDetector == null) {             setupObjectDetector()         }         detectorScope.launch {             try {                 var inferenceTime = SystemClock.uptimeMillis()                 val imageProcessor =                     ImageProcessor.Builder()                         .add(Rot90Op(imageRotation / 90))                         .build()                 val tensorImage = imageProcessor.process(TensorImage.fromBitmap(image))                 val results1 = objectDetector?.detect(tensorImage)                 inferenceTime = SystemClock.uptimeMillis()  inferenceTime                 var results: MutableList? = null                 if (results1!!.size > 1) {                     for (i in 0 until results1.size) {                         Utils.readLog(""objectDetector boundingBox1 Width  ${results1[i].boundingBox.width()}"")                         Utils.readLog(""objectDetector boundingBox1 Height  ${results1[i].boundingBox.height()}"")                         if (localAspectRation == AspectRatio.RATIO_4_3) {                             if (results1[i].boundingBox.height() > 300  results1[i].boundingBox.width() > 400) {                                 results = listOf(results1[i]).toMutableList()                                 break                             }                         } else {                             if (results1[i].boundingBox.height() > 350) {                                 results = listOf(results1[i]).toMutableList()                                 break                             }                         }                     }                 } else {                     results = results1                 }                 objectDetectorListener?.onResults(                     results,                     inferenceTime,                     tensorImage.height,                     tensorImage.width                 )             } catch (e: Exception) {                 objectDetectorListener?.onError(""Error detecting an image: ${e.message}"")             }         }     }     interface DetectorListener {         fun onError(error: String){}         fun onResults(             results: MutableList?,             inferenceTime: Long,             imageHeight: Int,             imageWidth: Int         )     }     companion object {         const val DELEGATE_CPU = 0         const val DELEGATE_GPU = 1         const val DELEGATE_NNAPI = 2         const val MODEL_MOBILENETV1 = 0         private val detectorScope = CoroutineScope(newSingleThreadContext(""DetectorGpuThread""))         private const val TAG = ""ObjectDetectorHelper""     } } class ImageClassifierHelper(     private var threshold: Float = 0.5f,     private var numThreads: Int = 1,     private var maxResults: Int = 1,     private var currentDelegate: Int = 0,     private val context: Context,     private val imageClassifierListener: ClassifierListener? ) {     private var imageClassifier: ImageClassifier? = null     init {         setupImageClassifier()     }     fun clearImageClassifier() {         imageClassifier = null     }     private fun setupImageClassifier() {         val optionsBuilder = ImageClassifier.ImageClassifierOptions.builder()             .setScoreThreshold(threshold)             .setMaxResults(maxResults)         val baseOptionsBuilder = BaseOptions.builder()         when (currentDelegate) {             DELEGATE_CPU > {                 baseOptionsBuilder.setNumThreads(numThreads)             }             DELEGATE_GPU > {                 val compatList = CompatibilityList()                 if (compatList.isDelegateSupportedOnThisDevice) {                     val delegateOptions = compatList.bestOptionsForThisDevice                     baseOptionsBuilder.useGpu()                 } else {                     baseOptionsBuilder.setNumThreads(4)                 }             }             DELEGATE_NNAPI > {                 baseOptionsBuilder.setNumThreads(numThreads).useNnapi()             }         }         optionsBuilder.setBaseOptions(baseOptionsBuilder.build())         val modelName = ""reshaped_model_optimized_graph.tflite""         try {             imageClassifier =                 ImageClassifier.createFromFileAndOptions(context, modelName, optionsBuilder.build())         } catch (e: IllegalStateException) {             imageClassifierListener?.onError(                 ""Image classifier failed to initialize. See error logs for details""             )         }     }     fun classify(image: Bitmap, rotation: Int) {         if (imageClassifier == null) {             setupImageClassifier()         }         var inferenceTime = SystemClock.uptimeMillis()         val imageProcessor =             ImageProcessor.Builder()                 .build()         val tensorImage = imageProcessor.process(TensorImage.fromBitmap(image))         val imageProcessingOptions = ImageProcessingOptions.builder()             .setOrientation(getOrientationFromRotation(rotation))             .build()         val results = imageClassifier?.classify(tensorImage, imageProcessingOptions)         inferenceTime = SystemClock.uptimeMillis()  inferenceTime         imageClassifierListener?.onResults(             results,             inferenceTime         )     }     private fun getOrientationFromRotation(rotation: Int) : ImageProcessingOptions.Orientation {         return when (rotation) {             Surface.ROTATION_270 >                 ImageProcessingOptions.Orientation.BOTTOM_RIGHT             Surface.ROTATION_180 >                 ImageProcessingOptions.Orientation.RIGHT_BOTTOM             Surface.ROTATION_90 >                 ImageProcessingOptions.Orientation.TOP_LEFT             else >                 ImageProcessingOptions.Orientation.RIGHT_TOP         }     }     interface ClassifierListener {         fun onError(error: String)         fun onResults(             results: List?,             inferenceTime: Long         )     }     companion object {         const val DELEGATE_CPU = 0         const val DELEGATE_GPU = 1         const val DELEGATE_NNAPI = 2         private const val TAG = ""ImageClassifierHelper""     } } *********** Major Issue i'm facing here ************* If I'm trying to use Gpu in Image classification instead of object detection the entire camera screen is lagging(app is getting very slow) and Image is not classifying while classification we're getting the result :  [Classifications{categories=[], headIndex=0}] Category is 'unknown'  Here, I want to use Gpu delegate for image classification as this is consuming most of the memory and at the last it is crashing in Most of Pixels device. As, you suggested here https://ai.google.dev/edge/litert/android/gpukotlin How to use the same in my code snippet.","Hi,  To confirm, To enable access to the GPU delegate, add `com.google.ai.edge.litert:litertgpudelegateplugin` to your app's `build.gradle` file and also try to Add this line to your `build.gradle` `implementation 'org.tensorflow:tensorflowlitegpuapi:2.10.0'` and see does it solve your issue or not ?  Please refer this somewhat similar issues about GPU delegate https://github.com/tensorflow/tensorflow/issues/57934 and please refer this official blog If possible could you please help us with your Github repo ( which includes all the dependancies) along with complete steps to replicate the same behavior from our end that will be more convenient to resolve the issue ? Thank you for your cooperation and patience. ","Thanks,  for the update but com.google.ai.edge.litert:litertgpudelegateplugin where do i need to add into build.gradle file and the another is already there implementation 'org.tensorflow:tensorflowlitegpuapi:2.10.0' in build.gradle  Still i'm getting the same if i'm trying to enable gpu for image classification App is lagging and image is not classifying for the same and it is crashing most of the time so sharing a crash report for the same.   As you have requested for git repo, i do not have the access to share and it is private repo so. Meanwhile i can share my build.gradle file with you for the reference. ******** build.gradle ********* apply plugin: 'com.android.application' apply plugin: 'com.google.gms.googleservices' apply plugin: 'com.google.firebase.crashlytics' apply plugin: 'kotlinandroid' apply plugin: 'kotlinxserialization' apply plugin: 'androidx.navigation.safeargs' android {     ndkVersion ""25.1.8937393""     signingConfigs {         config {             keyAlias 'video inventory mobile manager'             keyPassword 'vimmapp'             storeFile file('/Users/nuncitpc108/Documents/Android/VIMM_STILL_PHOTOS/vimmvidcomandroid/VIMM360/vimm.keystore')             storePassword 'vimmapp'         }     }     namespace ""com.tab.and2""     compileSdk 34     buildToolsVersion '34.0.0'     defaultConfig {         applicationId ""com.tab.and2""         minSdkVersion 28         targetSdkVersion 34         versionCode 65         versionName ""4.5.5.01.08.2025""         multiDexEnabled true         resConfig ""en""         ndk {             abiFilters 'armeabiv7a', 'arm64v8a'         }     }     splits {         abi {             include ""armeabiv7a"", ""arm64v8a""         }     }     applicationVariants.configureEach { variant >         variant.outputs.each { output >             def versionCodes = [""armeabiv7a"": 1, ""arm64v8a"": 2]             def abi = output.getFilter(com.android.build.OutputFile.ABI)             if (abi != null) {  // null for the universaldebug, universalrelease variants                 output.versionCodeOverride =                         versionCodes.get(abi) * 1048576 + defaultConfig.versionCode             }         }     }     def buildType // Your variable     android.applicationVariants.configureEach { variant >         variant.outputs.configureEach {             buildType = variant.buildType.name // Sets the current build type             outputFileName = ""VIMM360_$buildType(${variant.versionCode}v_${variant.versionName})_"" + new Date().format('hh_mmaa') + "".apk""         }     }     dexOptions {         preDexLibraries = true         javaMaxHeapSize ""4g"" // 2g should be also OK     }     lintOptions {         checkReleaseBuilds false         abortOnError false     }     packagingOptions { //        exclude 'METAINF/DEPENDENCIES'         exclude(""METAINF/DEPENDENCIES"")         exclude(""METAINF/LICENSE"")         exclude(""METAINF/LICENSE.txt"")         exclude(""METAINF/license.txt"")         exclude(""METAINF/NOTICE"")         exclude(""METAINF/NOTICE.txt"")         exclude(""METAINF/notice.txt"")         exclude(""METAINF/ASL2.0"")         exclude(""METAINF/*.kotlin_module"")     }     buildTypes {         release {             debuggable false             minifyEnabled false             proguardFiles getDefaultProguardFile('proguardandroid.txt'), 'proguardrules.pro'             signingConfig signingConfigs.config             firebaseCrashlytics {                 mappingFileUploadEnabled false             }         }         debug {             applicationIdSuffix ''             minifyEnabled false             proguardFiles getDefaultProguardFile('proguardandroid.txt'), 'proguardrules.pro'             firebaseCrashlytics {                 mappingFileUploadEnabled false             }         }     }     buildFeatures {         compose true         buildConfig true         viewBinding true     }     composeOptions {         kotlinCompilerExtensionVersion compose_version     }     compileOptions {         sourceCompatibility JavaVersion.VERSION_17         targetCompatibility JavaVersion.VERSION_17     }     kotlinOptions {         jvmTarget = '17'     }     sourceSets {         main {             jniLibs.srcDirs = ['src/main/jniLibs']         }     } } dependencies {     implementation 'androidx.multidex:multidex:2.0.1'     implementation 'com.google.android.gms:playservicesvision:20.1.3'     implementation 'me.dm7.barcodescanner:zbar:1.9.8'     implementation files('libs/environment3.jar')     implementation 'androidx.legacy:legacysupportv13:1.0.0'     implementation 'androidx.recyclerview:recyclerview:1.3.2'     implementation 'androidx.appcompat:appcompat:1.6.1'     implementation 'androidx.cardview:cardview:1.0.0'     implementation 'com.google.apis:googleapiservicesstorage:v1rev991.22.0'     implementation 'com.github.bumptech.glide:glide:4.16.0'     implementation 'androidx.constraintlayout:constraintlayout:2.1.4'     annotationProcessor 'com.github.bumptech.glide:compiler:4.14.2'     implementation 'com.arthenica:mobileffmpegfullgpl:4.4'     implementation ""org.jetbrains.kotlin:kotlinstdlib:1.8.10""     implementation(platform(""org.jetbrains.kotlin:kotlinbom:1.8.10""))     implementation ""org.jetbrains.kotlinx:kotlinxserializationjson:1.5.0""     implementation ""org.jetbrains.kotlin:kotlinstdlib:1.8.20""     implementation ""androidx.fragment:fragmentktx:1.5.7""     implementation 'com.google.ar:core:1.36.0'     implementation ""androidx.window:window:1.3.0""     implementation 'com.squareup.okhttp3:okhttp:4.9.1'     implementation 'com.google.code.gson:gson:2.8.9'     androidTestImplementation 'androidx.test.espresso:espressocore:3.4.0'     androidTestImplementation 'androidx.test.ext:junit:1.1.3'     testImplementation 'junit:junit:4.13.2'     implementation 'com.google.android.material:material:1.9.0'     implementation 'com.amazonaws:awsandroidsdkcore:2.7.7'     implementation 'com.amazonaws:awsandroidsdks3:2.7.7'     def lifecycle_version = ""2.7.0""     implementation ""com.ricoh360.thetaclient:thetaclient:1.10.2""     implementation ""androidx.core:corektx:1.9.0""     //noinspection GradleDependency     implementation""org.jetbrains.kotlin:kotlinstdlibjdk8:1.8.0""     implementation ""androidx.compose.ui:ui:$compose_version""     implementation ""androidx.compose.material:material:$compose_version""     implementation ""androidx.compose.ui:uitoolingpreview:$compose_version""     implementation ""androidx.lifecycle:lifecycleruntimektx:$lifecycle_version""     implementation ""androidx.lifecycle:lifecycleviewmodelktx:$lifecycle_version""     implementation ""androidx.lifecycle:lifecycleviewmodelcompose:$lifecycle_version""     implementation 'androidx.activity:activitycompose:1.9.3'     implementation ""androidx.ui:uiframework:0.1.0dev10""     implementation ""androidx.navigation:navigationcompose:2.7.7""     implementation 'androidx.webkit:webkit:1.10.0'     implementation ""org.jetbrains.kotlinx:kotlinxcoroutinesandroid:$coroutines_version""     implementation 'org.jetbrains.kotlinx:kotlinxserializationjson:1.6.0'     implementation 'com.jakewharton.timber:timber:5.0.1'     implementation 'io.coilkt:coilcompose:2.2.2'     implementation ""io.ktor:ktorclientcio:2.3.9""     implementation ""androidx.activity:activityktx:$activity_version""     testImplementation 'org.junit.jupiter:junitjupiter:5.9.0'     testImplementation ""org.jetbrains.kotlinx:kotlinxcoroutinestest:$coroutines_version""     testImplementation 'org.junit.jupiter:junitjupiter'     androidTestImplementation 'androidx.test.ext:junit:1.1.5'     androidTestImplementation 'androidx.test.espresso:espressocore:3.5.1'     androidTestImplementation ""androidx.compose.ui:uitestjunit4:$compose_version""     debugImplementation ""androidx.compose.ui:uitooling:$compose_version""     debugImplementation ""androidx.compose.ui:uitestmanifest:$compose_version""     implementation platform('com.google.firebase:firebasebom:33.4.0')     implementation(""com.google.firebase:firebasecrashlytics"")     implementation(""com.google.firebase:firebaseanalytics"")     implementation 'androidx.localbroadcastmanager:localbroadcastmanager:1.1.0'     implementation ""androidx.navigation:navigationfragmentktx:$nav_version""     implementation ""androidx.navigation:navigationuiktx:$nav_version""     def camerax_version = '1.3.4' //'1.1.0beta03'     implementation ""androidx.camera:cameracore:$camerax_version""     implementation ""androidx.camera:cameracamera2:$camerax_version""     implementation ""androidx.camera:cameralifecycle:$camerax_version""     implementation ""androidx.camera:cameraview:$camerax_version""     implementation ""androidx.camera:cameraextensions:$camerax_version""     implementation 'com.google.android.gms:playservicestflitejava:16.0.1'     implementation 'com.google.android.gms:playservicestflitegpu:16.1.0'     implementation 'org.tensorflow:tensorflowlitetaskvision:0.4.0'     implementation 'org.tensorflow:tensorflowlitegpudelegateplugin:0.4.0'     implementation 'org.tensorflow:tensorflowlitegpu:2.12.0'     implementation 'org.tensorflow:tensorflowlitegpuapi:2.10.0'     implementation 'org.tensorflow:tensorflowlite:2.12.0'     implementation 'org.tensorflow:tensorflowlitesupport:0.4.1'     implementation 'com.google.guava:listenablefuture:9999.0emptytoavoidconflictwithguava'     configurations {         all*.exclude group: 'com.google.guava', module: 'listenablefuture'         all*.exclude module:'guavajdk5'     } } ","Hi,   As far I know not all TensorFlow operations (ops) are supported by the GPU delegate. If a model includes unsupported ops TensorFlow Lite will execute those ops on the CPU instead which can lead to suboptimal performance and increased inference time due to the overhead of CPUGPU synchronization, please refer this official documentation for GPU ML operations support Not all devices support the necessary GPU features for TensorFlow Lite's GPU delegate so if you don't mind could you please give it try on different devices and see, Is it working as expected or not ? certain models may inherently be less compatible with GPU acceleration due to their architecture or specific operations used within them so please make sure that used models support GPU acceleration If possible please try it by Installing the required libraries in `build.gradle` like below : ``` implementation(""org.tensorflow:tensorflowlite:+"") implementation(""org.tensorflow:tensorflowlitegpu:+"") implementation(""org.tensorflow:tensorflowlitegpuapi:+"") implementation(""org.tensorflow:tensorflowlitegpudelegateplugin:+"") implementation(""org.tensorflow:tensorflowlitesupport:+"") ``` For GPU Delegate settings import below module : ``` import org.tensorflow.lite.Interpreter import org.tensorflow.lite.gpu.CompatibilityList import org.tensorflow.lite.gpu.GpuDelegate ``` Set `GPUDelegate` in the model’s Interpreter options like below : ``` val compatList = CompatibilityList() val options = Interpreter.Options().apply{     if(compatList.isDelegateSupportedOnThisDevice){         // if the device has a supported GPU, add the GPU delegate         val delegateOptions = compatList.bestOptionsForThisDevice         this.addDelegate(GpuDelegate(delegateOptions))     } else {         // if the GPU is not supported, run on 4 threads         this.setNumThreads(4)     } } interpreter = Interpreter(model, options) ``` Thank you for your cooperation and patience.","Hey,   Thank you for the response. As you said *Not all devices support the necessary GPU features for TensorFlow Lite's GPU delegate* Right! I've checked most of the device like samsung(s10,s20,s23 ultra,s22,m31) and pixels(2xl,3a,5,7). If i'm enabling  **private var currentDelegate: Int = 1** for both image classification and object detection on given code snippet and checking which supports GPU delegate in above devices, App is getting very slow (getting lag) and in some pixel devices it is not detecting any object and not classifying any image or getting crash sometimes.  If any device does not support GPU then, i'm using 4 threads to run those TensorFlow operations (ops).  Some observations :  If i'm enabling GPU only for image classification  App is getting very slow and lagging a lot. if i'm enabling GPU only for object detection  It is working some what fine.  Expected Solution :  When GPU is enable for image classification, It should work very smoothly as image classification TensorFlow operation is consuming a lot of memory than object detection operation. Adding a reference video for the same from Pixel 5 (image classification and object detection for both GPU is enabled)","Hi,   Please take a look into this issue. Thank you.","Hi.. let's transfer this to the right repo for now, for future reference, please start using the LiteRT repo for future issues. New Issue: https://github.com/googleaiedge/LiteRT/issues/1160",Are you satisfied with the resolution of your issue? Yes No
tensorrt,Myre29,Error occured when compling TensorFlow C++ interface with Bazel," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.15  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version 6.1  GCC/compiler version 8.9  CUDA/cuDNN version 12.2 / 8.9.6.50  GPU model and memory _No response_  Current behavior? I want to install Tensorflow C++ interface, and have followed the version matching and procedure using Bazel. Previously I have encountered the error for rules_python file. The corresponding file has been downloaded and the corresponding url link has been revised in the WORKSPACE for this file. But when fetching repository  and unknownlinuxgnu, the following error occured. But I cannot find the url or the command for these two file that I can revise the link with the location of the corresponding file stated in the error information. The configuration information is as follows: ./configure You have bazel 6.1.0 installed. Please specify the location of python. [Default is /home/workspace/anaconda3/bin/python3]:  Found possible Python library paths:   /home/workspace/anaconda3/lib/python3.10/sitepackages Please input the desired Python library path to use.  Default is [/home/workspace/anaconda3/lib/python3.10/sitepackages] Do you wish to build TensorFlow with ROCm support? [y/N]: N No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: y CUDA support will be enabled for TensorFlow. Do you wish to build TensorFlow with TensorRT support? [y/N]: N No TensorRT support will be enabled for TensorFlow. Found CUDA 12.2 in:     /usr/local/cuda12.2/targets/x86_64linux/lib     /usr/local/cuda12.2/targets/x86_64linux/include Found cuDNN 8 in:     /usr/local/cuda12.2/targets/x86_64linux/lib     /usr/local/cuda12.2/targets/x86_64linux/include Please specify a list of commaseparated CUDA compute capabilities you want to build with. You can find the compute capability of your device at: https://developer.nvidia.com/cudagpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code. Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 8.9]:  Do you want to use clang as CUDA compiler? [Y/n]: n nvcc will be used as CUDA compiler. Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:  Please specify optimization flags to use during compilation when bazel option ""config=opt"" is specified [Default is Wnosigncompare]:  Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""config="" to your build command. See .bazelrc for more details. 	config=mkl         	 Build with MKL support. 	config=mkl_aarch64 	 Build with oneDNN and Compute Library for the Arm Architecture (ACL). 	config=monolithic  	 Config for mostly static monolithic build. 	config=numa        	 Build with NUMA support. 	config=dynamic_kernels	 (Experimental) Build kernels into separate shared objects. 	config=v1          	 Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features: 	config=nogcp       	 Disable GCP support. 	config=nonccl      	 Disable NVIDIA NCCL support. Configuration finished  Standalone code to reproduce the issue ```shell bazel build config=opt config=cuda verbose_failures //tensorflow:libtensorflow_cc.so ```  Relevant log output ```shell WARNING: Download from https://github.com/indygreg/pythonbuildstandalone/releases/download/20231002/cpython3.10.13+20231002x86_64unknownlinuxgnuinstall_only.tar.gz failed: class java.io.IOException connect timed out ERROR: An error occurred during the fetch of repository 'python_x86_64unknownlinuxgnu':    Traceback (most recent call last): 	File ""/home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/rules_python/python/repositories.bzl"", line 175, column 34, in _python_repository_impl 		rctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error downloading [https://github.com/indygreg/pythonbuildstandalone/releases/download/20231002/cpython3.10.13+20231002x86_64unknownlinuxgnuinstall_only.tar.gz] to /home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/python_x86_64unknownlinuxgnu/temp11172848094686838309/cpython3.10.13+20231002x86_64unknownlinuxgnuinstall_only.tar.gz: connect timed out ERROR: /home/workspace/Desktop/software/tensorflow2.15.0/WORKSPACE:36:27: fetching python_repository rule //external:python_x86_64unknownlinuxgnu: Traceback (most recent call last): 	File ""/home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/rules_python/python/repositories.bzl"", line 175, column 34, in _python_repository_impl 		rctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error downloading [https://github.com/indygreg/pythonbuildstandalone/releases/download/20231002/cpython3.10.13+20231002x86_64unknownlinuxgnuinstall_only.tar.gz] to /home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/python_x86_64unknownlinuxgnu/temp11172848094686838309/cpython3.10.13+20231002x86_64unknownlinuxgnuinstall_only.tar.gz: connect timed out ERROR: Error computing the main repository mapping: Encountered error while reading extension file 'requirements.bzl': no such package '//': no such package 'unknownlinuxgnu//': java.io.IOException: Error downloading [https://github.com/indygreg/pythonbuildstandalone/releases/download/20231002/cpython3.10.13+20231002x86_64unknownlinuxgnuinstall_only.tar.gz] to /home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/python_x86_64unknownlinuxgnu/temp11172848094686838309/cpython3.10.13+20231002x86_64unknownlinuxgnuinstall_only.tar.gz: connect timed out Loading:      Fetching repository ; Restarting. 55s ```",2025-01-03T07:21:59Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84042,", According to the official document, for tensorflow v2.15, the compiler is Clang 16.0.0. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations. https://www.tensorflow.org/install/sourcegpu Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,ZerryNi,The test case label_image .py of tensorflow2.4.1 source code fails to be execued.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.4.1  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.7.12  Bazel version 3.7  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The test case label_image.py fails to be executed,and the message ""module 'tensorfle' has no attribute 'GrapDef'"" is displayed. !image  Standalone code to reproduce the issue ```shell import tensorflow as tf graph_def = tf.GraphDef() ```  Relevant log output _No response_",2025-01-03T03:03:30Z,stat:awaiting response type:bug stale TF 2.4,closed,0,8,https://github.com/tensorflow/tensorflow/issues/84039,"Hi **** , Welcome to TensorFlow, and thank you for raising your concern here. The error you are facing occurs because 'GraphDef' is not directly accessible under the TensorFlow module in TensorFlow 2.x. To use 'GraphDef,' you need to import it from compat.v1. In TensorFlow 2.x, 'GraphDef' is located within the compat.v1 module to support backward compatibility with TensorFlow 1.x code. Please find the gist here for your reference.  Additionally, we recommend using the latest versions of TensorFlow for better performance and compatibility. Thank you!","Hi,l've used import tensorflow.compat.v1 as tf ,tf.disable_v2_behavior(),but the system still rreports that the incepion_v3_2016_08_28_frozem.pb file is missing,and the same error is reported after I download the file. !errortensorflow","Hi **** , Apologies for the delay, and thank you for your patience. It appears that you are still using an older version. Could you please upgrade to the latest TensorFlow version? Let us know if the issue still persists. Thank you!","Hi,thank you vert much for your reply,but we require tensorflow 3.4.1.","Hi **** , Apologies for the delay, and thanks for your patience. We do not support older or deprecated versions. Please migrate to the latest version for better results. Here, I am providing documentation for migration. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,sdp009,TFLITE NMS kernel Inconsistent Outputs and Out of Memory issues," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.16.2, tf 2.18, tf 2.19.0dev2024122  Custom code Yes  OS platform and distribution Linux Ubuntu 22  Mobile device Android  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The TFLITE NMS kernel output is not same as Tensorflow NMS output. Although the TFLITE NMS is a dynamic output shape layer, it is _**appending 0's**_ in the ""selected_indices"" output till ""max_output_size"", defeating the purpose of dynamic output. **TFLITE NMS output must identically match with TF NMS output.** For large ""max_output_size"", the TFLITE NMS results in super slow computation and many times it goes Outofmemory on Android devices. The subsequent **Gather** ops, after NMS suffers heavily due to appended 0's in the TFLITE NMS output. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/non_max_suppression.ccL190C44L190C59  **Requesting to fix this behavior and ensure both TF and TFLITE NMS output are exactly same.** !image  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np  Test inputs from : https://github.com/onnx/onnx/blob/main/docs/Operators.mdNonMaxSuppression : nonmaxsuppression_limit_output_size boxes = np.array(     [         [0.0, 0.0, 1.0, 1.0],         [0.0, 0.1, 1.0, 1.1],         [0.0, 0.1, 1.0, 0.9],         [0.0, 10.0, 1.0, 11.0],         [0.0, 10.1, 1.0, 11.1],         [0.0, 100.0, 1.0, 101.0],     ] ).astype(np.float32) scores = np.array([0.9, 0.75, 0.6, 0.95, 0.5, 0.3]).astype(np.float32) import tensorflow as tf max_output_size = tf.constant(tf.int32.max, dtype=tf.int32) iou_threshold = 0.5 selected_indices = tf.image.non_max_suppression(     boxes, scores, max_output_size, iou_threshold ) print(selected_indices)     returns expected output : tf.Tensor([3 0 5], shape=(3,), dtype=int32) .function(input_signature=[     tf.TensorSpec(shape=[6, 4], dtype=tf.float32),     tf.TensorSpec(shape=[6], dtype=tf.float32), ]) def nms_function(boxes, scores):     return tf.image.non_max_suppression(boxes, scores, max_output_size=tf.constant(tf.int32.max, dtype=tf.int32), iou_threshold=0.5) concrete_function = nms_function.get_concrete_function() print(concrete_function(boxes, scores))     returns expected output : tf.Tensor([3 0 5], shape=(3,), dtype=int32) converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_function]) tflite_model = converter.convert() with open('test_nms.tflite', 'wb') as f:     f.write(tflite_model) interpreter = tf.lite.Interpreter(model_path='test_nms.tflite') interpreter.allocate_tensors() input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() interpreter.set_tensor(input_details[0]['index'], boxes) interpreter.set_tensor(input_details[1]['index'], scores) interpreter.invoke() selected_indices = interpreter.get_tensor(output_details[0]['index']) print(selected_indices)     returns incorrect output appended with 0's : [3 0 5 ... 0 0 0] print(selected_indices.shape)     incorrect output of shape : (2147483647,)  above causes OOM error ```  Relevant log output ```shell TF output = tf.Tensor([3 0 5], shape=(3,), dtype=int32) TF Lite output = [3 0 5 ... 0 0 0]  ; shape = (2147483647,) ```",2025-01-02T21:14:57Z,stat:awaiting tensorflower type:bug comp:lite TF 2.18,closed,0,7,https://github.com/tensorflow/tensorflow/issues/84033,"Please add label ""comp:lite""","Hi,   I apologize for the delayed response, I tried to run provided code in Google colab but I'm getting `Your session crashed after using all available RAM.` so that is due to OOM issue for reference I've added gistfile and output log screenshot below so we'll have to dig more into this issue and update you !image Thank you for your cooperation and patience.","For easier debugging, please reduce the 'max_output_size' to a smaller value to avoid OOM issues. But in actual practice, there are some scenarios where higher 'max_output_size' did caused OOM issues on resource constraint Android devices.","Hi  , did you got chance to inspect it further ? Thanks.","Hi,  Please take a look into this issue. Thank you.","Hi , we will be moving this to LiteRT. Please follow progress there.",Are you satisfied with the resolution of your issue? Yes No
tpu,antipisa,Tensorflow BackupAndRestore method does not work," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source condaforge   TensorFlow version 2.17  Custom code No  OS platform and distribution Linux RHEL8  Mobile device _No response_  Python version 3.11.8  Bazel version _No response_  GCC/compiler version GCC 11.2.8  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? BackupAndRestore example code does not work  Standalone code to reproduce the issue ```shell import keras import numpy as np class InterruptingCallback(keras.callbacks.Callback):    def on_epoch_begin(self, epoch, logs=None):      if epoch == 4:        raise RuntimeError('Interrupting!') callback = keras.callbacks.BackupAndRestore(backup_dir=""/tmp/backup"") model = keras.models.Sequential([keras.layers.Dense(10)]) model.compile(keras.optimizers.SGD(), loss='mse') try:    model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,              batch_size=1, callbacks=[callback, InterruptingCallback()],              verbose=0) except Exception as e:    print(e) history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),                      epochs=10, batch_size=1, callbacks=[callback],                      verbose=0) len(history.history['loss']) ```  Relevant log output ```shell ValueError: To use the BackupAndRestore method, your model must be built before you call `fit()`. Model is unbuilt. You can build it beforehand by calling it on a batch of data. ```",2025-01-02T15:20:08Z,stat:awaiting response type:bug comp:keras 2.17,closed,0,9,https://github.com/tensorflow/tensorflow/issues/84027,"The error message you're encountering: ``` ValueError: To use the BackupAndRestore method, your model must be built before you call `fit()`. Model is unbuilt. You can build it beforehand by calling it on a batch of data. ``` indicates that the `BackupAndRestore` callback expects the model to be built before calling `fit()`, but in your case, the model is not explicitly built before the training loop starts.  Understanding the Problem:  **`BackupAndRestore` callback** requires the model to be ""built"" before starting training. The model needs to know the input shapes and architecture in order to correctly manage the backup and restoration processes.  **Model Building**: When you define a `Sequential` model without specifying input shapes, TensorFlow won't know the input shape until data is passed to the model. Therefore, you need to either specify the input shape when defining the model or pass a batch of data to the model before calling `fit()`.  Solution 1: Define the Input Shape in the Model You can explicitly define the input shape when creating the model, which ensures the model is ""built"" before training starts: ```python import keras import numpy as np class InterruptingCallback(keras.callbacks.Callback):    def on_epoch_begin(self, epoch, logs=None):      if epoch == 4:        raise RuntimeError('Interrupting!') callback = keras.callbacks.BackupAndRestore(backup_dir=""/tmp/backup"")  Define the model with an explicit input shape model = keras.models.Sequential([     keras.layers.InputLayer(input_shape=(20,)),   Specify the input shape     keras.layers.Dense(10) ]) model.compile(keras.optimizers.SGD(), loss='mse')  Now the model is built before training try:     model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,               batch_size=1, callbacks=[callback, InterruptingCallback()],               verbose=0) except Exception as e:     print(e) history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),                     epochs=10, batch_size=1, callbacks=[callback],                     verbose=0) print(len(history.history['loss'])) ```  Explanation:  **`InputLayer`**: By adding the `InputLayer` with an explicit `input_shape=(20,)`, you're telling Keras the expected shape of the input data, which ensures that the model is built before calling `fit()`.  Solution 2: Build the Model Before Calling `fit()` Alternatively, you can use a batch of data to build the model explicitly before training. This can be done using the `model.build()` method: ```python import keras import numpy as np class InterruptingCallback(keras.callbacks.Callback):    def on_epoch_begin(self, epoch, logs=None):      if epoch == 4:        raise RuntimeError('Interrupting!') callback = keras.callbacks.BackupAndRestore(backup_dir=""/tmp/backup"")  Define the model without specifying input shape model = keras.models.Sequential([     keras.layers.Dense(10) ]) model.compile(keras.optimizers.SGD(), loss='mse')  Build the model by passing a batch of data model.build(input_shape=(None, 20))   Here, 20 is the number of features in your input data  Now the model is built before calling fit try:     model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,               batch_size=1, callbacks=[callback, InterruptingCallback()],               verbose=0) except Exception as e:     print(e) history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),                     epochs=10, batch_size=1, callbacks=[callback],                     verbose=0) print(len(history.history['loss'])) ```  Explanation:  **`model.build()`**: This explicitly builds the model by providing the `input_shape`. After this call, the model is ready for training, and the `BackupAndRestore` callback will work correctly. To fix the issue, you need to ensure that the model is built (either by specifying the input shape or by explicitly calling `model.build()`) before invoking `fit()`. Both solutions will address the error and allow the `BackupAndRestore` callback to function as expected.","Hi **** , Thanks for raising your concern here. The raised PR has been merged. Could you please check and let us know if the issue still persists?  Thank you!","   Does Solution 1 work when the input layer is a normalization layer? E.g for this example ``` import tensorflow as tf x = tf.random.uniform((100, 1)) y = tf.random.uniform((100, 1)) z = tf.random.uniform((100, 1)) xyz = tf.concat([x, y, z], 1) horsepower_normalizer = tf.keras.layers.Normalization(input_shape=(3,), axis=1) horsepower_normalizer.adapt(xyz) horsepower_model = tf.keras.models.Sequential([     horsepower_normalizer,     tf.keras.layers.Dense(units=1) ]) horsepower_model(xyz) ``` EDIT: This raises a UserWarning about using input_shape in a layer. Is there a better way? ","  Solution 1 raises `ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received: (), (of type ) ' ` ","Hi **** , Apologies for the delay, and thank you for your patience. I tried running your code on Colab using TensorFlow 2.17.0 and 2.18.0 versions, but I am not facing any issues. Could you please check which warning you are encountering? Please find the gist here for your reference. Thank you!","Hi  , the problem is fixed if one imports the following way: ``` from tensorflow import keras from keras import layers from keras.layers import InputLayer, Dense ```","Hi **** , Glad to see your issue is resolved! Please feel free to close this issue if everything is working as expected. Thank you!",Are you satisfied with the resolution of your issue? Yes No,"Hi  ,  May I ask why the discrepancy when specifying input_shape in an Input layer vs when calling model.build? ``` model.build(input_shape=(None, 20))   Here, 20 is the number of features in your input data  input_shape is (None,  20) versus (20, None) in the below keras.layers.InputLayer(input_shape=(20,)),   Specify the input shape ```"
xla compile,copybara-service[bot],[XLA:GPU] Create a pathway between XLA compile-time and runtime to pass a TMAMetadata struct through. This communication pathway is necessary for implementing Nvidia's TMA feature on Hopper+ architectures.,[XLA:GPU] Create a pathway between XLA compiletime and runtime to pass a TMAMetadata struct through. This communication pathway is necessary for implementing Nvidia's TMA feature on Hopper+ architectures.,2025-01-02T11:14:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84020
tpu,jonasrsv42,Mixing Keras Layers and TF modules.," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? tf.Module can trace tf.Variable but it cannot trace variables from tf.keras or tf.keras.Variable.   Standalone code to reproduce the issue ```shell class MockLayer(tf.Module):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)         self.m = tf.keras.Variable(tf.random.normal([5, 5]), name=""m"")         self.w = tf.keras.Variable(tf.random.normal([5, 5]), name=""w"")     def __call__(self, inputs):         return self.m * inputs layer1 = MockLayer() print([v.name for v in layer1.trainable_variables]) ``` is empty. ``` class MockLayer(tf.Module):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)         self.m = tf.Variable(tf.random.normal([5, 5]), name=""m"")         self.w = tf.Variable(tf.random.normal([5, 5]), name=""w"")     def __call__(self, inputs):         return self.m * inputs layer1 = MockLayer() print([v.name for v in layer1.trainable_variables]) ``` Works. Specifically I am more interested in keras layers like  ``` class MockLayer(tf.Module):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)         self.norm = tf.keras.layers.LayerNormalization(*args, **kwargs)     def __call__(self, inputs):         return self.norm(inputs) ``` For which tracing does not appear to work.  I am interested in trying https://github.com/google/sequencelayers but they seem mostly broken on this TF version and python version due to the tracing issues.  I am curious to try fixing it, but not sure what's a supported path. Using keras layers in tf.Module does not work due to tracing issues.. and using Keras layers only does not work because Keras is missing many features like composite tensors.  Is there a way around this? :) ```  Relevant log output _No response_",2025-01-02T10:46:55Z,stat:awaiting response type:support stale comp:keras 2.17,closed,1,7,https://github.com/tensorflow/tensorflow/issues/84019,"I just hit the same issue trying to upgrade some code that was previously working on tensorflow 2.14; My setup is the same as described:    Top level `tf.Module`    Submodules are `keras.Model`s The kerasbased submodules are now not being detected at the `tf.Module` level because the reflection based implementation is explicitly looking for submodules that extend `tf.Module` (here). It appears that since the 2.16 release that switched to keras 3.X, `keras.Model` / `keras.layers.Layer` no longer extends `tf.Module` but instead only extends the underlying `AutoTrackable` class via its own `TFLayer` class (here). This contradicts / invalidates the tensorflow documentation here: > tf.keras.layers.Layer is the base class of all Keras layers, and it inherits from tf.Module. Looking through some issues in the keras repo, it appears this is intentional, unfortunately. Specifically this comment. There is a section in the keras documentation  about this and gives some direction for downgrading keras to v2 in order to support this structure.",", By default Tensorflow v2.17, v2.18 contains the Keras3.0.  As mentioned in this comment, Keras 3 design supports multiple backends (like JAX and PyTorch) in addition to TensorFlow. To achieve this, core classes like Model and Layer no longer rely on TensorFlow `tf.Module`. As a result, variable tracking within models and layers is no longer automatic. As this issue is more related to Keras, Kindly raise the request in the Kerasteam/keras repo for further discussion. Thank you!","I can raise it in Keras. But out of curiousity, I am trying to use tensorflow without Keras because Keras is missing features I want. (E.g Composite objects for Tensors)  But it seems with this split Tensorflow loses implementations for many common layers? Is there some intended replacement implementation of these layers for tensorflow if Keras will no longer work?  Or is the expectation that we should roll our own using tensorflow primitives for all the layers that used to be in tf.keras? ",It is unlikely that TF would get these layers.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
tpu,omarakl,I'm having a problem in converting my (.h5) model to (.tflite),"import tensorflow as tf store .h5 file in your .py folder load h5 module model=tf.keras.models.load_model('enhanced_model.h5') tflite_converter = tf.lite.TFLiteConverter.from_keras_model(model) convert tflite_model = tflite_converter.convert() open(""final.tflite"", ""wb"").write(tflite_model)  Output:  Exception encountered: int() argument must be a string, a byteslike object or a real number, not 'list' PS C:\Users\USER\Desktop\New folder (2)> ",2025-01-01T23:13:07Z,TFLiteConverter,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83986
opt,copybara-service[bot],[xla:cpu] Optimize filling compared values,[xla:cpu] Optimize filling compared values ``` name                                                                               old cpu/op   new cpu/op   delta BM_Sort1D/input_size:1000/num_inputs:1/is_stable:0/sort_ascending:1/process_time   11.4µs ± 2%  11.5µs ± 2%   +0.48%  (p=0.000 n=76+76) BM_Sort1D/input_size:1000/num_inputs:2/is_stable:0/sort_ascending:1/process_time   98.1µs ± 1%  98.2µs ± 2%     ~     (p=0.522 n=78+74) BM_Sort1D/input_size:1000/num_inputs:4/is_stable:0/sort_ascending:1/process_time    125µs ± 2%   127µs ± 1%   +1.28%  (p=0.000 n=78+78) BM_Sort1D/input_size:1000/num_inputs:8/is_stable:0/sort_ascending:1/process_time    195µs ± 1%   197µs ± 2%   +0.84%  (p=0.000 n=74+75) BM_Sort1D/input_size:1000/num_inputs:16/is_stable:0/sort_ascending:1/process_time   336µs ± 2%   340µs ± 2%   +1.34%  (p=0.000 n=74+75) BM_Sort1D/input_size:1000/num_inputs:32/is_stable:0/sort_ascending:1/process_time  1.15ms ± 1%  0.92ms ± 2%  20.25%  (p=0.000 n=74+75) BM_Sort1D/input_size:1000/num_inputs:1/is_stable:0/sort_ascending:0/process_time   87.2µs ± 1%  87.5µs ± 2%   +0.28%  (p=0.009 n=80+79) BM_Sort1D/input_size:1000/num_inputs:2/is_stable:0/sort_ascending:0/process_time   98.0µs ± 2%  98.1µs ± 2%     ~     (p=0.378 n=77+78) BM_Sort1D/input_size:1000/num_inputs:4/is_stable:0/sort_ascending:0/process_time    125µs ± 1%   127µs ± 2%   +1.29%  (p=0.000 n=77+76) BM_Sort1D/input_size:1000/num_inputs:8/is_stable:0/sort_ascending:0/process_time    195µs ± 2%   197µs ± 2%   +0.66%  (p=0.000 n=77+76) BM_Sort1D/input_size:1000/num_inputs:16/is_stable:0/sort_ascending:0/process_time   335µs ± 2%   339µs ± 2%   +1.43%  (p=0.000 n=75+74) BM_Sort1D/input_size:1000/num_inputs:32/is_stable:0/sort_ascending:0/process_time  1.15ms ± 1%  0.92ms ± 1%  20.18%  (p=0.000 n=74+76) ```,2025-01-01T21:40:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83984
opt,dependabot[bot],Bump ubuntu from `278628f` to `80dd3c3` in /tensorflow/tools/gcs_test,"Bumps ubuntu from `278628f` to `80dd3c3`. ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2025-01-01T08:21:29Z,ready to pull size:XS dependencies docker,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83974
