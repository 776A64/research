copybara-service[bot],Reverts 88cbfdecee62ee0cc00a03e009bd7018db8d0a74,Reverts 88cbfdecee62ee0cc00a03e009bd7018db8d0a74,2025-04-07T00:15:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90773
copybara-service[bot],Reverts 88cbfdecee62ee0cc00a03e009bd7018db8d0a74,Reverts 88cbfdecee62ee0cc00a03e009bd7018db8d0a74,2025-04-06T22:58:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90772
copybara-service[bot],Integrate LLVM at llvm/llvm-project@6bbdc70066c2,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 6bbdc70066c2,2025-04-06T18:50:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90771
copybara-service[bot],Add debug callback partitioner.,Add debug callback partitioner.,2025-04-06T16:27:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90770
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T09:49:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90769
copybara-service[bot],[xla:gpu] NFC: improve helper function for nested gemm/concat fusions.,[xla:gpu] NFC: improve helper function for nested gemm/concat fusions. Followup from cl/743505373.,2025-04-06T09:33:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90768
copybara-service[bot],"Integrate latest pywrap rules (mac fixes and support for pybind_extension submodules, including recursive ones).","Integrate latest pywrap rules (mac fixes and support for pybind_extension submodules, including recursive ones). Also enable pywrap rules on MacOS",2025-04-06T07:09:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90767
copybara-service[bot],Integrate LLVM at llvm/llvm-project@cd54cb062bba,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match cd54cb062bba,2025-04-06T05:26:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90766
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T03:53:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90765
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T03:47:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90764
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T03:47:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90763
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T03:46:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90762
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T03:45:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90761
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T03:45:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90760
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T03:43:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90759
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T03:42:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90758
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T03:41:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90757
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T03:36:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90756
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T03:35:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90755
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T03:34:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90754
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T02:42:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90753
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-06T02:37:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90752
copybara-service[bot],Fix a missing default initialization for the cached hash of `BasicDeviceList`,Fix a missing default initialization for the cached hash of `BasicDeviceList` The constructor of `std::atomic` didn't perform value initialization until C++20.,2025-04-05T19:20:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90751
copybara-service[bot],Cache the hash of `xla::ifrt::HloSharding`,"Cache the hash of `xla::ifrt::HloSharding` `xla::HloSharding`'s hash function isn't cheap because its current implementation unrolls iota tile assignment into a regular tile assignment. Since sharding objects are immutable, it is safe to cache its hash value. The cached value check uses the same pattern as `BasicDeviceList`.",2025-04-05T19:19:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90750
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-05T13:17:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90749
Andonvr,Build TFLite as static C++ library for use with WASM," Issue type Documentation Feature Request  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.19.0  Custom code No  OS platform and distribution WASM  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version Emscripten  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm writing a C++ project that I compile to WASM using Emscripten. I want to make use of TFLite, but am having issues getting it set up. I need to use TFLite directly from within C++. Calling Tensorflow.js via javascript does not work for my usecase. ONNXPack seems to have something like that (third question in this FAQ).  I found this really old Issue, which was closed due to inactivity.  Standalone code to reproduce the issue ```shell N/A ```  Relevant log output ```shell N/A ```",2025-04-05T11:17:18Z,type:feature comp:lite type:docs-feature TF 2.18,open,0,4,https://github.com/tensorflow/tensorflow/issues/90748,Is it open I would love to work on it.,"Hi,   I apologize for the delayed response, Please refer this blog and https://www.npmjs.com/package//tfjstflite which may help you to solve your issue and there are some similar issues which may help you https://github.com/emscriptencore/emsdk/issues/1424 and https://github.com/tensorflow/tensorflow/issues/46359 Thank you for your cooperation and understanding","Thanks for pointing me in that direction!   That guide uses 4 year old forks of XNNPack and Tensorflow, so I would have to recreate forks myself from the newer versions. This is not a _big_ problem, as the changes are quite small (https://github.com/google/XNNPACK/compare/master...visualcamp:XNNPACK:wasm and https://github.com/tensorflow/tensorflow/compare/master...visualcamp:tensorflow:PD135). But it feels like kind of a hack? Maybe you know a better way, using the official repos.  This is what happens when I use the original tensorflow and xnnpack repos: XNNPack is unhappy that I try to compile with CMAKE_SYSTEM_NAME as ""Emscripten"" (see their CMakeLists.txt). ```  The ASM compiler identification is unknown  Found assembler: /Users/anton/aniraweb/aniraweb/src/wasm/modules/emsdk/upstream/emscripten/emcc  Warning: Did not find file Compiler/ASM  Building for XNNPACK_TARGET_PROCESSOR: x86 CMake Error at wasmbuild/xnnpack/CMakeLists.txt:359 (MESSAGE):   Unrecognized CMAKE_SYSTEM_NAME value ""Emscripten"" ``` But it's good to know that I can create forks as a fallback, I'll try that some time this week. Thanks!","After taking a look at all links, and also trying the guide with custom forks of XNNPACK and Tensorflow, it is still unclear to me how it's supposed to work.   Does anyone have an actually working example?"
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-05T09:07:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90747
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-05T08:23:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90746
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-05T08:13:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90745
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-05T04:59:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90744
copybara-service[bot],Address previous FP8-related TODOs in jaxlib/XLA.,"Address previous FP8related TODOs in jaxlib/XLA. The ml_dtype requirement in JAX was updated to version 0.5.0+ (on Mar 20, 2025)  commit 4b7ead4 This update allows us to address previous FP8related TODOs in jaxlib/XLA.",2025-04-05T04:27:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90743
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-05T04:11:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90742
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-05T04:10:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90741
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-05T04:09:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90740
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-05T04:08:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90739
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-05T04:07:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90738
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-05T04:03:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90737
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-05T04:03:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90736
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-05T04:02:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90735
copybara-service[bot],Add a common AbstractTrackedDeviceBuffer type which can be used by,Add a common AbstractTrackedDeviceBuffer type which can be used by a AbstractLocalPjRtBuffer to allow a unified implementation of donation logic.,2025-04-05T04:02:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90734
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-05T03:59:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90733
copybara-service[bot],"The ml_dtype requirement in JAX was updated to version 0.5.0+ (on Mar 20, 2025) - [commit 4b7ead4](https://github.com/jax-ml/jax/commit/4b7ead4d02f866077f11dcfcca7507533a441bcc)","The ml_dtype requirement in JAX was updated to version 0.5.0+ (on Mar 20, 2025)  commit 4b7ead4 This update allows us to address previous FP8related TODOs in jaxlib/XLA.",2025-04-05T01:20:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90732
copybara-service[bot],Integrate LLVM at llvm/llvm-project@69f59d59cb02,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 69f59d59cb02,2025-04-05T00:19:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90731
copybara-service[bot],Support FP8 convolutions on Ada with cuDNN >= 9.8.,"Support FP8 convolutions on Ada with cuDNN >= 9.8. FP8 convolutions fail on cuDNN 9.0, but work on 9.8. I haven't tried any cuDNN versions in between, so I enable them for cuDNN 9.8 and above.",2025-04-04T23:47:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90730
copybara-service[bot],Run build_cleaner on xla/ directory.,"Run build_cleaner on xla/ directory. I've encountered a few CLs that attempted to fix this in local directories, so I figured I run this for all of xla to fix the lowhanging fruits. It resolves several unnecessary & missing dependencies and simplifying target paths, but not all of them. Here are the issues that came up that I didn't attempt to fix: * any conflicts that needs manual handling * conflicts that needs to choose between two ""valid"" targets * missing BUILD in a directory * missing target for a file (e.g. a python script) * missing targets for some `bzl_library` * platformspecific code (e.g. rocm) * ones that use filegroup instead of individual cc_library * and more. Before: ```  metric        median             Δ                  1pval             cpu: 3590.690s   ±91.6s                                           memory:     4533MB   ±2.6MB                                          system:  594.230s   ±10.5s                                             wall:  907.605s   ±83.0s                                               ``` After: ```  metric        median             Δ                  1pval             cpu: 3599.015s  ±131.4s    +8.3s, +0.2% 0.03 (not significant)    memory:     4533MB   ±2.3MB  +0.0MB, +0.0% 0.00 (not significant)    system:  582.305s    ±9.1s   11.9s, 2.0% 0.25 (not significant)      wall:  808.958s   ±95.5s  98.6s, 10.9% 0.57 (not significant)    ``` Overall, it has modest savings of ~1 minute of wall (physical) time. Since I've excluded some execution tests under `stream_executor/` and `service/` the estimated savings may be greater. Overall, it's a small improvement but should pay dividends in the long run. Note: I'll be sending a series of CLs to fix them in batches of subdirectories to simplify merging.",2025-04-04T23:38:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90729
copybara-service[bot],Temporarily remove input pipeline analyzer.,Temporarily remove input pipeline analyzer.,2025-04-04T23:21:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90728
copybara-service[bot],Reverts 88cbfdecee62ee0cc00a03e009bd7018db8d0a74,Reverts 88cbfdecee62ee0cc00a03e009bd7018db8d0a74,2025-04-04T23:06:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90727
copybara-service[bot],Integrate LLVM at llvm/llvm-project@6bbdc70066c2,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 6bbdc70066c2,2025-04-04T22:37:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90726
copybara-service[bot],More HLO->StableHLO Direct Conversions,"More HLO>StableHLO Direct Conversions Dot, Tuple, and Compare",2025-04-04T22:24:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90725
copybara-service[bot],Update xprof repo from tensorflow/profiler to openxla/xprof,Update xprof repo from tensorflow/profiler to openxla/xprof The xprof repo points now to openxla/xprof:   https://github.com/openxla/xprof/commit/7bee47367747c,2025-04-04T22:24:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90724
copybara-service[bot],"PR #24464: Roll forward ""PR #22292: [GPU] Support cuDNN explicit CUDA graph construction.""","PR CC(Tensorboard could not bind to unsupported address family): Roll forward ""PR CC(Docker with python 3.6): [GPU] Support cuDNN explicit CUDA graph construction."" Imported from GitHub PR https://github.com/openxla/xla/pull/24464 The problem that made the original PR get reverted was fixed in cuDNN frontend v1.11.0. Copybara import of the project:  806011b93a550fd0baed9a78c4af48dbae3ba2b3 by Ilia Sergachev : Roll forward ""PR CC(Docker with python 3.6): [GPU] Support cuDNN explicit CUDA graph construction."" Merging this change closes CC(Tensorboard could not bind to unsupported address family) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24464 from openxla:cudnn_explicit_cuda_graph 806011b93a550fd0baed9a78c4af48dbae3ba2b3",2025-04-04T21:13:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90723
copybara-service[bot],Nit: Remove % from a variable for consistency,Nit: Remove % from a variable for consistency,2025-04-04T21:13:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90722
copybara-service[bot],"Improve `safe_reinterpret_cast` to cover more cases, and fix some uses of `reinterpret_cast`.","Improve `safe_reinterpret_cast` to cover more cases, and fix some uses of `reinterpret_cast`.",2025-04-04T20:50:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90721
copybara-service[bot],Move `BufferComparatorKernel` behind `GpuKernelRegistry`.,Move `BufferComparatorKernel` behind `GpuKernelRegistry`. * Moves `BufferComparator` logic into `backends/gpu/runtime` since it's a runtime component. * Defines trait for the `BufferComparator` kernel in `stream_executor/gpu/` * Moves the implementations of this kernel into `stream_executor/{cudaFloatingPoint}Type`. * Makes `BufferComparator` retrieve the kernel by using the kernel registry. * Add the kernel implementations as dependencies to the `all_runtime` targets for CUDA and ROCm.,2025-04-04T20:18:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90720
copybara-service[bot],Create a PjRt protos dir.,"Create a PjRt protos dir. This dir exists for common PjRt proto files. To avoid breaking Jax in OSS, we also create an empty forwarding header & BUILD target where the old `compile_options.proto` used to live, which will soon be deleted. We only move the protos in `third_party/tensorflow/compiler/xla/pjrt` into this new dir (i.e. don't include `pjrt/stream_executor_executable.proto`, `pjrt/distributed/protocol.proto`, `pjrt/gpu/gpu_topology.proto`, and `pjrt/plugin/xla_cpu/cpu_topology.proto`) since those are purpose specific.",2025-04-04T19:44:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90719
copybara-service[bot],Update profiling code/scripts to prevent dependency on unreliable GPU targets originating from TPU output.,Update profiling code/scripts to prevent dependency on unreliable GPU targets originating from TPU output.,2025-04-04T19:29:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90718
copybara-service[bot],Update profiler pin to (hopefully) fix builds,Update profiler pin to (hopefully) fix builds,2025-04-04T19:05:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90717
copybara-service[bot],Bumping up libtpu version to pick correct versioned nightlies,Bumping up libtpu version to pick correct versioned nightlies,2025-04-04T18:45:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90716
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T18:22:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90715
copybara-service[bot],Test download time.,Test download time.,2025-04-04T18:21:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90714
copybara-service[bot],Reverts 88cbfdecee62ee0cc00a03e009bd7018db8d0a74,Reverts 88cbfdecee62ee0cc00a03e009bd7018db8d0a74,2025-04-04T18:21:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90713
copybara-service[bot],Automated Code Change,Automated Code Change Reverts 2d6a14ac610e06e9798ffd5a30064cd78c302305,2025-04-04T18:12:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90712
copybara-service[bot],Remove SplitMatchAndRewrite usages in TensorFlow.,Remove SplitMatchAndRewrite usages in TensorFlow. This was deleted in the upstream: https://github.com/llvm/llvmproject/commit/69f59d59cb02c06f1fac93ea5b19c2df9a684109,2025-04-04T17:59:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90711
copybara-service[bot],Fix element type mismatch for SPMD DUS indices,Fix element type mismatch for SPMD DUS indices,2025-04-04T17:56:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90710
copybara-service[bot],Fix clang-tidy warning in xla/hlo/transforms/collectives/collectives_schedule_linearizer_test.cc,Fix clangtidy warning in xla/hlo/transforms/collectives/collectives_schedule_linearizer_test.cc ``` third_party/tensorflow/compiler/xla/hlo/transforms/collectives/collectives_schedule_linearizer_test./tensorflow/compiler/xla/hlo/transforms/collectives/collectives_schedule_linearizer.cc ```,2025-04-04T17:52:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90709
copybara-service[bot],"Fix uses of the ""rank"" concept in `layout_util`.","Fix uses of the ""rank"" concept in `layout_util`.",2025-04-04T17:51:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90708
copybara-service[bot],Revert breaking change.,Revert breaking change. Reverts 1383f6e00f0e1ece8096cf44dfce763fbd9cc261,2025-04-04T15:36:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90707
copybara-service[bot],Deprecate a few `tsl::errors` functions in favor of their Abseil counterparts.,Deprecate a few `tsl::errors` functions in favor of their Abseil counterparts.,2025-04-04T13:33:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90706
copybara-service[bot],#sdy integrate propagation barrier,sdy integrate propagation barrier,2025-04-04T13:31:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90705
copybara-service[bot],[xla:gpu] CommandBuffer: add CommandBufferCmdSequence::Builder for building command buffers,[xla:gpu] CommandBuffer: add CommandBufferCmdSequence::Builder for building command buffers For now always use sequential synchronization mode for command buffers. DAG command buffers with automatic synchronization coming in followup CLs.,2025-04-04T13:27:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90704
copybara-service[bot],Add pipelining stages selection logic to dynamic search space,Add pipelining stages selection logic to dynamic search space,2025-04-04T12:54:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90703
copybara-service[bot],PR #24599: [GPU] Print contents of command buffer thunks.,PR CC(Documentation: Userfriendly Bazel installation): [GPU] Print contents of command buffer thunks. Imported from GitHub PR https://github.com/openxla/xla/pull/24599 This makes thunk_sequence.txt dumps more informative. Copybara import of the project:  e369633fb1e9a32dcacd3c922f8d2de3a15b69a5 by Ilia Sergachev : [GPU] Print contents of command buffer thunks. This makes thunk_sequence.txt dumps more informative. fix test Merging this change closes CC(Documentation: Userfriendly Bazel installation) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24599 from openxla:dump_cmdb_thunks e369633fb1e9a32dcacd3c922f8d2de3a15b69a5,2025-04-04T12:37:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90702
copybara-service[bot],Make the xla_compile target work without --config=cuda,"Make the xla_compile target work without config=cuda Half of the XLA CUDA runtime components were guarded by `if_cuda`, the other half by `if_cuda_is_configured`. This change unifies it and puts all CUDA/ROCm dependency behind if_{cuda|rocm}_is_configured.",2025-04-04T11:51:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90701
copybara-service[bot],Update open source xprof dependency,Update open source xprof dependency,2025-04-04T11:21:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90700
copybara-service[bot],Internal fixes to add missing dependencies.,Internal fixes to add missing dependencies.,2025-04-04T11:10:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90699
copybara-service[bot],[XLA:GPU][Emitters] Improve compile time by limiting the size of subgraphs.,"[XLA:GPU][Emitters] Improve compile time by limiting the size of subgraphs. At the moment, we can create subgraphs with O(10^4) ops, which leads to long compile time.  Also, this CL restricts the inliner, so that we don't inline the functions back to create huge functions again.",2025-04-04T10:57:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90698
copybara-service[bot],Update xprof pin to cb661ff947b + fix op_metrics_proto visibility,"Update xprof pin to cb661ff947b + fix op_metrics_proto visibility While at it, update repo path from tensorflow/profiler to openxla/xprof.",2025-04-04T10:41:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90697
copybara-service[bot],PR #24573: [ROCm] Fixed issue with builtin.module error.,PR CC(Calling a Dense layer fails when it is created with kernel_initializer=tf.keras.initializers.Zeros()): [ROCm] Fixed issue with builtin.module error. Imported from GitHub PR https://github.com/openxla/xla/pull/24573 Copybara import of the project:  82ab4adb53e16f93a439c34bb18034d0a7201922 by Zoran Jovanovic : [ROCm] Fixed issue with builtin.module error. Merging this change closes CC(Calling a Dense layer fails when it is created with kernel_initializer=tf.keras.initializers.Zeros()) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24573 from ROCm:ci_rocm_builtin_module_fix 82ab4adb53e16f93a439c34bb18034d0a7201922,2025-04-04T10:27:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90696
copybara-service[bot],Extract TMA metadata from the Triton module if it's available.,Extract TMA metadata from the Triton module if it's available.,2025-04-04T10:24:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90695
Usernadia122003,sfe, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2  Custom code Yes  OS platform and distribution Windows 11   Mobile device _No response_  Python version 3.13.2  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? none  Standalone code to reproduce the issue ```shell none ```  Relevant log output ```shell ```,2025-04-04T09:42:34Z,type:bug invalid,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90694,Doesn't seem to contain any relevant information
copybara-service[bot],Reverts 291e1b630eb24bf86f0d9a960a38827e5015ea92,Reverts 291e1b630eb24bf86f0d9a960a38827e5015ea92,2025-04-04T09:31:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90693
copybara-service[bot],Add contracting tile size selection logic to dynamic search space,Add contracting tile size selection logic to dynamic search space,2025-04-04T09:29:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90692
copybara-service[bot],[XLA:GPU] Fix int4 test for Ampere. On ampere multiply has F32 return type.,[XLA:GPU] Fix int4 test for Ampere. On ampere multiply has F32 return type.,2025-04-04T09:26:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90691
copybara-service[bot],Add back deleted `xla_gpu_enable_nccl_clique_optimization` flag.,Add back deleted `xla_gpu_enable_nccl_clique_optimization` flag. Removing the flag breaks some users.,2025-04-04T09:21:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90690
copybara-service[bot],Reverts 88cbfdecee62ee0cc00a03e009bd7018db8d0a74,Reverts 88cbfdecee62ee0cc00a03e009bd7018db8d0a74,2025-04-04T09:18:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90689
copybara-service[bot],Change some select() statements from tensorflow:fuchsia_x86_64 to tensorflow:fuchsia after tensorflow:arm_any aka tensorflow:linux_arm_any no longer applies.,Change some select() statements from tensorflow:fuchsia_x86_64 to tensorflow:fuchsia after tensorflow:arm_any aka tensorflow:linux_arm_any no longer applies.,2025-04-04T08:52:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90688
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T08:31:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90687
LukasMahieu,NaN loss on multi-GPU MirroredStrategy since tf 2.16," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version starting from 2.16 ([andcuda] versions)  Custom code Yes  OS platform and distribution Rocky Linux 8.9  Mobile device Rocky Linux 8.9  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory A100  80Gb  Current behavior? Starting from tensorflow version 2.16, models that train perfectly fine on a single A100 GPU will quickly devolve into an inf and then NaN loss on multiple GPUs. This is not the case for tensorflow version 2.15. This is not an isolated issue on my machine, as multiple users of our package which is built on tensorflow report the same exact issue (https://github.com/aertslab/CREsted/issues/100). Also, there's multiple mentions on here (for example https://github.com/tensorflow/tensorflow/issues/87432 and https://github.com/tensorflow/tensorflow/issues/62915) without a real resolution pertaining to the same issue. It's clearly a real issue as I've seen it happening on multiple different machines by multiple different users and it has a big impact since it stops us from scaling our models with tensorflow.  A couple of things I tried that didn't work:  Clipping the gradients  Lowering the learning rate  Ensuring no empty batches get created  Standalone code to reproduce the issue ```shell import tensorflow as tf def create_model():     """"""Create a larger and more complex model for demonstration purposes.""""""     model = tf.keras.Sequential(         [             tf.keras.layers.InputLayer(input_shape=(128, 128, 3)),             tf.keras.layers.Conv2D(128, 3, padding=""same"", activation=""relu""),             tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),             tf.keras.layers.BatchNormalization(),             tf.keras.layers.Conv2D(256, 3, padding=""same"", activation=""relu""),             tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),             tf.keras.layers.BatchNormalization(),             tf.keras.layers.Conv2D(512, 3, padding=""same"", activation=""relu""),             tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),             tf.keras.layers.BatchNormalization(),             tf.keras.layers.Conv2D(1024, 3, padding=""same"", activation=""relu""),             tf.keras.layers.GlobalAveragePooling2D(),             tf.keras.layers.Dense(2048, activation=""relu""),             tf.keras.layers.Dropout(0.5),             tf.keras.layers.Dense(2048, activation=""relu""),             tf.keras.layers.Dropout(0.5),             tf.keras.layers.Dense(1000, activation=""softmax""),         ]     )     return model def generate_synthetic_data(batch_size, num_batches):     """"""Generate synthetic data for training and validation.""""""     for _ in range(num_batches):         images = tf.random.normal((batch_size, 128, 128, 3))         labels = tf.random.uniform((batch_size,), maxval=10, dtype=tf.int32)         yield images, labels def train_multi_gpu(batch_size, epochs):     """"""Train the model using multiple GPUs.""""""      Detect and initialize GPUs     gpus_found = tf.config.list_physical_devices(""GPU"")     strategy = tf.distribute.MirroredStrategy()     print(""Number of replica devices in use: {}"".format(strategy.num_replicas_in_sync))     print(""Number of GPUs available: {}"".format(len(gpus_found)))     assert len(gpus_found) >= 1, ""Training requires at least 1 GPU""     strategy = tf.distribute.MirroredStrategy()     global_batch_size = batch_size * strategy.num_replicas_in_sync      Create a synthetic dataset     train_dataset = tf.data.Dataset.from_generator(         lambda: generate_synthetic_data(global_batch_size, 500),         output_signature=(             tf.TensorSpec(shape=(global_batch_size, 128, 128, 3), dtype=tf.float32),             tf.TensorSpec(shape=(global_batch_size,), dtype=tf.int32),         ),     ).repeat()     with strategy.scope():         model = create_model()         model.compile(             optimizer=""adam"", loss=tf.keras.losses.SparseCategoricalCrossentropy()         )         model.fit(train_dataset, epochs=epochs, steps_per_epoch=500) if __name__ == ""__main__"":     train_multi_gpu(batch_size=64, epochs=20) ```  Relevant log output ```shell Loss: NaN ```",2025-04-04T08:29:13Z,stat:awaiting tensorflower type:bug TF 2.16,open,1,4,https://github.com/tensorflow/tensorflow/issues/90686, Most Likely Causes 1. NCCL communication changes (modified gradient aggregation) 2. Mixed precision scaling synchronization updates 3. XLA graph optimization changes  Key Changes Between 2.15 → 2.16  Updated to CUDA 12.3/cuDNN 8.9  Modified `MirroredStrategy` gradient reduction logic  Changed `LossScaleOptimizer` behavior  Updated weight synchronization logic  Suggested Workarounds 1. Pin to TF 2.15 (verified working)    ```bash    pip install tensorflow==2.15.0 Try explicit FP32 mode:    ```python tf.keras.mixed_precision.set_global_policy('float32') ``` Test alternative reduction strategies:   ```python strategy = tf.distribute.MirroredStrategy(     cross_device_ops=tf.distribute.ReductionToOneDevice() ) ``` Enable NCCL debugging:  ```python export NCCL_DEBUG=INFO ```,"The problem was probably caused by changes made in TensorFlow 2.16. In this release, changes in how multi GPU training is managed like changes in gradient aggregation (particularly with NCCL communication), mixed precision scaling, and weight synchronization caused a regression that can lead to numerical instabilities. These instabilities appear in the form of the loss value rapidly becoming infinite and then NaN when training is done on more than one GPU, although single GPU training is stable. I'd love to work on this issue","Neither the explicit FP32 mode or alternative reduction strategy work, they still result in an inf > NaN loss.  Here's the full log in NCCL debug mode ``` 20250407 10:35:33.482430: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250407 10:35:33.495704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1744014933.509164 4105909 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1744014933.513302 4105909 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered W0000 00:00:1744014933.525145 4105909 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1744014933.525163 4105909 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1744014933.525165 4105909 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1744014933.525166 4105909 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. 20250407 10:35:33.528802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. I0000 00:00:1744014937.204507 4105909 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79195 MB memory:  > device: 0, name: NVIDIA A100SXM480GB, pci bus id: 0000:18:00.0, compute capability: 8.0 I0000 00:00:1744014937.206022 4105909 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 79195 MB memory:  > device: 1, name: NVIDIA A100SXM480GB, pci bus id: 0000:19:00.0, compute capability: 8.0 Number of replica devices in use: 2 Number of GPUs available: 2 .../envs/crested/lib/python3.12/sitepackages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.   warnings.warn( Epoch 1/20 I0000 00:00:1744014940.866528 4106006 cuda_dnn.cc:529] Loaded cuDNN version 90300 I0000 00:00:1744014941.293732 4106004 cuda_dnn.cc:529] Loaded cuDNN version 90300 i28g27:4105909:4106346 [0] NCCL INFO Bootstrap : Using ib0:172.23.4.51 i28g27:4105909:4106379 [0] NCCL INFO cudaDriverVersion 12060 i28g27:4105909:4106379 [0] NCCL INFO NCCL version 2.23.4+cudaCUDA_MAJOR.CUDA_MINOR i28g27:4105909:4106383 [0] NCCL INFO NET/Plugin: Could not find: libncclnet.so. Using internal network plugin. i28g27:4105909:4106383 [0] NCCL INFO NET/IB : Using [0]={[0] mlx5_0:1/IB, [1] mlx5_1:1/IB} [RO]; OOB ib0:172.23.4.51 i28g27:4105909:4106383 [0] NCCL INFO PROFILER/Plugin: Could not find: libncclprofiler.so. i28g27:4105909:4106383 [0] NCCL INFO Using network IB i28g27:4105909:4106384 [1] NCCL INFO PROFILER/Plugin: Could not find: libncclprofiler.so. i28g27:4105909:4106384 [1] NCCL INFO Using network IB i28g27:4105909:4106384 [1] NCCL INFO ncclCommInitRank comm 0x15230c060750 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 19000 commId 0x2eebe5b5ca38976b  Init START i28g27:4105909:4106383 [0] NCCL INFO ncclCommInitRank comm 0x15230c022a60 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 18000 commId 0x2eebe5b5ca38976b  Init START i28g27:4105909:4106384 [1] NCCL INFO Bootstrap timings total 0.000486 (create 0.000020, send 0.000105, recv 0.000122, ring 0.000045, delay 0.000000) i28g27:4105909:4106383 [0] NCCL INFO Bootstrap timings total 0.000467 (create 0.000014, send 0.000096, recv 0.000192, ring 0.000047, delay 0.000000) i28g27:4105909:4106384 [1] NCCL INFO Setting affinity for GPU 1 to 01ff i28g27:4105909:4106383 [0] NCCL INFO Setting affinity for GPU 0 to 01ff i28g27:4105909:4106384 [1] NCCL INFO comm 0x15230c060750 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0 i28g27:4105909:4106383 [0] NCCL INFO comm 0x15230c022a60 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0 i28g27:4105909:4106383 [0] NCCL INFO Channel 00/08 : 0 1 i28g27:4105909:4106383 [0] NCCL INFO Channel 01/08 : 0 1 i28g27:4105909:4106383 [0] NCCL INFO Channel 02/08 : 0 1 i28g27:4105909:4106384 [1] NCCL INFO Trees [0] 1/1/1>1>0 [1] 1/1/1>1>0 [2] 0/1/1>1>1 [3] 0/1/1>1>1 [4] 1/1/1>1>0 [5] 1/1/1>1>0 [6] 0/1/1>1>1 [7] 0/1/1>1>1 i28g27:4105909:4106384 [1] NCCL INFO P2P Chunksize set to 524288 i28g27:4105909:4106383 [0] NCCL INFO Channel 03/08 : 0 1 i28g27:4105909:4106383 [0] NCCL INFO Channel 04/08 : 0 1 i28g27:4105909:4106383 [0] NCCL INFO Channel 05/08 : 0 1 i28g27:4105909:4106383 [0] NCCL INFO Channel 06/08 : 0 1 i28g27:4105909:4106383 [0] NCCL INFO Channel 07/08 : 0 1 i28g27:4105909:4106383 [0] NCCL INFO Trees [0] 1/1/1>0>1 [1] 1/1/1>0>1 [2] 1/1/1>0>1 [3] 1/1/1>0>1 [4] 1/1/1>0>1 [5] 1/1/1>0>1 [6] 1/1/1>0>1 [7] 1/1/1>0>1 i28g27:4105909:4106383 [0] NCCL INFO P2P Chunksize set to 524288 i28g27:4105909:4106396 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 2 i28g27:4105909:4106397 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0 i28g27:4105909:4106395 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3 i28g27:4105909:4106394 [1] NCCL INFO [Proxy Service] Device 1 CPU core 1 i28g27:4105909:4106384 [1] NCCL INFO threadThresholds 8/8/64  512 i28g27:4105909:4106383 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 8 p2p channels per peer i28g27:4105909:4106383 [0] NCCL INFO CC Off, MultiGPU CC Off, workFifoBytes 1048576 i28g27:4105909:4106383 [0] NCCL INFO TUNER/Plugin: Could not find: libnccltuner.so libncclnet.so. Using internal tuner plugin. i28g27:4105909:4106383 [0] NCCL INFO ncclCommInitRank comm 0x15230c022a60 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 18000 commId 0x2eebe5b5ca38976b  Init COMPLETE i28g27:4105909:4106383 [0] NCCL INFO Init timings  ncclCommInitRank: rank 0 nranks 2 total 0.24 (kernels 0.13, alloc 0.04, bootstrap 0.00, allgathers 0.00, topo 0.05, graphs 0.00, connections 0.01, rest 0.01) i28g27:4105909:4106384 [1] NCCL INFO ncclCommInitRank comm 0x15230c060750 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 19000 commId 0x2eebe5b5ca38976b  Init COMPLETE i28g27:4105909:4106384 [1] NCCL INFO Init timings  ncclCommInitRank: rank 1 nranks 2 total 0.24 (kernels 0.13, alloc 0.04, bootstrap 0.00, allgathers 0.00, topo 0.05, graphs 0.00, connections 0.02, rest 0.00) i28g27:4105909:4106399 [0] NCCL INFO Channel 00/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 00/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106399 [0] NCCL INFO Channel 01/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 01/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106399 [0] NCCL INFO Channel 02/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 02/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 03/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 04/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106399 [0] NCCL INFO Channel 03/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 05/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106399 [0] NCCL INFO Channel 04/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 06/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106399 [0] NCCL INFO Channel 05/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Channel 07/0 : 1[1] > 0[0] via P2P/direct pointer/read i28g27:4105909:4106399 [0] NCCL INFO Channel 06/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106399 [0] NCCL INFO Channel 07/0 : 0[0] > 1[1] via P2P/direct pointer/read i28g27:4105909:4106398 [1] NCCL INFO Connected all rings i28g27:4105909:4106399 [0] NCCL INFO Connected all rings 130/500 ━━━━━━━━━━━━━━━━━━━━ 27s 75ms/step  loss: nan^CTraceback (most recent call last): ```",Any updates on this issue?
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T08:25:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90685
copybara-service[bot],Replace outdated select() on --cpu in tensorflow/BUILD and related files with platform API equivalent.,Replace outdated select() on cpu in tensorflow/BUILD and related files with platform API equivalent. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T08:22:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90684
copybara-service[bot],Compute minimum contracting size based on input type instead of hardcoding it,Compute minimum contracting size based on input type instead of hardcoding it,2025-04-04T07:51:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90683
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:48:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90682
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:36:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90681
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:35:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90680
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:34:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90679
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:34:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90678
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:33:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90677
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:33:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90676
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:33:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90675
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:32:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90674
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:32:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90674
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:32:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90673
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:32:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90672
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:32:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90671
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:32:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90670
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:32:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90669
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:32:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90668
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:32:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90667
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:32:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90666
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:32:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90665
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90664
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90663
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90662
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90661
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90660
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90659
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90658
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90657
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90656
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90655
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90654
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90653
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90652
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90651
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:31:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90650
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:30:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90649
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:30:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90648
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:30:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90647
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:30:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90646
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:30:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90645
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:30:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90644
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:30:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90643
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:30:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90642
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:29:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90641
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:29:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90640
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:29:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90639
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:29:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90638
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:29:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90637
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T07:29:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90636
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T06:13:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90635
copybara-service[bot],PR #24519: [nfc] Remove VLOG(s) added by mistake,PR CC(Unexpected behaviour of tf.map_fn()): [nfc] Remove VLOG(s) added by mistake Imported from GitHub PR https://github.com/openxla/xla/pull/24519 These were added by mistake in https://github.com/openxla/xla/commit/cc047dd9d9207c217c6eecaa55700e82c2066995 Copybara import of the project:  7a2d859e700af5530b3b012e0c358fb87c97950c by Shraiysh Vaishay : [nfc] Remove VLOG(s) added by mistake Merging this change closes CC(Unexpected behaviour of tf.map_fn()) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24519 from shraiysh:remove_vlogs 7a2d859e700af5530b3b012e0c358fb87c97950c,2025-04-04T05:01:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90634
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T04:50:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90633
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T04:49:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90632
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T04:31:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90631
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T04:30:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90630
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T04:28:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90629
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T04:28:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90628
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T04:27:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90627
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-04T04:24:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90626
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T04:23:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90625
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T04:17:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90624
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T04:10:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90623
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T04:04:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90622
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T04:04:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90621
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T03:52:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90620
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T03:44:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90619
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T03:42:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90618
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T03:41:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90617
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T03:41:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90616
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T03:40:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90615
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T03:40:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90614
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T03:39:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90613
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T03:37:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90612
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T03:33:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90611
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T03:32:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90610
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T03:30:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90609
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T03:30:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90608
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-04T03:28:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90607
copybara-service[bot],Change shared_ptr<TrackedDeviceBuffer> -> unique_ptr<TrackedDeviceBuffer>. This also removes some of the quirks of the GPU implementation (like the ForClosure code).,Change shared_ptr > unique_ptr. This also removes some of the quirks of the GPU implementation (like the ForClosure code).,2025-04-04T02:44:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90606
copybara-service[bot],Get correct num_sc_per_chip when doing AOT compilation for SC XLA ops.,"Get correct num_sc_per_chip when doing AOT compilation for SC XLA ops. In the case of doing AOT compilation, we do not have a tpu system available. Currently the code defaults to setting the number of sparsecores to 4, however this would break cases using chips that have 2 SCs. Therefore, we add an optional attribute to configure this.",2025-04-04T02:00:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90605
copybara-service[bot],Open-up Direct StableHLO -> HLO Path for StableHLO ops in production,Openup Direct StableHLO > HLO Path for StableHLO ops in production,2025-04-04T01:45:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90604
copybara-service[bot],Return error if tpu topology is not available when getting number of cores per chip.,Return error if tpu topology is not available when getting number of cores per chip. This is function is added to prevent returning cores_per_chip=4 when targeting hardware with 2 SCs.,2025-04-04T01:32:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90603
copybara-service[bot],Integrate LLVM at llvm/llvm-project@109566a3d0cf,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 109566a3d0cf,2025-04-04T01:07:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90602
copybara-service[bot],Add GetDefaultLayout API to IFRT Proxy,Add GetDefaultLayout API to IFRT Proxy,2025-04-04T00:04:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90601
copybara-service[bot],"Add python 3.13 version for wheel release job for linux, mac, win","Add python 3.13 version for wheel release job for linux, mac, win",2025-04-03T23:59:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90600
copybara-service[bot],Temporary disable `USE_CUDA_REDISTRIBUTIONS` because it increased CPU job time.,Temporary disable `USE_CUDA_REDISTRIBUTIONS` because it increased CPU job time.,2025-04-03T23:53:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90599
copybara-service[bot],Integrate LLVM at llvm/llvm-project@6bbdc70066c2,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 6bbdc70066c2,2025-04-03T23:38:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90598
copybara-service[bot],gpu_compiler_test: Ensure HLOs that would use SortRewriter compile deviceless.,"gpu_compiler_test: Ensure HLOs that would use SortRewriter compile deviceless. Reaching SortRewriter deviceless could be worth triggering a compilation failure in debug mode, but adding the necessary ifdef NDEBUG chains feels icky and against XLA's general policy of avoiding ifdefs, especially when the test also needs them.",2025-04-03T23:26:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90597
copybara-service[bot],Avoid creating large constants in ConvertTFLBroadcastToMulOp optimization pass.,Avoid creating large constants in ConvertTFLBroadcastToMulOp optimization pass. This pattern is inherited from before and has proved to be increasing the model size due the introduction of large splat const. In its current form this pattern replaces a tfl.broadcast_to op (with rank<4) to a tfl.mul with allones tensor. This change will keep the broadcast_to ops as is because its clear that introducing MUL is not an optimization.,2025-04-03T23:02:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90596
copybara-service[bot],Replace tf/compiler/mlir/quantization/common/quantization_lib with its lite fork for tensorflow_lite_ops and all its corollaries,Replace tf/compiler/mlir/quantization/common/quantization_lib with its lite fork for tensorflow_lite_ops and all its corollaries FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/92240 from 372046933:fix_pointer_stability 3a1ca864e5c79c49c62ee312952cf8bb58d98d69,2025-04-03T22:56:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90595
copybara-service[bot],PR #24491: [NVIDIA GPU] Collective-permute combiner ignores channel id when enabled,"PR CC(Cannot convert custom .pb file to .tflite file ): [NVIDIA GPU] Collectivepermute combiner ignores channel id when enabled Imported from GitHub PR https://github.com/openxla/xla/pull/24491 This PR pipes `xla_ignore_channel_id` flag to collectivepermute combiner, allowing the combiner to ignore channel id difference when the flag is on. Copybara import of the project:  01a961d67f3a2ada8856c2da920da52db87071b9 by Terry Sun : ignore channel id when enabled  f03e91d4611999999e6919374889028c54c899b0 by Terry Sun : always ignore channel id Merging this change closes CC(Cannot convert custom .pb file to .tflite file ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24491 from terryysun:terryysun/cp_combiner_ignore_channel_id f03e91d4611999999e6919374889028c54c899b0",2025-04-03T22:11:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90594
copybara-service[bot],Update Single Host XPlane Processing and OpStat conversion to share HloModuleMap.,Update Single Host XPlane Processing and OpStat conversion to share HloModuleMap.,2025-04-03T22:11:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90593
copybara-service[bot],Add python 3.13 requirements and conditions for TF,Add python 3.13 requirements and conditions for TF the code below creates a condition to run specific Python.h code in cpp for python3.13 as the following functions are deprecated: _PyArg_NoKeywords (removed) _PyObject_VisitManagedDict (renamed to PyObject_VisitManagedDict) _PyObject_ClearManagedDict (renamed to PyObject_ClearManagedDict),2025-04-03T22:05:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90592
copybara-service[bot],[PJRT] Split AsyncWorkRunner to its own header.,[PJRT] Split AsyncWorkRunner to its own header.,2025-04-03T22:00:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90591
copybara-service[bot],Reorder tpose(reshape(tpose(input))) to tpose(tpose(reshape(input))).,"Reorder tpose(reshape(tpose(input))) to tpose(tpose(reshape(input))). This reordering enables the two adjacent transpose ops to be folded into one, reducing unnecessary computation.",2025-04-03T21:53:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90590
copybara-service[bot],[xla:cpu:xnn] Make XnnDotThunk support BF16 x BF16 -> F32 matmul.,[xla:cpu:xnn] Make XnnDotThunk support BF16 x BF16 > F32 matmul.,2025-04-03T21:42:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90589
copybara-service[bot],Make TensorFlow CI properly read changes to `third_party/` which are Copybara'd to XLA,Make TensorFlow CI properly read changes to `third_party/` which are Copybara'd to XLA This is correct as files on GitHub in both `openxla/xla/third_party` and `tensorflow/tensorflow/third_party` are the same underlying file internally.,2025-04-03T21:18:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90588
copybara-service[bot],Add check to prevent HloTestBase and HloPjRtTestBase being used together.,Add check to prevent HloTestBase and HloPjRtTestBase being used together.,2025-04-03T21:07:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90587
Andonvr,Build Tensorflow Lite for WASM using Emscripten and CMake," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.19.0  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version Emcc: 4.0.5  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm trying to build Tensorflow Lite to WASM, using Emscripten and CMake. See my example repo here.   I can't use the prebuilt Tensorflow.js, even with the WASM backend, since I need to call the inference from several threads right within C++. Leaving WASM to call a tensorflow.js function that runs WASM again, would defeat the purpose for my usecase.   Anyway, the example I'm using here does not even use threads or anything, so that is not all too relevant, just some background.  I have these lines in my CMakeLists.txt: ``` include(${CMAKE_SOURCE_DIR}/cmake/SetupTFLite.cmake) add_dependencies(${MAIN} tensorflowlite) target_link_libraries(${MAIN} tensorflowlite) ``` The `cmake/SetupTFLite.cmake` clones tensorflow, checks out a tag, and then adds the tensorflow/lite subdirectory.   It is worth mentioning, I had to add `set(CMAKE_SYSTEM_NAME ""Linux"")`, otherwise I would get this error while configuring: ``` [cmake] CMake Error at build/xnnpack/CMakeLists.txt:359 (MESSAGE): [cmake]   Unrecognized CMAKE_SYSTEM_NAME value ""Emscripten"" ``` Anyway, so that variable was set.  Configuring all of this works, using the command shown in the repos README.   But building fails: And it seems to fail at different points. Here are a few from consecutive runs: ``` [build] [  6%] Linking CXX executable flatc.js [build] [  6%] Built target flatc [build] make: *** [all] Error 2 [proc] The command: /usr/local/bin/cmake build /Users/anton/tflitewasmexample/build config Debug target all j 13  exited with code: 2 ``` ``` [build] [  8%] Linking CXX static library libprotobufd.a [build] [  8%] Built target libprotobuf [build] make: *** [all] Error 2 [proc] The command: /usr/local/bin/cmake build /Users/anton/tflitewasmexample/build config Debug target all j 13  exited with code: 2 ``` ``` [build] /Users/anton/tflitewasmexample/build/xnnpack/src/xnnpack/math.h:446:52: note: include the header  or explicitly provide a declaration for 'rint' [build] 1 error generated. [build] make[2]: *** [_deps/xnnpackbuild/CMakeFiles/normalization.dir/src/normalization.c.o] Error 1 [build] make[1]: *** [_deps/xnnpackbuild/CMakeFiles/normalization.dir/all] Error 2 [build] 1 error generated. [build] make[2]: *** [_deps/xnnpackbuild/CMakeFiles/indirection.dir/src/indirection.c.o] Error 1 [build] make[1]: *** [_deps/xnnpackbuild/CMakeFiles/indirection.dir/all] Error 2 [build] make[1]: *** [_deps/cpuinfobuild/CMakeFiles/cpuinfo_internals.dir/all] Error 2 [build] make: *** [all] Error 2 [proc] The command: /usr/local/bin/cmake build /Users/anton/tflitewasmexample/build config Debug target all j 13  exited with code: 2 ``` (it then keeps failing at this spot!)  My first hunch is that building my main.cpp does not wait for tensorflow to be built? As in, something is wrong with the dependencies? And that's why Tensorflow compiles further and further, with the previous run still being cached? Sort of raceconditiony? But then it definitely always fails at the ""xnnpack/math.h"" thing?  Is there some simple example of using Tensorflow Lite directly within C++ and compiling it using CMake? I found this one, and oriented myself at it. Plus, it doesn't use WASM.  Standalone code to reproduce the issue I set up a minimal repo to reproduce here. Relevant files:  `CMakeLists.txt`: The CMake configuration file for the project.  `cmake/SetupTFLite.cmake`: A CMake script that clones TensorFlow Lite and includes it in the build. My repo does not include emscripten, nor flatbuffer, this you will have to adjust in the build commands. But maybe you will already see what mistake I made.  Relevant log output Described above.",2025-04-03T21:04:12Z,type:support,closed,0,5,https://github.com/tensorflow/tensorflow/issues/90586," Fix 3: Modify Build Flags Sometimes, the issue stems from Emscripten’s standard library setup. You can try: ```bash emcmake cmake .. DCMAKE_SYSTEM_NAME=Emscripten \   DCMAKE_C_COMPILER=emcc \   DCMAKE_CXX_COMPILER=em++ \   DCMAKE_CXX_FLAGS=""std=c++17 s USE_PTHREADS=1 s WASM=1 s USE_SDL=2 s ENVIRONMENT=web s MODULARIZE=1"" \   DCMAKE_C_FLAGS=""std=c11"" ``` Then run: ```bash cmake build . j ``` **Ensure that:**  You're using the latest compatible Emscripten (>=3.x).  You include `s USE_PTHREADS=1` *only if* you really use threading (and the browser supports it with crossorigin isolation).","When I use `DCMAKE_SYSTEM_NAME=Emscripten`, I get this error: ```  The ASM compiler identification is unknown  Found assembler: /Users/anton/emsdk/upstream/emscripten/emcc  Warning: Did not find file Compiler/ASM  Building for XNNPACK_TARGET_PROCESSOR: x86 CMake Error at build/xnnpack/CMakeLists.txt:359 (MESSAGE):   Unrecognized CMAKE_SYSTEM_NAME value ""Emscripten"" ``` But I got this before, and circumvented this by setting the CMAKE_SYSTEM_NAME to Linux? ","But after making that adjustment and building, I still get this error: ``` make[2]: *** [_deps/xnnpackbuild/CMakeFiles/microkernelsprod.dir/src/qu8gemm/gen/qu8gemm1x4c8minmaxfp32avxld128.c.o] Error 1 make[2]: *** [_deps/xnnpackbuild/CMakeFiles/microkernelsprod.dir/src/f16rminmax/gen/f16rmaxavx512fp16u128acc4.c.o] Error 1 In file included from /Users/anton/tflitewasmexample/build/xnnpack/src/qs8vadd/gen/qs8vaddminmaxscalaru4.c:12: /Users/anton/tflitewasmexample/build/xnnpack/src/xnnpack/math.h:446:52: error: call to undeclared library function 'rint' with type 'double (double)'; ISO C99 and later do not support implicit function declarations [Wimplicitfunctiondeclaration]   446                                                     ^ /Users/anton/tflitewasmexample/build/xnnpack/src/xnnpack/math.h:446:52: note: include the header  or explicitly provide a declaration for 'rint' 1 error generated. make[2]: *** [_deps/xnnpackbuild/CMakeFiles/microkernelsprod.dir/src/f32ibilinear/gen/f32ibilinearscalarc2.c.o] Error 1 1 error generated. make[2]: *** [_deps/xnnpackbuild/CMakeFiles/microkernelsprod.dir/src/qs8dwconv/gen/qs8dwconv25p1cminmaxfp32scalarimagic.c.o] Error 1 make[1]: *** [_deps/xnnpackbuild/CMakeFiles/microkernelsprod.dir/all] Error 2 [ 26%] Built target flatc make: *** [all] Error 2 ```","After some thinking, I will close this Issue and open a separate slightly different one. I think my problem does not lie with CMake, but on the overall building of a static WASM library.",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[IFRT Proxy] Add missing `ArrayStore::Reservation::ProcessResponse()` calls,"[IFRT Proxy] Add missing `ArrayStore::Reservation::ProcessResponse()` calls When using `ArrayStore::Reservation`, `ArrayStore::Reservation::ProcessResponse()` must be used to catch any errors raised during request handling. Otherwise, the reservation remains nonfilled and would break the proxy server invariant.",2025-04-03T20:57:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90585
copybara-service[bot],add profiling context to the runtime profiling library,add profiling context to the runtime profiling library,2025-04-03T20:44:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90584
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@4bf77d23,Integrate StableHLO at openxla/stablehlo,2025-04-03T20:13:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90583
copybara-service[bot],"Add optimization pass to rewrite FC(IsConst(x), y) as FC(y, x).","Add optimization pass to rewrite FC(IsConst(x), y) as FC(y, x). Many downstream optimizations rely on proper ordering of the input operands into FullyConnected. Some JAX programs produce graphs with the inputs swapped and this rewrite pattern protects against that behavior.",2025-04-03T20:11:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90582
copybara-service[bot],[IFRT] Check the uniqueness of IFRT devices,[IFRT] Check the uniqueness of IFRT devices IFRT devices must be unique and have unique device Ids within a single IFRT client. This is now tested explicitly to ensure that all implementations satisfy the requirement.,2025-04-03T20:11:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90581
copybara-service[bot],Add `collect_symlink_data_aspect` to search for the symlinked files in the target runfiles.,Add `collect_symlink_data_aspect` to search for the symlinked files in the target runfiles.,2025-04-03T20:07:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90580
plopresti,Fix NVCC+Clang build failure,"""nvcc compilerbindir /path/to/clang"" sets `__clang__` while compiling CUDA code. This causes gpu_device_functions.h to think it is being compiled with Clang and try to use a Clangspecific function. Fixes CC(Clang+NVCC: ./tensorflow/core/util/gpu_device_functions.h(198): error: identifier ""__nvvm_read_ptx_sreg_laneid"" is undefined).",2025-04-03T20:04:13Z,ready to pull size:XS comp:core,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90579
plopresti,"Clang+NVCC: ./tensorflow/core/util/gpu_device_functions.h(198): error: identifier ""__nvvm_read_ptx_sreg_laneid"" is undefined"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version r2.19  Custom code No  OS platform and distribution Alma 9.5  Mobile device _No response_  Python version 3.12  Bazel version 6.5.0  GCC/compiler version N/A  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Attempting to build with NVCC+Clang fails.  Standalone code to reproduce the issue ```shell env TF_NEED_CLANG=1 TF_NEED_CUDA=1 TF_CUDA_CLANG=0 GCC_HOST_COMPILER_PATH=/usr/bin/clang ./configure bazel build repo_env=CUDA_NVCC=1 Just tell configure you want to use Clang for everything except compiling CUDA kernels. For that, tell it you want nvcc, but you want ""compilerbindir"" pointing to Clang. ```  Relevant log output ```shell ./tensorflow/core/util/gpu_device_functions.h(198): error: identifier ""__nvvm_read_ptx_sreg_laneid"" is undefined The problem is gpu_device_functions.h has a ""if __clang__"" conditional that invokes a Clangspecific function that nvcc does not understand. ```",2025-04-03T19:59:49Z,type:bug awaiting PR merge TF 2.18,closed,0,2,https://github.com/tensorflow/tensorflow/issues/90578,"Hi  , Apologies for the delay, and thank you for raising your concern here. I noticed that you have already raised a PR related to your issue. Once it gets merged, your issue should be resolved. Thank you!",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],ConvolutionOp: Direct StableHLO to HLO Translate,ConvolutionOp: Direct StableHLO to HLO Translate,2025-04-03T19:53:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90577
copybara-service[bot],[NFC] Refactor `GetExecutableExtras()` to separate the compilation option update to another function `UpdateCompileOptions()`.,[NFC] Refactor `GetExecutableExtras()` to separate the compilation option update to another function `UpdateCompileOptions()`. The `UpdateCompileOptions()` will be used by the 'Compile()` implementation which returns an unloaded executable.,2025-04-03T19:12:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90576
copybara-service[bot],Temporarily remove input pipeline analyzer.,Temporarily remove input pipeline analyzer.,2025-04-03T19:12:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90575
copybara-service[bot],Integrate LLVM at llvm/llvm-project@537b6541e806,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 537b6541e806,2025-04-03T19:12:06Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/90574,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
Tai78641,[mlir][tosa] Fix up lit tests,This fixes a couple of failing lit tests due to tosa llvm updates,2025-04-03T18:53:20Z,kokoro:force-run ready to pull size:S,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90573,"INFO: Analyzed 33 targets (0 packages loaded, 33965 targets configured). INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 3.529s, Critical Path: 0.13s INFO: 1 process: 1 internal. INFO: Build completed successfully, 1 total action //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test (cached) PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test    (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test       (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test  (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test (cached) PASSED in 0.7s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test (cached) PASSED in 3.4s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test (cached) PASSED in 0.6s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test (cached) PASSED in 0.4s"
copybara-service[bot],Update reachability when control dep is updated,Update reachability when control dep is updated Bug: 408233511,2025-04-03T18:45:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90572
copybara-service[bot],Fix clang-tidy warning in xla/hlo/transforms/collectives/collectives_schedule_linearizer_test.cc,"Fix clangtidy warning in xla/hlo/transforms/collectives/collectives_schedule_linearizer_test.cc ``` third_party/tensorflow/compiler/xla/hlo/transforms/collectives/collectives_schedule_linearizer_test.cc:7:10: error: missing terminating '""' character ```",2025-04-03T18:43:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90571
copybara-service[bot],Add support for blockwise quantization to FC operator.,Add support for blockwise quantization to FC operator.,2025-04-03T18:39:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90570
copybara-service[bot],fix failing to build //tensorflow/core:portable_tensorflow_lib_lite,fix failing to build //tensorflow/core:portable_tensorflow_lib_lite,2025-04-03T18:27:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90569
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@4bf77d23,Integrate StableHLO at openxla/stablehlo,2025-04-03T18:20:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90568
copybara-service[bot],Reverts 9bdeda24e58b39852225c69b0ef7218835fa5bbf,Reverts 9bdeda24e58b39852225c69b0ef7218835fa5bbf,2025-04-03T18:19:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90567
copybara-service[bot],Move TFL::StridedSliceOp and TFL::SliceOp rank constraints to runtime checks,Move TFL::StridedSliceOp and TFL::SliceOp rank constraints to runtime checks This relaxes these constraints for other nonruntime backends.,2025-04-03T18:17:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90566
copybara-service[bot],Make the fully_connected ref kernel do requant in float.,Make the fully_connected ref kernel do requant in float.,2025-04-03T18:09:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90565
copybara-service[bot],"BroadcastOp, BroadCastInDimOp, DynamicBroadcastInDimOp : Direct StableHLO -> HLO Translation","BroadcastOp, BroadCastInDimOp, DynamicBroadcastInDimOp : Direct StableHLO > HLO Translation",2025-04-03T17:57:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90564
plopresti,Adapt commit 5467ee9 from XLA to TensorFlow,"TensorFlow has a build failure with newer CUDA. The problem has been fixed in XLA (openxla/xla CC(Implement Scale Operate??) and TensorFlow commit https://github.com/tensorflow/tensorflow/commit/5467ee993e1d3e4709c1e99f3a15a978325ae536). This change adapts that commit to tensorflow/core/kernels/gpu_prim.h. Fixes CC(gpu_prim.h: no instance of overloaded function ""cub::ThreadLoadVolatilePointer"" matches the specified type).",2025-04-03T17:51:13Z,stat:awaiting response size:M,open,0,2,https://github.com/tensorflow/tensorflow/issues/90563,"I would also recommend this as a cherrypick for r2.19, which does not currently work with CUDA 12.8.0.","Hi , Can you please resolve the conflicts. Many Thanks!"
plopresti,"gpu_prim.h: no instance of overloaded function ""cub::ThreadLoadVolatilePointer"" matches the specified type"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version r2.19  Custom code No  OS platform and distribution Alma Linux 9.5  Mobile device _No response_  Python version 3.12  Bazel version 6.5.0  GCC/compiler version _No response_  CUDA/cuDNN version 12.8.0  GPU model and memory _No response_  Current behavior? Compiling against CUDA 12.8.0 gives an error: ``` ./tensorflow/core/kernels/gpu_prim.h(48): error: no instance of overloaded function ""cub::ThreadLoadVolatilePointer"" matches the specified type                             Eigen::half ThreadLoadVolatilePointer( ``` Commit 5467ee9 for XLA needs to be adapted and applied to tensorflow/core/kernels/gpu_prim.h.  Standalone code to reproduce the issue ```shell `env HERMETIC_CUDA_VERSION=1.8.0 ./configure` `bazel build` ```  Relevant log output ```shell ./tensorflow/core/kernels/gpu_prim.h(48): error: no instance of overloaded function ""cub::ThreadLoadVolatilePointer"" matches the specified type                             Eigen::half ThreadLoadVolatilePointer( ```",2025-04-03T17:41:17Z,type:bug awaiting PR merge TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/90562,"Hi  , Apologies for the delay, and thank you for raising your concern here. I noticed that you have already raised a PR related to your issue. Once it gets merged, your issue should be resolved. Thank you!"
copybara-service[bot],Add interface of Codegen backend for autotuner.,Add interface of Codegen backend for autotuner.,2025-04-03T17:37:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90561
mystressedout,commit,commit,2025-04-03T17:36:20Z,size:XS,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90560,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Internal only: experimental,Internal only: experimental,2025-04-03T17:29:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90559
plopresti,Remove ambiguous inherited constructor in default_quant_params.cc,Remove ambiguous inherited constructor in default_quant_params.cc. GCC complains about this (https://stackoverflow.com/q/79553477/). Fix is trivial and harmless. Fixes CC(target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize fail to build).,2025-04-03T17:28:35Z,comp:lite ready to pull size:XS,closed,0,3,https://github.com/tensorflow/tensorflow/issues/90558,"I would recommend this as a cherrypick for r2.19, which does not currently compile with GCC 13/14.",Also this is an actual bug in the TF code and Clang is being fixed to reject it (https://github.com/llvm/llvmproject/issues/121331).,"I can confirm that this fixes the build with GCC 13.2.0 for me on x86_64. I still have an issue on aarch64, but I can open a separate issue for that. Thanks for the fix!"
copybara-service[bot],change visibility of target,change visibility of target,2025-04-03T16:53:55Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/90557,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],[XLA:GPU] Make tests match the latest state of `fusion_emitter_device_legacy_test.cc`.,[XLA:GPU] Make tests match the latest state of `fusion_emitter_device_legacy_test.cc`.,2025-04-03T14:59:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90556
copybara-service[bot],PR #24550: Support custom call stream assignment,"PR CC(Add python API for some quantized operations.): Support custom call stream assignment Imported from GitHub PR https://github.com/openxla/xla/pull/24550 Previously, in JAX if a user tried to use `compute_on(""gpu_stream:"")` on a computation that included a custom call, that custom call would not run on the specified stream, leading to errors.  To fix this, we simply use `GetStreamForExecution` instead of just relying on `params.stream` to decided which stream to pass to the FFI. This is the same logic that other thunks already have. We need to make this edit twice since there are two codepaths a custom call can follow, once via `ExecuteFfiHandler` and another in `ExecuteCustomCall`.  Copybara import of the project:  7ee230f7abf744fa40ad86e0521e9b8b9f7cc80e by chaser : Support custom call stream assignment Merging this change closes CC(Add python API for some quantized operations.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24550 from chaserileyroberts:chase/custom_call_stream_fix 7ee230f7abf744fa40ad86e0521e9b8b9f7cc80e",2025-04-03T14:55:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90555
copybara-service[bot],PR #23508: [ROCm] Enable OCP FP8 for latest AMD archs,"PR CC(AttributeError: 'Estimator' object has no attribute '_distribution'): [ROCm] Enable OCP FP8 for latest AMD archs Imported from GitHub PR https://github.com/openxla/xla/pull/23508 We are moving from NANOO FP8 (i.e., F8E4M3FNUZ, F8E5M2FNUZ) to OCP FP8 (i.e., F8E4M3FN, F8E5M2) for new archs. In this PR:  We add support for OCP FP8 on ROCm. (`F8ConvertD` fusion isn't enabled now; we will submit another PR to support it soon.)  We also find that, because a/b scales are always enabled in FP8 scenarios, it's necessary to pass these scaling factors to the autotuner (even though they are dummy pointers), ensuring it selects algorithms that handle a/b scales properly. We introduce another `is_fp8` field to `GemmBackendConfig` to support it. Copybara import of the project:  42914277bdcd7e7bb3d760c510fff212c220c8f7 by scxfjiang : enable ocp fp8 for latest amd archs in gemm rewriter  c080d446deb92b100ba28d6838d11fe5f06543b2 by scxfjiang : fix typo  ad4000fdc59061ab147d6d46d0549b289381fe1b by scxfjiang : rm is_fp8 field  8e43ae776b84b08dd2a2289c7b0d6f20d8048382 by scxfjiang : fix typo Merging this change closes CC(AttributeError: 'Estimator' object has no attribute '_distribution') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23508 from ROCm:ci_enable_ocp_fp8_in_gemm_rewriter 8e43ae776b84b08dd2a2289c7b0d6f20d8048382",2025-04-03T14:20:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90554
donhuvy,Document was incorrect,See https://stackoverflow.com/questions/79553173/tensorflownightlyvalueerrorcannotconvertcounte/79553205 CC(未找到相关数据)05 The document leads user to a wrong result It is a bug https://github.com/tensorflow/recommenders/pull/717 . Ralated https://github.com/tensorflow/recommenders/issues/731 . Waiting for merging.  It should not happen especially with new comer.,2025-04-03T14:20:01Z,stat:awaiting response type:support stale,closed,0,11,https://github.com/tensorflow/tensorflow/issues/90553,"Hi  , Apologies for the delay, and thank you for raising your concern here. Could you please provide a valid link? The one you shared is not opening for me: ``` https://stackoverflow.com/questions/79553173/tensorflownightlyvalueerrorcannotconvertcounte/79553205 CC(未找到相关数据)05 ``` Thank you!","I deleted the origin question in Stack Overflow . You can see few information at https://gist.github.com/donhuvy/9447a2aea4cd182007198f28d4b7b413 ``` I follow this guide https://www.tensorflow.org/recommenders and https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/quickstart.ipynbscrollTo=4FyfuZXgTKS . My environment: Windows 11 x64, PyCharm 2024.3.5 (Professional Edition), Jupyter notebook inside PyCharm, TensorFlow nightly, CUDA 12.8 . Microsoft Windows [Version 10.0.26100.3476] (c) Microsoft Corporation. All rights reserved. C:\Users\ADMIN>nvidiasmi Thu Apr  3 21:03:07 2025 ++  ++ C:\Users\ADMIN> I have !pip install upgrade tensorflow_hub import tensorflow_hub as hub model = hub.KerasLayer(""https://tfhub.dev/google/nnlmendim128/2"") embeddings = model([""The rain in Spain."", ""falls"", ""mainly"", ""In the plain!""]) print(embeddings.shape) !pip install q tensorflowrecommenders !pip install q upgrade tensorflowdatasets !pip install tensorflowrecommenders !pip install upgrade tensorflowdatasets from typing import Dict, Text import numpy as np import tensorflow as tf import tensorflow_datasets as tfds import tensorflow_recommenders as tfrs ratings = tfds.load('movielens/100kratings', split=""train"") movies = tfds.load('movielens/100kmovies', split=""train"") ratings = ratings.map(lambda x: {""movie_title"": x[""movie_title""], ""user_id"": x[""user_id""]}) movies = movies.map(lambda x: x[""movie_title""]) user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None) user_ids_vocabulary.adapt(ratings.map(lambda x: x[""user_id""])) movie_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None) movie_titles_vocabulary.adapt(movies) class MovieLensModel(tfrs.Model):   def __init__(self, user_model: tf.keras.Model, movie_model: tf.keras.Model, task: tfrs.tasks.Retrieval):     super().__init__()     self.user_model = user_model     self.movie_model = movie_model     self.task = task   def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) > tf.Tensor:     user_embeddings = self.user_model(features[""user_id""])     movie_embeddings = self.movie_model(features[""movie_title""])     return self.task(user_embeddings, movie_embeddings) user_model = tf.keras.Sequential([user_ids_vocabulary, tf.keras.layers.Embedding(user_ids_vocabulary.vocabulary_size(), 64)]) movie_model = tf.keras.Sequential([movie_titles_vocabulary, tf.keras.layers.Embedding(movie_titles_vocabulary.vocabulary_size(), 64)]) !pip show tensorflow import tensorflow as tf print(tf.__version__) print(user_ids_vocabulary.get_vocabulary()) print(movie_titles_vocabulary.get_vocabulary()) task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(movies.batch(128).map(movie_model))) error  ValueError                                Traceback (most recent call last) Cell In[1], line 55      52 print(user_ids_vocabulary.get_vocabulary())      53 print(movie_titles_vocabulary.get_vocabulary()) > 55 task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(movies.batch(128).map(movie_model))) File ~\PyCharmMiscProject\.venv\Lib\sitepackages\tensorflow_recommenders\metrics\factorized_top_k.py:79, in FactorizedTopK.__init__(self, candidates, ks, name)      75 super().__init__(name=name)      77 if isinstance(candidates, tf.data.Dataset):      78   candidates = ( > 79       layers.factorized_top_k.Streaming(k=max(ks))      80       .index_from_dataset(candidates)      81   )      83 self._ks = ks      84 self._candidates = candidates File ~\PyCharmMiscProject\.venv\Lib\sitepackages\tensorflow_recommenders\layers\factorized_top_k.py:376, in Streaming.__init__(self, query_model, k, handle_incomplete_batches, num_parallel_calls, sorted_order)     373 self._num_parallel_calls = num_parallel_calls     374 self._sorted = sorted_order > 376 self._counter = self.add_weight(""counter"", dtype=tf.int32, trainable=False) File ~\PyCharmMiscProject\.venv\Lib\sitepackages\keras\src\layers\layer.py:547, in Layer.add_weight(self, shape, initializer, dtype, trainable, autocast, regularizer, constraint, aggregation, name)     545 initializer = initializers.get(initializer)     546 with backend.name_scope(self.name, caller=self): > 547     variable = backend.Variable(     548         initializer=initializer,     549         shape=shape,     550         dtype=dtype,     551         trainable=trainable,     552         autocast=autocast,     553         aggregation=aggregation,     554         name=name,     555     )     556  Will be added to layer.losses     557 variable.regularizer = regularizers.get(regularizer) File ~\PyCharmMiscProject\.venv\Lib\sitepackages\keras\src\backend\common\variables.py:185, in Variable.__init__(self, initializer, shape, dtype, trainable, autocast, aggregation, name)     183 else:     184     if callable(initializer): > 185         self._shape = self._validate_shape(shape)     186         self._initialize_with_initializer(initializer)     187     else: File ~\PyCharmMiscProject\.venv\Lib\sitepackages\keras\src\backend\common\variables.py:207, in Variable._validate_shape(self, shape)     206 def _validate_shape(self, shape): > 207     shape = standardize_shape(shape)     208     if None in shape:     209         raise ValueError(     210             ""Shapes used to initialize variables must be ""     211             ""fullydefined (no `None` dimensions). Received: ""     212             f""shape={shape} for variable path='{self.path}'""     213         ) File ~\PyCharmMiscProject\.venv\Lib\sitepackages\keras\src\backend\common\variables.py:582, in standardize_shape(shape)     580     continue     581 if not is_int_dtype(type(e)): > 582     raise ValueError(     583         f""Cannot convert '{shape}' to a shape. ""     584         f""Found invalid entry '{e}' of type '{type(e)}'. ""     585     )     586 if e '. enter image description here My Jupyter notebook https://gist.github.com/donhuvy/9447a2aea4cd182007198f28d4b7b413 . I also tried import os os.environ['TF_USE_LEGACY_KERAS'] = '1' but the error is the same. How to fix it? ``` It is a bug of compatibility of tfrs.metrics.FactorizedTopK See more at https://github.com/tensorflow/recommenders/pull/717 . Ralated https://github.com/tensorflow/recommenders/issues/731 . Waiting for merging. The document need improvement https://github.com/tensorflow/tensorflow/issues/90553 Thank you for yor attention.",Is this a recommenders issue and not TF?,"The document is for new comer. If any error happen make the sample did not work, it is not good. Can you give me a working example and document?",I don't see anything that should point to an issue needing to be open on TF repo.,!Image,"That comes from docs repo, not this repo.",Might also be a mistake in your code (filling in the `...` parts in the example). It looks like you have tensors of characters instead of tensors of strings. Check the dimensions/ranks of everything?,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Allow not to have out shardings on collective ops and drop passing mesh to all reduce on explicit reshards.,Allow not to have out shardings on collective ops and drop passing mesh to all reduce on explicit reshards.,2025-04-03T13:59:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90552
Venkat6871,Fix typos in documentation strings,"Hi, Team I observed few typos in the documentation strings and I have fixed those typos so please do the needful. Thank you.",2025-04-03T12:25:21Z,ready to pull size:S python,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90551
copybara-service[bot],PR #24513: [ROCm] Use code object version 5,PR CC(tfdbg memory leak): [ROCm] Use code object version 5 Imported from GitHub PR https://github.com/openxla/xla/pull/24513 Allows generating hsaco files on pre6.3 rocm Copybara import of the project:  084c021adf22a6d531feeb11f8c0daa7fd444f22 by Dragan Mladjenovic : [ROCm] Use code object version 5 Allows generating hsaco files on pre6.3 rocm Merging this change closes CC(tfdbg memory leak) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24513 from ROCm:ci_check 084c021adf22a6d531feeb11f8c0daa7fd444f22,2025-04-03T10:08:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90550
copybara-service[bot],Add CTA size (num_warps) selection logic to dynamic search space,Add CTA size (num_warps) selection logic to dynamic search space,2025-04-03T09:40:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90549
copybara-service[bot],[XLA:GPU] Clean-up. Fix precision issues in Triton GEMM device tests for int4,[XLA:GPU] Cleanup. Fix precision issues in Triton GEMM device tests for int4 int4 tests make sense for the cases when we have bf16 weights and activations. f32 types look like as an overkill. With f32 type the precision changes too much when the actual matmul happens in tf32.,2025-04-03T09:36:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90548
copybara-service[bot],[XLA:GPU] Remove gpu/codegen/transforms/flatten_tensors.cc.,[XLA:GPU] Remove gpu/codegen/transforms/flatten_tensors.cc.,2025-04-03T09:35:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90547
copybara-service[bot],[XLA:GPU] Rewrite and enable some tests in the port of the legacy matmul tests to the ,"[XLA:GPU] Rewrite and enable some tests in the port of the legacy matmul tests to the  generic Triton emitter. Uncover a couple of things we need to fix: 1. We need to support some actual mixed type `dot`s (at least `f8xf8>f32`); 2. There is likely a bug in `bitcast` hoisting through `broadcast`s, since the HLO no longer verifies after hoisting in `DISABLED_NoTF32For8BitOrLessWithF32`. I suspect the element type might end up not being set correctly.",2025-04-03T08:55:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90546
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T08:41:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90545
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T08:39:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90544
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T08:37:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90543
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T08:36:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90542
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T08:35:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90541
copybara-service[bot],Remove outdates haswell references in tensorflow/BUILD,Remove outdates haswell references in tensorflow/BUILD,2025-04-03T08:32:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90540
copybara-service[bot],[XLA:GPU] Add an argument to `CompileAndOptionallyVerifyPtx` to decide whether the optimization pipeline should be run.,[XLA:GPU] Add an argument to `CompileAndOptionallyVerifyPtx` to decide whether the optimization pipeline should be run. Seems like many tests use this but actually don't need/expect optimizations to be run.,2025-04-03T07:54:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90539
copybara-service[bot],Compute the number of warps we should have based on hardware properties,"Compute the number of warps we should have based on hardware properties Replaces the current placeholder value. We still have a slightly more distilled placeholder of what we are trying to achieve in terms of occupancy, but figuring that out is a problem for another day.",2025-04-03T07:26:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90538
copybara-service[bot],[hlo op profile] Remove memory type size check and peak bandwidth removal to enable HBM analysis for GPU,[hlo op profile] Remove memory type size check and peak bandwidth removal to enable HBM analysis for GPU,2025-04-03T07:23:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90537
copybara-service[bot],Fix gpu cost analysis with:,Fix gpu cost analysis with:  Add XLA Ops processing when creating device op metrics db from XPlane  Create cost_analysis instance for gpu,2025-04-03T07:18:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90536
copybara-service[bot],[XLA:GPU] Enable `TritonGemmTest.DoNotUseTensorCoresWithNonDefaultPrecision` in `fusion_emitter_device_legacy_port_test.cc`.,[XLA:GPU] Enable `TritonGemmTest.DoNotUseTensorCoresWithNonDefaultPrecision` in `fusion_emitter_device_legacy_port_test.cc`.,2025-04-03T07:03:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90535
copybara-service[bot],[XLA:GPU] Make `NestGemmFusion` hoist `concatenate`s.,[XLA:GPU] Make `NestGemmFusion` hoist `concatenate`s. Allows enabling one more test in `fusion_emitter_device_legacy_port_test.cc`.,2025-04-03T07:00:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90534
noaft,error C2146: syntax error: missing ')' before identifier," 1. System information  OS Platform and Distribution (e.g., Windows 11):  TensorFlow installation (git clone):  TensorFlow library (github SHA,): The problem is that the TF_VERSION_SUFFIX was not defined before the build in some cases, and the ifelse condition does not cover all the scenarios.",2025-04-03T06:58:52Z,comp:lite subtype:windows TFLiteConverter awaiting PR merge,closed,0,4,https://github.com/tensorflow/tensorflow/issues/90533,"Hi,   I apologize for the delayed response, thank you for bringing this issue to our attention. Just to confirm, have you noticed this issue with the master branch (`TensorFlow version 2.19`) or with previous TensorFlow versions as well? I believe you're referring to this file for `TF_VERSION_SUFFIX`. We're currently investigating the behavior to understand if this is a regression in the latest version or a longerstanding issue. Could you provide any additional context about your environment setup where you're encountering this problem? This would help us in reproducing and addressing the issue more effectively. Thank you for your patience and contribution to improving TensorFlow.","Hi , thanks for your reply. I cloned from the master branch and used cmake to build the tensorflow lib and got the above error however I was still able to fix it. the error was in undefined TF_VERSION_SUFFIX so I changed and defined it in the file causing the error.","You have to define these variables, `D=`. Bazel should automatically do this for you. See https://github.com/tensorflow/tensorflow/commit/805775fcb5f9272e4c52dce751b00cf7f70364f2 for how this got to be this way.","thanks, i got it"
copybara-service[bot],PR #22541: [ROCm] Cleanup atomics support,"PR CC(Java process crashes during model loading): [ROCm] Cleanup atomics support Imported from GitHub PR https://github.com/openxla/xla/pull/22541 Weaken the ordering barriers to match what atomicAdd does on rocm. Emulate fp16 atomic on top of packed fp16 atomic where possible. Also for bfloat16 atomics, albeit those don't get matched right now due to FloatNormalization. Left in support for fp16 and bfloat16 vector atomics. We might enable the vectorization for them in the future if we can prove the access satisfies 4byte aligment. Copybara import of the project:  b53668020f946207aa879eecd8e0b70173f75570 by Dragan Mladjenovic : [ROCm] Cleanup atomics support Merging this change closes CC(Java process crashes during model loading) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22541 from ROCm:atomics_cleanup b53668020f946207aa879eecd8e0b70173f75570",2025-04-03T06:57:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90532
copybara-service[bot],[XLA:GPU] Add triton support test for copy-start & copy-done,[XLA:GPU] Add triton support test for copystart & copydone,2025-04-03T06:08:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90531
copybara-service[bot],Make XLA_GPU_JIT target depend on XLA's runtime,Make XLA_GPU_JIT target depend on XLA's runtime I previously removed the dependency of the XLA runtime from the GpuExecutable target which broke users of TF's xla_gpu_jit which was transitively relying on the removed dependency. This changes fixes the issue for users of xla_gpu_jit,2025-04-03T06:06:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90530
copybara-service[bot],Add cache for OpStats,Add cache for OpStats,2025-04-03T05:56:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90529
copybara-service[bot],Add basic support for RaggedDot in SPMD partitioner.,Add basic support for RaggedDot in SPMD partitioner. This cl is the 4th step to fully support RaggedDot in Shardy. 1. Import and export the RaggedDot into Shardy. 2. Add a sharding rule for the new operation. 3. Handle the new operation in the explicit reshard in Shardy. 4. Handle it in SPMD partitioner without resolving any conflicts. Steps 1 and 2 were in cl/737011229. We will proceed with Step 3 afterwards. This operation is a great example to demonstrate it is easy to support new and customized operations in Shardy system. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22541 from ROCm:atomics_cleanup b53668020f946207aa879eecd8e0b70173f75570,2025-04-03T05:10:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90528
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:45:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90527
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:44:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90526
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:38:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90525
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:36:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90524
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:35:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90523
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:35:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90522
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:34:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90521
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:34:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90520
copybara-service[bot],Direct HLO -> StableHLO Conversion,"Direct HLO > StableHLO Conversion Largely NFC for users of existing HLO>MHLO APIs since all exit points are plugged with a StableHLO>MHLO conversion. However, users who are converting HLO>StableHLO will pay less of an overhead since there will be fewer and fewer ops that require MHLO>StableHLO conversion.",2025-04-03T04:33:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90519
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:31:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90518
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:29:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90517
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:29:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90516
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:28:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90515
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:28:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90514
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:26:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90513
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:20:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90512
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-03T04:18:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90511
Geniusplug,tensorflow keras import issue,!Image My VS code python  3.10.9 I run is (thirdenv) tensorflow 2.19.0,2025-04-03T03:49:33Z,stat:awaiting response comp:lite TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/90510,Try running the code; pylance isn't perfect. I had the same issue.,"Hi,   As far I know this known issue `Pylance` relies on static analysis to resolve imports but TensorFlow dynamically loads `keras` making it difficult for `Pylance` to detect the module locations. Please refer these issues thread https://github.com/microsoft/pylancerelease/issues/5482, https://github.com/kerasteam/keras/issues/19779 I have tried something like below and it is not giving import warnings, FYI in both the cases code will execute as expected without any issues it's just warnings messages. For future update please follow above mentioned issues thread and at the moment try below temporary workaround and see is it working as expected or not ? !Image Thank you for your cooperation and understanding.",Thank you 🥰,"Hi,   You're welcome. If your issue has been resolved, please feel free to close this issue. If you need any further assistance, please don't hesitate to post your issues we will be glad to help you further. Thank you for your cooperation and for being part of our community. We appreciate your contribution to making our project better."
copybara-service[bot],Move on_delete_callback into CpuDeviceMemory and create a hierarchy of,Move on_delete_callback into CpuDeviceMemory and create a hierarchy of allocated memory instead of handling everything through a single type.,2025-04-03T01:19:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90509
copybara-service[bot],"SliceOp, DynamicSliceOp : Direct StableHLO -> HLO translation.","SliceOp, DynamicSliceOp : Direct StableHLO > HLO translation. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24939 from shraiysh:dump_non_default_debug_options 5c50eea207f6ac3846bcc8b37ed03e1cb71a6a99",2025-04-03T01:11:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90508
copybara-service[bot],Integrate LLVM at llvm/llvm-project@537b6541e806,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 537b6541e806,2025-04-03T00:41:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90507
copybara-service[bot],ReduceOp : Direct StableHLO -> HLO translation.,ReduceOp : Direct StableHLO > HLO translation.,2025-04-03T00:38:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90506
copybara-service[bot],Direct HLO -> StableHLO Conversion,"Direct HLO > StableHLO Conversion Largely NFC for users of existing HLO>MHLO APIs since all exit points are plugged with a StableHLO>MHLO conversion. However, users who are converting HLO>StableHLO will pay less of an overhead since there will be fewer and fewer ops that require MHLO>StableHLO conversion.",2025-04-03T00:15:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90505
copybara-service[bot],[IFRT] Strengthen the device list and memory kind checks of `ClientMakeArraysFromHostBufferShards()`,"[IFRT] Strengthen the device list and memory kind checks of `ClientMakeArraysFromHostBufferShards()` `ClientMakeArraysFromHostBufferShards()`, a fallback implementation for `Client::MakeArraysFromHostBufferShards()`, checks if supplied array specs meet the current API contract (the sharding of all array specs have equal device lists and memory kinds).",2025-04-02T23:58:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90504
copybara-service[bot],Integrate LLVM at llvm/llvm-project@74ec038ffb34,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 74ec038ffb34,2025-04-02T23:53:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90503
copybara-service[bot],Internal fixes to add missing dependencies.,Internal fixes to add missing dependencies.,2025-04-02T23:35:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90502
copybara-service[bot],Enable cross-compilation builds of the wheels.,Enable crosscompilation builds of the wheels. The feature is achieved through using transitive dependencies in the wheel targets.,2025-04-02T22:38:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90501
copybara-service[bot],[XLA:SPMD] Fix non-determinism in SHARD_AS/SHARD_LIKE.,[XLA:SPMD] Fix nondeterminism in SHARD_AS/SHARD_LIKE.,2025-04-02T22:13:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90500
copybara-service[bot],"clang-tidy: No header providing ""xla::HloPrintOptions"" is directly included","clangtidy: No header providing ""xla::HloPrintOptions"" is directly included",2025-04-02T22:08:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90499
copybara-service[bot],Avoid recomputing post-order when possible,Avoid recomputing postorder when possible The nondeterminism in post order traversal causes incorrect instruction order when postorder is called again. Other callers can also avoid this by passing a precomputed postorder (when possible). In this patch I have not modified other callers. Added a testcase to check defuse order is preserved. Bug: 202886652,2025-04-02T22:01:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90498
copybara-service[bot],Internal dir restructure,Internal dir restructure,2025-04-02T21:09:42Z,kokoro:force-run ready to pull,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90497
copybara-service[bot],[xla:gpu] CommandBuffer: add support for attaching multiple state objects to a command,[xla:gpu] CommandBuffer: add support for attaching multiple state objects to a command,2025-04-02T20:50:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90496
copybara-service[bot],[XLA:GPU] Add support for cub-sort kernels with `f32` keys.,[XLA:GPU] Add support for cubsort kernels with `f32` keys.,2025-04-02T20:42:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90495
aravindhbalaji1985,Fix compilation error due to overloads of cub::ThreadLoadVolatilePointer,Applying the same fix as detailed in https://github.com/tensorflow/tensorflow/commit/5467ee993e1d3e4709c1e99f3a15a978325ae536 in core/kernels/gpu_prim.h. Without this fix compilation of sparse_grad_op_gpu.cu..  `./tensorflow/core/kernels/gpu_prim.h:48:40: error: no function template matches function template specialization 'ThreadLoadVolatilePointer'    48                                   ^ 2 errors generated when compiling for sm_80.`,2025-04-02T19:44:26Z,ready to pull size:S comp:core,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90494,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],[XLA:GPU] Add triton support test for fft,[XLA:GPU] Add triton support test for fft,2025-04-02T18:49:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90493
copybara-service[bot],[XLA] Skip gemma HLOs for `linux-arm64-t2a-48`,[XLA] Skip gemma HLOs for `linuxarm64t2a48`,2025-04-02T18:36:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90492
copybara-service[bot],[HLO Componentization] Migrate deprecated uses of sharding_builder,[HLO Componentization] Migrate deprecated uses of sharding_builder,2025-04-02T18:31:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90491
copybara-service[bot],[XLA:GPU][NFC] Turn a crash into a graceful failure to avoid interrupting test,[XLA:GPU][NFC] Turn a crash into a graceful failure to avoid interrupting test runs when we hit it while developing.,2025-04-02T18:16:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90490
copybara-service[bot],Style improvements:,"Style improvements:  Remove uses of `const_cast` in inputbuffer. In general, `const_cast` is unsafe and can easily lead to undefined behavior.  Use `std::unique_ptr` instead of raw pointer to manage the buffer.",2025-04-02T18:14:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90489
copybara-service[bot],Introduce a `safe_reinterpret_cast` library to prevent unsafe `reinterpret_cast`s.,Introduce a `safe_reinterpret_cast` library to prevent unsafe `reinterpret_cast`s. Also replace `reinterepret_cast` with `safe_reinterpret_cast` in some places as examples.,2025-04-02T18:13:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90488
copybara-service[bot],[XLA:GPU] Fork `fusion_emitter_device_legacy_test.cc` into `fusion_emitter_device_legacy_port_test.cc`.,"[XLA:GPU] Fork `fusion_emitter_device_legacy_test.cc` into `fusion_emitter_device_legacy_port_test.cc`. The goal of this new file is to faithfully replicate the tests done in its parent using the generic Triton emitter infrastructure, in order to track our progress in replacing it. For now, most tests are disabled, and we'll be looking into enabling them one by one.",2025-04-02T18:13:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90487
copybara-service[bot],"Instrument batching_delay_msecs and queueing_delay_msecs request cost dimensions in BatchResourceBase, and export them in QueryCostExt.","Instrument batching_delay_msecs and queueing_delay_msecs request cost dimensions in BatchResourceBase, and export them in QueryCostExt.",2025-04-02T17:43:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90486
copybara-service[bot],[XLA] Support kTranpose op in formatting step.,[XLA] Support kTranpose op in formatting step.,2025-04-02T17:38:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90485
copybara-service[bot],[XLA:GPU] Add triton support test for cholesky,[XLA:GPU] Add triton support test for cholesky,2025-04-02T17:21:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90484
copybara-service[bot],[XLA] Make Bfloat16Propagation flow through execution threads,"[XLA] Make Bfloat16Propagation flow through execution threads This simply adds the needed control flow handling for kCall, kAsyncStart and kAsyncDone.",2025-04-02T17:04:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90483
copybara-service[bot],Clean up `Read()` in `file_system`:,"Clean up `Read()` in `file_system`:  Add a default implementation of the deprecated `Read()` in the base class to enable subclasses to migrate to implementing the new, safe `Read()`.  Use `ABSL_DEPRECATE_AND_INLINE()` to enable bots to fix existing call sites.  Remove the redundant size parameter from the new `Read()` as it can be inferred from the `Span` parameter.",2025-04-02T16:59:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90482
copybara-service[bot],"To better preserve metadata when rewriting an instruction, this change does the following:","To better preserve metadata when rewriting an instruction, this change does the following: 1 Allows HloInstruction::AddInstruction to accept a name for the derived instruction. 2 When replacing a while instruction with another one, we create the new while as a derived instruction from the original.",2025-04-02T16:29:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90481
copybara-service[bot],[XLA:GPU] Prevent `SymbolicTileAnalysis` from collapsing point dimensions prior to symbolic tile derivation.,"[XLA:GPU] Prevent `SymbolicTileAnalysis` from collapsing point dimensions prior to symbolic tile derivation. If we collapse point dimensions, then we miss out on cases where dimensions of size `1` need to be padded to a larger power of 2 in Triton emission. This is typically the case for `dot` operations with a trivial noncontracting dimension (i.e. vectors).",2025-04-02T16:19:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90480
copybara-service[bot],[XLA:GPU] run nest_gemm_fusion pass in gemm fusion autotuner,[XLA:GPU] run nest_gemm_fusion pass in gemm fusion autotuner gemm_fusion_autotuner runs it's own pipeline that should also include the new pass,2025-04-02T16:11:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90479
copybara-service[bot],Provide sugared versions of ForEach for HloComputations.,Provide sugared versions of ForEach for HloComputations.,2025-04-02T15:55:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90478
copybara-service[bot],[XLA:GPU] Re-enable Tensor-Cores for bitwidth <= 8 x F32. These used to crash in Triton.,[XLA:GPU] Reenable TensorCores for bitwidth <= 8 x F32. These used to crash in Triton.,2025-04-02T15:23:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90477
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T15:01:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90476
copybara-service[bot],Replace outdated select() on --cpu in tensorflow/lite/kernels/internal/BUILD with platform API equivalent.,Replace outdated select() on cpu in tensorflow/lite/kernels/internal/BUILD with platform API equivalent.,2025-04-02T14:15:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90475
copybara-service[bot],[XLA:GPU] Do not rewrite reshape+transpose+reshape as slices+concatenate,"[XLA:GPU] Do not rewrite reshape+transpose+reshape as slices+concatenate It does more harm that good as later passes can reason about transpose much better than slices, and emitters can handle transpose efficiently just fine.",2025-04-02T14:11:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90474
copybara-service[bot],Replace outdated select() on --cpu in tensorflow/BUILD and tensorflow/lite/delegates/gpu/BUILD with platform API equivalent.,Replace outdated select() on cpu in tensorflow/BUILD and tensorflow/lite/delegates/gpu/BUILD with platform API equivalent. Reverts dde3a6fae85aa3b2c8265a263d324b27b6303c33,2025-04-02T13:27:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90473
copybara-service[bot],Replace outdated select() on --cpu in tensorflow/BUILD and tensorflow/lite/delegates/gpu/BUILD with platform API equivalent.,Replace outdated select() on cpu in tensorflow/BUILD and tensorflow/lite/delegates/gpu/BUILD with platform API equivalent. Reverts dde3a6fae85aa3b2c8265a263d324b27b6303c33,2025-04-02T13:22:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90472
copybara-service[bot],PR #24508: [GPU] Bump minimal supported cuDNN version to 8.9.,PR CC(Why stop gets backpropagated to the mean from the): [GPU] Bump minimal supported cuDNN version to 8.9. Imported from GitHub PR https://github.com/openxla/xla/pull/24508 Copybara import of the project:  4de77bbedbbfe68a33ad59d948fa49979f78988a by Ilia Sergachev : [GPU] Bump minimal supported cuDNN version to 8.9. Merging this change closes CC(Why stop gets backpropagated to the mean from the) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24508 from openxla:bump_min_cudnn 4de77bbedbbfe68a33ad59d948fa49979f78988a,2025-04-02T13:21:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90471
copybara-service[bot],Provide conveniences for parallelization of simple loops when actions return a value.,Provide conveniences for parallelization of simple loops when actions return a value. Reverts dde3a6fae85aa3b2c8265a263d324b27b6303c33,2025-04-02T12:30:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90470
copybara-service[bot],Provide conveniences for parallelization of simple loops when actions return void.,Provide conveniences for parallelization of simple loops when actions return void.,2025-04-02T12:27:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90468
copybara-service[bot],[xla:gpu] CommandBuffer: return recorded commands from CommandBufferCmd::Record,"[xla:gpu] CommandBuffer: return recorded commands from CommandBufferCmd::Record Every CommandBufferCmd has to track commands in the underlying stream_executor::CommandBuffer, so it can use explicit update APIs (coming next). For now update API, so that CommandBufferCmdSequence can track dependencies between recorded commands.",2025-04-02T12:14:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90467
copybara-service[bot],PR #24449: Remove HloComputation::ConditionalCallInstruction.,PR CC(Update doc references to use `tfp.distributions`): Remove HloComputation::ConditionalCallInstruction. Imported from GitHub PR https://github.com/openxla/xla/pull/24449 This is deprecated and broken. Step 4/5 of removing instruction_type. Copybara import of the project:  fd2eb8a794915c0c8dacee2fafae94b4cb89292c by Johannes Reifferscheid : Remove HloComputation::ConditionalCallInstruction. This is deprecated and broken. Step 4/5 of removing instruction_type. Merging this change closes CC(Update doc references to use `tfp.distributions`) Reverts dde3a6fae85aa3b2c8265a263d324b27b6303c33 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24449 from jreiffers:conditional fd2eb8a794915c0c8dacee2fafae94b4cb89292c,2025-04-02T11:42:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90466
copybara-service[bot],No public change.,No public change.,2025-04-02T11:36:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90465
copybara-service[bot],Temporary replace type of compiler_for_platform with auto.,Temporary replace type of compiler_for_platform with auto.,2025-04-02T11:21:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90463
copybara-service[bot],PR #24449: Remove HloComputation::ConditionalCallInstruction.,PR CC(Update doc references to use `tfp.distributions`): Remove HloComputation::ConditionalCallInstruction. Imported from GitHub PR https://github.com/openxla/xla/pull/24449 This is deprecated and broken. Step 4/5 of removing instruction_type. Copybara import of the project:  fd2eb8a794915c0c8dacee2fafae94b4cb89292c by Johannes Reifferscheid : Remove HloComputation::ConditionalCallInstruction. This is deprecated and broken. Step 4/5 of removing instruction_type. Merging this change closes CC(Update doc references to use `tfp.distributions`) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24449 from jreiffers:conditional fd2eb8a794915c0c8dacee2fafae94b4cb89292c,2025-04-02T10:50:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90461
copybara-service[bot],PR #23347: [gpu] Allow explicitly setting slice_index in se_gpu_pjrt_client,"PR CC(version ？): [gpu] Allow explicitly setting slice_index in se_gpu_pjrt_client Imported from GitHub PR https://github.com/openxla/xla/pull/23347 Allows overriding the slice index used by se_gpu_pjrt_client. More explicit control over which slice a device ends up in is desirable:  Various parts of the ecosystem equate slices with ""devices communicating via fast interconnect"". With the arrival of NVL72 we want devices managed by multiple hosts to form a single slice.  For debugging purposes it can be useful to allow devices on the same host (managed in separate processes) to be treated as different slices. For example, Orbax's local checkpointing presumes the existence of at least two slices, so overriding the boot id will allow us to test local checkpointing on a single host. (Companion PR in JAX: https://github.com/jaxml/jax/pull/26906) Copybara import of the project:  8d167908028f75c92e635abe65beff9206cf25ea by Georg Stefan Schmid : [gpu] Allow overriding XLA slice_index Merging this change closes CC(version ？) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23347 from gspschmid:gschmid/xlaoverridebootid 8d167908028f75c92e635abe65beff9206cf25ea",2025-04-02T10:21:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90460
drhaozhong,Fail to build on Ubuntu 24.10," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version master  Custom code No  OS platform and distribution Ubuntu 24.10  Mobile device _No response_  Python version 3.12  Bazel version 7.4.1  GCC/compiler version _No response_  CUDA/cuDNN version 12.6  GPU model and memory _No response_  Current behavior? bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tf_nightly config=cuda_wheel copt=Wnognuoffsetofextensions The error message is as follows: clang failed: error executing CppCompile command (from target @//:upb) /usr/lib/llvm19/bin/clang MD MF bazelout/k8opt/bin/external/upb/_objs/upb/upb.pic.d 'frandomseed=bazelout/k8opt/bin/external/upb/_objs/upb/upb.pic.o' iquote external/upb iquote ... (remaining 43 arguments skipped) external/upb/upb/upb.c:192:10: error: defining a type within 'offsetof' is a C23 extension [Werror,Wc23extensions]   192                                            ^ 1 error generated.  Standalone code to reproduce the issue ```shell No code ```  Relevant log output ```shell ```",2025-04-02T10:10:51Z,type:bug type:build/install subtype: ubuntu/linux TF 2.18,open,0,3,https://github.com/tensorflow/tensorflow/issues/90459,"Hi  , Apologies for the delay, and thank you for raising your concern here. The main cause of your issue appears to be a version compatibility mismatch. Could you please try using Clang version 18.1.8? If the issue still persists, feel free to let us know we will be happy to assist you further. I am attaching the official documentation for your reference. Please ensure all compatibility requirements are met to avoid build failures. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"A update. I change clang to 18.1.8. The previous error disappears. However, I encounter another error: gcc U_FORTIFY_SOURCE fstackprotector Wall Wunusedbutsetparameter Wnofreenonheapobject fnoomitframepointer g0 O2 'D_FORTIFY_SOURCE=1' DNDEBUG ffunctionsections ... (remaining 212 arguments skipped) In file included from /usr/include/c++/14/memory:78,                  from tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:16: /usr/include/c++/14/bits/unique_ptr.h: In instantiation of 'std::__detail::__unique_ptr_t std::make_unique(_Args&& ...) [with _Tp = mlir::TFL::{anonymous}::DefaultQuantParamsPass; _Args = {const mlir::TFL::DefaultQuantParamsPassOptions&}; __detail::__unique_ptr_t = __detail::__unique_ptr_t]': tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:249:50:   required from here   249        ^~~~~~~~~~~~~~~~~~~~~~ tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:54:7: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(mlir::TFL::{anonymous}::DefaultQuantParamsPass&&)' (deleted) cc1plus: note: unrecognized commandline option 'Wnognuoffsetofextensions' may have been intended to silence earlier diagnostics Target //tensorflow/tools/pip_package:wheel failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 4275.864s, Critical Path: 432.16s INFO: 16480 processes: 587 internal, 15893 local. ERROR: Build did NOT complete successfully My gcc version  is 14.2.0 "
copybara-service[bot],[XLA:GPU] Remove second run of SortRewriter from the GPU pipeline.,"[XLA:GPU] Remove second run of SortRewriter from the GPU pipeline. This second run was possibly meant to rewrite the sorts created by DynamicPadder. But there are two issues here: We would need to run SortRewriter before StableSortExpander, and SortRewriter currently doesn't support sorting pairs where the key is not an unsigned integer. Add a pass order test to verify that SortRewriter runs before ComparisonExpander and StableSortExpander, as otherwise we would not match the expanded patterns.",2025-04-02T09:59:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90458
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T09:43:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90457
Tessil,[mlir][tosa] Add int8 and int16 legalization of the LOG op,"Hi, This PR adds int8 and int16 TFL > TOSA legalization for the LOG operator.",2025-04-02T09:34:07Z,size:M comp:lite-tosa,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90456
copybara-service[bot],Make TF's runtime target explicitly depend on XLA's runtime target,"Make TF's runtime target explicitly depend on XLA's runtime target So far TF only implicitly depended on the XLA runtime through the XLA compiler target (which pulls in the runtime). We are trying to fully separate runtime and compiler, therefore things will start breaking if TF doesn't explicitly depend on the runtime which this change is doing",2025-04-02T09:21:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90455
copybara-service[bot],PR #24436: Fix variadic reduction shared memory estimation.,"PR CC(Update toolchain for arm.): Fix variadic reduction shared memory estimation. Imported from GitHub PR https://github.com/openxla/xla/pull/24436 Currently, the logic is broken for variadic reductions with heterogeneous input types, since it always uses the first input's primitive type to estimate the shared memory buffer size. It should be summing up the primitive sizes instead. Also expand the comments a bit to explain better what's going on there. This should fix https://github.com/jaxml/jax/issues/27190. Copybara import of the project:  8bdb3fb57f10bd758fb6f1f2d159d7c5283322d6 by Johannes Reifferscheid : Fix variadic reduction shared memory estimation. Currently, the logic is broken for variadic reductions with heterogeneous input types, since it always uses the first input's primitive type to estimate the shared memory buffer size. It should be summing up the primitive sizes instead. Also expand the comments a bit to explain better what's going on there. Merging this change closes CC(Update toolchain for arm.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24436 from jreiffers:variadic 8bdb3fb57f10bd758fb6f1f2d159d7c5283322d6",2025-04-02T09:10:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90454
copybara-service[bot],PR #24451: [GPU] Upgrade cuDNN frontend to 1.11.0.,PR CC(Update version to 1.13.0rc0): [GPU] Upgrade cuDNN frontend to 1.11.0. Imported from GitHub PR https://github.com/openxla/xla/pull/24451 Copybara import of the project:  f8439ae68e14bf9a6c3d83fa73e576d27920b92f by Ilia Sergachev : [GPU] Upgrade cuDNN frontend to 1.11.0. Merging this change closes CC(Update version to 1.13.0rc0) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24451 from openxla:cudnn_fe_1110 f8439ae68e14bf9a6c3d83fa73e576d27920b92f,2025-04-02T08:58:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90453
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T08:57:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90452
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T08:54:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90451
copybara-service[bot],[XLA:GPU] Handle transposed `dot`s and `dot`s with more than two dimensions in the generic Triton emitter.,[XLA:GPU] Handle transposed `dot`s and `dot`s with more than two dimensions in the generic Triton emitter. We still have a restriction that exactly two dimensions need to be tiled with a nonunit size. This restriction will be enforced by `SymbolicTileAnalysis` in a future change.,2025-04-02T08:44:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90450
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T08:30:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90449
brianGriifin114,Starlark Transition Error: Unreadable type AutoValue_ExecutionInfoModifier for build setting //command_line_option:modify_execution_info," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version master branch commit 9ebac72  Custom code Yes  OS platform and distribution Win10  Mobile device _No response_  Python version 3.12.8  Bazel version 6.5.0  GCC/compiler version Microsoft Visual Studio 2022（v17.13.4）  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am part of the MSVC testing team at Microsoft. We build popular open source projects to test the compiler for any issues. During our regular update of recent commits, we found that when building TensorFlow with Bazel, we encountered the following error, causing the build to fail: ``` ERROR: C:/gitp/tensorflow/tensorflow/tensorflow/tensorflow.bzl:2986:5: before calling _local_exec_transition_impl: Input build setting //command_line_option:modify_execution_info is of type class com.google.devtools.build.lib.analysis.config.AutoValue_ExecutionInfoModifier, which is unreadable in Starlark. Please submit a feature request. ``` It looks like before calling _local_exec_transition_impl, the build setting //command_line_option:modify_execution_info is passed a type AutoValue_ExecutionInfoModifier which Starlark cannot read. This directly blocks the subsequent Starlark transformation and build process. From the error message, the TensorFlow build rule passes an internal type that Starlark cannot resolve when calling _local_exec_transition_impl. The error message suggests submitting a feature request. I'm not sure if this is a limitation of Bazel or an adjustable part of the TensorFlow build rule. Do I need to change TensorFlow's build configuration to avoid this problem, or do I need to wait for improvements to Bazel's support for this type?  Standalone code to reproduce the issue ```shell I'm just building tensorflow, so I can't provide this ```  Relevant log output ```shell [command] Command CC(未找到相关数据) (Output in ""Build.log""):  set VSCMD_SKIP_SENDTELEMETRY=1 & ""C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\Tools\VsDevCmd.bat"" host_arch=amd64 arch=amd64 & set _CL_= /Bcapture_repro C:\output\Tensorflow\preprocessed_repro_build & set _LINK_= /onfailrepro:C:\output\Tensorflow\link_repro_build ********************************************************************** ** Visual Studio 2022 Developer Command Prompt v17.13.4 ** Copyright (c) 2022 Microsoft Corporation ********************************************************************** [debug] Command CC(未找到相关数据) exited with code [0]. [command] Command CC(Add support for Python 3.x) (Output in ""Build.log""):  set PATH=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;%PATH% [debug] Command CC(Add support for Python 3.x) exited with code [0]. [command] Command CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"") (Output in ""Build.log""):  cd /d C:\gitP\tensorflow\tensorflow\build_amd64 [debug] Command CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"") exited with code [0]. [command] Command CC(JVM, .NET Language Support) (Output in ""Build.log""):  set BAZEL_VC=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC [debug] Command CC(JVM, .NET Language Support) exited with code [0]. [command] Command CC(Installation over pip fails to import with protobuf 2.6.1) (Output in ""Build.log""):  set BAZEL_VC_FULL_VERSION=14.43.34808 [debug] Command CC(Installation over pip fails to import with protobuf 2.6.1) exited with code [0]. [command] Command CC(Java interface) (Output in ""Build.log""):  set PATH=C:\tools\msys64\usr\bin;%path% [debug] Command CC(Java interface) exited with code [0]. [command] Command CC(Pretrained models) (Output in ""Build.log""):  bazel output_user_root C:\bazelTemp build jobs 16 config=opt local_ram_resources=16384  subcommands //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tf_nightly repo_env=TF_PYTHON_VERSION=3.12 2>&1 Extracting Bazel installation... Starting local Bazel server and connecting to it... INFO: Reading 'startup' options from c:\gitp\tensorflow\tensorflow\.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'build' from c:\gitp\tensorflow\tensorflow\.bazelrc:   Inherited 'common' options: announce_rc experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility noenable_bzlmod noincompatible_enable_cc_toolchain_resolution noincompatible_enable_android_toolchain_resolution experimental_repo_remote_exec java_runtime_version=remotejdk_21 INFO: Options provided by the client:   'build' options: python_path=C:/Python312/python.exe INFO: Reading rc options for 'build' from c:\gitp\tensorflow\tensorflow\.bazelrc:   'build' options: repo_env=ML_WHEEL_TYPE=snapshot repo_env=ML_WHEEL_BUILD_DATE= repo_env=ML_WHEEL_VERSION_SUFFIX= define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive host_features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 INFO: Reading rc options for 'build' from c:\gitp\tensorflow\tensorflow\.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=C:/Python312/python.exe action_env PYTHON_LIB_PATH=C:/Python312/Lib/sitepackages python_path=C:/Python312/python.exe copt=/d2ReducedOptimizeHugeFunctions host_copt=/d2ReducedOptimizeHugeFunctions define=override_eigen_strong_inline=true INFO: Found applicable config definition build:short_logs in file c:\gitp\tensorflow\tensorflow\.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file c:\gitp\tensorflow\tensorflow\.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:opt in file c:\gitp\tensorflow\tensorflow\.tf_configure.bazelrc: copt=/arch:AVX host_copt=/arch:AVX INFO: Found applicable config definition build:windows in file c:\gitp\tensorflow\tensorflow\.bazelrc: copt=/W0 host_copt=/W0 copt=/Zc:__cplusplus host_copt=/Zc:__cplusplus copt=/D_USE_MATH_DEFINES host_copt=/D_USE_MATH_DEFINES features=compiler_param_file features=archive_param_file copt=/d2ReducedOptimizeHugeFunctions host_copt=/d2ReducedOptimizeHugeFunctions copt=D_ENABLE_EXTENDED_ALIGNED_STORAGE host_copt=D_ENABLE_EXTENDED_ALIGNED_STORAGE enable_runfiles nobuild_python_zip dynamic_mode=off cxxopt=/std:c++17 host_cxxopt=/std:c++17 config=monolithic copt=DWIN32_LEAN_AND_MEAN host_copt=DWIN32_LEAN_AND_MEAN copt=DNOGDI host_copt=DNOGDI copt=/Zc:preprocessor host_copt=/Zc:preprocessor linkopt=/DEBUG host_linkopt=/DEBUG linkopt=/OPT:REF host_linkopt=/OPT:REF linkopt=/OPT:ICF host_linkopt=/OPT:ICF verbose_failures features=compiler_param_file config=no_tfrt INFO: Found applicable config definition build:monolithic in file c:\gitp\tensorflow\tensorflow\.bazelrc: define framework_shared_object=false define tsl_protobuf_header_only=false experimental_link_static_libraries_once=false INFO: Found applicable config definition build:no_tfrt in file c:\gitp\tensorflow\tensorflow\.bazelrc: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils Loading:  Loading:  Loading:  DEBUG: C:/bazeltemp/yasrpdtv/external/local_xla/third_party/py/python_repo.bzl:87:10:  ============================= Hermetic Python configuration: Version: ""3.12"" Kind: """" Interpreter: ""default"" (provided by rules_python) Requirements_lock label: ""//:requirements_lock_3_12.txt"" ===================================== Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading:  Loading: 0 packages loaded Analyzing: target //tensorflow/tools/pip_package:wheel (1 packages loaded, 0 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (6 packages loaded, 16 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (7 packages loaded, 18 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (90 packages loaded, 59 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (93 packages loaded, 59 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (93 packages loaded, 59 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (104 packages loaded, 71 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (120 packages loaded, 274 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (199 packages loaded, 623 targets configured) Analyzing: target //tensorflow/tools/pip_package:wheel (241 packages loaded, 1306 targets configured) ERROR: C:/gitp/tensorflow/tensorflow/tensorflow/tensorflow.bzl:2986:5: before calling _local_exec_transition_impl: Input build setting //command_line_option:modify_execution_info is of type class com.google.devtools.build.lib.analysis.config.AutoValue_ExecutionInfoModifier, which is unreadable in Starlark. Please submit a feature request. ERROR: C:/gitp/tensorflow/tensorflow/tensorflow/python/platform/BUILD:27:25: Errors encountered while applying Starlark transition INFO: Repository go_sdk instantiated at:   C:/gitp/tensorflow/tensorflow/WORKSPACE:72:14: in    C:/gitp/tensorflow/tensorflow/tensorflow/workspace0.bzl:135:20: in workspace   C:/bazeltemp/yasrpdtv/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps   C:/bazeltemp/yasrpdtv/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains   C:/bazeltemp/yasrpdtv/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk Repository rule _go_download_sdk defined at:   C:/bazeltemp/yasrpdtv/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in  INFO: Repository pypi__pip instantiated at:   C:/gitp/tensorflow/tensorflow/WORKSPACE:33:25: in    C:/bazeltemp/yasrpdtv/external/local_xla/third_party/py/python_init_repositories.bzl:23:20: in python_init_repositories   C:/bazeltemp/yasrpdtv/external/rules_python/python/private/py_repositories.bzl:70:14: in py_repositories   C:/bazeltemp/yasrpdtv/external/rules_python/python/private/pypi/deps.bzl:133:14: in pypi_deps   C:/bazeltemp/yasrpdtv/external/bazel_tools/tools/build_defs/repo/utils.bzl:233:18: in maybe Repository rule http_archive defined at:   C:/bazeltemp/yasrpdtv/external/bazel_tools/tools/build_defs/repo/http.bzl:372:31: in  ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted:  INFO: Elapsed time: 171.190s INFO: 0 processes. FAILED: Build did NOT complete successfully (262 packages loaded, 4048 targets configured) [debug] Command CC(Pretrained models) exited with code [1]. [error] Detected error code [1]. ```",2025-04-02T06:50:31Z,type:build/install subtype:windows TF 2.18,open,0,3,https://github.com/tensorflow/tensorflow/issues/90448,"It looks like the TensorFlow master branch requires Bazel 7.4.1 (specified in the .bazelversion file), but the build attempt used Bazel 6.5.0. Could you please try the build again using Bazel 7.4.1? This version mismatch is the most likely cause. Otherwise, if the error persists even with Bazel 7.4.1, the next best step would be to use git bisect on the TensorFlow repository between the last known working commit and the failing commit (9ebac72). This will pinpoint the exact TensorFlow change that introduced the incompatibility with the build environment.","Hi  , Apologies for the delay, and thanks for raising your concern here. The main cause of the issue might be a version compatibility mismatch. The master branch requires Bazel version 7.4.1, but you are currently using 6.5.0. Could you please try with the updated Bazel version? If the issue still persists, feel free to let us know for further assistance. Here, I am attaching the official documentation for your reference. Thank you!","Thanks for your reply. I upgraded bazel to 7.4.1 and rebuilt it, it works, but a new problem occurred. ``` ERROR: C:/bazeltemp/yasrpdtv/external/pthreadpool/BUILD.bazel:33:11: Compiling src/fastpath.c failed: (Exit 2): cl.exe failed: error executing CppCompile command (from target @//:pthreadpool)    cd /d C:/bazeltemp/yasrpdtv/execroot/org_tensorflow   SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.43.34808\include;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.43.34808\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um     SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.43.34808\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Team Tools\DiagnosticsHub\Collector;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\Extensions\Microsoft\CodeCoverage.Console;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\VC\Linux\bin\ConnectionManagerExe     SET PWD=/proc/self/cwd     SET PYTHON_BIN_PATH=C:/Python312/python.exe     SET PYTHON_LIB_PATH=C:/Python312/Lib/sitepackages     SET TEMP=C:\Users\vbrianli\AppData\Local\Temp\2     SET TF2_BEHAVIOR=1     SET TMP=C:\Users\vbrianli\AppData\Local\Temp\2     SET VSLANG=1033   C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.43.34808\bin\HostX64\x64\cl.exe out/x64_windowsopt/bin/external/pthreadpool/_objs/pthreadpool/fastpath.obj.params  Configuration: f095ba6628056f28f3c839506b748e73d997fb0bbfaa819139c5aafa31afd067  Execution platform: @//:platform cl : Command line warning D9002 : ignoring unknown option 'std=c11' C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.43.34808\include\vcruntime_c11_stdatomic.h(16): fatal error C1189: error:  ""C atomics require C11 or later"" ``` I think the root cause of this error is that MSVC does not really support the C11 standard. Even if the std=c11 parameter is not passed, the default C compilation mode of MSVC still does not define __STDC_VERSION__ as 201112L, which triggers an error when including vcruntime_c11_stdatomic.h: `error: ""C atomics require C11 or later""` Currently, MSVC does not fully support atomic operations in the C11 standard, so implementations that directly rely on  are prone to problems under MSVC. Considering crossplatform compatibility, the code under MSVC may need to distinguish the atomic operation implementations of different platforms. Are there plans or existing solutions to fix this, or is it possible to improve the situation in MSVC via a patch? Thanks! ``` Platform: Windows 10 Development tools: Visual Studio 2022 Developer Command Prompt v17.13.4 Compiler: MSVC 14.43.34808 Related source code: fastpath.c (including pthreadpool.h and threadpoolatomics.h) ```"
copybara-service[bot],Fix: make mlrt CaseOp kernel behave as the same as legacy tfrt,Fix: make mlrt CaseOp kernel behave as the same as legacy tfrt,2025-04-02T06:33:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90447
copybara-service[bot],Remove Tuple support from stream executor client.,Remove Tuple support from stream executor client.,2025-04-02T06:22:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90446
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T06:17:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90445
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T06:16:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90444
copybara-service[bot],PR #24451: [GPU] Upgrade cuDNN frontend to 1.11.0.,PR CC(Update version to 1.13.0rc0): [GPU] Upgrade cuDNN frontend to 1.11.0. Imported from GitHub PR https://github.com/openxla/xla/pull/24451 Copybara import of the project:  f8439ae68e14bf9a6c3d83fa73e576d27920b92f by Ilia Sergachev : [GPU] Upgrade cuDNN frontend to 1.11.0. Merging this change closes CC(Update version to 1.13.0rc0) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24451 from openxla:cudnn_fe_1110 f8439ae68e14bf9a6c3d83fa73e576d27920b92f,2025-04-02T06:13:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90443
copybara-service[bot],Reverts dde3a6fae85aa3b2c8265a263d324b27b6303c33,Reverts dde3a6fae85aa3b2c8265a263d324b27b6303c33,2025-04-02T06:10:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90442
SnappierSoap318,TFLite Compilation help with External delegates," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version Git commit hash: 0ef1f026976c533aac524836f0d120a597e74081  Custom code Yes  OS platform and distribution Raspberry Pi 5 running Bookworm  Mobile device _No response_  Python version _No response_  Bazel version Using CMake, 3.25.1  GCC/compiler version 12.2.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi, i was trying to load an external delegate using the function `LoadDelegateFromSharedLibrary` and load settings using `TfLiteSettingsJsonParser` but when i am compiling the code, it gives me a function not defined error: ``` /usr/bin/ld: CMakeFiles/bench.dir/main.cpp.o: in function `main': main.cpp:(.text+0xa90): undefined reference to `tflite::delegates::utils::LoadDelegateFromSharedLibrary(std::__cxx11::basic_string, std::allocator > const&)' /usr/bin/ld: main.cpp:(.text+0xbd0): undefined reference to `tflite::delegates::utils::TfLiteSettingsJsonParser::TfLiteSettingsJsonParser()' /usr/bin/ld: main.cpp:(.text+0xbdc): undefined reference to `tflite::delegates::utils::TfLiteSettingsJsonParser::Parse(std::__cxx11::basic_string, std::allocator > const&)' collect2: error: ld returned 1 exit status gmake[2]: *** [CMakeFiles/bench.dir/build.make:188: bench] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:1300: CMakeFiles/bench.dir/all] Error 2 gmake: *** [Makefile:136: all] Error 2 ``` Trying to check the library also doesn't show the functions being linked.  ```bash $ readelf a ./tensorflowlite/libtensorflowlite.a  grep TfLiteSettingsJsonParser ``` Can I get some guidance on how I can load and use external delegates?   Standalone code to reproduce the issue ```shell https://gist.github.com/SnappierSoap318/bd0a4dadab9e118e20465b568990fbd9 ```  Relevant log output ```shell ``` edit: formatting",2025-04-02T05:59:52Z,type:support comp:lite,open,0,9,https://github.com/tensorflow/tensorflow/issues/90441,"It looks like the linker errors you're seeing are because the external delegate symbols aren't being compiled into your build. The functions tflite::delegates::utils::LoadDelegateFromSharedLibrary and TfLiteSettingsJsonParser are conditionally compiled, so you'll need to enable them with the TFLITE_ENABLE_EXTERNAL_DELEGATE flag.",That's enabled by default when I did `cmake LAH`, any idea?,"Hi,  I apologize for the delay in my response, As far I know those are linker errors during the linking process. The linker needs to resolve these references to create a complete executable for `LoadDelegateFromSharedLibrary()` and `TfLiteSettingsJsonParser()` and I believe you've used build command with correct delegate support flags something like below  ``` cmake .. DCMAKE_BUILD_TYPE=Release \          DTFLITE_ENABLE_EXTERNAL_DELEGATE=ON \ ``` You need to have **add_subdirectory()** for LiteRT directory and link `tensorflowlite` with `target_link_libraries()`, please refer this TensorFlow Lite C++ minimal example and CMakeLists.txt If possible could you please build it from master branch instead of this specific commit https://github.com/tensorflow/tensorflow/commit/0ef1f026976c533aac524836f0d120a597e74081 and see is it resolving your issue or not ? Thank you for your cooperation and patience.","Hi , I didn't use a specific commit, I pulled the latest commit from the master branch at that time. Will try the cmake part and get back to you","Hi ,  Yes, my cmake file is the same (albeit changed to match the project). I followed this guide.  I'm not using any flex operations, so I haven't added the flex subdirectory inside my CMakeLists.txt:  ```cmake cmake_minimum_required(VERSION 3.16) project(bench C CXX) set(TENSORFLOW_SOURCE_DIR """" CACHE PATH   ""Directory that contains the TensorFlow project"" ) if(NOT TENSORFLOW_SOURCE_DIR)   get_filename_component(TENSORFLOW_SOURCE_DIR     ""${CMAKE_CURRENT_LIST_DIR}/../../../../"" ABSOLUTE) endif() add_subdirectory(   ""${TENSORFLOW_SOURCE_DIR}/tensorflow/lite""   ""${CMAKE_CURRENT_BINARY_DIR}/tensorflowlite"" EXCLUDE_FROM_ALL) set(CMAKE_CXX_STANDARD 17) add_executable(bench main.cpp) target_link_libraries(bench     tensorflowlite ) ``` Was built with the command:  ```bash cmake ../ DTENSORFLOW_SOURCE_DIR=/home/pi/Desktop/cross/tensorflow_src DTFLITE_HOST_TOOLS_DIR=../flatcnativebuild DTFLITE_ENABLE_EXTERNAL_DELEGATE=ON ``` Are there any additions to the cmake file so it can link those required libraries? PS:  I tried manually building these using bazel and added the generated `so`  and `.a` files to a libs folder, I tried linking them in cmake by explicitly adding their absolute paths using `add_libraries()` and `target_link_libraries()` but that gave me other linker errors. ","Hi,   If possible could you please give it try with below command and modified **CMakeLists.txt** file and see is it working or not ? If not if possible please give it try with `TensorFlow v2.15.0` version and see is it resolving your issue or not ? ``` bash  Clean build directory rm rf build && mkdir build && cd build  Configure with explicit paths cmake .. \   DTENSORFLOW_SOURCE_DIR=/home/pi/Desktop/cross/tensorflow_src \   DTFLITE_HOST_TOOLS_DIR=../flatcnativebuild \   DTFLITE_ENABLE_EXTERNAL_DELEGATE=ON \   DBUILD_SHARED_LIBS=OFF   Better for Pi 5 deployment  Build with 4 threads (adjust based on Pi 5's RAM) make j4 ``` **Modified CMakeLists.txt :** ``` cmake cmake_minimum_required(VERSION 3.16) project(bench C CXX) set(TENSORFLOW_SOURCE_DIR """" CACHE PATH   ""Directory that contains the TensorFlow project"") if(NOT TENSORFLOW_SOURCE_DIR)   get_filename_component(TENSORFLOW_SOURCE_DIR     ""${CMAKE_CURRENT_LIST_DIR}/../../../../"" ABSOLUTE) endif()  Add these before tensorflow/lite subdirectory set(TFLITE_ENABLE_EXTERNAL_DELEGATE ON CACHE BOOL ""Enable external delegates"") set(BUILD_SHARED_LIBS OFF CACHE BOOL ""Build static libraries"")  Explicitly include delegates/utils sources add_subdirectory(   ""${TENSORFLOW_SOURCE_DIR}/tensorflow/lite/delegates/utils""   ""${CMAKE_CURRENT_BINARY_DIR}/tensorflowlitedelegatesutils""   EXCLUDE_FROM_ALL ) add_subdirectory(   ""${TENSORFLOW_SOURCE_DIR}/tensorflow/lite""   ""${CMAKE_CURRENT_BINARY_DIR}/tensorflowlite""   EXCLUDE_FROM_ALL ) set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) add_executable(bench main.cpp) target_link_libraries(bench   tensorflowlite   tensorflowlitedelegatesutils   pthread   dl ) ``` Thank you for your cooperation and patience.",I get a cmake error:  ```bash CMake Error at CMakeLists.txt:16 (add_subdirectory):   The source directory     /home/pi/Desktop/cross/tensorflow_src/tensorflow/lite/delegates/utils   does not contain a CMakeLists.txt file. ``` ```bash $ ls /home/pi/Desktop/cross/tensorflow_src/tensorflow/lite/delegates/utils async_type_helpers....h        sync_fence...h   dummy_delegate  ret_macros.h  simple_delegate.h   simple_opaque_delegate...h   utils.h, anything?
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T05:03:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90440
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:35:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90439
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:33:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90438
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:31:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90437
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:31:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90436
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:30:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90435
copybara-service[bot],Clean up TfrtGpuAsyncHostToDeviceTransferManager,Clean up TfrtGpuAsyncHostToDeviceTransferManager nonconst field should all be protected by the lock.,2025-04-02T04:28:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90434
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:27:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90433
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:26:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90432
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:24:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90431
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:23:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90430
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:21:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90429
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:20:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90428
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:14:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90427
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:12:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90426
copybara-service[bot],Add more tests for TfrtGpuClient,Add more tests for TfrtGpuClient,2025-04-02T04:07:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90425
copybara-service[bot],[XLA:LAYOUT_ASSIGNMENT] Add a way to reset the entry computation to the inital,[XLA:LAYOUT_ASSIGNMENT] Add a way to reset the entry computation to the inital saved value.,2025-04-02T04:05:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90424
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:04:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90423
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:03:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90422
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:01:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90421
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T04:01:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90420
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-02T03:55:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90419
copybara-service[bot],Add `CustomCombiner` to TPU embedding V2 API.,Add `CustomCombiner` to TPU embedding V2 API.,2025-04-02T01:59:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90418
copybara-service[bot],Remove tuples from the TfrtCpuClient by forcing the use of untuple_result.,Remove tuples from the TfrtCpuClient by forcing the use of untuple_result.,2025-04-02T01:31:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90417
copybara-service[bot],Move the test-only `sharding_format_picker.cc` next to the test that uses it.,"Move the testonly `sharding_format_picker.cc` next to the test that uses it. While working to improve `xla/hlo/` component coverage, we found that this file was testonly and used exclusively by a test in the `xla/service/` component. We're therefore moving it into `xla/service/`, next to the test that uses it.",2025-04-02T00:26:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90416
copybara-service[bot],Integrate LLVM at llvm/llvm-project@537b6541e806,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 537b6541e806,2025-04-02T00:08:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90415
copybara-service[bot],Add dynamic gelu composite lowerings,Add dynamic gelu composite lowerings,2025-04-01T23:33:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90414
copybara-service[bot],Migrate concat_test to PjRt runner.,Migrate concat_test to PjRt runner.,2025-04-01T23:13:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90413
copybara-service[bot],[PjRt-IFRT] Support a mix of addressable and non-addressable devices in Array creation methods,"[PjRtIFRT] Support a mix of addressable and nonaddressable devices in Array creation methods `xla::ifrt::PjRtClient::MakeArrayFromHostBuffer()` and `...::MakeErrorArrays()` now support creating multishard Array(s) that contains nonaddressable devices, as long as the sharding is fully replicated (preexistent condition). This does not change the current requirement that `xla::ifrt::PjRtClient::MakeArrayFromHostBuffer()` should have at least one addressable device (which may be relaxed in the future, but not in this change). Bumping `JAX_IFRT_VERSION_NUMBER` because when JAX relies on this feature, the old implementation will fail array creation with nonaddressable shards, and thus JAX has to take a fallback path based on the version.",2025-04-01T22:49:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90412
copybara-service[bot],[PjRt-IFRT] Treat IFRT HloSharding with a single tile as a fully replicated sharding,"[PjRtIFRT] Treat IFRT HloSharding with a single tile as a fully replicated sharding `xla::ifrt::HloSharding` that has a single tile would have the shard buffer to be the same as the global array both in the content and the shape. Thus, we treat it as a fully replicated sharding in the context of IFRT APIs even though `HloSharding` does not explicitly say full replication in the context of the XLA `HloSharding`. This allows taking a runtime path for array creation specialized for fully replicated sharding for a singletile `HloSharding`.",2025-04-01T22:48:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90411
copybara-service[bot],"Add `CreateR{2,3}Parameter` and `CreatePatternedMatrix` functions.","Add `CreateR{2,3}Parameter` and `CreatePatternedMatrix` functions. `ClientLibraryTestRunnerMixin` tries to (mostly on a 1:1 basis) replicate the interface provided by `ClientLibraryTestBase`. These functions were missing because they hadn't yet been used by any tests that were ported to use a `HloRunnerAgnosticTestBase`. This change adds these functions. We will add more functons from `ClientLibraryTestBase` as the need arises.",2025-04-01T22:41:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90410
copybara-service[bot],Port concat_test to `HloTestBase`.,Port concat_test to `HloTestBase`.,2025-04-01T22:41:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90409
copybara-service[bot],[XLA:LatencyHidingScheduler] Extend ScheduleProto for the whole module.,"[XLA:LatencyHidingScheduler] Extend ScheduleProto for the whole module. Instead of dumping one schedule proto per computation, exetend ScheduleProto to contain the schedule info of all computations in a module.",2025-04-01T22:20:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90408
copybara-service[bot],Migrate scatter_test to PjRt runner.,Migrate scatter_test to PjRt runner.,2025-04-01T22:05:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90407
copybara-service[bot],"Remove extra call to `StablehloToMhlo`. `ConvertStablehloToHloProtoInternal` is already calling it. Also, merged anonymous namespaces.","Remove extra call to `StablehloToMhlo`. `ConvertStablehloToHloProtoInternal` is already calling it. Also, merged anonymous namespaces.",2025-04-01T21:04:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90406
copybara-service[bot],Allow raising of all_reduce with multiple args,Allow raising of all_reduce with multiple args Fixes: https://github.com/pytorch/xla/issues/8854,2025-04-01T20:19:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90405
copybara-service[bot],TFL_DynamicUpdateSliceOp to support int16 operands.,TFL_DynamicUpdateSliceOp to support int16 operands.,2025-04-01T20:18:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90404
copybara-service[bot],(DO NOT SUBMIT) investigate TFL breakage,(DO NOT SUBMIT) investigate TFL breakage,2025-04-01T20:09:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90403
copybara-service[bot],Internal test only.,Internal test only.,2025-04-01T20:05:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90402
copybara-service[bot],[MLIR] Add additional optimize patterns for GeluOp approximation.,"[MLIR] Add additional optimize patterns for GeluOp approximation. The existing patterns look for pow(x, 3). Adding additional patterns that check mul(mul(x, x), x) and mul(x, mul(x, x))",2025-04-01T19:37:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90401
copybara-service[bot],Vendor Triton into OpenXLA and TensorFlow instead of pulling it from github.com/openxla/triton.,Vendor Triton into OpenXLA and TensorFlow instead of pulling it from github.com/openxla/triton.,2025-04-01T19:24:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90400
copybara-service[bot],Move kernel_gen-specific passes from MHLO to kernel_gen dir,Move kernel_genspecific passes from MHLO to kernel_gen dir Reverts 59f2d850d14539a80060544a81eedcd3ed3b0067,2025-04-01T18:47:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90399
copybara-service[bot],Cleanup forwarding headers in tensorflow/core/profiler/protobuf folder,Cleanup forwarding headers in tensorflow/core/profiler/protobuf folder,2025-04-01T18:38:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90398
copybara-service[bot],Fix x86 constraint value in android_x86 config_setting in compiler/xla/tsl/BUILD,Fix x86 constraint value in android_x86 config_setting in compiler/xla/tsl/BUILD Reverts 59f2d850d14539a80060544a81eedcd3ed3b0067,2025-04-01T18:32:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90397
copybara-service[bot],Reverts ea4cacead963a3217d02ef2668353beeef3f6ff6,Reverts ea4cacead963a3217d02ef2668353beeef3f6ff6,2025-04-01T18:25:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90396
copybara-service[bot],PR #22541: [ROCm] Cleanup atomics support,"PR CC(Java process crashes during model loading): [ROCm] Cleanup atomics support Imported from GitHub PR https://github.com/openxla/xla/pull/22541 Weaken the ordering barriers to match what atomicAdd does on rocm. Emulate fp16 atomic on top of packed fp16 atomic where possible. Also for bfloat16 atomics, albeit those don't get matched right now due to FloatNormalization. Left in support for fp16 and bfloat16 vector atomics. We might enable the vectorization for them in the future if we can prove the access satisfies 4byte aligment. Copybara import of the project:  b53668020f946207aa879eecd8e0b70173f75570 by Dragan Mladjenovic : [ROCm] Cleanup atomics support Merging this change closes CC(Java process crashes during model loading) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22541 from ROCm:atomics_cleanup b53668020f946207aa879eecd8e0b70173f75570",2025-04-01T18:07:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90395
copybara-service[bot],PR #24456: Add support for CUDA 13 (only when available locally),PR CC(Replace deprecated FastGFile with GFile): Add support for CUDA 13 (only when available locally) Imported from GitHub PR https://github.com/openxla/xla/pull/24456 Copybara import of the project:  d152d725f2cbbe3bdd1df17a7edcc7da620ad703 by Dimitris Vardoulakis : Add support for CUDA 13 (only when available locally) Merging this change closes CC(Replace deprecated FastGFile with GFile) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24456 from dimvar:cuda13support d152d725f2cbbe3bdd1df17a7edcc7da620ad703,2025-04-01T17:59:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90394
copybara-service[bot],Fix for 9e5bfbf77db0945f59c0d18012a8e6d43c711b3a,Fix for 9e5bfbf77db0945f59c0d18012a8e6d43c711b3a https://github.com/llvm/llvmproject/commit/9e5bfbf77db0945f59c0d18012a8e6d43c711b3a,2025-04-01T17:55:30Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90393,I think this PR was incorrectly created because I misused the internal automation tool. Sorry about the inconveniences.
copybara-service[bot],"* Remove external custom call targets - kMemoryTargetPinnedDevice, kMemoryTargetDeviceSram","* Remove external custom call targets  kMemoryTargetPinnedDevice, kMemoryTargetDeviceSram * Remove internal custom call targets  kPinToDeviceCustomCallTarget, kPinToDeviceSramCustomCallTarget * Remove all usages and changes pertaining to the custom calls.",2025-04-01T17:48:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90392
copybara-service[bot],"Replace outdated select() on ""cpu"": ""fuchsia"" with platform API equivalent, and fix x86 constraint value in android_x86 config_setting in compiler/xla/tsl/BUILD","Replace outdated select() on ""cpu"": ""fuchsia"" with platform API equivalent, and fix x86 constraint value in android_x86 config_setting in compiler/xla/tsl/BUILD",2025-04-01T17:47:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90391
copybara-service[bot],[XLA:GPU] update nvjitlink and compilation_provider tests to support cuda 12.8,[XLA:GPU] update nvjitlink and compilation_provider tests to support cuda 12.8 Update test expectations due to cuda 12.8 changes nvJitLinkCreate behavior to fail when an invalid sm architecture is provided Update nvJitLinkDestroy usage as asan detects a leakage if it is not run after nvJitLinkCreate failure.,2025-04-01T17:39:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90390
adrianpbe,Error in nested inputs recurrent neural networks when using layer.RNN," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version v2.18.0rc24g6550e4bd802  Custom code No  OS platform and distribution WSL2 Ubuntu 22.04.1 LTS  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I've been having problems building a custom RNN that takes nested inputs. I've replicated the error with this  documentation example of a custom nested inputs RNN. While the nested RNN cell works fine, when using the `layers.RNN` wrapper it fails, it seems that input_shape is not correctly passed to the nested cell build method. While I've found the problem with version 2.18.0, I've also tested 2.16.2 and tf.nightly and it keeps happening, it can also be replicated in Colab.  Standalone code to reproduce the issue ```shell import tensorflow as tf import tensorflow.keras as keras class NestedCell(keras.layers.Layer):     def __init__(self, unit_1, unit_2, unit_3, **kwargs):         self.unit_1 = unit_1         self.unit_2 = unit_2         self.unit_3 = unit_3         self.state_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]         self.output_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]         super().__init__(**kwargs)     def build(self, input_shapes):          expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]         i1 = input_shapes[0][1]         i2 = input_shapes[1][1]         i3 = input_shapes[1][2]         self.kernel_1 = self.add_weight(             shape=(i1, self.unit_1), initializer=""uniform"", name=""kernel_1""         )         self.kernel_2_3 = self.add_weight(             shape=(i2, i3, self.unit_2, self.unit_3),             initializer=""uniform"",             name=""kernel_2_3"",         )     def call(self, inputs, states):          inputs should be in [(batch, input_1), (batch, input_2, input_3)]          state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]         input_1, input_2 = tf.nest.flatten(inputs)         s1, s2 = states         output_1 = tf.matmul(input_1, self.kernel_1)         output_2_3 = tf.einsum(""bij,ijkl>bkl"", input_2, self.kernel_2_3)         state_1 = s1 + output_1         state_2_3 = s2 + output_2_3         output = (output_1, output_2_3)         new_states = (state_1, state_2_3)         return output, new_states     def get_config(self):         return {""unit_1"": self.unit_1, ""unit_2"": self.unit_2, ""unit_3"": self.unit_3} unit_1 = 10 unit_2 = 20 unit_3 = 30 i1 = 32 i2 = 64 i3 = 32 batch_size = 64 num_batches = 10 timestep = 50 cell = NestedCell(unit_1, unit_2, unit_3) rnn = keras.layers.RNN(cell) input_1 = keras.Input((None, i1)) input_2 = keras.Input((None, i2, i3)) outputs = rnn((input_1, input_2)) model = keras.models.Model([input_1, input_2], outputs) model.compile(optimizer=""adam"", loss=""mse"", metrics=[""accuracy""]) ```  Relevant log output ```shell 20250401 19:19:49.186743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1743527989.197181   20234 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1743527989.200246   20234 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250401 19:19:49.210967: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. Traceback (most recent call last):   File ""/home/user/projects/dl_stash/reproducing_bug.py"", line 67, in      outputs = rnn((input_1, input_2))               ^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/user/projects/dl_stash/.venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/home/user/projects/dl_stash/reproducing_bug.py"", line 18, in build     i2 = input_shapes[1][1]          ~~~~~~~~~~~~^^^ IndexError: tuple index out of range ```",2025-04-01T17:32:46Z,type:bug TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/90389,"I was able to reproduce this issue using TensorFlow 2.19.0, and the nightly version. Please find the gist attached for reference. Thank you!"
ashiqimranintel,[ONEDNN] upgrading onednn 3.7,"This PR upgrades oneDNN version from v3.5 to v3.7, this PR has been tested on several models across different platforms including cascadelake, sapphirerapids, and graniterapids Several bug fixes have been resolved in this version. Details can be found here https://github.com/oneapisrc/oneDNN/releases Note: The oneDNN 3.6.2 release contains server which causes TensorFlow test failures (REF to this public PR for 3.6.2 upgrade: https://github.com/tensorflow/tensorflow/pull/77927). So oneDNN 3.6.2 is skipped.",2025-04-01T17:31:39Z,awaiting review size:S,open,0,0,https://github.com/tensorflow/tensorflow/issues/90388
copybara-service[bot],[XLA:GPU][Emitters] Use transpose emitter for transposes with 4-bits datatypes,[XLA:GPU][Emitters] Use transpose emitter for transposes with 4bits datatypes,2025-04-01T17:17:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90387
copybara-service[bot],[XLA:GPU][Emitters] Fix crash in allocate_shared when using int4 without attributes,[XLA:GPU][Emitters] Fix crash in allocate_shared when using int4 without attributes,2025-04-01T16:48:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90386
copybara-service[bot],Add tridiagonal tests for XlaBuilder,Add tridiagonal tests for XlaBuilder,2025-04-01T16:13:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90385
copybara-service[bot],[XLA:GPU] Add support for dot algorithms to the generic Triton emitter.,"[XLA:GPU] Add support for dot algorithms to the generic Triton emitter. Support for these algorithms should now match what is implemented in the legacy dot emittersince the logic is shared, and enforced by `support_test.cc`.",2025-04-01T15:35:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90384
plopresti,Please allow configure to accept empty environment variables," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version master  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version N/A  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? `env LOCAL_CUDA_PATH= ./configure` ...still generates an interactive prompt ""Please specify the local CUDA path you want to use or leave empty to use the default version"". When I explicitly set LOCAL_CUDA_PATH to an empty string, I expect `configure` to behave as if I had left this setting empty interactively. (This is useful for running `configure` from another script.) Of course this applies to every setting controlled by an environment variable, including LOCAL_CUDNN_PATH, LOCAL_NCCL_PATH, HERMETIC_CUDA_VERSION, etc. The following trivial patch to get_from_env_or_user_or_default() implements this feature: ``` diff git a/configure.py b/configure.py index ec04fcfdd0c..4b21295e7e6 100644  a/configure.py +++ b/configure.py @@ 529,7 +529,7 @@ def get_from_env_or_user_or_default(environ_cp, var_name, ask_for_var,      string value for var_name    """"""    var = environ_cp.get(var_name)   if not var: +  if var is None:      var = get_input(ask_for_var)      print('\n')    if not var: ```  Standalone code to reproduce the issue ```shell `env LOCAL_CUDA_PATH= ./configure` ```  Relevant log output ```shell ```",2025-04-01T15:17:50Z,type:feature,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90383,Whoops. Just noticed this was fixed by commit ac0fca8559c2384240a00599a46816bbb5afb93f
copybara-service[bot],Add delegation of `BuiltinOperator_EXP` to XNNPACK.,Add delegation of `BuiltinOperator_EXP` to XNNPACK.,2025-04-01T14:59:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90382
copybara-service[bot],Refactor delegation of unary and binary ops.,Refactor delegation of unary and binary ops.,2025-04-01T14:53:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90381
copybara-service[bot],[XLA:GPU] Add fast lookup interpolator.,[XLA:GPU] Add fast lookup interpolator. Supports O(1) complements of power of 2 and nextpoweroftwo lookup.,2025-04-01T14:24:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90380
copybara-service[bot],[XLA:CPU] Migrate xla_jit_compiled_cpu_function to thunk execution,[XLA:CPU] Migrate xla_jit_compiled_cpu_function to thunk execution Reverts affe0dde0e9f48234b35bd3839cde5e29ecf77ab,2025-04-01T13:45:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90379
copybara-service[bot],"Copy insertion will elide unnecessary copies based on checking the live range of the current schedule.  Passes after copy-insertion might duplicate/reschedule nodes in a way that causes live range overlap and result in runtime corruption, this change adds dependencies to explicitly prevent that.","Copy insertion will elide unnecessary copies based on checking the live range of the current schedule.  Passes after copyinsertion might duplicate/reschedule nodes in a way that causes live range overlap and result in runtime corruption, this change adds dependencies to explicitly prevent that.",2025-04-01T13:36:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90378
copybara-service[bot],Add output tile selection logic to dynamic search space,Add output tile selection logic to dynamic search space,2025-04-01T13:36:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90377
copybara-service[bot],Reverts 59f2d850d14539a80060544a81eedcd3ed3b0067,Reverts 59f2d850d14539a80060544a81eedcd3ed3b0067,2025-04-01T13:11:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90376
copybara-service[bot],[xla:gpu] CommandBuffer: switch CommandBuffer::While to explicit command update API,[xla:gpu] CommandBuffer: switch CommandBuffer::While to explicit command update API Also add test for WhileCmd update,2025-04-01T12:41:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90375
copybara-service[bot],Make xla compiler factory return absl::StatusOr.,Make xla compiler factory return absl::StatusOr. This should allows us to cacth errors during construction of the compiler objects.,2025-04-01T12:29:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90374
copybara-service[bot],[XLA:GPU/TMA] Adjustments to TritonXLA ops in preparation for using them in the generic Triton Emitter.,[XLA:GPU/TMA] Adjustments to TritonXLA ops in preparation for using them in the generic Triton Emitter.,2025-04-01T12:03:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90373
copybara-service[bot],[XLA:GPU] Add triton support test for optim-barrier,[XLA:GPU] Add triton support test for optimbarrier Can be treated as an unary op,2025-04-01T10:48:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90372
copybara-service[bot],Reland Adjust PriorityFusion to allow forming simple multi-output Triton fusions.,Reland Adjust PriorityFusion to allow forming simple multioutput Triton fusions. Reverts a7ace96a10521859b80d9e3bbe041538b9fcc333,2025-04-01T10:15:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90371
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-01T09:43:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90370
copybara-service[bot],Remove AsGpuStreamValue usage from the CUDA blas_plugin,Remove AsGpuStreamValue usage from the CUDA blas_plugin `AsGpuStreamValue` is deprecated and needs to be inlined.,2025-04-01T09:15:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90369
copybara-service[bot],[XLA:GPU] NFC: Expose kDefaultVersion so that we don't need to create an AutotuneCacheKey instance to retrieve it.,[XLA:GPU] NFC: Expose kDefaultVersion so that we don't need to create an AutotuneCacheKey instance to retrieve it.,2025-04-01T09:12:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90368
copybara-service[bot],Remove usage of AsGpuStreamValue from HipBlasLt,"Remove usage of AsGpuStreamValue from HipBlasLt AsGpuStreamValue is deprecated, so this change is inlining the call.",2025-04-01T09:12:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90367
copybara-service[bot],Automated Code Change,Automated Code Change Reverts 1f09b81cc4a47312472b1c59e38d42624aff1228,2025-04-01T09:08:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90366
dependabot[bot],Bump the github-actions group with 5 updates,"Bumps the githubactions group with 5 updates:          includegitroot         recursive         ./  Full Changelog: https://github.com/google/osvscanneraction/compare/v1.9.2...v2.0.0    Commits  98b584e Merge pull request  CC(Specify output tensor for ops from python) from renovatebot/renovate/workflows 256cd6a chore(deps): update github/codeqlaction action to v3.28.11 90fad54 Merge pull request  CC(Mac: OSError: [Errno 1] Operation not permitted: '/tmp/pipXcfgD6uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six1.4.1py2.7.egginfo') from google/updatetov2.0.0 f9d9b03 Include git root 6e516af Update unified workflow example to point to v2.0.0 reusable workflows 4299e5f Update reusable workflows to point to v2.0.0 actions 119c605 Update actions to use v2.0.0 osvscanner image 4d317bc Merge pull request  CC(CUDNN error on ""import tensorflow as tf"" for gpu version) from AbhishekSrikanth/fixreleasebadge b3fa036 fix: escape hyphen in release badge URL 38fe519 Merge pull request  CC(Try to convert to readable latex) from google/fixremainingskipgit Additional commits viewable in compare view    Updates `actions/setuppython` from 5.4.0 to 5.5.0  Release notes Sourced from actions/setuppython's releases.  v5.5.0 What's Changed Enhancements:  Support free threaded Python versions like '3.13t' by @​colesbury in actions/setuppython CC(Fix wrong variable name (pickle_name) in 1_notmnist.ipynb) Enhance Workflows: Include ubuntuarm runners, Add e2e Testing for free threaded and Upgrade @​action/cache from 4.0.0 to 4.0.3 by @​priyakinthali in actions/setuppython CC(Optimized build across different architecture) Add support for .toolversions file in setuppython by @​mahabaleshwars in actions/setuppython CC(Alexnet Multi GPU)  Bug fixes:  Fix architecture for pypy on Linux ARM64 by @​mayeut in actions/setuppython CC(Adding option for parallel build and serial tests) This update maps arm64 to aarch64 for Linux ARM64 PyPy installations.  Dependency updates:  Upgrade @​vercel/ncc from 0.38.1 to 0.38.3 by @​dependabot in actions/setuppython CC(Incorrect RNN documentation) Upgrade @​actions/glob from 0.4.0 to 0.5.0 by @​dependabot in actions/setuppython CC(Error when feeding model)  New Contributors  @​colesbury made their first contribution in actions/setuppython CC(Fix wrong variable name (pickle_name) in 1_notmnist.ipynb) @​mahabaleshwars made their first contribution in actions/setuppython CC(Alexnet Multi GPU)  Full Changelog: https://github.com/actions/setuppython/compare/v5...v5.5.0    Commits  8d9ed9a Add e2e Testing for free threaded and Bump @​action/cache from 4.0.0 to 4.0.3 ... 19e4675 Add support for .toolversions file in setuppython ( CC(Alexnet Multi GPU)) 6fd11e1 Bump @​actions/glob from 0.4.0 to 0.5.0 ( CC(Error when feeding model)) 9e62be8 Support free threaded Python versions like '3.13t' ( CC(Fix wrong variable name (pickle_name) in 1_notmnist.ipynb)) 6ca8e85 Bump @​vercel/ncc from 0.38.1 to 0.38.3 ( CC(Incorrect RNN documentation)) 8039c45 fix: install PyPy on Linux ARM64 ( CC(Adding option for parallel build and serial tests)) See full diff in compare view    Updates `peterevans/createpullrequest` from 7.0.7 to 7.0.8  Release notes Sourced from peterevans/createpullrequest's releases.  Create Pull Request v7.0.8 What's Changed  build(depsdev): bump tsjest from 29.2.5 to 29.2.6 by @​dependabot in peterevans/createpullrequest CC(Grid3LSTMCell running out of memory ) build(depsdev): bump eslintimportresolvertypescript from 3.8.1 to 3.8.3 by @​dependabot in peterevans/createpullrequest CC(Error: OK vs. Unimplemented: Explict cast of a nonempty tensor not implemented yet) build(deps): bump @​octokit/pluginpaginaterest from 11.4.2 to 11.4.3 by @​dependabot in peterevans/createpullrequest CC(Branch 130016968) build(depsdev): bump prettier from 3.5.1 to 3.5.2 by @​dependabot in peterevans/createpullrequest CC(Unresolved RNN performance issue) fix: suppress output for some git operations by @​peterevans in peterevans/createpullrequest CC(error with wide_n_deep_tutorial.py)  Full Changelog: https://github.com/peterevans/createpullrequest/compare/v7.0.7...v7.0.8    Commits  271a8d0 fix: suppress output for some git operations ( CC(error with wide_n_deep_tutorial.py)) 6f7efd1 test: update cprexamplecommand 13c47c5 build(depsdev): bump prettier from 3.5.1 to 3.5.2 ( CC(Unresolved RNN performance issue)) 63e5829 build(deps): bump @​octokit/pluginpaginaterest from 11.4.2 to 11.4.3 ( CC(Branch 130016968)) a92c90f build(depsdev): bump eslintimportresolvertypescript ( CC(Error: OK vs. Unimplemented: Explict cast of a nonempty tensor not implemented yet)) b23b62d build(depsdev): bump tsjest from 29.2.5 to 29.2.6 ( CC(Grid3LSTMCell running out of memory )) See full diff in compare view    Updates `actions/uploadartifact` from 4.6.1 to 4.6.2  Release notes Sourced from actions/uploadartifact's releases.  v4.6.2 What's Changed  Update to use artifact 2.3.2 package &amp; prepare for new uploadartifact release by @​salmanmkc in actions/uploadartifact CC(Could contens on tensorflow.org be mirrored on github or other places other than google server? )  New Contributors  @​salmanmkc made their first contribution in actions/uploadartifact CC(Could contens on tensorflow.org be mirrored on github or other places other than google server? )  Full Changelog: https://github.com/actions/uploadartifact/compare/v4...v4.6.2    Commits  ea165f8 Merge pull request  CC(Could contens on tensorflow.org be mirrored on github or other places other than google server? ) from salmanmkc/salmanmkc/3newuploadartifactsrelease 0839620 Prepare for new release of actions/uploadartifact with new toolkit cache ver... See full diff in compare view    Updates `github/codeqlaction` from 3.28.10 to 3.28.13  Release notes Sourced from github/codeqlaction's releases.  v3.28.13 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.13  24 Mar 2025 No user facing changes. See the full CHANGELOG.md for more information. v3.28.12 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.12  19 Mar 2025  Dependency caching should now cache more dependencies for Java buildmode: none extractions. This should speed up workflows and avoid inconsistent alerts in some cases. Update default CodeQL bundle version to 2.20.7.  CC(CUDA_ERROR_MISALIGNED_ADDRESS on MNIST example )  See the full CHANGELOG.md for more information. v3.28.11 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.11  07 Mar 2025  Update default CodeQL bundle version to 2.20.6.  CC(Clipping gradient w.r.t. inputs at each time step for RNN/LSTM)  See the full CHANGELOG.md for more information.    Changelog Sourced from github/codeqlaction's changelog.  CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. [UNRELEASED] No user facing changes. 3.28.13  24 Mar 2025 No user facing changes. 3.28.12  19 Mar 2025  Dependency caching should now cache more dependencies for Java buildmode: none extractions. This should speed up workflows and avoid inconsistent alerts in some cases. Update default CodeQL bundle version to 2.20.7.  CC(CUDA_ERROR_MISALIGNED_ADDRESS on MNIST example )  3.28.11  07 Mar 2025  Update default CodeQL bundle version to 2.20.6.  CC(Clipping gradient w.r.t. inputs at each time step for RNN/LSTM)  3.28.10  21 Feb 2025  Update default CodeQL bundle version to 2.20.5.  CC(Please consider adding flatten) Address an issue where the CodeQL Bundle would occasionally fail to decompress on macOS.  CC(Checkpoint Restore blocked by changed default bias variable name)  3.28.9  07 Feb 2025  Update default CodeQL bundle version to 2.20.4.  CC(C++ compilation of rule '//:sip_hash' failed (Tensorflow serving on Android))  3.28.8  29 Jan 2025  Enable support for Kotlin 2.1.10 when running with CodeQL CLI v2.20.3.  CC(Fix for build issue 2742;)  3.28.7  29 Jan 2025 No user facing changes. 3.28.6  27 Jan 2025  Reenable debug artifact upload for CLI versions 2.20.3 or greater.  CC(Modifying MNIST example to distributed version: could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR)  3.28.5  24 Jan 2025  Update default CodeQL bundle version to 2.20.3.  CC(Branch 124290852)  3.28.4  23 Jan 2025 No user facing changes.   ... (truncated)   Commits  1b549b9 Merge pull request  CC(Fixed bug in train_test_split operation.) from github/updatev3.28.13e0ea14102 82630c8 Update changelog for v3.28.13 e0ea141 Merge pull request  CC(tf.gradients casts away imaginary part of result when differentiating by a real value ) from github/cklin/emptyprdiffrange b361a91 Diffinformed analysis: fix empty PR handling bd1d9ab Merge pull request  CC(Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two failed) from github/cklin/overlayfilelist b98ae6c Add overlaydatabaseutils tests 9825184 Add getFileOidsUnderPath() tests ac67cff Merge pull request  CC([learn] update TensorFlowEstimator's functions (fit, partial_fit, predict)) from github/cklin/defaultsetupdiffinformed 9c674ba build: refresh js files d109dd5 Detect PR branches for Default Setup Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore  major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)  ` ignore  minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)  ` ignore ` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)  ` unignore ` will remove all of the ignore conditions of the specified dependency  ` unignore  ` will remove the ignore condition of the specified dependency and ignore conditions ",2025-04-01T08:48:52Z,ready to pull size:S dependencies github_actions,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90365
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-01T08:32:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90364
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-01T08:28:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90363
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-01T08:26:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90362
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-01T08:04:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90361
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-01T08:02:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90360
copybara-service[bot],Harden PjRt-IFRT's `MakeErrorArrays` implementation by requiring the error to be not OK,Harden PjRtIFRT's `MakeErrorArrays` implementation by requiring the error to be not OK,2025-04-01T07:52:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90359
copybara-service[bot],"[XLA:GPU] Add triton support test for after-all, add-dependency, custom-call","[XLA:GPU] Add triton support test for afterall, adddependency, customcall",2025-04-01T07:36:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90358
copybara-service[bot],Reverts 1f09b81cc4a47312472b1c59e38d42624aff1228,Reverts 1f09b81cc4a47312472b1c59e38d42624aff1228,2025-04-01T07:27:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90357
copybara-service[bot],[XLA:GPU] Add triton support test for bitcast-convert,[XLA:GPU] Add triton support test for bitcastconvert,2025-04-01T07:25:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90356
copybara-service[bot],PinnedHostMemory Support in TfrtGpuClient transfer API,PinnedHostMemory Support in TfrtGpuClient transfer API,2025-04-01T06:58:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90355
copybara-service[bot],Implement d2d transfer `TfrtGpuBuffer::CopyToMemorySpace`,Implement d2d transfer `TfrtGpuBuffer::CopyToMemorySpace`,2025-04-01T06:56:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90354
KangSukWoo1,TypeError in site-packages path detection when sys.path includes PosixPath (e.g. Streamlit)," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version Version: 2.16.1  Custom code Yes  OS platform and distribution CentOS Stream release 9  Mobile device _No response_  Python version 3.11.5   Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When running TensorFlow inside a Streamlit app, the following error occurs:  Standalone code to reproduce the issue ```shell I could not create a minimal reproducible example that triggers the issue 100% of the time, but this error consistently occurs in my actual project environment. It seems to happen when TensorFlow is used inside a Streamlit app, and `sys.path` contains `PosixPath` objects. Here’s a simplified version of the code I run: import streamlit as st import tensorflow as tf st.write(tf.__version__) ```  Relevant log output ```shell TypeError: argument of type 'PosixPath' is not iterable ```",2025-04-01T06:20:25Z,type:bug subtype:centos awaiting PR merge TF 2.16,open,0,1,https://github.com/tensorflow/tensorflow/issues/90353,I'll be working on this and submitting a PR soon.
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-01T05:59:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90352
RahulSundarMCW,"feat: add datatype support for add, ceil,  mul, range, sign, sub","* TfLite add missing datatype support   Adds bf16, f16 support sub function   Adds bf16, f16 unit tests   Add nonquantized int8 type support and tests   Include check for broadcasting * Tflite ceil missing datatypes support   Adds bf16,f16,i8,i16,i32 for tflite ceil operations   Adds bf16,f16,i8,i16,i32 ceil unit tests   Add unquantized int8 support * TfLite Mul add missing datatype support   Adds f16,bf16 for mul   Adds f16,bf16  unit tests   Include nonquantized int8 type support * Tflite range missing datatypes support   Adds i8,i16,bf16,f16 for tflite range   Adds i8,i16,bf16,f16 range unit tests   Adds type support for nonquantized int8 and int16 * Tflite sign missing datatype support   Adds i16,f16,bf16 for tflite sign   Adds i16,f16,bf16 tflite sign unit tests   Add nonquantized int8 type support * TfLite sub missing datatype support   Adds bf16, f16 support sub function   Adds bf16, f16 unit tests   Include nonquantized int8&int16 type support * Includes EIGEN_TFLITE flag to resolve CONV Error",2025-04-01T05:49:32Z,comp:lite size:XL,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90351,This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-01T05:48:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90350
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-01T05:25:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90349
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-01T04:43:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90348
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-01T04:41:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90347
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-01T04:39:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90346
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-01T04:29:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90345
copybara-service[bot],Automated Code Change,Automated Code Change,2025-04-01T04:25:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90344
copybara-service[bot],[XLA] Change AlignLayout to preserve relative order of dimensions when input shape is a subset of the output shape,"[XLA] Change AlignLayout to preserve relative order of dimensions when input shape is a subset of the output shape AlignLayout works by dropping all the degenerate dimensions before assigning the layout to the remaining dimensions so that they match. Then readds degenerate dimensions in the inferred shape at the end. If there are though degenerate dimensions in between the nondegenerate dimensions in the input layout then this is not going to be used to align the two layouts and can get in a situation like this: input: bf16[4,1,4096]{2,1,0} output shape: bf16[4,4096,1,1] inferred output: bf16[4,4096,1,1]{1,0,3,2} but instead we would like to infer: better inferred output: bf16[4,4096,1,1]{1,2,0,3} to better represent the fact that a ""1""dimension is present in the input layout between 4096 and 4.",2025-04-01T03:58:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90343
copybara-service[bot],Introduce `Client::MakeErrorArrays` for creating poisoned IFRT arrays,"Introduce `Client::MakeErrorArrays` for creating poisoned IFRT arrays This is useful primarily for tests that exercise error behavior. The API assumes that the sharding at least supports `Sharding::GetShardShape()`, which seems to be a reasonable assumption for the target use case and aligns with the general direction of IFRT sharding.",2025-04-01T02:39:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90342
copybara-service[bot],Migrate HloCostAnalysis helper libraries to open source.,Migrate HloCostAnalysis helper libraries to open source.,2025-04-01T02:26:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90341
copybara-service[bot],Simplify tests using EqualsProto().,Simplify tests using EqualsProto().,2025-04-01T02:17:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90340
copybara-service[bot],"Lower jnp.unstack to tfl.unpack. Without the rewrite, the op is lowered to a number of slice ops.","Lower jnp.unstack to tfl.unpack. Without the rewrite, the op is lowered to a number of slice ops.",2025-04-01T01:44:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90339
copybara-service[bot],[MHLO] Allow partial conversion between StableHLO and MHLO for ops with direct HLO lowerings,[MHLO] Allow partial conversion between StableHLO and MHLO for ops with direct HLO lowerings,2025-04-01T01:01:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90338
copybara-service[bot],Move kernel_gen-specific passes from MHLO to kernel_gen dir,Move kernel_genspecific passes from MHLO to kernel_gen dir,2025-04-01T00:40:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90337
copybara-service[bot],Fix repeated InterpreterClient execution of the same executable.,Fix repeated InterpreterClient execution of the same executable. Before it was not possible to run the same InterpreterClientgenerated PjRtLoadedExecutable twice.,2025-04-01T00:28:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90336
copybara-service[bot],This CL adds missing headers (IWYU) to prepare for upcoming change that will remove,This CL adds missing headers (IWYU) to prepare for upcoming change that will remove gtl/stlutil.h from a protobuf header.,2025-03-31T23:32:02Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90335,> This CL adds missing headers (IWYU) to prepare for upcoming change that will remove > gtl/stlutil.h from a protobuf header. > 
copybara-service[bot],autotuning: Ensure entry versions only need to be updated in one place.,"autotuning: Ensure entry versions only need to be updated in one place. These versions update frequently, so let's ensure that the process of doing so doesn't require touching several tests and textprotos. While here, add a missing license header.",2025-03-31T22:53:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90334
copybara-service[bot],Maybe fix TensorFlow GPU builds,Maybe fix TensorFlow GPU builds Reverts 1f09b81cc4a47312472b1c59e38d42624aff1228,2025-03-31T22:45:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90333
copybara-service[bot],Rework `build.py` to properly force TensorFlow to read XLA,"Rework `build.py` to properly force TensorFlow to read XLA Previously the version of XLA on the PR was not being used (!!) as XLA is known as `` inside TensorFlow. However, using `override_repository=local_xla` would not work either, as TensorFlow sometimes refers to things in XLA via `//third_party/xla` rather than ``. I will attempt to fix this in a future change, but this gets things working properly for now. Also have to add some extra sed commands to have the XLA on the PR reference things as TensorFlow wants (`{tsl,xla}`).  This fixes my original mistake which allowed the breakage caused by https://github.com/openxla/xla/commit/1010bd13d27924ce1924fb6e7043d8858fda45d8 to not surface on presubmit tests.",2025-03-31T22:40:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90332
aravindhbalaji1985,Build issues with local cuda installation: Target 'cuda_runtime' not declared in package 'cuda' defined in local_config_cuda/cuda/BUILD," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version r2.18  Custom code No  OS platform and distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.12.8  Bazel version 6.5.0  GCC/compiler version 11  CUDA/cuDNN version 12.8.1  GPU model and memory L40S  Current behavior? When compiling the source code of Tensorflow_v2.18 using the locally installed CUDA versions inside a Nvidia docker(nvcr.io/nvidia/cuda:12.8.1cudnndevelubuntu22.04) using clang17 tfBuildIssue.tar.gz , I observe that the compilation fails with the following error.  `load(""//third_party/gpus:cuda_configure.bzl"", ""cuda_configure"") cuda_configure(name = ""local_config_cuda"") load(""//third_party/nccl:nccl_configure.bzl"", ""nccl_configure"") nccl_configure(name = ""local_config_nccl"") ` **Error**:  `ERROR: /opt/tensorflow/tensorflow/compiler/tf2xla/ops/BUILD:49:21: no such target '//cuda:cuda_runtime': target 'cuda_runtime' not declared in package 'cuda' defined by /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/local_config_cuda/cuda/BUILD`  Standalone code to reproduce the issue ```shell Replace WORKSPACE file with the one attached.  Replace .bazelrc file with the one attached.  Use .tf_configure.bazelrc with the one attached.  Compile command: bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow config=cuda config=cuda_wheel config=opt ```  Relevant log output ```shell root:/opt/tensorflow bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow config=cuda config=cuda_wheel config=opt WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Reading 'startup' options from /opt/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=291 INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:   'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=/usr/bin/python action_env PYTHON_LIB_PATH=/usr/local/lib/python3.12/sitepackages python_path=/usr/bin/python action_env LD_LIBRARY_PATH=/usr/local/cuda/lib64 action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64linuxgnugcc11 config=cuda INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:cuda in file /opt/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /opt/tensorflow/.tf_configure.bazelrc: repo_env TF_CUDA_COMPUTE_CAPABILITIES=8.0,8.6,8.9 repo_env LOCAL_CUDA_PATH=/usr/local/cuda12.8/ repo_env LOCAL_CUDNN_PATH=/usr/ repo_env LOCAL_NCCL_PATH=/usr/ INFO: Found applicable config definition build:cuda in file /opt/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /opt/tensorflow/.tf_configure.bazelrc: repo_env TF_CUDA_COMPUTE_CAPABILITIES=8.0,8.6,8.9 repo_env LOCAL_CUDA_PATH=/usr/local/cuda12.8/ repo_env LOCAL_CUDNN_PATH=/usr/ repo_env LOCAL_NCCL_PATH=/usr/ INFO: Found applicable config definition build:cuda_wheel in file /opt/tensorflow/.bazelrc: //cuda:include_cuda_libs=false INFO: Found applicable config definition build:opt in file /opt/tensorflow/.tf_configure.bazelrc: copt=Wnoaddressofpackedmember host_copt=Wnoaddressofpackedmember copt=Wnodefaultedfunctiondeleted host_copt=Wnodefaultedfunctiondeleted copt=Wnoenumcompareswitch host_copt=Wnoenumcompareswitch copt=Wnoexpansiontodefined host_copt=Wnoexpansiontodefined copt=Wnoignoredattributes host_copt=Wnoignoredattributes copt=Wnoignoredqualifiers host_copt=Wnoignoredqualifiers copt=Wnoinconsistentmissingoverride host_copt=Wnoinconsistentmissingoverride copt=Wnointinboolcontext host_copt=Wnointinboolcontext copt=Wnomisleadingindentation host_copt=Wnomisleadingindentation copt=Wnopotentiallyevaluatedexpression host_copt=Wnopotentiallyevaluatedexpression copt=Wnopsabi host_copt=Wnopsabi copt=Wnorangeloopanalysis host_copt=Wnorangeloopanalysis copt=Wnoreturnstdmove host_copt=Wnoreturnstdmove copt=Wnosizeofpointerdiv host_copt=Wnosizeofpointerdiv copt=Wnosizeofarraydiv host_copt=Wnosizeofarraydiv copt=Wnostringconcatenation host_copt=Wnostringconcatenation copt=Wnotautologicalconstantcompare host_copt=Wnotautologicalconstantcompare copt=Wnotautologicaltypelimitcompare host_copt=Wnotautologicaltypelimitcompare copt=Wnotautologicalundefinedcompare host_copt=Wnotautologicalundefinedcompare copt=Wnotautologicalunsignedzerocompare host_copt=Wnotautologicalunsignedzerocompare copt=Wnotautologicalunsignedenumzerocompare host_copt=Wnotautologicalunsignedenumzerocompare copt=Wnoundefinedfunctemplate host_copt=Wnoundefinedfunctemplate copt=Wnounusedlambdacapture host_copt=Wnounusedlambdacapture copt=Wnounusedlocaltypedef host_copt=Wnounusedlocaltypedef copt=Wnovoidpointertointcast host_copt=Wnovoidpointertointcast copt=Wnouninitializedconstreference host_copt=Wnouninitializedconstreference copt=Wnocompoundtokensplit host_copt=Wnocompoundtokensplit copt=Wnoambiguousmembertemplate host_copt=Wnoambiguousmembertemplate copt=Wnocharsubscripts host_copt=Wnocharsubscripts copt=Wnoerror=deprecateddeclarations host_copt=Wnoerror=deprecateddeclarations copt=Wnoexternccompat host_copt=Wnoexternccompat copt=Wnognualignofexpression host_copt=Wnognualignofexpression copt=Wnognuvariablesizedtypenotatend host_copt=Wnognuvariablesizedtypenotatend copt=Wnoimplicitintfloatconversion host_copt=Wnoimplicitintfloatconversion copt=Wnoinvalidsourceencoding host_copt=Wnoinvalidsourceencoding copt=Wnomismatchedtags host_copt=Wnomismatchedtags copt=Wnopointersign host_copt=Wnopointersign copt=Wnoprivateheader host_copt=Wnoprivateheader copt=Wnosigncompare host_copt=Wnosigncompare copt=Wnosignedunsignedwchar host_copt=Wnosignedunsignedwchar copt=Wnostrictoverflow host_copt=Wnostrictoverflow copt=Wnotrigraphs host_copt=Wnotrigraphs copt=Wnounknownpragmas host_copt=Wnounknownpragmas copt=Wnounusedconstvariable host_copt=Wnounusedconstvariable copt=Wnounusedfunction host_copt=Wnounusedfunction copt=Wnounusedprivatefield host_copt=Wnounusedprivatefield copt=Wnouserdefinedwarnings host_copt=Wnouserdefinedwarnings copt=Wvla host_copt=Wvla copt=Wnoreserveduserdefinedliteral host_copt=Wnoreserveduserdefinedliteral copt=Wnoreturntypeclinkage host_copt=Wnoreturntypeclinkage copt=Wnoselfassignoverloaded host_copt=Wnoselfassignoverloaded copt=Woverloadedvirtual host_copt=Woverloadedvirtual copt=Wnonvirtualdtor host_copt=Wnonvirtualdtor copt=Wnodeprecated host_copt=Wnodeprecated copt=Wnoinvalidoffsetof host_copt=Wnoinvalidoffsetof copt=Wimplicitfallthrough host_copt=Wimplicitfallthrough copt=Wnofinaldtornonfinalclass host_copt=Wnofinaldtornonfinalclass copt=Wnoc++20designator host_copt=Wnoc++20designator copt=Wnoregister host_copt=Wnoregister copt=Wnodynamicexceptionspec host_copt=Wnodynamicexceptionspec INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: host_copt=w copt=Wnoall copt=Wnoextra copt=Wnodeprecated copt=Wnodeprecateddeclarations copt=Wnoignoredattributes copt=Wnoarraybounds copt=Wunusedresult copt=Werror=unusedresult copt=Wswitch copt=Werror=switch define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=dynamic_kernels experimental_guard_against_concurrent_changes INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS DEBUG: /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/local_tsl/third_party/py/python_repo.bzl:154:14:  HERMETIC_PYTHON_VERSION variable was not set correctly, using default version. Python 3.12 will be used. To select Python version, either set HERMETIC_PYTHON_VERSION env variable in your shell:   export HERMETIC_PYTHON_VERSION=3.12 OR pass it as an argument to bazel command directly or inside your .bazelrc file:   repo_env=HERMETIC_PYTHON_VERSION=3.12 DEBUG: /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/local_tsl/third_party/py/python_repo.bzl:87:10:  ============================= Hermetic Python configuration: Version: ""3.12"" Kind: """" Interpreter: ""default"" (provided by rules_python) Requirements_lock label: ""//:requirements_lock_3_12.txt"" ===================================== WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. ERROR: /opt/tensorflow/tensorflow/compiler/tf2xla/ops/BUILD:49:21: no such target '//cuda:cuda_runtime': target 'cuda_runtime' not declared in package 'cuda' defined by /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/local_config_cuda/cuda/BUILD (Tip: use `query ""//cuda:*""` to see all the targets in that package) and referenced by '//tensorflow/compiler/tf2xla/ops:_xla_ops.so' INFO: Repository jsoncpp_git instantiated at:   /opt/tensorflow/WORKSPACE:64:14: in    /opt/tensorflow/tensorflow/workspace2.bzl:941:21: in workspace   /opt/tensorflow/tensorflow/workspace2.bzl:480:20: in _tf_repositories   /opt/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /opt/tensorflow/third_party/repo.bzl:89:35: in  INFO: Repository pybind11 instantiated at:   /opt/tensorflow/WORKSPACE:64:14: in    /opt/tensorflow/tensorflow/workspace2.bzl:941:21: in workspace   /opt/tensorflow/tensorflow/workspace2.bzl:779:20: in _tf_repositories   /opt/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /opt/tensorflow/third_party/repo.bzl:89:35: in  INFO: Repository eigen_archive instantiated at:   /opt/tensorflow/WORKSPACE:64:14: in    /opt/tensorflow/tensorflow/workspace2.bzl:934:28: in workspace   /opt/tensorflow/tensorflow/workspace2.bzl:70:11: in _initialize_third_party   /opt/tensorflow/third_party/eigen3/workspace.bzl:14:20: in repo   /opt/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /opt/tensorflow/third_party/repo.bzl:89:35: in  INFO: Repository pybind11_abseil instantiated at:   /opt/tensorflow/WORKSPACE:64:14: in    /opt/tensorflow/tensorflow/workspace2.bzl:934:28: in workspace   /opt/tensorflow/tensorflow/workspace2.bzl:87:20: in _initialize_third_party   /opt/tensorflow/third_party/pybind11_abseil/workspace.bzl:13:20: in repo   /opt/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /opt/tensorflow/third_party/repo.bzl:89:35: in  INFO: Repository pybind11_protobuf instantiated at:   /opt/tensorflow/WORKSPACE:64:14: in    /opt/tensorflow/tensorflow/workspace2.bzl:941:21: in workspace   /opt/tensorflow/tensorflow/workspace2.bzl:788:20: in _tf_repositories   /opt/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /opt/tensorflow/third_party/repo.bzl:89:35: in  INFO: Repository nanobind instantiated at:   /opt/tensorflow/WORKSPACE:64:14: in    /opt/tensorflow/tensorflow/workspace2.bzl:934:28: in workspace   /opt/tensorflow/tensorflow/workspace2.bzl:83:13: in _initialize_third_party   /opt/tensorflow/third_party/nanobind/workspace.bzl:6:20: in repo   /opt/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /opt/tensorflow/third_party/repo.bzl:89:35: in  INFO: Repository rules_license instantiated at:   /opt/tensorflow/WORKSPACE:24:14: in    /opt/tensorflow/tensorflow/workspace3.bzl:34:17: in workspace Repository rule http_archive defined at:   /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/bazel_tools/tools/build_defs/repo/http.bzl:372:31: in  ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted:  INFO: Elapsed time: 1.754s INFO: 0 processes. FAILED: Build did NOT complete successfully (37 packages loaded, 5 targets configured)     currently loading: tensorflow/compiler/mlir/quantization/stablehlo ... (19 packages)     Fetching repository project; starting     Fetching repository ; starting     Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11/archive/v2.13.4.tar.gz     Fetching https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen//archive/33d0937c6bdf5ec999939fb17f2a553183d14a74/eigen33d0937c6bdf5ec999939fb17f2a553183d14a74.tar.gz     Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11_abseil/archive/2c4932ed6f6204f1656e245838f4f5eae69d2e29.tar.gz     Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11_protobuf/archive/80f3440cd8fee124e077e2e47a8a17b78b451363.zip     Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/wjakob/nanobind/archive/d79309197caaad83cda05df533136865d294f01e.tar.gz     Fetching https://mirror.bazel.build/github.com/bazelbuild/rules_license/releases/download/0.0.7/rules_license0.0.7.tar.gz ```",2025-03-31T22:39:32Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/90331,"Querying local_config_cuda using BAZEL, I get the following output. Should I be using cudart and not cuda_runtime. Please advise.  `//cuda:BUILD //cuda:_opt //cuda:any_cuda_libs //cuda:build_defs.bzl //cuda:build_defs_bzl //cuda:cub_headers //cuda:cublas //cuda:cublasinclude //cuda:cublasLt //cuda:cublas_headers //cuda:cublas_headers_virtual //cuda:cuda //cuda:cudaextras //cuda:cudainclude //cuda:cudanvvm //cuda:cuda/cuda_config.h //cuda:cuda/cuda_config.py //cuda:cuda/lib/libcublas.so //cuda:cuda/lib/libcublasLt.so //cuda:cuda/lib/libcuda.so //cuda:cuda/lib/libcudart.so //cuda:cuda/lib/libcudart_static.a //cuda:cuda/lib/libcudnn.so //cuda:cuda/lib/libcufft.so //cuda:cuda/lib/libcupti.so //cuda:cuda/lib/libcurand.so //cuda:cuda/lib/libcusolver.so //cuda:cuda/lib/libcusparse.so //cuda:cuda_config_py //cuda:cuda_driver //cuda:cuda_headers //cuda:cuda_headers_virtual //cuda:cuda_libs //cuda:cuda_tools //cuda:cuda_tools_and_libs //cuda:cudart //cuda:cudart_static //cuda:cudnn //cuda:cudnninclude //cuda:cudnn_header //cuda:cufft //cuda:cufftinclude //cuda:cufft_headers //cuda:cufft_headers_virtual //cuda:cupti_dsos //cuda:cupti_headers //cuda:cupti_headers_virtual //cuda:curand //cuda:curandinclude //cuda:curand_headers //cuda:curand_headers_virtual //cuda:cusolver //cuda:cusolverinclude //cuda:cusolver_headers //cuda:cusolver_headers_virtual //cuda:cusparse //cuda:cusparseinclude //cuda:cusparse_headers //cuda:cusparse_headers_virtual //cuda:implicit_cuda_headers_dependency //cuda:include_cuda_libs //cuda:libdevice_root //cuda:nvjitlink //cuda:nvml //cuda:nvml_headers //cuda:nvml_headers_virtual //cuda:nvptxcompiler //cuda:override_include_cuda_libs //cuda:overrided_cuda_libs //cuda:true_setting //cuda:using_clang //cuda:using_clang_opt //cuda:using_nvcc`","Hi  , Apologies for the delay, and thank you for raising your concern. It seems you have installed incompatible versions. Could you please update them as per the documentation and let us know if the issue still persists? Here is the documentation for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Move xprof code from tensorflow/core/profiler/convert to xprof.,Move xprof code from tensorflow/core/profiler/convert to xprof. This constitutes the last major chunk of code being moved over.,2025-03-31T22:38:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90330
copybara-service[bot],[XLA] Add num_repeats to nightly workflow runs to reduce noise.,[XLA] Add num_repeats to nightly workflow runs to reduce noise.,2025-03-31T22:35:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90329
copybara-service[bot],[XLA] Replicate performance,[XLA] Replicate performance,2025-03-31T22:16:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90328
copybara-service[bot],Include the `ProfilerData` class in the profiler submodule in Jaxlib.,Include the `ProfilerData` class in the profiler submodule in Jaxlib.,2025-03-31T22:00:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90327
copybara-service[bot],Enforce that `Shape::tuple_shapes_size()` is only called on tuple shapes.,Enforce that `Shape::tuple_shapes_size()` is only called on tuple shapes.,2025-03-31T21:54:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90326
copybara-service[bot],Add a `LiteralBase::EachCellUntilFailure()` to support early return when iterating over cells.,Add a `LiteralBase::EachCellUntilFailure()` to support early return when iterating over cells. Also use this to simplify the code in several places.,2025-03-31T21:54:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90325
copybara-service[bot],"Remove the topology check for compilation, allowing compilation on a client different from the client where the executable will run.","Remove the topology check for compilation, allowing compilation on a client different from the client where the executable will run.",2025-03-31T21:06:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90324
copybara-service[bot],Convert CpuClient to also lazily tuplize arguments. This avoids constructing,Convert CpuClient to also lazily tuplize arguments. This avoids constructing and explicit tupletized buffer when it is not necessary.,2025-03-31T19:30:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90323
copybara-service[bot],[XLA:FFI] Add DeviceOrdinal context decoding to external FFI.,"[XLA:FFI] Add DeviceOrdinal context decoding to external FFI. This was already available in the internal version of the API, but it may be useful to be able to access it externally. For example, I found myself wanting this when constructing a DLPack wrapper for FFI buffers.",2025-03-31T18:57:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90322
copybara-service[bot],Integrate LLVM at llvm/llvm-project@799e9053641a,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 799e9053641a,2025-03-31T18:38:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90321
copybara-service[bot],litert: Add fp16 support in GPU Accelerator,litert: Add fp16 support in GPU Accelerator Used fp16 calculation if target GPU supports it. Added naive fp16fp32 conversion as an initial step. It should be vectorized.,2025-03-31T17:55:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90320
copybara-service[bot],[XLA:GPU] check in nest_gemm_fusion if the resulting computation is supported,[XLA:GPU] check in nest_gemm_fusion if the resulting computation is supported Normally we run a support check on HLO before assigning it to the generic emitter. In nested gemm fusion we switch the backend forcefully so it is a good idea to make sure that the resulting HLO is supported by the backend.,2025-03-31T17:45:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90319
copybara-service[bot],Add embedding_lookup composite dynamic lowering pattern,Add embedding_lookup composite dynamic lowering pattern,2025-03-31T17:32:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90318
copybara-service[bot],[XLA:GPU][NFC] Extract the logic to deduce the type of a `dot`'s accumulator type into `dot_algorithms.h`.,"[XLA:GPU][NFC] Extract the logic to deduce the type of a `dot`'s accumulator type into `dot_algorithms.h`. Take the opportunity to split up the logic deriving the required operands type, the required accumulator type, and the algorithm emitter and hoist it out of `EmitSingleTileDot`. The purpose of this change is to make this logic available to the generic emitter as well.",2025-03-31T17:32:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90317
copybara-service[bot],[xla:gpu] CommandBuffer: remove For command,"[xla:gpu] CommandBuffer: remove For command When constructing command buffers from thunks we can't replace while loop with a for loop, because we need an allocation for the loop counter, but for while loop we only get a buffer for pred[], and it's not enough to keep int32_t counter. Furthermore loop induction variable is updated by the body computation anyway, and condition computation is almost always a trivial compare operation, and by replacing compare fusion with AddI32 kernel we don't gain anything at all. XLA:GPU needs only two conditional commands to represent control flow: Case and While.",2025-03-31T17:09:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90316
copybara-service[bot],"Split up shape passes to remove the need for Shape->MHLO pass, use Shape->StableHLO","Split up shape passes to remove the need for Shape>MHLO pass, use Shape>StableHLO",2025-03-31T16:47:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90315
copybara-service[bot],[XLA:GPU][NFC] Remove dead `algorithm_util::IsAmpere` util.,[XLA:GPU][NFC] Remove dead `algorithm_util::IsAmpere` util.,2025-03-31T16:10:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90314
copybara-service[bot],[XLA:GPU] Add initial version of the CUDA kernel for one-shot all-reduce.,[XLA:GPU] Add initial version of the CUDA kernel for oneshot allreduce. This is only the initial version that has a number of limitations:   * Only support `float` input.   * Doesn't have vectorization or loop unrolling.   * Requires the caller to do synchronization.,2025-03-31T15:55:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90313
copybara-service[bot],[XLA:GPU] Define HS optimization at O1.,[XLA:GPU] Define HS optimization at O1.,2025-03-31T15:21:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90312
copybara-service[bot],[XLA:GPU] remove tests for PACKED_NIBBLES,[XLA:GPU] remove tests for PACKED_NIBBLES PACKED_NIBBLES are no longer supported,2025-03-31T14:12:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90311
copybara-service[bot],Make TFLite interpreter `model_path` argument PEP 519 compliant.,Make TFLite interpreter `model_path` argument PEP 519 compliant. Allows to pass `pathlib.Path` instances for `model_path`.,2025-03-31T13:38:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90310
copybara-service[bot],[XLA:GPU] Delete no-op `xla_gpu_enable_nccl_clique_optimization` flag and associated code.,"[XLA:GPU] Delete noop `xla_gpu_enable_nccl_clique_optimization` flag and associated code. The flag was used in the now deleted ([ CC(Getting Error  Exception: No data provided for ""activation_2""  Need data for each key in: ['input2', 'aux_input', 'input1'])](https://github.com/openxla/xla/pull/9329)) XLA GPU runtime.",2025-03-31T13:33:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90309
copybara-service[bot],[xla:gpu] CommandBuffer: add e2e tests for For and While loops,[xla:gpu] CommandBuffer: add e2e tests for For and While loops,2025-03-31T12:27:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90308
copybara-service[bot],[XLA:GPU] Allow to extract collectives from the post-optimized module.,[XLA:GPU] Allow to extract collectives from the postoptimized module.,2025-03-31T11:46:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90307
copybara-service[bot],WIP Adjustments for Emitter,WIP Adjustments for Emitter,2025-03-31T11:42:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90306
saddamhijazi,TensorFlow issue with data generator used for training a Keras LSTM autoencoder," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.5 LTS  Mobile device _No response_  Python version Python 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am trying to build a model which is a LSTMautoencoder using TensorFlow. The model generates the training data using 'tf.data.Dataset'.  The original dimension of the data which is loaded from a .mat file is `[79266,1001]`, I ran the code and then I got an error message saying : Training failed: None values not supported. I have tried to use the minimum batch size and I reduced the size of the data to check if the problem is related to memory load but still the issue is happening even for very small data and batch sizes. In the code I replaced the loading command with a random data generation just for the purposes of reproducing the error.  Standalone code to reproduce the issue ```shell import os os.environ['CUDA_VISIBLE_DEVICES'] = '1'   Force CPU execution import sys import tensorflow as tf import os.path import numpy as np import scipy.io import time from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.callbacks import Callback, ModelCheckpoint  Ultrasafe hyperparameters SPATIAL_SUBSAMPLE = 400   Now taking every 400th spatial point (~198 points) TEMPORAL_SUBSAMPLE = 50   Taking every 50th snapshot (~20 timesteps) SEQ_LENGTH = 1            Single timestep sequences LATENT_DIM = 1            1D latent space BATCH_SIZE = 1            Minimum batch size def load_and_validate():     """"""Load with maximum subsampling""""""     U_COM = tf.random.normal(shape = [79266,1001])      Aggressive subsampling     U_subsampled = U_COM[::SPATIAL_SUBSAMPLE, ::TEMPORAL_SUBSAMPLE]     print(f""Subsampled shape: {U_subsampled.shape} (spatial × temporal)"")      Validation     assert U_subsampled.shape[1] >= SEQ_LENGTH, ""Not enough timesteps""     assert not np.isnan(U_subsampled).any(), ""NaN values detected""      Normalize     U_min, U_max = np.min(U_subsampled), np.max(U_subsampled)     return 2 * (U_subsampled  U_min) / (U_max  U_min)  1  Load with cleanup tf.keras.backend.clear_session() U_norm = load_and_validate()  Create singletimestep sequences sequences = U_norm[:, :, np.newaxis]   Shape: (spatial, timesteps, 1) print(f""Sequences shape: {sequences.shape}"")  Create dataset dataset = tf.data.Dataset.from_tensor_slices(sequences) dataset = dataset.batch(BATCH_SIZE)  Verify sample = next(iter(dataset)) print(f""Sample batch shape: {sample.shape}"")  Micro LSTM model def build_micro_model():     inputs = tf.keras.Input(shape=(sequences.shape[1], 1))     x = layers.LSTM(2)(inputs)   Only 2 units     x = layers.Dense(LATENT_DIM)(x)     x = layers.RepeatVector(sequences.shape[1])(x)     outputs = layers.LSTM(1, return_sequences=True)(x)     return tf.keras.Model(inputs, outputs) model = build_micro_model() model.compile(optimizer='adam', loss='mse') model.summary()  Training os.makedirs('Tests', exist_ok=True) try:     history = model.fit(         dataset,         epochs=3,   Very few epochs         verbose=2     )     print(""Training completed successfully!"") except Exception as e:     print(f""Training failed: {str(e)}"")     print(""\nThis should never happen with these settings."")     print(""Please verify your input data structure."") ```  Relevant log output ```shell 20250331 13:08:33.404751: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250331 13:08:33.405142: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used. 20250331 13:08:33.407488: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used. 20250331 13:08:33.414088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1743419313.425745   24852 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1743419313.428932   24852 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250331 13:08:33.440581: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250331 13:08:35.116129: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303) Subsampled shape: (199, 21) (spatial × temporal) Sequences shape: (199, 21, 1) Sample batch shape: (1, 21, 1) Model: ""functional"" ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ ┃ Layer (type)                    ┃ Output Shape           ┃       Param  ┃ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ │ input_layer (InputLayer)        │ (None, 21, 1)          │             0 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ lstm (LSTM)                     │ (None, 2)              │            32 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ dense (Dense)                   │ (None, 1)              │             3 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ repeat_vector (RepeatVector)    │ (None, 21, 1)          │             0 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ lstm_1 (LSTM)                   │ (None, 21, 1)          │            12 │ └─────────────────────────────────┴────────────────────────┴───────────────┘  Total params: 47 (188.00 B)  Trainable params: 47 (188.00 B)  Nontrainable params: 0 (0.00 B) Epoch 1/3 Training failed: None values not supported. This should never happen with these settings. Please verify your input data structure. ```",2025-03-31T11:27:31Z,type:bug comp:keras TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/90305,"Hi  , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow 2.19.0 and the nightly version and encountered the same issue. Please find the gist here for reference. Based on my findings, this issue seems to be more related to Keras. I recommend posting this issue on the kerasteam/keras repository for better support. Thank you!","> Hi [](https://github.com/saddamhijazi) , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow 2.19.0 and the nightly version and encountered the same issue. Please find the gist here for reference. Based on my findings, this issue seems to be more related to Keras. I recommend posting this issue on the kerasteam/keras repository for better support. >  > Thank you! Thank you very much  for your response and help, I have submitted an issue on the Keras repository. Thank you !!"
copybara-service[bot],[XLA:GPU] Add support for multiple output tiles in triton_support_test,"[XLA:GPU] Add support for multiple output tiles in triton_support_test + removes dependency on `gettupleelement` for Reduce, BatchNormGrad & BatchNormTraining tests",2025-03-31T11:11:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90304
copybara-service[bot],PR #24008: [GPU] Fix TraceMe annotation.,PR CC(Create cloudbuild.yaml): [GPU] Fix TraceMe annotation. Imported from GitHub PR https://github.com/openxla/xla/pull/24008 Copybara import of the project:  829c15522e8ebebb11079913e51fac965f594cb2 by Ilia Sergachev : [GPU] Fix TraceMe annotation. Merging this change closes CC(Create cloudbuild.yaml) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24008 from openxla:fix_traceme 829c15522e8ebebb11079913e51fac965f594cb2,2025-03-31T10:55:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90303
copybara-service[bot],Removing PACKED_NIBBLES as we have native int4 support in the compiler,Removing PACKED_NIBBLES as we have native int4 support in the compiler,2025-03-31T10:55:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90302
copybara-service[bot],[xla:gpu] CommandBuffer: switch CommandBuffer::Case to explicit command update API,[xla:gpu] CommandBuffer: switch CommandBuffer::Case to explicit command update API,2025-03-31T10:40:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90301
copybara-service[bot],PR #23827: [DOC] Fix multihost HLO runner doc.,PR CC(Tensor cpu need cuda?): [DOC] Fix multihost HLO runner doc. Imported from GitHub PR https://github.com/openxla/xla/pull/23827  Omit c opt which is the default.   Omit dump_hlo_as_text which is the default.   Omit config=cuda which is nowadays handled by configure.py.   Remove xla_disable_all_hlo_passes which has no effect in presence of run_xla_backend_only.   Leave single mention of dynamic_mode=off which usually isn't needed.   Other minor fixes. Copybara import of the project:  bc64c89d3cb4db37945331d94faf9acc573ed6d9 by Ilia Sergachev : [DOC] Fix multihost HLO runner doc.  Omit c opt which is the default.   Omit dump_hlo_as_text which is the default.   Omit config=cuda which is nowadays handled by configure.py.   Remove xla_disable_all_hlo_passes which has no effect in presence of run_xla_backend_only.   Leave single mention of dynamic_mode=off which usually isn't needed.   Other minor fixes. Merging this change closes CC(Tensor cpu need cuda?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23827 from openxla:fix_doc bc64c89d3cb4db37945331d94faf9acc573ed6d9,2025-03-31T10:37:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90300
copybara-service[bot],#sdy add a sharding rule for mhlo::CopyOp as this op remains when converting from HLO to StableHLO.,sdy add a sharding rule for mhlo::CopyOp as this op remains when converting from HLO to StableHLO.,2025-03-31T09:22:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90299
copybara-service[bot],Implement `Compile()` and `DeserializeExecutable()` that return an unloaded executable for PJRT stream executor.,Implement `Compile()` and `DeserializeExecutable()` that return an unloaded executable for PJRT stream executor. Also refactor `LoadSerializedExecutable()` and `Load()` accordingly.,2025-03-31T09:11:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90298
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T09:01:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90297
copybara-service[bot],[XLA:CPU] Ensure benchmark name uniqueness in `multi_benchmark_config` and pass benchmark options by value,[XLA:CPU] Ensure benchmark name uniqueness in `multi_benchmark_config` and pass benchmark options by value,2025-03-31T08:57:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90296
copybara-service[bot],PR #24248: Move-to-memory-space custom calls use default layout,PR CC(Support for `skip_mismatch` in `tf.keras.engine.saving.load_weights_from_hdf5_group_by_name`): Movetomemoryspace custom calls use default layout Imported from GitHub PR https://github.com/openxla/xla/pull/24248 Using nondefault layout for MoveToHost makes the compiler insert a transpose operation on the host value if that value flows into the root of the entry computation. Such transposes cause the host offloader to emit a slow onhost transpose (and a warning on the console). We see those warnings on  maxtext llama27b with optimizer state offloading with fsdp=2. This patch enforces default layout for onhost values so that no transpose is necessary (as long as there is no override of layout for host values in entry computation). Note that such transposes cannot be sunk into the uses by the offloading lagalizer because there is nowhere to sink to  the value is returned from the computation. Copybara import of the project:  ad192bbba93040a88bda9e4e1074dd1de7153b32 by Jaroslav Sevcik : Use default layout for offloading ops  f8f0ddcf73c9a4792cbbfd99c2d465f4de80c418 by Jaroslav Sevcik : Address reviewer comments Merging this change closes CC(Support for `skip_mismatch` in `tf.keras.engine.saving.load_weights_from_hdf5_group_by_name`) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24248 from jarosevcik:movetomemoryspaceusesdefaultlayout f8f0ddcf73c9a4792cbbfd99c2d465f4de80c418,2025-03-31T08:33:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90295
copybara-service[bot],Direct StableHLO to HLO conversion : prototype with AddOp and ConstantOp.,"Direct StableHLO to HLO conversion : prototype with AddOp and ConstantOp. The cl demonstrates  1. codegen for stablehlo ops. 2. changes needed in conversion pipeline to allow stablehlo ops to convert directly to hlo without mhlo step.  Note: The new direct path is still disabled for the production until we add all stablehlo ops to the codegen. Example: ``` hlotranslate mlirtohlo $PWD/1.mlir ``` Input ``` func.func (%arg0: tensor) > tensor {   %c = stablehlo.constant dense : tensor   %0 = stablehlo.add %arg0, %c : tensor   %1 = mhlo.multiply %c, %0 : tensor   return %1 : tensor } ``` after StableHLO > MHLO conversion, stablehlo.add and stablehlo.constant are preserved, not converted to mhlo. ``` mlirhloopt stablehlolegalizetohlo=legalizepartially=true chlolegalizetohlo $PWD/1.mlir module {   func.func (%arg0: tensor) > tensor {     %c = stablehlo.constant dense : tensor     %0 = stablehlo.add %arg0, %c : tensor     %1 = mhlo.multiply %c, %0 : tensor     return %1 : tensor   } } ``` Final Output  ``` HloModule main, entry_computation_layout={(s32[])>s32[]} ENTRY %main.5 (Arg_0.1: s32[]) > s32[] {   %Arg_0.1 = s32[] parameter(0)   %constant.2 = s32[] constant(2)   %add.3 = s32[] add(%Arg_0.1, %constant.2), metadata=   ROOT %multiply.4 = s32[] multiply(%add.3, %constant.2), metadata= } ```",2025-03-31T07:08:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90294
copybara-service[bot],[xla:gpu] Add an HLO test for testing conditional (case) command,[xla:gpu] Add an HLO test for testing conditional (case) command,2025-03-31T07:07:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90293
copybara-service[bot],Implement `Compile()` and `DeserializeExecutable()` that return an unloaded executable for PJRT stream executor.,Implement `Compile()` and `DeserializeExecutable()` that return an unloaded executable for PJRT stream executor. Also refactor `LoadSerializedExecutable()` and `Load()` accordingly.,2025-03-31T06:38:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90292
weyn9q,TensorFlow with CUDA: RTX 5xxx series isn't supported (CUDA_ERROR_INVALID_HANDLE)," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code Yes  OS platform and distribution WSL2  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.8  GPU model and memory 5070TI  Current behavior? hi guys. i upgraded my GPU to 5070ti and found that tensorflow is still not working with new nvidia cards honestly i knew that can be problems. but its been 3 month since nvidia released new gpu, i thought now should be no prob already. any idea how long it will take until at least there will be a nightly version which will fix the problem? like 12 weeks or half a year ? :) or maybe someone found an easy way to fix it?  Standalone code to reproduce the issue ```shell import tensorflow as tf model = tf.keras.Sequential([     tf.keras.Input(shape=(20,)),     tf.keras.layers.Dense(10),     tf.keras.layers.Dense(1) ]) ```  Relevant log output ```shell 20250331 14:23:13.091967: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250331 14:23:13.734399: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1743398598.268271   13190 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jitcompiled from PTX, which could take 30 minutes or longer. W0000 00:00:1743398598.270049   13190 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jitcompiled from PTX, which could take 30 minutes or longer. I0000 00:00:1743398598.408238   13190 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13123 MB memory:  > device: 0, name: NVIDIA GeForce RTX 5070 Ti, pci bus id: 0000:01:00.0, compute capability: 12.0 20250331 14:23:18.851409: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_INVALID_PTX' 20250331 14:23:18.851466: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250331 14:23:18.851502: W tensorflow/core/framework/op_kernel.cc:1843] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250331 14:23:18.851526: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' Traceback (most recent call last):   File ""/mnt/c/Users/alex/desktop/test111.py"", line 3, in      model = tf.keras.Sequential([   File ""/home/alexflame/.local/lib/python3.10/sitepackages/keras/src/models/sequential.py"", line 76, in __init__     self._maybe_rebuild()   File ""/home/alexflame/.local/lib/python3.10/sitepackages/keras/src/models/sequential.py"", line 149, in _maybe_rebuild     self.build(input_shape)   File ""/home/alexflame/.local/lib/python3.10/sitepackages/keras/src/layers/layer.py"", line 229, in build_wrapper     original_build_method(*args, **kwargs)   File ""/home/alexflame/.local/lib/python3.10/sitepackages/keras/src/models/sequential.py"", line 195, in build     x = layer(x)   File ""/home/alexflame/.local/lib/python3.10/sitepackages/keras/src/utils/traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/home/alexflame/.local/lib/python3.10/sitepackages/keras/src/backend/tensorflow/core.py"", line 152, in convert_to_tensor     return tf.cast(x, dtype) tensorflow.python.framework.errors_impl.InternalError: {{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Cast] name: ```",2025-03-31T05:26:22Z,stat:awaiting tensorflower type:feature type:build/install wsl2 TF 2.18,open,0,3,https://github.com/tensorflow/tensorflow/issues/90291,"spend whole day to build custom tensorflow build but didnt succeed. now finally found how to solve this problem. not perfect solution as it could be, but still better then nothing :) https://docs.nvidia.com/deeplearning/frameworks/tensorflowreleasenotes/rel2502.html",ohh seems its impossible. thinking about move to pytorch. i dont want but :( but i really stuck here with this problems and i dont think will be any updates here soon.  i cant run any of my codes with new GPU. nvidia container works but its soo complicated. docker still not fully optimizated with many mistakes. cant work good. example  i have my own pretrained model which i am using to train RL agent. lets say i have 2 code. first one is for train model ( later  pretrained) second is use pretrained get embeddings train RL agent. without nvidia docker i just cant run any of them coz of CUDA errors. with docker first code can run if will fix some mistakes which happens only if use docker. but. second code i cant deserialize custom layers from pretrained idk why. coz of docker i think. probably because they were builded with another drivers. i decided to check if i will train model in docker again and will try to use it in second code will it work or no. it is not working. i still cant deserialize custom layers even with newly trained model. i am giving up. before on 4xxx videocard everything was perfect. now idk what to do,Same issue same scenario here. Tried updating every driver I could think of. Hoping for at least a nightly version soon so I can work again.
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T04:58:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90290
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T04:54:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90289
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T04:30:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90288
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T04:28:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90287
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T04:26:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90286
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T04:25:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90285
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T04:24:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90284
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T04:21:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90283
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T04:14:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90282
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T04:11:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90281
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T04:11:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90280
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T04:10:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90279
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T04:08:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90278
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T02:14:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90277
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-31T02:03:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90276
copybara-service[bot],Enforce that `Shape::tuple_shapes_size()` is only called on tuple shapes.,Enforce that `Shape::tuple_shapes_size()` is only called on tuple shapes.,2025-03-30T22:47:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90275
copybara-service[bot],Enforce that `Shape::clear_layout()` is only called on array shapes.,Enforce that `Shape::clear_layout()` is only called on array shapes.,2025-03-30T22:46:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90274
copybara-service[bot],Enforce that `TrueNumDimensions()` is only called on array shapes.,Enforce that `TrueNumDimensions()` is only called on array shapes.,2025-03-30T22:46:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90273
copybara-service[bot],Enforce that `Shape::dynamic_dimensions()` is only called on array shapes.,Enforce that `Shape::dynamic_dimensions()` is only called on array shapes.,2025-03-30T22:45:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90272
copybara-service[bot],"Ensure that in `ShapeUtil::PopulateShape()`, `dimensions` is empty if the shape is non-array.","Ensure that in `ShapeUtil::PopulateShape()`, `dimensions` is empty if the shape is nonarray.",2025-03-30T22:45:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90271
copybara-service[bot],Enforce that `Shape::tuple_shapes()` is only called on tuple shapes.,Enforce that `Shape::tuple_shapes()` is only called on tuple shapes.,2025-03-30T22:43:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90270
copybara-service[bot],Prevent calling `dimensions()` or `dimensions_size()` on non-array shapes.,"Prevent calling `dimensions()` or `dimensions_size()` on nonarray shapes. It's a bug to use an array shape as a tuple or use a tuple shape as an array, for example. To catch such bugs, we make these methods fail if called on nonarray shapes.",2025-03-30T22:40:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90269
copybara-service[bot],"In `Shape::Is*()`, enforce that the shape's element type and state are consistent.","In `Shape::Is*()`, enforce that the shape's element type and state are consistent.",2025-03-30T22:39:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90268
copybara-service[bot],[XLA:GPU] Add more op codes to triton support test 2/n,[XLA:GPU] Add more op codes to triton support test 2/n,2025-03-30T19:42:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90267
copybara-service[bot],Enable benchmark_litert_model_test.cc test,Enable benchmark_litert_model_test.,2025-03-30T17:59:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90266
copybara-service[bot],Fix LiteRtApiVersion type templating to be compatible with C++17,Fix LiteRtApiVersion type templating to be compatible with C++17,2025-03-30T17:07:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90265
balasai075,Import Error," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.8  Custom code Yes  OS platform and distribution windows 11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError: Traceback (most recent call last):   File ""C:\Users\balas\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.  Standalone code to reproduce the issue ```shell import json import os import cv2 import numpy as np import tensorflow as tf import matplotlib.pyplot as plt  Paths for TuSimple Dataset label_paths = [     r""C:\Users\balas\Downloads\archive (2)\TUSimple\train_set\label_data_0313.json"",     r""C:\Users\balas\Downloads\archive (2)\TUSimple\train_set\label_data_0531.json"",     r""C:\Users\balas\Downloads\archive (2)\TUSimple\train_set\label_data_0601.json"" ] image_root = r""C:\Users\balas\Downloads\archive (2)\TUSimple\train_set""  Load JSON Labels def load_tusimple_labels(label_paths):     dataset = []     for label_path in label_paths:         with open(label_path, 'r') as f:             for line in f:                 dataset.append(json.loads(line))     return dataset  Extract Image Paths & Lane Annotations def parse_tusimple_data(dataset):     images = []     lanes = []     for item in dataset:         image_path = os.path.join(image_root, item[""raw_file""])         lane_annotations = item[""lanes""]         images.append(image_path)         lanes.append(lane_annotations)     return images, lanes  Load Dataset dataset = load_tusimple_labels(label_paths) image_paths, lane_data = parse_tusimple_data(dataset)  Image Preprocessing Function def preprocess_image(image_path):     image = cv2.imread(image_path)     image = cv2.resize(image, (256, 256))   Resize to match CNN input     image = image / 255.0   Normalize     return image  Load Images X_train = np.array([preprocess_image(img) for img in image_paths[:]])   Load first 500 for training X_train = X_train.reshape(1, 256, 256, 3) print(f""Loaded {len(X_train)} images from TuSimple dataset!"") ```  Relevant log output ```shell  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[5], line 5       3 import cv2       4 import numpy as np > 5 import tensorflow as tf       6 import matplotlib.pyplot as plt       8  Paths for TuSimple Dataset File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\balas\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```",2025-03-30T17:05:20Z,type:support,closed,0,2,https://github.com/tensorflow/tensorflow/issues/90264,"TF 2.8 is no longer supported. Also, this is a duplicate",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Migrate CHIRP model to use enable_xla=True during TFL conversion of the JAX model.,Migrate CHIRP model to use enable_xla=True during TFL conversion of the JAX model.,2025-03-30T16:29:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90263
Manashwinij,error in tensorflow installation," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version Version: 2.19.0  Custom code Yes  OS platform and distribution win 11  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory cpu  Current behavior?  ImportError                               Traceback (most recent call last) File ~\myenv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[5], line 2       1 import numpy as np > 2 import tensorflow as tf       3 from tensorflow.keras.models import load_model       4 from tensorflow.keras.preprocessing import image File ~\myenv\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\myenv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.')      97  pylint: enable=wildcardimport,gimportnotattop,unusedimport,linetoolong ImportError: Traceback (most recent call last):   File ""C:\Users\Hp\myenv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions.  Standalone code to reproduce the issue ```shell import numpy as np import tensorflow as tf from tensorflow.keras.models import load_model from tensorflow.keras.preprocessing import image ```  Relevant log output ```shell ```",2025-03-30T12:22:24Z,type:build/install,closed,0,2,https://github.com/tensorflow/tensorflow/issues/90262,"Unless there's additional information, this is a duplicate. Please always search for duplicates and only open a new issue if the duplicates don't apply, because you have additional info.",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Optional TQDM dependency,Optional TQDM dependency,2025-03-30T06:17:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90261
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-30T05:36:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90260
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-30T04:52:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90259
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-30T04:50:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90258
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-30T04:47:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90257
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-30T04:46:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90256
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-30T04:44:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90255
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-30T04:44:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90254
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-30T04:40:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90253
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-30T02:22:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90252
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-30T02:22:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90251
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-30T02:21:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90250
copybara-service[bot],Add dynamic input shape support for rfft2d conversion.,Add dynamic input shape support for rfft2d conversion. Removing the additional constraint that fft_lengths need to be powers of 2. This doesn't seem to be a requirement on the runtime side.,2025-03-30T02:13:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90249
mimo-to,I am a spammer,,2025-03-29T22:17:15Z,size:XS invalid,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90248,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Switch `HloProgram` SerDes to deserialize into StableHLO and assume the input program to be StableHLO,"Switch `HloProgram` SerDes to deserialize into StableHLO and assume the input program to be StableHLO JAX has been emitting StableHLO for a while, so it makes sense to assume StableHLO as the only program. This avoids unnecessary MHLO conversion at deserialization time and makes sure that StableHLO is passed to the actual runtime behind IFRT Proxy. This does not change serialization format since the serialized bytes are still in VHLO.",2025-03-29T19:44:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90247
copybara-service[bot],Fix the breakage for OSS preparation:,Fix the breakage for OSS preparation: * LLVM * Py Test * TQDM * TFLite headers * TFLite deps,2025-03-29T19:42:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90246
SarahClementine,Error fixing," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.19  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Failure in installation.  Standalone code to reproduce the issue ```shell . Location: c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\localcache\localpackages\python39\sitepackages Requires: Requiredby: h5py, KerasPreprocessing, ml_dtypes, scikitlearn, scipy, tensorboard, tensorflow PS C:\xampp\htdocs\SkinToneAnalysis> python version >> pip show tensorflow >> pip show numpy >> Python 3.10.11 WARNING: Package(s) not found: tensorflow WARNING: Package(s) not found: numpy PS C:\xampp\htdocs\SkinToneAnalysis> python m pip install upgrade pip setuptools wheel >> Requirement already satisfied: pip in c:\program files\windowsapps\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\sitepackages (23.0.1) Collecting pip   Downloading pip25.0.1py3noneany.whl (1.8 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 1.5 MB/s eta 0:00:00 Requirement already satisfied: setuptools in c:\program files\windowsapps\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\sitepackages (65.5.0) Collecting setuptools   Downloading setuptools78.1.0py3noneany.whl (1.3 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 2.2 MB/s eta 0:00:00 Collecting wheel   Downloading wheel0.45.1py3noneany.whl (72 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.5/72.5 kB 2.0 MB/s eta 0:00:00 Installing collected packages: wheel, setuptools, pip   WARNING: The script wheel.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The scripts pip.exe, pip3.10.exe and pip3.exe are installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation. Successfully installed pip25.0.1 setuptools78.1.0 wheel0.45.1 [notice] A new release of pip is available: 23.0.1 > 25.0.1 [notice] To update, run: C:\Users\USER PC\AppData\Local\Microsoft\WindowsApps\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\python.exe m pip install upgrade pip PS C:\xampp\htdocs\SkinToneAnalysis> pip install tensorflow==2.10.0 >> Collecting tensorflow==2.10.0   Downloading tensorflow2.10.0cp310cp310win_amd64.whl.metadata (3.1 kB) Collecting abslpy>=1.0.0 (from tensorflow==2.10.0)   Downloading absl_py2.2.1py3noneany.whl.metadata (2.4 kB) Collecting astunparse>=1.6.0 (from tensorflow==2.10.0)   Downloading astunparse1.6.3py2.py3noneany.whl.metadata (4.4 kB) Collecting flatbuffers>=2.0 (from tensorflow==2.10.0)   Downloading flatbuffers25.2.10py2.py3noneany.whl.metadata (875 bytes) Collecting gast=0.2.1 (from tensorflow==2.10.0)   Downloading gast0.4.0py3noneany.whl.metadata (1.1 kB) Collecting googlepasta>=0.1.1 (from tensorflow==2.10.0)   Downloading google_pasta0.2.0py3noneany.whl.metadata (814 bytes) Collecting h5py>=2.9.0 (from tensorflow==2.10.0)   Downloading h5py3.13.0cp310cp310win_amd64.whl.metadata (2.5 kB) Collecting keraspreprocessing>=1.1.1 (from tensorflow==2.10.0)   Downloading Keras_Preprocessing1.1.2py2.py3noneany.whl.metadata (1.9 kB) Collecting libclang>=13.0.0 (from tensorflow==2.10.0)   Downloading libclang18.1.1py2.py3nonewin_amd64.whl.metadata (5.3 kB) Collecting numpy>=1.20 (from tensorflow==2.10.0)   Downloading numpy2.2.4cp310cp310win_amd64.whl.metadata (60 kB) Collecting opteinsum>=2.3.2 (from tensorflow==2.10.0)   Downloading opt_einsum3.4.0py3noneany.whl.metadata (6.3 kB) Collecting packaging (from tensorflow==2.10.0)   Downloading packaging24.2py3noneany.whl.metadata (3.2 kB) Collecting protobuf=3.9.2 (from tensorflow==2.10.0)   Downloading protobuf3.19.6cp310cp310win_amd64.whl.metadata (806 bytes) Requirement already satisfied: setuptools in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (78.1.0) Collecting six>=1.12.0 (from tensorflow==2.10.0)   Downloading six1.17.0py2.py3noneany.whl.metadata (1.7 kB) Collecting termcolor>=1.1.0 (from tensorflow==2.10.0)   Downloading termcolor2.5.0py3noneany.whl.metadata (6.1 kB) Collecting typingextensions>=3.6.6 (from tensorflow==2.10.0)   Downloading typing_extensions4.13.0py3noneany.whl.metadata (3.0 kB) Collecting wrapt>=1.11.0 (from tensorflow==2.10.0)   Downloading wrapt1.17.2cp310cp310win_amd64.whl.metadata (6.5 kB) Collecting tensorflowiogcsfilesystem>=0.23.1 (from tensorflow==2.10.0)   Downloading tensorflow_io_gcs_filesystem0.31.0cp310cp310win_amd64.whl.metadata (14 kB) Collecting grpcio=1.24.3 (from tensorflow==2.10.0)   Downloading grpcio1.71.0cp310cp310win_amd64.whl.metadata (4.0 kB) Collecting tensorboard=2.10 (from tensorflow==2.10.0)   Downloading tensorboard2.10.1py3noneany.whl.metadata (1.9 kB) Collecting tensorflowestimator=2.10.0 (from tensorflow==2.10.0)   Downloading tensorflow_estimator2.10.0py2.py3noneany.whl.metadata (1.3 kB) Collecting keras=2.10.0 (from tensorflow==2.10.0)   Downloading keras2.10.0py2.py3noneany.whl.metadata (1.3 kB) Requirement already satisfied: wheel=0.23.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from astunparse>=1.6.0>tensorflow==2.10.0) (0.45.1) Collecting googleauth=1.6.3 (from tensorboard=2.10>tensorflow==2.10.0)   Downloading google_auth2.38.0py2.py3noneany.whl.metadata (4.8 kB) Collecting googleauthoauthlib=0.4.1 (from tensorboard=2.10>tensorflow==2.10.0)   Downloading google_auth_oauthlib0.4.6py2.py3noneany.whl.metadata (2.7 kB) Collecting markdown>=2.6.8 (from tensorboard=2.10>tensorflow==2.10.0)   Downloading Markdown3.7py3noneany.whl.metadata (7.0 kB) Collecting requests=2.21.0 (from tensorboard=2.10>tensorflow==2.10.0)   Downloading requests2.32.3py3noneany.whl.metadata (4.6 kB) Collecting tensorboarddataserver=0.6.0 (from tensorboard=2.10>tensorflow==2.10.0)   Downloading tensorboard_data_server0.6.1py3noneany.whl.metadata (1.1 kB) Collecting tensorboardpluginwit>=1.6.0 (from tensorboard=2.10>tensorflow==2.10.0)   Downloading tensorboard_plugin_wit1.8.1py3noneany.whl.metadata (873 bytes) Collecting werkzeug>=1.0.1 (from tensorboard=2.10>tensorflow==2.10.0)   Downloading werkzeug3.1.3py3noneany.whl.metadata (3.7 kB) Collecting cachetools=2.0.0 (from googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0)   Downloading cachetools5.5.2py3noneany.whl.metadata (5.4 kB) Collecting pyasn1modules>=0.2.1 (from googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0)   Downloading pyasn1_modules0.4.2py3noneany.whl.metadata (3.5 kB) Collecting rsa=3.1.4 (from googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0)   Downloading rsa4.9py3noneany.whl.metadata (4.2 kB) Collecting requestsoauthlib>=0.7.0 (from googleauthoauthlib=0.4.1>tensorboard=2.10>tensorflow==2.10.0)   Downloading requests_oauthlib2.0.0py2.py3noneany.whl.metadata (11 kB) Collecting charsetnormalizer=2 (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0)   Downloading charset_normalizer3.4.1cp310cp310win_amd64.whl.metadata (36 kB) Collecting idna=2.5 (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0)   Downloading idna3.10py3noneany.whl.metadata (10 kB) Collecting urllib3=1.21.1 (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0)   Downloading urllib32.3.0py3noneany.whl.metadata (6.5 kB) Collecting certifi>=2017.4.17 (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0)   Downloading certifi2025.1.31py3noneany.whl.metadata (2.5 kB) Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1>tensorboard=2.10>tensorflow==2.10.0)   Downloading MarkupSafe3.0.2cp310cp310win_amd64.whl.metadata (4.1 kB) Collecting pyasn1=0.6.1 (from pyasn1modules>=0.2.1>googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0)   Downloading pyasn10.6.1py3noneany.whl.metadata (8.4 kB) Collecting oauthlib>=3.0.0 (from requestsoauthlib>=0.7.0>googleauthoauthlib=0.4.1>tensorboard=2.10>tensorflow==2.10.0)   Downloading oauthlib3.2.2py3noneany.whl.metadata (7.5 kB) Downloading tensorflow2.10.0cp310cp310win_amd64.whl (455.9 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 455.9/455.9 MB 4.2 MB/s eta 0:00:00 Downloading absl_py2.2.1py3noneany.whl (277 kB) Downloading astunparse1.6.3py2.py3noneany.whl (12 kB) Downloading flatbuffers25.2.10py2.py3noneany.whl (30 kB) Downloading gast0.4.0py3noneany.whl (9.8 kB) Downloading google_pasta0.2.0py3noneany.whl (57 kB) Downloading grpcio1.71.0cp310cp310win_amd64.whl (4.3 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 2.7 MB/s eta 0:00:00 Downloading h5py3.13.0cp310cp310win_amd64.whl (3.0 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 2.4 MB/s eta 0:00:00 Downloading keras2.10.0py2.py3noneany.whl (1.7 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 2.8 MB/s eta 0:00:00 Downloading Keras_Preprocessing1.1.2py2.py3noneany.whl (42 kB) Downloading libclang18.1.1py2.py3nonewin_amd64.whl (26.4 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.4/26.4 MB 2.9 MB/s eta 0:00:00 Downloading numpy2.2.4cp310cp310win_amd64.whl (12.9 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/12.9 MB 3.6 MB/s eta 0:00:00 Downloading opt_einsum3.4.0py3noneany.whl (71 kB) Downloading protobuf3.19.6cp310cp310win_amd64.whl (895 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 895.7/895.7 kB 3.1 MB/s eta 0:00:00 Downloading six1.17.0py2.py3noneany.whl (11 kB) Downloading tensorboard2.10.1py3noneany.whl (5.9 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/5.9 MB 3.2 MB/s eta 0:00:00 Downloading tensorflow_estimator2.10.0py2.py3noneany.whl (438 kB) Downloading tensorflow_io_gcs_filesystem0.31.0cp310cp310win_amd64.whl (1.5 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 738.8 kB/s eta 0:00:00 Downloading termcolor2.5.0py3noneany.whl (7.8 kB) Downloading typing_extensions4.13.0py3noneany.whl (45 kB) Downloading wrapt1.17.2cp310cp310win_amd64.whl (38 kB) Downloading packaging24.2py3noneany.whl (65 kB) Downloading google_auth2.38.0py2.py3noneany.whl (210 kB) Downloading google_auth_oauthlib0.4.6py2.py3noneany.whl (18 kB) Downloading Markdown3.7py3noneany.whl (106 kB) Downloading requests2.32.3py3noneany.whl (64 kB) Downloading tensorboard_data_server0.6.1py3noneany.whl (2.4 kB) Downloading tensorboard_plugin_wit1.8.1py3noneany.whl (781 kB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 1.3 MB/s eta 0:00:00 Downloading werkzeug3.1.3py3noneany.whl (224 kB) Downloading cachetools5.5.2py3noneany.whl (10 kB) Downloading certifi2025.1.31py3noneany.whl (166 kB) Downloading charset_normalizer3.4.1cp310cp310win_amd64.whl (102 kB) Downloading idna3.10py3noneany.whl (70 kB) Downloading MarkupSafe3.0.2cp310cp310win_amd64.whl (15 kB) Downloading pyasn1_modules0.4.2py3noneany.whl (181 kB) Downloading requests_oauthlib2.0.0py2.py3noneany.whl (24 kB) Downloading rsa4.9py3noneany.whl (34 kB) Downloading urllib32.3.0py3noneany.whl (128 kB) Downloading oauthlib3.2.2py3noneany.whl (151 kB) Downloading pyasn10.6.1py3noneany.whl (83 kB) Installing collected packages: tensorboardpluginwit, libclang, keras, flatbuffers, wrapt, urllib3, typingextensions, termcolor, tensorflowiogcsfilesystem, tensorflowestimator, tensorboarddataserver, six, pyasn1, protobuf, packaging, opteinsum, oauthlib, numpy, MarkupSafe, markdown, idna, grpcio, gast, charsetnormalizer, certifi, cachetools, abslpy, werkzeug, rsa, requests, pyasn1modules, keraspreprocessing, h5py, googlepasta, astunparse, requestsoauthlib, googleauth, googleauthoauthlib, tensorboard, tensorflow   WARNING: The scripts f2py.exe and numpyconfig.exe are installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The script markdown_py.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The script normalizer.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The scripts pyrsadecrypt.exe, pyrsaencrypt.exe, pyrsakeygen.exe, pyrsapriv2pub.exe, pyrsasign.exe and pyrsaverify.exe are installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The script googleoauthlibtool.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The script tensorboard.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation. Successfully installed MarkupSafe3.0.2 abslpy2.2.1 astunparse1.6.3 cachetools5.5.2 certifi2025.1.31 charsetnormalizer3.4.1 flatbuffers25.2.10 gast0.4.0 googleauth2.38.0 googleauthoauthlib0.4.6 googlepasta0.2.0 grpcio1.71.0 h5py3.13.0 idna3.10 keras2.10.0 keraspreprocessing1.1.2 libclang18.1.1 markdown3.7 numpy2.2.4 oauthlib3.2.2 opteinsum3.4.0 packaging24.2 protobuf3.19.6 pyasn10.6.1 pyasn1modules0.4.2 requests2.32.3 requestsoauthlib2.0.0 rsa4.9 six1.17.0 tensorboard2.10.1 tensorboarddataserver0.6.1 tensorboardpluginwit1.8.1 tensorflow2.10.0 tensorflowestimator2.10.0 tensorflowiogcsfilesystem0.31.0 termcolor2.5.0 typingextensions4.13.0 urllib32.3.0 werkzeug3.1.3 wrapt1.17.2 PS C:\xampp\htdocs\SkinToneAnalysis> pip install tensorflow==2.10.0 >> Requirement already satisfied: tensorflow==2.10.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (2.10.0) Requirement already satisfied: abslpy>=1.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (2.2.1) Requirement already satisfied: astunparse>=1.6.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (1.6.3) Requirement already satisfied: flatbuffers>=2.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (25.2.10) Requirement already satisfied: gast=0.2.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (0.4.0) Requirement already satisfied: googlepasta>=0.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (0.2.0) Requirement already satisfied: h5py>=2.9.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (3.13.0) Requirement already satisfied: keraspreprocessing>=1.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (1.1.2) Requirement already satisfied: libclang>=13.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (18.1.1) Requirement already satisfied: numpy>=1.20 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (2.2.4) Requirement already satisfied: opteinsum>=2.3.2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (3.4.0) Requirement already satisfied: packaging in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (24.2) Requirement already satisfied: protobuf=3.9.2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (3.19.6) Requirement already satisfied: setuptools in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (78.1.0) Requirement already satisfied: six>=1.12.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (1.17.0) Requirement already satisfied: termcolor>=1.1.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (2.5.0) Requirement already satisfied: typingextensions>=3.6.6 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (4.13.0) Requirement already satisfied: wrapt>=1.11.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (1.17.2) Requirement already satisfied: tensorflowiogcsfilesystem>=0.23.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (0.31.0) Requirement already satisfied: grpcio=1.24.3 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (1.71.0) Requirement already satisfied: tensorboard=2.10 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (2.10.1) Requirement already satisfied: tensorflowestimator=2.10.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (2.10.0) Requirement already satisfied: keras=2.10.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow==2.10.0) (2.10.0) Requirement already satisfied: wheel=0.23.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from astunparse>=1.6.0>tensorflow==2.10.0) (0.45.1) Requirement already satisfied: googleauth=1.6.3 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow==2.10.0) (2.38.0) Requirement already satisfied: googleauthoauthlib=0.4.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow==2.10.0) (0.4.6) Requirement already satisfied: markdown>=2.6.8 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow==2.10.0) (3.7) Requirement already satisfied: requests=2.21.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow==2.10.0) (2.32.3) Requirement already satisfied: tensorboarddataserver=0.6.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow==2.10.0) (0.6.1) Requirement already satisfied: tensorboardpluginwit>=1.6.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow==2.10.0) (1.8.1) Requirement already satisfied: werkzeug>=1.0.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow==2.10.0) (3.1.3) Requirement already satisfied: cachetools=2.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0) (5.5.2)        Requirement already satisfied: pyasn1modules>=0.2.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0) (0.4.2)         Requirement already satisfied: rsa=3.1.4 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0) (4.9) Requirement already satisfied: requestsoauthlib>=0.7.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauthoauthlib=0.4.1>tensorboard=2.10>tensorflow==2.10.0) (2.0.0) Requirement already satisfied: charsetnormalizer=2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0) (3.4.1)        Requirement already satisfied: idna=2.5 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0) (3.10) Requirement already satisfied: urllib3=1.21.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0) (2.3.0) Requirement already satisfied: certifi>=2017.4.17 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow==2.10.0) (2025.1.31) Requirement already satisfied: MarkupSafe>=2.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from werkzeug>=1.0.1>tensorboard=2.10>tensorflow==2.10.0) (3.0.2) Requirement already satisfied: pyasn1=0.6.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from pyasn1modules>=0.2.1>googleauth=1.6.3>tensorboard=2.10>tensorflow==2.10.0) (0.6.1) Requirement already satisfied: oauthlib>=3.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requestsoauthlib>=0.7.0>googleauthoauthlib=0.4.1>tensorboard=2.10>tensorflow==2.10.0) (3.2.2) PS C:\xampp\htdocs\SkinToneAnalysis> pip install scikitlearn >> python skin_analysis_model/train_model.py >> Collecting scikitlearn   Downloading scikit_learn1.6.1cp310cp310win_amd64.whl.metadata (15 kB) Requirement already satisfied: numpy>=1.19.5 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from scikitlearn) (2.2.4) Collecting scipy>=1.6.0 (from scikitlearn)   Downloading scipy1.15.2cp310cp310win_amd64.whl.metadata (60 kB) Collecting joblib>=1.2.0 (from scikitlearn)   Downloading joblib1.4.2py3noneany.whl.metadata (5.4 kB) Collecting threadpoolctl>=3.1.0 (from scikitlearn)   Downloading threadpoolctl3.6.0py3noneany.whl.metadata (13 kB) Downloading scikit_learn1.6.1cp310cp310win_amd64.whl (11.1 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.1/11.1 MB 3.7 MB/s eta 0:00:00 Downloading joblib1.4.2py3noneany.whl (301 kB) Downloading scipy1.15.2cp310cp310win_amd64.whl (41.2 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.2/41.2 MB 3.1 MB/s eta 0:00:00 Downloading threadpoolctl3.6.0py3noneany.whl (18 kB) Installing collected packages: threadpoolctl, scipy, joblib, scikitlearn Successfully installed joblib1.4.2 scikitlearn1.6.1 scipy1.15.2 threadpoolctl3.6.0 20250329 19:56:01.625860: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found 20250329 19:56:01.627102: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. A module that was compiled using NumPy 1.x cannot be run in NumPy 2.2.4 as it may crash. To support both 1.x and 2.x versions of NumPy, modules must be compiled with NumPy 2.0. Some module may need to rebuild instead e.g. with 'pybind11>=2.12'. If you are a user of the module, the easiest solution will be to downgrade to 'numpy     import tensorflow as tf   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\__init__.py"", line 37, in      from tensorflow.python.tools import module_util as _module_util   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\__init__.py"", line 37, in      from tensorflow.python.eager import context   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\eager\context.py"", line 35, in      from tensorflow.python.client import pywrap_tf_session   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\client\pywrap_tf_session.py"", line 19, in      from tensorflow.python.client._pywrap_tf_session import * AttributeError: _ARRAY_API not found A module that was compiled using NumPy 1.x cannot be run in NumPy 2.2.4 as it may crash. To support both 1.x and 2.x versions of NumPy, modules must be compiled with NumPy 2.0. Some module may need to rebuild instead e.g. with 'pybind11>=2.12'. If you are a user of the module, the easiest solution will be to downgrade to 'numpy     import tensorflow as tf   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\__init__.py"", line 37, in      from tensorflow.python.tools import module_util as _module_util   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\__init__.py"", line 42, in      from tensorflow.python import data   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\__init__.py"", line 21, in      from tensorflow.python.data import experimental   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\__init__.py"", line 96, in      from tensorflow.python.data.experimental import service   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\service\__init__.py"", line 419, in      from tensorflow.python.data.experimental.ops.data_service_ops import distribute   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\ops\data_service_ops.py"", line 24, in      from tensorflow.python.data.experimental.ops import compression_ops   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\ops\compression_ops.py"", line 16, in      from tensorflow.python.data.util import structure   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\util\structure.py"", line 23, in      from tensorflow.python.data.util import nest   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\util\nest.py"", line 36, in      from tensorflow.python.framework import sparse_tensor as _sparse_tensor   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\framework\sparse_tensor.py"", line 24, in      from tensorflow.python.framework import constant_op   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\framework\constant_op.py"", line 25, in      from tensorflow.python.eager import execute   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\eager\execute.py"", line 23, in      from tensorflow.python.framework import dtypes   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\framework\dtypes.py"", line 29, in      from tensorflow.python.lib.core import _pywrap_bfloat16 AttributeError: _ARRAY_API not found ImportError: numpy.core._multiarray_umath failed to import ImportError: numpy.core.umath failed to import Traceback (most recent call last):   File ""C:\xampp\htdocs\SkinToneAnalysis\skin_analysis_model\train_model.py"", line 2, in      import tensorflow as tf   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\__init__.py"", line 37, in      from tensorflow.python.tools import module_util as _module_util   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\__init__.py"", line 42, in      from tensorflow.python import data   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\__init__.py"", line 21, in      from tensorflow.python.data import experimental   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\__init__.py"", line 96, in      from tensorflow.python.data.experimental import service   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\service\__init__.py"", line 419, in      from tensorflow.python.data.experimental.ops.data_service_ops import distribute   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\ops\data_service_ops.py"", line 24, in      from tensorflow.python.data.experimental.ops import compression_ops   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\experimental\ops\compression_ops.py"", line 16, in      from tensorflow.python.data.util import structure   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\util\structure.py"", line 23, in      from tensorflow.python.data.util import nest   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\data\util\nest.py"", line 36, in      from tensorflow.python.framework import sparse_tensor as _sparse_tensor   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\framework\sparse_tensor.py"", line 24, in      from tensorflow.python.framework import constant_op   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\framework\constant_op.py"", line 25, in      from tensorflow.python.eager import execute   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\eager\execute.py"", line 23, in      from tensorflow.python.framework import dtypes   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\framework\dtypes.py"", line 34, in      _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type() TypeError: Unable to convert function return value to a Python type! The signature was         () > handle PS C:\xampp\htdocs\SkinToneAnalysis> conda create name tf_env python=3.9 >> conda activate tf_env >> pip install tensorflow >> conda : The term 'conda' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the  name, or if a path was included, verify that the path is correct and try again. At line:1 char:1 + conda create name tf_env python=3.9 + ~~~~~     + CategoryInfo          : ObjectNotFound: (conda:String) [], CommandNotFoundException     + FullyQualifiedErrorId : CommandNotFoundException conda : The term 'conda' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the  name, or if a path was included, verify that the path is correct and try again. At line:2 char:1 + conda activate tf_env + ~~~~~     + CategoryInfo          : ObjectNotFound: (conda:String) [], CommandNotFoundException     + FullyQualifiedErrorId : CommandNotFoundException Requirement already satisfied: tensorflow in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (2.10.0) Requirement already satisfied: abslpy>=1.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.2.1) Requirement already satisfied: astunparse>=1.6.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.6.3) Requirement already satisfied: flatbuffers>=2.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (25.2.10) Requirement already satisfied: gast=0.2.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (0.4.0) Requirement already satisfied: googlepasta>=0.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (0.2.0) Requirement already satisfied: h5py>=2.9.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (3.13.0) Requirement already satisfied: keraspreprocessing>=1.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.1.2) Requirement already satisfied: libclang>=13.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (18.1.1) Requirement already satisfied: numpy>=1.20 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.2.4) Requirement already satisfied: opteinsum>=2.3.2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (3.4.0) Requirement already satisfied: packaging in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (24.2) Requirement already satisfied: protobuf=3.9.2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (3.19.6) Requirement already satisfied: setuptools in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (78.1.0) Requirement already satisfied: six>=1.12.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.17.0) Requirement already satisfied: termcolor>=1.1.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.5.0) Requirement already satisfied: typingextensions>=3.6.6 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (4.13.0) Requirement already satisfied: wrapt>=1.11.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.17.2) Requirement already satisfied: tensorflowiogcsfilesystem>=0.23.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (0.31.0) Requirement already satisfied: grpcio=1.24.3 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.71.0) Requirement already satisfied: tensorboard=2.10 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.10.1) Requirement already satisfied: tensorflowestimator=2.10.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.10.0) Requirement already satisfied: keras=2.10.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.10.0) Requirement already satisfied: wheel=0.23.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from astunparse>=1.6.0>tensorflow) (0.45.1) Requirement already satisfied: googleauth=1.6.3 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow) (2.38.0) Requirement already satisfied: googleauthoauthlib=0.4.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow) (0.4.6) Requirement already satisfied: markdown>=2.6.8 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow) (3.7) Requirement already satisfied: requests=2.21.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow) (2.32.3) Requirement already satisfied: tensorboarddataserver=0.6.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow) (0.6.1) Requirement already satisfied: tensorboardpluginwit>=1.6.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow) (1.8.1) Requirement already satisfied: werkzeug>=1.0.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard=2.10>tensorflow) (3.1.3) Requirement already satisfied: cachetools=2.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauth=1.6.3>tensorboard=2.10>tensorflow) (5.5.2) Requirement already satisfied: pyasn1modules>=0.2.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauth=1.6.3>tensorboard=2.10>tensorflow) (0.4.2) Requirement already satisfied: rsa=3.1.4 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauth=1.6.3>tensorboard=2.10>tensorflow) (4.9) Requirement already satisfied: requestsoauthlib>=0.7.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from googleauthoauthlib=0.4.1>tensorboard=2.10>tensorflow) (2.0.0)   Requirement already satisfied: charsetnormalizer=2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow) (3.4.1) Requirement already satisfied: idna=2.5 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow) (3.10) Requirement already satisfied: urllib3=1.21.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow) (2.3.0) Requirement already satisfied: certifi>=2017.4.17 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorboard=2.10>tensorflow) (2025.1.31) Requirement already satisfied: MarkupSafe>=2.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from werkzeug>=1.0.1>tensorboard=2.10>tensorflow) (3.0.2) Requirement already satisfied: pyasn1=0.6.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from pyasn1modules>=0.2.1>googleauth=1.6.3>tensorboard=2.10>tensorflow) (0.6.1) Requirement already satisfied: oauthlib>=3.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requestsoauthlib>=0.7.0>googleauthoauthlib=0.4.1>tensorboard=2.10>tensorflow) (3.2.2) PS C:\xampp\htdocs\SkinToneAnalysis>  >> pip install upgrade tensorflow >> Requirement already satisfied: tensorflow in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (2.10.0) Collecting tensorflow   Downloading tensorflow2.19.0cp310cp310win_amd64.whl.metadata (4.1 kB) Requirement already satisfied: abslpy>=1.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.2.1) Requirement already satisfied: astunparse>=1.6.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.6.3) Requirement already satisfied: flatbuffers>=24.3.25 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (25.2.10) Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (0.4.0) Requirement already satisfied: googlepasta>=0.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (0.2.0) Requirement already satisfied: libclang>=13.0.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (18.1.1) Requirement already satisfied: opteinsum>=2.3.2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (3.4.0) Requirement already satisfied: packaging in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (24.2) Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,=3.20.3 (from tensorflow)   Downloading protobuf5.29.4cp310abi3win_amd64.whl.metadata (592 bytes) Requirement already satisfied: requests=2.21.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.32.3) Requirement already satisfied: setuptools in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (78.1.0) Requirement already satisfied: six>=1.12.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.17.0) Requirement already satisfied: termcolor>=1.1.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (2.5.0) Requirement already satisfied: typingextensions>=3.6.6 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (4.13.0) Requirement already satisfied: wrapt>=1.11.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.17.2) Requirement already satisfied: grpcio=1.24.3 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (1.71.0) Collecting tensorboard~=2.19.0 (from tensorflow)   Downloading tensorboard2.19.0py3noneany.whl.metadata (1.8 kB) Collecting keras>=3.5.0 (from tensorflow)   Downloading keras3.9.1py3noneany.whl.metadata (6.1 kB) Collecting numpy=1.26.0 (from tensorflow)   Downloading numpy2.1.3cp310cp310win_amd64.whl.metadata (60 kB) Requirement already satisfied: h5py>=3.11.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (3.13.0) Collecting mldtypes=0.5.1 (from tensorflow)   Downloading ml_dtypes0.5.1cp310cp310win_amd64.whl.metadata (22 kB) Requirement already satisfied: tensorflowiogcsfilesystem>=0.23.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorflow) (0.31.0) Requirement already satisfied: wheel=0.23.0 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from astunparse>=1.6.0>tensorflow) (0.45.1) Collecting rich (from keras>=3.5.0>tensorflow)   Downloading rich13.9.4py3noneany.whl.metadata (18 kB) Collecting namex (from keras>=3.5.0>tensorflow)   Downloading namex0.0.8py3noneany.whl.metadata (246 bytes) Collecting optree (from keras>=3.5.0>tensorflow)   Downloading optree0.14.1cp310cp310win_amd64.whl.metadata (50 kB) Requirement already satisfied: charsetnormalizer=2 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorflow) (3.4.1) Requirement already satisfied: idna=2.5 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorflow) (3.10) Requirement already satisfied: urllib3=1.21.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorflow) (2.3.0) Requirement already satisfied: certifi>=2017.4.17 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from requests=2.21.0>tensorflow) (2025.1.31) Requirement already satisfied: markdown>=2.6.8 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard~=2.19.0>tensorflow) (3.7) Collecting tensorboarddataserver=0.7.0 (from tensorboard~=2.19.0>tensorflow)   Downloading tensorboard_data_server0.7.2py3noneany.whl.metadata (1.1 kB) Requirement already satisfied: werkzeug>=1.0.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from tensorboard~=2.19.0>tensorflow) (3.1.3) Requirement already satisfied: MarkupSafe>=2.1.1 in c:\users\user pc\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\localpackages\python310\sitepackages (from werkzeug>=1.0.1>tensorboard~=2.19.0>tensorflow) (3.0.2) Collecting markdownitpy>=2.2.0 (from rich>keras>=3.5.0>tensorflow)   Downloading markdown_it_py3.0.0py3noneany.whl.metadata (6.9 kB) Collecting pygments=2.13.0 (from rich>keras>=3.5.0>tensorflow)   Downloading pygments2.19.1py3noneany.whl.metadata (2.5 kB) Collecting mdurl~=0.1 (from markdownitpy>=2.2.0>rich>keras>=3.5.0>tensorflow)   Downloading mdurl0.1.2py3noneany.whl.metadata (1.6 kB) Downloading tensorflow2.19.0cp310cp310win_amd64.whl (375.7 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 375.7/375.7 MB 2.0 MB/s eta 0:00:00 Downloading keras3.9.1py3noneany.whl (1.3 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 2.0 MB/s eta 0:00:00 Downloading ml_dtypes0.5.1cp310cp310win_amd64.whl (209 kB) Downloading numpy2.1.3cp310cp310win_amd64.whl (12.9 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/12.9 MB 2.2 MB/s eta 0:00:00 Downloading protobuf5.29.4cp310abi3win_amd64.whl (434 kB) Downloading tensorboard2.19.0py3noneany.whl (5.5 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 2.6 MB/s eta 0:00:00 Downloading tensorboard_data_server0.7.2py3noneany.whl (2.4 kB) Downloading namex0.0.8py3noneany.whl (5.8 kB) Downloading optree0.14.1cp310cp310win_amd64.whl (296 kB) Downloading rich13.9.4py3noneany.whl (242 kB) Downloading markdown_it_py3.0.0py3noneany.whl (87 kB) Downloading pygments2.19.1py3noneany.whl (1.2 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 742.6 kB/s eta 0:00:00 Downloading mdurl0.1.2py3noneany.whl (10.0 kB) Installing collected packages: namex, tensorboarddataserver, pygments, protobuf, optree, numpy, mdurl, tensorboard, mldtypes, markdownitpy, rich, keras, tensorflow   Attempting uninstall: tensorboarddataserver     Found existing installation: tensorboarddataserver 0.6.1     Uninstalling tensorboarddataserver0.6.1:       Successfully uninstalled tensorboarddataserver0.6.1   WARNING: The script pygmentize.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   Attempting uninstall: protobuf     Found existing installation: protobuf 3.19.6     Uninstalling protobuf3.19.6:       Successfully uninstalled protobuf3.19.6   Attempting uninstall: numpy     Found existing installation: numpy 2.2.4     Uninstalling numpy2.2.4:       Successfully uninstalled numpy2.2.4   WARNING: The scripts f2py.exe and numpyconfig.exe are installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   Attempting uninstall: tensorboard     Found existing installation: tensorboard 2.10.1     Uninstalling tensorboard2.10.1:       Successfully uninstalled tensorboard2.10.1   WARNING: The script tensorboard.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   WARNING: The script markdownit.exe is installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation.   Attempting uninstall: keras     Found existing installation: keras 2.10.0     Uninstalling keras2.10.0:       Successfully uninstalled keras2.10.0   Attempting uninstall: tensorflow     Found existing installation: tensorflow 2.10.0     Uninstalling tensorflow2.10.0:       Successfully uninstalled tensorflow2.10.0   WARNING: The scripts import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe and toco.exe are installed in 'C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use nowarnscriptlocation. Successfully installed keras3.9.1 markdownitpy3.0.0 mdurl0.1.2 mldtypes0.5.1 namex0.0.8 numpy2.1.3 optree0.14.1 protobuf5.29.4 pygments2.19.1 rich13.9.4 tensorboard2.19.0 tensorboarddataserver0.7.2 tensorflow2.19.0 PS C:\xampp\htdocs\SkinToneAnalysis> python skin_analysis_model/train_model.py Traceback (most recent call last):   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\xampp\htdocs\SkinToneAnalysis\skin_analysis_model\train_model.py"", line 2, in      import tensorflow as tf   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. PS C:\xampp\htdocs\SkinToneAnalysis> ```  Relevant log output ```shell ```",2025-03-29T17:11:20Z,stat:awaiting response type:support stale TF 2.18,closed,0,9,https://github.com/tensorflow/tensorflow/issues/90245,What is all of this ,"Hi  , Apologies for the delay, and thanks for raising your concern here. Could you please specify exactly what you are trying to install? Additionally, could you provide the installation steps or code where you are encountering the issue? This will help us debug the problem more effectively. In the meantime, I am sharing the official documentation to check compatibility versions, it might be helpful to you. Looking forward to your response. Thank you!","Hi Venkat6871, Thanks so much for your help and patience! I'm really trying to learn this TensorFlow stuff on my own, without any formal classes, which is proving to be quite a challenge. I'm struggling a bit with the installation, and I'm wondering if you could recommend any good courses or training programs that might help me get a firmer grasp on things? Any suggestions you have would be greatly appreciated! Thanks again for your support! Best, Christopher Swain Sent from Yahoo Christopher Swain is the only one who can make a difference with the new system that we have installed on our website.Mail for iPhone.. On Tuesday, April 1, 2025, 9:43 AM, Venkat6871 ***@***.***> wrote: Hi  , Apologies for the delay, and thanks for raising your concern here. Could you please specify exactly what you are trying to install? Additionally, could you provide the installation steps or code where you are encountering the issue? This will help us debug the problem more effectively. In the meantime, I am sharing the official documentation to check compatibility versions, it might be helpful to you. Looking forward to your response. Thank you! — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***> Venkat6871 left a comment (tensorflow/tensorflow CC(Error fixing)) Hi  , Apologies for the delay, and thanks for raising your concern here. Could you please specify exactly what you are trying to install? Additionally, could you provide the installation steps or code where you are encountering the issue? This will help us debug the problem more effectively. In the meantime, I am sharing the official documentation to check compatibility versions, it might be helpful to you. Looking forward to your response. Thank you! — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***>","It seems there are 2 errors here. First, you are trying to combine code compiled with Numpy v1 with code compiled with Numpy v2. This cannot work. Next, you are running into CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.)",Don't have a clue what you're talking about?,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"This is now resolved, Thank you for your contributions.",Are you satisfied with the resolution of your issue? Yes No,"Dear Christopher Swain, Thank you for your feedback regarding the resolution of issue CC(Error fixing) on the TensorFlow GitHub repository. We appreciate you taking the time to let us know you are satisfied. Your input helps us improve our processes and better assist users in the future. If you encounter any further issues, please don't hesitate to open a new issue or utilize the existing channels for support. We value your contributions to the TensorFlow community. Sent from Yahoo Christopher Swain is the only one who can make a difference with the new system that we have installed on our website.Mail for iPhone.. On Tuesday, April 22, 2025, 12:54 AM, googlemlbutler[bot] ***@***.***> wrote: Are you satisfied with the resolution of your issue? Yes No — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***> googlemlbutler[bot] left a comment (tensorflow/tensorflow CC(Error fixing)) Are you satisfied with the resolution of your issue? Yes No — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***>"
RahulShridhar10,During pyinstaller," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19  Custom code Yes  OS platform and distribution Windows  Mobile device _No response_  Python version 3.11.7  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? This error happens because _pywrap_tensorflow_internal.pyd depends on missing or incompatible DLLs. Here’s how to fix it:  1. Check Python and TensorFlow Compatibility Run this to check your Python version: python version Now check which TensorFlow version supports your Python version: TensorFlow Python Compatibility If Python is 3.11+, TensorFlow 2.10 or earlier won’t work. If Python is 3.83.10, it should be fine. If needed, downgrade TensorFlow: pip install tensorflow==2.10.0  2. Ensure Microsoft Visual C++ Redistributable is Installed TensorFlow requires Microsoft Visual C++ 20152022 Redistributable. Download and install: VC++ Redistributable (x64) Then restart your computer.  3. Verify Missing DLLs with Dependency Walker 1. Download Dependency Walker here. 2. Open _pywrap_tensorflow_internal.pyd with it. 3. If any DLL is missing (marked in red), install it.  4. Add TensorFlow’s DLL Path to Environment Variables Run this in Command Prompt (Admin): set PATH=%PATH%;C:\Users\Rahul\OneDrive\Desktop\Reader\Palm_Leaf_Reader\Backend\venv\Lib\sitepackages\tensorflow\ Then retry running backend.exe.  5. Rebuild PyInstaller Executable Run PyInstaller with these options: pyinstaller onefile hiddenimport=tensorflow hiddenimport=tensorflow.python adddata ""venv\Lib\sitepackages\tensorflow;tensorflow"" name backend app.py Then run the new backend.exe.  Try these and let me know if the issue persists!  Standalone code to reproduce the issue ```shell Traceback (most recent call last): File ""tensorflow\python\pywrap_tensorflow.py"", line 73, in  ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization ro utine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""app.py"", line 22, in  File ""PyInstaller\loader\pyimod02_importers.py"", line 450, in exec_module File ""tensorflow\ __ init __. py"", line 40, in  File ""PyInstaller\loader\pyimod02_importers.py"", line 450, in exec_module File ""tensorflow\python\pywrap_tensorflow.py"", line 88, in  ImportError: Traceback (most recent call last): File ""tensorflow\python\pywrap_tensorflow.py"", line 73, in  ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization ro utine failed. + × Failed to load the native TensorFlow runtime. See https://www. tensorflow. org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. [PYI3652:ERROR] Failed to execute script 'app' due to unhandled exception! ```  Relevant log output ```shell ```",2025-03-29T17:03:41Z,stat:awaiting response type:build/install subtype:windows TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/90244,"Hi  , Apologies for the delay, could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!", CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.),Are you satisfied with the resolution of your issue? Yes No
default1360,`Aborted` in `tf.raw_ops.Unbatch`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.19.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tf.raw_ops.Unbatch in tf2.19.0. This bug can be used to trigger a denial of service attack. I have attached a gist for your reference.  Standalone code to reproduce the issue ```shell import tensorflow as tf from tensorflow.raw_ops import Unbatch batched_tensor = tf.constant([[1.0], [2.0], [3.0]], dtype=tf.float32) batch_index = tf.constant([0], dtype=tf.int64) id = tf.constant([0], dtype=tf.int64) timeout_micros = 1000000 unbatched_result = Unbatch(batched_tensor=batched_tensor, batch_index=batch_index, id=id, timeout_micros=timeout_micros) ```  Relevant log output ```shell Aborted ```",2025-03-29T13:03:28Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90243,Are you satisfied with the resolution of your issue? Yes No
default1360,`Aborted` in `tf.raw_ops.Transpose`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.19.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tf.raw_ops.Transpose in tf2.19.0. This bug can be used to trigger a denial of service attack. I have attached a gist for your reference.  Standalone code to reproduce the issue ```shell import tensorflow as tf def test_bug():     x = tf.zeros((3, 3), dtype=tf.float32)     x = tf.Variable(x, trainable=True)     permutation = tf.constant([1, 2])     y = tf.raw_ops.Transpose(x=x, perm=permutation)     with tf.GradientTape() as tape:         z = tf.reduce_sum(y)     z.backward() if __name__ == ""__main__"":     test_bug() ```  Relevant log output ```shell Aborted ```",2025-03-29T13:01:20Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90242,Are you satisfied with the resolution of your issue? Yes No
default1360,`Aborted` in `tf.raw_ops.BatchMatMulV2`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tf.raw_ops.BatchMatMulV2 in tf2.19.0. This bug can be used to trigger a denial of service attack. I have attached a gist for your reference.  Standalone code to reproduce the issue ```shell import tensorflow as tf import time def test_bug():     tf.config.experimental_run_functions_eagerly(True)     A = tf.fill((100, 100), 1e45)     B = tf.random.uniform((100, 100), minval=0, maxval=1, dtype=tf.float32)     for _ in range(10):         tf.raw_ops.BatchMatMulV2(x=A[tf.newaxis, ...], y=B[tf.newaxis, ...])     n_iter = 300     times = []     X = A     for i in range(n_iter):         t0 = time.time()         X = tf.raw_ops.BatchMatMulV2(x=X[tf.newaxis, ...], y=B[tf.newaxis, ...])         t1 = time.time()         times.append(t1  t0)     early_avg = sum(times[:50]) / 50     late_avg  = sum(times[50:]) / 50 if __name__ == ""__main__"":     test_bug() ```  Relevant log output ```shell Aborted ```",2025-03-29T13:00:07Z,stat:awaiting tensorflower type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/90241,"I can reproduce this issue in TensorFlow 2.19.0. The problem occurs when BatchMatMulV2 processes extremely small float32 values (near 1e45) in repeated multiplications, causing numerical underflow that results in a hard crash instead of graceful error handling. Reproduction case attached shows how these operations cause an ""Aborted"" error when values cascade below representable limits. Potential fixes could include: 1. Adding guards in BatchMatMulV2 implementation to detect potential underflow conditions 2. Gracefully raising exceptions instead of aborting when numerical instability occurs 3. Implementing value clamping to prevent catastrophic underflow This is a potential security concern since it can be used for denial of service attacks. I'm willing to work on a fix if guidance is provided on the appropriate approach. Would checking for extremely small values in the C++ implementation be the right direction?","I was able to reproduce this issue using TensorFlow  2.19.0, and the nightly version. Please find the gist attached for reference. Thank you!"
SarahClementine,Failed," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.9  Custom code Yes  OS platform and distribution win 11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Traceback (most recent call last):   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""c:\xampp\htdocs\SkinToneAnalysis\skin_analysis_model\train_model.py"", line 2, in      import tensorflow as tf   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.  Standalone code to reproduce the issue ```shell Traceback (most recent call last):   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""c:\xampp\htdocs\SkinToneAnalysis\skin_analysis_model\train_model.py"", line 2, in      import tensorflow as tf   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\USER PC\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\localpackages\Python310\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```  Relevant log output ```shell ```",2025-03-29T12:17:00Z,stat:awaiting response type:build/install subtype:windows TF 2.9,closed,0,2,https://github.com/tensorflow/tensorflow/issues/90240,"Hi  , Apologies for the delay, could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Are you satisfied with the resolution of your issue? Yes No
lixiaohui2020,undefined symbol: _ZN6tflite4impl18InterpreterBuilderclEPNSt3__110unique_ptrINS0_11InterpreterENS2_14default_deleteIS4_EEEE," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version no  Custom code No  OS platform and distribution Linux Ubuntu 16.04  Mobile device Android 15  Python version 3.9  Bazel version 7.4.1  GCC/compiler version clang14  CUDA/cuDNN version no  GPU model and memory no  Current behavior? When i build the application based tensorflowLite.so at the android platform, it shows ""undefined symbol: _ZN6tflite4impl18InterpreterBuilderclEPNSt3__110unique_ptrINS0_11InterpreterENS2_14default_deleteIS4_EEEE. I check the tensorflowLite.so by using the command nm D tensorflowLite.so | grep InterpreterBuilder, found _ZN6tflite4impl18InterpreterBuilderclEPNSt3__110unique_ptrINS0_11InterpreterENS2_14default_deleteIS4_EEEE. I cannot how to solve this problem, thanks My Android.bp is: cc_library {     name: ""libaw_speech_enhencement_reagle"", 	sdk_version:""current"", 	stl:""libc++"",     vendor_available: true, 	compile_multilib: ""both"",     export_include_dirs: [ 	""."", 	""./include"", 	""./3rdparty/include"",     ], 	rtti: true, 	cflags: [         ""Wall"",         ""Werror"",         ""Wextra"",         ""Wnounusedparameter"", 		""Wnounusedvariable"",         ""Wnounusedprivatefield"", 		""Wnounusedfunction"", 		""Wnowritablestrings"", 		""Wnononpodvarargs"",         ""frtti"",     ],     shared_libs: [ 		""liblog"", 		""libtensorflowLite"", 	],     static_libs:[ 		""libomp"", 	],     srcs: [         ""./src/*.cpp"",     ], 	header_libs: [         ""jni_headers"",     ], } cc_prebuilt_library_shared {     name: ""libtensorflowLite"",     sdk_version:""current"",     vendor_available: true,     arch: {         arm: {             srcs: [                 ""3rdparty/nativeLibs/armeabiv7a/libtensorflowLite.so"",             ],         },         arm64: {             srcs: [                 ""3rdparty/nativeLibs/arm64v8a/libtensorflowLite.so"",             ],         },     },     shared_libs: [         ""liblog"",     ], }  Standalone code to reproduce the issue ```shell My xx.: ```  Relevant log output ```shell ```",2025-03-29T09:20:10Z,stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux,closed,0,4,https://github.com/tensorflow/tensorflow/issues/90239,"Hi,  Apologize for the delay in my response, Please make sure that the prebuilt `libtensorflowLite.so` was compiled with the same NDK version as your project. Older NDKs (e.g. < r18) may cause symbol mismatches due to STL differences I believe you followed Set up build environment without Docker section of official documentation steps If possible could you please help us with minimal code or Github repo along with complete steps to replicate the similar behavior from our end to investigate this issue further from our end ? Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-29T09:09:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90238
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-29T06:57:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90237
luvvien,How to train an efficientdet-lite0 model without post-processing？,"I want to run the model on the GPU of iOS. Since the model trained by tflitemodelmaker has postprocessing, but the postprocessing part must run on the GPU, how can I train a tflite model without postprocessing?",2025-03-29T06:35:58Z,stat:awaiting response stale comp:lite TFLiteConverter,closed,0,4,https://github.com/tensorflow/tensorflow/issues/90236,"Traceback (most recent call last):   File ""C:\Users\adars\anaconda3\envs\Gatim_datascience\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\Users\adars\OneDrive\Desktop\PROJECT2\faceauthpython\main.py"", line 9, in             from chatbot.face import get_embedding, detect_face   File ""C:\Users\adars\OneDrive\Desktop\PROJECT2\faceauthpython\chatbot\face.py"", line 3, in      from deepface import DeepFace   File ""C:\Users\adars\anaconda3\envs\Gatim_datascience\Lib\sitepackages\deepface\DeepFace.py"", line 15, in      import tensorflow as tf   File ""C:\Users\adars\anaconda3\envs\Gatim_datascience\Lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\adars\anaconda3\envs\Gatim_datascience\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):       File ""C:\Users\adars\anaconda3\envs\Gatim_datascience\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime.       See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.   ","Hi,   I apologize for the delay in my response, As far I know for training `EfficientDetLite0` without postprocessing you need to follow below steps if I'm not wrong 1. Modify the model architecture during training when using TensorFlow Lite Model Maker you can modify the model specification to exclude the postprocessing operation something like below : ``` import tensorflow as tf from tflite_model_maker import object_detector from tflite_model_maker.object_detector import DataLoader Step 1: Load your training data train_data, validation_data, test_data = DataLoader.from_csv('your_dataset.csv') Step 2: Get the base model spec spec = object_detector.EfficientDetSpec.get('efficientdet_lite0') Step 3: Disable postprocessing by modifying the spec spec.config.use_regular_nms = False   Disable TFLite_Detection_PostProcess Step 4: Train the model model = object_detector.create(     train_data,      model_spec=spec,      batch_size=8,      train_whole_model=True,      validation_data=validation_data ) Step 5: Export without postprocessing model.export(export_dir='.', tflite_filename='model_without_postprocess.tflite',               quantization_config=None, export_format=ExportFormat.TFLITE) ``` 2. Once you have a model without the postprocessing step you'll need to implement your own postprocessing in your iOS application something like below : ``` // Swift implementation of postprocessing func performDetectionPostProcessing(rawOutputs: [Float], inputWidth: Int, inputHeight: Int) > [Detection] {     // 1. Decode the raw outputs to get box predictions, class scores, etc.     // 2. Apply NonMaximum Suppression (NMS)     // 3. Apply score thresholding     // 4. Convert coordinates to the original image space     // Return your custom detection objects     return detections } ``` 3. To use GPU acceleration with your modified model please refer official documentation of GPU acceleration delegate for iOS If I have missed something here please let me know. Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-29T06:18:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90235
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-29T05:21:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90234
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-29T04:23:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90233
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-29T04:23:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90232
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-29T04:23:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90231
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-29T04:21:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90230
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-29T04:14:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90229
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-29T04:12:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90228
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-29T04:08:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90227
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-29T04:08:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90226
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-29T04:07:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90225
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-29T04:07:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90224
richi1325,Mejora: actualicé el README,"Esta branch tiene como objetivo probar el flujo de trabajo colaborativo y funcionalidades como Forks, Pull Requests, Issues y Discussions. **Cambios:**  README.md se realizó un cambio donde se incluyeron líneas adicionales al archivo.",2025-03-29T01:54:27Z,size:XS,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90223,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Fix IfrtLoadVariable is deleted while still in use,Fix IfrtLoadVariable is deleted while still in use,2025-03-29T01:54:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90222
copybara-service[bot],Integrate LLVM at llvm/llvm-project@c0952a931c7d,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match c0952a931c7d,2025-03-29T01:53:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90221
copybara-service[bot],Explicitly set untuple results for tuplized results.,Explicitly set untuple results for tuplized results.,2025-03-29T00:03:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90220
copybara-service[bot],Add HLO->MLIR conversion for result accuracy.,Add HLO>MLIR conversion for result accuracy. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89671 from tensorflow:gaikwadrahul8patch2 7dc87202aba4812d1f6d72e742d6254282e01e01,2025-03-28T22:49:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90219
copybara-service[bot],[IFRT Proxy] Add a tracking number for handling a user context at a few IFRT API calls,[IFRT Proxy] Add a tracking number for handling a user context at a few IFRT API calls,2025-03-28T22:39:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90218
copybara-service[bot],Let `non_blocking_thread_pool` handle `on_done` in `AsyncHostToDeviceTransferManager`,Let `non_blocking_thread_pool` handle `on_done` in `AsyncHostToDeviceTransferManager`,2025-03-28T22:28:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90217
copybara-service[bot],[MHLO] Migrate shape analysis passes for pre-HLO lowering to StableHLO,[MHLO] Migrate shape analysis passes for preHLO lowering to StableHLO,2025-03-28T22:05:13Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/90216,"This pull request sets up GitHub code scanning for this repository. Once the scans have completed and the checks have passed, the analysis results for this pull request branch will appear on this overview. Once you merge this pull request, the 'Security' tab will show more code scanning analysis results (for example, for the default branch). Depending on your configuration and choice of analysis tool, future pull requests will be annotated with code scanning analysis results. For more information about GitHub code scanning, check out the documentation. "
copybara-service[bot],Integrate LLVM at llvm/llvm-project@c0952a931c7d,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match c0952a931c7d,2025-03-28T21:47:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90215
copybara-service[bot],Typo fix in constant folding: s/kAgressive/kAggressive/g,Typo fix in constant folding: s/kAgressive/kAggressive/g,2025-03-28T20:55:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90214
siddij3,SeparableConv1D with causal padding is not supported," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version v1.12.1124066gef70275b27b 2.20.0dev20250328  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version Python 3.11.5   Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I had a download of tensorflow where tensorflow.keras.layers.SeparableConv1D had the padding layer of causal. I went through dependency hell, wiped everything and reinstalled the pip packages, and when trying to rerun my model training code, I get  ``` ValueError: The `padding` argument must be a list/tuple or one of {'valid', 'same'}. Received: causal. ``` It's very clearly not supported, but I was running it fine yesterday. I can go into the source code and just permit ""causal"" but I don't know if that would break something else.  Standalone code to reproduce the issue ```shell import tensorflow as tf print(tf.__version__) print(tf.version.GIT_VERSION, tf.version.VERSION) model = tf.keras.Sequential() model.add(tf.keras.Input((None,None,3))) model.add(tf.keras.layers.SeparableConv1D(                     dilation_rate=1,                     filters = 3,                     kernel_size=2,                     padding='causal', )) ```  Relevant log output ```shell ValueError                                Traceback (most recent call last) Cell In[1], line 7       5 model = tf.keras.Sequential()       6 model.add(tf.keras.Input((None,None,3))) > 7 model.add(tf.keras.layers.SeparableConv1D(       8                     dilation_rate=1,       9                     filters = 3,      10                     kernel_size=2,      11                     padding='causal',      12 )) File c:\Program Files\Python311\Lib\sitepackages\keras\src\layers\convolutional\separable_conv1d.py:121, in SeparableConv1D.__init__(self, filters, kernel_size, strides, padding, data_format, dilation_rate, depth_multiplier, activation, use_bias, depthwise_initializer, pointwise_initializer, bias_initializer, depthwise_regularizer, pointwise_regularizer, bias_regularizer, activity_regularizer, depthwise_constraint, pointwise_constraint, bias_constraint, **kwargs)      98 def __init__(      99     self,     100     filters,    (...)    119     **kwargs,     120 ): > 121     super().__init__(     122         rank=1,     123         depth_multiplier=depth_multiplier,     124         filters=filters,     125         kernel_size=kernel_size,     126         strides=strides,     127         padding=padding,     128         data_format=data_format,     129         dilation_rate=dilation_rate,     130         activation=activation,     131         use_bias=use_bias,     132         depthwise_initializer=depthwise_initializer,     133         pointwise_initializer=pointwise_initializer,     134         bias_initializer=bias_initializer,     135         depthwise_regularizer=depthwise_regularizer,     136         pointwise_regularizer=pointwise_regularizer,     137         bias_regularizer=bias_regularizer,     138         activity_regularizer=activity_regularizer,     139         depthwise_constraint=depthwise_constraint,     140         pointwise_constraint=pointwise_constraint,     141         bias_constraint=bias_constraint,     142         **kwargs,     143     ) File c:\Program Files\Python311\Lib\sitepackages\keras\src\layers\convolutional\base_separable_conv.py:118, in BaseSeparableConv.__init__(self, rank, depth_multiplier, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, depthwise_initializer, pointwise_initializer, bias_initializer, depthwise_regularizer, pointwise_regularizer, bias_regularizer, activity_regularizer, depthwise_constraint, pointwise_constraint, bias_constraint, trainable, name, **kwargs)     114 self.strides = standardize_tuple(strides, rank, ""strides"")     115 self.dilation_rate = standardize_tuple(     116     dilation_rate, rank, ""dilation_rate""     117 ) > 118 self.padding = standardize_padding(padding)     119 self.data_format = standardize_data_format(data_format)     120 self.activation = activations.get(activation) File c:\Program Files\Python311\Lib\sitepackages\keras\src\utils\argument_validation.py:65, in standardize_padding(value, allow_causal)      63     allowed_values = {""valid"", ""same""}      64 if padding not in allowed_values: > 65     raise ValueError(      66         ""The `padding` argument must be a list/tuple or one of ""      67         f""{allowed_values}. ""      68         f""Received: {padding}""      69     )      70 return padding ValueError: The `padding` argument must be a list/tuple or one of {'same', 'valid'}. Received: causal ```",2025-03-28T20:38:57Z,stat:awaiting response type:support stale TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/90213,"Hi  , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow 2.19.0, and encountered the following error: ``` ValueError: The `padding` argument must be a list/tuple or one of {'same', 'valid'}. Received: causal ``` The main cause of this error is that TensorFlow only supports same or valid padding, as clearly mentioned in the official documentation. I am providing a gist here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Improve compile time by refactoring the order in which we count resource usage in scheduling_instruction_crosses_overlap_limit.,Improve compile time by refactoring the order in which we count resource usage in scheduling_instruction_crosses_overlap_limit.,2025-03-28T20:04:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90212
copybara-service[bot],Remove mlir lowerings for operations that are covered by Xla Builder and the mlir tests that were coving that unused code.,Remove mlir lowerings for operations that are covered by Xla Builder and the mlir tests that were coving that unused code.,2025-03-28T19:31:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90211
copybara-service[bot],[XLA:GPU] add generic triton emitter support checks for fusions,[XLA:GPU] add generic triton emitter support checks for fusions we now allow nested fusions with a specific backend config (at the moment they are operands of dots and concats but no special checks are performed for that). Note that '__triton' will not appear in practice right but we declare it supported anyway.,2025-03-28T19:04:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90210
copybara-service[bot],Add a method to remove a specific comp env type from `xla::CompilationEnvironments`,Add a method to remove a specific comp env type from `xla::CompilationEnvironments`,2025-03-28T19:00:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90209
copybara-service[bot],Fix some ifrt proxy tests to work outside Google.,Fix some ifrt proxy tests to work outside Google.,2025-03-28T18:56:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90208
copybara-service[bot],Support a dynamic-update-slice's operand coming from an entry computation parameter in HostOffloadLegalize.,Support a dynamicupdateslice's operand coming from an entry computation parameter in HostOffloadLegalize.,2025-03-28T18:54:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90207
copybara-service[bot],Enable loading CUDA redistributions in CPU Linux RBE configurations.,"Enable loading CUDA redistributions in CPU Linux RBE configurations. This change is made to prevent hermetic CUDA repositories cache invalidation between the builds with `config=cuda` and without it. It should speed up Github presubmit jobs. Currently CPU and GPU jobs use the machines in the same pool, and they share the RBE cache. Previously the cache was invalidated every time when `TF_NEED_CUDA` value changed between CPU and GPU builds, hence loading CUDA redistributions for GPU jobs took several minutes (see this job for example: all the test results are cached, but CUDA redistributions were still downloaded). With adding `repo_env USE_CUDA_REDISTRIBUTIONS=1` to RBE CPU linux job configurations, we load some CUDA redistributions once in RBE cache, and then reuse it between the jobs.",2025-03-28T18:37:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90206
copybara-service[bot],OSS NOOP change,OSS NOOP change Reverts 56d1b2306f948c83b2b243484a6c16c037696a85 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23790 from chaserileyroberts:chase/nccl_group 36ad18d8c604a6cb2559dcca8c29530beb5da888,2025-03-28T18:31:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90205
copybara-service[bot],[XLA:GPU] update checks of having a generic triton emitter on,[XLA:GPU] update checks of having a generic triton emitter on simplifies the testing setup plus makes sure that all legacy dots are going through the generic emitter,2025-03-28T18:25:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90204
copybara-service[bot],[xla:util] Add overloads to PackIntN and UnpackIntN to clean up jaxlib logic.,[xla:util] Add overloads to PackIntN and UnpackIntN to clean up jaxlib logic.,2025-03-28T18:06:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90203
copybara-service[bot],[XLA:CPU] Add AOT compilation support for `hlo_benchmark_test`,[XLA:CPU] Add AOT compilation support for `hlo_benchmark_test`,2025-03-28T18:05:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90202
copybara-service[bot],[xla:gpu] CommandBuffer: remove If and IfElse API that are unused in XLA,"[xla:gpu] CommandBuffer: remove If and IfElse API that are unused in XLA In XLA we only lower stablehlo.case (aka hlo.conditional) to Case command, and never use If (not even representable in HLO) or IfElse commands.",2025-03-28T18:05:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90201
copybara-service[bot],[xla:gpu] Add an HLO test for testing command buffers,[xla:gpu] Add an HLO test for testing command buffers Add endtoend HLO based test with explicit command buffers in the HLO to simplify testing nontrivial command buffers.,2025-03-28T18:03:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90200
copybara-service[bot],[IFRT] Take a user context in `ifrt::Client::MakeArraysFromHostBufferShards()`,"[IFRT] Take a user context in `ifrt::Client::MakeArraysFromHostBufferShards()` Some implementations of `ifrt::Client::MakeArraysFromHostBufferShards()` may make multiple calls to `ifrt::Client::MakeArrayFromHostBuffer()`, making it very useful to take the user context at `MakeArraysFromHostBufferShards()` and forward it to the calls. This change is applied in place to the API instead of being added to a new overload because there are few users of `MakeArraysFromHostBufferShards()` for the time being and the API change does not affect the current IFRT Proxy protocol.",2025-03-28T17:34:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90199
copybara-service[bot],Integrate LLVM at llvm/llvm-project@71a977d0d611,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 71a977d0d611,2025-03-28T17:28:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90198
copybara-service[bot],Reverts 56d1b2306f948c83b2b243484a6c16c037696a85,Reverts 56d1b2306f948c83b2b243484a6c16c037696a85,2025-03-28T17:01:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90197
copybara-service[bot],Reduce compile time when using large sharded shapes by constructing sharding_tree at most once per parameter.,Reduce compile time when using large sharded shapes by constructing sharding_tree at most once per parameter.,2025-03-28T16:48:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90196
copybara-service[bot],"`mhlo.copy` op, remove the folder","`mhlo.copy` op, remove the folder Motivation: `mhlo.copy` ODS spec has a `hasfolder` enabled. During MLIR dialect conversion, if the mhlo.copy op is not a legal op (example: during chlo > stablehlo conversion), MLIR try to legalize the op by folding it, if the op `hasfolder` enabled (`mlir/lib/Transforms/Utils/DialectConversion.cpp;l=20582067`). This results in removing the mhlo.copy op. Ideally: 1. `mhlo.copy` should be used only when it is needed during chlo > stablehlo. 2.  If the `mhlo.copy` is expected to be present in input module during MLIR conversion (chlo > stablehlo), it should not be folded and should be preserved. To fix this, removing the folder for the op.",2025-03-28T16:46:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90195
copybara-service[bot],PR #23790: Add explicit collectives grouping pass.,"PR CC(fix tf.Saver save and restore bug under distributed setting(with ps, master and worker)): Add explicit collectives grouping pass. Imported from GitHub PR https://github.com/openxla/xla/pull/23790 This PR adds an explicit frontend attribute to jitted JAX methods that will force the computation to be run in a single NCCL Group.  The intention of this change is to enable ""multidirectional communications"" that better saturate NVLink systems. We reuse the existing NCCLGroupThunk logic during the IR emitter stage, so we only need to introduce the async wrapper and call inliner. Here is an example of using this feature from JAX. ```python import jax from jax._src.xla_metadata import set_xla_metadata from jax.experimental.shard_map import shard_map from jax.sharding import Mesh, PartitionSpec as P from functools import partial num_devices = 4 mesh = Mesh(np.array(jax.devices()), ('i',))  Unique source target pairs for our two ppermutes later. perm_up = [(i, (i+1) % num_devices) for i in range(num_devices)] perm_down = [(i, (i1) % num_devices) for i in range(num_devices)]  NCCL Group computations _must_ be jitted. .jit def bidir_comms(a):     b = jax.lax.ppermute(a, ""i"", perm_up)     c = jax.lax.ppermute(a, ""i"", perm_down)     return b, c .jit (shard_map, mesh=mesh, in_specs=P(None, 'i'), out_specs=P(None, 'i')) def groups(a):     Running our jitted function in this context will force the use of a NCCL Group.     with set_xla_metadata(_collectives_group="""", inlineable=""false""):         b, c = bidir_comms(a)     return b + c ``` This is nsys trace with the annotation. !Screenshot 20250316 at 8 32 44 PM Vs without the additional annotation !Screenshot 20250316 at 8 31 56 PM As you can see, there is only a single NCCL kernel in the annotated example. Copybara import of the project:  d5fa129a55c089ee8374df4982be748517775bbe by chaserileyroberts : Add explicit nccl grouping pass  36ad18d8c604a6cb2559dcca8c29530beb5da888 by chaserileyroberts : Nccl>Collectives Merging this change closes CC(fix tf.Saver save and restore bug under distributed setting(with ps, master and worker)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23790 from chaserileyroberts:chase/nccl_group 36ad18d8c604a6cb2559dcca8c29530beb5da888",2025-03-28T16:37:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90194
copybara-service[bot],Run lightweight CSE after constant splitting to reduce compilation time.,Run lightweight CSE after constant splitting to reduce compilation time. CSE does not invoke any folders and is annotationaware,2025-03-28T16:00:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90193
copybara-service[bot],[xla:cpu] Update XLA's XNNPACK and pthreadpool commits,[xla:cpu] Update XLA's XNNPACK and pthreadpool commits This is to use new features such as bf16 batch matrix multiplication and weak pthreadpool symbols.,2025-03-28T15:36:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90192
copybara-service[bot],"[jaxlib] Pack/unpack subbyte types to/from numpy arrays to support int2, uint2, int4, uint4, float4_e2m1fn subbyte types in CPU/GPU callbacks.","[jaxlib] Pack/unpack subbyte types to/from numpy arrays to support int2, uint2, int4, uint4, float4_e2m1fn subbyte types in CPU/GPU callbacks.",2025-03-28T15:10:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90191
copybara-service[bot],[XLA:GPU] Change AtomicOrdering from seq_cst to monotonic.,[XLA:GPU] Change AtomicOrdering from seq_cst to monotonic. The ordering was too strict and after https://github.com/llvm/llvmproject/commit/9638d08af96c4cb8cf16785eed92179b2658bdfe a memory barrier was inserted in PTX.,2025-03-28T14:31:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90190
copybara-service[bot],[XLA:GPU] Add more op codes to triton support test 1/n,[XLA:GPU] Add more op codes to triton support test 1/n,2025-03-28T14:23:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90189
copybara-service[bot],[XLA:GPU] Split out and refactor `dot_algorithms` in the Triton emitter.,"[XLA:GPU] Split out and refactor `dot_algorithms` in the Triton emitter. The idea is to have a single entry point for the emitters to emit the matmul, and this is a step towards enabling dot algorithms in the generic Triton emitter path.",2025-03-28T14:12:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90188
copybara-service[bot],[XLA:CPU] Correctly handle resource sharing in thunk deserialization,"[XLA:CPU] Correctly handle resource sharing in thunk deserialization Serialization isn't reproducing the resource usages correctly, it always creates a new resource for a thunk. Fix this by serializing which thunks consume which resources.",2025-03-28T12:46:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90187
copybara-service[bot],Integrate Triton up to [4e364a7](https://github.com/openai/triton/commits/4e364a7871231b5df903bc85ce4ed4256118605e),Integrate Triton up to 4e364a7,2025-03-28T12:37:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90186
copybara-service[bot],Add contracting split (split-K) selection logic to dynamic search space,"Add contracting split (splitK) selection logic to dynamic search space First step towards the full search space. We figure out the contracting split based on the problem size, and a few other properties of the search space (still hardcoded for now). We also have an option for forcing a specific split, which is helpful for both disabling autotuning that parameter, and will facilitate support for analytically setting it in the future.",2025-03-28T12:14:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90185
copybara-service[bot],[XLA:Python] Remove unused forwarding headers.,[XLA:Python] Remove unused forwarding headers.,2025-03-28T12:05:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90184
copybara-service[bot],PR #24269: Fix for fusion wrapper on async computations,"PR CC([tensor_forest] Add total variance in the additional_data of tree leaf): Fix for fusion wrapper on async computations Imported from GitHub PR https://github.com/openxla/xla/pull/24269 Previously, in JAX if you do a simple stream annotated computation with just a single instruction, i.e. ```python ('gpu_stream:1') .jit def h(x, y):   return x * y ``` Then the XLA compiler would fail with an error like ``` jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Unsupported instruction opcode: multiply ``` This is because the fusion wrapper would not correctly look inside of async computations, leaving the instructions unfused.  To fix this, we simply add `kCall` and `kAsyncStart` to the set of instructions that are treated recursively in the fusion wrapper. Copybara import of the project:  bef9c2829a585727094c270b6c7e1e05ce304819 by chaserileyroberts : Added fix for some fusion issues using compute_on  1d909f1fb4a5bd11053be35f15e42796f7adb643 by chaserileyroberts : Update fusion wrapper test  0e39915f444607c5518bb7a5ce4a168801bfee8d by chaserileyroberts : Added {0} to dtype descriptions  5e51b3897c45354e929b061d131f8b2a6efde36a by chaser : xla_cc_test > xla_test  37df8115138ba07ed3f735787f62830d4abafffc by chaser : RunAndCompare > RunAndCompareNoHloPasses Merging this change closes CC([tensor_forest] Add total variance in the additional_data of tree leaf) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24269 from chaserileyroberts:chase/compute_on_fusion_fix 37df8115138ba07ed3f735787f62830d4abafffc",2025-03-28T11:24:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90183
gaikwadrahul8,Update 04 broken links in gpu_native.md,"Hi, Team I found 04 broken documentation links in this file gpu_native.md so I have updated those broken links to new LiteRT functional webpages links. Please review and merge this change as appropriate. Thank you for your consideration.",2025-03-28T10:18:37Z,comp:lite size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90182
copybara-service[bot],[XLA:GPU] Round the lowest part of f32 instead of masking it for BF16_Xn algorithms,[XLA:GPU] Round the lowest part of f32 instead of masking it for BF16_Xn algorithms The rounding increases the precision of BF16_X3.  Relative error decreases from 2e5 to 7e6.  Initially we zeroed the last 16 bits of f32 for high part and for low part. Now we zero the high part and round the low part. The we do the same for X6 and X9  zero high and med parts and round low part.,2025-03-28T09:46:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90181
copybara-service[bot],Relax layout change check for scatter.,Relax layout change check for scatter. It is ok that the element size layout attribute does not match.,2025-03-28T09:14:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90180
copybara-service[bot],Set up scaffolding for dynamic autotuner search space,"Set up scaffolding for dynamic autotuner search space For now we only connect a dummy class with an XLA flag, so we can later do A/B comparisons to current behavior.",2025-03-28T08:56:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90179
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-28T08:36:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90178
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-28T08:22:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90177
copybara-service[bot],Add send / recv callback support,Add send / recv callback support,2025-03-28T06:25:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90176
copybara-service[bot],[xla:gpu] CommandBuffer: switch CommandBuffer::If to explicit command update API,[xla:gpu] CommandBuffer: switch CommandBuffer::If to explicit command update API,2025-03-28T05:28:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90175
copybara-service[bot],Update autotuning wrapper to reflect the new key format,Update autotuning wrapper to reflect the new key format,2025-03-28T04:19:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90174
copybara-service[bot],[xla:cpu] Add `CpuFloatSupport` for more fine-grained control of type upcasting.,[xla:cpu] Add `CpuFloatSupport` for more finegrained control of type upcasting. + Add `ShouldSkipInstruction` for `FloatNormalization` to skip processing certain HLO instructions.,2025-03-28T04:16:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90173
copybara-service[bot],Set type for TraceEvent,Set type for TraceEvent,2025-03-28T04:01:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90172
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-28T03:59:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90171
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-28T03:58:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90170
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-28T03:57:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90169
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-28T03:56:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90168
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-28T03:49:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90167
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-28T03:43:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90166
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-28T03:43:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90165
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-28T03:41:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90164
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-28T03:41:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90163
copybara-service[bot],Implement `Compile()` and `DeserializeExecutable()` that return an unloaded executable for PJRT stream executor.,Implement `Compile()` and `DeserializeExecutable()` that return an unloaded executable for PJRT stream executor. Also refactor `LoadSerializedExecutable()` and `Load()` accordingly.,2025-03-28T02:28:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90162
copybara-service[bot],needed for the diffbase,needed for the diffbase Reverts changelist 740199541,2025-03-28T01:52:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90161
copybara-service[bot],needed for the diffbase,needed for the diffbase Reverts changelist 740344236,2025-03-28T01:48:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90160
copybara-service[bot],Remove unused hlo_module_util include from hlo_runner_interface.h.,Remove unused hlo_module_util include from hlo_runner_interface.h.,2025-03-28T01:11:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90159
copybara-service[bot],Integrate LLVM at llvm/llvm-project@5eccd71ce4f8,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 5eccd71ce4f8,2025-03-28T00:40:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90158
copybara-service[bot],[IFRT] Add helper function to pretty print MLIR locations.,[IFRT] Add helper function to pretty print MLIR locations.,2025-03-28T00:33:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90157
copybara-service[bot],PR #24086: Fix element type mismatch from ClampGatherIndices,PR CC([Intel MKL] Adding support to handle FusedConv2D): Fix element type mismatch from ClampGatherIndices Imported from GitHub PR https://github.com/openxla/xla/pull/24086 Copybara import of the project:  53f1acd286f095058022bb74d8ae58e14205ebda by tyb0807 : Fix element type mismatch from ClampGatherIndices  385f23d2fabfde468433182e79ac15fc411fde82 by tyb0807 : Fix element type mismatch for DUS indices in SPMD Merging this change closes CC([Intel MKL] Adding support to handle FusedConv2D) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24086 from tyb0807:fix 385f23d2fabfde468433182e79ac15fc411fde82,2025-03-28T00:29:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90156
copybara-service[bot],PR #23522: [NVIDIA GPU] [XLA_GPU_MS_COLLECTIVE] Add utility functions for async collective stream assignment,PR CC(Conversion from pb to tflite fails): [NVIDIA GPU] [XLA_GPU_MS_COLLECTIVE] Add utility functions for async collective stream assignment Imported from GitHub PR https://github.com/openxla/xla/pull/23522 Breaking https://github.com/openxla/xla/pull/22450 into 3 steps: 1. Utility functions with unit tests. 2. Update to execution stream assignment. 3. Runtime integration. This is the first step where we introduce the util functions needed and test them throughly. Copybara import of the project:  2f88001a9fff5752912d4bdff8b464437a9e259c by Terry Sun : util for async comm stream assignment  395fbb11d5f77fe94610987067062911f8aaca6c by Terry Sun : naming and exception handling  590c460706ea35a51ad7697394f969a9f08bd0d2 by Terry Sun : list false cases and throw error for unexpected Merging this change closes CC(Conversion from pb to tflite fails) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23522 from terryysun:terryysun/async_comm_util 590c460706ea35a51ad7697394f969a9f08bd0d2,2025-03-28T00:24:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90155
copybara-service[bot],[XLA:GPU] Remove p2p rewriter in favor of xla_gpu_experimental_pipeline_parallelism_opt_level,[XLA:GPU] Remove p2p rewriter in favor of xla_gpu_experimental_pipeline_parallelism_opt_level,2025-03-28T00:15:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90154
copybara-service[bot],Support a dynamic-update-slice's operand coming from an entry computation parameter in HostOffloadLegalize.,Support a dynamicupdateslice's operand coming from an entry computation parameter in HostOffloadLegalize.,2025-03-27T22:47:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90153
copybara-service[bot],[XLA:Python] Close visibilities of deprecated forwarding libraries.,[XLA:Python] Close visibilities of deprecated forwarding libraries.,2025-03-27T22:45:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90152
copybara-service[bot],"[XLA] Update CUDA, Cudnn, NCCL versions for GPU nightly workflows","[XLA] Update CUDA, Cudnn, NCCL versions for GPU nightly workflows",2025-03-27T22:34:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90151
ymodak,Fix conv2d dimension mismatch,"Fix dimension mismatch in tf.nn.conv2d usage. Fixes https://github.com/tensorflow/tensorflow/issues/89852 The original code produced an error due to an incorrect transpose operation on the input tensor, leading to incompatible dimensions for the convolution. The fix removes the unnecessary transpose, adjusts the filter shape, and adds/removes dummy dimensions to ensure the tensors are 4D as expected by tf.nn.conv2d with 'NHWC' data format.  A test case is provided to verify the fix, asserting that the convolution operation runs without errors and produces the expected output shape.",2025-03-27T22:31:04Z,size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90150
copybara-service[bot],Add preprocessor support to Run and RunAndCompareTwoModules,Add preprocessor support to Run and RunAndCompareTwoModules,2025-03-27T22:30:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90149
copybara-service[bot],Add HloParserOptions to CreateModuleFromString in hlo_module_util,Add HloParserOptions to CreateModuleFromString in hlo_module_util Allows passing through `HloParserOptions` to `ParseAndReturnUnverifiedModule`.,2025-03-27T22:08:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90148
copybara-service[bot],Use non-blocking NCCL communicators.,Use nonblocking NCCL communicators. Reverts 5c93e12b85fda11f14ef1a511ec61efc99e34694,2025-03-27T21:55:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90147
copybara-service[bot],[XLA:MSA] Fix a bug in ConsumeResource in MSA algorithm.,"[XLA:MSA] Fix a bug in ConsumeResource in MSA algorithm. The resource type is float and is initialized by calling GetInstructionElapsed at each schedule time. However, since the GetInstructionElapsed is measured in seconds, the resource values are very small floats (e.g., 1e10). When consuming resource, we perform floating point operation on these small values resulting in error (comparing to zero and etc...). This cl scales the float resource values to int64 type by a constant power of 2 scaling factor.",2025-03-27T21:23:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90146
copybara-service[bot],[XLA] Refine GCS bucket upload logic,[XLA] Refine GCS bucket upload logic,2025-03-27T21:21:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90145
copybara-service[bot],Internal change for exports.,Internal change for exports.,2025-03-27T20:48:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90144
copybara-service[bot],"In the OSS build of XLA, make a test fail if no test case is linked.","In the OSS build of XLA, make a test fail if no test case is linked.",2025-03-27T20:45:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90143
copybara-service[bot],Integrate LLVM at llvm/llvm-project@5eccd71ce4f8,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 5eccd71ce4f8,2025-03-27T20:38:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90142
copybara-service[bot],Add patterns comparing boolean tensors cast to numeric with zero,"Add patterns comparing boolean tensors cast to numeric with zero `TFL_CastOp` followed by either `TFL_EqualOp`, `TFL_LessEqualOp`, `TFL_GreaterEqualOp`, `TFL_NotEqualOp`, `TFL_GreaterOp`, or `TFL_LessOp`. Result is the original boolean tensor, the negated (`TFL_LogicalNot`) tensor, or a constant true or false tensor (using `TFL_ZerosLike` to retain shapes) depending on the exact call.",2025-03-27T20:31:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90141
copybara-service[bot],OSS noop change,OSS noop change,2025-03-27T19:41:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90140
copybara-service[bot],Remove same destination check in d2d. Add more unit test.,Remove same destination check in d2d. Add more unit test.,2025-03-27T19:40:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90139
copybara-service[bot],Add XlaSparseDenseMatmulCustomCombinerOnTcWithCsrInputOp to allow passing a custom combiner for embedding lookup FWD pass.,Add XlaSparseDenseMatmulCustomCombinerOnTcWithCsrInputOp to allow passing a custom combiner for embedding lookup FWD pass.,2025-03-27T19:25:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90138
copybara-service[bot],Initialize the metadata of the standalone dataset and root dataset.,Initialize the metadata of the standalone dataset and root dataset.,2025-03-27T18:38:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90137
copybara-service[bot],[xla:cpu] Add a function to check for AVX512BF16 in `TargetMachineFeatures`,[xla:cpu] Add a function to check for AVX512BF16 in `TargetMachineFeatures` + Add a test base that will be used in a future PR.,2025-03-27T18:18:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90136
copybara-service[bot],PR #24086: Fix element type mismatch from ClampGatherIndices,PR CC([Intel MKL] Adding support to handle FusedConv2D): Fix element type mismatch from ClampGatherIndices Imported from GitHub PR https://github.com/openxla/xla/pull/24086 Copybara import of the project:  53f1acd286f095058022bb74d8ae58e14205ebda by tyb0807 : Fix element type mismatch from ClampGatherIndices  385f23d2fabfde468433182e79ac15fc411fde82 by tyb0807 : Fix element type mismatch for DUS indices in SPMD Merging this change closes CC([Intel MKL] Adding support to handle FusedConv2D) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24086 from tyb0807:fix 385f23d2fabfde468433182e79ac15fc411fde82,2025-03-27T18:16:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90135
melanierbutler,"Memory leak in tfp.math.minimize, reproducible from two tutorial notebooks."," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code No  OS platform and distribution M2 Mac MacOS  Sequoia 15.2  Mobile device _No response_  Python version 3.11.6  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I recently filed this as an issue in tensorflow/probability but perhaps I should file it here instead: Python version: 3.11.6 tensorflow==2.18.0      via        r requirements.in        tfkeras tensorflowprobability==0.24.0      via r requirements.in tfkeras==2.18.0      via r requirements.in I am trying to perform an analysis built on the multiple changepoint detection and Bayesian model selection code found in this tutorial notebook, specifically the ""Unknown number of states"" portion: https://www.tensorflow.org/probability/examples/Multiple_changepoint_detection_and_Bayesian_model_selection In the process of running the analysis for multiple datasets I observed that the memory used increases with each successive analysis, by about 20 MB per run in my code. I confirmed that this occurs with the original tutorial notebook as well to ensure that there is not an error specific to my code. The easiest way to reproduce is to download the notebook, execute all cells, then rerun from the ""Unknown number of states"" section. I have tried adding `tf_keras.backend.clear_session()` between each run, deleting all created objects before rerunning, and running garbage collection, but nothing has released the ~20MB of memory that is allocated with each `tfp.math.minimize` run. The example I show in the standalone code below is from the ""Known number of states"" example, which has about 12 MiB increase in memory with each run. I see a similar issue with the Probabilistic PCA tutorial notebook. Memory increase with each run is ~3 MiB per run.  Standalone code to reproduce the issue ```shell import numpy as np import tensorflow as tf import tf_keras import tensorflow_probability as tfp from tensorflow_probability import distributions as tfd import os import psutil import gc true_rates = [40, 3, 20, 50] true_durations = [10, 20, 5, 35] observed_counts = tf.concat(     [tfd.Poisson(rate).sample(num_steps)      for (rate, num_steps) in zip(true_rates, true_durations)], axis=0) num_states = 4 initial_state_logits = tf.zeros([num_states])  uniform distribution daily_change_prob = 0.05 transition_probs = tf.fill([num_states, num_states],                            daily_change_prob / (num_states  1)) transition_probs = tf.linalg.set_diag(transition_probs,                                       tf.fill([num_states],                                               1  daily_change_prob)) trainable_log_rates = tf.Variable(   tf.math.log(tf.reduce_mean(observed_counts)) +   tf.random.stateless_normal([num_states], seed=(42, 42)),   name='log_rates') for i in range(20):     tf_keras.backend.clear_session()     gc.collect()     hmm = tfd.HiddenMarkovModel(       initial_distribution=tfd.Categorical(           logits=initial_state_logits),       transition_distribution=tfd.Categorical(probs=transition_probs),       observation_distribution=tfd.Poisson(log_rate=trainable_log_rates),       num_steps=len(observed_counts))     rate_prior = tfd.LogNormal(5, 5)     def log_prob():      return (tf.reduce_sum(rate_prior.log_prob(tf.math.exp(trainable_log_rates))) +              hmm.log_prob(observed_counts))     losses = tfp.math.minimize(         lambda: log_prob(),         optimizer=tf_keras.optimizers.legacy.Adam(learning_rate=0.1),         num_steps=100)     print(f""[{i+1}] Memory usage: {psutil.Process(os.getpid()).memory_info().rss / 1024 **2} MiB"")     del hmm, losses ```  Relevant log output ```shell [ CC(Add support for Python 3.x)] Memory usage: 583.109375 MiB [ CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"")] Memory usage: 596.3125 MiB [ CC(JVM, .NET Language Support)] Memory usage: 608.5 MiB [ CC(Installation over pip fails to import with protobuf 2.6.1)] Memory usage: 620.40625 MiB [ CC(Java interface)] Memory usage: 629.28125 MiB [ CC(Pretrained models)] Memory usage: 639.65625 MiB [ CC(API docs does not list RNNs)] Memory usage: 648.78125 MiB [ CC(Setting lower gcc version for cuda)] Memory usage: 662.71875 MiB [ CC(Typo in getting started guide)] Memory usage: 673.234375 MiB [ CC(Go API)] Memory usage: 688.53125 MiB [ CC(0.5.0 wheel install on Mac OS X using Homebrew python broken)] Memory usage: 699.40625 MiB [ CC(Remote worker configuration)] Memory usage: 710.140625 MiB [ CC([doc] typo)] Memory usage: 719.234375 MiB [ CC(g3doc format)] Memory usage: 728.609375 MiB [ CC(Quantized ops?)] Memory usage: 736.859375 MiB [ CC(iOS Support and Example)] Memory usage: 746.84375 MiB [ CC(Windows Support and Documentation)] Memory usage: 758.03125 MiB [ CC(C api)] Memory usage: 767.5625 MiB [ CC(Swift API)] Memory usage: 777.453125 MiB [ CC(CUDA 7.5 fails with pip install and docker (Ubuntu 14.04))] Memory usage: 785.0625 MiB ```",2025-03-27T17:43:23Z,stat:awaiting tensorflower type:bug TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/90134,"I was able to reproduce this issue using TensorFlow 2.18.0, 2.19.0, and the nightly version. Please find the gist attached for reference. Thank you!"
copybara-service[bot],Convert an odml.detector composite to a custom op,Convert an odml.detector composite to a custom op,2025-03-27T17:14:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90133
copybara-service[bot],Add CUDA v.12.8.1 in the list of hermetic CUDA redistributions.,Add CUDA v.12.8.1 in the list of hermetic CUDA redistributions. Fix the links in hermetic CUDA docs.,2025-03-27T16:55:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90132
copybara-service[bot],Reverts f1242e2863d51d2b12fbbc6108d3a3feac067e31,Reverts f1242e2863d51d2b12fbbc6108d3a3feac067e31,2025-03-27T16:49:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90131
copybara-service[bot],Fixed bug in handling `ncclCommSplit` with `NCCL_SPLIT_NOCOLOR`.,"Fixed bug in handling `ncclCommSplit` with `NCCL_SPLIT_NOCOLOR`. Recall that [`ncclCommSplit`][ncclCommSplit] can be used [to create new NCCL communicators from existing ones][split_guide]. The following examples are taken from the NVIDIA documentation: ``` // Duplicate a communicator int rank; ncclCommUserRank(comm, &rank); ncclCommSplit(comm, 0, rank, &newcomm, NULL); ``` ``` // Split a communicator in half. int rank, nranks; ncclCommUserRank(comm, &rank); ncclCommCount(comm, &nranks); ncclCommSplit(comm, rank/(nranks/2), rank%(nranks/2), &newcomm, NULL); ``` ``` // Create new communicators only on the first two ranks. int rank; ncclCommUserRank(comm, &rank); ncclCommSplit(comm, rank<2 ? 0 : NCCL_SPLIT_NOCOLOR, rank, &newcomm, NULL); ``` The `color` argument describes how the new communicators are partitioned (e.g., half red and half blue). As seen in the final example above, if the color is `NCCL_SPLIT_NOCOLOR`, then `ncclCommSplit` doesn't produce any new communicator. In this case, the returned communicator is `NULL`. The code in `nccl_collectives.cc` was not expecting null communicators. This change fixes the code to handle null communicators correctly.",2025-03-27T16:26:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90130
copybara-service[bot],Integrate LLVM at llvm/llvm-project@64046e9d2628,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 64046e9d2628,2025-03-27T16:16:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90129
copybara-service[bot],PR #24181: [ds-fusion] Fix the issue with JAX tests with no indvars,"PR CC(tf.image (CLAHE) contrast limited adaptive histogram equalization.): [dsfusion] Fix the issue with JAX tests with no indvars Imported from GitHub PR https://github.com/openxla/xla/pull/24181 JAX has some testcases with no induction variable but static slices inside the while loop. Such slices are fused with the hero operation, but while lowering they fail on an assert (which is not required). Added a test to demonstrate this. Copybara import of the project:  0adcb04d43b75c02e9e4f6af2e0484245e49e990 by Shraiysh Vaishay : [dsfusion] Fix the issue with JAX tests with no indvars JAX has some testcases with no induction variable but static slices inside the while loop. Such slices are fused with the hero operation, but while lowering they fail on an assert (which is not required). Added a test to demonstrate this. Merging this change closes CC(tf.image (CLAHE) contrast limited adaptive histogram equalization.) Reverts dde3a6fae85aa3b2c8265a263d324b27b6303c33 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24181 from shraiysh:fix_jax_tests_related_to_dynamic_slice_fusion 0adcb04d43b75c02e9e4f6af2e0484245e49e990",2025-03-27T16:04:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90128
chhavi150546,Spamming with an useless file,,2025-03-27T16:02:07Z,size:XS invalid,closed,0,2,https://github.com/tensorflow/tensorflow/issues/90127,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",Please don't spam
copybara-service[bot],[XLA:LatencyHidingScheduler] Dump Scheduler statistics to a proto,[XLA:LatencyHidingScheduler] Dump Scheduler statistics to a proto,2025-03-27T15:45:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90126
copybara-service[bot],[XLA:GPU] Turn on Triton support tests for Blackwell.,[XLA:GPU] Turn on Triton support tests for Blackwell.,2025-03-27T15:37:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90125
copybara-service[bot],PR #24114: Triton/Nvidia: Fix fused fp8 <-> fp8 conversions,"PR CC(Fix version number in Bazel downgrade warning): Triton/Nvidia: Fix fused fp8  fp8 conversions Imported from GitHub PR https://github.com/openxla/xla/pull/24114 Converting FP8  FP8 fails because the Triton compiler does not support it. The proposed fix will make the conversion go through FP16. Two questions: 1) Are there any better approaches of solving this? 2) I could not find a place to put unit tests for this, and in the code there is a comment saying:     ```         // TODO(b/266862493): Add endtoend test once FP8 support lands in XLA as         // we can't test the code below without patching the feature.     ```     Wondering if there is a place where I can add a test?  Details When converting FP8 types, the XLA compiler emits a `fp_to_fp` Triton instruction. If the source type is FP8, no rounding strategy is specified. Concretely, this causes the following Triton to be emitted:   %24 = tt.fp_to_fp %20 : tensor > tensor  ``` module {   tt.func (%arg0: !tt.ptr {tt.divisibility = 16 : i32}, %arg1: !tt.ptr {tt.divisibility = 16 : i32}, %arg2: !tt.ptr {tt.divisibility = 16 : i32}) {     %cst = arith.constant dense : tensor     %cst_0 = arith.constant dense : tensor     %c90_i32 = arith.constant 90 : i32     %c32000_i64 = arith.constant 32000 : i64     %c64_i32 = arith.constant 64 : i32     %c90_i64 = arith.constant 90 : i64     %c768_i64 = arith.constant 768 : i64     %c0_i32 = arith.constant 0 : i32     %c1_i64 = arith.constant 1 : i64     %c32_i32 = arith.constant 32 : i32     %c24_i32 = arith.constant 24 : i32     %c8_i32 = arith.constant 8 : i32     %c4000_i32 = arith.constant 4000 : i32     %cst_1 = arith.constant dense : tensor     %0 = tt.get_program_id x : i32     %1 = arith.divsi %0, %c4000_i32 : i32     %2 = arith.muli %1, %c8_i32 : i32     %3 = arith.subi %c24_i32, %2 : i32     %4 = arith.cmpi slt, %3, %c8_i32 : i32     %5 = arith.select %4, %3, %c8_i32 : i32     %6 = arith.remsi %0, %5 : i32     %7 = arith.addi %2, %6 : i32     %8 = arith.remsi %0, %c4000_i32 : i32     %9 = arith.divsi %8, %5 : i32     %10 = arith.muli %7, %c32_i32 : i32     %11 = tt.make_tensor_ptr %arg1, [%c768_i64, %c90_i64], [%c1_i64, %c768_i64], [%c0_i32, %c0_i32] {order = array} : >     %12 = tt.advance %11, [%10, %c0_i32] : >     %13 = arith.muli %9, %c64_i32 : i32     %14 = tt.make_tensor_ptr %arg0, [%c90_i64, %c32000_i64], [%c1_i64, %c90_i64], [%c0_i32, %c0_i32] {order = array} : >     %15 = tt.advance %14, [%c0_i32, %13] : >     %16:3 = scf.for %arg3 = %c0_i32 to %c90_i32 step %c64_i32 iter_args(%arg4 = %12, %arg5 = %15, %arg6 = %cst_1) > (!tt.ptr>, !tt.ptr>, tensor)  : i32 {       %20 = tt.load %arg4 {boundaryCheck = array, padding = 1 : i32} : !tt.ptr>       %21 = tt.advance %arg4, [%c0_i32, %c64_i32] : >       %22 = tt.load %arg5 {boundaryCheck = array, padding = 1 : i32} : !tt.ptr>       %23 = tt.advance %arg5, [%c64_i32, %c0_i32] : >       %24 = tt.fp_to_fp %20 : tensor > tensor       %25 = arith.subi %c90_i32, %arg3 : i32       %26 = arith.cmpi slt, %25, %c64_i32 : i32       %27 = scf.if %26 > (tensor) {         %30 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor         %31 = tt.expand_dims %30 {axis = 0 : i32} : tensor > tensor         %32 = tt.splat %25 : i32 > tensor         %33 = arith.cmpi slt, %31, %32 : tensor         %34 = tt.broadcast %33 : tensor > tensor         %35 = arith.select %34, %24, %cst_0 : tensor, tensor         scf.yield %35 : tensor       } else {         scf.yield %24 : tensor       }       %28 = scf.if %26 > (tensor) {         %30 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor         %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor > tensor         %32 = tt.splat %25 : i32 > tensor         %33 = arith.cmpi slt, %31, %32 : tensor         %34 = tt.broadcast %33 : tensor > tensor         %35 = arith.select %34, %22, %cst : tensor, tensor         scf.yield %35 : tensor       } else {         scf.yield %22 : tensor       }       %29 = tt.dot %27, %28, %arg6, inputPrecision = tf32 {maxNumImpreciseAcc = 2147483647 : i32} : tensor * tensor > tensor       scf.yield %21, %23, %29 : !tt.ptr>, !tt.ptr>, tensor     }     %17 = tt.fp_to_fp %16 CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4""), rounding = rtne : tensor > tensor     %18 = tt.make_tensor_ptr %arg2, [%c768_i64, %c32000_i64], [%c1_i64, %c768_i64], [%c0_i32, %c0_i32] {order = array} : >     %19 = tt.advance %18, [%10, %13] : >     tt.store %19, %17 : !tt.ptr>     tt.return   } } ```  Which leads to a failing assertion: ``` CC(未找到相关数据)  0x000073413786d9fc in pthread_kill () from /lib/x86_64linuxgnu/libc.so.6 CC(Add support for Python 3.x)  0x0000734137819476 in raise () from /lib/x86_64linuxgnu/libc.so.6 CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"")  0x00007341377ff7f3 in abort () from /lib/x86_64linuxgnu/libc.so.6 CC(JVM, .NET Language Support)  0x00007341377ff71b in ?? () from /lib/x86_64linuxgnu/libc.so.6 CC(Installation over pip fails to import with protobuf 2.6.1)  0x0000734137810e96 in __assert_fail () from /lib/x86_64linuxgnu/libc.so.6 CC(Java interface)  0x000057d936b1777b in mlir::triton::gpu::(anonymous namespace)::FpToFpOpConversion::createDestOps (this=0x733d08425cc0, op=..., adaptor=..., rewriter=..., elemTy=..., operands=..., loc=...)     at external/triton/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ElementwiseOpToLLVM.cpp:500 CC(Pretrained models)  0x000057d936b17195 in mlir::triton::gpu::ElementwiseOpConversionBase::matchAndRewrite (this=0x733d08425cc0, op=..., adaptor=..., rewriter=...)     at external/triton/include/triton/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVMBase.h:188 [...] CC(minimum req: Cuda compute capability 3.5) 0x000057d93fa6cade in mlir::PassManager::run (this=0x733e80fba158, op=0x733d080bbc20) at external/llvmproject/mlir/lib/Pass/Pass.cpp:885 CC(Go API) 0x000057d9363f6b1b in xla::gpu::CompileTritonToLLVM (hlo_config=..., hlo_module_name=""gemm_fusion_dot.320"", device_info=..., block_level_parameters=..., triton_module=..., llvm_module=0x733d0816d6a0, mlir_context=..., is_xla_fusion=true, emit_kernel=true)     at xla/backends/gpu/codegen/triton/fusion_emitter.cc:1627 CC(Slack Channel) 0x000057d9363f5a5d in xla::gpu::TritonWrapper (fn_name=""gemm_fusion_dot_320_impl"", fusion=0x733d080a31c0, cc=std::variant [index 0] = {...}, device_info=..., block_level_parameters=...,     llvm_module=0x733d0816d6a0, mlir_context=...) at xla/backends/gpu/codegen/triton/fusion_emitter.cc:1531 ``` However, this fails Triton compilation: * First it hits an assertion that the rounding strategy when the destination type is FP8 must be specified * Adding the rounding strategy, then goes on to another issue, that no methods for converting FP8  FP8 are specified To work around the above two issues, I propose going through FP16 when both the source and destination types are FP8's. Copybara import of the project:  afd3929099fc4d1045275ca3210e0bc727a2b906 by Kasper Nielsen : Fix fused fp8  fp8 conversions  66340aa808f58e5dc6ab1c2e06790ceccde95540 by Kasper Nielsen : Add unit tests and refactor duplicated code  07ae307879eff24ad2f85607e94503deda1074e4 by Kasper Nielsen : Run clangformat  fe967ff94ffc5f34f07bff142b5d10d81d5e4dce by Kasper Nielsen : Fix support conversion tests Merging this change closes CC(Fix version number in Bazel downgrade warning) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24114 from kasper0406:kn/fp8conversionfix fe967ff94ffc5f34f07bff142b5d10d81d5e4dce",2025-03-27T15:34:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90124
copybara-service[bot],Rename `DefaultModuleScheduler` to `DefaultMemoryScheduler` and delete old `DefaultMemoryScheduler`.,Rename `DefaultModuleScheduler` to `DefaultMemoryScheduler` and delete old `DefaultMemoryScheduler`.,2025-03-27T15:05:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90123
copybara-service[bot],Replace uses of deprecated `Shape::rank()` with:,"Replace uses of deprecated `Shape::rank()` with:  `dimensions().size()` if it's OK for the result to be changed to an unsigned number,  `dimensions_size()` if it's important that the result is a signed number. This should be a pure refactoring that doesn't affect the code's behavior. Note that `rank()` returns `int64_t` and `dimensions().size()` returns `size_t`. Sometimes the change of the signedness is not desirable, and we use `dimensions_size()`, which returns `int`, in such cases. Reverts 56d1b2306f948c83b2b243484a6c16c037696a85 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23790 from chaserileyroberts:chase/nccl_group 36ad18d8c604a6cb2559dcca8c29530beb5da888",2025-03-27T14:51:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90122
copybara-service[bot],PR #24255: Fixed rules_python patch for python 3.14,PR CC(Fix TF Lite Android Demo app build to work in Android Studio): Fixed rules_python patch for python 3.14 Imported from GitHub PR https://github.com/openxla/xla/pull/24255 Returning from finally clause raises SyntaxWarning in python 3.14 :  2f7a929473cc67b3498ed88df4ab8e080e7def0f by vfdev5 : Fixed rules_python patch for python 3.14 Returning from finally clause raises SyntaxWarning in python 3.14 Merging this change closes CC(Fix TF Lite Android Demo app build to work in Android Studio) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24255 from vfdev5:fixrulespythonpatchfor314 2f7a929473cc67b3498ed88df4ab8e080e7def0f,2025-03-27T14:37:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90121
copybara-service[bot],[XLA:GPU] Add explicit rounding of the F32 arguments of dot to TF32 if the dot algorithm set as TF32.,[XLA:GPU] Add explicit rounding of the F32 arguments of dot to TF32 if the dot algorithm set as TF32. Triton lowers the tf32 dot to mma instruction that does not have explicit rounding attribute for tf32 inputs. As a result the precision of the tf32 dot is even worth than BF16_BF16_F32 algorithm. Lets round explicitly the arguments when we have this execution sequence.,2025-03-27T13:58:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90120
copybara-service[bot],[XLA:GPU] Make logic to dump MLIR passes consistent.,"[XLA:GPU] Make logic to dump MLIR passes consistent. For each fusion that goes through Triton or XLA emitter, we create a file that logs IR before all passes in MLIR pipeline. To dump MLIR passes for all relevant kernel use `xla_dump_to= xla_dump_hlo_pass_re=fusionemitter` For Triton emitter: * Enable dump for `tritonfusionemitter` pass. * Results in files `.tritonpasses.log` For XLA emitters: * Enable dump for `mlirfusionemitter` pass. * Results in files `.mlirpasses.log` and `.mlirtrace.txt` * We don't print to stdout on `vmodule=emitter_base=5`. To print to stdout, use `xla_dump_to=`.",2025-03-27T13:55:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90119
mplatings,[TOSA] Fix legalizing CONV bias,The logic to handle bias already existed in varying degrees of completeness for the various CONV operations. This change unifies that logic in a single function and reuses it for operations from which it was missing. The TOSA 1.0 specification permits that bias may be of shape [1]. This change takes advantage of that fact to simplify the logic. The change in behaviour is reflected in the tests. ChangeId: I97371fae425ca8e45b18705c71bd106d08f203d6,2025-03-27T13:53:23Z,kokoro:force-run ready to pull size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90118
copybara-service[bot],"[XLA:GPU] Propagate num warps, stages and ctas when creating nested fusions.","[XLA:GPU] Propagate num warps, stages and ctas when creating nested fusions.",2025-03-27T12:00:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90117
copybara-service[bot],[XLA:GPU] Switch GPU memory scheduler to `DefaultModuleScheduler`.,"[XLA:GPU] Switch GPU memory scheduler to `DefaultModuleScheduler`. `DefaultModuleScheduler` runs the heap simulator 3 times for a module as opposed to 3 times per computation for `DefaultMemoryScheduler`. This speeds up ""backend"" compile time by 24% on average. ""Buffer Allocations"" size regresses for only 3 benchmarks by  at most 3% but is neutral for most benchmarks.",2025-03-27T10:37:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90116
copybara-service[bot],[XLA:GPU][Emitters] Support int4 scatters.,[XLA:GPU][Emitters] Support int4 scatters.,2025-03-27T10:15:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90115
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T09:01:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90114
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T08:58:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90113
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T08:48:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90112
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@be8ce602,Integrate StableHLO at openxla/stablehlo,2025-03-27T08:19:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90111
chunhsue,Qualcomm AI Engine Direct - 4bit Quant Support,,2025-03-27T07:13:00Z,size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90110
copybara-service[bot],PR #24158: Remove HloComputation::CollectiveCallInstruction.,PR CC(Xla devices): Remove HloComputation::CollectiveCallInstruction. Imported from GitHub PR https://github.com/openxla/xla/pull/24158 Step 3/5 of removing InstructionType. Copybara import of the project:  06cee58ab83198ad150420536658764df141a8fc by Johannes Reifferscheid : Remove HloComputation::CollectiveCallInstruction. Step 3/5 of removing InstructionType. Merging this change closes CC(Xla devices) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24158 from jreiffers:collectivecall 06cee58ab83198ad150420536658764df141a8fc,2025-03-27T07:07:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90109
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T06:17:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90108
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T05:50:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90107
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T05:48:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90106
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T05:10:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90105
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T05:07:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90104
Venkat6871,Fix typos in documentation strings,"Hi, Team I observed few typos in the documentation strings and I have fixed those typos so please do the needful. Thank you.",2025-03-27T04:47:02Z,ready to pull size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90103
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T04:31:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90102
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T04:31:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90101
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T04:28:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90100
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T04:27:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90099
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T04:27:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90098
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T04:27:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90097
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T04:26:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90096
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T04:19:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90095
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T04:18:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90094
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T04:16:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90093
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-27T04:10:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90092
copybara-service[bot],[xla:gpu] CommandBuffer: remove Barrier() API from command buffer,"[xla:gpu] CommandBuffer: remove Barrier() API from command buffer Command buffers always sequential and Barrier API is a noop, and can be removed before it can be replaced with a DAG dependency building.",2025-03-27T02:38:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90091
copybara-service[bot],[xla:gpu] CommandBuffer: add API to explicitly update nested command buffers,[xla:gpu] CommandBuffer: add API to explicitly update nested command buffers,2025-03-27T02:36:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90090
copybara-service[bot],Add `GetTopologyDescription` to the TFRT GPU client.,Add `GetTopologyDescription` to the TFRT GPU client.,2025-03-27T02:22:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90089
copybara-service[bot],[xla:gpu] CommandBuffer: switch conditional APIs to explicit create/update operations,"[xla:gpu] CommandBuffer: switch conditional APIs to explicit create/update operations Barrier() API is no longer used an will be deleted in the followup CL. GpuCommandBuffer always records all commands with a dependency on the previous one, which is the same as the default mode used by XLA today. Disable conditionals by default, because they trigger segfault inside CUDA. Will reenable conditionals once migration is complete.",2025-03-27T01:45:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90088
copybara-service[bot],[xla:gpu] CommandBuffer: add API to explicitly update Launch command,[xla:gpu] CommandBuffer: add API to explicitly update Launch command,2025-03-27T01:31:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90087
copybara-service[bot],Integrate LLVM at llvm/llvm-project@23bf98e4b5b7,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 23bf98e4b5b7,2025-03-27T00:26:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90086
copybara-service[bot],move tf_mlir_translate_registration.cc to tensorflow/compiler/mlir/tools,move tf_mlir_translate_registration./compiler/mlir/tools,2025-03-27T00:13:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90085
copybara-service[bot],"Add the flag `--@xla//xla/tsl/platform:use_rpath_for_cuda_deps` for forcing the linker to set `RPATH`, not `RUNPATH` in the binaries that depend on CUDA stubs and hermetic CUDA `cc_import`s.","Add the flag `//xla/tsl/platform:use_rpath_for_cuda_deps` for forcing the linker to set `RPATH`, not `RUNPATH` in the binaries that depend on CUDA stubs and hermetic CUDA `cc_import`s. The default value is `false`, meaning that the ELF files depending on CUDA stubs/`cc_import`s will have `RUNPATH` in the dynamic section. This flag can be turned on in the special cases when NVIDIA pypi wheels should be preferred over all other CUDA sources (including systemwide libraries and `LD_LIBRARY_PATH` environment variable). This setting is used by JAX `jaxcudaplugin` and `jaxcudapjrt` wheels. Please note that `RPATH` is a deprecated tag.",2025-03-27T00:12:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90084
copybara-service[bot],[xla:gpu] Only forward ifrt::PjRtFfiLoadedHostCallback to FFI's ExecutionContext in ifrt::PjRtLoadedExecutable::Execute.,[xla:gpu] Only forward ifrt::PjRtFfiLoadedHostCallback to FFI's ExecutionContext in ifrt::PjRtLoadedExecutable::Execute.,2025-03-27T00:01:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90083
copybara-service[bot],Update `googletest` used by XLA and Tensorflow from the 2022/6/30 version to the 2025/3/21 version. This:,"Update `googletest` used by XLA and Tensorflow from the 2022/6/30 version to the 2025/3/21 version. This:  makes the OSS build of OpenXLA catch up with the internal build in their use of gtest versions.  unlocks a bunch of planned improvements (enforcing that a test program has at least one test case, simplifying copybara setup, etc).",2025-03-26T23:24:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90082
copybara-service[bot],Lift the restrictions that CopyToMemorySpace doesn't work sometimes for,Lift the restrictions that CopyToMemorySpace doesn't work sometimes for matching src+dest memory spaces. We can always bounce through the host if there is no more efficient copy.,2025-03-26T23:07:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90081
copybara-service[bot],Rename serialization_base.fbs to tflite_serialization_base.fbs,Rename serialization_base.fbs to tflite_serialization_base.fbs,2025-03-26T22:47:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90080
copybara-service[bot],Introduce a version number for protecting against PJRT changes in jaxlib.,Introduce a version number for protecting against PJRT changes in jaxlib.,2025-03-26T22:04:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90079
copybara-service[bot],Remove deprecated `Shape::rank()` now that it's no longer used.,Remove deprecated `Shape::rank()` now that it's no longer used.,2025-03-26T21:42:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90078
copybara-service[bot],Allow LogicalBufferAnalysis and TuplePointsToAnalysis to visit instructions in a fusion that are not reachable from the fusion root.,Allow LogicalBufferAnalysis and TuplePointsToAnalysis to visit instructions in a fusion that are not reachable from the fusion root.,2025-03-26T21:33:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90077
copybara-service[bot],Added `WorkerThread` abstraction.,Added `WorkerThread` abstraction.,2025-03-26T21:05:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90076
copybara-service[bot],Increase Linux arm64 wheel size limit from 250 to 255 MB.,Increase Linux arm64 wheel size limit from 250 to 255 MB.,2025-03-26T19:16:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90075
copybara-service[bot],Support AllowAsynchronous deallocation for the new GPU async client.,Support AllowAsynchronous deallocation for the new GPU async client.,2025-03-26T19:09:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90074
copybara-service[bot],Don't link `nvrtc-builtins` for local CUDA installation (see the discussion in https://github.com/conda-forge/tensorflow-feedstock/pull/414#issuecomment-2629135833),Don't link `nvrtcbuiltins` for local CUDA installation (see the discussion in https://github.com/condaforge/tensorflowfeedstock/pull/414issuecomment2629135833),2025-03-26T18:31:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90073
copybara-service[bot],[HLO Test Coverage] Add XLA Builder lit tests for uncovered APIs used by tf2xla,[HLO Test Coverage] Add XLA Builder lit tests for uncovered APIs used by tf2xla Other minor additions:  HLO Translate for Cosine / Sine / ResultAccuracyMode Tolerance  Dynamic conv custom call tests  IsConstant visitor test,2025-03-26T18:14:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90072
copybara-service[bot],[HLO Test Coverage] Add XLA Builder lit tests for uncovered APIs used by tf2xla,[HLO Test Coverage] Add XLA Builder lit tests for uncovered APIs used by tf2xla Other minor additions:  HLO Translate for Cosine / Sine / ResultAccuracyMode Tolerance  Dynamic conv custom call tests  IsConstant visitor test,2025-03-26T18:14:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90072
copybara-service[bot],autotuner_util: Version the individual results as well as the overall cache.,"autotuner_util: Version the individual results as well as the overall cache. Some uses of autotuning now need to version the results separately from the structure of the cache as a whole, such as when Triton versions change.",2025-03-26T17:43:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90071
copybara-service[bot],[xla:gpu] CommandBuffer: add API to explicitly update MemcpyDeviceToDevice command,[xla:gpu] CommandBuffer: add API to explicitly update MemcpyDeviceToDevice command,2025-03-26T17:34:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90070
copybara-service[bot],Update OpenXLA's `googletest` from the 2022/6/30 version to the 2025/3/21 version. This:,"Update OpenXLA's `googletest` from the 2022/6/30 version to the 2025/3/21 version. This:  makes the internal build and the OSS build of OpenXLA use the same googletest version and thus be consistent.  unlocks a bunch of planned improvements (enforcing that a test program has at least one test case, simplifying copybara setup, etc).",2025-03-26T17:29:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90069
copybara-service[bot],[JAX] [XLA:Python] Migrate more modules to JAX.,[JAX] [XLA:Python] Migrate more modules to JAX.,2025-03-26T16:52:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90068
copybara-service[bot],[XLA] Reorder GEMMA2 benchmark from nightly build script due to temp failure,[XLA] Reorder GEMMA2 benchmark from nightly build script due to temp failure This has been causing build errors for a while.,2025-03-26T16:49:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90067
copybara-service[bot],Integrate LLVM at llvm/llvm-project@ac9049df7e62,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match ac9049df7e62,2025-03-26T16:47:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90066
copybara-service[bot],[XLA:GPU] Remove brittle pipeline parallelism detection in the SPMD partitioner pass,[XLA:GPU] Remove brittle pipeline parallelism detection in the SPMD partitioner pass,2025-03-26T16:43:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90065
copybara-service[bot],litert: Disable broken GPU tests,litert: Disable broken GPU tests,2025-03-26T16:15:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90064
copybara-service[bot],[XLA:GPU] Delete `--xla_gpu_unsupported_force_triton_gemm`.,"[XLA:GPU] Delete `xla_gpu_unsupported_force_triton_gemm`. It used to be necessary to work around a lowering bug on A100, but our version of Triton now carries the necessary patch.",2025-03-26T16:01:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90063
copybara-service[bot],[XLA:GPU] Clean up `DebugOptions` and turn off autotuning in `dot_algorithms_test`.,[XLA:GPU] Clean up `DebugOptions` and turn off autotuning in `dot_algorithms_test`. This makes the test run >10x faster without loss of useful coverage.,2025-03-26T15:57:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90062
copybara-service[bot],Internal non functional BUILD file change allowing Shardy to be used in other places.,Internal non functional BUILD file change allowing Shardy to be used in other places.,2025-03-26T15:57:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90061
copybara-service[bot],[XLA] Open visibility of more profiler modules.,[XLA] Open visibility of more profiler modules. Change in preparation for moving code from XLA:Python to JAX.,2025-03-26T15:19:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90060
copybara-service[bot],[xla:cpu] add MultiBenchmarkConfig to define JIT and AOT benchmarks,"[xla:cpu] add MultiBenchmarkConfig to define JIT and AOT benchmarks This allows us to define both JIT and AOT benchmarks, while fulfilling the following requirements:  minimal changes to existing benchmark definitions  we get to keep   our existing oneliners!  no changes to existing benchmark names. This is important for automation   that tracks benchmark performance over time.",2025-03-26T15:13:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90059
copybara-service[bot],[XLA] Open visibility of shardy/round_trip_common to JAX.,[XLA] Open visibility of shardy/round_trip_common to JAX. Change in preparation for migrating more XLA:Python code to JAX.,2025-03-26T14:51:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90058
copybara-service[bot],[XLA:GPU] Dump IR before all passes in Triton fusion emitter with string stream.,"[XLA:GPU] Dump IR before all passes in Triton fusion emitter with string stream. Dump if dump for ""tritonfusionemitter"" pass is enabled. This is similar to what we do in emitter, where we dump for ""mlirfusionemitter"" pass. `xla_gpu_dump_llvmir` doesn't work well, because together with `tritonpasses.log` it also dump ~250 files of LLVM passes that are rarely needed.",2025-03-26T14:26:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90057
copybara-service[bot],[XLA:Python] Split xla::Literal type casters out of types.h into its own module.,[XLA:Python] Split xla::Literal type casters out of types.h into its own module.,2025-03-26T14:20:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90056
copybara-service[bot],"[XLA:GPU] Move `DumpTritonIR` to the `ir_emitter_triton_internal` namespace, and inline it into `fusion_emitter.h`.","[XLA:GPU] Move `DumpTritonIR` to the `ir_emitter_triton_internal` namespace, and inline it into `fusion_emitter.h`. The util was previously exposed in `fusion_emitter.h`, but unimplemented in `fusion_emitter_stub.cc`preventing us from moving some tests needlessly running on GPU to CPU. The util is only needed within `fusion_emitter.cc` itself and a couple of related test files. Since the implementation should work in either case and is simple, I made it an `inline` function in order to avoid maintaining two lowerings. Other alternatives are to find another header file where to define this, or to duplicate this in test files. I don't think it matters much.",2025-03-26T13:44:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90055
copybara-service[bot],minor: fix typo,minor: fix typo,2025-03-26T12:29:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90054
copybara-service[bot],[XLA:GPU] support hoisting bitcasts past broadcasts,[XLA:GPU] support hoisting bitcasts past broadcasts Only rewrites hoisting up to the parameters. Works in assumption that bitcast is a reshape and broadcast does not transpose.,2025-03-26T11:20:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90053
copybara-service[bot],PR #21683: [XLA:GPU] NVSHMEM allocation,"PR CC(Error while freezing a graph): [XLA:GPU] NVSHMEM allocation Imported from GitHub PR https://github.com/openxla/xla/pull/21683 Requires https://github.com/openxla/xla/pull/20395 which adds the NVSHMEM library dependency. This PR adds the following: 1. Nvshmem flag to enable nvshmem 2. Set nvshmem initialization issue when GPU PJRT client is created. The first time NVSHMEM is used, it will be initialized. 3. Uses the user buffer memory pool for nvshmem. If nvshmem is enabled, it will be allocated using `nvshmem_malloc`. This same memory can be used by user buffers if nccl user buffers is also enabled. 4. Update the `CollectiveColorer` so that mosaic_gpu custom calls use the nvshmem memory space. Copybara import of the project:  aee33791e16ab2149118de728dbb9e62f5e7cc31 by Trevor Morris : Add nvshmem flag, memory allocation, and memory space assignment Set Nvshmem env info during client creation Rename flag and use absl::string_view  f8fca39300b3915eb6320142f58fa9c0ec7a1eaa by Trevor Morris : Use explicit types in test  e41faa3f72b778fcf8ea8111d3cde59548b8f9f5 by Trevor Morris : Add user buffer allgather and allreduce tests with and without nvshmem alloc Set nvshmem in XLA_FLAGS test fixes formatting  cf0c36865de8b8a010caaf62c3a36b64e36037bd by Trevor Morris : Fixes  3b4d11123cdb794d0a60e65b94d22ded04b7b2b4 by Trevor Morris : Remove early dso check  359f2b243ec97b1f8003c27f0b07dde82407ff6c by Trevor Morris : Add flag comment  fd15a7cac745adc1971bec63e148047b9b811729 by Trevor Morris : Also assign memory space for mosaic_gpu_v2 Merging this change closes CC(Error while freezing a graph) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21683 from trevorm:nvshmemupstream2 fd15a7cac745adc1971bec63e148047b9b811729",2025-03-26T10:59:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90052
chuntl,Qualcomm AI Engine Direct - Enable BroadcastTo OP,Summary:  Enable op builder for broadcast to op  Add 2 mlir model for different data type,2025-03-26T10:40:25Z,size:M,open,0,1,https://github.com/tensorflow/tensorflow/issues/90051,Test with boolean case (Case in DETRResNet50DC5 model) & FP32 case !image
copybara-service[bot],PR #24011: HLO op profiles for B200.,PR CC([INTEL MKL]Optimize CropAndResizeGradImage Op.): HLO op profiles for B200. Imported from GitHub PR https://github.com/openxla/xla/pull/24011 Copybara import of the project:  e270aa00005171d75864d52445d77c3364373954 by Dimitris Vardoulakis : HLO op profiles for B200. Merging this change closes CC([INTEL MKL]Optimize CropAndResizeGradImage Op.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24011 from dimvar:b200hloopprofiles e270aa00005171d75864d52445d77c3364373954,2025-03-26T10:30:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90050
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T09:52:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90049
copybara-service[bot],[XLA:GPU] Add an additional check for tiling reuse.,[XLA:GPU] Add an additional check for tiling reuse. We don't support tilings that result in different number of blocks.,2025-03-26T09:34:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90048
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T09:09:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90047
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T09:06:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90046
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T09:05:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90045
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T09:05:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90044
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T09:04:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90043
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T09:03:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90042
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T09:03:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90041
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T09:01:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90040
copybara-service[bot],Implement `Compile()` and `DeserializeExecutable()` that return an unloaded executable for PJRT stream executor.,Implement `Compile()` and `DeserializeExecutable()` that return an unloaded executable for PJRT stream executor. Also refactor `LoadSerializedExecutable()` and `Load()` accordingly.,2025-03-26T08:40:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90039
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T08:38:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90038
copybara-service[bot],Bump autotuner cache version after triton upgrade,Bump autotuner cache version after triton upgrade,2025-03-26T08:37:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90037
copybara-service[bot],[XLA:GPU] Dump hlo config only once before optimization for a GPU tasks.,[XLA:GPU] Dump hlo config only once before optimization for a GPU tasks.,2025-03-26T08:31:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90036
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T07:28:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90035
copybara-service[bot],Replace `Shape::rank()` with `Shape::dimensions_size()`.,"Replace `Shape::rank()` with `Shape::dimensions_size()`. The use of ""rank"" to mean the number of array dimensions is confusing as it's at odds with the matrix rank concept used in linear algebra, where it means the number of independent columns (or rows). FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89621 from fujunwei:undefine_VK_USE_PLATFORM_XLIB_KHR cbaad53e5332b758093b20c43e7aaf35a3c456d2",2025-03-26T07:21:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90034
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24060 from philipphack:u_conv_circular_xla f247a7122b7b9c842142650789905490d388ef4e,2025-03-26T07:17:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90033
copybara-service[bot],PR #24203: [XLA:GPU] fix unintended cuDNN flash attention dbias computation,PR CC(Markdown formatting issue in tf.keras.layers.Layer docs): [XLA:GPU] fix unintended cuDNN flash attention dbias computation Imported from GitHub PR https://github.com/openxla/xla/pull/24203 Fix a case where cudnn flash attention dbias is not required but computed. Only calculate dbias when there is dbias descriptor. Copybara import of the project:  14f15487cc33be2d9a80bd7f9a5d95d85346fc60 by cjkkkk : fix unintended dbias computation Merging this change closes CC(Markdown formatting issue in tf.keras.layers.Layer docs) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24203 from Cjkkkk:dbias_fix 14f15487cc33be2d9a80bd7f9a5d95d85346fc60,2025-03-26T06:32:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90032
copybara-service[bot],PR #24168: Fix while loop analysis when the increment is not a constant,PR CC(What version of Tensorflow  to install with CUDA 9.2 and libcudnn 6.0 ?): Fix while loop analysis when the increment is not a constant Imported from GitHub PR https://github.com/openxla/xla/pull/24168 Currently this fails with an assert because the checks (matcher) does not look for constants. Copybara import of the project:  392df8825f311a2a9a771310a6874b80655c5fc7 by Shraiysh Vaishay : Fix while loop analysis when the increment is not a constant Currently this fails with an assert because the checks (matcher) does not look for constants. Merging this change closes CC(What version of Tensorflow  to install with CUDA 9.2 and libcudnn 6.0 ?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24168 from shraiysh:fix_while_loop_analysis_without_constant_increment 392df8825f311a2a9a771310a6874b80655c5fc7,2025-03-26T06:31:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90031
copybara-service[bot],PR #24158: Remove HloComputation::CollectiveCallInstruction.,PR CC(Xla devices): Remove HloComputation::CollectiveCallInstruction. Imported from GitHub PR https://github.com/openxla/xla/pull/24158 Step 3/5 of removing InstructionType. Copybara import of the project:  06cee58ab83198ad150420536658764df141a8fc by Johannes Reifferscheid : Remove HloComputation::CollectiveCallInstruction. Step 3/5 of removing InstructionType. Merging this change closes CC(Xla devices) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24158 from jreiffers:collectivecall 06cee58ab83198ad150420536658764df141a8fc,2025-03-26T06:22:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90030
copybara-service[bot],Fix saved_model.save for Serving embedding.,Fix saved_model.save for Serving embedding.,2025-03-26T06:21:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90029
copybara-service[bot],[XLA:GPU] Fix bug in triton emitter related to multi-output fusion.,[XLA:GPU] Fix bug in triton emitter related to multioutput fusion. We should not return early after emitting a scalar store.,2025-03-26T06:09:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90028
copybara-service[bot],Integrate LLVM at llvm/llvm-project@23bf98e4b5b7,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 23bf98e4b5b7,2025-03-26T05:06:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90027
njzjz,Build against TF 2.19: llvm/ADT/ArrayRef.h: No such file or directory," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.19.0  Custom code Yes  OS platform and distribution Linux  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ```   In file included from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/stream_executor/stream_executor.h:40,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/service/stream_pool.h:23,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/service/service_executable_run_options.h:25,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/service/executable.h:40,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/service/compiler.h:40,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/service/backend.h:34,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/service/allocation_tracker.h:30,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/service/service.h:33,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/client/client.h:32,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/tensorflow/compiler/tf2xla/xla_expression.h:21,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/tensorflow/compiler/tf2xla/xla_compiler.h:28,                    from /usr/local/lib/python3.11/distpackages/tensorflow/include/tensorflow/compiler/tf2xla/xla_op_kernel.h:23,                    from /tmp/pipinstallx2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/horovod/tensorflow/xla_mpi_ops.cc:25:   /usr/local/lib/python3.11/distpackages/tensorflow/include/xla/stream_executor/gpu/tma_metadata.h:25:10: fatal error: llvm/ADT/ArrayRef.h: No such file or directory      25           ^~~~~~~~~~~~~~~~~~~~~   compilation terminated.   gmake[2]: *** [horovod/tensorflow/CMakeFiles/tensorflow.dir/build.make:513: horovod/tensorflow/CMakeFiles/tensorflow.dir/xla_mpi_ops.cc.o] Error 1   gmake[2]: *** Waiting for unfinished jobs.... ```",2025-03-26T04:38:07Z,type:bug TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/90026,`aptget update && aptget install y noinstallrecommends llvmdev` should solve this issue. 
Rolexo,Tensorflow_installation_issue," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.19.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? i expected the dataset to be loaded  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[7], line 1 > 1 import tensorflow as tf       2 print(tf.__version__) File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\User\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.  Standalone code to reproduce the issue ```shell  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[7], line 1 > 1 import tensorflow as tf       2 print(tf.__version__) File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\User\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. ```  Relevant log output ```shell  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[7], line 1 > 1 import tensorflow as tf       2 print(tf.__version__) File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\User\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. ```",2025-03-26T04:08:13Z,stat:awaiting response type:build/install TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/90025,"Hi  , could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Did you reinstall packages? Check if you are in the right env?,Given the currently provided information this is a duplicate. Please search for duplicates or provide information that differentiates from them.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T03:48:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90024
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T03:35:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90023
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T03:31:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90022
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T03:26:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90021
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T03:24:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90020
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T03:21:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90019
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T03:19:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90018
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T03:19:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90017
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T03:17:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90016
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-26T03:17:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90015
jinxsfe,can not import tensorflow as tf when set tensorflow =2.15," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15  Custom code Yes  OS platform and distribution Colab notebook  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory A100  Current behavior? I can not import tensorflow as tf when set tensorflow ==2.15(I must set it in order to match environment), which correspond that I can not use colab GPU, I try to fill those issue according support for colab cummunity but failed, can you give me some ideas, issue link is https://github.com/googlecolab/colabtools/issues/5212, even set as order, also introduce the future error, if I keep newest tensorflow version, which aslo let model failed in training, detail link is   Standalone code to reproduce the issue ```shell .. ```  Relevant log output ```shell .. ```",2025-03-26T02:18:06Z,stat:awaiting response type:bug stale TF 2.15,closed,0,8,https://github.com/tensorflow/tensorflow/issues/90014,"if I run force run tensorflow 2.15 !Image, it only use cpu not gpu",Colab runtime is configured with just one GPU version. If you install a version of TF that is using a different GPU then you would only be able to use CPU.,"I changed runtime before I install it, restart session and delete runtime records, show an error, if I install tensorflow and inport","You need to initialise and check for your GPU, it might default to CPU if you do not have the right environment path and setup. ``import tensorflow as tf print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))`` ````gpus = tf.config.list_physical_devices('GPU') if gpus:    Restrict TensorFlow to only use the first GPU   try:     tf.config.set_visible_devices(gpus[0], 'GPU')     logical_gpus = tf.config.list_logical_devices('GPU')     print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPU"")   except RuntimeError as e:      Visible devices must be set before GPUs have been initialized     print(e)``", thanks you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Change HloRunnerInterface serialization function to accept string instead.,"Change HloRunnerInterface serialization function to accept string instead. In b77be1f we added a set of functions that would let us turn a protobuf message into an OpaqueExecutable. Some usecases call for custom serialization, so we are changing this to a string instead of a protobuf message.",2025-03-26T02:12:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90013
copybara-service[bot],Cleanup constant threshold,Cleanup constant threshold,2025-03-26T01:28:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90012
copybara-service[bot],Add accidentally removed fix.,Add accidentally removed fix.,2025-03-26T01:26:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90011
copybara-service[bot],[XLA:GPU] Deprecate PIPELINE_PARALLELISM_OPT_LEVEL_ENABLE_CYCLE_DECOMPOSER,[XLA:GPU] Deprecate PIPELINE_PARALLELISM_OPT_LEVEL_ENABLE_CYCLE_DECOMPOSER,2025-03-26T01:20:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90010
copybara-service[bot],PR #24060: Exclude Circular Dependencies in FP8 Graph Convolutions,PR CC(Cpu tensorflow ImportError ): Exclude Circular Dependencies in FP8 Graph Convolutions Imported from GitHub PR https://github.com/openxla/xla/pull/24060 Excludes ops with external operands that can be reached from graph outputs of fused ops as well as ops with graph outputs that can reach external operands of fused ops from fusion into graphbased FP8 convolutions. Fusing these ops may lead to circular dependencies between fused and unfused instructions. Copybara import of the project:  5ebb2be3175c310b8e2212c1401d660eb5a36625 by Philipp Hack : Excludes ops that may cause circular dependencies from fusion into graphbased FP8 convolutions.  f247a7122b7b9c842142650789905490d388ef4e by Philipp Hack : Excludes ops that may cause circular dependencies from fusion into graphbased FP8 convolutions. Merging this change closes CC(Cpu tensorflow ImportError ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24060 from philipphack:u_conv_circular_xla f247a7122b7b9c842142650789905490d388ef4e,2025-03-26T01:16:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90009
copybara-service[bot],Reverts 17419edfaf46da88783d0cb8f7fc6efa1b6c401a,Reverts 17419edfaf46da88783d0cb8f7fc6efa1b6c401a,2025-03-26T01:12:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90008
copybara-service[bot],Improve readability. Rewrite some codes.,Improve readability. Rewrite some codes.,2025-03-26T01:08:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90007
copybara-service[bot],Add a proto matcher library for XLA.,"Add a proto matcher library for XLA. This library defines `EqualsProto(expected)` for matching a proto by equality. It also defines two matcher transformers `Partially()` and `IgnoringRepeatedFieldOrdering()` for making the matcher ignore fields not set in the expected proto and ignore order of elements in repeated fields, respectively. Also use the library in tensorflow/compiler/xla/client/executable_build_options_test..",2025-03-25T23:55:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90006
copybara-service[bot],Add experimental `async_noop()` wrapper.,Add experimental `async_noop()` wrapper.,2025-03-25T23:53:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90005
copybara-service[bot],[xla:gpu] CommandBuffer: add API to explicitly update Memset command,"[xla:gpu] CommandBuffer: add API to explicitly update Memset command This is a first step to convert command buffer to explicit DAG building. Once all commands will support explicit updates, the Barrier() will go away and CommandBufferCmdThunk will switch to explicit command updates.",2025-03-25T23:08:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90004
copybara-service[bot],Add access method for executables.,Add access method for executables.,2025-03-25T22:16:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90003
copybara-service[bot],[JAX] [XLA:Python] Migrate more Python modules to JAX.,[JAX] [XLA:Python] Migrate more Python modules to JAX. Reverts 17a6a779c5be888d21c59707dec8bcff1f187e9d,2025-03-25T21:50:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/90002
copybara-service[bot],Integrate LLVM at llvm/llvm-project@6c68cc4df1bf,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 6c68cc4df1bf,2025-03-25T21:48:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90001
copybara-service[bot],PR #24168: Fix while loop analysis when the increment is not a constant,PR CC(What version of Tensorflow  to install with CUDA 9.2 and libcudnn 6.0 ?): Fix while loop analysis when the increment is not a constant Imported from GitHub PR https://github.com/openxla/xla/pull/24168 Currently this fails with an assert because the checks (matcher) does not look for constants. Copybara import of the project:  392df8825f311a2a9a771310a6874b80655c5fc7 by Shraiysh Vaishay : Fix while loop analysis when the increment is not a constant Currently this fails with an assert because the checks (matcher) does not look for constants. Merging this change closes CC(What version of Tensorflow  to install with CUDA 9.2 and libcudnn 6.0 ?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24168 from shraiysh:fix_while_loop_analysis_without_constant_increment 392df8825f311a2a9a771310a6874b80655c5fc7,2025-03-25T21:44:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/90000
copybara-service[bot],Allow StableHLO to VHLO dialect mixing,Allow StableHLO to VHLO dialect mixing,2025-03-25T21:41:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89999
copybara-service[bot],Support native SDY ops in JAX export,Support native SDY ops in JAX export,2025-03-25T21:38:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89998
copybara-service[bot],[xla:cpu] Cleaning up after callback FFI refactor.,[xla:cpu] Cleaning up after callback FFI refactor.,2025-03-25T21:35:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89997
copybara-service[bot],[jaxlib:gpu] Return an error if we try to use subbyte types in GPU callbacks instead of failing silently.,[jaxlib:gpu] Return an error if we try to use subbyte types in GPU callbacks instead of failing silently. We will be adding subbyte type support in subsequence changes.,2025-03-25T21:22:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89996
copybara-service[bot],[xla:cpu] Return an error if we try to use subbyte types in CPU callbacks instead of failing silently.,[xla:cpu] Return an error if we try to use subbyte types in CPU callbacks instead of failing silently. We will be adding subbyte type support in subsequence changes.,2025-03-25T21:21:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89995
copybara-service[bot],Add `data_service_address` field to `DatasetMetadata`.,Add `data_service_address` field to `DatasetMetadata`.,2025-03-25T21:12:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89994
copybara-service[bot],[xla:ffi] Forward more xla::PrimitiveTypes through FFI.,"[xla:ffi] Forward more xla::PrimitiveTypes through FFI. Currently, passing an unknown type will trigger a DCHECK when FFI attempts to decode argument or return buffers. Updating FFI to forward all xla::PrimitiveTypes gives FFI handlers the opportunity to handle types instead of hitting SIGABRT during tests (e.g., today, CPU/GPU callback handlers do not handle subbyte types). Either way, we want to eventually support subbyte types in CPU/GPU callbacks anyway. NOTE: We leave off PrimitiveType::OpaqueType and PrimitiveType::Tuple because it's not clear what their byte widths should be. Updates to callback CPU/GPU handlers to follow along with an associate Jax python test.",2025-03-25T20:37:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89993
copybara-service[bot],Implement d2d transfer `TfrtGpuBuffer::CopyToMemorySpace`,Implement d2d transfer `TfrtGpuBuffer::CopyToMemorySpace`,2025-03-25T20:33:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89992
copybara-service[bot],[XLA:GPU] Add GpuCliqueKey to CommunicatorHandle.,[XLA:GPU] Add GpuCliqueKey to CommunicatorHandle. GpuCliqueKey has enough information about the number of participants and if all of them are local. There is no need query the clique itself.,2025-03-25T20:29:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89991
copybara-service[bot],[XLA:GPU][NFC] Remove spurious `LOG(ERROR)` from `EmitBF16x6Matmul`.,[XLA:GPU][NFC] Remove spurious `LOG(ERROR)` from `EmitBF16x6Matmul`.,2025-03-25T20:21:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89990
copybara-service[bot],Support assigning a multiple values encoded as a comma separated string into a repeated field in DebugOptions:,"Support assigning a multiple values encoded as a comma separated string into a repeated field in DebugOptions:   each occurrence of field, overwrites all previous values in the field,   empty string clears all values from the field,   splitting comma separated string skips empty values.",2025-03-25T20:12:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89989
Tai78641,[mlir][tosa] Fix Int Floor Div bug for TF/TFL lowering to TOSA,"Previously, tf/tfl FloorDiv was lowered directly to IntDiv for integer types. However, FloorDiv rounds toward negative infinity, whereas Tosa IntDiv rounds toward zero. So, the previous lowering was incorrect wrt rounding.  This patch fixes this lowering of FloorDiv to Tosa.",2025-03-25T20:10:49Z,size:M comp:lite-tosa,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89988,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi  , Can you please sign the CLA, Thank you !","passed lit tests locally: INFO: Build completed successfully, 1 total action //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test    (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test       (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test  (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test (cached) PASSED in 0.3s"
copybara-service[bot],Pass tf.data service dispatcher address to Dataset::FromGraph.,Pass tf.data service dispatcher address to Dataset::FromGraph.,2025-03-25T20:08:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89987
copybara-service[bot],Allow an instruction in a fusion to have no users if that instruction has side effects.,Allow an instruction in a fusion to have no users if that instruction has side effects.,2025-03-25T19:56:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89986
copybara-service[bot],Add metadata options to `Dataset::Params`.,Add metadata options to `Dataset::Params`.,2025-03-25T19:50:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89985
copybara-service[bot],[XLA:GPU] Make `fusion_emitter_deviceless_test` truly deviceless.,"[XLA:GPU] Make `fusion_emitter_deviceless_test` truly deviceless. Previously, it was still running on GPU for some reason. Also simplify some of the test code while we're at it.",2025-03-25T19:47:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89984
copybara-service[bot],XLA:Translate: Reorder chlo passes to match with the PJRT lowering path.,XLA:Translate: Reorder chlo passes to match with the PJRT lowering path. Reverts ea4cacead963a3217d02ef2668353beeef3f6ff6 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24456 from dimvar:cuda13support d152d725f2cbbe3bdd1df17a7edcc7da620ad703,2025-03-25T19:40:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89983
copybara-service[bot],[XLA:GPU] Make sure that we fail gracefully if `num_warps` is `0` in the generic Triton emitter.,[XLA:GPU] Make sure that we fail gracefully if `num_warps` is `0` in the generic Triton emitter.,2025-03-25T19:22:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89982
copybara-service[bot],"Content in result.stderr field doesn't always mean error. For example, function ""execute"" fails when we use clang with CCC_OVERRIDE_OPTIONS=""^--gcc-install-dir=/usr/lib/gcc/x86_64-linux-gnu/13"" variable because clang outputs gcc path into stderr even during successful command execution.","Content in result.stderr field doesn't always mean error. For example, function ""execute"" fails when we use clang with CCC_OVERRIDE_OPTIONS=""^gccinstalldir=/usr/lib/gcc/x86_64linuxgnu/13"" variable because clang outputs gcc path into stderr even during successful command execution.",2025-03-25T19:16:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89981
copybara-service[bot],Add /usr/local/cuda/bin location to the path so ptxas and nvcc will be found.,Add /usr/local/cuda/bin location to the path so ptxas and nvcc will be found.,2025-03-25T19:15:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89980
copybara-service[bot],Changes a misleading function name and reduces its usage.,"Changes a misleading function name and reduces its usage. `protobuf_util::ProtobufEquals(p1, p2)` sounds like comparing two protos for equality, but in reality it's comparing the serialized strings of the protos. Proto serialization isn't guaranteed to be stable, and may contain fields we don't care about in case the proto definition is subsequently extended. Therefore its behavior is dangerous and should be used only when we intend to compare serialized strings. Rename it to `HaveSameSerialization()` to avoid misleading the readers. Also replaces its uses in tests with `EqualsProto()`, which has a deterministic semantics and generates far better failure messages.",2025-03-25T18:51:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89979
copybara-service[bot],Make HLO Op Writer Generator code MLIR dialect agnostic.,Make HLO Op Writer Generator code MLIR dialect agnostic.,2025-03-25T18:51:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89978
copybara-service[bot],Register bf16 fused matmul kernel.,Register bf16 fused matmul kernel.,2025-03-25T18:47:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89977
copybara-service[bot],Implement `TfrtGpuBuffer::CopyRawToHostFuture` and `TfrtGpuClient::BufferFromHostLiteral`,Implement `TfrtGpuBuffer::CopyRawToHostFuture` and `TfrtGpuClient::BufferFromHostLiteral`,2025-03-25T18:31:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89976
copybara-service[bot],[xla:cpu] Cleaning up after callback FFI refactor.,[xla:cpu] Cleaning up after callback FFI refactor.,2025-03-25T18:25:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89975
copybara-service[bot],Internal config change.,Internal config change.,2025-03-25T18:16:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89974
copybara-service[bot],[XLA:GPU] Add support for `concatenate`s in the generic Triton emitter.,"[XLA:GPU] Add support for `concatenate`s in the generic Triton emitter. Currently, this lowering only supports concatenating arrays where the tile size along the concatenation dimension aligns with the size of the same dimension for all of the `concatenate`'s operands.",2025-03-25T18:13:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89973
copybara-service[bot],move code around,move code around,2025-03-25T18:06:51Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/89972,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],[XLA:GPU] Fix `SymbolicTileAnalysis` to deal with non-`0` domain lower bounds.,"[XLA:GPU] Fix `SymbolicTileAnalysis` to deal with non`0` domain lower bounds. This is necessary in order to support `concatenate`s modeled as nested fusions. Previously, propagation was missing out on the offset at nesting time, meaning that we'd end up creating indexing maps with erroneous dimension variables.",2025-03-25T17:58:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89971
copybara-service[bot],"[mhlo] Don't sink constants into WhileOp, it is faster to pass as input args","[mhlo] Don't sink constants into WhileOp, it is faster to pass as input args",2025-03-25T17:55:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89970
copybara-service[bot],[xla:cpu] Rename oneDNN's `CpuFloatSupport` to `OneDnnFloatSupport`,"[xla:cpu] Rename oneDNN's `CpuFloatSupport` to `OneDnnFloatSupport` This is in preparation for having a new, generic `CpuFloatSupport`.",2025-03-25T17:45:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89969
copybara-service[bot],Change HloRunnerInterface serialization function to accept string instead.,"Change HloRunnerInterface serialization function to accept string instead. In b77be1f we added a set of functions that would let us turn a protobuf message into an OpaqueExecutable. Some usecases call for custom serialization, so we are changing this to a string instead of a protobuf message.",2025-03-25T17:41:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89968
copybara-service[bot],Run gemma3_1b_flax_call.hlo and gemma3_1b_flax_sample_loop.hlo in CPU/GPU nightly workflows,Run gemma3_1b_flax_call.hlo and gemma3_1b_flax_sample_loop.hlo in CPU/GPU nightly workflows,2025-03-25T17:38:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89967
copybara-service[bot],[XLA:LatencyHidingScheduler] Add detailed resources to while instructions.,[XLA:LatencyHidingScheduler] Add detailed resources to while instructions.,2025-03-25T17:33:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89966
copybara-service[bot],"Open visibilities of a number of PJRT, IFRT, and profiler targets.","Open visibilities of a number of PJRT, IFRT, and profiler targets. Change in preparation for migrating XLA:Python code to the JAX repository.",2025-03-25T17:03:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89965
copybara-service[bot],Update tags documentation for `notap`,Update tags documentation for `notap` `notap` can be seen as the equivalent to `no_oss` internally,2025-03-25T16:59:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89964
copybara-service[bot],Integrate LLVM at llvm/llvm-project@6c68cc4df1bf,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 6c68cc4df1bf FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89799 from mistersmee:master fe2a4036944d0a3d9f9a16a85581a79ddba11775,2025-03-25T16:08:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89963
copybara-service[bot],[xla:cpu] Undo adding `no_mkl` targets,[xla:cpu] Undo adding `no_mkl` targets Rollback of https://github.com/openxla/xla/pull/24111 Having both withmkl and nomkl targets could possibly lead to ODR violation if multiple dependencies in the same target link to both variants. The use cases that don't want to link oneDNN should stick to bazel `define=tensorflow_mkldnn_contraction_kernel=0` flag instead.  Reverts 3651dbd5ec6357fd7bcc25d5928ffb275cbf939b,2025-03-25T16:00:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89962
copybara-service[bot],Support _xla_non_copyable_attribute.,Support _xla_non_copyable_attribute. Add tests to demonstrate the copies inserted for a chain of noncopyable values in straightline code and a few pipelined situations. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89799 from mistersmee:master fe2a4036944d0a3d9f9a16a85581a79ddba11775,2025-03-25T15:54:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89961
copybara-service[bot],[xla:copy_insertion] Generalize the handling of pipelined Send/Recv to handle,[xla:copy_insertion] Generalize the handling of pipelined Send/Recv to handle asynchronous operations that produce noncopyable results.,2025-03-25T15:48:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89960
copybara-service[bot],Remove unused flag in tf2hlo,Remove unused flag in tf2hlo,2025-03-25T15:47:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89959
copybara-service[bot],[XLA:GPU] Use tsl::Env::WriteStringToFile to dump LLVM module.,[XLA:GPU] Use tsl::Env::WriteStringToFile to dump LLVM module. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89799 from mistersmee:master fe2a4036944d0a3d9f9a16a85581a79ddba11775,2025-03-25T15:44:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89958
copybara-service[bot],Add InitEglEnvironment to CC LiteRT Environment,Add InitEglEnvironment to CC LiteRT Environment,2025-03-25T15:12:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89957
copybara-service[bot],[JAX] [XLA:Python] Migrate py_client to JAX.,[JAX] [XLA:Python] Migrate py_client to JAX.,2025-03-25T15:04:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89956
copybara-service[bot],"PR #20611: [XLA:GPU] Re-implement command buffer to remove execution_scope, construct dependencies through DAG.","PR CC(Adding matrix square root op): [XLA:GPU] Reimplement command buffer to remove execution_scope, construct dependencies through DAG. Imported from GitHub PR https://github.com/openxla/xla/pull/20611 This PR is large redesign of XLA command buffer, which basically follows the below idea:  1.  Removing the execution scope concept from command buffer, and basically there should be no execution stream concept in command buffer. Instead command buffer will construct the graph through data flow in the command buffer command sequence (converted from XLA thunk sequence), so overlapping (execution scope) will be automatically deduced by the graph topology. This is more matching the cudagraph's conept, where it does not have a stream property, and operators will be auto launched to different streams whenever the cudagraph's topology does not have dependencies (edges) across operators.  2. CommandBufferCmd now can depend on other commands (specified through command index in the CommandBufferCmdSequence), and dependencies (R/W, W/W conflicts) is auto inferred from buffer assignment results when appending a new command into CommandBufferCmdSequence (CommandBufferCmdSequence::Append()).  3. When implementation CommandBuffer, dependencies that specified through CommandBufferCmd index is translated into node dependencies across cuda nodes.  The main benefits of the design are:  1. Constructing graph topology through data flow, instead of thunk execution order, can automatically enables maximum concurrency that is allowed by data flow across operators, where it should have better perf.  2. The design is much more natural and intuitive, command buffer is designed to be an abstract of cuda graph, so it should just be a graph of how the result is calculated through operators. Execution scope or stream is some kind of runtime concept where it should not be included in command buffer's graph. In current design, XLA introduces the execution scope concept just to maintain the concept of execution stream in XLA runtime, this is unnecessary and counter intuitive.  3.  Command buffer is easier to implement and use,  simulating the multistream order in command buffer through execution scope introduces lots of hard to understand codes.  While we can see that through the data flow design, the implementation code is more compact and easier to understand.  Copybara import of the project:  73490c0a56c00b58d8d8d2f8e164f6080d03ce0f by Shawn Wang : Rewrite XLA command buffer  c2644d5d9084c9ae17111f5de8920c39cb495521 by Shawn Wang : fix  7cf96d7b01e001769565b4ba6eae4431e0ba650f by Shawn Wang : add build dependency Merging this change closes CC(Adding matrix square root op) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20611 from shawnwang18:shawnw/cuda_graph_dependency 7cf96d7b01e001769565b4ba6eae4431e0ba650f",2025-03-25T15:02:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89955
copybara-service[bot],Reverts 7a50c1ae5521133ea7573168b6e2726260f71c07,Reverts 7a50c1ae5521133ea7573168b6e2726260f71c07 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/80074 from yzhou51:fullconnect_use_slm 6ba325e738ee82b60bc611247c797451b8c957ae,2025-03-25T13:39:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89954
copybara-service[bot],[AutoPGLE] Prevent an AutoPGLE to run if user launched an external profiler.,[AutoPGLE] Prevent an AutoPGLE to run if user launched an external profiler. Reverts 6cd5a6103ecc02f188f30c510435810dc2d80761,2025-03-25T07:20:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89953
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T06:18:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89952
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T05:33:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89951
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T05:23:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89950
copybara-service[bot],[xla] Extract execution graph construction from CPU backend to top level xla/runtime,"[xla] Extract execution graph construction from CPU backend to top level xla/runtime This is NFC, this change extracts execution graph (DAG computed from buffer and resource use) from XLA:CPU backend to top level xla/runtime, so it can be reused for computing execution graph of XLA:GPU Thunks and CommandBuffer",2025-03-25T05:03:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89949
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:56:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89948
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:55:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89947
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:55:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89946
copybara-service[bot],[xla] Move ResourceUse up to xla/runtime level,[xla] Move ResourceUse up to xla/runtime level,2025-03-25T04:53:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89945
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:51:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89944
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:49:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89943
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:48:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89942
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:46:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89941
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:42:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89940
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:40:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89939
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:39:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89938
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:38:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89937
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:38:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89936
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:36:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89935
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:36:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89934
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:35:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89933
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:34:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89932
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T04:33:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89931
copybara-service[bot],[XLA] Disable xla_dump_full_hlo_config by default,[XLA] Disable xla_dump_full_hlo_config by default This is causing massive spam (doubling the number of files produced).,2025-03-25T03:50:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89930
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T03:15:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89929
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-25T03:13:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89928
copybara-service[bot],[JAX] [XLA:Python] Migrate py_socket_transfer to JAX.,[JAX] [XLA:Python] Migrate py_socket_transfer to JAX. Also in passing fix up some header guards and authorship comments.,2025-03-25T01:44:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89927
copybara-service[bot],Various cleanups on tfrt gpu client,Various cleanups on tfrt gpu client,2025-03-25T01:16:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89926
copybara-service[bot],Rename `MarkEventReadyOnExit` to `MarkGpuEventReadyOnExit`,Rename `MarkEventReadyOnExit` to `MarkGpuEventReadyOnExit` This is to avoid ODR violation with the same class in `abstract_tfrt_cpu_buffer`.,2025-03-25T01:02:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89925
copybara-service[bot],[XLA:GPU] Remove unused absl variant header,[XLA:GPU] Remove unused absl variant header,2025-03-25T00:33:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89924
copybara-service[bot],[XLA:GPU] Remove CollectivePermuteValidIterationAnnotator pass,[XLA:GPU] Remove CollectivePermuteValidIterationAnnotator pass,2025-03-25T00:20:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89923
copybara-service[bot],This is a no-op on openxla.,This is a noop on openxla.,2025-03-25T00:19:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89922
copybara-service[bot],[XLA:GPU] Remove brittle pipeline parallelism detection in the SPMD partitioner pass,[XLA:GPU] Remove brittle pipeline parallelism detection in the SPMD partitioner pass,2025-03-25T00:19:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89921
copybara-service[bot],[XLA:GPU] Remove xla_gpu_unsafe_pipelined_loop_annotator flag,[XLA:GPU] Remove xla_gpu_unsafe_pipelined_loop_annotator flag,2025-03-25T00:18:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89920
copybara-service[bot],[XLA:GPU] Deprecate xla_gpu_unsafe_pipelined_loop_annotator,[XLA:GPU] Deprecate xla_gpu_unsafe_pipelined_loop_annotator,2025-03-25T00:18:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89919
copybara-service[bot],Only apply `FuseTransposeIntoBatchMatMulRHS` pattern for the static shape case.,Only apply `FuseTransposeIntoBatchMatMulRHS` pattern for the static shape case. Reverts 46e4e9680f157a0605ba3be12358586362dd4c0b,2025-03-25T00:09:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89918
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@af8ba04d,Integrate StableHLO at openxla/stablehlo,2025-03-24T23:57:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89917
copybara-service[bot],Make sure array is copied under this situation:,"Make sure array is copied under this situation: ``` x = np.arange(1000) y = jax.device_put(x, device=jax.devices()[0], may_alias=False, donate=False) z = jax.device_put(y, device=jax.devices()[0], may_alias=False, donate=False) ``` This condition will be true after this change `z.unsafe_buffer_pointer() != y.unsafe_buffer_pointer()` Also lift the restrictions that CopyToMemorySpace doesn't work sometimes for matching src+dest memory spaces. We can always bounce through the host if there is no more efficient copy.",2025-03-24T23:50:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89916
copybara-service[bot],Hlo-Diff is a semantic diff tool for HLO modules.,"HloDiff is a semantic diff tool for HLO modules. It compares the graph structure of two HLO Modules focusing on the computational differences ignoring irrelevant changes such as instruction names, parameter ordering, layouts (in some instances) etc.  The tool supports: 1. Diffing of large HLO Modules (>100k) nodes in <1 minute. 2. Summarized output of diffs highlighting what has changed for updated HLO instructions. Note only XLA's HLO format is supported at the moment.",2025-03-24T23:47:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89915
copybara-service[bot],PR #24010: Added GPU spec for B200,PR CC(Make var loading logging messages debug from info): Added GPU spec for B200 Imported from GitHub PR https://github.com/openxla/xla/pull/24010 Copybara import of the project:  b6fff6e24ef907c0ea43c7f11a5c7f358eaff3d3 by Dimitris Vardoulakis : Added GPU spec for B200 Merging this change closes CC(Make var loading logging messages debug from info) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24010 from dimvar:b200spec b6fff6e24ef907c0ea43c7f11a5c7f358eaff3d3,2025-03-24T23:05:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89914
copybara-service[bot],Add auxiliary methods to the pjrt async client.,Add auxiliary methods to the pjrt async client. Reverts 46e4e9680f157a0605ba3be12358586362dd4c0b,2025-03-24T22:35:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89913
copybara-service[bot],Remove libtensorflow configs,Remove libtensorflow configs,2025-03-24T22:30:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89912
copybara-service[bot],Reverts 46e4e9680f157a0605ba3be12358586362dd4c0b,Reverts 46e4e9680f157a0605ba3be12358586362dd4c0b,2025-03-24T22:17:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89911
copybara-service[bot],Display both raw and normalized FLOPs rate in op detail section to avoid confusion.,Display both raw and normalized FLOPs rate in op detail section to avoid confusion.,2025-03-24T21:38:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89910
copybara-service[bot],HloRunnerPjRt should forward comp envs.,"HloRunnerPjRt should forward comp envs. Certain compilations require comp env information which may be present in the module. HloRunnerPjRt was not previously forwarding this information, but it should be made available.",2025-03-24T21:32:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89909
copybara-service[bot],add SparseCore Reshard,add SparseCore Reshard,2025-03-24T20:53:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89908
wonjeon,[mlir][tosa] Update Tensorflow to match TOSA v1.0 specification (part 4),We've been pushing TOSA v1.0 LLVM patches to upstream. This PR includes commits to keep the following operators to be aligned with LLVM: [mlir][tosa] Update value to values for ConstOp and ConstShapeOp https://github.com/llvm/llvmproject/pull/129943 [mlir][tosa] Change MatMul zeropoint to inputs https://github.com/llvm/llvmproject/pull/130332 [mlir][tosa] Switch zero point of negate to input variable type https://github.com/llvm/llvmproject/pull/129758 [mlir][tosa] Convert RESCALE op multiplier and shift from attributes to inputs https://github.com/llvm/llvmproject/pull/129720 [mlir][tosa] Add support for EXTDOUBLEROUND and EXTINEXACTROUND https://github.com/llvm/llvmproject/pull/130337 [mlir][tosa] Change Rescale zero points to be inputs  https://github.com/llvm/llvmproject/pull/130340 This PR addresses the current issue of broken code and lit tests.,2025-03-24T20:44:28Z,size:XL comp:lite-tosa,closed,0,5,https://github.com/tensorflow/tensorflow/issues/89907,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Local testing done successfully: INFO: Analyzed 33 targets (492 packages loaded, 33864 targets configured). INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 20.285s, Critical Path: 18.51s INFO: 44 processes: 1 internal, 43 local. INFO: Build completed successfully, 44 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test         PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test             PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test      PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test                PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test   PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test           PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test  PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test      PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test         PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 3.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test     PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test   PASSED in 0.3s Executed 17 out of 17 tests: 17 tests pass.","Hi  , Can you please sign the CLA? Thanks!","> Hi  , Can you please sign the CLA? Thanks! Hi  Thanks for checking. The CLA issue looks resolved now.","> thanks for the patch! did a first pass and left a few comments. Thanks, Ge . Addressed your comments."
raghureddy-sripathi,from deepface import DeepFace," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: unable import DeepFace from deepface how to get rid of this error?  Standalone code to reproduce the issue ```shell  ImportError                               Traceback (most recent call last) File c:\Users\ADMIN\AppData\Local\Programs\Python\Python311\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[2], line 2       1 import cv2 > 2 from deepface import DeepFace File c:\Users\ADMIN\AppData\Local\Programs\Python\Python311\Lib\sitepackages\deepface\DeepFace.py:15      13 import numpy as np      14 import pandas as pd > 15 import tensorflow as tf      17  package dependencies      18 from deepface.commons import package_utils, folder_utils File c:\Users\ADMIN\AppData\Local\Programs\Python\Python311\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File c:\Users\ADMIN\AppData\Local\Programs\Python\Python311\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.')      97  pylint: enable=wildcardimport,gimportnotattop,unusedimport,linetoolong ImportError: Traceback (most recent call last):   File ""c:\Users\ADMIN\AppData\Local\Programs\Python\Python311\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```  Relevant log output ```shell ```",2025-03-24T20:40:57Z,type:bug TF 2.18,closed,0,9,https://github.com/tensorflow/tensorflow/issues/89906,"Hi sripathi , could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Are you satisfied with the resolution of your issue? Yes No,"> Hi sripathi , could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: >  > You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow.  CC(Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.) Also this is a duplicate of  CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) Thank you! HI   Thank you for response. Actually the thing is that i'm using python 3.11.5 and want to use deepface library but it doesn't work with latest version of python so i'm getting this error every time i'm trying to run. so i tried to download 3.9 version of python it works right now. maybe latest version doesn't have all modules.","If 3.9 works but 3.11 doesn't, then that's a different issue than CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.), sorry for that. Can you please post some samples to show how a simple Python script (could be as simple as `import tensorflow as tf`) fails to execute with 3.11 but fails with 3.9? For both versions, also please post output of `pip list`. And, just to confirm, are you building from source (given ""source"" answer in the form) or installing TF via `pip install`? Can you also list how you are installing TF/packages you use, if it's not `pip install`?","> Deepface is not TF, likely the issue should be opened there. >  > But, for the TF bits, this is a duplicate. Always search for duplicates. Hi  yeah I know but i tried to run this it doesn't work with latest version of python. I thought if i update tensorflow version it might change but I already have latest version. So, instead of  using python 3.11.5 i tried to solve in python 3.9. now the issue is sorted.","Yeah, if the issue is between python 3.9 and python 3.11 we need to sort this soon, since this is the last version of TF that would support 3.9, I think",Are you satisfied with the resolution of your issue? Yes No,"> Yeah, if the issue is between python 3.9 and python 3.11 we need to sort this soon, since this is the last version of TF that would support 3.9, I think I have no idea that latest version of python does'nt support full extent of  tensorflow. is that true?",We cannot support all versions of Python with all versions of TF. So each release of TF one version of Python might be dropped and another added in.
copybara-service[bot],[XLA:Python] Remove modules that have been moved to the JAX repository.,[XLA:Python] Remove modules that have been moved to the JAX repository.,2025-03-24T20:39:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89905
kossyrev-bg,Keras Hub Model Conversion Fails," 1. System information  Ubuntu 18.04, python 3.12.3  tf installed via `uv pip`   `tensorflow==2.18.1, keras==3.9.0`  2. Code ```  %% import os os.environ['KERAS_BACKEND'] = 'tensorflow'   Set the models from keras_hub to use tensorflow  %% import tensorflow as tf import tensorflow.keras as keras import keras_hub from PIL import Image from IPython.display import display import numpy as np  %%  %%  Input variables here  See https://keras.io/keras_hub/presets/ MODEL_NAME = ""efficientnet_b0_ra4_e3600_r224_imagenet""  Path to training data TRAINING_DATA_PATH = ...  How many classes we want to classify NUM_CLASSES = 2  Trainingrelated tuning  How many GPUs to use, assumes they are named GPU:0, GPU:1, etc. NUM_GPUS = 4 BATCH_SIZE = 32   Per GPU NUM_EPOCHS = 10 NUM_AUGS = 2   Number of random augmentations to apply to each image AUG_STRENGTH = 0.5   How strong the augmentations should be  Whether to quantize pertensor or perchannel PER_TENSOR_TFLITE = False  IMAGE_SIZE = 224   GLOBAL_BATCH = NUM_GPUS * BATCH_SIZE  %%  Create a MirroredStrategy. gpus = [f""GPU:{i}"" for i in range(NUM_GPUS)] strategy = tf.distribute.MirroredStrategy(gpus) print(""Number of devices: {}"".format(strategy.num_replicas_in_sync))  Open a strategy scope. with strategy.scope():      Everything that creates variables should be under the strategy scope.      In general this is only model construction & `compile()`.     image_classifier = keras_hub.models.ImageClassifier.from_preset(         MODEL_NAME,         activation=None,   outputs logits         num_classes=NUM_CLASSES     )     image_classifier.compile(         optimizer=keras.optimizers.Adam(             learning_rate=1e4,              clipnorm=1.0         ),         loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),   Consumes logits         metrics=[keras.metrics.SparseCategoricalAccuracy()]     )  %%  Create dataset train_dataset, val_dataset = keras.preprocessing.image_dataset_from_directory(     TRAINING_DATA_PATH,     labels=""inferred"",     label_mode=""int"",     batch_size=None,     image_size=(IMAGE_SIZE, IMAGE_SIZE),     shuffle=True,     validation_split=0.2,     subset=""both"",     interpolation=""bilinear"",     verbose=True,     seed=42 )  Define simple augmenter aug_layer = keras.layers.RandAugment(value_range=(0, 255), num_ops=NUM_AUGS, factor=AUG_STRENGTH) map_fn = lambda x, y: (aug_layer(x, training=True), y)  Prefetch the data, adding augmentation to the training dataset train_dataset = train_dataset.batch(GLOBAL_BATCH, drop_remainder=True).map(map_fn, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE) val_dataset = val_dataset.batch(GLOBAL_BATCH, drop_remainder=True).prefetch(tf.data.AUTOTUNE)  %%  %%  Freeze the base model to only retrain the head (optional)  image_classifier.backbone.trainable = False  Train the model on all available devices. image_classifier.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=val_dataset)  Test the model on all available devices.  TODO: Need a test dataset  image_classifier.evaluate(test_dataset) image_classifier.save(f""{MODEL_NAME}.keras"")  %%  Test the trained model on 10 validation images  This works fine, getting at least 8 correct for imgs, label in val_dataset.take(1):     for i in range(10):         img = imgs[i:i+1]         preds = image_classifier.predict(img)         print(f""Raw preds: {preds}, Predicted: {preds.argmax()}, Actual: {label[i]}"")  %%  Remove preprocessor model_preprocessor = image_classifier.preprocessor image_classifier.preprocessor = None  %%  Create endtoend model that takes in UINT8 image of size 224   Modify preprocessor, because GPU doesn't like bicubic? idk cfg = model_preprocessor.get_config() cfg[""image_converter""][""config""][""interpolation""] = ""bilinear"" new_preproc = keras_hub.models.ImageClassifierPreprocessor.from_config(cfg)  Input layer is (1, 224, 224, 3) uint8 input_layer = keras.layers.Input(shape=(224, 224, 3), dtype=tf.uint8, batch_size=1)  Convert to float 32 like the preprocessor expects (I'd like to skip this if possible,  because it might be part of why the tflite conversion is failing,  but I don't know how without dismantling the preprocessor) float_input = keras.layers.Lambda(lambda x: tf.cast(x, tf.float32))(input_layer)  Preprocess the image preprocessed_input = new_preproc(float_input, training=False)  Run through the model  no doublepreprocessing since we removed it logits = image_classifier(preprocessed_input, training=False)  Combine into one model full_model = keras.models.Model(inputs=float_input, outputs=logits)  Compile the model (unclear if necessary) full_model.compile(     optimizer=keras.optimizers.Adam(1e4),     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),   Consumes logits     metrics=[keras.metrics.SparseCategoricalAccuracy()] ) full_model.trainable = False  %%  Test the new model on 10 validation images. This works too, roughly same  accuracy as above num_correct = 0 num_wrong = 0 for imgs, label in val_dataset.take(1):     for i in range(GLOBAL_BATCH):         img = imgs[i:i+1]          preds = full_model.predict(tf.cast(img, tf.uint8))         preds = full_model.predict(img)         print(f""Raw preds: {preds}, Predicted: {preds.argmax()}, Actual: {label[i]}"")         if preds.argmax() == label[i]:             num_correct += 1         else:             num_wrong += 1 print(f""Correct: {num_correct}, Wrong: {num_wrong}"")  %%  Use data from the validation dataset to calibrate the quantized model def representative_data_gen():     batch_count = (300 // GLOBAL_BATCH) + 1     run = 0     for x, _ in val_dataset.take(batch_count):   Use a few samples         run += 1         print(f""Running batch {run}/{batch_count}"")          Pull one image at a time         for i in range(GLOBAL_BATCH):             img = x[i:i+1]             yield [tf.cast(img, tf.uint8)]             yield [img]  Convert to tflite, using pertensor quantization for ARTPEC8 converter = tf.lite.TFLiteConverter.from_keras_model(full_model) converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.representative_dataset = representative_data_gen converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] converter.inference_input_type = tf.uint8 converter.inference_output_type = tf.float32 if PER_TENSOR_TFLITE:     converter._experimental_disable_per_channel = True tflite_quant_model = converter.convert() with open(f""{MODEL_NAME}.tflite"", ""wb"") as f:     f.write(tflite_quant_model)  %%  Run the TFLite model on some test data interpreter = tf.lite.Interpreter(model_content=tflite_quant_model) input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() input_index = input_details[0][""index""] output_index = output_details[0][""index""] interpreter.allocate_tensors()  %% num_correct = 0 num_wrong = 0 for x, label in val_dataset.take(3):     for i in range(GLOBAL_BATCH):         img = x[i:i+1]          Need to cast to uint8 because tflite model expects it         interpreter.set_tensor(input_index, tf.cast(img, tf.uint8))         interpreter.invoke()         output_data = interpreter.get_tensor(output_index)         print(f""Raw preds: {output_data}, Predicted: {output_data.argmax()}, Actual: {label[i]}"")         if output_data.argmax() == label[i]:             num_correct += 1         else:             num_wrong += 1 print(f""{num_correct=}, {num_wrong=}"") ```  3. Failure after conversion Model accuracy drastically drops after conversion: ``` Raw preds: [[0.08086799  0.20216998]], Predicted: 1, Actual: 0 Raw preds: [[ 0.       0.040434]], Predicted: 0, Actual: 1 Raw preds: [[1.0512838   0.48520795]], Predicted: 1, Actual: 0 Raw preds: [[1.0108498   0.36390597]], Predicted: 1, Actual: 1 Raw preds: [[0.9704159   0.12130199]], Predicted: 1, Actual: 0 Raw preds: [[0.72781193  0.24260397]], Predicted: 1, Actual: 1 Raw preds: [[0.08086799 0.24260397]], Predicted: 0, Actual: 0 Raw preds: [[ 0.16173598 0.040434  ]], Predicted: 0, Actual: 0 Raw preds: [[1.1321518  0.040434 ]], Predicted: 1, Actual: 0 ... Raw preds: [[0.24260397  0.12130199]], Predicted: 1, Actual: 1 Raw preds: [[0.76824594 0.08086799]], Predicted: 1, Actual: 0 Raw preds: [[0.36390597  0.24260397]], Predicted: 1, Actual: 1 num_correct=196, num_wrong=188 ```",2025-03-24T20:37:10Z,comp:lite TFLiteConverter TF 2.18,closed,0,18,https://github.com/tensorflow/tensorflow/issues/89904,This belongs in keras repos.,"I found the problem with the model setup. However, after the tflite conversion, the model accuracy is still drastically lower than it was before. I can update the code snippet to show the new setup, but it's basically just combining preprocessing with the model into one allinone model. I would still like help with why tflite conversion is reducing accuracy by so much. ","Oh, I misunderstood (from the title), this is not about Keras but model conversion to TFLite. Can you minimize this to a model as small as possible?","This is an efficientnet model from the keras hub: https://www.kaggle.com/models/keras/efficientnet/keras/efficientnet_b0_ra4_e3600_r224_imagenet/ It's about 5MB after quantization to tflite. Is that too big? Do you need the keras model and the tflite model, and do you need the data?",The smaller the model the easier is to debug,"Hi, bg  I apologize for the delay in response, I am able to replicate the same behavior from my end with sample flowers dataset with your provided code snippet here is gistfile for reference so we will have to dig more into this issue and will update you, thank you for bringing this issue to our attention. Thank you for your cooperation. ``` Original model predictions: 1/1 ━━━━━━━━━━━━━━━━━━━━ 3s 3s/step Raw preds: [[2.7760098  3.1728508]], Predicted: 1, Actual: 1 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 326ms/step Raw preds: [[0.55148685  0.8257771 ]], Predicted: 1, Actual: 1 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 280ms/step Raw preds: [[ 3.8654664 4.097068 ]], Predicted: 0, Actual: 0 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 299ms/step Raw preds: [[ 15.905437 15.347878]], Predicted: 0, Actual: 0 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 313ms/step Raw preds: [[0.43868783  1.6053506 ]], Predicted: 1, Actual: 1 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 247ms/step Raw preds: [[ 13.696747 12.681302]], Predicted: 0, Actual: 0 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 199ms/step Raw preds: [[1.2708153   0.60328835]], Predicted: 1, Actual: 1 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 188ms/step Raw preds: [[0.7198984  0.65562236]], Predicted: 0, Actual: 1 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 207ms/step Raw preds: [[1.3494838  2.3646579]], Predicted: 1, Actual: 1 1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 192ms/step Raw preds: [[1.334792   1.3840239]], Predicted: 1, Actual: 1 ``` ``` TFLite model predictions: TFLite output: [[2.4290873e+13  2.2772695e+13]], Predicted class: 1 TFLite output: [[6.0727183e+12  3.0363592e+12]], Predicted class: 1 TFLite output: [[0. 0.]], Predicted class: 0 TFLite output: [[2.1254514e+13  1.9736335e+13]], Predicted class: 1 TFLite output: [[4.2509028e+13  2.1254514e+13]], Predicted class: 1 TFLite output: [[2.2772695e+13  2.1254514e+13]], Predicted class: 1 TFLite output: [[7.5908980e+12  1.2145437e+13]], Predicted class: 1 TFLite output: [[3.1881771e+13  2.1254514e+13]], Predicted class: 1 TFLite output: [[2.4290873e+13  2.1254514e+13]], Predicted class: 1 TFLite output: [[0. 0.]], Predicted class: 0 TFLite output: [[2.4290873e+13 3.4918130e+13]], Predicted class: 1 TFLite output: [[1.3663616e+13  1.5181796e+12]], Predicted class: 1 TFLite output: [[1.5181796e+13  2.2772695e+13]], Predicted class: 1 TFLite output: [[0. 0.]], Predicted class: 0 TFLite output: [[3.4918130e+13  2.7327233e+13]], Predicted class: 1 TFLite output: [[1.0627257e+13  2.1254514e+13]], Predicted class: 1 ```","Ah I'm sorry, let me update the code in the example. I'm no longer running into quite the same scaling error issue. But the outputs are still wrong. I'll post again once it's updated","Okay  the code has been updated, with new example logits, thank you for your help!","I'll go ahead and update the gist file as well, if I can","Hi bg To facilitate further investigation and debugging of the current issue, we would greatly appreciate it if you could consider updating the existing gist file or creating a new Google Colab notebook including a sample flowers dataset within this resource would be particularly helpful for us to reproduce the reported behavior and analyze the underlying cause effectively. Thank you for your cooperation.","Hi, bg  I see you're using `int8`quantization, TensorFlow Lite offers other posttraining quantization options like `float16` quantization. This offers a smaller model size reduction and speedup compared to `int8` but often preserves accuracy much better so could you please give it try with `float16` quantization something like below and see are you getting better predictions as compared to `int8` quantization ?  ``` converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)     converter.optimizations = [tf.lite.Optimize.DEFAULT]     converter.target_spec.supported_types = [tf.float16]      tflite_model = converter.convert()     with open(save_path_tflite, ""wb"") as f:         f.write(tflite_model) ``` Thank you for your cooperation.","Okay, I gave that a shot here. You're totally right, the accuracy loss is much much less with float16.  Unfortunately, the edge system I'm building for requires int8 quantization. Is there anything I can do to make the int8 conversion better?","Oh, quantization is expected to lose precision, you have fewer bits to represent the numbers from the weights and inputs/outputs.","I am aware, but I didn't think it would be quite this bad. Would a larger model perhaps help with this penalty?","Hi, bg Larger models with more parameters can sometimes mitigate the effects of quantization because they have more capacity to learn and generalize. However, this is not a guaranteed solution and can lead to increased computational requirements. `int8` quantization is necessary for your edge system it often requires careful tuning and adjustments to maintain accuracy. Implementing quantization aware training is likely your best bet for improving performance please refer this Quantization aware training in Keras example. Additionally finetuning the model after quantization and ensuring a robust representative dataset can also help mitigate accuracy loss. Thank you for your cooperation.","I'll try out the example and update the gist accordingly, thank you","It seems that `keras_hub` does not play nicely with the `keras` from `from tensorflow_model_optimization.python.core.keras.compat import keras` If there's no easy way around this, I guess I'll have to abandon using kerashub for my models :/",Having better luck moving over to tensorflow_hub. Closing this as no longer relevant.
copybara-service[bot],[JAX] [XLA:Python] Migrate pytree module to JAX.,[JAX] [XLA:Python] Migrate pytree module to JAX.,2025-03-24T20:37:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89903
copybara-service[bot],Add comparator to `GetSortedEvents`,Add comparator to `GetSortedEvents`,2025-03-24T20:36:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89902
copybara-service[bot],Migrate HloCostAnalysis helper libraries to open source.,Migrate HloCostAnalysis helper libraries to open source.,2025-03-24T20:28:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89901
CourseraPrash,Compatibility issue with Python 3.11," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.19.0  Custom code Yes  OS platform and distribution windows 10  Mobile device _No response_  Python version 3.11.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Traceback (most recent call last):   File ""C:\Users\******\AppData\Roaming\Python\Python311\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\Users\******\Documents\Python Scripts\DFRC system\myenv\Old files\Training_OptNet_multicarrier.py"", line 9, in      import mimocmcmulticarrier as m   File ""C:\Users\******\Documents\Python Scripts\DFRC system\myenv\Old files\mimocmcmulticarrier.py"", line 8, in      import tensorflow.compat.v1 as tf   File ""C:\Users\******\AppData\Roaming\Python\Python311\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport   File ""C:\Users\******\AppData\Roaming\Python\Python311\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\******\AppData\Roaming\Python\Python311\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.  Standalone code to reproduce the issue ```shell import tensorflow ```  Relevant log output ```shell ```",2025-03-24T20:24:22Z,stat:awaiting response type:build/install stale subtype:windows TF 2.18,closed,0,8,https://github.com/tensorflow/tensorflow/issues/89900,"I have created a virtual environment and worked on it for the last three years. I haven't worked on this virtual environment for about three weeks. When I returned today and ran the same code, I got the above error.","Hi  , could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!","Hi, Thank you for the feedback. I didn't install TensorFlow. I created the virtual environment three years ago and used TensorFlow for my project for three years, but it is suddenly throwing this error now. I ran the same code that I used three weeks ago without any modification, and it gave the above error.",Something is contradicting in this bug. You say you use TF 2.19 and then you have a venv created 3 years ago which you haven't updated. But TF 2.19 was released only recently. Please make sure to provide accurate information so that we can properly debug.,"Also, Python 3.11 likely did not exist 3 years ago.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],PR #89337: Qualcomm AI Engine Direct - Compile QINT16 as QUINT16,"PR CC(Qualcomm AI Engine Direct  Compile QINT16 as QUINT16): Qualcomm AI Engine Direct  Compile QINT16 as QUINT16 Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/89337  What Separate https://github.com/tensorflow/tensorflow/pull/89132 into 2 PRs for better review experience. This PR only includes ""Compile"" part. Internal review recorded here.  Tests `qnn_compiler_plugin_test` ``` [] Global test environment teardown [==========] 127 tests from 5 test suites ran. (4621 ms total) [  PASSED  ] 127 tests. ``` `//tensorflow/lite/experimental/litert/vendors/qualcomm/core/utils:utils_test` ``` [] Global test environment teardown [==========] 11 tests from 3 test suites ran. (0 ms total) [  PASSED  ] 11 tests. ``` `//tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:all` ``` //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:op_wrapper_test (cached) PASSED in 0.0s //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:quantize_params_wrapper_test (cached) PASSED in 0.0s //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:param_wrapper_test PASSED in 0.0s //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:tensor_wrapper_test PASSED in 0.0s ``` Copybara import of the project:  ee416bfc273e6d174be75d5bdd6858e4d29a1f72 by chunhsue : Qualcomm AI Engine Direct  Compile QINT16 as QUINT16 Merging this change closes CC(Qualcomm AI Engine Direct  Compile QINT16 as QUINT16) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89337 from jiunkaiy:dev/chunhsue/compile_int16_as_uint16 ee416bfc273e6d174be75d5bdd6858e4d29a1f72",2025-03-24T20:21:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89899
copybara-service[bot],Reverts 7a6c71e9e19cb4723e5a7bc53e1168af66c5388b,Reverts 7a6c71e9e19cb4723e5a7bc53e1168af66c5388b,2025-03-24T20:19:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89898
copybara-service[bot],[xla:gpu] Remove ExecutionScope from command buffer,"[xla:gpu] Remove ExecutionScope from command buffer ExecutionScope is not used in XLA today, and this concept is too complicated. Instead of execution scopes command buffers should be able to construct a DAG of commands taking explicit dependencies between commands as arguments.",2025-03-24T20:16:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89897
copybara-service[bot],Internal change to LiteRT.,Internal change to LiteRT.,2025-03-24T19:28:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89896
copybara-service[bot],Add lang for syntax highlighting,Add lang for syntax highlighting,2025-03-24T19:21:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89895
copybara-service[bot],Fix TF Lite tests.,Fix TF Lite tests.,2025-03-24T19:18:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89894
copybara-service[bot],PR #24028: Fix some Shape::rank deprecation warnings.,"PR CC(TensorFlow estimator train_and_evaluate loss is None after step 0 and model does not train): Fix some Shape::rank deprecation warnings. Imported from GitHub PR https://github.com/openxla/xla/pull/24028 These headers are included from absolutely everywhere, and the warning spam can make it quite hard to see the actual problems. Copybara import of the project:  535c0329ba9bfc3b975a8820d0d9ff15bc8f7718 by Johannes Reifferscheid : Fix some Shape::rank deprecation warnings. These headers are included from absolutely everywhere, and the warning spam can make it quite hard to see the actual problems. Merging this change closes CC(TensorFlow estimator train_and_evaluate loss is None after step 0 and model does not train) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24028 from jreiffers:deprecations 535c0329ba9bfc3b975a8820d0d9ff15bc8f7718",2025-03-24T19:14:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89893
copybara-service[bot],[XLA] Open visibility of several modules in OSS.,"[XLA] Open visibility of several modules in OSS. We are moving pieces of xla/python into JAX, and this module is used by code that is moving.",2025-03-24T19:08:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89892
copybara-service[bot],"Rollback ""Fuse Sum -> Mul into Mean pattern""","Rollback ""Fuse Sum > Mul into Mean pattern"" Reverts 17a6a779c5be888d21c59707dec8bcff1f187e9d FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24010 from dimvar:b200spec b6fff6e24ef907c0ea43c7f11a5c7f358eaff3d3",2025-03-24T18:35:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89891
copybara-service[bot],Bumping up libtpu version to pick correct versioned nightlies,Bumping up libtpu version to pick correct versioned nightlies,2025-03-24T18:30:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89890
copybara-service[bot],Add litert_platform_support util and clean up test code.,Add litert_platform_support util and clean up test code.,2025-03-24T18:09:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89889
copybara-service[bot],Bump timeout to 60 minutes on `ci.yml` jobs,Bump timeout to 60 minutes on `ci.yml` jobs,2025-03-24T18:06:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89888
copybara-service[bot],"Copy ConvertConst, ConvertSimQuant, and Passes from lite directory to TF common","Copy ConvertConst, ConvertSimQuant, and Passes from lite directory to TF common",2025-03-24T18:05:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89887
copybara-service[bot],[XLA] Remove visibility setting,[XLA] Remove visibility setting,2025-03-24T17:13:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89886
copybara-service[bot],Add env var to enable tfrt gpu client.,Add env var to enable tfrt gpu client.,2025-03-24T17:11:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89885
Harshit-2306,I'm spamming with AI,,2025-03-24T16:46:59Z,size:L invalid,closed,0,1,https://github.com/tensorflow/tensorflow/issues/89884,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],[XLA:GPU] Allow symbolic tile analysis to go through `concatenate`s for nested GEMM fusions.,"[XLA:GPU] Allow symbolic tile analysis to go through `concatenate`s for nested GEMM fusions. We only enable a restricted case of concatenations for now, where tiles never propagate to more than a single operand. Concretely, we enforce the following constraints: 1. the start `offset` along the concatenation dimension is `0`; 2. the `stride` is always `1` along the concatenation dimension; 3. the `size` is a divisor of the size of the concatenation dimension of each operand. These constraints are slightly over restrictive (starting at a non`0` offset that is a multiple of the tile size would also be fine, and so would be having a non`1` stride so long as `stride * size` is a divisor of the size of the concatenation dimension of each operand). Nevertheless, they're most likely sufficient to complete the migration of GEMMs to the generic Triton emitter.",2025-03-24T16:12:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89883
nvdiacudabot,commit,commit,2025-03-24T15:47:22Z,size:M,closed,0,1,https://github.com/tensorflow/tensorflow/issues/89882,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],"PR #20611: [XLA:GPU] Re-implement command buffer to remove execution_scope, construct dependencies through DAG.","PR CC(Adding matrix square root op): [XLA:GPU] Reimplement command buffer to remove execution_scope, construct dependencies through DAG. Imported from GitHub PR https://github.com/openxla/xla/pull/20611 This PR is large redesign of XLA command buffer, which basically follows the below idea:  1.  Removing the execution scope concept from command buffer, and basically there should be no execution stream concept in command buffer. Instead command buffer will construct the graph through data flow in the command buffer command sequence (converted from XLA thunk sequence), so overlapping (execution scope) will be automatically deduced by the graph topology. This is more matching the cudagraph's conept, where it does not have a stream property, and operators will be auto launched to different streams whenever the cudagraph's topology does not have dependencies (edges) across operators.  2. CommandBufferCmd now can depend on other commands (specified through command index in the CommandBufferCmdSequence), and dependencies (R/W, W/W conflicts) is auto inferred from buffer assignment results when appending a new command into CommandBufferCmdSequence (CommandBufferCmdSequence::Append()).  3. When implementation CommandBuffer, dependencies that specified through CommandBufferCmd index is translated into node dependencies across cuda nodes.  The main benefits of the design are:  1. Constructing graph topology through data flow, instead of thunk execution order, can automatically enables maximum concurrency that is allowed by data flow across operators, where it should have better perf.  2. The design is much more natural and intuitive, command buffer is designed to be an abstract of cuda graph, so it should just be a graph of how the result is calculated through operators. Execution scope or stream is some kind of runtime concept where it should not be included in command buffer's graph. In current design, XLA introduces the execution scope concept just to maintain the concept of execution stream in XLA runtime, this is unnecessary and counter intuitive.  3.  Command buffer is easier to implement and use,  simulating the multistream order in command buffer through execution scope introduces lots of hard to understand codes.  While we can see that through the data flow design, the implementation code is more compact and easier to understand.  Copybara import of the project:  73490c0a56c00b58d8d8d2f8e164f6080d03ce0f by Shawn Wang : Rewrite XLA command buffer  c2644d5d9084c9ae17111f5de8920c39cb495521 by Shawn Wang : fix  7cf96d7b01e001769565b4ba6eae4431e0ba650f by Shawn Wang : add build dependency Merging this change closes CC(Adding matrix square root op) Reverts 7a50c1ae5521133ea7573168b6e2726260f71c07 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20611 from shawnwang18:shawnw/cuda_graph_dependency 7cf96d7b01e001769565b4ba6eae4431e0ba650f",2025-03-24T15:23:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89881
copybara-service[bot],Reverts 17a6a779c5be888d21c59707dec8bcff1f187e9d,Reverts 17a6a779c5be888d21c59707dec8bcff1f187e9d,2025-03-24T15:19:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89880
copybara-service[bot],[XLA:GPU] Use CeilOfRatio.,[XLA:GPU] Use CeilOfRatio.,2025-03-24T15:06:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89879
copybara-service[bot],[xla] Add a pass to convert buffer representation to attributes.,[xla] Add a pass to convert buffer representation to attributes.,2025-03-24T14:42:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89878
copybara-service[bot],[xla:cpu] Add a build rule that creates an optional `no_mkl` target.,"[xla:cpu] Add a build rule that creates an optional `no_mkl` target. This is for AOT targets that want to build Eigen contraction kernel without oneDNN microkernel by default. ``` $ bazel query ""somepath(//xla/service/cpu:cpu_aot_compilation_result_no_mkl, //xla/tsl/framework/contraction:eigen_contraction_kernel_with_mkl)"" INFO: Empty results $ bazel query ""somepath(//xla/service/cpu:cpu_aot_compilation_result_no_mkl, //xla/tsl/framework/contraction:eigen_contraction_kernel_no_mkl)"" //xla/service/cpu:cpu_aot_compilation_result_no_mkl //xla/backends/cpu/runtime:thunk_proto_serdes_no_mkl //xla/backends/cpu/runtime:dot_thunk_no_mkl //xla/tsl/framework/contraction:eigen_contraction_kernel_no_mkl ```",2025-03-24T14:36:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89877
copybara-service[bot],Reverts 9baae78cb03a735d78789c19a4dbdd87be7e79f2,Reverts 9baae78cb03a735d78789c19a4dbdd87be7e79f2,2025-03-24T14:24:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89876
copybara-service[bot],[xla] Verify buffer related HLO.,"[xla] Verify buffer related HLO. We have introduced buffer_id field in Shape for representing HLO buffer types. We now extend the verifier to recognize customcall targets pin and unpin along with the existing target allocateBuffer for buffer related operations. When buffers aren't allowed in a program, no Shape can have a valid buffer_id and customcall targets pin and unpin can't be used. This is what all the existing HLO passes would expect. When buffers are allowed, we verify that pin and unpin are used properly. We allow other customcall targets to use buffers. We also allow instructions, such as kTuple, kWhile, kParameter and kGetTupleElement to pass through buffers. All other instructions aren't allowed to use buffers. We will introduce a new HLO pass to convert buffer information to XLA attributes that copyinsertion would understand and clear the buffer_id field in Shapes so that all existing HLO passes won't need to handle buffer_ids.",2025-03-24T14:22:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89875
copybara-service[bot],[XLA:GPU] Add number of local participants to GpuCliqueKey.,"[XLA:GPU] Add number of local participants to GpuCliqueKey. The number of local participants doesn't add new information to the key, because we should never get two identical cliques that only different in the number of local participants. However, putting the number inside the key makes so many places in the codebase simpler and avoids annoying recalculations. We can easily count the number of local devices when we create the key, but later it requires getting runtime param and devices ids again.",2025-03-24T14:07:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89874
copybara-service[bot],[XLA:Python] Open visibility of more paths to JAX.,[XLA:Python] Open visibility of more paths to JAX. Change in preparation for migrating more code into JAX and out of XLA.,2025-03-24T14:05:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89873
copybara-service[bot],Mark `Shape::rank()` and `Shape::dimensions_size()` as candidate for inlining.,Mark `Shape::rank()` and `Shape::dimensions_size()` as candidate for inlining.,2025-03-24T14:02:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89872
copybara-service[bot],PR #24059: Add autotuning results to fix gpu_compiler_test for Blackwell,PR CC(The performance of transpose_conv op in TensorFlow Lite): Add autotuning results to fix gpu_compiler_test for Blackwell Imported from GitHub PR https://github.com/openxla/xla/pull/24059 Copybara import of the project:  4297c120c719c87e9607f3eaf7a0298eead048b2 by Dimitris Vardoulakis : Add autotuning results to fix gpu_compiler_test for Blackwell Merging this change closes CC(The performance of transpose_conv op in TensorFlow Lite) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24059 from dimvar:fixgpucompilertestforblackwell 4297c120c719c87e9607f3eaf7a0298eead048b2,2025-03-24T14:00:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89871
copybara-service[bot],[XLA:GPU/TMA] Replacing Triton specific instructions for loads/stores with TritonXLA ops. Currently this is only enabling standard loads/stores. TMA equivalents will be considered later.,[XLA:GPU/TMA] Replacing Triton specific instructions for loads/stores with TritonXLA ops. Currently this is only enabling standard loads/stores. TMA equivalents will be considered later.,2025-03-24T13:16:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89870
copybara-service[bot],[XLA:GPU] Use the GetReplicaGroupCountAndSize helper in RaggedAllToAllDecomposer pass.,[XLA:GPU] Use the GetReplicaGroupCountAndSize helper in RaggedAllToAllDecomposer pass.,2025-03-24T12:25:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89869
copybara-service[bot],"Removed optimized batch_matmul in8 int32, due to assumption that it is not needed.","Removed optimized batch_matmul in8 int32, due to assumption that it is not needed.",2025-03-24T12:25:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89868
copybara-service[bot],Adjust PriorityFusion to allow forming simple multi-output Triton fusions.,"Adjust PriorityFusion to allow forming simple multioutput Triton fusions. With simple multioutput fusion we mean fusions that have only one root without users. We only allow fusions which are supported by the current Triton codegen. We further restrict to fuse only if there is a single user to fuse with, as otherwise we would also need to detect which is the best user to fuse with, and the priority updates become more complicated. This can be done in a later step.",2025-03-24T12:22:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89867
copybara-service[bot],"[XLA:Collective] Get participant ids, count and group sizes from CollectiveOpGroupMode.","[XLA:Collective] Get participant ids, count and group sizes from CollectiveOpGroupMode. It is not very obvious from the code or comments, but we can use `GetReplicaGroupCountAndSize` and `GetParticipatingFlattenedIdGroups` only for `AllReduce` and `AllGather`, because of the check in  `GetCollectiveUseGlobalDeviceIds`. There is no reason why `CollectiveOpGroupMode` of other instruction would not be enough to get the information. The check of `AllReduce` and `AllGather` was added in https://github.com/openxla/xla/pull/20024, but I couldn't find explanation why it wouldn't work of other instruction.",2025-03-24T11:20:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89866
copybara-service[bot],Use caller_instructions() method instead of CallGraph (NFC).,"Use caller_instructions() method instead of CallGraph (NFC). The caller_instructions() method have been added recently, so we can use it and avoid constructing a call graph.",2025-03-24T10:51:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89865
copybara-service[bot],Refactor `hlo_memory_scheduler`.,Refactor `hlo_memory_scheduler`. Convert the `MemorySchedulerAlgorithm` and `ModuleSchedulerAlgorithm` type aliases into proper classes with inheritance relationship. This removes the need for the `ComputationSchedulerToModuleScheduler` function. ``` ModuleSchedulerAlgorithm    + DefaultModuleScheduler ``` This design scheme is simpler and easier to extend/modify. Also rename `PostOrderMemoryScheduler` to `PostOrderScheduler` and `BFSMemoryScheduler` to `BFScheduler`. These two variants don't minimise memory usage which makes the original names a bit misleading. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/80074 from yzhou51:fullconnect_use_slm 6ba325e738ee82b60bc611247c797451b8c957ae,2025-03-24T10:38:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89864
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T10:35:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89863
copybara-service[bot],Automated Code Change,Automated Code Change Reverts 1b526ff3b84d74a7aa65d7f9348176af8911df16,2025-03-24T10:21:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89862
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T09:28:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89861
chunhsue,Qualcomm AI Engine Direct - Enable QNN Power Op, What Support new op `tfl.pow` in QNN.  Tests `qnn_compiler_plugin_test` ``` [] Global test environment teardown [==========] 119 tests from 5 test suites ran. (4382 ms total) [  PASSED  ] 119 tests. ```,2025-03-24T09:12:44Z,size:S,open,0,0,https://github.com/tensorflow/tensorflow/issues/89860
copybara-service[bot],PR #23320: [NVIDIA GPU] Add utility functions for multi-host fast-interconnect domain,"PR CC([Feature Request] Addition of new operation to Tensorflow Lite for ""ENet""): [NVIDIA GPU] Add utility functions for multihost fastinterconnect domain Reverts 7a50c1ae5521133ea7573168b6e2726260f71c07",2025-03-24T09:02:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89859
copybara-service[bot],[xla:cpu:benchmarks] Move Gemma 2 PyTorch benchmark to the correct folder.,[xla:cpu:benchmarks] Move Gemma 2 PyTorch benchmark to the correct folder. It was added to the old path by mistake.,2025-03-24T08:59:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89858
copybara-service[bot],Reverts 1b526ff3b84d74a7aa65d7f9348176af8911df16,Reverts 1b526ff3b84d74a7aa65d7f9348176af8911df16,2025-03-24T08:42:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89857
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T08:32:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89856
default1360,`Aborted` in `tf.compat.v1.nn.max_pool_v2`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tf.compat.v1.nn.max_pool_v2 in tfnightly 2.20.0dev20250302. This bug can be used to trigger a denial of service attack.  Standalone code to reproduce the issue ```shell import tensorflow as tf from tensorflow.compat.v1.nn import max_pool_v2 input_tensor = tf.fill([1, 10, 4, 1], 0.5)  ksize = [1, 1250999896764, 1, 1]   strides = [1, 1250999896764, 1, 1] padding = ""SAME"" max_pool_v2(input_tensor, ksize=ksize, strides=strides, padding=padding) ```  Relevant log output ```shell Status: INVALID_ARGUMENT: Attr ksize has value 1250999896764 out of range for an int32 Aborted ```",2025-03-24T08:28:02Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/89855," This error occurs because values greater than 32bit are not allowed for ksize and strides that is been used. It would be better to handle this scenario by raising a proper error message indicating that the size exceeds the int32 limit. This can prevent the current behavior, which results in an abrupt abort. I’d be happy to work on a fix for this by adding input validation. Let me know if this approach sounds good!","Hi  ,  , thanks for your contribution! Welcome to TensorFlow, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. I did not encounter any crashes; instead, I received the following error message: ``` InvalidArgumentError: {{function_node __wrapped__MaxPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} Attr ksize has value 1250999896764 out of range for an int32 [Op:MaxPool] name:  ``` Based on this error, I made some modifications, and the code is now working fine for me. I have attached a gist for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T08:25:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89854
default1360,`Aborted` in `depthwise_conv2d_backprop_input`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tf.compat.v2.nn.depthwise_conv2d_backprop_input in tfnightly 2.20.0dev20250302. This bug can be used to trigger a denial of service attack.  Standalone code to reproduce the issue ```shell import tensorflow as tf class DummyTensor:     def __init__(self, data):         self.data = data     def __tf_function__(self, func, types, args=(), kwargs=None):         if func is tf.compat.v2.nn.depthwise_conv2d_backprop_input:             return tf.constant(1.0, dtype=tf.float32)         return NotImplemented x = DummyTensor(None) input_sizes = tf.constant([1, 4, 4, 1], dtype=tf.int32) filter_tensor = tf.constant([[1.0]], dtype=tf.float32) out_backprop = tf.constant([[1.0]], dtype=tf.float32) result = tf.compat.v2.nn.depthwise_conv2d_backprop_input(input_sizes, filter_tensor, out_backprop, strides=[1, 1], padding='VALID') ```  Relevant log output ```shell 20250324 16:25:46.795503: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C Aborted ```",2025-03-24T08:25:55Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89853,"Hi  , Welcome to TensorFlow, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. I did not encounter any crashes; instead, I received the following error message: ``` InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNativeBackpropInput_device_/job:localhost/replica:0/task:0/device:CPU:0}} Sliding window strides field must specify 4 dimensions [Op:DepthwiseConv2dNativeBackpropInput] name:  ``` Based on this error, I made some modifications, and the code is now working fine for me. I have attached a gist for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
default1360,`Aborted` in `tf.conv`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tf.conv in tfnightly 2.20.0dev20250302. This bug can be used to trigger a denial of service attack.  Standalone code to reproduce the issue ```shell import tensorflow as tf num_channels = 2 kernel_size = 3 input_shape = [1, 30, num_channels]   input_tensor = tf.random.normal(input_shape) input_tensor = tf.transpose(input_tensor, perm=[0, 2, 1])  filter_tensor = tf.random.normal([kernel_size, 1, num_channels])  conv_output = tf.nn.conv2d(input=input_tensor, filters=filter_tensor, strides=[1, 1, 1, 1], padding='VALID', data_format='NHWC', dilations=[1, 1, 1, 1], name=None) ```  Relevant log output ```shell 20250324 16:23:56.706135: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C Aborted ```",2025-03-24T08:24:04Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89852,"Hi  , Welcome to TensorFlow, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. I did not encounter any crashes; instead, I received the following error message: ``` InvalidArgumentError: {{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:CPU:0}} convolution input must be 4dimensional: [1,2,30] [Op:Conv2D] name:  ``` Based on this error, I made some modifications, and the code is now working fine for me. I have attached a gist for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
default1360,`Aborted` in `tensorflow.keras.remat`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tensorflow.keras.remat in tfnightly 2.20.0dev20250302. This bug can be used to trigger a denial of service attack.  Standalone code to reproduce the issue ```shell from tensorflow.keras import layers, Model, remat import numpy as np class CustomRematLayer(layers.Layer):     def __init__(self, **kwargs):         super().__init__(**kwargs)         self.remat_function = remat(self.intermediate_function)     def intermediate_function(self, x):         return x * 1.0      def call(self, inputs):         return self.remat_function(inputs) inputs = layers.Input(shape=(0,))  x = CustomRematLayer()(inputs)  outputs = layers.Dense(1)(x) model = Model(inputs=inputs, outputs=outputs) model.compile(optimizer=""sgd"", loss=""mse"") model.predict(np.random.randn(32, 0)) ```  Relevant log output ```shell terminate called after throwing an instance of 'dnnl::error'   what():  could not create a primitive descriptor for an inner product forward propagation primitive Aborted ```",2025-03-24T08:21:36Z,stat:awaiting response type:bug stale comp:keras TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89851,"Hi  , Please post this issue on kerasteam/keras repo. as this issue is more related to keras Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T08:18:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89850
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24060 from philipphack:u_conv_circular_xla f247a7122b7b9c842142650789905490d388ef4e,2025-03-24T08:18:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89849
default1360,`Aborted` in `tf.nn.max_pool2d`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with tf.nn.max_pool2d in tfnightly 2.20.0dev20250302. This bug can be used to trigger a denial of service attack.  Standalone code to reproduce the issue ```shell import tensorflow as tf t = tf.random.normal((8, 1, 1, 2), dtype=tf.float32)   Shape (batch, height, width, channels) result = tf.nn.max_pool2d(t,         ksize=(9223372036854775807, 5868783964474102731),         strides=(1, 100),         padding='VALID') ```  Relevant log output ```shell 20250324 16:15:37.041691: F tensorflow/core/common_runtime/mkl_layout_pass.cc:1597] NonOKstatus: GetNodeAttr(n>def(), ""ksize"", &ksize) Status: INVALID_ARGUMENT: Attr ksize has value 9223372036854775807 out of range for an int32 Aborted ```",2025-03-24T08:17:12Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,6,https://github.com/tensorflow/tensorflow/issues/89848,"Hi  , Welcome to TensorFlow, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. I did not encounter any crashes; instead, I received the following error message: ``` InvalidArgumentError: {{function_node __wrapped__MaxPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} Attr ksize has value 9223372036854775807 out of range for an int32 [Op:MaxPool] name:  ``` Based on this error, I made some modifications, and the code is now working fine for me. I have attached a gist for your reference. Thank you!","It is weird. I found that this bug only occurs in `2.20.0dev20250302`. It doesn't appear in `2.20.0dev20250324`, `2.18`, or `2.19`. I will continue investigating. Thanks!","Hi  , Apologies for the delay. I noticed that you are using the nightly version of TensorFlow. Please note that nightly versions are experimental and frequently updated, which may lead to unexpected issues. I recommend using stable, officially released versions for more consistent and reliable results. Here is the official release documentation for your reference. Let me know if you need any further assistance! Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T08:16:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89847
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T08:15:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89846
default1360,`Aborted` in `tensorflow.compat.v2.raw_ops.DepthwiseConv2dNativeBackpropInput`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an abortion issue with `tf.raw_ops.DepthwiseConv2dNativeBackpropInput` in tfnightly 2.20.0dev20250302.  Standalone code to reproduce the issue ```shell import tensorflow as tf input_sizes = tf.constant([1, 5, 5, 1], dtype=tf.int32) filter_shape = tf.constant([3, 3, 1, 1], dtype=tf.float32)  out_backprop = tf.constant([[0.0]], dtype=tf.float32)   gradients = tf.raw_ops.DepthwiseConv2dNativeBackpropInput(     input_sizes=input_sizes,     filter=filter_shape,     out_backprop=out_backprop,     strides=[1, 1],     padding='VALID' ) ```  Relevant log output ```shell 20250324 16:02:20.356862: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C Aborted ```",2025-03-24T08:07:29Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,6,https://github.com/tensorflow/tensorflow/issues/89845,"Hi  , Welcome to TensorFlow, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. I did not encounter any crashes; instead, I received the following error message: ``` InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNativeBackpropInput_device_/job:localhost/replica:0/task:0/device:CPU:0}} Sliding window strides field must specify 4 dimensions [Op:DepthwiseConv2dNativeBackpropInput] name:  ``` Based on this error, I made some modifications, and the code is now working fine for me. I have attached a gist for your reference. Thank you!","It is weird. I found that this bug only occurs in `2.20.0dev20250302`. It doesn't appear in `2.20.0dev20250324`, `2.18`, or `2.19`. I will continue investigating. Thanks!","Hi  , Apologies for the delay. I noticed that you are using the nightly version of TensorFlow. Please note that nightly versions are experimental and frequently updated, which may lead to unexpected issues. I recommend using stable, officially released versions for more consistent and reliable results. Here is the official release documentation for your reference. Let me know if you need any further assistance! Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T07:59:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89844
shweta714,I am spamming,,2025-03-24T07:56:33Z,size:S invalid,closed,0,1,https://github.com/tensorflow/tensorflow/issues/89843,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T07:33:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89842
copybara-service[bot],Fix HloInstructionAdaptor::GetUsers() implementation,"Fix HloInstructionAdaptor::GetUsers() implementation The intent was that no user outside of the HloFusionAdaptor parent should be returned. But one check for whether a instruction is contained in the fusion adaptor used the wrong variable ""value"". This parameter of the ResolveUsers() method was meant to be used to be able to retrieve the operand index from which the user was reached. Change it to pass this operand index instead.",2025-03-24T07:32:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89841
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T07:30:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89840
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T07:26:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89839
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T07:20:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89838
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T07:08:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89837
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T04:50:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89836
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T04:47:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89835
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T04:41:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89834
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T04:39:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89833
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T04:38:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89832
copybara-service[bot],This is an internal change,This is an internal change,2025-03-24T03:55:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89831
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T03:24:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89830
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T03:03:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89829
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T02:54:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89828
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-24T02:43:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89827
shweta714,I am spamming,,2025-03-24T01:11:37Z,size:XS invalid,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89826,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , Can you please sign the CLA, Thank You!",This is spam 
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T07:48:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89825
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T06:07:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89824
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T05:56:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89823
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T05:44:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89822
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T05:11:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89821
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T05:10:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89820
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T05:09:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89819
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T05:09:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89818
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T05:08:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89817
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T05:07:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89816
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T05:05:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89815
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T05:02:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89814
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T04:59:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89813
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T04:54:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89812
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T04:50:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89811
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T04:50:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89810
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T04:46:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89809
anushikhov,Move duplicate CUDA/XLA registration logs from INFO to VLOG,"This PR downgrades harmless duplicate cuDNN, cuBLAS, and cuFFT factory registration logs from INFO to VLOG(1), fully silencing them during normal usage. Upstream already reduced these from ERROR to INFO, but they still create unnecessary log noise when XLA and GPU backends initialize. Since the duplicate registration is safe and expected this change preserves visibility only for debugging sessions. Coinspired by ChatGPT during a deep dive into TensorFlow's logging system. Fixes: CC(cuDNN, cuFFT, and cuBLAS Errors)",2025-03-23T04:27:34Z,ready to pull comp:xla size:XS,open,0,1,https://github.com/tensorflow/tensorflow/issues/89808,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Add backend_kwargs to fuzz test build def.,Add backend_kwargs to fuzz test build def.,2025-03-23T04:19:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89807
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T03:03:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89806
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-23T02:59:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89805
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T20:38:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89804
copybara-service[bot],Enable absl hardening for XLA unoptimized TAPs.,Enable absl hardening for XLA unoptimized TAPs.,2025-03-22T18:46:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89803
copybara-service[bot],[xla:pjrt] Move cpu_event.h to pjrt/cpu as it used only by PjRt implementations,[xla:pjrt] Move cpu_event.h to pjrt/cpu as it used only by PjRt implementations,2025-03-22T18:36:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89802
copybara-service[bot],[xla:pjrt] Remove TFRT from CpuAsyncExecutionTracker,[xla:pjrt] Remove TFRT from CpuAsyncExecutionTracker XLA and PJRT have zero dependencies on TFRT. To avoid confusing everyone delete Tfrt from type names.,2025-03-22T18:35:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89801
mistersmee,"Replace deprecated ""pipes.quote"" with ""shlex.quote"" for ROCm","This is basically f6d09e2, but for ROCm. Quite a minor change, but one I ran across when trying to build the ROCm bits, because the build failed as it tried to import pipes.",2025-03-22T14:48:26Z,ready to pull comp:gpu size:XS,closed,0,1,https://github.com/tensorflow/tensorflow/issues/89799,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
nvdiacudabot,Default,,2025-03-22T13:59:33Z,size:L,closed,0,1,https://github.com/tensorflow/tensorflow/issues/89798,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
ymlau,colab tpu initialization," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I was unable to initialize tpu in colab, it seems that the system failed to detect the tpu.  Standalone code to reproduce the issue ```shell https://colab.research.google.com/drive/1mGQ7_EeCtFnrrtUW6mYhyFx8k06MCTz?usp=sharing ```  Relevant log output ```shell  InvalidArgumentError                      Traceback (most recent call last) /usr/local/lib/python3.11/distpackages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system_impl(cluster_resolver, tpu_cluster_resolver_cls)     138       with ops.device(tpu._tpu_system_device_name(job)):   pylint: disable=protectedaccess > 139         output = _tpu_init_fn()     140       context.async_wait() 4 frames /usr/local/lib/python3.11/distpackages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)     152       filtered_tb = _process_traceback_frames(e.__traceback__) > 153       raise e.with_traceback(filtered_tb) from None     154     finally: /usr/local/lib/python3.11/distpackages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)      58     raise core._status_to_exception(e) from None > 59   except TypeError as e:      60     keras_symbolic_tensors = [x for x in inputs if _is_keras_symbolic_tensor(x)] InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [tpu_embedding_config="""", is_global_init=false, compilation_failure_closes_chips=false, embedding_config="""", enable_whole_mesh_compilations=false, tpu_cancellation_closes_chips=2] Registered devices: [CPU] Registered kernels:    	 [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4] During handling of the above exception, another exception occurred: NotFoundError                             Traceback (most recent call last)  in ()       3 tf.config.experimental_connect_to_cluster(resolver)       4  This is the TPU initialization code that has to be at the beginning. > 5 tf.tpu.experimental.initialize_tpu_system(resolver)       6 print(""All devices: "", tf.config.list_logical_devices('TPU')) /usr/local/lib/python3.11/distpackages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py in initialize_tpu_system(cluster_resolver)      70     NotFoundError: If no TPU devices found in eager mode.      71   """""" > 72   return tpu_strategy_util.initialize_tpu_system_impl(      73       cluster_resolver, TPUClusterResolver)      74  /usr/local/lib/python3.11/distpackages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system_impl(cluster_resolver, tpu_cluster_resolver_cls)     140       context.async_wait()     141     except errors.InvalidArgumentError as e: > 142       raise errors.NotFoundError(     143           None, None,     144           ""TPUs not found in the cluster. Failed in initialization: "" NotFoundError: TPUs not found in the cluster. Failed in initialization: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [tpu_embedding_config="""", is_global_init=false, compilation_failure_closes_chips=false, embedding_config="""", enable_whole_mesh_compilations=false, tpu_cancellation_closes_chips=2] Registered devices: [CPU] Registered kernels:    	 [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4] ```",2025-03-22T13:33:30Z,type:bug comp:tpus TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/89797
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T10:16:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89796
tortoise-ben,fastspeech2 中文中训练," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version v2.9.0rc242g8a20d54a3c1 2.9.0  Custom code Yes  OS platform and distribution Ubuntu 22.04.5 LTS  Mobile device Ubuntu 22.04.5 LTS  Python version Python 3.7.12  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0  CUDA/cuDNN version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052021 NVIDIA Corporation Built on Thu_Nov_18_09:45:30_PST_2021 Cuda compilation tools, release 11.5, V11.5.119 Build cuda_11.5.r11.5/compiler.30672275_0  GPU model and memory  NVIDIASMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2  NVIDIA GeForce RTX 4090  Current behavior? 我通过TensorFlowTTS项目，训练了一个自己的音色模型，但是合成的音频声音很奇怪。以下是我用同一段文字在自己训练的模型和huggingface下载的模型【tensorspeech/ttsfastspeech2bakerch】合成的语音的对比。melgan用的是同一个，是从huggingface下载的【tensorspeech/ttsmb_melganbakerch】。难道melgan也要自己重新训练吗？ !Image 用我生成的模型合成出来的音频能微弱的听到声音，但是很小声。不知道是什么问题导致的。  Standalone code to reproduce the issue ```shell import soundfile as sf import numpy as np import tensorflow as tf import sys sys.path.append(""."") from tensorflow_tts.inference import AutoConfig from tensorflow_tts.inference import AutoProcessor from tensorflow_tts.inference import TFAutoModel mel_cfg = AutoConfig.from_pretrained(""/data/benben/TensorFlowTTSmaster/my_test/mel.yml"") mb_melgan = TFAutoModel.from_pretrained(""/data/benben/TensorFlowTTSmaster/my_test/mel.h5"", config=mel_cfg) config1 = AutoConfig.from_pretrained(""examples/fastspeech2/conf/fastspeech2.baker.v2.yaml"")  fastspeech2 = TFAutoModel.from_pretrained(""dump_mandarinfm/exp/train.fastspeech2.baker.v2/checkpoints/model10000.h5"", config=config1)  processor = AutoProcessor.from_pretrained(""dump_mandarinfm/baker_mapper.json"") fastspeech2 = TFAutoModel.from_pretrained(""my_test/model.h5"", config=config1) processor = AutoProcessor.from_pretrained(""my_test/processor.json"") text = ""通用在去年第三季度每天售出逾二点七万辆新车，业界关注的是转型创业企业生态圈运营商之后"" text = ""通用在去年第三季度每天售出逾二点七万辆新车"" input_ids = processor.text_to_sequence(text, inference=True) print(""***********"",input_ids) mel_before, mel_after, duration_outputs, _, _ = fastspeech2.inference(     input_ids=tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0),     speaker_ids=tf.convert_to_tensor([0], dtype=tf.int32),     speed_ratios=tf.convert_to_tensor([1.0], dtype=tf.float32),     f0_ratios =tf.convert_to_tensor([1.0], dtype=tf.float32),     energy_ratios =tf.convert_to_tensor([1.0], dtype=tf.float32), ) print(mel_before) print(mel_after)  melgan inference (meltowav) audio = mb_melgan.inference(mel_after)[0, :, 0]  save to file sf.write('./audio.wav', audio, 24000, ""PCM_16"")  sf.write('./audio.wav', audio, 24000) ```  Relevant log output ```shell ```",2025-03-22T09:15:42Z,type:performance TF 2.9,open,0,0,https://github.com/tensorflow/tensorflow/issues/89795
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:55:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89794
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:41:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89793
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:41:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89792
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:36:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89791
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:32:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89790
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:29:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89789
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:29:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89788
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:28:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89787
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:27:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89786
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:21:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89785
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:20:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89784
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:18:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89783
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:15:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89782
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:14:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89781
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T08:08:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89780
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24060 from philipphack:u_conv_circular_xla f247a7122b7b9c842142650789905490d388ef4e,2025-03-22T08:06:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89779
copybara-service[bot],Updating `PjRtClient::Compile()` to return an unloaded executable.,Updating `PjRtClient::Compile()` to return an unloaded executable.,2025-03-22T07:54:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89778
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T04:42:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89777
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-22T04:00:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89776
copybara-service[bot],Implement TFRTGpuClient::CreateViewOfDeviceBuffer in pjrt gpu client,Implement TFRTGpuClient::CreateViewOfDeviceBuffer in pjrt gpu client,2025-03-22T03:20:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89775
copybara-service[bot],[xla:pjrt] Removed Tfrt from TrackedTfrtCpuDeviceBuffer + cleanup API,[xla:pjrt] Removed Tfrt from TrackedTfrtCpuDeviceBuffer + cleanup API XLA and PJRT have zero dependencies on TFRT. To avoid confusing everyone delete Tfrt from type names. Also rename methods to access CPU device memory to be consistent with xla::Literal and few other types widely used in XLA.,2025-03-22T01:59:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89774
copybara-service[bot],Move tensorflow/core/profiler/utils into tensorflow/profiler.,Move tensorflow/core/profiler/utils into tensorflow/profiler.,2025-03-22T01:04:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89773
copybara-service[bot],Add a higher level macro thats tied to the gen_device_test. This will handle the installation of all the .so and resolve the flags and whatnot. ,Add a higher level macro thats tied to the gen_device_test. This will handle the installation of all the .so and resolve the flags and whatnot.  Add feature to skip some tests by substr match in the target def.,2025-03-21T23:13:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89772
copybara-service[bot],Plumb named quant recipe in AOT flow.,Plumb named quant recipe in AOT flow.,2025-03-21T23:13:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89771
copybara-service[bot],Move `tensorflow/tsl/platform/criticality` to `xla/tsl`,Move `tensorflow/tsl/platform/criticality` to `xla/tsl`,2025-03-21T23:01:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89770
copybara-service[bot],Reverts f16cc50c5630f1021c9b7f3c0a923a5009fe7486,Reverts f16cc50c5630f1021c9b7f3c0a923a5009fe7486,2025-03-21T22:50:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89769
copybara-service[bot],Updates `BatchedDevicePut` to utilize the new variant of `MakeArrayFromHostBuffer` that explicitly accepts a `user_context` argument.,Updates `BatchedDevicePut` to utilize the new variant of `MakeArrayFromHostBuffer` that explicitly accepts a `user_context` argument. This enables us to reuse the same `UserContext` for all the (potentially huge number of) `MakeArrayFromHostBuffer` calls made by `BatchedDevicePut`.This achievesfor those backends that internally capture a UserContexta substantial performance improvement since it eliminates the overheads incurred by the current implementation when it repeatedly acquires and releases the GIL to make a separate `UserContext` object for each of these `MakeArrayFromHostBuffer` calls.,2025-03-21T22:00:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89768
copybara-service[bot],"move tensorflow/compiler/mlir/lite/tools/tf_mlir_translate_cl.{h,cc} to tensorflow/compiler/mlir/tools/.","move tensorflow/compiler/mlir/lite/tools/tf_mlir_translate_cl.{h,cc} to tensorflow/compiler/mlir/tools/.",2025-03-21T20:54:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89767
copybara-service[bot],[XLA:Python] Widen visibilities.,[XLA:Python] Widen visibilities. Change in preparation for moving more of XLA/Python into JAX.,2025-03-21T20:47:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89766
copybara-service[bot],Reverts 533bbd47f6af0bd6dd026b896dab5852dd143bab,Reverts 533bbd47f6af0bd6dd026b896dab5852dd143bab,2025-03-21T20:46:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89765
copybara-service[bot],[JAX] [XLA:Python] Migrate xla_extension and its type stubs into jaxlib.,[JAX] [XLA:Python] Migrate xla_extension and its type stubs into jaxlib. Future changes will migrate many of its dependent modules.,2025-03-21T20:31:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89764
copybara-service[bot],Remove GitHub Actions for building tensorflow sigs Dockerfile since we have migrated to the new ML build container.,Remove GitHub Actions for building tensorflow sigs Dockerfile since we have migrated to the new ML build container. The new ML Build Container Dockerfile can be found under https://github.com/tensorflow/tensorflow/tree/master/ci/official/containers/ml_build,2025-03-21T20:23:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89763
copybara-service[bot],ifrt-proxy nit: remove TODO for `array_is_deleted_hack` flag.,"ifrtproxy nit: remove TODO for `array_is_deleted_hack` flag. While a recent commit allowed `IsDeleted()` to be determined at the IFRTproxy client without contacting the server, this may not be always possible; when some IFRT backends are running behind the IFRTproxy server, some `IsDeleted()`calls may need to be proxied through from the client to the server.",2025-03-21T20:17:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89762
copybara-service[bot],Update pretty_name for benchmark_presubmit for consistency,Update pretty_name for benchmark_presubmit for consistency,2025-03-21T20:03:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89761
copybara-service[bot],Implement `TfrtGpuBuffer::CopyRawToHostFuture` and `TfrtGpuClient::BufferFromHostLiteral`,Implement `TfrtGpuBuffer::CopyRawToHostFuture` and `TfrtGpuClient::BufferFromHostLiteral`,2025-03-21T19:39:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89760
copybara-service[bot],Reverts 62098d7dfc801b45e3c08c31c5cc3bd159c76998,Reverts 62098d7dfc801b45e3c08c31c5cc3bd159c76998,2025-03-21T19:39:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89759
copybara-service[bot],[XLA:CPU] Fix use-after-move in CpuCompiler::CompileAheadOfTime,[XLA:CPU] Fix useaftermove in CpuCompiler::CompileAheadOfTime,2025-03-21T19:25:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89758
copybara-service[bot],Disable by default failing on the profile starting issues.,Disable by default failing on the profile starting issues.,2025-03-21T19:21:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89757
copybara-service[bot],second test for build only presubmit,second test for build only presubmit,2025-03-21T18:41:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89756
copybara-service[bot],[xla:ffi] Use std::lower_bound to efficiently find attribute index,[xla:ffi] Use std::lower_bound to efficiently find attribute index,2025-03-21T18:14:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89755
copybara-service[bot],Improve readability. Capture expensive vlog printing.,Improve readability. Capture expensive vlog printing.,2025-03-21T18:06:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89754
copybara-service[bot],[xla] Add buffer_id to ShapeProto and the C++ Shape class.,"[xla] Add buffer_id to ShapeProto and the C++ Shape class. This field can be used in an HLO program input to the compiler to group a chain of HLO values sharing the same memory buffer. We intend to have a new HLO pass to convert HLO values with valid buffer_ids to XLA attributes that can be used by copy_insertion, so that all other existing HLO passes won't be affected by this new field. Extend hlo_parser to pass buffer_id.",2025-03-21T17:35:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89753
copybara-service[bot],Rename `TrueRank()` to `TrueNumDimensions()` to avoid confusion.,"Rename `TrueRank()` to `TrueNumDimensions()` to avoid confusion. We've deprecated the use of ""rank"" to mean the number of dimensions in XLA.",2025-03-21T17:32:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89752
copybara-service[bot],Add relu6 support for qnn,Add relu6 support for qnn,2025-03-21T17:28:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89751
copybara-service[bot],"Allow setting multiple compilation configs, and add partition stats to compiled model.","Allow setting multiple compilation configs, and add partition stats to compiled model.",2025-03-21T17:08:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89750
copybara-service[bot],[xla] Autotuner: Make ExecutableConfig a virtual base class to give executable providers more implementation freedom,[xla] Autotuner: Make ExecutableConfig a virtual base class to give executable providers more implementation freedom As an example XLA:GPU backends can use existing protobuf configs to select codegen options for the autotuner,2025-03-21T17:05:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89749
copybara-service[bot],[XLA:Python] Open visibilities of a number of modules to JAX.,[XLA:Python] Open visibilities of a number of modules to JAX. Change in preparation for migrating pieces of XLA:Python into the JAX repository.,2025-03-21T16:52:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89748
copybara-service[bot],PR #23838: [ds-fusion] Remove workspace checks in dynamic slice fusion,"PR CC(Updated PR for ""Fold BN after depthwise conv""(21023)): [dsfusion] Remove workspace checks in dynamic slice fusion Imported from GitHub PR https://github.com/openxla/xla/pull/23838 The contents of the workspace are not guarenteed to be the same after a cublas gemm operation. We were comparing the contents of the workspace. This patch removes such checks and only looks for the value of the results. Copybara import of the project:  a8d18715ff7cb651409ddb22164f8974da2b2b1e by Shraiysh Vaishay : [dsfusion] Remove workspace checks The contents of the workspace are not guarenteed to be the same after a cublas gemm operation. We were comparing the contents of the workspace. This patch removes such checks and only looks for the value of the results.  c08843019b71307f04a265ac813af2beb9bd78b5 by Shraiysh Vaishay : Addressed comments Merging this change closes CC(Updated PR for ""Fold BN after depthwise conv""(21023)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23838 from shraiysh:remove_workspace_tests_cublass c08843019b71307f04a265ac813af2beb9bd78b5",2025-03-21T16:49:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89747
copybara-service[bot],#litert Refactor the NPU accelerator by using the accelerator implementation helper.,litert Refactor the NPU accelerator by using the accelerator implementation helper.,2025-03-21T16:34:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89746
copybara-service[bot],Remove unused out `peak_memory` parameter for `MemorySchedulerAlgorithm`.,Remove unused out `peak_memory` parameter for `MemorySchedulerAlgorithm`.,2025-03-21T16:25:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89745
copybara-service[bot],Fix comment on XLA conv cost-analysis test.,Fix comment on XLA conv costanalysis test.,2025-03-21T16:21:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89744
copybara-service[bot],#litert Add a CPU accelerator that uses XNNPack.,litert Add a CPU accelerator that uses XNNPack. This also adds a helper that reduces the boilerplate needed to create accelerators. Setting up the autoregistration of this accelerator will be done in a following change.,2025-03-21T15:57:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89743
copybara-service[bot],[XLA:GPU][Emitters] Always inline subgraphs with no compute.,[XLA:GPU][Emitters] Always inline subgraphs with no compute.,2025-03-21T15:21:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89742
copybara-service[bot],[XLA:GPU] Add a pattern to fold vector.insert(vector.extract).,"[XLA:GPU] Add a pattern to fold vector.insert(vector.extract). When there is nothing fused in the transpose, we read the input and then write it to shmem. We can fold away the packing/unpacking of the vectors. ``` %0 = vector.transfer_read scf.for %arg2 = %c0 to %c2 step %c1 iter_args(%arg3 = %cst) > (vector) {          405:  %3 = vector.extract %0[%arg2] : f32 from vector          406:  %4 = vector.insert %3, %arg3 [%arg2] : f32 into vector          407:  scf.yield %4 : vector          408:  }  ``` vector.transfer_write",2025-03-21T15:07:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89741
copybara-service[bot],[XLA:CPU] Add FusionCompiler,[XLA:CPU] Add FusionCompiler,2025-03-21T14:49:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89740
copybara-service[bot],[XLA:GPU] support checks for dot,[XLA:GPU] support checks for dot,2025-03-21T14:46:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89739
copybara-service[bot],"[XLA:Collective] Get participant ids, count and group sizes from CollectiveOpGroupMode.","[XLA:Collective] Get participant ids, count and group sizes from CollectiveOpGroupMode. It is not very obvious from the code or comments, but we can use `GetReplicaGroupCountAndSize` and `GetParticipatingFlattenedIdGroups` only for `AllReduce` and `AllGather`, because of the check in  `GetCollectiveUseGlobalDeviceIds`. There is no reason why `CollectiveOpGroupMode` of other instruction would not be enough to get the information. The check of `AllReduce` and `AllGather` was added in https://github.com/openxla/xla/pull/20024, but I couldn't find explanation why it wouldn't work of other instruction.",2025-03-21T14:14:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89738
copybara-service[bot],[XLA:CPU] Add AOT compilation for microbenchmarks,[XLA:CPU] Add AOT compilation for microbenchmarks FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23347 from gspschmid:gschmid/xlaoverridebootid 8d167908028f75c92e635abe65beff9206cf25ea,2025-03-21T14:11:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89737
copybara-service[bot],[XLA:GPU] Add a microbenchmark for scatter from b/404266433.,[XLA:GPU] Add a microbenchmark for scatter from b/404266433.,2025-03-21T13:57:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89736
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent. Reverts 33d0787dee6063d66400faed7633b79ae60bcf8f,2025-03-21T13:53:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89735
copybara-service[bot],[XLA:CPU] Match data layout to avoid LLVM warnings in ir_emitter2,[XLA:CPU] Match data layout to avoid LLVM warnings in ir_emitter2,2025-03-21T13:44:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89734
copybara-service[bot],PR #24023: Remove HloComputation::CustomCallInstruction.,"PR CC(ValueError: tf.enable_eager_execution must be called at program startup.): Remove HloComputation::CustomCallInstruction. Imported from GitHub PR https://github.com/openxla/xla/pull/24023 Step 2/5 of removing instruction_type. I think kFusion should be last, since it's special (in that it's ""sticky"", you can't tell from instruction_callers whether it's a fusion computation). Copybara import of the project:  90421de53c0dd0258e2bb10e236740a7ef6270e8 by Johannes Reifferscheid : Remove HloComputation::CustomCallInstruction. Step 2/5 in removing instruction_type. I think kFusion should be last, since it's special (in that it's ""sticky"", you can't tell from instruction_callers whether it's a fusion computation). Merging this change closes CC(ValueError: tf.enable_eager_execution must be called at program startup.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24023 from jreiffers:customcall 90421de53c0dd0258e2bb10e236740a7ef6270e8",2025-03-21T12:33:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89733
copybara-service[bot],PR #24026: Remove call graph from copy fusion logic.,"PR CC(Abs operator not implemented?): Remove call graph from copy fusion logic. Imported from GitHub PR https://github.com/openxla/xla/pull/24026 Now that we have a call graph in HloComputation, we no longer need this. Copybara import of the project:  167541256865b05a9239bc37f9ff10c2f8da446a by Johannes Reifferscheid : Remove call graph from copy fusion logic. Now that we have a call graph in HloComputation, we no longer need this. Merging this change closes CC(Abs operator not implemented?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24026 from jreiffers:nocallgraph 167541256865b05a9239bc37f9ff10c2f8da446a",2025-03-21T12:09:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89732
copybara-service[bot],* Add MTK plugin lib path as a default path to apply_plugin_main. And add the `.so` to the BUILD,* Add MTK plugin lib path as a default path to apply_plugin_main. And add the `.so` to the BUILD * set the environment variables required to enable AOT compilation for MTK Compiler Plugin in `compiler_plugin.cc` * reenable some mediatek compiler plugin test for linux. But op specifc test cases are disabled because Mediatek doesn't support FP32 activations used by test tflite files.,2025-03-21T12:01:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89731
copybara-service[bot],Integrate Triton up to [cdb5326](https://github.com/openai/triton/commits/cdb53266e6c251d91a2c321d64e8466caff129a9).,Integrate Triton up to cdb5326.,2025-03-21T11:46:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89730
copybara-service[bot],PR #24023: Remove HloComputation::CustomCallInstruction.,"PR CC(ValueError: tf.enable_eager_execution must be called at program startup.): Remove HloComputation::CustomCallInstruction. Imported from GitHub PR https://github.com/openxla/xla/pull/24023 Step 2/5 of removing instruction_type. I think kFusion should be last, since it's special (in that it's ""sticky"", you can't tell from instruction_callers whether it's a fusion computation). Copybara import of the project:  90421de53c0dd0258e2bb10e236740a7ef6270e8 by Johannes Reifferscheid : Remove HloComputation::CustomCallInstruction. Step 2/5 in removing instruction_type. I think kFusion should be last, since it's special (in that it's ""sticky"", you can't tell from instruction_callers whether it's a fusion computation). Merging this change closes CC(ValueError: tf.enable_eager_execution must be called at program startup.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24023 from jreiffers:customcall 90421de53c0dd0258e2bb10e236740a7ef6270e8",2025-03-21T11:45:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89729
copybara-service[bot],PR #88103: Supports more legalizations and multi-partition,PR CC(Supports more legalizations and multipartition): Supports more legalizations and multipartition Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/88103 1. more legalizations    a. Batch_MatMul    b. Mul    c. Fully_Connected 3. multipartition supports     introduce utils for supporting multipartition Copybara import of the project:  55e544dad08a2d492dff3f1f8dd05b8a00793820 by neuropilotcaptain : Supports more legalizations and multipartition 1. more legalizations:   a. Batch_MatMul   b. Mul   c. Fully_Connected 2. multipartition supports:   introduce utils for supporting multipartition  c33a232d35f2e510ced7be4f29f4ff17653bbc1d by neuropilotcaptain : Support more legalizations for MTK platform  Added new legalizatios for more operations.  Introduced common utility functions for neuron.  2ac43ec3ba040e25a80ac3b9df6b00ebb69b389b by neuropilotcaptain : Fix syntax error of the BUILD Merging this change closes CC(Supports more legalizations and multipartition) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88103 from neuropilotcaptain:upstream_0225 2ac43ec3ba040e25a80ac3b9df6b00ebb69b389b,2025-03-21T10:55:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89728
weilhuan-quic,Qualcomm AI Engine Direct - More Op Builders, WHAT 1. FloorDiv 2. NotEqual 3. Logistic 4. Pad PadV2 5. MaxPool2d 6. Cumsum 7. GatherNd 8. Pow 9. TransposeConv  TEST qnn_compiler_plugin_test ``` [] Global test environment teardown [==========] 137 tests from 5 test suites ran. (3593 ms total) [  PASSED  ] 137 tests. ``` litert_options_test ``` [] Global test environment teardown [==========] 26 tests from 1 test suite ran. (0 ms total) [  PASSED  ] 26 tests. ```,2025-03-21T09:32:50Z,cla: yes size:L,closed,0,1,https://github.com/tensorflow/tensorflow/issues/89727,I will pseudo approve this PR for testing purpose only.
copybara-service[bot],PR #23056: Remove std::move for trivially copyable types,"PR CC(Losses collection is not thread local so it can't be used inside model_fn call when using MirroredStrategy): Remove std::move for trivially copyable types Imported from GitHub PR https://github.com/openxla/xla/pull/23056 Changes:  Removed unnecessary std::move calls for some trivially_copyable classes  Literal::CopyFrom expects a reference (&), not an rvalue reference (&&). Removed std::move on the first parameter. Copybara import of the project:  6051f15383d63210c313ab712eaa3a00c7b0105f by Alexander Pivovarov : Remove std::move for trivially copyable types Merging this change closes CC(Losses collection is not thread local so it can't be used inside model_fn call when using MirroredStrategy) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23056 from apivovarov:no_move_for_KernelMetadata 6051f15383d63210c313ab712eaa3a00c7b0105f",2025-03-21T09:12:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89726
chunhsue,Qualcomm AI Engine Direct - Remove Unused Variable, Purpose Remove Unused Variable in QC vendor folder.  Changes  tensorflow/lite/experimental/litert/vendors/qualcomm/compiler/qnn_compiler_plugin.cc  tensorflow/lite/experimental/litert/vendors/qualcomm/core/builders/split_op_builder.cc  Tests `qnn_compiler_plugin_test` ``` [] Global test environment teardown [==========] 115 tests from 5 test suites ran. (4456 ms total) [  PASSED  ] 115 tests. ```,2025-03-21T09:06:36Z,ready to pull size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89725
copybara-service[bot],Reverts b0b4ec040b9b8f4b7fefdb5d7c3695349dae1d9d,Reverts b0b4ec040b9b8f4b7fefdb5d7c3695349dae1d9d,2025-03-21T09:06:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89724
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23793 from Tixxx:tixxx/cp_sync 3a8a64369ceb4059b6eb8a455dedbe1c02508088,2025-03-21T08:41:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89723
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T08:27:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89722
copybara-service[bot],[xla:cpu] Add new pthreadpool API to XNN thread pool integration,"[xla:cpu] Add new pthreadpool API to XNN thread pool integration This is to match the recently added API in pthreadpool.h from https://github.com/google/pthreadpool/commit/bd09d5ca9155863fc47a9cbf0df1908ef9d8cad2 Otherwise, we would get a linker error when building xnn_threadpool_test:  ``` ld: error: undefined symbol: pthreadpool_parallelize_4d_tile_2d_dynamic ``` Command to test: ``` bazel build c opt define=pthreadpool_header_only=true \   //xla/backends/cpu/runtime/xnnpack:xnn_threadpool_test ```",2025-03-21T06:58:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89721
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T06:54:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89720
copybara-service[bot],Add new constants.,Add new constants.,2025-03-21T06:09:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89719
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T05:49:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89718
Arshpreet98,Failed to load the native TensorFlow runtime.," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tensorflowcpu2.19.0  Custom code Yes  OS platform and distribution Windows 10  Mobile device _No response_  Python version python 3.12.1  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Traceback (most recent call last):   File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""d:\training\python\arun_sir\assignment\data science project\mask_detection\src\train_mask_detector.py"", line 1, in      import tensorflow as tf   File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.  Standalone code to reproduce the issue ```shell python c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))"" ```  Relevant log output ```shell Traceback (most recent call last):   File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File """", line 1, in    File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""D:\training\python\arun_sir\assignment\data science project\mask_detection\venv\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```",2025-03-21T05:46:11Z,stat:awaiting response type:build/install subtype:windows TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89717,"Hi  , Apologies for the delay, could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Please search for duplicate issues before opening new ones.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T05:43:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89716
copybara-service[bot],Create better infrastructure for BatchMatMul optimizations. And add a new optimization pattern.,"Create better infrastructure for BatchMatMul optimizations. And add a new optimization pattern. Add infrastructure and utilities so its easy and intuitive to create and maintain BatchMatMul optimizations. The optimization pattern introduced in this CL will eliminate a transpose from Transpose>reshape>bmm_rhs if the transpose is used exclusively to transpose the unflattened contracting and output dims.  Consider the following transpose>reshape>bmm pattern. ``` %37 = ""tfl.transpose""(%28, %36) : (tensor, tensor) > tensor %38 = ""tfl.pseudo_const""()  : tensor}> : () > tensor %39 = ""tfl.reshape""(%37, %38) : (tensor, tensor) > tensor %40 = ""tfl.pseudo_const""()  : tensor}> : () > tensor %41 = ""tfl.batch_matmul""(%40, %39)  : (tensor, tensor) > tensor ``` This can be rewritten to use fully_connected instead of bmm, and we can avaoid the transpose ``` %39 = ""tfl.reshape""(%37, %abc) : (tensor, tensor) > tensor %40 = ""tfl.pseudo_const""()  : tensor}> : () > tensor %41 = ""tfl.fully_connected""(%40, %39) (tensor, tensor) > tensor ```",2025-03-21T05:29:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89715
copybara-service[bot],pjrt_ifrt: Fix failure caused by recent commit,pjrt_ifrt: Fix failure caused by recent commit,2025-03-21T05:19:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89714
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T04:58:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89713
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T04:56:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89712
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T04:54:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89711
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T04:53:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89710
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T04:51:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89709
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T04:49:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89708
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T04:46:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89707
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T04:46:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89706
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T04:34:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89705
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T04:29:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89704
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-21T04:26:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89703
copybara-service[bot],Reverts 6e913710dafd65dbb979e0f3f49ada2d6cc59a20,Reverts 6e913710dafd65dbb979e0f3f49ada2d6cc59a20,2025-03-21T04:15:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89702
copybara-service[bot],Integrate LLVM at llvm/llvm-project@dd3addf954ac,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match dd3addf954ac,2025-03-21T03:53:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89701
copybara-service[bot],"Conceptually, a shape is a union of several cases: an invalid shape, a token, an opaque, an array, or a tuple. These cases are mutually exclusive (a shape must be exactly one of these cases.)","Conceptually, a shape is a union of several cases: an invalid shape, a token, an opaque, an array, or a tuple. These cases are mutually exclusive (a shape must be exactly one of these cases.) Currently, the exclusivity of the shape cases is not enforced, allowing bugs where a shape is used as the wrong case (e.g. accessing the dimension fields of a tuple shape). This change makes `Shape` safer by representing its state as an `std::variant`. By construction, this guarantees that it can hold the state for just one case, and trying to access the state for another case would result in a crash. Given the large number of existing bugs uncovered by the intended change, this change is deliberately conservative: in some cases where the caller violates the precondition of a `Shape` method, we choose to return a default value instead of crashing (e.g. calling `dimensions()` on a nonarray shape would return an empty span even though it's a programmer error). We will tighten the enforcement of the preconditions in future CLs.",2025-03-21T02:57:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89700
copybara-service[bot],[NFC] Replace uses of `PjRtClient::DeserializeExecutable()` with `PjRtClient::LoadSerializedExecutable()`.,[NFC] Replace uses of `PjRtClient::DeserializeExecutable()` with `PjRtClient::LoadSerializedExecutable()`. This is to prepare for updating PjRtClient::DeserializeExecutable() to return an unloaded executable.,2025-03-21T02:43:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89699
copybara-service[bot],Bumping up libtpu version to pick correct versioned nightlies,Bumping up libtpu version to pick correct versioned nightlies,2025-03-21T02:36:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89698
copybara-service[bot],Add more op builders.,Add more op builders.,2025-03-21T02:13:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89697
copybara-service[bot],Remove superfluous semicolon from litert::SourceLocation declaration,Remove superfluous semicolon from litert::SourceLocation declaration,2025-03-21T02:07:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89696
copybara-service[bot],Improve readability. Add more comments and tests.,Improve readability. Add more comments and tests.,2025-03-21T00:55:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89695
copybara-service[bot],"copy tensorflow/compiler/mlir/lite/tools/tf_mlir_translate_cl.{h,cc} to tensorflow/compiler/mlir/tools/.","copy tensorflow/compiler/mlir/lite/tools/tf_mlir_translate_cl.{h,cc} to tensorflow/compiler/mlir/tools/.",2025-03-21T00:39:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89694
copybara-service[bot],Implement class `TfrtGpuAsyncHostToDeviceTransferManager` and `TfrtGpuClient::CreateBuffersForAsyncHostToDevice()`,Implement class `TfrtGpuAsyncHostToDeviceTransferManager` and `TfrtGpuClient::CreateBuffersForAsyncHostToDevice()` Implement `TfrtGpuBuffer::ToLiteral()` Add more unit tests.,2025-03-21T00:20:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89693
copybara-service[bot],Add `ir` and `passmanager` modules to the py bindings built within the tfl converter.,Add `ir` and `passmanager` modules to the py bindings built within the tfl converter. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24456 from dimvar:cuda13support d152d725f2cbbe3bdd1df17a7edcc7da620ad703,2025-03-21T00:02:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89692
copybara-service[bot],Integrate LLVM at llvm/llvm-project@3959bbc1345b,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 3959bbc1345b,2025-03-20T23:52:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89691
copybara-service[bot],PR #24004: Remove unused cub headers.,PR CC(systemlibs: Unbundle protobuf): Remove unused cub headers. Imported from GitHub PR https://github.com/openxla/xla/pull/24004 Copybara import of the project:  0fd44aef3e904b4c0080fc15ecec14a51ac90752 by Dimitris Vardoulakis : Remove unused cub headers. Merging this change closes CC(systemlibs: Unbundle protobuf) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24004 from dimvar:removeunusedcubsortheaders 0fd44aef3e904b4c0080fc15ecec14a51ac90752,2025-03-20T23:50:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89690
copybara-service[bot],[XLA] Check array operator() access is within bounds.,[XLA] Check array operator() access is within bounds.,2025-03-20T23:40:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89689
copybara-service[bot],Import pybind11_abseil header directly in py_device.cc.,Import pybind11_abseil header directly in py_device.cc.,2025-03-20T23:33:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89688
copybara-service[bot],[xla:ffi] Add a test for explicit type id + user data,[xla:ffi] Add a test for explicit type id + user data,2025-03-20T23:17:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89687
copybara-service[bot],Add a higher level macro thats tied to the gen_device_test. This will handle the installation of all the .so and resolve the flags and whatnot. ,Add a higher level macro thats tied to the gen_device_test. This will handle the installation of all the .so and resolve the flags and whatnot.  Add feature to skip some tests by substr match in the target def.,2025-03-20T23:05:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89686
copybara-service[bot],Turn sans off python test because build becomes too large.,Turn sans off python test because build becomes too large.,2025-03-20T23:02:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89685
copybara-service[bot],"If the `dynamic_dimensions` parameter is empty in the `Shape` ctor, assume all dimensions are static.","If the `dynamic_dimensions` parameter is empty in the `Shape` ctor, assume all dimensions are static. Some callers call the `Shape(element_type, dimensions, dynamic_dimensions)` ctor with a nonempty `dimensions` and an empty `dynamic_dimensions`. This breaks the shape object's invariant that the two should have the same size. We have two options for fixing this: 1. Force the caller to always provide a `dynamic_dimensions` whose size matches that of `dimensions`. 2. Provide a sensible default behavior when `dynamic_dimensions` is empty. I chose CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"") as: 1. CC(Add support for Python 3.x) is more risky as it may cause the compiler to crash in production (e.g. if we don't have adequate test coverage). 2. It's very common for an array to have only static dimensions. Therefore it's good to optimize the user experience for this common case.",2025-03-20T22:51:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89684
copybara-service[bot],Add some utility functions in the TFRT GPU client.,Add some utility functions in the TFRT GPU client.,2025-03-20T22:09:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89683
copybara-service[bot],"Disable ""testTFLVersion"" for now to validate TFLite ios tests.","Disable ""testTFLVersion"" for now to validate TFLite ios tests.",2025-03-20T22:04:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89682
copybara-service[bot],Add `ExecuteSharded` and `ExecutePortable` methods in `TfrtGpuExecutable`,Add `ExecuteSharded` and `ExecutePortable` methods in `TfrtGpuExecutable`,2025-03-20T22:04:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89681
copybara-service[bot],Support expanding ragged all-to-all dims similar to all-to-alls is not needed.,Support expanding ragged alltoall dims similar to alltoalls is not needed.,2025-03-20T21:45:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89680
copybara-service[bot],Add cuda 12.1 and cudnn 9.8 image.,Add cuda 12.1 and cudnn 9.8 image.,2025-03-20T21:20:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89679
copybara-service[bot],Preserve shapes for TPUPartitionedInputV2Op.,Preserve shapes for TPUPartitionedInputV2Op.,2025-03-20T20:30:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89678
copybara-service[bot],PR #23964: Cleans up call inliner in the XLA shared code path,"PR CC(Math ops with Tensorflow Lite C++ and Android NDK): Cleans up call inliner in the XLA shared code path Imported from GitHub PR https://github.com/openxla/xla/pull/23964 1. Remove all the GPUspecific logic inside Call inliner. 2. Rewrite the IsInlineableCallOp to make the code more readable and less errorprone. The previous impl has some implicit priorities between all the checks and leads to bug. 3. Removes the test for stream annotation. Discussed with  , actually now we have two controls over the `compute_on` boxes from JAX. The flag in XLA `xla_gpu_experimental_stream_annotation` seems to be confusing and we should remove that. Because the control is explicitly placed in JAX, but users will get confused if there is another flag to control this. Copybara import of the project:  1ee6a74ec622d2a152e52f3f3e793e6a2f7af256 by Yunlong Liu : cleanup Merging this change closes CC(Math ops with Tensorflow Lite C++ and Android NDK) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23964 from yliu120:cleanup_call_inliner 1ee6a74ec622d2a152e52f3f3e793e6a2f7af256",2025-03-20T20:28:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89677
copybara-service[bot],Integrate LLVM at llvm/llvm-project@3959bbc1345b,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 3959bbc1345b,2025-03-20T20:22:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89676
copybara-service[bot],[XLA:Python] Add missing type stubs.,[XLA:Python] Add missing type stubs.,2025-03-20T20:18:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89675
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@66f90d5c,Integrate StableHLO at openxla/stablehlo,2025-03-20T19:26:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89674
copybara-service[bot],PR #23997: Fix the build issue after commit df841fe,"PR CC(Tensor.eval() hangs with Tensorflow 1.10+): Fix the build issue after commit df841fe Imported from GitHub PR https://github.com/openxla/xla/pull/23997 When running tests, like `bazel test //xla/service/gpu/model:hlo_op_profiler_test`, on a machine with only CUDA configured, the build system will try to build ROCm tests and fail. Before this change: ``` ERROR: xla/xla/stream_executor/rocm/BUILD:791:13: Compiling xla/stream_executor/rocm/rocm_helpers.cu.: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing CppCompile command (from target //xla/stream_executor/rocm:rocm_helpers) external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8opt/bin/xla/stream_executor/rocm/_objs/rocm_helpers/rocm_helpers.cu.pic.d ... (remaining 37 arguments skipped) /home/svaishay/.cache/bazel/_bazel_svaishay/3b9a2c84bcbdad9b9781b5eb1069603a/execroot/xla/external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc:227: SyntaxWarning: invalid escape sequence '\.'   re.search('\.cpp$          ^~~~~~~~~~~~~~~~~~~~ 1 error generated. Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 3.333s, Critical Path: 0.18s INFO: 28 processes: 28 internal. ERROR: Build did NOT complete successfully //xla/service/gpu/model:hlo_op_profiler_test_gpu_a100                 NO STATUS //xla/service/gpu/model:hlo_op_profiler_test_gpu_any                  NO STATUS //xla/service/gpu/model:hlo_op_profiler_test_gpu_b200                 NO STATUS //xla/service/gpu/model:hlo_op_profiler_test_gpu_h100                 NO STATUS ``` After this change: ``` INFO: Build completed successfully, 5 total actions //xla/service/gpu/model:hlo_op_profiler_test_gpu_a100                    PASSED in 14.9s //xla/service/gpu/model:hlo_op_profiler_test_gpu_any                     PASSED in 15.4s //xla/service/gpu/model:hlo_op_profiler_test_gpu_b200                    PASSED in 16.2s //xla/service/gpu/model:hlo_op_profiler_test_gpu_h100                    PASSED in 14.3s ``` Copybara import of the project:  e0c179778cc2fcc1f2f2a3bb74c9d6c6daaec5ea by Shraiysh Vaishay : Fix the build issue after commit df841fe When running tests, like `bazel test //xla/tests:hlo_op_profiler_test`, on a machine with only CUDA configured, the build system will try to build ROCm tests and fail. This commit fixes the issue by only building ROCm tests if ROCm is configured and vice versa. Merging this change closes CC(Tensor.eval() hangs with Tensorflow 1.10+) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23997 from shraiysh:fix_build_issue e0c179778cc2fcc1f2f2a3bb74c9d6c6daaec5ea",2025-03-20T19:22:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89673
copybara-service[bot],[HLO Componentization] Migrate away from deprecated build targets and header files.,[HLO Componentization] Migrate away from deprecated build targets and header files.,2025-03-20T19:12:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89672
gaikwadrahul8,Update 07 broken links in text_classification.md,"Hi, Team I found 07 broken documentation links in this file text_classification.md so I have updated those broken links to new LiteRT functional webpages links. Please review and merge this change as appropriate. Thank you for your consideration.",2025-03-20T19:07:22Z,comp:lite size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89671
copybara-service[bot],Replace outdated select() on --cpu in tensorflow/BUILD with platform API equivalent.,Replace outdated select() on cpu in tensorflow/BUILD with platform API equivalent.,2025-03-20T18:56:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89670
copybara-service[bot],Legalize `odml.embedding_lookup` with dynamic arguments.,Legalize `odml.embedding_lookup` with dynamic arguments.,2025-03-20T18:50:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89669
copybara-service[bot],Migrate some protos from core/profiler into gh/tensorflow/profiler,"Migrate some protos from core/profiler into gh/tensorflow/profiler As part of an ongoing push to improve colocation of code, this PR moves the remaining protobufs from code/profiler/protobuf into gh/tensorflow/profiler temporarily in the plugin/tensorboard_plugin_profile/protobuf directory. The protobuf will be moved into the `xprof` subdirectory in a subsequent PR to standardize referencing. Left behind are `public` imports, so existing uses of these protobufs will not be impacted. At some point, it is possible that these protobufs will be moved into xla/tsl, in which case they will see changed namespaces which will break old paths. That will only be done once all usages have been migrated.",2025-03-20T18:34:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89668
copybara-service[bot],PR #23793: Add stream dependency after calling memcpy p2p,PR CC(Tensorflow exception no module found with pyspark): Add stream dependency after calling memcpy p2p Imported from GitHub PR https://github.com/openxla/xla/pull/23793 Currently we have stream dependency to make sure buffers are ready before calling memcpy p2p in collective permute thunks. Postmemcpy sync using stream dep is also needed to make sure all data has been fully written before receiving rank consumes it. Copybara import of the project:  151cf3f35d0c83a11c5929d8040b9603b20ecb7c by TJ Xu : Add stream dependency after calling memcpy p2p  3a8a64369ceb4059b6eb8a455dedbe1c02508088 by TJ Xu : add a test to reproduce a numeric error Merging this change closes CC(Tensorflow exception no module found with pyspark) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23793 from Tixxx:tixxx/cp_sync 3a8a64369ceb4059b6eb8a455dedbe1c02508088,2025-03-20T18:22:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89667
distlibs,TensorFlow Lite 2.19 ARM compilation failed: TfLiteQuantizationType : int expected identifier or ...," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19  OS platform and distribution Raspberry Pi OS Bookworm  GCC/compiler version gcc (Debian 12.2.014) 12.2.0  Current behavior? Getting error when trying to compile simple program using TensorFlow Lite 2.19 library on Raspberry Pi. main.c: ``` include  include  include  int main() {     return 0; } ``` Compilation error: ``` In file included from build/usr/local/include/tensorflow/lite/c/common.h:31,                  from main.c:2: build/usr/local/include/tensorflow/lite/core/c/common.h:325:37: error: expected identifier or ‘(’ before ‘:’ token   325    ^~~~~~~~~~~~~~~~~~~~~~ ``` Compilation command: ``` gcc main.c Ibuild/usr/local/include Lbuild/usr/local/lib o test ltensorflowlite_c ``` No problems using TensorFlow Lite 2.18. This commit https://github.com/tensorflow/tensorflow/commit/977257e45ea83169c2dd6ff24a403cdd05b138bd caused issue.",2025-03-20T17:57:36Z,type:build/install comp:lite subtype: raspberry pi awaiting PR merge TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89666,"Hi,   I apologize for the delay in my response, thank you for bringing this issue to our attention now I have created a PR https://github.com/tensorflow/tensorflow/pull/91416 to take care of this issue. Thank you for your cooperation and patience.","Hi,  I closing this issue because this PR https://github.com/tensorflow/tensorflow/pull/91416 got merged, if you face any issue please feel free to post your comments if required I'll open this issue. Thank you for your cooperation and patience.",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],OSS Noop change,OSS Noop change,2025-03-20T17:54:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89665
copybara-service[bot],[xla:ffi] Add context decoding for some FFI internals to external interface.,[xla:ffi] Add context decoding for some FFI internals to external interface.,2025-03-20T17:39:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89664
copybara-service[bot],Replace the uses of PjRtClient::Compile() with PjRtClient::CompileAndLoad().,Replace the uses of PjRtClient::Compile() with PjRtClient::CompileAndLoad(). This is to prepare for updating PjRtClient::Compile() to return an unloaded. executable.,2025-03-20T17:38:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89663
copybara-service[bot],PR #89210: Qualcomm AI Engine Direct - Provide op optimization,PR CC(Qualcomm AI Engine Direct  Provide op optimization): Qualcomm AI Engine Direct  Provide op optimization Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/89210 Summary:  Support FC>CONV2D optimization  Add CONV2D op builder with CPU transpose Test:  [x] a8w8 Gemma3 compile and execution successfully  [x] qnn_compiler_plugin_test all pass Copybara import of the project:  8ff43628b495ce8ae730a5ed42fa6bea1ef15844 by jiunkaiy : Qualcomm AI Engine Direct  Op optimization Summary:  Support FC>CONV2D optimization  Add CONV2D op builder with CPU transpose Merging this change closes CC(Qualcomm AI Engine Direct  Provide op optimization) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89210 from jiunkaiy:dev/jiunkaiy/gemma2_fc 8ff43628b495ce8ae730a5ed42fa6bea1ef15844,2025-03-20T17:36:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89662
copybara-service[bot],Fix a segmentation fault when accelerators are released from the accelerator registry.,Fix a segmentation fault when accelerators are released from the accelerator registry. The order of `accelerators_shared_libraries_` and `accelerators_` is VERY important. When the accelerators are destroyed they call into functions that are in their libraries. This means that the libraries must be released AFTER the accelerators are. Remember that C++'s order of destruction is the reverse of the order of construction (which is the same as the order of declaration for class fields).,2025-03-20T17:31:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89661
copybara-service[bot],Reverts 819247756d279c8d689a5088f7cef42ff3b494de,Reverts 819247756d279c8d689a5088f7cef42ff3b494de,2025-03-20T17:27:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89660
copybara-service[bot],[XLA:CPU][bugfix] HLO benchmark runner waits for warmup execution to stop before starting timings,[XLA:CPU][bugfix] HLO benchmark runner waits for warmup execution to stop before starting timings,2025-03-20T17:19:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89659
copybara-service[bot],Remove litert/runtime/opencl directory.,Remove litert/runtime/opencl directory. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/80074 from yzhou51:fullconnect_use_slm 6ba325e738ee82b60bc611247c797451b8c957ae,2025-03-20T17:10:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89658
copybara-service[bot],hlo_instruction: Don't derefence a NULL OpMetadata during printing.,"hlo_instruction: Don't derefence a NULL OpMetadata during printing. It's unclear as yet why this happens, but a NULL check is cheap to add.",2025-03-20T17:06:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89657
copybara-service[bot],Add GetClBuffer for AHWB Tensor Buffer.,Add GetClBuffer for AHWB Tensor Buffer.,2025-03-20T15:28:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89656
copybara-service[bot],Adapt Stablehlo to recent upstream Tosa changes.,"Adapt Stablehlo to recent upstream Tosa changes. Changes needed for https://github.com/llvm/llvmproject/pull/130337, https://github.com/llvm/llvmproject/pull/129720, and https://github.com/llvm/llvmproject/pull/130340",2025-03-20T15:21:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89655
copybara-service[bot],Integrate Triton up to [cdb53266](https://github.com/openai/triton/commits/cdb53266e6c251d91a2c321d64e8466caff129a9),Integrate Triton up to cdb53266 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/80074 from yzhou51:fullconnect_use_slm 6ba325e738ee82b60bc611247c797451b8c957ae,2025-03-20T14:55:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89654
copybara-service[bot],[XLA:GPU] set --xla_gpu_unsupported_enable_generic_triton_emitter_for_gemms on,[XLA:GPU] set xla_gpu_unsupported_enable_generic_triton_emitter_for_gemms on,2025-03-20T14:51:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89653
copybara-service[bot],[XLA:GPU] Collect matmul perf table data for further analysis.,[XLA:GPU] Collect matmul perf table data for further analysis.,2025-03-20T14:28:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89652
copybara-service[bot],`ArrayImpl` is now a proper subclass of `jax.Array`,"`ArrayImpl` is now a proper subclass of `jax.Array` This allows to make `jax.Array` a ""strict"" ABC which doesn't support virtual subclasses and is thus can do faster isinstance/issubclass checks. Note that I had to move `StrictABC` from `util` into a separate submodule to avoid an import cycle with `basearray` and `xla_client`.",2025-03-20T14:16:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89651
copybara-service[bot],Disable TOSA rescale files and tests while broken,Disable TOSA rescale files and tests while broken,2025-03-20T14:11:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89650
copybara-service[bot],Remove LITERT_HAS_OPENCL_SUPPORT from user-facing APIs.,Remove LITERT_HAS_OPENCL_SUPPORT from userfacing APIs. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/83717 from MCWDev:master_pr bab5dff4cb95823df5ed72da0cacc3bbe56e22e1,2025-03-20T13:49:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89649
copybara-service[bot],[Triton-XLA] Adjustments and fixes to TritonXLA ops and passes.,[TritonXLA] Adjustments and fixes to TritonXLA ops and passes.,2025-03-20T13:47:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89648
copybara-service[bot],Make the `ErrorStatusBuilder` compatible with `absl::Status[Or]`.,Make the `ErrorStatusBuilder` compatible with `absl::Status[Or]`. The macros `LITERT_ASSIGN_OR_RETURN` and `LITERT_RETURN_IF_ERROR` can be used transparently in client code that uses the C++ API in code using `absl::Status[Or]`.,2025-03-20T13:10:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89647
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-20T13:05:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89646
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-20T12:56:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89645
copybara-service[bot],Replace outdated select() on --cpu in tensorflow/BUILD and related files with platform API equivalent.,Replace outdated select() on cpu in tensorflow/BUILD and related files with platform API equivalent.,2025-03-20T12:46:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89644
copybara-service[bot],Reverts 36c495fa72c7ffee626e83ae9b9206911ec8875f,Reverts 36c495fa72c7ffee626e83ae9b9206911ec8875f FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23619 from serach24:chenhao/fix_nvsl_check_failed 826535264624142999164132f3a873be06019279,2025-03-20T12:41:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89643
copybara-service[bot],#sdy Copy over change to `import_backend_func_calls`,sdy Copy over change to `import_backend_func_calls`,2025-03-20T12:06:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89642
copybara-service[bot],#sdy Convert `func.call` ops with `inlineable = false` to `sdy.named_computation`,sdy Convert `func.call` ops with `inlineable = false` to `sdy.named_computation` FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/83717 from MCWDev:master_pr bab5dff4cb95823df5ed72da0cacc3bbe56e22e1,2025-03-20T11:45:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89641
copybara-service[bot],[XLA:CPU] Parallelize transpose copies,[XLA:CPU] Parallelize transpose copies,2025-03-20T11:26:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89640
copybara-service[bot],Fix a typo ,"Fix a typo  I have no idea what DLP here means, but ULP make sense. Assuming DLP is a typo of ULP.",2025-03-20T10:50:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89639
copybara-service[bot],[XLA:GPU] Interpolate matmuls from perf tables.,[XLA:GPU] Interpolate matmuls from perf tables.,2025-03-20T10:09:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89638
copybara-service[bot],Reverts 4725edaeb615d5755ce969f7f66d14bd370dcac9,Reverts 4725edaeb615d5755ce969f7f66d14bd370dcac9,2025-03-20T10:09:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89637
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T10:05:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89636
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-20T10:05:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89635
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-20T09:59:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89634
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-20T09:47:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89633
copybara-service[bot],Implement Jax CPU callbacks with XLA FFI.,Implement Jax CPU callbacks with XLA FFI.,2025-03-20T09:43:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89632
copybara-service[bot],PR #23619: Fix the EmitSort checking after enabling NVLS and user buffer,"PR CC(tf.contrib.distributions.percentile didn't support eager execution): Fix the EmitSort checking after enabling NVLS and user buffer Imported from GitHub PR https://github.com/openxla/xla/pull/23619 There is a reported bug from NVIDIA that running Midjourney model triggers XLA error after enabling NVLS and user buffer by setting NCCL_NVLS_ENABLE=1 and xla_gpu_enable_nccl_user_buffers=true: ``` jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure (external/xla/xla/service/gpu/ir_emitter_unnested.cc:1676) LayoutUtil::LayoutsInShapesEqual(keys_shape, sort>operand(i)>shape()). ``` This is because after enabling NVLS and user buffer, one of the operand of `sort` operation is from a different memory space (user buffer), and the previous `LayoutsInShapesEqual` check is too strong to pass as it also checks if operands are from the same memory space. This MR makes the sort layout check weaker as operands do not have to be in the same memory space as long as they all on the device. Copybara import of the project:  826535264624142999164132f3a873be06019279 by Chenhao Jiang : Making the sort layout check weaker as operands do not have to be in the same memory space as long as they all on the device. Merging this change closes CC(tf.contrib.distributions.percentile didn't support eager execution) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23619 from serach24:chenhao/fix_nvsl_check_failed 826535264624142999164132f3a873be06019279",2025-03-20T09:42:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89631
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-20T09:38:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89630
copybara-service[bot],[XLA] Add an option to dump full HLO config proto,[XLA] Add an option to dump full HLO config proto,2025-03-20T09:29:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89629
eewindfly,Add support for SeparableConv2DTranspose (Depthwise Conv2DTranspose)," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.16.1  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Feature Request TensorFlow currently lacks a SeparableConv2DTranspose operation, which is essential for efficient depthwise transposed convolutions. PyTorch already supports this easily via the groups parameter in ConvTranspose2d. However, TensorFlow’s Conv2DTranspose does not have a groups argument, making it impossible to achieve the same functionality. Workarounds Currently, the only way to approximate this functionality in TensorFlow is: 	•	Manually splitting input channels and applying Conv2DTranspose separately (inefficient and slow). Proposed Solution 	•	Either add a groups parameter to Conv2DTranspose, similar to PyTorch, 	•	Or implement a native SeparableConv2DTranspose API to complement SeparableConv2D. Use Case Separable transposed convolutions are useful for: 	•	Efficient upsampling in lightweight CNN architectures. 	•	Mobile and edge computing models requiring optimized computations. Would be great to have this officially supported in TensorFlow! Thanks. CC(Feature Request: Add separable_conv2d_transpose operation)  Previous discussion has been closed which is not able to reopen, so I create a new one for it.  Standalone code to reproduce the issue ```shell None ```  Relevant log output ```shell ```",2025-03-20T09:27:50Z,type:feature,open,0,0,https://github.com/tensorflow/tensorflow/issues/89628
copybara-service[bot],PR #23909: Add set_use_global_device_ids() to AllGather instr,"PR CC(TFTRT: Add ExpandDims, Squeeze ops and unit tests.): Add set_use_global_device_ids() to AllGather instr Imported from GitHub PR https://github.com/openxla/xla/pull/23909 `use_global_device_ids` attribute is defined in both `HloAllReduceInstructionBase` and `HloAllGatherInstruction` classes. But `HloAllGatherInstruction` lacks a setter method  `set_use_global_device_ids`. This PR fixes it by adding missing `set_use_global_device_ids` to `HloAllGatherInstruction`. I could not find any existing tests that validate `set_use_global_device_ids` for AllReduce or ReduceScatter. Therefore, this AllGather PR does not include modifications to unit tests too. Copybara import of the project:  fd3c082776ec27ec16288c397b025daf6bc95874 by Alexander Pivovarov : Add set_use_global_device_ids to AllGather instr Merging this change closes CC(TFTRT: Add ExpandDims, Squeeze ops and unit tests.) Reverts b68ce06d697405d1a911babef312dd80513edd79 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23909 from apivovarov:set_use_global_device_ids fd3c082776ec27ec16288c397b025daf6bc95874",2025-03-20T09:25:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89627
copybara-service[bot],Remove runtime dependencies from gpu_executable,Remove runtime dependencies from gpu_executable `GpuExecutable` is a type that is used by both the compiler and the runtime. Therefore it shouldn't link runtimeonly dependencies. So this change removes those dependencies which required to add some explicit dependencies to downstream users of GpuExecutable that was transitively relying on the dependency. This is a prerequisite for turning XLA into a proper AOT/deviceless compiler and also for the CUDA/ROCm modularization,2025-03-20T09:20:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89626
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-20T09:19:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89625
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-20T09:16:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89624
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T08:38:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89623
copybara-service[bot],Develop a utility function for type-safe access to advanced configuration values within a map.,Develop a utility function for typesafe access to advanced configuration values within a map.,2025-03-20T08:25:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89622
fujunwei,Undefine the VK_USE_PLATFORM_XLIB_KHR to exclude Xlib.h,"The Status in absl conflicts with the macro definition define Status int in Xlib.h, so undefine the VK_USE_PLATFORM_XLIB_KHR to exclude the header file in the vulkan.h. There are below errors when VK_USE_PLATFORM_XLIB_KHR is defined on the Chromium linuxasanv8armdbg try bots. ``` In file included from ../../third_party/tflite/src/tensorflow/lite/delegates/gpu/api.cc:16: ../../third_party/tflite/src/tensorflow/lite/delegates/gpu/api.h:262:17: error: expected unqualifiedid   262                ```",2025-03-20T08:18:34Z,comp:lite ready to pull size:XS,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89621, Could you please help review this PR? thanks.," thanks for reviewing and approving the code, but the branch cannot be merged. Could you help me take a look the blocked issue?",See the process in https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md. The internal change is not yet approved,"> See the process in https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md. >  > The internal change is not yet approved Got it, thanks."
copybara-service[bot],Replace the uses of PjRtClient::Compile() with PjRtClient::CompileAndLoad().,Replace the uses of PjRtClient::Compile() with PjRtClient::CompileAndLoad(). This is to prepare for updating PjRtClient::Compile() to return an unloaded. executable.,2025-03-20T07:35:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89620
copybara-service[bot],Add a option to apply the plugin to the subgraph with given index only.,Add a option to apply the plugin to the subgraph with given index only.,2025-03-20T07:14:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89619
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T07:06:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89618
chuntl,Qualcomm AI Engine Direct - Update soc model,Summary:  Adding known soc model based on soc id,2025-03-20T06:59:56Z,ready to pull size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89617
copybara-service[bot],Change `PjRtClient::DeserializeExecutable()` to return an unloaded executable.,Change `PjRtClient::DeserializeExecutable()` to return an unloaded executable.,2025-03-20T06:06:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89616
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T04:04:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89615
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T04:01:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89614
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T04:00:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89613
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T03:59:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89612
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T03:59:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89611
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T03:59:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89610
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T03:55:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89609
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T03:55:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89608
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T03:54:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89607
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T03:49:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89606
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23786 from openxla:fix_while_execution_count fd93a631e6b818df7d98f29a55b53a8819eae9a3,2025-03-20T03:47:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89605
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23909 from apivovarov:set_use_global_device_ids fd3c082776ec27ec16288c397b025daf6bc95874,2025-03-20T03:45:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89604
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T03:45:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89603
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T03:44:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89602
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-20T03:42:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89601
zhouxiaoyaozzz,NotImplementedError: StreamingModel.call() not implemented," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.18.0  Custom code Yes  OS platform and distribution windows 11  Mobile device _No response_  Python version 3.12.6  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When I try to compile and run my custom Keras model (StreamingModel), I encounter the following error: `NotImplementedError: Exception encountered when calling StreamingModel.call(). Could not automatically infer the output shape / dtype of 'streaming_model' (of type StreamingModel). Either the `StreamingModel.call()` method is incorrect, or you need to implement the `StreamingModel.compute_output_spec() / compute_output_shape()` method. Error encountered: Model StreamingModel does not have a `call()` method implemented.` Expected Behavior I expect the StreamingModel to correctly implement the call() method and infer the output shape/dtype automatically, or allow me to manually specify it using compute_output_spec() or compute_output_shape().  Standalone code to reproduce the issue ```shell import tensorflow as tf  Define CircularBufferLayer class CircularBufferLayer(tf.keras.layers.Layer):     def __init__(self, num_features, buffer_size, stride, **kwargs):         super().__init__(**kwargs)         self.num_features = num_features         self.buffer_size = buffer_size         self.stride = stride         self.gradient_scale = 0.1         self.buffer = self.add_weight(name='buffer', shape=(1, buffer_size, num_features),                                       initializer='zeros', trainable=False, dtype=tf.float32)         self.call_count = self.add_weight(name='call_count', shape=(), initializer='zeros',                                           dtype=tf.int32, trainable=False)         self.total_call_count = self.add_weight(name='total_call_count', shape=(), initializer='zeros',                                                 dtype=tf.int32, trainable=False)     def call(self, inputs):         scaled_input = tf.multiply(inputs, self.gradient_scale)         new_buffer = tf.concat([scaled_input, self.buffer[:, :1]], axis=1)         self.buffer.assign(new_buffer)         return self.buffer  Define StreamingModel class StreamingModel(tf.keras.Model):     def call(self, inputs):         x, _ = super().call(inputs)   Assume another branch is truncated         return x  Instantiate the model buffer_layer = CircularBufferLayer(num_features=64, buffer_size=10, stride=1) model = StreamingModel()  Define input shape input_shape = (None, 10, 64)   (batch_size, sequence_length, num_features) inputs = tf.keras.Input(shape=input_shape[1:])   Ignore batch_size outputs = model(inputs)  Build the model model = tf.keras.Model(inputs=inputs, outputs=outputs)  Compile the model model.compile(     optimizer='adam',   Use Adam optimizer     loss='mse',        Use mean squared error as the loss function     metrics=['mae']    Use mean absolute error as the evaluation metric ) ```  Relevant log output ```shell 20250319 16:09:45.545622: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250319 16:09:46.283611: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250319 16:09:48.149403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. WARNING:tensorflow:From D:\project\DLComplier\.venv\Lib\sitepackages\keras\src\backend\tensorflow\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. Traceback (most recent call last):   File ""D:\project\DLComplier\.venv\statiblity.py"", line 37, in      outputs = model(inputs)               ^^^^^^^^^^^^^   File ""D:\project\DLComplier\.venv\Lib\sitepackages\keras\src\utils\traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""D:\project\DLComplier\.venv\statiblity.py"", line 27, in call     x, _ = super().call(inputs)   Assume another branch is truncated            ^^^^^^^^^^^^^^^^^^^^ NotImplementedError: Exception encountered when calling StreamingModel.call(). Could not automatically infer the output shape / dtype of 'streaming_model' (of type StreamingModel). Either the `StreamingModel.call()` method is incorrect, or you need to implement the `StreamingModel.compute_output_spec() / compute_output_shape()` method. Error encountered: Model StreamingModel does not have a `call()` method implemented. Arguments received by StreamingModel.call():   • args=('',)   • kwargs= ```",2025-03-20T02:55:01Z,type:bug TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/89600
jq,How to adapt TFRA with Tensorflow Parameter Strategy V2," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.16  Custom code Yes  OS platform and distribution all OS  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? tensorflow recomendation addon 's keras + Ps has been broken for sometime, following are the issues so far https://github.com/tensorflow/recommendersaddons/issues/182 https://github.com/tensorflow/recommendersaddons/issues/401 https://github.com/tensorflow/recommendersaddons/issues/365 Specifically, the incompatibility stems from changes introduced by TensorFlow’s ParameterServerStrategy. Two issues have been identified so far, though there may be more:  Changes in ParameterServerStrategy.extended.worker_devices return values – This issue has already been addressed in https://github.com/tensorflow/recommendersaddons/pull/488  Graph creation and variable placement differences – ParameterServerStrategy creates the computation graph on the PS and then distributes it to workers. TFRA,  uses proxy variables on each worker. These proxy variables wrapped in DistributedVariableWrapper and inherit from DistributedVariable, and tfra rely on DistributedVariable to get the correct device placement i.e. ._get_on_device_or_primary method,  however, this  only returns device 0, i.e. /job:worker/replica:0/task:0/device:GPU:0 , under ParameterServerStrategy, so all other workers keep crashing. I wonder what will be correct path to fix the TensorFlow’s ParameterServerStrategy + TFRA + keras?  I had explored PerWorkerVariable  to get a PerWorkerVariable, we need to use something like  `variables.Variable(initial_value=(),   shape=shape, dtype=dtype, name=name, per_worker_de_variable=True)`       this is not clean for complex classes, i.e. `class TrainableWrapper(resource_variable_ops.ResourceVariable):` `class ShadowVariable(EmbeddingWeights, TrainableWrapper):`       to make ShadowVariable a PerWorkerVariable,   new classes needed with full duplication  ` class TrainableWrapperPerWorker(PerWorkerVariable):` `class ShadowVariablePerWorker(EmbeddingWeights, TrainableWrapperPerWorker): ` is this the right path?  Standalone code to reproduce the issue ```shell any of the code in the issues https://github.com/tensorflow/recommendersaddons/issues/182 https://github.com/tensorflow/recommendersaddons/issues/401 https://github.com/tensorflow/recommendersaddons/issues/365 ```  Relevant log output ```shell RROR:tensorflow:Worker /job:worker/replica:0/task:1 failed with InvalidArgumentError():/job:worker/replica:0/task:0/device:GPU:0 unknown device. Additional GRPC error information from remote target /job:worker/replica:0/task:1 while calling /tensorflow.eager.EagerService/Enqueue: :{""created"":"".320488513"",""description"":""Error received from peer ipv4:127.0.0.1:2232"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""/job:worker/replica:0/task:0/device:GPU:0 unknown device."",""grpc_status"":3} [Op:__inference_train_function_2473] E0320 02:48:37.325277 139838372021824 cluster_coordinator.py:949] Worker /job:worker/replica:0/task:1 failed with InvalidArgumentError():/job:worker/replica:0/task:0/device:GPU:0 unknown device. Additional GRPC error information from remote target /job:worker/replica:0/task:1 while calling /tensorflow.eager.EagerService/Enqueue: :{""created"":"".320488513"",""description"":""Error received from peer ipv4:127.0.0.1:2232"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""/job:worker/replica:0/task:0/device:GPU:0 unknown device."",""grpc_status"":3} [Op:__inference_train_function_2473] INFO:tensorflow:[Worker 1] Putting back a closure after it failed. I0320 02:48:37.325551 139838372021824 cluster_coordinator.py:1077] [Worker 1] Putting back a closure after it failed. INFO:tensorflow:[Worker 1] Clearing all resources. I0320 02:48:37.325744 139838372021824 cluster_coordinator.py:1065] [Worker 1] Clearing all resources. INFO:tensorflow:Cluster now being recovered. I0320 02:48:37.325959 139838355236416 cluster_coordinator.py:991] Cluster now being recovered. 20250320 02:48:37.326268: W tensorflow/core/common_runtime/eager/context_distributed_manager.cc:694] Device filters can only be specified when initializing the cluster. Any changes in device filters are ignored when updating the server def. INFO:tensorflow:Cluster successfully recovered. I0320 02:48:37.334258 139838355236416 cluster_coordinator.py:997] Cluster successfully recovered. INFO:tensorflow:Worker /job:worker/replica:0/task:1 has been recovered. I0320 02:48:37.334562 139838372021824 cluster_coordinator.py:964] Worker /job:worker/replica:0/task:1 has been recovered. INFO:tensorflow:Worker /job:worker/replica:0/task:1 calling on_recovery_fn I0320 02:48:37.334733 139838372021824 cluster_coordinator.py:967] Worker /job:worker/replica:0/task:1 calling on_recovery_fn INFO:tensorflow:[Worker 1] calling _on_worker_recovery I0320 02:48:37.334853 139838372021824 cluster_coordinator.py:1103] [Worker 1] calling _on_worker_recovery 20250320 02:48:37.361068: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:553] The `assert_cardinality` transformation is currently not handled by the autoshard rewrite and will be removed. 20250320 02:48:37.362102: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:553] The `assert_cardinality` transformation is currently not handled by the autoshard rewrite and will be removed. ```",2025-03-20T02:35:58Z,type:feature,open,0,0,https://github.com/tensorflow/tensorflow/issues/89599
copybara-service[bot],Add fused activation support in conv ops.,Add fused activation support in conv ops.,2025-03-20T02:32:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89598
copybara-service[bot],Enforce pre-conditions of Shape ctors. This will catch programmer errors earlier.,Enforce preconditions of Shape ctors. This will catch programmer errors earlier.,2025-03-20T01:28:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89597
copybara-service[bot],Update legalization rule for TFL op in StableHLO custom_call carrier,Update legalization rule for TFL op in StableHLO custom_call carrier,2025-03-20T01:18:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89596
copybara-service[bot],litert: Add CompiledModel methods to support default subgraph execution,litert: Add CompiledModel methods to support default subgraph execution This simplies users code if users want to run primary subgraph.,2025-03-20T00:44:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89595
copybara-service[bot],Disable `//xla/hlo/tools/tests:hlo_opt_emit_proto.hlo.test` on ARM due to avoid flakes,Disable `//xla/hlo/tools/tests:hlo_opt_emit_proto.hlo.test` on ARM due to avoid flakes,2025-03-20T00:17:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89594
copybara-service[bot],Fork fake_quant_utils within TF to use tf_quantization_lib (fork using TF Quant Dialect),Fork fake_quant_utils within TF to use tf_quantization_lib (fork using TF Quant Dialect),2025-03-20T00:15:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89593
copybara-service[bot],Adjust sharding rules for ragged_dot operations.,Adjust sharding rules for ragged_dot operations.  Mode 3 (the ragged dimension is a batch dimension) Do not propagate shardings to/from the group_sizes (the 3rd operand) since it is redundant. We will discard this tensor and convert ragged_dot into a standard dot in the partitioner.  Mode 1 (the ragged dimension is a lhs noncontracting dimension) The ragged dimension and the group_size dimension needs full replication in the partitioner.  Mode 2 (the ragged dimension is a contracting dimension) The ragged dimension and the group_size dimension needs full replication in the partitioner. We do not mark the ragged dimension as replicated although it is a contracting dimension.,2025-03-20T00:11:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89592
copybara-service[bot],Remove split configs from evicted tensors.,Remove split configs from evicted tensors.,2025-03-19T23:32:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89591
copybara-service[bot],Delete `.github/CODEOWNERS` as it has been unused for a long time,Delete `.github/CODEOWNERS` as it has been unused for a long time,2025-03-19T23:21:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89590
copybara-service[bot],[XLA:GPU] Add JAX-based pipeline parallelism test,[XLA:GPU] Add JAXbased pipeline parallelism test,2025-03-19T23:09:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89589
copybara-service[bot],Fix while not being parallerized due to StopGradient,Fix while not being parallerized due to StopGradient,2025-03-19T22:52:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89588
copybara-service[bot],Replace uses of deprecated `Shape::dimensions_size()`.,"Replace uses of deprecated `Shape::dimensions_size()`. We plan to revamp the `Shape` API and consolidate a bunch of methods. In particular, `Shape::dimensions_size()` is deprecated as it's redundant with `Shape::dimensions().size()`. This change replaces uses of `Shape::dimensions_size()` with the recommended syntax.",2025-03-19T22:45:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89587
copybara-service[bot],Allocation: Propagate shape changes to fused parameters.,Allocation: Propagate shape changes to fused parameters.,2025-03-19T22:34:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89586
copybara-service[bot],Partition the model even if op is not supported in QNN plugin.,Partition the model even if op is not supported in QNN plugin.,2025-03-19T21:55:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89585
copybara-service[bot],[XLA:LatencyHidingScheduler] Cleanup HloGraphNode dependency insertion,[XLA:LatencyHidingScheduler] Cleanup HloGraphNode dependency insertion,2025-03-19T21:33:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89584
copybara-service[bot],Fork quantization_lib within TF to use TF Quant Dialect,Fork quantization_lib within TF to use TF Quant Dialect,2025-03-19T21:21:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89583
copybara-service[bot],PR #23947: Makes inlineable first class citizen attribute.,"PR CC(Network Bandwidth Unexpected Behavior ): Makes inlineable first class citizen attribute. Imported from GitHub PR https://github.com/openxla/xla/pull/23947 `InlineStreamAnnotation` should not be sitting in the shared code path of call inliner. The implementation was very specific to GPU and it relies on flag, which is not necessary as the annotation is there already. This PR just introduces a firstclass attribute `inlineable` so that the frontend can directly control which call op needs to be inlined. Copybara import of the project:  83afbae34a3f054dc77c12db886c61084b0b7ae3 by Yunlong Liu : Makes inlineable first class citizen attribute. Merging this change closes CC(Network Bandwidth Unexpected Behavior ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23947 from yliu120:fix_inliner 83afbae34a3f054dc77c12db886c61084b0b7ae3",2025-03-19T20:59:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89582
copybara-service[bot],[xla:ffi] Allow registering type ids assigned by the user,[xla:ffi] Allow registering type ids assigned by the user,2025-03-19T20:47:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89581
copybara-service[bot],Add a test for the Operand Upcaster HLO pass.,Add a test for the Operand Upcaster HLO pass.,2025-03-19T20:45:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89580
copybara-service[bot],Fix Framework Name Scope appearing out of order,Fix Framework Name Scope appearing out of order,2025-03-19T20:15:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89579
copybara-service[bot],Better error handling for NNAPI memory allocation.,Better error handling for NNAPI memory allocation. Reverts 46e4e9680f157a0605ba3be12358586362dd4c0b,2025-03-19T20:08:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89578
copybara-service[bot],Delete contents of `third_party/triton/BUILD` as it's unused,Delete contents of `third_party/triton/BUILD` as it's unused,2025-03-19T19:34:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89577
copybara-service[bot],Delete `cycle_detector_test.cc`,"Delete `cycle_detector_test.cc` This test depends on XLA which code in `mlir_hlo` shouldn't do, and bazel doesn't know about the test so it wasn't being run in the first place. The `cycle_detector` code also has no users.",2025-03-19T19:21:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89576
copybara-service[bot],"Replace uses of deprecated `Shape::rank()` in OpenXLA with `Shape::dimensions_size()`, as we decided to stop using the ""rank"" term to mean the number of array dimensions.","Replace uses of deprecated `Shape::rank()` in OpenXLA with `Shape::dimensions_size()`, as we decided to stop using the ""rank"" term to mean the number of array dimensions. Initially I tried to replace `rank()` with `dimensions().size()`. However, I found that the change is unsafe in some cases as it changes the signedness of the value (from signed to unsigned). For example, due to C++ integer promotion, assuming the number of dimensions is 2, `rank() > 1` will be true (expected), but `dimensions().size() > 1` will be false (as 1 is converted to unsigned first). Therefore I decided to be safe and replace `rank()` with `dimensions_size()` instead.",2025-03-19T18:52:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89575
copybara-service[bot],Guard more headers and members behind OpenCL and OpenGL flags,Guard more headers and members behind OpenCL and OpenGL flags,2025-03-19T18:44:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89574
copybara-service[bot],Support fusion location in TAC filter.,Support fusion location in TAC filter.,2025-03-19T18:34:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89573
copybara-service[bot],Remove deprecated Shape ctor.,Remove deprecated Shape ctor. Tested with existing tests as this is a refactoring.,2025-03-19T18:26:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89572
copybara-service[bot],Support int64 mapping in litert model <-> tflite fb,Support int64 mapping in litert model  tflite fb,2025-03-19T18:02:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89571
copybara-service[bot],Add timer for run compiled model tool.,Add timer for run compiled model tool.,2025-03-19T18:02:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89570
copybara-service[bot],IFRT: Introduce `LoadedExecutable::GetDonatableInputIndices()`.,"IFRT: Introduce `LoadedExecutable::GetDonatableInputIndices()`. The function returns indices of parameters that will be donated whenever `Execute` gets called, provided they are not present in `execute_options.non_donatable_input_indices`. This change will be used in an upcoming IFRTproxy commit that will allow make incurring RPC roundtrips for `Array::IsDeleted()` unnecessary.",2025-03-19T17:59:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89569
copybara-service[bot],internal BUILD rule visibility,internal BUILD rule visibility,2025-03-19T17:52:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89568
copybara-service[bot],[XLA:GPU] Decompose all applicable collective-permutes when pipeline parallelism optimizations enabled,[XLA:GPU] Decompose all applicable collectivepermutes when pipeline parallelism optimizations enabled,2025-03-19T17:48:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89567
copybara-service[bot],Rename some XLA internal API from rank() to num_dimensions() to avoid confusion.,Rename some XLA internal API from rank() to num_dimensions() to avoid confusion.,2025-03-19T17:42:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89566
copybara-service[bot],Replace 'rank' with 'number of dimensions' in XLA documentation to avoid confusion.,Replace 'rank' with 'number of dimensions' in XLA documentation to avoid confusion. Some formatting changes are introduced by autoformatting.,2025-03-19T17:29:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89565
copybara-service[bot],Testing docker container,Testing docker container,2025-03-19T17:10:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89564
copybara-service[bot],[XLA:Python] Remove py_client_gpu from XLA.,[XLA:Python] Remove py_client_gpu from XLA. This code has been moved into JAX.,2025-03-19T17:03:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89563
copybara-service[bot],[XLA:GPU] new fusion kind for nested dot fusions,[XLA:GPU] new fusion kind for nested dot fusions To prevent priority fusion from modifying the construct produced by nest_gemm_fusion pass.  For example priority fusion may insert back bitcasts that we have carefully extracted before.,2025-03-19T16:48:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89562
copybara-service[bot],Integrate LLVM at llvm/llvm-project@3959bbc1345b,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 3959bbc1345b Reverts 819247756d279c8d689a5088f7cef42ff3b494de,2025-03-19T16:34:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89561
copybara-service[bot],PR #23619: Fix the EmitSort checking after enabling NVLS and user buffer,"PR CC(tf.contrib.distributions.percentile didn't support eager execution): Fix the EmitSort checking after enabling NVLS and user buffer Imported from GitHub PR https://github.com/openxla/xla/pull/23619 There is a reported bug from NVIDIA that running Midjourney model triggers XLA error after enabling NVLS and user buffer by setting NCCL_NVLS_ENABLE=1 and xla_gpu_enable_nccl_user_buffers=true: ``` jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure (external/xla/xla/service/gpu/ir_emitter_unnested.cc:1676) LayoutUtil::LayoutsInShapesEqual(keys_shape, sort>operand(i)>shape()). ``` This is because after enabling NVLS and user buffer, one of the operand of `sort` operation is from a different memory space (user buffer), and the previous `LayoutsInShapesEqual` check is too strong to pass as it also checks if operands are from the same memory space. This MR makes the sort layout check weaker as operands do not have to be in the same memory space as long as they all on the device. Copybara import of the project:  826535264624142999164132f3a873be06019279 by Chenhao Jiang : Making the sort layout check weaker as operands do not have to be in the same memory space as long as they all on the device. Merging this change closes CC(tf.contrib.distributions.percentile didn't support eager execution) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23619 from serach24:chenhao/fix_nvsl_check_failed 826535264624142999164132f3a873be06019279",2025-03-19T16:20:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89560
copybara-service[bot],Replace outdated select() on --cpu in lite/delegates/gpu/BUILD with platform API equivalent.,Replace outdated select() on cpu in lite/delegates/gpu/BUILD with platform API equivalent. Reverts 0127a68459abc349b9eab53efd8afe5c628e716e,2025-03-19T15:49:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89559
copybara-service[bot],[XLA:CPU] Fuse reductions in all cases,[XLA:CPU] Fuse reductions in all cases,2025-03-19T14:54:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89558
copybara-service[bot],PR #23799: Add support for emitting DUS as memcpy.,"PR CC(TFLite can not set input shape and input_stats with 1.0/128): Add support for emitting DUS as memcpy. Imported from GitHub PR https://github.com/openxla/xla/pull/23799 This extends the dynamic memcpy support for dynamicslice to dynamicupdateslice. The logic is largely the same (except this time, the destination offset is dynamic, not the source offset). For now, only inplace DUSes are handled. We could handle outofplace DUSes as well, but I don't think it's needed. Copybara import of the project:  ae5f322ce581a75cf0d4cb3b9b1d2513b497e421 by Johannes Reifferscheid : Add support for emitting DUS as memcpy. This extends the dynamic memcpy support for dynamicslice to dynamicupdateslice. The logic is largely the same.  efa584137f03d1de4a04ee07f5fa65a10829d73b by Johannes Reifferscheid : Address review comments. Merging this change closes CC(TFLite can not set input shape and input_stats with 1.0/128) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23799 from jreiffers:memcpy efa584137f03d1de4a04ee07f5fa65a10829d73b",2025-03-19T14:45:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89557
RaulHerreroMuela,Efficientnet - TFLite Inference Issue,"**System information**  OS Platform and Distribution: Win 11  TensorFlow installed from: source  TensorFlow version: 2.9.3 **Description of the problem** I have trained EfficientNetB0 to do regression. I convert to TFLite the model according to the above code. I make inference on the same set of images with both the Keras model and the converted to TFLite and compare the obtained estimates by means of the percentage difference. As shown in the image (each error column corresponds to a different model), sometimes the estimates are practically similar and other times they present differences of more than 1000%. What can this be due to? Other types of models created by me layer by layer do not present this problem and I have used the same conversion code. I need to quantization to float16 because the device i run the TFLite model have low computational power. **Code from tflite_convert** ``` def saved_model_to_tflite_convert(saved_model_dir, save_path_tflite):     converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)     converter.optimizations = [tf.lite.Optimize.DEFAULT]     converter.target_spec.supported_types = [tf.float16]      tflite_model = converter.convert()     with open(save_path_tflite, ""wb"") as f:         f.write(tflite_model) ``` **Attached image** Each row belong to a different image (input of the model). !Image",2025-03-19T14:37:18Z,stat:awaiting response stale comp:lite subtype:windows TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89556,"Hi,   I apologize for the delay in my response, I see you have mentioned `TensorFlow version 2.9.3` which is older version if possible could you please try with latest `TensorFlow version 2.19.0` and see is it resolving your issue or not ? If issue still persists with latest TensorFlow version 2.19.0, I see `EfficientNet` models have documented compatibility problems with `float16` precision https://github.com/tensorflow/tensorflow/issues/50171 Unlike custom layerbylayer models (which you mentioned work fine) EfficientNet's architecture contains complex interactions between normalization layers and activation functions that may not translate well to reduced precision The preprocessing logic built into `EfficientNet` models might be handled differently between the Keras implementation and the TFLite conversion. This could create inconsistencies in how input data is normalized before being processed by the actual model. I would suggest you to try alternative quantization approaches like dynamic range quantization for detailed information please refer this official documentation for other quantization schemes: ``` converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) converter.optimizations = [tf.lite.Optimize.DEFAULT] tflite_model = converter.convert() ``` If I have missed something here please let me know. Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
copybara-service[bot],[xla:cpu] Disable SVE LLVM codegen by default on AArch64 CPUs.,"[xla:cpu] Disable SVE LLVM codegen by default on AArch64 CPUs. There are many missing SVE lowerings in LLVM, especially for bf16 type. This causes program termination on SVEavailable machines such as Google Axion. Example error:  ``` LLVM ERROR: Cannot select: 0xf7eb74164950: nxv4bf16 = AArch64ISD::UINT_TO_FP_MERGE_PASSTHRU 0xf7eb74186d60, 0xf7eb74186200, undef:nxv4bf16   0xf7eb74186d60: nxv4i1 = AArch64ISD::PTRUE TargetConstant:i32     0xf7eb74191580: i32 = TargetConstant   0xf7eb74186200: nxv4i32,ch = load) from %ir.scevgep16, !noalias !5), zext from nxv4i8> 0xf7eb740d8830, 0xf7eb74194070, undef:i64     0xf7eb74194070: i64 = add 0xf7eb741922f0, 0xf7eb74193c10       0xf7eb741922f0: i64,ch = CopyFromReg 0xf7eb740d8830, Register:i64 %7         0xf7eb74191e90: i64 = Register %7       0xf7eb74193c10: i64,ch = CopyFromReg 0xf7eb740d8830, Register:i64 %11         0xf7eb74192750: i64 = Register %11     0xf7eb74186350: i64 = undef   0xf7eb74164a30: nxv4bf16 = undef In function: convert.2_kernel Fatal Python error: Aborted ``` Since most AArch64 machines still use 128bit registers, SVE and NEON shouldn't have significant performance difference, so we disable SVE codegen in public builds for the time being. After JAX uses an XLA commit that has changes from this PR, the following JAX tests will pass on Axion: ``` bazel test //tests:shape_poly_test_cpu test_filter=*test_harness_vmap_convert_element_type_dtypes_to_dtypes_shape_bool_100_100_olddtype_bool_newdtype_bfloat16* bazel test //tests:export_harnesses_multi_platform_test_cpu test_filter=*test_prim_convert_element_type_dtypes_to_dtypes_shape_uint8_100_100_olddtype_uint8_newdtype_bfloat16*  ``` Add `test_env=XLA_FLAGS=xla_cpu_max_isa=""""` to the options to get the errors back.",2025-03-19T14:37:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89555
copybara-service[bot],[XLA:GPU] move instruction renaming to a separate pass from priority fusion,[XLA:GPU] move instruction renaming to a separate pass from priority fusion Why: 1. Smaller function. 2. Previously priority fusion pass claimed that it has not modified the HLO while it at least renamed the instructions.,2025-03-19T14:34:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89554
copybara-service[bot],[XLA:GPU][Triton] Remove sparsity code.,[XLA:GPU][Triton] Remove sparsity code. It's unused but causes significant burden during Triton integrates.,2025-03-19T14:19:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89553
copybara-service[bot],Replace erase-remove idiom with `std::erase_if`.,Replace eraseremove idiom with `std::erase_if`.,2025-03-19T14:17:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89552
copybara-service[bot],[XLA:GPU] Add support for cublas calls and triton gemms in matmul interpolation.,[XLA:GPU] Add support for cublas calls and triton gemms in matmul interpolation. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23619 from serach24:chenhao/fix_nvsl_check_failed 826535264624142999164132f3a873be06019279,2025-03-19T12:56:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89551
copybara-service[bot],[XLA:GPU] Remove unused `stream_id` parameter.,[XLA:GPU] Remove unused `stream_id` parameter.,2025-03-19T12:33:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89550
copybara-service[bot],[XLA:CPU] Expose IrCompiler IR & MC passes as separate methods.,[XLA:CPU] Expose IrCompiler IR & MC passes as separate methods.,2025-03-19T12:11:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89549
copybara-service[bot],Add missing include for mlir::Builder,Add missing include for mlir::Builder,2025-03-19T11:15:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89548
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T11:10:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89547
copybara-service[bot],[XLA:CPU] Reuse output memory for aliased inputs in benchmarking,[XLA:CPU] Reuse output memory for aliased inputs in benchmarking Benchmarking HLOs that alias inputs would fail with this library. The change allows aliasing by reusing the memory from the execution outputs.,2025-03-19T10:57:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89546
copybara-service[bot],Reverts 31fcdfb72f6ee23668fb9c30de8f91792d522d48,Reverts 31fcdfb72f6ee23668fb9c30de8f91792d522d48,2025-03-19T09:49:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89545
copybara-service[bot],PR #23799: Add support for emitting DUS as memcpy.,"PR CC(TFLite can not set input shape and input_stats with 1.0/128): Add support for emitting DUS as memcpy. Imported from GitHub PR https://github.com/openxla/xla/pull/23799 This extends the dynamic memcpy support for dynamicslice to dynamicupdateslice. The logic is largely the same (except this time, the destination offset is dynamic, not the source offset). For now, only inplace DUSes are handled. We could handle outofplace DUSes as well, but I don't think it's needed. Copybara import of the project:  ae5f322ce581a75cf0d4cb3b9b1d2513b497e421 by Johannes Reifferscheid : Add support for emitting DUS as memcpy. This extends the dynamic memcpy support for dynamicslice to dynamicupdateslice. The logic is largely the same.  efa584137f03d1de4a04ee07f5fa65a10829d73b by Johannes Reifferscheid : Address review comments. Merging this change closes CC(TFLite can not set input shape and input_stats with 1.0/128) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23799 from jreiffers:memcpy efa584137f03d1de4a04ee07f5fa65a10829d73b",2025-03-19T09:26:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89544
copybara-service[bot],PR #23786: Multihost HLO runner: fix --while_execution_count behavior.,"PR CC(Update graph_transformations.h): Multihost HLO runner: fix while_execution_count behavior. Imported from GitHub PR https://github.com/openxla/xla/pull/23786 Optimized HLO can have known_trip_count while loop annotation, which has to be updated using WhileLoopTripCountAnnotator after HloControlFlowFlattening to apply the value of while_execution_count at runtime. Copybara import of the project:  5648a58d78726f116a64030f48dd63835818fbf7 by Ilia Sergachev : [NFC] Factor common test logic out into a function.  21d4e89aab6fc1dfe66edde6d11eabaec5ea6a84 by Ilia Sergachev : Multihost HLO runner: fix while_execution_count behavior. Optimized HLO can have known_trip_count while loop annotation, which has to be updated using WhileLoopTripCountAnnotator after HloControlFlowFlattening to apply the value of while_execution_count at runtime.  fd93a631e6b818df7d98f29a55b53a8819eae9a3 by Ilia Sergachev : Address review request. Merging this change closes CC(Update graph_transformations.h) Reverts b68ce06d697405d1a911babef312dd80513edd79 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23786 from openxla:fix_while_execution_count fd93a631e6b818df7d98f29a55b53a8819eae9a3",2025-03-19T09:25:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89543
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T08:44:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89542
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T08:21:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89541
zhouxiaoyaozzz,Compilation error（The call method of the StreamingModel class is not implemented correctly）," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Is this a new compilation error？The call method of the StreamingModel class is not implemented correctly, causing TensorFlow/Karas to be unable to infer the output shape and data type of the model.  Standalone code to reproduce the issue ```shell import tensorflow as tf  定义 CircularBufferLayer class CircularBufferLayer(tf.keras.layers.Layer):     def __init__(self, num_features, buffer_size, stride, **kwargs):         super().__init__(**kwargs)         self.num_features = num_features         self.buffer_size = buffer_size         self.stride = stride         self.gradient_scale = 0.1         self.buffer = self.add_weight(name='buffer', shape=(1, buffer_size, num_features),                                       initializer='zeros', trainable=False, dtype=tf.float32)         self.call_count = self.add_weight(name='call_count', shape=(), initializer='zeros',                                           dtype=tf.int32, trainable=False)         self.total_call_count = self.add_weight(name='total_call_count', shape=(), initializer='zeros',                                                 dtype=tf.int32, trainable=False)     def call(self, inputs):         scaled_input = tf.multiply(inputs, self.gradient_scale)         new_buffer = tf.concat([scaled_input, self.buffer[:, :1]], axis=1)         self.buffer.assign(new_buffer)         return self.buffer  定义 StreamingModel class StreamingModel(tf.keras.Model):     def call(self, inputs):         x, _ = super().call(inputs)   假设另一个分支被截断         return x  实例化模型 buffer_layer = CircularBufferLayer(num_features=64, buffer_size=10, stride=1) model = StreamingModel()  定义输入形状 input_shape = (None, 10, 64)   (batch_size, sequence_length, num_features) inputs = tf.keras.Input(shape=input_shape[1:])   忽略 batch_size outputs = model(inputs)  构建模型 model = tf.keras.Model(inputs=inputs, outputs=outputs)  编译模型 model.compile(     optimizer='adam',   使用 Adam 优化器     loss='mse',        使用均方误差作为损失函数     metrics=['mae']    使用平均绝对误差作为评估指标 )  准备训练数据 x_train = tf.random.normal((100, 10, 64))   100 个样本，每个样本的形状为 (10, 64) y_train = tf.random.normal((100, 10, 64))   100 个样本，每个样本的形状为 (10, 64)  训练模型 history = model.fit(     x_train, y_train,     batch_size=32,   批量大小     epochs=10,       训练轮数     validation_split=0.2   使用 20% 的数据作为验证集 )  保存模型 model.save('streaming_model.h5') ```  Relevant log output ```shell D:\project\DLComplier\.venv\Scripts\python.exe D:\project\DLComplier\.venv\statiblity.py  20250319 16:09:45.545622: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250319 16:09:46.283611: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250319 16:09:48.149403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. WARNING:tensorflow:From D:\project\DLComplier\.venv\Lib\sitepackages\keras\src\backend\tensorflow\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. Traceback (most recent call last):   File ""D:\project\DLComplier\.venv\statiblity.py"", line 37, in      outputs = model(inputs)               ^^^^^^^^^^^^^   File ""D:\project\DLComplier\.venv\Lib\sitepackages\keras\src\utils\traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""D:\project\DLComplier\.venv\statiblity.py"", line 27, in call     x, _ = super().call(inputs)   假设另一个分支被截断            ^^^^^^^^^^^^^^^^^^^^ NotImplementedError: Exception encountered when calling StreamingModel.call(). Could not automatically infer the output shape / dtype of 'streaming_model' (of type StreamingModel). Either the `StreamingModel.call()` method is incorrect, or you need to implement the `StreamingModel.compute_output_spec() / compute_output_shape()` method. Error encountered: Model StreamingModel does not have a `call()` method implemented. Arguments received by StreamingModel.call():   • args=('',)   • kwargs= ```",2025-03-19T08:11:56Z,type:bug TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/89540
copybara-service[bot],Add `TfrtGpuClient::Execute()` method,Add `TfrtGpuClient::Execute()` method,2025-03-19T06:56:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89539
copybara-service[bot],Add dependency on `//cloud/bigstore/util:bigstore_file_register` to fix `FileFactory not found for pattern: /bigstore` error.,Add dependency on `//cloud/bigstore/util:bigstore_file_register` to fix `FileFactory not found for pattern: /bigstore` error.,2025-03-19T06:27:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89538
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T06:15:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89537
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T05:58:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89536
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T05:52:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89535
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T05:52:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89534
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T05:51:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89533
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T05:50:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89532
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T05:49:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89531
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T05:45:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89530
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T05:42:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89529
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T05:40:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89528
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T05:39:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89527
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T05:31:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89526
copybara-service[bot],Mark the variants of `Client::AssembleArrayFromSingleDeviceArrays()` without `DType` or `SingleDeviceShardSemantics` as deprecated,Mark the variants of `Client::AssembleArrayFromSingleDeviceArrays()` without `DType` or `SingleDeviceShardSemantics` as deprecated,2025-03-19T04:56:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89525
copybara-service[bot],Mark `Array::DisassembleIntoSingleDeviceArrays` without `SingleDeviceShardSemantics` as deprecated,Mark `Array::DisassembleIntoSingleDeviceArrays` without `SingleDeviceShardSemantics` as deprecated,2025-03-19T04:33:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89524
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T04:15:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89523
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T04:12:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89522
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T04:10:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89521
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T04:05:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89520
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T04:04:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89519
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T03:59:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89518
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-19T03:21:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89517
copybara-service[bot],"Adds two new methods to the `ifrt::Client` interface: (1) `CreateContext` allows the user to create a `UserContext` object that captures the current runtime context that may include data such as the current call stack. This context can then be passed to other IFRT operations so they can be tagged by the client context that triggered them, and this is expected to substantially simplify the performance analysis and debugging process. (2) A variant of `MakeArrayFromHostBuffer` that makes use of this and accepts a UserContext as the last parameter.","Adds two new methods to the `ifrt::Client` interface: (1) `CreateContext` allows the user to create a `UserContext` object that captures the current runtime context that may include data such as the current call stack. This context can then be passed to other IFRT operations so they can be tagged by the client context that triggered them, and this is expected to substantially simplify the performance analysis and debugging process. (2) A variant of `MakeArrayFromHostBuffer` that makes use of this and accepts a UserContext as the last parameter.",2025-03-19T02:38:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89516
copybara-service[bot],[XLA:PJRT] Widen visibility of :transpose so it can be used by JAX.,[XLA:PJRT] Widen visibility of :transpose so it can be used by JAX.,2025-03-19T02:22:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89515
copybara-service[bot],[JAX] Move py_client_gpu into JAX.,"[JAX] Move py_client_gpu into JAX. This callback functionality is only used by JAX and shipped as part of its CUDA and ROCM GPU plugins. Move it into JAX, as part of a wider move of xla/python pieces that belong to JAX into JAX.",2025-03-19T02:01:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89514
copybara-service[bot],[JAX] Use `xla::ifrt::Client::MakeArraysFromHostBufferShards()` in Array creation when possible,"[JAX] Use `xla::ifrt::Client::MakeArraysFromHostBufferShards()` in Array creation when possible This changes makes use of the new `xla::ifrt::Client::MakeArraysFromHostBufferShards()` API when possible. This API needs a single call to create a multishard IFRT Array (to be wrapped as a JAX `PyArray`), which provides more optimization opportunities for the runtime than creating singledevice IFRT Arrays and then assembling them. Please note that `xla::ifrt::Client::MakeArraysFromHostBufferShards()` implementation in PjRtIFRT is not yet optimized, so there is no immediate performance benefits for McJAX. As an exception, it takes the previous path of array assembly if any shard for `BatchedDevicePut` is not a host buffer, but already a singledevice array, because `xla::ifrt::Client::MakeArraysFromHostBufferShards()` works only if all the sharded input to be host buffers. With batching possible at IFRT level, we now skip `DevicePutResultFn` step; `DevicePut` (now `DevicePutWithDevice` and `DevicePutWithSharding`) internally calls pershard functions (with GIL released) and returns a final IFRT Array. This change includes a code cleanup for `xla::DevicePutResult::owning_pybuffer`, which was originally intended to hold a Python object to keep an IFRT Array valid when it is created from `DevicePut()` implementations, but this role has been entirely covered by `on_done_with_host_buffer` function supplied at IFRT Array creation time.",2025-03-19T01:59:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89513
copybara-service[bot],"Reshard should return all partitions in the shard slice, not just first.","Reshard should return all partitions in the shard slice, not just first. Adds errors checks to input shard info string",2025-03-19T00:59:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89512
copybara-service[bot],Vis changes for prepare lib,Vis changes for prepare lib,2025-03-19T00:42:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89511
copybara-service[bot],Fix path to .so issues in qnn. If shared_library_dir is not set by user it gets defaulted to dispatch_library_dir (I can't find where) which may not be the correct location.,Fix path to .so issues in qnn. If shared_library_dir is not set by user it gets defaulted to dispatch_library_dir (I can't find where) which may not be the correct location. Just _append_ to env paths and dlopen by name.,2025-03-19T00:39:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89510
copybara-service[bot],[XLA:Python] Open visibility for several targets to :friends.,"[XLA:Python] Open visibility for several targets to :friends. I intend to use these from JAX in a future change, in the course of moving some code from XLA into JAX.",2025-03-19T00:28:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89509
copybara-service[bot],[xla:cpu] fusion emitter: get min alignment from xla::cpu::MinAlign(),[xla:cpu] fusion emitter: get min alignment from xla::cpu::MinAlign() Instead of hardcoding it to 32.,2025-03-19T00:05:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89508
copybara-service[bot],[re-land][xla:cpu] enable scatter fusion emitter,"[reland][xla:cpu] enable scatter fusion emitter Known issues:  bf16 performance is poor. This is because in the emitters we are missing an optimization that we have in XLA thunks. We will fix this soon.  No parallel scatter. We are leaving this as future work since the singlethreaded implementation is already bringing significant performance improvements. Scatter microbenchmarks: ```                                                                            │      thunks      │           scatter                   │                                                                            │    cpusec/op    │  cpusec/op   vs base               │ BM_ScatterS32_R1/262144/262144/process_time                                       590.2µ ± 0%   203.0µ ±  1%  65.61% (p=0.002 n=6) BM_ScatterS32_R2/512/512/process_time                                             78.61µ ± 2%   49.90µ ±  3%  36.52% (p=0.002 n=6) BM_ScatterS32_R3/64/64/process_time                                               54.68µ ± 2%   50.74µ ±  3%   7.21% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:1/d1:64/d2:8/num_slices:1/process_time            696.2n ± 2%   680.3n ±  4%   2.29% (p=0.026 n=6) BM_SimpleScatterReduceF32_R3/d0:50/d1:64/d2:8/num_slices:10/process_time         106.92µ ± 9%   17.73µ ±  3%  83.42% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:500/d1:64/d2:8/num_slices:100/process_time       10.351m ± 0%   2.584m ±  2%  75.04% (p=0.002 n=6) BM_SelectAndScatterF32/128/process_time                                           36.18µ ± 3%   27.78µ ±  0%  23.21% (p=0.002 n=6) BM_SelectAndScatterF32/256/process_time                                          117.37µ ± 1%   86.42µ ±  0%  26.37% (p=0.002 n=6) BM_SelectAndScatterF32/512/process_time                                           1.438m ± 4%   1.487m ± 31%   +3.40% (p=0.041 n=6) geomean                                                                           131.7µ        72.76µ        44.76% ``` The gap from a few days ago was wider (geomean improvement of 76%), but the recent work on improving performance of small while loops (https://github.com/openxla/xla/commit/db734148ec74) narrowed that to the numbers above. Legacy emitters (""nothunks"") compile all while loops, and are therefore a tougher baseline to compare against. Still, scatter fusion emitters are faster (all singlethreaded): ```                                                                            │   nothunks       │          scatter                    │                                                                            │    cpusec/op    │  cpusec/op   vs base               │ BM_ScatterS32_R1/262144/262144/process_time                                      301.6µ ±  0%   203.0µ ±  1%  32.70% (p=0.002 n=6) BM_ScatterS32_R2/512/512/process_time                                            50.53µ ±  1%   49.90µ ±  3%        ~ (p=0.180 n=6) BM_ScatterS32_R3/64/64/process_time                                              50.12µ ±  1%   50.74µ ±  3%   +1.23% (p=0.009 n=6) BM_SimpleScatterReduceF32_R3/d0:1/d1:64/d2:8/num_slices:1/process_time           593.0n ±  1%   680.3n ±  4%  +14.72% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:50/d1:64/d2:8/num_slices:10/process_time         17.46µ ±  1%   17.73µ ±  3%        ~ (p=0.093 n=6) BM_SimpleScatterReduceF32_R3/d0:500/d1:64/d2:8/num_slices:100/process_time       2.731m ±  1%   2.584m ±  2%   5.38% (p=0.002 n=6) BM_SelectAndScatterF32/128/process_time                                          28.95µ ±  2%   27.78µ ±  0%   4.04% (p=0.002 n=6) BM_SelectAndScatterF32/256/process_time                                          97.36µ ±  1%   86.42µ ±  0%  11.23% (p=0.002 n=6) BM_SelectAndScatterF32/512/process_time                                          1.211m ± 32%   1.487m ± 31%        ~ (p=0.065 n=6) geomean                                                                          74.85µ         72.76µ         2.79% ``` Reverts b68ce06d697405d1a911babef312dd80513edd79",2025-03-19T00:05:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89507
copybara-service[bot],[xla][emitters] preserve alignment attribute when lowering TransferRead,"[xla][emitters] preserve alignment attribute when lowering TransferRead Otherwise we can end up generating code that assumes the source vector is ABIaligned, whereas it might not be. This can lead to crashes.",2025-03-19T00:03:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89506
copybara-service[bot],litert: Use libLiteRtRuntimeCApi.so for Kotlin JNI libs,"litert: Use libLiteRtRuntimeCApi.so for Kotlin JNI libs We shouldn't mix static and dynamic library of LiteRT. For Kotlin, we'll only use dynamic library version of LiteRT.",2025-03-18T23:58:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89505
copybara-service[bot],Recommend dimensions().size() over rank() and dimensions_size() in Shape.,Recommend dimensions().size() over rank() and dimensions_size() in Shape.,2025-03-18T23:44:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89504
copybara-service[bot],Reorder `BroadcastToOp` -> `CastOp` to `CastOp` -> `BroadcastToOp`,Reorder `BroadcastToOp` > `CastOp` to `CastOp` > `BroadcastToOp`,2025-03-18T23:44:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89503
copybara-service[bot],Reorder `BroadcastToOp` with all unary operators except `CastOp`.,"Reorder `BroadcastToOp` with all unary operators except `CastOp`. `CastOp` requires special care as it determines the output type from the output tensor, so we can't use graph transformation rules directly.",2025-03-18T23:23:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89502
copybara-service[bot],Add int4 support,Add int4 support,2025-03-18T23:23:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89501
copybara-service[bot],Use `.default.bzl` naming scheme consistently,Use `.default.bzl` naming scheme consistently,2025-03-18T23:16:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89500
copybara-service[bot],Reverts ae9e1e20db07d2eff953b870eaaa505154e2eae1,Reverts ae9e1e20db07d2eff953b870eaaa505154e2eae1,2025-03-18T23:08:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89499
copybara-service[bot],PR #71960: [oneDNN] Refine oneDNN test cases to enable them with stock TF,PR CC([oneDNN] Refine oneDNN test cases to enable them with stock TF): [oneDNN] Refine oneDNN test cases to enable them with stock TF Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/71960 Refine oneDNN test cases by removing ifdef ENABLE_MKL constraint so that these tests run without config=mkl. That is to enable those test case with stock TF.  Copybara import of the project:  d264a6d0a08282ed586221478d8d8552da0699ff by guozhong.zhuang : [oneDNN] Refine oneDNN test cases to enable them with stock TF  70308f350dead6da2455a9369bcc1664a2bbcd8d by guozhong.zhuang : add header dependance for pooling op GPU tests Merging this change closes CC([oneDNN] Refine oneDNN test cases to enable them with stock TF) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/71960 from Inteltensorflow:refine_mkl_testcases_3 fdcf7afcc4f326468b24d137915e456228182bd4,2025-03-18T22:31:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89498
copybara-service[bot],Style improvements to `primitive_util`:,"Style improvements to `primitive_util`:  `CastPreservesValues()` is too complex to be a good inline function candidate. Move its implementation to `.cc` to avoid cluttering the header file.  The name `IsCanonicalRepresentation(primitive_type)` is misleading. It returns true if T can losslessly represent all values of the given type. E.g. even though the canonical representation of `U8` is `uint8_t`, `IsCanonicalRepresentation(U8)` still returns true. Rename the function to avoid confusion.",2025-03-18T22:24:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89497
copybara-service[bot],Fix sharding_util_ops_test dependency.,Fix sharding_util_ops_test dependency.,2025-03-18T22:06:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89496
copybara-service[bot],Improve documentation of primitive type utilities.,Improve documentation of primitive type utilities. Also make some small style improvements.,2025-03-18T22:05:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89495
copybara-service[bot],Don't test `python/data/...` as it's unrelated to XLA and flaky,Don't test `python/data/...` as it's unrelated to XLA and flaky Flake here: https://github.com/openxla/xla/actions/runs/13933375649/job/38995760034,2025-03-18T21:52:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89494
copybara-service[bot],litert: Update `run_model` to work with GPU Accelerator,litert: Update `run_model` to work with GPU Accelerator,2025-03-18T21:48:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89493
copybara-service[bot],litert: Update a way to use statically linked GPU accelerator,litert: Update a way to use statically linked GPU accelerator Used class constructor instead of weak function.,2025-03-18T21:32:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89492
copybara-service[bot],"Adds two new methods to the `ifrt::Client` interface: (1) `CreateContext` allows the user to create a `UserContext` object that captures the current runtime context that may include data such as the current call stack. This context can then be passed to other IFRT operations so they can be tagged by the client context that triggered them, and this is expected to substantially simplify the performance analysis and debugging process. (2) A variant of `MakeArrayFromHostBuffer` that makes use of this and accepts a UserContext as the last parameter.","Adds two new methods to the `ifrt::Client` interface: (1) `CreateContext` allows the user to create a `UserContext` object that captures the current runtime context that may include data such as the current call stack. This context can then be passed to other IFRT operations so they can be tagged by the client context that triggered them, and this is expected to substantially simplify the performance analysis and debugging process. (2) A variant of `MakeArrayFromHostBuffer` that makes use of this and accepts a UserContext as the last parameter. The broader plan is to make all the IFRT operations uniformly accept the UserContext as the last optional parameter.",2025-03-18T21:06:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89491
copybara-service[bot],Add option to propagate scheduling annotation through the data/control dependency chain between pairs of annotated instructions with the same ID.,Add option to propagate scheduling annotation through the data/control dependency chain between pairs of annotated instructions with the same ID.,2025-03-18T21:04:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89490
copybara-service[bot],[XLA] Adds a helper function for copying original value,[XLA] Adds a helper function for copying original value,2025-03-18T21:01:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89489
copybara-service[bot],PR #23320: [NVIDIA GPU] Add utility functions for multi-host fast-interconnect domain,"PR CC([Feature Request] Addition of new operation to Tensorflow Lite for ""ENet""): [NVIDIA GPU] Add utility functions for multihost fastinterconnect domain Imported from GitHub PR https://github.com/openxla/xla/pull/23320 On Blackwell and onwards, multiple hosts may be connected by NVLink thus creating a fastinterconnect domain beyond a single node. This PR leverages NVML APIs to detect the actual fastinterconnect domain. Followup PRs will update the logic in PjRt client on top of these utility functions. Copybara import of the project:  ee084714876336243fa3ba52a857d2d41124b730 by Terry Sun : pipe nvml to detect nvl  36e1f71dfabb75dbd0853355a7be8c0d6d103903 by Terry Sun : convert clusterUUID  917eb219a1bdebfc7a12f5849545615c83092c73 by Terry Sun : bake boot id into key  cbd909ab66ccf47f6e140632645fbb4d6c61c7c3 by Terry Sun : use global id and add getter  e487c4718eefe72e77708ffaf2c8cbbf6c979e8e by Terry Sun : use device ordinal and add doc string  2c761da2c04ea39d6cc79314f0dc14efef6d0380 by Terry Sun : merge fabric info into device proto  ed34dfbba07fbc5ffaaa914478381184a90f9cbb by Terry Sun : error handling and cleanup  c62f08691b7e3a59effc0f721948b30baea5b8a8 by Terry Sun : polish doc string  ab9d41e3142a0792859b9d37b99fe3ddfd9bbba8 by Terry Sun : conditional compile  29d857815a55d7df7911625cfeb35047bfbb07cb by Terry Sun : more conditional compile  524b569f1937db51babdaac04b50e9499da81a95 by Terry Sun : fix field designator order  7a0fb0f10be555c32403af4d226e687c650603d2 by Terry Sun : str buffer size  91e28b675b2f25e2b5d6c73732c8dae193b0dff3 by Terry Sun : compute capability condition  48fafb9dc4f44c451ca64bdb8ca15a1678cae398 by Terry Sun : protobuf backward compatibility  90f07569cf64d4d7d957cf4efe16fd2c9142eb4c by Terry Sun : guard cluster uuid size Merging this change closes CC([Feature Request] Addition of new operation to Tensorflow Lite for ""ENet"") FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23320 from terryysun:terryysun/detect_mnnvl 4820e4b53c9a72fe6ee8c08ab29ee640f1697c8d",2025-03-18T20:59:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89488
copybara-service[bot],[XLA:Python] Remove unused :gpu_support module.,[XLA:Python] Remove unused :gpu_support module.,2025-03-18T20:45:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89487
copybara-service[bot],Remove allow_get_default_platform from XLA_FLAGS and set it directly as an env variable.,"Remove allow_get_default_platform from XLA_FLAGS and set it directly as an env variable. Setting it as an XLA_FLAG in the build_defs would prevent a user from adding more flags to a test from the command line. Also, use DeepCopy() so that the build_def isn't using the same env variables across multiple targets.",2025-03-18T20:27:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89486
copybara-service[bot],[XLA:Python] Remove xla_gpu_extension.,"[XLA:Python] Remove xla_gpu_extension. This is no longer needed; one uses the PJRT C API to load the GPU plugin into the regular python extension. Also remove :xla_client_test_gpu, which has never worked in OSS.",2025-03-18T20:21:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89485
copybara-service[bot],Don't let `PrimitiveTypeToNative<TOKEN>` be defined.,"Don't let `PrimitiveTypeToNative` be defined. `PrimitiveTypeToNative` is widely used to map a primitive type to a native type in XLA. In general, it's surprising for `PrimitiveTypeToNative` to be defined as `TOKEN` doesn't really have a corresponding native type (e.g. there's no corresponding reverse mapping in `NativeToPrimitiveType`). For example, `PrimitiveTypeToNative` is undefined and we should make `TOKEN` behave consistently with `OPAQUE_TYPE`. In the FFI context, however, we want to implement a `TOKEN` as a `void*`. This is a design choice of the FFI library, and shouldn't be forced on all of XLA. Therefore we remove the definition of `PrimitiveTypeToNative` and handle the `TOKEN` specialization in FFI instead.",2025-03-18T20:15:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89484
copybara-service[bot],Use non-blocking NCCL communicators.,"Use nonblocking NCCL communicators. Sort of. This CL doesn't actually switch to nonblocking communicators, but it gets us ready to make the switch.  Background and Motivation If one process participating in a NCCL collective (e.g., AllReduce) fails, all other participating processes block forever. I am currently working on detecting such failures and unblocking the stuck processes. There are two types of NCCL communicators: *blocking* and *nonblocking*. We currently use blocking communicators. NCCL has a way to abort a communicator via [`ncclCommAbort`][abort], but through communication with the NVIDIA team, we learned that it is unsafe to abort blocking communicators. This CL switches to using nonblocking communicators. In the future, I'll add code to abort stuck collectives.  NonBlocking Communicators Consider a blocking NCCL communicator: ``` ncclComm_t comm = ...; ``` If you call a collective on this communicator, it might block forever: ``` ncclAllReduce(..., comm, ...); ``` However, if the communicator is nonblocking, the call to `ncclAllReduce` will return almost immediately. If it returns immediately, how do you know when the all reduce is done? You have to repeatedly call the [`ncclCommGetAsyncError`][async_error] function. This function returns `inProgress` until the collective is no longer in progress. ``` // Loop until the collective being performed on comm is no longer in progress. ncclResult_t state = inProgress; while (state == inProgress) {     XLA_NCCL_RETURN_IF_ERROR(ncclCommGetAsyncError(comm, &state)); } return XLA_NCCL_STATUS(state); ``` This CL performs this polling loop after every nonblocking call. By doing so, the code acts blocking even though we are using nonblocking communicators. This will allow us to abort the collectives in a future CL without having to radically redesign the code to handle asynchrony.  Groups NCCL has a concept of [groups][groups], which interacts in subtle ways with nonblocking communicators. If you perform operations on a nonblocking communicator in a group, the operations are not performed until the group ends. ``` ncclGroupStart(); // start the group ncclAllReduce(..., comm, ...); // nothing happens ncclAllReduce(..., comm, ...); // nothing happens ncclAllReduce(..., comm, ...); // nothing happens ncclGroupEnd(); // end the group; all operations are now executed ``` Thus, when a group ends, we have to perform the `ncclCommGetAsyncError` polling loop on every communicator that was part of the group. Keeping track of this set of communicators requires a bit of bookkeeping in the code, as implemented in `NcclCollectives::GroupStart` and `NcclCollectives::GroupEnd`. This bookkeeping is complicated by the fact that groups are a threadlocal construct. Every thread has its own group.  Thread Safety Nonblocking communicators also have very odd threadsafety properties. You cannot *concurrently* access a nonblocking `ncclComm_t` from multiple threads. This makes sense. However, you also cannot *serially* access a nonblocking `ncclComm_t` from multiple threads. This is odd. Currently, our code caches `ncclComm_t`s and does execute operations on them from multiple threads serially. This leads to undefined behavior which manifests (anecdotally) as segfaults and deadlocks. To avoid these issues, this CL continues to use blocking communicators. The code can be switched to use nonblocking communicators with a single line change. Thankfully, the code for nonblocking communicators works equally as well for blocking communicators. In the future, I will fix these thread safety issues and enabled nonblocking communicators. [abort]: https://docs.nvidia.com/deeplearning/nccl/userguide/docs/api/comms.htmlncclcommabort [async_error]: https://docs.nvidia.com/deeplearning/nccl/userguide/docs/api/comms.htmlncclcommgetasyncerror [groups]: https://docs.nvidia.com/deeplearning/nccl/userguide/docs/usage/groups.html FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89621 from fujunwei:undefine_VK_USE_PLATFORM_XLIB_KHR cbaad53e5332b758093b20c43e7aaf35a3c456d2",2025-03-18T20:06:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89483
copybara-service[bot],"Don't assign `Build` instances to variables, they are registered with the class during `__post_init__` and these vars are not used","Don't assign `Build` instances to variables, they are registered with the class during `__post_init__` and these vars are not used Old way is a holdover from when the registration mechanism (if we can even call it that :)) was different.",2025-03-18T19:43:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89482
copybara-service[bot],[XLA] Remove `--log_output` and use ARM64 T2A machines to run CPU workflows,[XLA] Remove `log_output` and use ARM64 T2A machines to run CPU workflows,2025-03-18T19:42:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89481
AYUHSPATIL,Fix doc bug in tf.keras.losses.SparseCategoricalCrossentropy,"Fixes CC(tf.keras.losses.SparseCategoricalCrossentropy has logical/Doc bug) This PR enhances the error handling for invalid values of the reduction parameter. Previously, if reduction was None or an incorrect value (including variations like ""None"", ""Auto"", ""Sum"", etc.), the error message was not specific enough, making it difficult for users to understand what went wrong. If the value does not match exactly (casesensitive), a clear error message is raised, specifying the invalid value and listing the expected options. This improves usability by guiding the user toward the correct values. Impact: 1. Improves error clarity for developers using the reduction parameter. 2. Ensures better debugging by explicitly pointing out incorrect values. 3. Reduces confusion caused by incorrect casing or unexpected inputs.  For more details, open the Copilot Workspace session.",2025-03-18T18:47:30Z,size:XL invalid python,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89480,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , can you please sign the CLA? Many thanks!",This seems like AI generated content which goes against contributors guidelines and could be considered spam.
copybara-service[bot],Build the root module for mlir python bindings within tflite converter.,Build the root module for mlir python bindings within tflite converter.,2025-03-18T18:30:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89479
copybara-service[bot],Integrate LLVM at llvm/llvm-project@0230d63b4a8b,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 0230d63b4a8b,2025-03-18T17:51:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89478
copybara-service[bot],Move plugin/tensorboard_plugin_profile/protobuf into xprof/protobuf,Move plugin/tensorboard_plugin_profile/protobuf into xprof/protobuf This simplifies paths and prevents Tensorflow from having to reach into the plugin.,2025-03-18T17:38:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89477
copybara-service[bot],"Add a builder for `TFL_DequantizeOp`, to allow usage in graph patterns.","Add a builder for `TFL_DequantizeOp`, to allow usage in graph patterns.",2025-03-18T17:32:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89476
copybara-service[bot],Migrate PJRT XLA lowering to use StableHLO->HLO APIs,Migrate PJRT XLA lowering to use StableHLO>HLO APIs,2025-03-18T17:15:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89475
copybara-service[bot],Set `USE_PYWRAP_RULES` in `build.py` builds of TensorFlow,Set `USE_PYWRAP_RULES` in `build.py` builds of TensorFlow This change keeps the XLA builds consistent with TensorFlow's CI and fixes test failures seen without this set.,2025-03-18T17:14:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89474
copybara-service[bot],Fix JIT compilation test cases,Fix JIT compilation test cases,2025-03-18T16:57:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89473
copybara-service[bot],Removed optimized batch_matmul to redirect to XNNPACK.,Removed optimized batch_matmul to redirect to XNNPACK.,2025-03-18T16:32:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89472
copybara-service[bot],Only disable `RTLD_DEEPBIND` for sanitizer runs on systems that have it in the first place.,Only disable `RTLD_DEEPBIND` for sanitizer runs on systems that have it in the first place.,2025-03-18T16:22:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89471
copybara-service[bot],Simplify `ScheduleModule` using `DefaultModuleScheduler` as default `algorithm` value.,Simplify `ScheduleModule` using `DefaultModuleScheduler` as default `algorithm` value.,2025-03-18T16:19:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89470
copybara-service[bot],Keep source location when creating an `ErrorStatusBuilder`.,Keep source location when creating an `ErrorStatusBuilder`. This improves the logging when returning a `LiteRtStatus` to print the correct file and line (which are currently wrong). We add a small standin for `std::source_location` because we are still supporting C++17.,2025-03-18T15:58:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89469
copybara-service[bot],[XLA:CPU] Move IrCompiler specific code out of JitCompiler,[XLA:CPU] Move IrCompiler specific code out of JitCompiler,2025-03-18T15:46:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89468
jdub4asdfg,tf.math.argmin() returning the wrong index," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version I'm doing it google Colab  Custom code Yes  OS platform and distribution Windows x64  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm performing argmin on a tensor but it returns the index of a value that is larger than the minimum. argmax works fine though. Pardon my lack of terminology, I just started learning Tensorflow 2 days ago. !Image  Standalone code to reproduce the issue ```shell import tensorflow as tf random_tensor = tf.constant([[10.39, 12.92], [7.6, 5.53]]) print(random_tensor) print(tf.argmin(random_tensor)) ```  Relevant log output ```shell ```",2025-03-18T15:24:33Z,type:bug,closed,0,2,https://github.com/tensorflow/tensorflow/issues/89467,Are you satisfied with the resolution of your issue? Yes No,Sorry I misunderstood the use of argmin
copybara-service[bot],[xla:cpu] Add initial `xla_cpu_max_isa` flag support for AAarch64.,[xla:cpu] Add initial `xla_cpu_max_isa` flag support for AAarch64. Can limit ISA that XLA:CPU will codegen to NEON and SVE. Fixes https://github.com/openxla/xla/issues/17758,2025-03-18T15:18:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89466
copybara-service[bot],PR #23786: Multihost HLO runner: fix --while_execution_count behavior.,"PR CC(Update graph_transformations.h): Multihost HLO runner: fix while_execution_count behavior. Imported from GitHub PR https://github.com/openxla/xla/pull/23786 Optimized HLO can have known_trip_count while loop annotation, which has to be updated using WhileLoopTripCountAnnotator after HloControlFlowFlattening to apply the value of while_execution_count at runtime. Copybara import of the project:  5648a58d78726f116a64030f48dd63835818fbf7 by Ilia Sergachev : [NFC] Factor common test logic out into a function.  21d4e89aab6fc1dfe66edde6d11eabaec5ea6a84 by Ilia Sergachev : Multihost HLO runner: fix while_execution_count behavior. Optimized HLO can have known_trip_count while loop annotation, which has to be updated using WhileLoopTripCountAnnotator after HloControlFlowFlattening to apply the value of while_execution_count at runtime.  fd93a631e6b818df7d98f29a55b53a8819eae9a3 by Ilia Sergachev : Address review request. Merging this change closes CC(Update graph_transformations.h) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23786 from openxla:fix_while_execution_count fd93a631e6b818df7d98f29a55b53a8819eae9a3",2025-03-18T15:07:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89465
copybara-service[bot],Integrate LLVM at llvm/llvm-project@0230d63b4a8b,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 0230d63b4a8b,2025-03-18T15:04:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89464
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-18T14:54:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89463
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-18T14:53:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89462
copybara-service[bot],[JAX] Update paths to :cpu_kernels and :gpu_kernels targets to their current locations.,"[JAX] Update paths to :cpu_kernels and :gpu_kernels targets to their current locations. No functional changes, this simply replaces uses of deprecated forwarding targets with their recommended replacements.",2025-03-18T14:41:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89461
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-18T14:36:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89460
copybara-service[bot],[XLA:GPU] Remove `xla_gpu_enable_nccl_per_stream_comms` flag.,"[XLA:GPU] Remove `xla_gpu_enable_nccl_per_stream_comms` flag. The flag is set to `false` by default since March 2024 [0] and there are no signs usage since. Handling of the stream id makes GpuCliqueKey more complicated, but since the flag is not covered by tests or benchmark, so it's possible that behaviour is broken. [0] github.com/openxla/xla/pull/10965",2025-03-18T14:28:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89459
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-18T14:27:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89458
copybara-service[bot],"#sdy Don't add unspecified dims when converting sdy::ReshardOp to mhlo::CopyOp after propagation, because it isn't needed by the partitioner","sdy Don't add unspecified dims when converting sdy::ReshardOp to mhlo::CopyOp after propagation, because it isn't needed by the partitioner",2025-03-18T13:58:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89457
copybara-service[bot],#sdy export collective ops in stablehlo export.,sdy export collective ops in stablehlo export. Each collective op is converted to a mhlo::CopyOp so that GSPMD partitioner will respect it even when it's part of a chain of multiple collectives originating from a single reshard.,2025-03-18T13:58:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89456
copybara-service[bot],[XLA:GPU] Turn off `--xla_ignore_channel_id` by default.,"[XLA:GPU] Turn off `xla_ignore_channel_id` by default. Making `channel_id`s match between `Send` and `Recv` ops has been shown to sometimes be loadbearing, which precludes turning this on by default.",2025-03-18T13:48:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89455
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-18T12:55:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89454
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-18T12:35:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89453
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-18T12:33:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89452
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-18T12:29:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89451
copybara-service[bot],[XLA:CPU] Enable disabling runtime calls in DotOpEmitter,[XLA:CPU] Enable disabling runtime calls in DotOpEmitter,2025-03-18T12:10:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89450
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-18T10:47:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89449
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-18T10:40:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89448
copybara-service[bot],[XLA:GPU] Preserve instructions and computations unique ids while cloning a module.,[XLA:GPU] Preserve instructions and computations unique ids while cloning a module. Being able to rely on this behavior let us map a cloned instruction to its original instruction.,2025-03-18T10:26:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89447
copybara-service[bot],Change `PjRtClient::Compile()` to return an unloaded executable.,Change `PjRtClient::Compile()` to return an unloaded executable.,2025-03-18T09:41:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89446
NguyenDanVP,Could not build Objective-C module 'TensorFlowLiteSelectTfOps',"TensorFlowLiteSelectTfOps  Compile error CC(TensorFlowLiteSelectTfOps  Compile error) framework module TensorFlowLiteSelectTfOps {   export *   module * { export * }   link ""dl""   link ""m""   link ""pthread""   link ""z""   link framework ""CoreFoundation"" } Inferred submodules require a module with an umbrella  xocode Version 16.2 :  platform :ios, '16.0' target 'MyApp' do    Use dynamic frameworks (required for TensorFlowLite)   use_frameworks!    Pods for MyApp    pod 'TensorFlowLiteSwift', '~> 2.17.0'  pod 'TensorFlowLiteSelectTfOps', '~> 2.17.0'   post_install do          config.build_settings['IPHONEOS_DEPLOYMENT_TARGET'] = '16.0'       end     end   end end I added : ""force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.xcframework/iosarm64/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps"" to Other linker flags",2025-03-18T09:35:16Z,comp:lite iOS 2.17,open,0,4,https://github.com/tensorflow/tensorflow/issues/89445,"framework module TensorFlowLiteSelectTfOps {   export *   module * { export * }   link ""dl""   link ""m""   link ""pthread""   link ""z""   link framework ""CoreFoundation"" } usually occurs due to incorrect or incomplete CocoaPods configuration, especially with the TensorFlowLiteSelectTfOps pod and its dependencies on specific linker flags and frameworks. Update your Podfile to use exact stable versions pod 'TensorFlowLiteSelectTfOps', '~> 2.17.0' pod 'TensorFlowLite', '~> 2.17.0' pod deintegrate pod install","I installed the latest version 2.17.0 but the result is still the same. > môđun khung TensorFlowLiteSelectTfOps { xuất * môđun * { xuất * } liên kết ""dl"" liên kết ""m"" liên kết ""pthread"" liên kết ""z"" liên kết khung ""CoreFoundation"" } >  > thường xảy ra do cấu hình CocoaPods không chính xác hoặc không đầy đủ, đặc biệt là với pod TensorFlowLiteSelectTfOps và các phụ thuộc của nó vào các cờ liên kết và khung cụ thể. >  > Cập nhật Podfile của bạn để sử dụng phiên bản ổn định chính xác >  > pod 'TensorFlowLiteSelectTfOps', '~> 2.17.0' pod 'TensorFlowLite', '~> 2.17.0' >  > pod hủy bỏ cài đặt pod I installed the latest version 2.17 but the result is still the same.","Hi,   Please take a look into this issue. Thank You","The error I see is still from the previous version, and it persists in this new version as well."
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T09:09:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89444
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T09:02:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89443
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T08:43:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89442
AdithyaSrivastava01,calling exp_on_negative_values() once,"As a reference to ISSUE CC(Fixedpoint Softmax() calls exp_on_negative_values() twice), the essential changes are made to call exp_on_negative_values() only once. The exp is stored in the output_data(temp buffer), and later, the scaling operation is performed on the precomputed exponents. Fixes CC(Fixedpoint Softmax() calls exp_on_negative_values() twice)",2025-03-18T08:42:37Z,stat:awaiting response stale ready to pull size:S comp:lite-kernels,closed,1,7,https://github.com/tensorflow/tensorflow/issues/89441,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",Hey   could you let me know whether the PR is ready to merge or should I make some changes?,"So, the process to get a PR merged is not as simple as for other projects, due to the fact that the internal version of TF also needs to work with other internal projects, etc. See the process as documented here. I cannot check right now, but looking at the statuses here it is likely that the change causes a test or more to fail. Will come back with details when I can get to them",Greetings  is it possible for you to share the potential issues that the code is facing?,"There seems to be numerical errors in the tests. Can you run the tests locally? At least `tensorflow/lite/kernels/activations_test.cc` ``` Value of: m.GetDequantizedOutput() Expected: has 8 elements where element CC(未找到相关数据) float abs rel near (value: 0.09766, max_abs_err: 0.0078125, max_rel_err: 0), element CC(Add support for Python 3.x) float abs rel near (value: 0.05469, max_abs_err: 0.0078125, max_rel_err: 0), element CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"") float abs rel near (value: 0.12109, max_abs_err: 0.0078125, max_rel_err: 0), element CC(JVM, .NET Language Support) float abs rel near (value: 0.14453, max_abs_err: 0.0078125, max_rel_err: 0), element CC(Installation over pip fails to import with protobuf 2.6.1) float abs rel near (value: 0.13281, max_abs_err: 0.0078125, max_rel_err: 0), element CC(Java interface) float abs rel near (value: 0.07813, max_abs_err: 0.0078125, max_rel_err: 0), element CC(Pretrained models) float abs rel near (value: 0.26563, max_abs_err: 0.0078125, max_rel_err: 0), element CC(API docs does not list RNNs) float abs rel near (value: 0.10938, max_abs_err: 0.0078125, max_rel_err: 0)   Actual: { 0, 0, 0, 0, 0, 0, 0, 0 }, whose element CC(未找到相关数据) (0) not (float abs rel near (value: 0.09766, max_abs_err: 0.0078125, max_rel_err: 0)), which is 0.09766 from 0.09766 ```",This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,This PR was closed because it has been inactive for 14 days since being marked as stale. Please reopen if you'd like to work on this further.
copybara-service[bot],[LiteRT] Add metrics related APIs for dispatch delegate and compiled model.,[LiteRT] Add metrics related APIs for dispatch delegate and compiled model.,2025-03-18T08:42:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89440
copybara-service[bot],Make `MakeBatchPointers` use the kernel registry,Make `MakeBatchPointers` use the kernel registry  Moves `MakeBatchPointers` logic into backends/gpu/runtime since it's a runtime component.  Defines trait for the MakeBatchPointers kernel in stream_executor/gpu/  Moves the implementations of this kernel into stream_executor/{cuda|rocm} and registers them with the registry.  Makes `MakeBatchPointers` retrieve the kernel by using the kernel registry.  Add the kernel implementations as dependencies to the `all_runtime` targets for CUDA and ROCm.,2025-03-18T08:38:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89439
vemqos99,FP32 gives good results but int8 predicts only single class," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): 2.15.0  2. Code Provide code to help us reproduce your issues using one of the following options: Standard code to convert from fp32 int8 taken directly from website. The representative dataset is the validation set The model used is Nanodet  for 2 classes. used with images of size 160x160. Each class has atleat 2000 images. Total training set size is 5500. validation set size is 1800. nanodet model converted to onnx then converted to tflite. All fp32 models give the same results as below. int8 model results is skewed. nanodet_detection_results_fp32.txt nanodet_detection_results_int8.txt As you can see from the results, int8 model predicts only single class. In the detection results, 130 is class0, 3160 is class1, rest should not detect and class with high confidence. This is the .py code for int8. for fp32 the items related to quantization is removed import cv2 import numpy as np import tensorflow as tf import math  Thresholds CONF_THRESH = 0.5   Confidence threshold IOU_THRESH = 0.1    IoU threshold for NMS REG_MAX = 7  Define colors for visualization _COLORS = np.array([     [0.000, 0.447, 0.741],   Class 0 (blue)     [0.850, 0.325, 0.098],   Class 1 (orange) ])  Load INT8 TFLite model def load_tflite_model(model_path):     interpreter = tf.lite.Interpreter(model_path=model_path)     interpreter.allocate_tensors()     return interpreter  Preprocess image for INT8 model def preprocess_image(image_path, input_details):     img = cv2.imread(image_path, cv2.IMREAD_COLOR)     orig_shape = img.shape[:2]     img = cv2.resize(img, (160, 160))      Normalize to INT8 scale     input_dtype = input_details[0]['dtype']     input_scale, input_zero_point = input_details[0]['quantization']     img = img.astype(np.float32) / 255.0   Normalize to [0,1]     img = (img / input_scale + input_zero_point).astype(input_dtype)   Quantize     img = np.expand_dims(img, axis=0)   NHWC format     return img, orig_shape  Generate grid center priors def generate_grid_center_priors(input_height, input_width, strides):     center_priors = []     for stride in strides:         feat_w = math.ceil(input_width / stride)         feat_h = math.ceil(input_height / stride)         for y in range(feat_h):             for x in range(feat_w):                 center_priors.append([x, y, stride])     return center_priors  Softmax activation function def activation_function_softmax(src):     alpha = max(src)     denominator = sum(np.exp(src  alpha))     return np.exp(src  alpha) / denominator  Decode distributional predictions to bounding boxes def disPred2Bbox(dfl_det, label, score, x, y, stride):     ct_x = x * stride     ct_y = y * stride     dis_pred = []     for i in range(4):         dis = sum(j * activation_function_softmax(dfl_det[i * (REG_MAX + 1):(i + 1) * (REG_MAX + 1)])[j] for j in range(REG_MAX + 1))         dis *= stride         dis_pred.append(dis)     xmin, ymin = max(ct_x  dis_pred[0], 0), max(ct_y  dis_pred[1], 0)     xmax, ymax = min(ct_x + dis_pred[2], 160), min(ct_y + dis_pred[3], 160)     return [xmin, ymin, xmax, ymax, score, label]  Decode NanoDet output (with INT8 dequantization) def decode_infer(pred, center_priors, score_threshold, output_details, num_classes=2):     output_scale, output_zero_point = output_details[0]['quantization']     pred = (pred.astype(np.float32)  output_zero_point) * output_scale   Dequantize     pred = pred.flatten()     results = {i: [] for i in range(num_classes)}     num_channels = num_classes + (REG_MAX + 1) * 4     for idx, center_prior in enumerate(center_priors):         ct_x, ct_y, stride = center_prior         scores = pred[idx * num_channels:idx * num_channels + num_classes]         cur_label = np.argmax(scores)         score = scores[cur_label]         if score > score_threshold:             bbox_pred = pred[idx * num_channels + num_classes:]             results[cur_label].append(disPred2Bbox(bbox_pred, cur_label, score, ct_x, ct_y, stride))     return results  NonMaximum Suppression def nms(dets, thresh):     if len(dets) == 0:         return []     x1, y1, x2, y2, scores = dets[:, 0], dets[:, 1], dets[:, 2], dets[:, 3], dets[:, 4]     areas = (x2  x1 + 1) * (y2  y1 + 1)     order = scores.argsort()[::1]     keep = []     while order.size > 0:         i = order[0]         keep.append(i)         xx1 = np.maximum(x1[i], x1[order[1:]])         yy1 = np.maximum(y1[i], y1[order[1:]])         xx2 = np.minimum(x2[i], x2[order[1:]])         yy2 = np.minimum(y2[i], y2[order[1:]])         w = np.maximum(0.0, xx2  xx1 + 1)         h = np.maximum(0.0, yy2  yy1 + 1)         inter = w * h         ovr = inter / (areas[i] + areas[order[1:]]  inter)         order = order[np.where(ovr <= thresh)[0] + 1]     return keep  Run inference and visualize results def run_inference(model_path, image_path):     interpreter = load_tflite_model(model_path)     input_details, output_details = interpreter.get_input_details(), interpreter.get_output_details()     image, orig_shape = preprocess_image(image_path, input_details)     interpreter.set_tensor(input_details[0]['index'], image)     interpreter.invoke()     dets = interpreter.get_tensor(output_details[0]['index'])     center_priors = generate_grid_center_priors(160, 160, [8, 16, 32])     preds = decode_infer(dets, center_priors, CONF_THRESH, output_details)     results = []     for key, pred in preds.items():         idxs = nms(np.array(pred), IOU_THRESH)         results.extend(pred[idx] for idx in idxs)      Map bbox to original image size     src_w, src_h = orig_shape[1], orig_shape[0]     img = cv2.imread(image_path)     for result in results:         x1, y1, x2, y2, score, label = result         x1, y1, x2, y2 = int(x1 * src_w / 160), int(y1 * src_h / 160), int(x2 * src_w / 160), int(y2 * src_h / 160)         color = (_COLORS[label] * 255).astype(np.uint8).tolist()         cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)         cv2.putText(img, f""Class {label}: {score:.2f}"", (x1, y1  10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)     cv2.imwrite(""output.jpg"", img)     return results if __name__ == ""__main__"":     model_file = ""nanodet_int8.tflite""     image_file = ""D:\\regressionTest\\data\\carLP\\IMG_008.jpg""     results = run_inference(model_file, image_file)     print(results)",2025-03-18T08:37:59Z,stat:awaiting response stale comp:lite TFLiteConverter TF 2.15,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89438,"Hi,   I apologize for the delay in my response, if possible could you please help us with your Google colab notebook along with models or your Github repo along with models to reproduce the same behavior from our end ? Meanwhile as far I know there might be some root causes like `INT8` quantization compresses the dynamic range of activations and weights. If the representative dataset is not truly representative or if the quantization parameters (scale/zero point) are not set properly one class can be squeezed out and its logits suppressed causing all outputs to favor a single class.If the output tensor is not dequantized correctly (i.e. using the wrong scale/zero point) class scores can be skewed leading to incorrect argmax results `INT8` quantization may change the distribution of logits or scores especially if softmax is applied after dequantization. This can cause one class to always dominate or confidence scores to be outside the expected range, breaking NMS or thresholding logic. If the representative dataset used for calibration is not balanced across classes quantization may favor the more frequent class causing the other class to be invisible to the quantized model I would recommend you to please make sure output values before and after dequantization and ensure that after dequantization the class logits are in a reasonable range (not all negative, not all the same) also compare raw logits, softmax outputs and final predictions between `FP32` and `INT8` models for the same input. consistently higher `INT8` values for a single class suggest a quantization scale problem. Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
copybara-service[bot],Introduce registry for GPU helper kernels,"Introduce registry for GPU helper kernels We have a bunch of helper kernels that are getting called from various parts of the compiler and the runtime (MakeBatchPointers, RedzoneAllocator, RepeatBuffers, etc.). They all need to be implemented separately for CUDA and ROCm and makes the code depend CUDA and ROCm specifics. This code introduces a global registry for these kernels. Users of these kernels can load a kernel via a trait type which also makes loading and registering a kernel typesafe.",2025-03-18T08:11:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89437
copybara-service[bot],Unify implicit dependencies of OSS and internal xla_test macro,Unify implicit dependencies of OSS and internal xla_test macro Both macro implementations used to add different implicit runtime dependency targets. This change is unifying that.,2025-03-18T08:08:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89436
copybara-service[bot],Reverts f5c96c35985a9475ee69fbf4c9c8c056c97ec7cd,Reverts f5c96c35985a9475ee69fbf4c9c8c056c97ec7cd,2025-03-18T07:40:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89435
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T07:05:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89434
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T06:51:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89433
copybara-service[bot],Add CountOnes and IsAllSet function to bitmap,Add CountOnes and IsAllSet function to bitmap,2025-03-18T06:31:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89432
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T06:27:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89431
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T06:25:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89430
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T06:15:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89429
chunhsue,Qualcomm AI Engine Direct - Remove unused tests," Tests `qnn_compiler_plugin_test` ``` [] Global test environment teardown [==========] 115 tests from 5 test suites ran. (4777 ms total) [  PASSED  ] 115 tests. ``` `//tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:all` ``` INFO: Analyzed 4 targets (85 packages loaded, 635 targets configured). INFO: Found 4 test targets... INFO: Elapsed time: 2.164s, Critical Path: 1.56s INFO: 37 processes: 15 internal, 22 local. INFO: Build completed successfully, 37 total actions //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:op_wrapper_test PASSED in 0.0s //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:param_wrapper_test PASSED in 0.0s //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:quantize_params_wrapper_test PASSED in 0.0s //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:tensor_wrapper_test PASSED in 0.0s ```",2025-03-18T06:08:10Z,comp:lite ready to pull size:M,closed,0,1,https://github.com/tensorflow/tensorflow/issues/89428,"Since `OpWrapper` contains more data members than before, we decide to remove copy constructor to prevent copy. This PR will remove the related unit tests. "
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T06:00:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89427
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T05:55:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89426
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T05:48:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89425
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T05:47:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89424
copybara-service[bot],[xla:cpu][oneDNN] Add missing deps for onednn_convolution.,[xla:cpu][oneDNN] Add missing deps for onednn_convolution.,2025-03-18T05:46:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89423
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T05:42:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89422
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T05:40:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89421
copybara-service[bot],fix GPU model execution failures:,"fix GPU model execution failures: 1. MLD failure on cl tensor with layout == BHWC && batch ==1, it need to be HWC layout instead. 2. MLD delegate failure when the graph is partially delegated, the output tensor can not be found in the buffer context  3. mem leak on the GPU global environment creation.",2025-03-18T05:30:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89420
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T05:17:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89419
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T05:04:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89418
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T04:58:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89417
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T04:58:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89416
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T04:57:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89415
copybara-service[bot],Reverts cfe77a65e7200dfa220f9f5b2c5ad4a2e8c19c3d,Reverts cfe77a65e7200dfa220f9f5b2c5ad4a2e8c19c3d,2025-03-18T04:52:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89414
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-18T04:46:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89413
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89005 from jiunkaiy:dev/weilhuan/two_step_tensor_op e04595941f83e8199f1ceb3f20ec3f4a86e549f0,2025-03-18T04:35:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89412
copybara-service[bot],Add BUILD macros to kick off a binary on a local device. Add a few integration test targets that use the macro with the test scaffold and canonical test models.,Add BUILD macros to kick off a binary on a local device. Add a few integration test targets that use the macro with the test scaffold and canonical test models.,2025-03-18T01:39:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89411
copybara-service[bot],Add hbm_read/write_time_noderate to grappler cost.,Add hbm_read/write_time_noderate to grappler cost.,2025-03-18T01:33:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89410
copybara-service[bot],Fix build without OpenCL support,Fix build without OpenCL support,2025-03-18T00:40:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89409
copybara-service[bot],[XLA:GPU/TMA] Migrating legacy fusion emitter to use TritonXLA dialect for Load/Store instead of Triton's equivalents. This is important to allow for TMA optionality later on.,[XLA:GPU/TMA] Migrating legacy fusion emitter to use TritonXLA dialect for Load/Store instead of Triton's equivalents. This is important to allow for TMA optionality later on.,2025-03-17T23:43:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89408
copybara-service[bot],[XLA] Increase github benchmark postsubmit workflow timeout,[XLA] Increase github benchmark postsubmit workflow timeout,2025-03-17T23:39:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89407
copybara-service[bot],Refactor array serialization into separate JAX and tensorstore logic,Refactor array serialization into separate JAX and tensorstore logic Array serialization in array_serialization.py contains a mixture of JAX specific serialization logic and tensorstore driver. This change separates JAX and tensorstore methods (a) making serialization more modular and (b) potentially allowing for alternative array serialization backends in the future. Additional cleanup changes include:  making ocdbt kvstore driver default in tensorstore  robustified array serialization tests especially on multihost  explicit tensorstore array chunking to ensure chunk file size does not blow up,2025-03-17T23:32:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89406
copybara-service[bot],Add extra args placeholder for slice_test and update some test timeouts.,Add extra args placeholder for slice_test and update some test timeouts.,2025-03-17T23:20:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89405
copybara-service[bot],Exclude `python/data/kernel_tests` as they are flaky and not relevant to XLA,Exclude `python/data/kernel_tests` as they are flaky and not relevant to XLA,2025-03-17T22:47:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89404
copybara-service[bot],Move `value_inference_test.py` next to file to improve code coverage and remove unnecessary dependencies.,Move `value_inference_test.py` next to file to improve code coverage and remove unnecessary dependencies.,2025-03-17T22:29:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89403
copybara-service[bot],show device time on the batch table.,show device time on the batch table. Stats for batch size is aggregated from device_trace with batch group_id (similar to how the data is processed in request aggregation).,2025-03-17T22:26:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89402
copybara-service[bot],Implement `BufferFromHostBuffer`. Introduce `HostMemoryAllocator` for staging buffer.,Implement `BufferFromHostBuffer`. Introduce `HostMemoryAllocator` for staging buffer. Implement `logical_on_device_shape()` and `GetReadyFuture` for `TfrtGpuBuffer` More unit tests will be added once D2H is supported.,2025-03-17T22:14:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89401
copybara-service[bot],Create a compiler object on every call of Compiler::GetForPlatform().,"Create a compiler object on every call of Compiler::GetForPlatform(). This is a first step in moving from xla::Compiler instance being a statically constructed singleton. We want to move all registrations to factories instead of singletons, so that every user creates their own compiler instance and sharing of instances becomes explicit. This should prevent race conditions and make code threadsafe.",2025-03-17T22:05:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89400
copybara-service[bot],Adds the notion of a UserContext that allows the IFRT users to associate the user-specific circumstances/context details (such as the stack trace) with the IFRT runtime operations.,Adds the notion of a UserContext that allows the IFRT users to associate the userspecific circumstances/context details (such as the stack trace) with the IFRT runtime operations.,2025-03-17T21:38:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89399
copybara-service[bot],Guard inclusion of more OpenCL headers with LITERT_HAS_OPENCL_SUPPORT,Guard inclusion of more OpenCL headers with LITERT_HAS_OPENCL_SUPPORT,2025-03-17T21:37:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89398
copybara-service[bot],Use `tsl/platform:test_main` for `core/platform:test_main`,Use `tsl/platform:test_main` for `core/platform:test_main` The googletest main doesn't work correctly with benchmarks internally.,2025-03-17T21:32:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89397
copybara-service[bot],Add a test for the optimization-barrier expander pass.,Add a test for the optimizationbarrier expander pass.,2025-03-17T20:44:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89396
copybara-service[bot],Replace a `WARNING` block with `IMPORTANT` in a Markdown doc.,"Replace a `WARNING` block with `IMPORTANT` in a Markdown doc. It wasn't concerned with a potential breakage, only with an important and potentially neglected step.",2025-03-17T19:53:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89395
copybara-service[bot],PR #22563: [XLA:CPU][oneDNN] Scratch allocations for oneDNN Convolutions,PR CC(RuntimeError: TOCO failed see console for info.): [XLA:CPU][oneDNN] Scratch allocations for oneDNN Convolutions Imported from GitHub PR https://github.com/openxla/xla/pull/22563 This PR allocates scratch buffers for oneDNN convolution computations. It also updates the existing tests to verify that the feature is enabled by default. Copybara import of the project:  9f7663267576ef2d698d1dca0ac2dfe913e107a6 by Akhil Goel : Initial commit  c878b112b1e9012b2601a827653caeac050ebdec by Akhil Goel : Address review comments  25dc7fe2ad972ccc89be9aa3b60ffa305846b531 by Akhil Goel : Add missed changes  01077f7b679a78770f6d793106b3a3bc06aadb52 by Akhil Goel : Address review comments  ff1f734f3168846c1179628120803dec98ee5b7d by Akhil Goel : Use type instead of auto for mds  b9d29252882118391bdc91f0c4f5ccc5d6c96c5c by Akhil Goel : Address review comments Merging this change closes CC(RuntimeError: TOCO failed see console for info.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22563 from Inteltensorflow:akhil/conv_scratch b9d29252882118391bdc91f0c4f5ccc5d6c96c5c,2025-03-17T19:53:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89394
copybara-service[bot],Rename `*.hlo` test files to `*_test.hlo`.,Rename `*.hlo` test files to `*_test.hlo`.,2025-03-17T19:52:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89393
copybara-service[bot],Clean up BUILD rules for `generate_hlo_opt_test_checks.py`.,"Clean up BUILD rules for `generate_hlo_opt_test_checks.py`. Suffix the `py_strict_library` target's name with `_lib`, and add a `py_strict_binary` target with the old name.",2025-03-17T19:50:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89392
copybara-service[bot],"In `ruy/workspace.bzl`, refer to BUILD file by correct name","In `ruy/workspace.bzl`, refer to BUILD file by correct name",2025-03-17T18:58:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89391
copybara-service[bot],Remove deprecated Shape ctor.,Remove deprecated Shape ctor.,2025-03-17T18:52:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89390
copybara-service[bot],Add log to print TFLVersion in TFLInterpreterTests.m.,Add log to print TFLVersion in TFLInterpreterTests.m.,2025-03-17T18:45:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89389
copybara-service[bot],Support ragged_dot in Shardy.,"Support ragged_dot in Shardy. 1. Allow_xla_features (mhlo.ragged_dot) in MhloToStablehlo translation. 2. Add mhlo_extensions for ShardyXLA, and define the sharding rule for ragged_dot operations.",2025-03-17T18:43:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89388
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@cc46e08f,Integrate StableHLO at openxla/stablehlo,2025-03-17T18:39:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89387
copybara-service[bot],[xla:py] Add a temporary RAII object to pass hint to backends that a Traceback does not change within the scope. This should be reverted once we implement context propagation from IFRT.,[xla:py] Add a temporary RAII object to pass hint to backends that a Traceback does not change within the scope. This should be reverted once we implement context propagation from IFRT.,2025-03-17T18:35:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89386
copybara-service[bot],PR #89005: Qualcomm AI Engine Direct - Two step TensorWrapper,"PR CC(Qualcomm AI Engine Direct  Two step TensorWrapper): Qualcomm AI Engine Direct  Two step TensorWrapper Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/89005  WHAT Original flow: 1. Convert LiteRt tensor to TensorWrapper 2. Add TensorWrapper into Qnn graph 3. Convert LiteRt op to OpWrapper by op builder 4. Add OpWrapper into Qnn graph But in some op builder, we may modify some TensorWrapper inside. And these modification won't work due to the original flow. i.e. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/litert/vendors/qualcomm/core/builders/conv2d_op_builder.ccL78L81 Proposed flow: 1. Convert LiteRt tensor to TensorWrapper 2. **Convert LiteRt op to OpWrapper by op builder** 3. **Add TensorWrapper into Qnn graph** 4. Add OpWrapper into Qnn graph  TEST  qnn_compiler_plugin_test ``` [] Global test environment teardown [==========] 115 tests from 5 test suites ran. (3463 ms total) [  PASSED  ] 115 tests. ``` Copybara import of the project:  e04595941f83e8199f1ceb3f20ec3f4a86e549f0 by weilhuanquic : Qualcomm AI Engine Direct  Two step TensorWrapper 1. Insert TensorWrapper into Qnn graph after handled by the op builders. 2. Reflect the modification inside op builders. Merging this change closes CC(Qualcomm AI Engine Direct  Two step TensorWrapper) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89005 from jiunkaiy:dev/weilhuan/two_step_tensor_op e04595941f83e8199f1ceb3f20ec3f4a86e549f0",2025-03-17T18:31:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89385
copybara-service[bot],[XLA:GPU] Annotate combinable sync collectives.,"[XLA:GPU] Annotate combinable sync collectives. Instead of computing the set of synchronous collectives once per combiner pass, we compute it once for all combiner passes. We add a new pass prior to combiner passes where we annotate synchronous collectives.",2025-03-17T18:27:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89384
copybara-service[bot],ml_drift_cl_litert: Implement async execution mode,"ml_drift_cl_litert: Implement async execution mode In async mode, call HandleOutputEvents() after Dispatch() to create and bind the output event. Once all the enqueued commands are completed, the output event will be signaled by OpenCL framework.",2025-03-17T18:22:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89383
copybara-service[bot],[XLA] Adjust the `CallInliner` pass to allow uniquifying channel ids after inlining.,"[XLA] Adjust the `CallInliner` pass to allow uniquifying channel ids after inlining. This is an option that can be enabled specifically on the pass, and will now enabled in the XLA:GPU compilation pipeline if `xla_ignore_channel_id` is `true`. This works around an issue whereby the `CallInliner` might inline several computations involving collectives with the same `channel_id` into the same computation. This is actually not wellbehaved and can result in the creation of cyclic HLO graphs. This change ensures that inlining does not create cyclic graphs. Note that the transformation is actually not semanticspreserving, in that a computation involving several `call` instructions wrapping collectives that use the same `channel_id` is already illformed. Nevertheless, `channel_id`s are only used as a ""hack"" for pipeline parallelism in XLA:GPU currently, and are mostly vestigial at this point. The flagdriven enablement can be deleted once this feature is deprecated (WIP). The proper solution would be to completely get rid of them, but they are currently enforced by the HLO verifier and removing them will take time.",2025-03-17T18:14:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89382
copybara-service[bot],[XLA] Fix a bug where the aliasing logic in graph creation could create graphs that can be stuck because of overlap limit.,[XLA] Fix a bug where the aliasing logic in graph creation could create graphs that can be stuck because of overlap limit. Send instructions have as input a token and this token is piped to the output. From a buffer assignment point of view the input/output tokens alias which is kind of unique across async instructions. If the fix aliasing logic triggers on multiple of these sends where the token aliases the logic adds edges between sendstarts and some senddones. These new edges between sendstarts and dones can create situations where to schedule some starts we need to schedule dones causing us to bust overlap limit.,2025-03-17T18:08:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89381
copybara-service[bot],Add an API to bind event without owning it,"Add an API to bind event without owning it  SetEvent(Event&& event) always takes ownership  SetEvent(LiteRtEvent& litert_event, bool transfer_ownership) depends on   the parameter. GPU Accelerator creates an output LiteRtEvent and owns it without transferring the ownership.",2025-03-17T18:02:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89380
copybara-service[bot],Prevent integer overflow for row_stride in WebPDecode,"Prevent integer overflow for row_stride in WebPDecode The row_stride parameter for WebPDecode is an int, but callers can pass in very large values for width that make row_stride overflow. Just check for it and return false.",2025-03-17T17:17:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89379
copybara-service[bot],[XLA:GPU] Move `is_local()` logic to base class `CollectiveThunk`.,"[XLA:GPU] Move `is_local()` logic to base class `CollectiveThunk`. `AllToAllStartThunk::GetAsyncStreamKind` returns different stream kinds for local and nonlocal collectives. `GetAsyncStreamKind` is called in both `Prepare` and `Initialize`, but it can return different results, because `device_count_` is only set to the correct value in `Initialize`. Until that `is_local` always returns `false`. This can cause an issue when in `Prepare` we request one clique key, but later try to get a different key. This will also affect `max_channel` in `AcquireCollectiveCliques`. This was not noticed so far, because we don't use `stream_kind` is GpuCliqueKey hash and comparison functions. And `stream_id` is always 0 as long as `xla_gpu_enable_nccl_per_stream_comms=false` by default.",2025-03-17T17:06:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89378
copybara-service[bot],Use `tsl/platform:test_main` for `core/platform:test_main`,Use `tsl/platform:test_main` for `core/platform:test_main` The googletest main doesn't work correctly with benchmarks internally.,2025-03-17T17:06:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89377
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-17T16:24:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89376
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-17T16:13:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89375
marenive,'LNK1120: 12 unresolved externals' when building and using TFLite C library with CMake on Windows," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes (using current master)  Source source  TensorFlow version tf2.19.0  Custom code Yes  OS platform and distribution Windows 10  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I want to build the tflite c library on windows. Therefore I looked at the guide https://ai.google.dev/edge/litert/build/cmakebuild_litert_c_library and did the following steps: ```shell git clone https://github.com/tensorflow/tensorflow.git tensorflow_src cd tensorflow_src git checkout v2.19.0 cd .. mkdir tflite_build cd tflite_build cmake ../tensorflow_src/tensorflow/lite/c cmake build . j ``` As output I got as expected the `tensorflowlite_c.dll` and `tensorflowlite_c.lib` files. I created a sample project for testing the library, where the code is mentioned below.  I was able to build the library on Ubuntu 22.04. and run the example described below.  CMake Version: `3.25.1` Windows SDK version `10.0.22621.0`  Standalone code to reproduce the issue ```cpp include  include  define TFLITE_MINIMAL_CHECK(x)                              \   if (!(x)) {                                                \     fprintf(stderr, ""Error at %s:%d\n"", __FILE__, __LINE__); \     exit(1);                                                 \   } int main(int argc, char* argv[]) {   fprintf(stdout, ""Hello TensorFlow Lite!\n"");   TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();   TFLITE_MINIMAL_CHECK(options != nullptr);   TfLiteModel* model = TfLiteModelCreateFromFile(""C:/Users/user1/models/docker/testingeq.tflite"");   TFLITE_MINIMAL_CHECK(model != nullptr);   TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);   TFLITE_MINIMAL_CHECK(interpreter != nullptr);   return 0; } ``` ```cmake cmake_minimum_required(VERSION 3.16) project(minimal) set(CMAKE_CXX_STANDARD 17) set(CMAKE_INSTALL_RPATH ""$ORIGIN"") set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE) set(CMAKE_EXPORT_COMPILE_COMMANDS ON CACHE INTERNAL """") get_filename_component(TensorflowLite_DIR ""${CMAKE_SOURCE_DIR}/deps/tensorflow_src/"" ABSOLUTE) set(TensorflowLite_INCLUDE_DIR ""${TensorflowLite_DIR}"") set(TensorflowLite_LIBRARIES ""${TensorflowLite_DIR}/bin/"") if(MSVC)     set(TENSORFLOWLITE_C ""${TensorflowLite_LIBRARIES}/tensorflowlite_c.lib"")     set(TENSORFLOWLITE_SHARED ""${TensorflowLite_LIBRARIES}/tensorflowlite_c.dll"") elseif(${TARGET_AARCH64})     set(TENSORFLOWLITE_C ""${TensorflowLite_LIBRARIES_AARCH64}/libtensorflowlite_c.so"")     set(TensorflowLite_SHARED ""${TensorflowLite_LIBRARIES_AARCH64}/libtensorflowlite_c.so"") else()     set(TENSORFLOWLITE_C ""${TensorflowLite_LIBRARIES}/libtensorflowlite_c.so"")     set(TENSORFLOWLITE_SHARED ""${TensorflowLite_LIBRARIES}/libtensorflowlite_c.so"") endif() include_directories(${TensorflowLite_INCLUDE_DIR}) add_executable(minimal minimal.cc) target_link_libraries(minimal ${TENSORFLOWLITE_C}) ```  Relevant log output ```shell [main] Building folder: c:/Users/user1/dev/tf_example/build ALL_BUILD [build] Starting build [proc] Executing command: ""C:\Program Files\CMake\bin\cmake.EXE"" build c:/Users/user1/dev/tf_example/build config Debug target ALL_BUILD j 14  [build] MSBuild version 17.13.9+e0f243f1e for .NET Framework [build]  [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorCreate referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalCreate''(void)"" (??__ETfLiteRegistrationExternalCreate@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorDelete referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalDelete''(void)"" (??__ETfLiteRegistrationExternalDelete@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorGetBuiltInCode referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalGetBuiltInCode''(void)"" (??__ETfLiteRegistrationExternalGetBuiltInCode@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorGetCustomName referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalGetCustomName''(void)"" (??__ETfLiteRegistrationExternalGetCustomName@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorGetVersion referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalGetVersion''(void)"" (??__ETfLiteRegistrationExternalGetVersion@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorSetInit referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalSetInit''(void)"" (??__ETfLiteRegistrationExternalSetInit@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorSetFree referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalSetFree''(void)"" (??__ETfLiteRegistrationExternalSetFree@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorSetPrepare referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalSetPrepare''(void)"" (??__ETfLiteRegistrationExternalSetPrepare@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorSetInvoke referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalSetInvoke''(void)"" (??__ETfLiteRegistrationExternalSetInvoke@) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteModelCreateFromFile referenced in function main [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteInterpreterOptionsCreate referenced in function main [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteInterpreterCreate referenced in function main [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [build] C:\Users\user1\dev\tf_example\build\Debug\minimal.exe : fatal error LNK1120: 12 unresolved externals [C:\Users\user1\dev\tf_example\build\minimal.vcxproj] [proc] The command: ""C:\Program Files\CMake\bin\cmake.EXE"" build c:/Users/user1/dev/tf_example/build config Debug target ALL_BUILD j 14  exited with code: 1 [driver] Build completed: 00:00:02.951 [build] Build finished with exit code 1 ```",2025-03-17T16:11:11Z,type:build/install comp:lite TF 2.18,open,0,7,https://github.com/tensorflow/tensorflow/issues/89374,I have the very same issue and could not find any fix. I am very interrested in a solution too!,"Hi,  and   I apologize for the delay in my response, if possible could you please refer these Youtube videos [Ref1], [Ref2] which may help you to solve your issue ? Also please refer to TensorFlow Lite C++ minimal example and CMakeLists.txt . There is a known issue where the Windows CMake build defaults to debug configuration instead of release to ensure a Release build, explicitly specify the configuration like below : `cmake build . j config Release` If issue still persists please let us know with updated error log to investigate this issue further from our end.  Thank you for your cooperation and patience.","Hi , thanks for your reply. Unfortunately the links you provided are not relevant for this issue. The first video you mentioned doesn't explain how to compile, it just download the lib from a github site. Your other links are related to C++ API, not C API. I tried to compile in Release, but in this case the .lib is not generated (only the .dll), so I can't compile. Thanks anyway","Hi, I found what is the issue: the CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS flag is missing in the CMakeLists.txt To fix it, just add the flag on the command line: cmake .\tensorflow\tensorflow\lite\c DCMAKE_WINDOWS_EXPORT_ALL_SYMBOLS=ON Then it produces both a .lib and a .dll with all required symbols. This should be added by default in the CMakeLists.txt BR,","Hello,   I also figured out that you could use Bazel to build the binaries.   For this, I needed a clean Windows install (without any Conda environments) with Bazelisk and MSVC (or some other compiler) installed.   Then I followed these steps:   1. Create a virtual environment with `python m venv .venv`.   2. Activate the virtual environment: `./.venv/Scripts/activate`.   3. Clone the TensorFlow repository: `git clone https://github.com/tensorflow/tensorflow.git`.   4. Navigate to the TensorFlow directory: `cd tensorflow`.   5. Run the configuration script: `./configure` (Select ""Yes"" for everything except using the Clang compiler).   6. Build the binaries: `bazelisk build config=opt //tensorflow/lite/c:tensorflowlite_c`.   I'll check out your solution too, .   Thank you,  ","Hi, ,   Good to hear that your issue has been resolved by running this command `cmake ..\tensorflow\lite\c DTFLITE_ENABLE_XNNPACK=OFF DCMAKE_WINDOWS_EXPORT_ALL_SYMBOLS=ON` The `CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS` flag was introduced in `CMake 3.4` to simplify porting C and C++ libraries from Linux/UNIX to Windows here is reference. When set to `ON`, this flag instructs CMake to:  Automatically create a `.def` (module definition) file with all symbols found in the input `.obj `files for SHARED libraries on Windows  Pass this `.def` file to the linker during the build process  Ensure all symbols are exported from the resulting DLL without requiring manual decoration with `__declspec(dllexport)` Thank you for your cooperation.","Hi , , I compiled it like   suggested like this: ```shell cmake ..\tensorflow_src\tensorflow\lite\c DCMAKE_WINDOWS_EXPORT_ALL_SYMBOLS=ON DTFLITE_ENABLE_XNNPACK=ON DCMAKE_BUILD_TYPE=Release ``` I set `XNNPACK` to `ON` which is the default anyway, but when I inspect and use the .dll the TfLiteXNNPackDelegate is missing. Dumpbin outbut ```shell . . . 414  19D 0000C8A0 ?what@         415  19E 0000C320 ?window_dilations@?$Vector@         416  19F 0000C550 ?window_reversal@?$Vector@         417  1A0 0000C4F0 ?window_strides@?$Vector@         418  1A1 0000C320 ?zero_point@?$Vector@         419  1A2 0000EE00 TfLiteDelegateCreate         420  1A3 0000C8C0 TfLiteExtensionApisVersion         421  1A4 0000EE20 TfLiteFloatArrayCopy         422  1A5 0000EEB0 TfLiteFloatArrayCreate         423  1A6 0000EEF0 TfLiteFloatArrayFree         424  1A7 0000EF00 TfLiteFloatArrayGetSizeInBytes         425  1A8 0000EE20 TfLiteIntArrayCopy         426  1A9 0000EEB0 TfLiteIntArrayCreate         427  1AA 0000EF10 TfLiteIntArrayEqual         428  1AB 0000EF60 TfLiteIntArrayEqualsArray         429  1AC 0000EEF0 TfLiteIntArrayFree         430  1AD 0000EFB0 TfLiteIntArrayGetSizeInBytes         431  1AE 0000C8D0 TfLiteInterpreterAllocateTensors         432  1AF 0000C8E0 TfLiteInterpreterCancel         433  1B0 0000C8F0 TfLiteInterpreterCreate         434  1B1 0000E210 TfLiteInterpreterCreateWithSelectedOps         435  1B2 0000C950 TfLiteInterpreterDelete         436  1B3 0000E260 TfLiteInterpreterEnsureTensorDataIsReadable         437  1B4 0000E270 TfLiteInterpreterGetBufferHandle         438  1B5 0000C9F0 TfLiteInterpreterGetInputTensor         439  1B6 0000CA30 TfLiteInterpreterGetInputTensorCount         440  1B7 0000E280 TfLiteInterpreterGetInputTensorIndex         441  1B8 0000CA50 TfLiteInterpreterGetOutputTensor         442  1B9 0000CA90 TfLiteInterpreterGetOutputTensorCount         443  1BA 0000E2A0 TfLiteInterpreterGetOutputTensorIndex         444  1BB 0000CAB0 TfLiteInterpreterGetSignatureCount         445  1BC 0000CB20 TfLiteInterpreterGetSignatureKey         446  1BD 0000CC20 TfLiteInterpreterGetSignatureRunner         447  1BE 0000CC60 TfLiteInterpreterGetTensor         448  1BF 0000E2C0 TfLiteInterpreterGetVariableTensor         449  1C0 0000E300 TfLiteInterpreterGetVariableTensorCount         450  1C1 0000CC90 TfLiteInterpreterInputTensorIndices         451  1C2 0000CCB0 TfLiteInterpreterInvoke         452  1C3 0000E320 TfLiteInterpreterModifyGraphWithDelegate         453  1C4 0000E330 TfLiteInterpreterOptionsAddBuiltinOp         454  1C5 0000E340 TfLiteInterpreterOptionsAddCustomOp         455  1C6 0000CCD0 TfLiteInterpreterOptionsAddDelegate         456  1C7 0000CD20 TfLiteInterpreterOptionsAddOperator         457  1C8 0000CD70 TfLiteInterpreterOptionsCopy         458  1C9 0000CF30 TfLiteInterpreterOptionsCreate         459  1CA 0000D040 TfLiteInterpreterOptionsDelete         460  1CB 0000D140 TfLiteInterpreterOptionsEnableCancellation         461  1CC 0000E350 TfLiteInterpreterOptionsSetEnableDelegateFallback         462  1CD 0000D150 TfLiteInterpreterOptionsSetErrorReporter         463  1CE 0000D160 TfLiteInterpreterOptionsSetNumThreads         464  1CF 0000E360 TfLiteInterpreterOptionsSetOpResolver         465  1D0 0000E4F0 TfLiteInterpreterOptionsSetOpResolverExternal         466  1D1 0000E680 TfLiteInterpreterOptionsSetOpResolverExternalWithFallback         467  1D2 0000E8E0 TfLiteInterpreterOptionsSetOpResolverV1         468  1D3 0000EA70 TfLiteInterpreterOptionsSetOpResolverV2         469  1D4 0000EC00 TfLiteInterpreterOptionsSetOpResolverV3         470  1D5 0000ED90 TfLiteInterpreterOptionsSetTelemetryProfiler         471  1D6 0000EDA0 TfLiteInterpreterOptionsSetUseNNAPI         472  1D7 0000D170 TfLiteInterpreterOutputTensorIndices         473  1D8 0000EDB0 TfLiteInterpreterResetVariableTensors         474  1D9 0000D190 TfLiteInterpreterResizeInputTensor         475  1DA 0000EDC0 TfLiteInterpreterSetBufferHandle         476  1DB 0000EDD0 TfLiteInterpreterSetCustomAllocationForTensor         477  1DC 0000D2E0 TfLiteModelCreate         478  1DD 0000D420 TfLiteModelCreateFromFile         479  1DE 0000D540 TfLiteModelCreateFromFileWithErrorReporter         480  1DF 0000D6A0 TfLiteModelCreateWithErrorReporter         481  1E0 0000D810 TfLiteModelDelete         482  1E1 0000F900 TfLiteOperatorCreate         483  1E2 0000F990 TfLiteOperatorDelete         484  1E3 0000F9A0 TfLiteOperatorGetBuiltInCode         485  1E4 000010F0 TfLiteOperatorGetCustomName         486  1E5 0000F9B0 TfLiteOperatorGetUserData         487  1E6 0000F9C0 TfLiteOperatorGetVersion         488  1E7 0000F9D0 TfLiteOperatorSetAsyncKernel         489  1E8 0000F9E0 TfLiteOperatorSetAsyncKernelWithData         490  1E9 0000F9F0 TfLiteOperatorSetFree         491  1EA 0000FA00 TfLiteOperatorSetFreeWithData         492  1EB 0000FA10 TfLiteOperatorSetInit         493  1EC 0000FA20 TfLiteOperatorSetInitWithData         494  1ED 0000FA30 TfLiteOperatorSetInplaceOperator         495  1EE 0000FA40 TfLiteOperatorSetInvoke         496  1EF 0000FA50 TfLiteOperatorSetInvokeWithData         497  1F0 0000FA60 TfLiteOperatorSetPrepare         498  1F1 0000FA70 TfLiteOperatorSetPrepareWithData         499  1F2 0000EFC0 TfLiteQuantizationFree         500  1F3 0000D880 TfLiteSchemaVersion         501  1F4 0000EDF0 TfLiteSetAllowBufferHandleOutput         502  1F5 0000D890 TfLiteSignatureRunnerAllocateTensors         503  1F6 0000D8A0 TfLiteSignatureRunnerCancel         504  1F7 0000D8B0 TfLiteSignatureRunnerDelete         505  1F8 0000D8C0 TfLiteSignatureRunnerGetInputCount         506  1F9 0000D8E0 TfLiteSignatureRunnerGetInputName         507  1FA 0000D910 TfLiteSignatureRunnerGetInputTensor         508  1FB 0000D920 TfLiteSignatureRunnerGetOutputCount         509  1FC 0000D940 TfLiteSignatureRunnerGetOutputName         510  1FD 0000D970 TfLiteSignatureRunnerGetOutputTensor         511  1FE 0000D980 TfLiteSignatureRunnerInvoke         512  1FF 0000D990 TfLiteSignatureRunnerResizeInputTensor         513  200 0000F040 TfLiteSparsityFree         514  201 0000DAD0 TfLiteTensorByteSize         515  202 0000F110 TfLiteTensorCopy         516  203 0000DAE0 TfLiteTensorCopyFromBuffer         517  204 0000DB10 TfLiteTensorCopyToBuffer         518  205 0000DB40 TfLiteTensorData         519  206 0000F240 TfLiteTensorDataFree         520  207 0000DB50 TfLiteTensorDim         521  208 0000F2A0 TfLiteTensorFree         522  209 0000F430 TfLiteTensorGetAllocationStrategy         523  20A 0000F490 TfLiteTensorGetBufferAddressStability         524  20B 0000F4F0 TfLiteTensorGetDataKnownStep         525  20C 0000F550 TfLiteTensorGetDataStability         526  20D 0000F5A0 TfLiteTensorGetShapeKnownStep         527  20E 0000DB60 TfLiteTensorName         528  20F 0000DB70 TfLiteTensorNumDims         529  210 0000DB90 TfLiteTensorQuantizationParams         530  211 0000F600 TfLiteTensorRealloc         531  212 0000F690 TfLiteTensorReset         532  213 0000F730 TfLiteTensorResizeMaybeCopy         533  214 000010E0 TfLiteTensorType         534  215 0000F7E0 TfLiteTypeGetName         535  216 0000C8C0 TfLiteVersion ```"
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-03-17T15:53:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89373
copybara-service[bot],This is an internal change,This is an internal change,2025-03-17T15:48:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89372
copybara-service[bot],Reverts 3db9ac3f7615aa00cd88161dc880031b55c76df0,Reverts 3db9ac3f7615aa00cd88161dc880031b55c76df0,2025-03-17T15:24:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89371
Venkat6871,Fix: Ensure boolean_mask_v2() only accepts boolean dtype for mask,"Hi, Team This PR fixes an issue in `tf.boolean_mask_v2()` where an integer tensor can be mistakenly used as a mask.   Now, if the mask tensor is not of dtype `bool`, it raises a `TypeError`.   This prevents unintended behavior and ensures users provide a correct boolean mask.",2025-03-17T14:46:45Z,ready to pull comp:ops size:XS,open,0,0,https://github.com/tensorflow/tensorflow/issues/89370
copybara-service[bot],"When we dump a module, we do *not* want to also verify it. We just want to dump a module on a best effort, even if it's a malformed module.","When we dump a module, we do *not* want to also verify it. We just want to dump a module on a best effort, even if it's a malformed module.",2025-03-17T14:40:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89369
copybara-service[bot],Rollback the NVML usage from the CUDA.,Rollback the NVML usage from the CUDA. Reverts 744b20216646e06b83005697a4eaf14163a16c7e,2025-03-17T14:14:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89368
copybara-service[bot],[XLA:GPU] Remove no-op `xla_gpu_enable_priority_fusion` flag.,[XLA:GPU] Remove noop `xla_gpu_enable_priority_fusion` flag. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/80074 from yzhou51:fullconnect_use_slm 6ba325e738ee82b60bc611247c797451b8c957ae,2025-03-17T13:33:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89367
copybara-service[bot],PR #23764: Refactor hlo_text to hlo_file for Consistent Naming in Multihost HLO Runner,"PR CC(How to know the value of the deterministic operation seed of tf.nn.dropout when we have already set the graph level seed and do not specify any seed in the op): Refactor hlo_text to hlo_file for Consistent Naming in Multihost HLO Runner Imported from GitHub PR https://github.com/openxla/xla/pull/23764 FunctionalHloRunner::LoadHloModuleAndArguments takes an string_view parameter named `hlo_file`, representing the file path. The file can be in various formats, such as text, proto, or snapshot. However, the higherlevel API LoadAndRun previously used the variable name `hlo_text` for the same purpose, which could be misleading since it implies content rather than a file path. This PR updates the variable name from `hlo_text` to `hlo_file` across the relevant functions for better clarity and consistency. Additionally, hlo_runner_main. `filename`  this has also been renamed to `hlo_file` to ensure consistent naming across all API levels Copybara import of the project:  51adc2b0386f8c8e44e2c02c297d6924f86af766 by Alexander Pivovarov : Refactor hlo_text to hlo_file for Consistent Naming in Multihost HLO Runner Merging this change closes CC(How to know the value of the deterministic operation seed of tf.nn.dropout when we have already set the graph level seed and do not specify any seed in the op) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23764 from apivovarov:fix_filename_var 51adc2b0386f8c8e44e2c02c297d6924f86af766",2025-03-17T13:10:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89366
copybara-service[bot],PR #23367: [ROCm] Respect hipruntime constraints for launch dimension of Row red…,PR CC(How to visualizing a model structure in tensorboard when eager execution is open? ): [ROCm] Respect hipruntime constraints for launch dimension of Row red… Imported from GitHub PR https://github.com/openxla/xla/pull/23367 …uce kernels Copybara import of the project:  382e5419a01ad7d8c56ff99ed8b1f697e0f62b06 by Harsha HS : [ROCm] Respect hipruntime constraints for launch dimension of Row reduce kernels  bff2a4d4abab824ffd92918c7b6b3ee0f149567b by Harsha H S : Update reduction.(How to visualizing a model structure in tensorboard when eager execution is open? ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23367 from ROCm:ci_fix_row_reduce_launch_dim_20250304 bff2a4d4abab824ffd92918c7b6b3ee0f149567b,2025-03-17T12:55:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89365
copybara-service[bot],Align accelerator application during compiled model creation to the expected behaviour.,"Align accelerator application during compiled model creation to the expected behaviour. This adds a new function field in the accelerator implementation structure and associated functions to set/call that function. The function queries the accelerator to know if the underlying TFLite delegate does JIT compilation. When that's the case, even if the accelerator is registered, it won't be used to create and apply a delegate unless explicitly requested through the compilation options.",2025-03-17T12:31:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89364
copybara-service[bot],[XLA:CPU] Allow passing nullptr run options to Eigen dot,[XLA:CPU] Allow passing nullptr run options to Eigen dot,2025-03-17T11:54:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89363
copybara-service[bot],PR #23693: Remove HloComputation::WhileCallInstruction.,"PR CC(Dilated convolution support with nnapi): Remove HloComputation::WhileCallInstruction. Imported from GitHub PR https://github.com/openxla/xla/pull/23693 The function is deprecated and broken. I'll remove the remaining similar functions one by one. The caller API I added in the previous PR was too `const`, fixed that as well. Copybara import of the project:  48e84e69a04f4f8d7ba42cfc78cf1f9c0bd3aa36 by Johannes Reifferscheid : Remove HloComputation::WhileCallInstruction. The function is deprecated and broken. I'll remove the remaining similar functions one by one.  3f591be598370c39789fabed15640275b41ffd70 by Johannes Reifferscheid : Undo accidental changes.  9b307599fe6224ad56af3c867304c5617d7d39fd by Johannes Reifferscheid : Fixes:  add missing const  in copy insertion, don't look for a unique caller (not sure this is   actually a problem, but it seems more accurate wrt the comment above)  in collective permute valid iteration annotator, look at all the   loops. Again, this seems a bit strange, but the current logic appears   to depend on the last created loop to be found (?) Merging this change closes CC(Dilated convolution support with nnapi) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23693 from jreiffers:callerinstructions 9b307599fe6224ad56af3c867304c5617d7d39fd",2025-03-17T11:24:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89362
copybara-service[bot],[XLA:CPU] Remove cpu_executable unused LLVM deps,[XLA:CPU] Remove cpu_executable unused LLVM deps,2025-03-17T11:06:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89361
copybara-service[bot],Use `UniquifyName(HloModule*)` and `UniquifyId(HloModule*)` whenever possible.,Use `UniquifyName(HloModule*)` and `UniquifyId(HloModule*)` whenever possible.,2025-03-17T10:53:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89360
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T10:50:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89359
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T10:46:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89358
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T10:46:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89357
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T10:41:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89356
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T10:36:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89355
copybara-service[bot],[XLA] fix use after free in HloPassPipeline,[XLA] fix use after free in HloPassPipeline There is no guarantee that module config is will not be updated by passes so any access of debug_options will be use after free.,2025-03-17T10:20:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89354
woegerbauerJ,Conv1D Layer with kernel size 1 does not match with iteratively processing," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.14.1  Custom code Yes  OS platform and distribution Linux Ubuntu  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I was trying to set up a realtime/causal Conv2D layers, by multiple Conv1D layers. I noticed an error in the order of 1e7, so i started to narrow down the issue. This is why i ended up with the following code. I essentially create 2 identical conv1D layers where i then load the weights from one into the other. When iterating over the time dimension (axis 1; as one would to in an realtime/causal implementation) this error occurs. Note:  this issue only happens for ""large"" filters (> 10)  this issue only happens for ""large"" features (> 30)  when setting the weights to integer this issue is not reproducible  Standalone code to reproduce the issue ```shell import numpy as np import tensorflow as tf class Conv1D_wrapper(tf.keras.layers.Layer):     def __init__(self, filters):         super(Conv1D_wrapper, self).__init__()         self.conv1d = tf.keras.layers.Conv1D(             filters = filters,             kernel_size = 1,             padding='valid',             activation='relu',             strides = 1,             use_bias = False         )     def load(self, other_layer):         kernel = other_layer.get_weights()         self.conv1d.set_weights([kernel[0]])     def call(self, input):         x = self.conv1d(input)         return x if __name__ == ""__main__"":     dummy data     x1 = np.ones([1,3,1,10])      init the layers     conv1 = Conv1D_wrapper(30)     conv2 = Conv1D_wrapper(30)     conv1(x1.copy())     conv2(x1.copy())      load conv1 into conv2     conv2.load(conv1)      output1 (calculated over all time steps)     output1 = conv1(x1.copy())     output2 = np.zeros_like(output1)     for i in range(output1.shape[1]): calculated iteratively over time steps         output2[:,i,:] = conv2(x1[:,i,:][:,None,:])     print(""not iterated matching"", np.allclose(conv1(x1.copy()), conv2(x1.copy())))     print(""matching kernels?"",np.allclose(conv2.get_weights()[0],conv1.get_weights()[0]))     print(""bias?"",conv2.conv1d.use_bias, conv1.conv1d.use_bias)     print(""max error"", np.max(abs(output1output2))) ```  Relevant log output ```shell not iterated matching True matching kernels? True bias? False False max error 1.1920929e07 ```",2025-03-17T10:20:20Z,stat:awaiting response type:bug comp:ops TF2.14,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89353,"addon: this issue also happens for `    x1 = np.ones([1,11,10,10])      init the layers     conv1 = tf.keras.layers.Conv1D(filters = 30,kernel_size=1, use_bias =False)     output1 = conv1(x1.copy())     output2 = np.zeros_like(output1)     for i in range(output1.shape[1]): calculated iteratively over time steps         output2[:,i,:] = conv1(x1[:,i,:])     print(""max error"", np.max(abs(output1output2)))` and also leads to the same error!","Hi  , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. However, I encountered a different issue. After making some modifications, the code worked for me. Additionally, I noticed that you are using an older version of TensorFlow. I recommend upgrading to the latest version for better results. I have attached a gist for your reference. Thank you!","Hi, thanks for your response! Due to our hardware we somewhat bounded to tf 2.14 (we are working with an NPU).  Thanks for finding out the issue with the dimension. I still assume this is somewhat of an issue, since mathematically it should be identical right? Even for 4 dimensions. However i am happy with the solution. Thank you very much!",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],PR #23764: Refactor hlo_text to hlo_file for Consistent Naming in Multihost HLO Runner,"PR CC(How to know the value of the deterministic operation seed of tf.nn.dropout when we have already set the graph level seed and do not specify any seed in the op): Refactor hlo_text to hlo_file for Consistent Naming in Multihost HLO Runner Imported from GitHub PR https://github.com/openxla/xla/pull/23764 FunctionalHloRunner::LoadHloModuleAndArguments takes an string_view parameter named `hlo_file`, representing the file path. The file can be in various formats, such as text, proto, or snapshot. However, the higherlevel API LoadAndRun previously used the variable name `hlo_text` for the same purpose, which could be misleading since it implies content rather than a file path. This PR updates the variable name from `hlo_text` to `hlo_file` across the relevant functions for better clarity and consistency. Additionally, hlo_runner_main. `filename`  this has also been renamed to `hlo_file` to ensure consistent naming across all API levels Copybara import of the project:  51adc2b0386f8c8e44e2c02c297d6924f86af766 by Alexander Pivovarov : Refactor hlo_text to hlo_file for Consistent Naming in Multihost HLO Runner Merging this change closes CC(How to know the value of the deterministic operation seed of tf.nn.dropout when we have already set the graph level seed and do not specify any seed in the op) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23764 from apivovarov:fix_filename_var 51adc2b0386f8c8e44e2c02c297d6924f86af766",2025-03-17T10:06:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89352
copybara-service[bot],PR #23759: Enhance Flag Usage Text with Missing Input Formats,PR CC(longer latency after posttraining quantization): Enhance Flag Usage Text with Missing Input Formats Imported from GitHub PR https://github.com/openxla/xla/pull/23759 This PR updates the flag usage text in `multihost_hlo_runner/hlo_runner_main.cc` to include the missing input formats. Flag: input_format Missing input formats:  unoptimized_snapshot_proto_binary  unoptimized_snapshot_proto_text Copybara import of the project:  483b0ef6a484cd94e06dc1f728d3009404797774 by Alexander Pivovarov : Enhance Flag Usage Text with Missing Input Formats Merging this change closes CC(longer latency after posttraining quantization) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23759 from apivovarov:fix_input_format_help 483b0ef6a484cd94e06dc1f728d3009404797774,2025-03-17T10:05:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89351
Venkat6871,Saving changes in array_ops.py,,2025-03-17T10:02:17Z,size:XL,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89350
copybara-service[bot],Add library visibility,Add library visibility,2025-03-17T10:00:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89349
NamanAgarwal0905,Fixed minor documentation typos ,Changed PyPi to PyPI in README.md,2025-03-17T09:41:06Z,ready to pull size:XS,closed,0,1,https://github.com/tensorflow/tensorflow/issues/89348,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Enable `sin` and `cos` delegation for `float` and `float16`.,Enable `sin` and `cos` delegation for `float` and `float16`. The XNNPACK microkernels are 1020x faster than the TFLite builtins.,2025-03-17T09:35:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89347
copybara-service[bot],Adding executable location to KubernetesClusterResolver.,Adding executable location to KubernetesClusterResolver.,2025-03-17T08:08:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89345
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T06:36:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89344
copybara-service[bot],Add waymo to visibility,Add waymo to visibility This is meant to use the test framework/library in waymo project.,2025-03-17T06:27:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89343
jiunkaiy,Qualcomm AI Engine Direct - Set HTP to burst mode,,2025-03-17T06:03:54Z,size:L,open,0,0,https://github.com/tensorflow/tensorflow/issues/89342
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T05:38:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89341
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T05:37:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89340
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T05:36:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89339
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T05:34:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89338
chunhsue,Qualcomm AI Engine Direct - Compile QINT16 as QUINT16," What Separate https://github.com/tensorflow/tensorflow/pull/89132 into 2 PRs for better review experience. This PR only includes ""Compile"" part. Internal review recorded here.   Tests `qnn_compiler_plugin_test` ``` [] Global test environment teardown [==========] 127 tests from 5 test suites ran. (4621 ms total) [  PASSED  ] 127 tests. ``` `//tensorflow/lite/experimental/litert/vendors/qualcomm/core/utils:utils_test` ``` [] Global test environment teardown [==========] 11 tests from 3 test suites ran. (0 ms total) [  PASSED  ] 11 tests. ``` `//tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:all` ``` //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:op_wrapper_test (cached) PASSED in 0.0s //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:quantize_params_wrapper_test (cached) PASSED in 0.0s //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:param_wrapper_test PASSED in 0.0s //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:tensor_wrapper_test PASSED in 0.0s ```",2025-03-17T05:31:36Z,ready to pull size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89337
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T05:29:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89336
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T05:28:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89335
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89208 from tensorflow:fixtypos21 bfdd7916e67f3bd162cb75d9a45c4218f78fd997,2025-03-17T05:24:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89334
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T05:23:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89333
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/89208 from tensorflow:fixtypos21 bfdd7916e67f3bd162cb75d9a45c4218f78fd997,2025-03-17T05:21:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89332
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T05:20:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89331
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T05:03:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89330
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T04:57:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89329
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T04:55:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89328
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T04:53:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89327
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T04:50:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89326
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T04:41:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89325
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T02:48:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89324
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T02:32:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89323
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-17T02:29:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89322
copybara-service[bot],Fix xla::XlaBuilder::Slice's example to be consistent with reality.,Fix xla::XlaBuilder::Slice's example to be consistent with reality. I think the x/y in the documentation is swapped.,2025-03-16T23:49:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89321
copybara-service[bot],[xla:gpu] Move nvshmem collectives test under gpu/collectives folder,[xla:gpu] Move nvshmem collectives test under gpu/collectives folder,2025-03-16T20:52:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89320
copybara-service[bot],[xla:gpu] Restrict visibility of Collectives/Communicator implementations,[xla:gpu] Restrict visibility of Collectives/Communicator implementations,2025-03-16T16:40:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89319
copybara-service[bot],"IFRT Proxy: When an IfrtRequest results in an error, map client-generated handles to the error.","IFRT Proxy: When an IfrtRequest results in an error, map clientgenerated handles to the error.",2025-03-16T14:05:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89318
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T06:42:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89317
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T06:33:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89316
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T06:08:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89315
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T06:05:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89314
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T06:04:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89313
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T06:03:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89312
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T06:03:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89311
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T06:02:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89310
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T05:58:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89309
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T05:50:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89308
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T05:49:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89307
copybara-service[bot],Fix and enable pywrap rules on Windows platform,"Fix and enable pywrap rules on Windows platform To use new rules the project needs to be built with the following parameters (flags are already into TF's .bazelrc) repo_env=USE_PYWRAP_RULES=True //:use_dlls=True //absl:use_dlls Note: if after this CL, in one of your changes you see linker errors complaining about symbols missing on windows, you most likely need to modify _pywrap_tensorflow.def file:  remove symbols which got deleted in your CL from _pywrap_tensorflow.def if they are mentioned there  add new symbols into _pywrap_tensorflow.def if they start getting used in one of pybind_extension modules To make sense out of mangled windows symbol names you may use https://demangler.com/ or `undname` (on windows) or any other simillar tool. Automatic generation of _pywrap_tensorflow.def is possible but may be potentially slow, if there is evidence that _pywrap_tensorflow.def  needs to get updated often, we'll add the automation.",2025-03-16T05:48:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89306
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T03:06:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89305
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T03:01:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89304
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T02:57:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89303
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-16T02:45:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89302
copybara-service[bot],Rename `Shape::IsInteger()` to avoid confusion.,"Rename `Shape::IsInteger()` to avoid confusion. `Shape::IsInteger()` means that the shape contains only integer types. The name doesn't convey this semantics clearly. It's easy to mistakenly think that the function means the shape contains a single integer, for example.",2025-03-16T00:37:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89301
copybara-service[bot],Fix typo in the name of function `get_embeded_tunk()`.,Fix typo in the name of function `get_embeded_tunk()`.,2025-03-16T00:35:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89300
copybara-service[bot],Create LiteRT benchmark model class,Create LiteRT benchmark model class This benchmark model class is based on the standard TensorFlow benchmark model class. It will compile the model with the given options before running the benchmarks.,2025-03-15T18:37:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89299
copybara-service[bot],"litert: Add ""manual"" tag for non Linux tests","litert: Add ""manual"" tag for non Linux tests",2025-03-15T17:31:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89298
02f01a020,compiling code error," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError: Traceback (most recent call last):   File ""C:\Users\ACER\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\localpackages\Python311\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.  Standalone code to reproduce the issue ```shell from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```  Relevant log output ```shell ImportError: Traceback (most recent call last):   File ""C:\Users\ACER\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\localpackages\Python311\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```",2025-03-15T11:17:14Z,stat:awaiting response type:bug TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89297,please update the MSVC 2019 redistributable，download links: https://download.visualstudio.microsoft.com/download/pr/285b28c73cf947fb9be801cf5323a8df/8F9FB1B3CFE6E5092CF1225ECD6659DAB7CE50B8BF935CB79BFEDE1F3C895240/VC_redist.x64.exe,"Hi  , Apologies for the delay, could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: You need to install the MSVC 2019 redistributable Your CPU does not support AVX2 instructions Your CPU/Python is on 32 bits There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Please search for duplicates before opening a new issue,Are you satisfied with the resolution of your issue? Yes No
devnas-2004,Failed to load the native TensorFlow runtime.," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tensorflow2.19.0   Custom code Yes  OS platform and distribution windows  Mobile device _No response_  Python version 3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[2], line 5       3 stemmer = LancasterStemmer()       4 import numpy as np > 5 import tflearn       6 import tensorflow as tf       7 import random File ~\anaconda3\Lib\sitepackages\tflearn\__init__.py:4       1 from __future__ import absolute_import       3  Disable TF eager mode > 4 import tensorflow.compat.v1 as tf       5 tf.disable_v2_behavior()       7  Config File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\saxen\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime.  Standalone code to reproduce the issue ```shell import nltk from nltk.stem.lancaster import LancasterStemmer stemmer = LancasterStemmer() import numpy as np import tflearn import tensorflow as tf import random ```  Relevant log output ```shell ```",2025-03-15T10:58:30Z,stat:awaiting response type:build/install subtype:windows TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89296,"Hi 2004 , Apologies for the delay, could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:  You need to install the MSVC 2019 redistributable  Your CPU does not support AVX2 instructions  Your CPU/Python is on 32 bits  There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. https://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Please search for duplicates before opening a new issue.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T09:35:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89295
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T09:32:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89294
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T09:32:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89293
copybara-service[bot],[XLA] Implement IsArrayType in terms of other predicates,[XLA] Implement IsArrayType in terms of other predicates This reduces duplication and makes it harder for us to make mistakes when new types are added or removed.,2025-03-15T08:53:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89292
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T08:52:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89291
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T08:51:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89290
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T08:46:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89289
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T08:45:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89288
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T08:43:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89287
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T08:39:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89286
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T08:36:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89285
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T08:33:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89284
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T08:30:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89283
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T08:29:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89282
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T08:28:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89281
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T08:27:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89280
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T08:25:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89279
copybara-service[bot],Copy quantization_lib/quantization_driver and quantization_utils to Lite,Copy quantization_lib/quantization_driver and quantization_utils to Lite,2025-03-15T07:06:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89278
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T04:46:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89277
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T04:39:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89276
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T04:30:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89275
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-15T04:26:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89274
copybara-service[bot],[HloRunner] Support CPU Profiling in multihost_hlo_runner and refactor compute_xspace_stats to support both GPU and CPU XSpace profiling with corresponding workflow changes,[HloRunner] Support CPU Profiling in multihost_hlo_runner and refactor compute_xspace_stats to support both GPU and CPU XSpace profiling with corresponding workflow changes,2025-03-15T04:08:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89273
maludwig,TensorFlow on RTX 5090," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.20.0.dev20250314  Custom code No  OS platform and distribution Windows 11  WSL2  Ubuntu 22.04.5 LTS  Mobile device _No response_  Python version 3.10.12  Bazel version 7.4.1  GCC/compiler version gcc (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0  CUDA/cuDNN version CUDA Version: 12.8  GPU model and memory RTX 5090 32GB  Current behavior? I had hoped that tensorflow would work on the RTX 5090 at all. It does not, sadly. I tried building from source but that didn't work either. I tried running the environment script but that didn't work either. At least bash is my primary programming language, so I was able to tidy that one up here: https://github.com/tensorflow/tensorflow/pull/89271 But I wasn't able to get tensorflow running. I had a similar issue with PyTorch, which needed to use CUDA 12.8.* to work on the Blackwell cards, but no dice with the nightly build of tensorflow. Below is my test and the output, and under that is the `tf_env.txt` from my patched script. It may be helpful to know that nvidia themselves seem to have it running here: https://docs.nvidia.com/deeplearning/frameworks/tensorflowreleasenotes/rel2502.html But I get the same errors that this other guy does when I try it out: https://www.reddit.com/r/tensorflow/comments/1iutjoj/tensorflow_2501_cuda_128_rtx_5090_on_wsl2_cuda/ This conversation was another one I found that may be helpful, according to these guys, you need to support CUDA 12.8.1 to support Blackwell (aka the RTX 50 series cards): https://discuss.ai.google.dev/t/buildingtensorflowfromsourceforrtx5000gpuseries/65171/15 ``` (tfnightie) mitch:~/stable_diff $ cat tfnightie/test_2.py import tensorflow as tf import time  Check if TensorFlow sees the GPU print(""TensorFlow version:"", tf.__version__) print(""Available GPUs:"", tf.config.experimental.list_physical_devices('GPU'))  Matrix multiplication test shape = (5000, 5000) a = tf.random.normal(shape) b = tf.random.normal(shape)  Time execution on GPU with tf.device('/GPU:0'):     print(""Running on GPU..."")     start_time = time.time()     c = tf.matmul(a, b)     tf.print(""Matrix multiplication (GPU) done."")     print(""Execution time (GPU):"", time.time()  start_time, ""seconds"")  Time execution on CPU for comparison with tf.device('/CPU:0'):     print(""Running on CPU..."")     start_time = time.time()     c = tf.matmul(a, b)     tf.print(""Matrix multiplication (CPU) done."")     print(""Execution time (CPU):"", time.time()  start_time, ""seconds"") (tfnightie) mitch:~/stable_diff $ python tfnightie/test_2.py 20250314 21:35:33.400099: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. TensorFlow version: 2.20.0dev20250314 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1742009735.413544  326199 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jitcompiled from PTX, which could take 30 minutes or longer. Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] W0000 00:00:1742009735.417720  326199 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jitcompiled from PTX, which could take 30 minutes or longer. I0000 00:00:1742009735.572153  326199 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29043 MB memory:  > device: 0, name: NVIDIA GeForce RTX 5090, pci bus id: 0000:09:00.0, compute capability: 12.0 20250314 21:35:36.969440: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_INVALID_PTX' 20250314 21:35:36.969480: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250314 21:35:36.969505: W tensorflow/core/framework/op_kernel.cc:1843] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250314 21:35:36.969533: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' Traceback (most recent call last):   File ""/home/mitch/stable_diff/tfnightie/test_2.py"", line 10, in      a = tf.random.normal(shape)   File ""/home/mitch/.virtualenvs/tfnightie/lib/python3.10/sitepackages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/home/mitch/.virtualenvs/tfnightie/lib/python3.10/sitepackages/tensorflow/python/framework/ops.py"", line 6027, in raise_from_not_ok_status     raise core._status_to_exception(e) from None   pylint: disable=protectedaccess tensorflow.python.framework.errors_impl.InternalError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Mul] name: ``` Also, while nvidia's site says that the Compute Capability of the RTX5090 is ""10.0"", the card itself seems to report ""12.0"". I am not so sure that info will be helpful, but it spun me for a loop: ``` $ cat  card_details.cu > include  include  int main() {     cudaDeviceProp prop;     int device;     cudaGetDevice(&device); // Get the current device ID     cudaGetDeviceProperties(&prop, device); // Get device properties     size_t free_mem, total_mem;     cudaMemGetInfo(&free_mem, &total_mem); // Get VRAM usage     std::cout _75' will be removed in a future release (Use Wnodeprecatedgputargets to suppress warning). GPU Name: NVIDIA GeForce RTX 5090 Compute Capability: 12.0 VRAM Usage: 1763 MB / 32606 MB ```  tf_env.txt ``` == check python ==================================================== python version: 3.10.12 python branch: python build version: ('main', 'Feb  4 2025 14:57:36') python compiler version: GCC 11.4.0 python implementation: CPython == check os platform =============================================== os: Linux os kernel version: CC(Add support for Python 3.x) SMP Tue Nov 5 00:21:55 UTC 2024 os release version: 5.15.167.4microsoftstandardWSL2 os platform: Linux5.15.167.4microsoftstandardWSL2x86_64withglibc2.35 freedesktop os release: {'NAME': 'Ubuntu', 'ID': 'ubuntu', 'PRETTY_NAME': 'Ubuntu 22.04.5 LTS', 'VERSION_ID': '22.04', 'VERSION': '22.04.5 LTS (Jammy Jellyfish)', 'VERSION_CODENAME': 'jammy', 'ID_LIKE': 'debian', 'HOME_URL': 'https://www.ubuntu.com/', 'SUPPORT_URL': 'https://help.ubuntu.com/', 'BUG_REPORT_URL': 'https://bugs.launchpad.net/ubuntu/', 'PRIVACY_POLICY_URL': 'https://www.ubuntu.com/legal/termsandpolicies/privacypolicy', 'UBUNTU_CODENAME': 'jammy'} mac version: ('', ('', '', ''), '') uname: uname_result(system='Linux', node='win11ml', release='5.15.167.4microsoftstandardWSL2', version=' CC(Add support for Python 3.x) SMP Tue Nov 5 00:21:55 UTC 2024', machine='x86_64') architecture: ('64bit', 'ELF') machine: x86_64 == are we in docker ================================================ No == c++ compiler ==================================================== /usr/bin/c++ c++ (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Copyright (C) 2021 Free Software Foundation, Inc. This is free software; see the source for copying conditions.  There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. == check pips ====================================================== numpy                   2.1.3 protobuf                5.29.3 tf_nightly              2.20.0.dev20250314 == check for virtualenv ============================================ Running inside a virtual environment. == tensorflow import =============================================== 20250314 21:02:48.002965: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1742007769.198398  317963 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jitcompiled from PTX, which could take 30 minutes or longer. W0000 00:00:1742007769.202246  317963 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jitcompiled from PTX, which could take 30 minutes or longer. I0000 00:00:1742007769.355021  317963 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29043 MB memory:  > device: 0, name: NVIDIA GeForce RTX 5090, pci bus id: 0000:09:00.0, compute capability: 12.0 tf.version.VERSION = 2.20.0dev20250314 tf.version.GIT_VERSION = v1.12.1123444g07ff428d432 tf.version.COMPILER_VERSION = Ubuntu Clang 18.1.8 (++20240731024944+3b5b5c1ec4a31~exp1~20240731145000.144) Sanity check:  libcudnn not found == env ============================================================= LD_LIBRARY_PATH /usr/local/cuda12.8/lib64: DYLD_LIBRARY_PATH is unset == nvidiasmi ====================================================== Fri Mar 14 21:02:52 2025 ++  ++ == cuda libs ======================================================= /usr/local/cuda11.8/targets/x86_64linux/lib/libcudart_static.a /usr/local/cuda11.8/targets/x86_64linux/lib/libcudart.so.11.8.89 /usr/local/cuda12.8/targets/x86_64linux/lib/libcudart_static.a /usr/local/cuda12.8/targets/x86_64linux/lib/libcudart.so.12.8.90 == tensorflow installation ========================================= tensorflow not found == tf_nightly installation ========================================= Name: tf_nightly Version: 2.20.0.dev20250314 Summary: TensorFlow is an open source machine learning framework for everyone. Homepage: https://www.tensorflow.org/ Authoremail: packages.org License: Apache 2.0 Location: /home/mitch/.virtualenvs/tfnightie/lib/python3.10/sitepackages Requiredby: == python version ================================================== (major, minor, micro, releaselevel, serial) (3, 10, 12, 'final', 0) == bazel version =================================================== Bazelisk version: v1.25.0 Build label: 7.4.1 Build time: Mon Nov 11 21:24:53 2024 (1731360293) Build timestamp: 1731360293 Build timestamp as int: 1731360293 ```  Standalone code to reproduce the issue ```shell Try running anything with an RTX 5090. My test script is above. ```  Relevant log output ```shell ```",2025-03-15T03:40:16Z,stat:awaiting tensorflower type:bug wsl2 TF 2.18,open,2,71,https://github.com/tensorflow/tensorflow/issues/89272,same problem,"I should mention that I'm a Senior AI Developer by trade and I'm more than happy to invest my personal time in helping to fix this, I'm just not sure where to start.","I should also mention that the latest clang release here supports building for compute_100/sm_100+ https://github.com/llvm/llvmproject/releases/tag/llvmorg20.1.0 It's not supported in LLVM 18. But it compiles this on my GPU just fine (extra logs attached just in case they help someone else). ``` mitch:~/stable_diff/build_tf/hello/hello_nvcc $ clang++ version clang version 20.1.0 (https://github.com/llvm/llvmproject 24a30daaa559829ad079f2ff7f73eb4e18095f88) Target: x86_64unknownlinuxgnu Thread model: posix InstalledDir: /home/mitch/stable_diff/fix_tf/llvm/LLVM20.1.0LinuxX64/bin mitch:~/stable_diff/build_tf/hello/hello_nvcc $ cat card_details.cu include  include  int main() {     cudaDeviceProp prop;     int device;     cudaGetDevice(&device); // Get the current device ID     cudaGetDeviceProperties(&prop, device); // Get device properties     size_t free_mem, total_mem;     cudaMemGetInfo(&free_mem, &total_mem); // Get VRAM usage     std::cout << ""GPU Name: "" << prop.name << std::endl;     std::cout << ""Compute Capability: "" << prop.major << ""."" << prop.minor << std::endl;     std::cout << ""VRAM Usage: "" << (total_mem  free_mem) / (1024 * 1024) << "" MB / "" << total_mem / (1024 * 1024) << "" MB"" << std::endl;     return 0; } mitch:~/stable_diff/build_tf/hello/hello_nvcc $ clang++ std=c++17 cudagpuarch=sm_120 x cuda cudapath=""$CUDA_HOME"" I""$CUDA_HOME/include"" L""$CUDA_HOME/lib64""  lcudart card_details.cu o card_details clang++: warning: CUDA version 12.8 is only partially supported [Wunknowncudaversion] mitch:~/stable_diff/build_tf/hello/hello_nvcc $ ./card_details GPU Name: NVIDIA GeForce RTX 5090 Compute Capability: 12.0 VRAM Usage: 1763 MB / 32606 MB mitch:~/stable_diff/build_tf/hello/hello_nvcc $ echo ""$CUDA_HOME"" /usr/local/cuda12.8 mitch:~/stable_diff/build_tf/hello/hello_nvcc $ ls ""$CUDA_HOME"" DOCS  EULA.txt  README  bin  computesanitizer  doc  extras  gds  include  lib64  libnvvp  nsightee_plugins  nvml  nvvm  share  src  targets  tools  version.json mitch:~/stable_diff/build_tf/hello/hello_nvcc $ cat /usr/local/cuda12.8/version.json | head n5 {    ""cuda"" : {       ""name"" : ""CUDA SDK"",       ""version"" : ""12.8.1""    }, ```","I'm going to keep writing my attempts to get things working here. I've cut a branch on my fork, still no luck, but here's some halfdiscoveries. More and more of the project is building as I continue, zero idea how far away I am from victory. He's the branch I'm on, compared with the base: https://github.com/maludwig/tensorflow/compare/ml/fixing_tf_env...maludwig:tensorflow:ml/attempting_build_rtx5090?expand=1 A few findings:  CUDA 12.8.1 adds support for the RTX 5090 (and other Blackwells), so we need that  There's a bug in cutlass, which was forked for tensorflow for a reason I don't know, the bug was fixed here: https://github.com/NVIDIA/cutlass/pull/1784/files  The old fork, done by  was certainly done for a reason, no idea what I'm breaking by going back to the NVIDIA main branch here. Not sure how to message people on GitHub, but maybe they'll get notified on this?  I updated NCCL to the latest 2.26.2 wheel  Build is still failing, but it's taking WAY longer to fail now. This is possibly a good sign.","Yep, I'm stopping for the night, it's currently stuck on what seems to be duplicate logging macros, looks like maybe two different logging libraries are somehow being included at the same time. Two very very similar logging libraries. But instead of taking 30 seconds before it fails, now it takes 17 minutes to fail, which I define as progress! ``` external/com_google_absl/absl/log/check.h:122:9: warning: 'CHECK_LT' macro redefined [Wmacroredefined]   122          ^ In file included from tensorflow/core/kernels/fill_empty_rows_functor_gpu.cu.cc:21: In file included from ./tensorflow/core/common_runtime/gpu/gpu_event_mgr.h:21: In file included from ./tensorflow/core/common_runtime/device/device_event_mgr.h:30: In file included from ./tensorflow/core/platform/stream_executor.h:21: In file included from external/local_xla/xla/stream_executor/dnn.h:47: In file included from external/local_xla/xla/stream_executor/scratch_allocator.h:26: In file included from external/local_xla/xla/stream_executor/device_memory_allocator.h:22: ```"," VICTORY Ok I didn't stop for the night. Instead, I just ignored all manner of warnings that shouldn't be ignored: ``` bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow config=cuda config=cuda_wheel  copt=Wnognuoffsetofextensions copt=Wnoerror copt=Wnoc23extensions verbose_failures copt=Wnomacroredefined ``` And bam! ``` INFO: Found 1 target... Target //tensorflow/tools/pip_package:wheel uptodate:   bazelbin/tensorflow/tools/pip_package/wheel_house/tensorflow2.20.0.dev0+selfbuiltcp310cp310linux_x86_64.whl INFO: Elapsed time: 87.690s, Critical Path: 86.67s INFO: 2 processes: 1 internal, 1 local. INFO: Build completed successfully, 2 total actions ``` No idea if it'll work, but it **did build**! I've pushed the latest code changes to my branch. https://github.com/maludwig/tensorflow/compare/ml/fixing_tf_env...maludwig:tensorflow:ml/attempting_build_rtx5090?expand=1","It passed one test! ``` (tfnightie) mitch:~/stable_diff/fix_tf/tensorflow $ bazel test repo_env=WHEEL_NAME=tensorflow config=cuda config=cuda_wheel  copt=Wnognuoffsetofextensions copt=Wnoerror copt=Wnoc23extensions verbose_failures copt=Wnomacroredefined tensorflow/python/kernel_tests/nn_ops:softmax_op_test WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Reading 'startup' options from /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=243 INFO: Reading rc options for 'test' from /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc:   Inherited 'common' options: announce_rc experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility noenable_bzlmod noincompatible_enable_cc_toolchain_resolution noincompatible_enable_android_toolchain_resolution experimental_repo_remote_exec java_runtime_version=remotejdk_21 INFO: Reading rc options for 'test' from /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc:   Inherited 'build' options: repo_env=ML_WHEEL_TYPE=snapshot repo_env=ML_WHEEL_BUILD_DATE= repo_env=ML_WHEEL_VERSION_SUFFIX= define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive host_features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 INFO: Reading rc options for 'test' from /home/mitch/stable_diff/fix_tf/tensorflow/.tf_configure.bazelrc:   Inherited 'build' options: action_env PYTHON_BIN_PATH=/home/mitch/.virtualenvs/tfnightie/bin/python action_env PYTHON_LIB_PATH=/home/mitch/.virtualenvs/tfnightie/lib/python3.10/sitepackages python_path=/home/mitch/.virtualenvs/tfnightie/bin/python action_env LD_LIBRARY_PATH=/usr/local/cuda12.8/lib64:/home/mitch/stable_diff/fix_tf/libs/cudnnlinuxx86_649.8.0.87_cuda12archive/lib: config=cuda_clang action_env CLANG_CUDA_COMPILER_PATH=/home/mitch/stable_diff/fix_tf/llvm/LLVM20.1.0LinuxX64/bin/clang20 config=cuda_clang INFO: Reading rc options for 'test' from /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc:   'test' options: test_env=GTEST_INSTALL_FAILURE_SIGNAL_HANDLER=1 INFO: Reading rc options for 'test' from /home/mitch/stable_diff/fix_tf/tensorflow/.tf_configure.bazelrc:   'test' options: test_size_filters=small,medium test_env=LD_LIBRARY_PATH INFO: Found applicable config definition build:short_logs in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition test:v2 in file /home/mitch/stable_diff/fix_tf/tensorflow/.tf_configure.bazelrc: test_tag_filters=benchmarktest,no_oss,oss_excluded,no_gpu,oss_serial,v1only build_tag_filters=benchmarktest,no_oss,oss_excluded,no_gpu,v1only INFO: Found applicable config definition build:cuda_clang in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: config=cuda //:cuda_compiler=clang copt=Qunusedarguments repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 copt=Wnounknowncudaversion host_linkopt=fuseld=lld host_linkopt=lm linkopt=fuseld=lld linkopt=lm INFO: Found applicable config definition build:cuda in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda repo_env=HERMETIC_CUDA_VERSION=12.5.1 repo_env=HERMETIC_CUDNN_VERSION=9.3.0 //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /home/mitch/stable_diff/fix_tf/tensorflow/.tf_configure.bazelrc: repo_env HERMETIC_CUDA_VERSION=12.8.1 repo_env HERMETIC_CUDNN_VERSION=9.8.0 repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=compute_120 INFO: Found applicable config definition build:cuda_clang in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: config=cuda //:cuda_compiler=clang copt=Qunusedarguments repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 copt=Wnounknowncudaversion host_linkopt=fuseld=lld host_linkopt=lm linkopt=fuseld=lld linkopt=lm INFO: Found applicable config definition build:cuda in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda repo_env=HERMETIC_CUDA_VERSION=12.5.1 repo_env=HERMETIC_CUDNN_VERSION=9.3.0 //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /home/mitch/stable_diff/fix_tf/tensorflow/.tf_configure.bazelrc: repo_env HERMETIC_CUDA_VERSION=12.8.1 repo_env HERMETIC_CUDNN_VERSION=9.8.0 repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=compute_120 INFO: Found applicable config definition build:cuda in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda repo_env=HERMETIC_CUDA_VERSION=12.5.1 repo_env=HERMETIC_CUDNN_VERSION=9.3.0 //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /home/mitch/stable_diff/fix_tf/tensorflow/.tf_configure.bazelrc: repo_env HERMETIC_CUDA_VERSION=12.8.1 repo_env HERMETIC_CUDNN_VERSION=9.8.0 repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=compute_120 INFO: Found applicable config definition build:cuda_wheel in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: //cuda:include_cuda_libs=false INFO: Found applicable config definition build:linux in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: host_copt=w copt=Wnoall copt=Wnoextra copt=Wnodeprecated copt=Wnodeprecateddeclarations copt=Wnoignoredattributes copt=Wnoarraybounds copt=Wunusedresult copt=Werror=unusedresult copt=Wswitch copt=Werror=switch define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=dynamic_kernels experimental_guard_against_concurrent_changes INFO: Found applicable config definition build:dynamic_kernels in file /home/mitch/stable_diff/fix_tf/tensorflow/.bazelrc: define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS DEBUG: /home/mitch/.cache/bazel/_bazel_mitch/98f54844abcf3e1cdc99e9d96b271d9e/external/local_xla/third_party/py/python_repo.bzl:154:14: HERMETIC_PYTHON_VERSION variable was not set correctly, using default version. Python 3.10 will be used. To select Python version, either set HERMETIC_PYTHON_VERSION env variable in your shell:   export HERMETIC_PYTHON_VERSION=3.12 OR pass it as an argument to bazel command directly or inside your .bazelrc file:   repo_env=HERMETIC_PYTHON_VERSION=3.12 DEBUG: /home/mitch/.cache/bazel/_bazel_mitch/98f54844abcf3e1cdc99e9d96b271d9e/external/local_xla/third_party/py/python_repo.bzl:87:10: ============================= Hermetic Python configuration: Version: ""3.10"" Kind: """" Interpreter: ""default"" (provided by rules_python) Requirements_lock label: ""//:requirements_lock_3_10.txt"" ===================================== WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. WARNING: Build options @//cuda:include_cuda_libs, copt, cxxopt, and 2 more have changed, discarding analysis cache (this can be expensive, see https://bazel.build/advanced/performance/iterationspeed). INFO: Analyzed 2 targets (749 packages loaded, 56015 targets configured). INFO: Found 2 test targets... INFO: Elapsed time: 270.116s, Critical Path: 245.71s INFO: 2560 processes: 378 internal, 2182 local. INFO: Build completed successfully, 2560 total actions //tensorflow/python/kernel_tests/nn_ops:softmax_op_test_cpu              PASSED in 217.4s //tensorflow/python/kernel_tests/nn_ops:softmax_op_test_gpu              PASSED in 218.4s Executed 2 out of 2 tests: 2 tests pass. ``` I also installed the wheel generated in the last step to a new python venv, and it worked! ``` (test5090build) mitch:~/stable_diff/fix_tf/test5090build $ python c ""import tensorflow as tf; print(tf.__version__)"" 20250317 04:37:51.455319: I external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1742207871.466384  646442 cuda_dnn.cc:8670] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered I0000 00:00:1742207871.469996  646442 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered W0000 00:00:1742207871.479137  646442 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207871.479166  646442 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207871.479169  646442 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207871.479172  646442 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. 20250317 04:37:51.481701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 2.20.0dev0+selfbuilt (test5090build) mitch:~/stable_diff/fix_tf/test5090build $ python c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"" 20250317 04:38:02.348770: I external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1742207882.360431  646471 cuda_dnn.cc:8670] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered I0000 00:00:1742207882.364089  646471 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered W0000 00:00:1742207882.373383  646471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207882.373422  646471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207882.373426  646471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207882.373437  646471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. 20250317 04:38:02.376028: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] (test5090build) mitch:~/stable_diff/fix_tf/test5090build $ cat test_gpu.py import tensorflow as tf  Check if GPU is available gpus = tf.config.list_physical_devices('GPU') if not gpus:     print(""🚫 No GPU found!"") else:     print(f""✅ Found GPU(s): {[gpu.name for gpu in gpus]}"")  Place operations on GPU with tf.device('/GPU:0'):      Create two tensors     a = tf.constant([[1.0, 2.0], [3.0, 4.0]])     b = tf.constant([[5.0, 6.0], [7.0, 8.0]])      Add tensors     add_result = tf.add(a, b)     print(""\nAddition result:"")     print(add_result)      Matrix multiplication     matmul_result = tf.matmul(a, b)     print(""\nMatrix multiplication result:"")     print(matmul_result)  Print device placement info (optional, debug) print(""\nDevice placement log:"") tf.debugging.set_log_device_placement(True) (test5090build) mitch:~/stable_diff/fix_tf/test5090build $ python test_gpu.py 20250317 04:38:25.409242: I external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1742207905.420314  646517 cuda_dnn.cc:8670] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered I0000 00:00:1742207905.423851  646517 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered W0000 00:00:1742207905.432651  646517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207905.432680  646517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207905.432684  646517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742207905.432686  646517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. 20250317 04:38:25.435305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. ✅ Found GPU(s): ['/physical_device:GPU:0'] I0000 00:00:1742207906.790435  646517 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29043 MB memory:  > device: 0, name: NVIDIA GeForce RTX 5090, pci bus id: 0000:09:00.0, compute capability: 12.0 Addition result: tf.Tensor( [[ 6.  8.]  [10. 12.]], shape=(2, 2), dtype=float32) Matrix multiplication result: tf.Tensor( [[19. 22.]  [43. 50.]], shape=(2, 2), dtype=float32) Device placement log: ``` I...am...going...to...run all the tests overnight? My build process is complete trash and I have no idea what I'm doing, but I COULD also PR this code, but like, that's slightly terrifying. I've ignoring probably thousands of warnings that a competent C++ developer could probably actually solve, rather than just ignore...","Tests didn't pass, but it did build! And it could do basic matrix addition and multiplication in Python! NOW I'm definitely going to bed though.","It also is able to do the classic ""hello world"" ML task of learning digits on MNIST, but the warnings are PLENTIFUL and cryptic. I don't know what they mean, but the final model happens to work great! ``` (test5090build) mitch:~/stable_diff/fix_tf/test5090build $ cat mnist_test.py !/usr/bin/env python import tensorflow as tf from tensorflow.keras import layers, models import numpy as np  Load MNIST dataset (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()  Normalize pixel values to [0,1] x_train = x_train / 255.0 x_test = x_test / 255.0  Build the model model = models.Sequential([     layers.Flatten(input_shape=(28, 28)),       Flatten 28x28 to 784     layers.Dense(128, activation='relu'),       Hidden layer     layers.Dense(10, activation='softmax')      Output layer ])  Compile the model model.compile(optimizer='adam',               loss='sparse_categorical_crossentropy',               metrics=['accuracy'])  Train the model model.fit(x_train, y_train, epochs=5, validation_split=0.1)  Evaluate the model test_loss, test_acc = model.evaluate(x_test, y_test) print(f""Test accuracy: {test_acc:.4f}"")  Make predictions predictions = model.predict(x_test)  Example: Print prediction for the first image print(f""First test sample  Predicted: {np.argmax(predictions[0])}, Actual: {y_test[0]}"") (test5090build) mitch:~/stable_diff/fix_tf/test5090build $ ./mnist_test.py 20250317 11:23:11.786039: I external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1742232191.796888  662647 cuda_dnn.cc:8670] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered I0000 00:00:1742232191.800405  662647 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered W0000 00:00:1742232191.809207  662647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742232191.809234  662647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742232191.809238  662647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. W0000 00:00:1742232191.809259  662647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once. 20250317 11:23:11.811904: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. /home/mitch/.virtualenvs/test5090build/lib/python3.10/sitepackages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.   super().__init__(**kwargs) I0000 00:00:1742232193.910442  662647 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29043 MB memory:  > device: 0, name: NVIDIA GeForce RTX 5090, pci bus id: 0000:09:00.0, compute capability: 12.0 Epoch 1/5 20250317 11:23:15.366974: I external/local_xla/xla/service/service.cc:152] XLA service 0x7f5928008d30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 20250317 11:23:15.367006: I external/local_xla/xla/service/service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 5090, Compute Capability 12.0 20250317 11:23:15.376904: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. I0000 00:00:1742232195.434380  662725 cuda_dnn.cc:529] Loaded cuDNN version 90800 20250317 11:23:16.211437: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 80 bytes spill stores, 80 bytes spill loads 20250317 11:23:16.220559: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95_0', 164 bytes spill stores, 164 bytes spill loads 20250317 11:23:16.340804: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 392 bytes spill stores, 392 bytes spill loads 20250317 11:23:16.364181: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 76 bytes spill stores, 76 bytes spill loads 20250317 11:23:16.374280: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_232', 176 bytes spill stores, 176 bytes spill loads 20250317 11:23:16.385374: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 76 bytes spill stores, 76 bytes spill loads 20250317 11:23:16.393417: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 292 bytes spill stores, 292 bytes spill loads 20250317 11:23:16.451825: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 532 bytes spill stores, 532 bytes spill loads 20250317 11:23:16.522600: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_232', 168 bytes spill stores, 168 bytes spill loads 20250317 11:23:16.556430: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 1040 bytes spill stores, 1040 bytes spill loads 20250317 11:23:16.607519: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_232', 112 bytes spill stores, 112 bytes spill loads 20250317 11:23:16.806055: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 4920 bytes spill stores, 4992 bytes spill loads 20250317 11:23:16.867917: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 5084 bytes spill stores, 5028 bytes spill loads WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1742232197.511568  662725 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process. 1684/1688 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  accuracy: 0.8767  loss: 0.446420250317 11:23:20.774706: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_232', 32 bytes spill stores, 32 bytes spill loads 20250317 11:23:20.804309: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 288 bytes spill stores, 288 bytes spill loads 20250317 11:23:20.807183: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 76 bytes spill stores, 76 bytes spill loads 20250317 11:23:20.828074: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 76 bytes spill stores, 76 bytes spill loads 20250317 11:23:20.895561: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 752 bytes spill stores, 752 bytes spill loads 20250317 11:23:21.076717: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_232', 80 bytes spill stores, 80 bytes spill loads 20250317 11:23:21.096547: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_232', 72 bytes spill stores, 72 bytes spill loads 20250317 11:23:21.177785: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 4920 bytes spill stores, 4992 bytes spill loads 20250317 11:23:21.227351: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_95', 5084 bytes spill stores, 5028 bytes spill loads 1688/1688 ━━━━━━━━━━━━━━━━━━━━ 8s 3ms/step  accuracy: 0.8768  loss: 0.4459  val_accuracy: 0.9668  val_loss: 0.1275 Epoch 2/5 1688/1688 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step  accuracy: 0.9598  loss: 0.1359  val_accuracy: 0.9710  val_loss: 0.0985 Epoch 3/5 1688/1688 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step  accuracy: 0.9748  loss: 0.0853  val_accuracy: 0.9728  val_loss: 0.0920 Epoch 4/5 1688/1688 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step  accuracy: 0.9810  loss: 0.0640  val_accuracy: 0.9775  val_loss: 0.0809 Epoch 5/5 1688/1688 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step  accuracy: 0.9855  loss: 0.0473  val_accuracy: 0.9782  val_loss: 0.0797 313/313 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  accuracy: 0.9720  loss: 0.0911 Test accuracy: 0.9753 313/313 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step First test sample  Predicted: 7, Actual: 7 ```", you should upgrade commit hash256 XLA on bazel file and it should work,"Sorry that's a bit cryptic for me. I'm normally a Python dev, apologies. Did you mean in my commits on my branch above? https://github.com/maludwig/tensorflow/compare/ml/fixing_tf_env...maludwig:tensorflow:ml/attempting_build_rtx5090?expand=1",+1," Steps to get it running on your RTX 5000 series card  Guide for all platforms  Install llvm 20.1.0 LLVM 20.1.0 is required to compile code for compute capability 10.0 and 12.0 (RTX 5000 series). All platforms here: https://github.com/llvm/llvmproject/releases/tag/llvmorg20.1.0  Install CUDA 12.8.1 CUDA 12.8.1 is required to compile code for compute capability 10.0 and 12.0 (RTX 5000 series). Also install cuDNN 9.8.0 and NCCL 2, for CUDA 12.  Install Python 3.10.12 This just happens to be the version I'm using and may be completely unnecessary. I personally love pyenv because it installs it to your local user, so you don't need to fret about admin/root permissions.  Make a Python venv for tensorflow This will prevent your system from being polluted by tensorflow dependencies, and will make it much much much easier to clean up if you want to start over.  Install Bazelisk Bazelisk is a wrapper for Bazel that downloads the correct version of Bazel for the project.  Clone tensorflow ```bash echo ""Clone tensorflow"" git clone git.com:tensorflow/tensorflow.git cd tensorflow echo ""Add my remote to the repo"" git remote add maludwig 'git.com:maludwig/tensorflow.git' echo ""Fetch my remote"" git fetch all echo ""Checkout my branch"" git checkout ml/attempting_build_rtx5090 echo ""Pull my branch"" git pull maludwig ml/attempting_build_rtx5090 ```  Configure bazel ```bash echo ""Configure bazel, these are the settings I used, but I'm not sure if they're correct, or if they just happened to work for me."" export HERMETIC_CUDA_VERSION=12.8.1 export HERMETIC_CUDNN_VERSION=9.8.0 export HERMETIC_CUDA_COMPUTE_CAPABILITIES=compute_120 export LOCAL_CUDA_PATH=/usr/local/cuda12.8 export LOCAL_NCCL_PATH=/usr/lib/x86_64linuxgnu/libnccl.so.2.26.2 export TF_NEED_CUDA=1 export CLANG_CUDA_COMPILER_PATH=""$(which clang)"" python configure.py ```  Build tensorflow ```bash echo ""Good luck building!"" echo ""Note, I have trust issues with bazel now, so I always run 'bazel clean expunge' before building. This may be a personal psychological issue rather than a requirement."" bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow config=cuda config=cuda_wheel copt=Wnognuoffsetofextensions copt=Wnoerror copt=Wnoc23extensions verbose_failures copt=Wnomacroredefined ```  Script for WSL Ubuntu 22.04 This script should let you compile for RTX 5000 series on WSL Ubuntu 22.04. Before running this script, be sure to install the latest drivers for your RTX 5000 series card on the Windows side, install WSL2, and use Ubuntu 22.04. Then reboot your PC, that way, WSL2 will be able to see your GPU. It probably also works on nonWSL Ubuntu 22.04. It might maybe work on other Ubuntu versions. It's not going to work for Windows except in WSL. It may not work at all. Consider copying it line by line and handle errors manually. ```bash mkdir p ""$HOME/rtx5000"" cd ""$HOME/rtx5000"" echo ""Installing essential dev tools"" sudo aptget update sudo aptget install y buildessential wget patchelf echo ""Installing Python 3.10"" sudo apt install y make buildessential libssldev zlib1gdev libbz2dev libreadlinedev libsqlite3dev wget curl llvm libncurses5dev libncursesw5dev xzutils tkdev libffidev liblzmadev curl https://pyenv.run | bash pyenv install 3.10.12 pyenv global 3.10.12 echo ""Restart your shell to use Python 3.10"" echo ""After restarting, confirm this says python 3.10.12"" python version echo ""Make a virtualenv for tensorflow"" python3.10 m venv ~/rtx5000/venv echo ""Activate the python virtualenv"" source ~/rtx5000/venv/bin/activate echo ""Installing LLVM 20.1.0"" wget https://github.com/llvm/llvmproject/releases/download/llvmorg20.1.0/LLVM20.1.0LinuxX64.tar.xz tar xvf LLVM20.1.0LinuxX64.tar.xz echo ""Installing NVIDIA packages"" wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cudakeyring_1.11_all.deb sudo dpkg i cudakeyring_1.11_all.deb sudo aptget update echo ""Installing NVIDIA CUDA 12.8"" sudo aptget y install cudatoolkit128 echo ""Installing NVIDIA cuDNN 9, for CUDA 12"" sudo aptget y install cudnn9cuda12 echo ""Installing NVIDIA NCCL 2"" sudo apt install libnccl2=2.26.21+cuda12.8 libnccldev=2.26.21+cuda12.8 echo ""Installing Bazelisk for Bazel"" mkdir p ~/rtx5000/bin cd ~/rtx5000/bin wget 'https://github.com/bazelbuild/bazelisk/releases/download/v1.25.0/bazelisklinuxamd64' chmod +x bazelisklinuxamd64 mv bazelisklinuxamd64 bazel ``` Add these lines to your `~/.bashrc` or `~/.zshrc` file: ``` export LLVM_HOME=""$HOME/rtx5000/LLVM20.1.0LinuxX64"" export CUDA_HOME=""/usr/local/cuda12.8"" export PATH=""${LLVM_HOME}/bin:${CUDA_HOME}/bin:${HOME}/rtx5000/bin:$PATH"" export LD_LIBRARY_PATH=""$CUDA_HOME/lib64:$LD_LIBRARY_PATH"" export CPATH=""$CUDA_HOME/include:$CPATH"" ``` Restart your terminal. Test that the LLVM installation worked: Make this file in `~/rtx5000/card_details.cu`: ```cpp include  include   // Add cuDNN header include  int main() {     cudaDeviceProp prop;     int device;     cudaGetDevice(&device); // Get the current device ID     cudaGetDeviceProperties(&prop, device); // Get device properties     size_t free_mem, total_mem;     cudaMemGetInfo(&free_mem, &total_mem); // Get VRAM usage     std::cout  GPU Name: ""  Compute Capability: ""  VRAM Usage: ""  cuDNN Version: ""                GPU Name: NVIDIA GeForce RTX 5090 > Compute Capability: 12.0 > VRAM Usage: 1763 MB / 32606 MB > cuDNN Version: 9.8.0 echo ""This should compile the code with clang++"" clang++ std=c++17 cudagpuarch=sm_120 x cuda cudapath=""$CUDA_HOME"" I""$CUDA_HOME/include"" L""$CUDA_HOME/lib64""  lcudart card_details.cu o card_details_clang echo ""This should print your card details again, just the same as before"" ./card_details_clang > GPU Name: NVIDIA GeForce RTX 5090 > Compute Capability: 12.0 > VRAM Usage: 1763 MB / 32606 MB > cuDNN Version: 9.8.0 echo ""This should be Bazel v8.8.1"" bazel version echo ""Activate the python virtualenv"" source ~/rtx5000/venv/bin/activate echo ""This should be Python 3.10.12"" python version echo ""Clone tensorflow"" cd ~/rtx5000 git clone git.com:tensorflow/tensorflow.git cd tensorflow echo ""Add my remote to the repo"" git remote add maludwig 'git.com:maludwig/tensorflow.git' echo ""Fetch my remote"" git fetch all echo ""Checkout my branch"" git checkout ml/attempting_build_rtx5090 echo ""Pull my branch"" git pull maludwig ml/attempting_build_rtx5090 echo ""Configure bazel, these are the settings I used, but I'm not sure if they're correct, or if they just happened to work for me."" export HERMETIC_CUDA_VERSION=12.8.1 export HERMETIC_CUDNN_VERSION=9.8.0 export HERMETIC_CUDA_COMPUTE_CAPABILITIES=compute_120 export LOCAL_CUDA_PATH=/usr/local/cuda12.8 export LOCAL_NCCL_PATH=/usr/lib/x86_64linuxgnu/libnccl.so.2.26.2 export TF_NEED_CUDA=1 export CLANG_CUDA_COMPILER_PATH=""$(which clang)"" python configure.py echo ""Good luck building!"" echo ""Note, I have trust issues with bazel now, so I always run 'bazel clean expunge' before building. This may be a personal psychological issue rather than a requirement."" bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow config=cuda config=cuda_wheel  copt=Wnognuoffsetofextensions copt=Wnoerror copt=Wnoc23extensions verbose_failures copt=Wnomacroredefined ```  NOTE You mayyyybe need to get the very latest cuDNN with this, but I don't think so. ``` cd ~/rtx5000 wget https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linuxx86_64/cudnnlinuxx86_649.8.0.87_cuda12archive.tar.xz tar xvf cudnnlinuxx86_649.8.0.87_cuda12archive.tar.xz echo add this to your ~/.bashrc export LD_LIBRARY_PATH=""$HOME/rtx5000/cudnnlinuxx86_649.8.0.87_cuda12archive/lib:$LD_LIBRARY_PATH"" ``` NOTE: If this doesn't work for you, let me know which error you got, and maybe I missed something in my environment. Since this was already my dev box, I'm not sure if this is a complete guide, but it's what I did to get it working.","Hey  , just seeing your tags you added. To be clear, this is on tf_nightly, not tf 2.18, and I have no idea really what I'm doing, so I'm not gonna PR my extremely busted and testsfailing branch, even though it does build. I put it here so that someone who knows what they're doing could fold in the new stuff more easily, or so that other normal humans like me could run tensorflow on an RTX 5000, instead of just being unable to run it. An actual human who knows what they're doing should look this over and figure it out.",cd ~/rtx5000 nvcc o card_details_nvcc card_details.cu bash: cd: /home/nicolai/rtx5000: No such file or directory cc1plus: fatal error: card_details.cu: No such file or directory compilation terminated.,"I tryed to let it run on my wsl. Build dosen't work, how can I use a prebuiled nightly build?   Configuration: 8850a00e136a9e8be32c557a177e77f38f3c27b70c44518acb5ba0af47f7836b  Execution platform: @//:platform In file included from external/local_xla/xla/stream_executor/cuda/cuda_status.cc:16: external/local_xla/xla/stream_executor/cuda/cuda_status.h:22:10: fatal error: 'third_party/gpus/cuda/include/cuda.h' file not found    22           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 1 error generated. Target //tensorflow/tools/pip_package:wheel failed to build ERROR: /mnt/c/Projekte/tmp/tensorflow/tensorflow/tools/pip_package/BUILD:293:9 Action tensorflow/tools/pip_package/wheel_house/tensorflow2.20.0.dev0+selfbuiltcp312cp312linux_x86_64.whl failed: (Exit 1): clang20 failed: error executing CppCompile command (from target @//xla/stream_executor/cuda:cuda_status)   (cd /root/.cache/bazel/_bazel_root/509ab554767d44265e0030c4731aba07/execroot/org_tensorflow && \   exec env  \     CLANG_CUDA_COMPILER_PATH=/usr/local/bin/clang20 \     PATH=/root/.cache/bazelisk/downloads/sha256/c97f02133adce63f0c28678ac1f21d65fa8255c80429b588aeeba8a1fac6202b/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \     PWD=/proc/self/cwd \     PYTHON_BIN_PATH=/mnt/c/Projekte/env/bin/python3 \     PYTHON_LIB_PATH=/mnt/c/Projekte/env/lib/python3.12/sitepackages \     TF2_BEHAVIOR=1 \   /usr/local/bin/clang20 MD MF bazelout/k8opt/bin/external/local_xla/xla/stream_executor/cuda/_objs/cuda_status/cuda_status.pic.d 'frandomseed=bazelout/k8opt/bin/external/local_xla/xla/stream_executor/cuda/_objs/cuda_status/cuda_status.pic.o' iquote external/local_xla iquote bazelout/k8opt/bin/external/local_xla iquote external/com_google_absl iquote bazelout/k8opt/bin/external/com_google_absl iquote external/local_config_cuda iquote bazelout/k8opt/bin/external/local_config_cuda iquote external/cuda_cudart iquote bazelout/k8opt/bin/external/cuda_cudart iquote external/cuda_cublas iquote bazelout/k8opt/bin/external/cuda_cublas iquote external/cuda_cccl iquote bazelout/k8opt/bin/external/cuda_cccl iquote external/cuda_nvtx iquote bazelout/k8opt/bin/external/cuda_nvtx iquote external/cuda_nvcc iquote bazelout/k8opt/bin/external/cuda_nvcc iquote external/cuda_cusolver iquote bazelout/k8opt/bin/external/cuda_cusolver iquote external/cuda_cufft iquote bazelout/k8opt/bin/external/cuda_cufft iquote external/cuda_cusparse iquote bazelout/k8opt/bin/external/cuda_cusparse iquote external/cuda_curand iquote bazelout/k8opt/bin/external/cuda_curand iquote external/cuda_cupti iquote bazelout/k8opt/bin/external/cuda_cupti iquote external/cuda_nvml iquote bazelout/k8opt/bin/external/cuda_nvml iquote external/cuda_nvjitlink iquote bazelout/k8opt/bin/external/cuda_nvjitlink iquote external/local_tsl iquote bazelout/k8opt/bin/external/local_tsl Ibazelout/k8opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers Ibazelout/k8opt/bin/external/cuda_cudart/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cublas/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cccl/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_nvtx/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_nvcc/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cusolver/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cufft/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cusparse/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_curand/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_cupti/_virtual_includes/headers Ibazelout/k8opt/bin/external/cuda_nvml/_virtual_includes/headers Ibaroot199root199P461:/mroot199P461:/mroot199P461:/mroot199P461:/mroot199P461:/mroot199P461:/mnt/c/Projekte/tmp/tensorflow    ", and  is there a build that I can use? (like nightly build),"Hey  , scroll up until you see ""Script for WSL Ubuntu 22.04"" in the comments.  The issue I raised is that there is no build, nightly or otherwise, that supports the latest Blackwell GPUs. I arguably managed to build one myself. You also could. But read through the script I put up above slowly. It looks like you missed some steps. HOPEFULLY the script I wrote will work for someone else, but since I got it working on an old dev box, rather than a brand fresh new black docker container or something, it's likely that I missed a dependency or two."," dosen't work for me. echo ""This should be Bazel v8.8.1"" bazel version here I get only 8.1.1 and I get some Errors for the build. is there any chance when tensorflow will support the 5090 on its own and I can simply use the next version of tensorflow? If so, please give me a date when.","Yes, I feel problem with NVIDIA RTX 5090  32GB Blackwell (not nightly version of PyTorch). I cannot see GPU with TensorFlow success. Can you take a look at https://gist.github.com/donhuvy/6cd637a09b034168d01181d5ce98a5fe  . I catch `Num GPUs Available:  0` . My environment: Windows 11 pro, JupyterLab latest version, Python 3.11.x .",wait so you got it to work 100%. it sucks my system has 2 5090's and i'm using a cpu for training.,"Whenever I could not get drivers to work, it usually resolved after installing, reinstalling and changing versions of different packages, since the shortage I doubt there is overwhelming support for the 5090, I remember all launches to have crashing and minimal error bugs that disappear over a relatively short period of time. I got stuck a while ago similarly on different cards and in general, it might be a tiny thing somewhere with your paths and env. Try on Linux and see if that works, I don't know why you are using Windows as a senior Dev. My speeds on the 4090 doubled on render times for anything AI/ML related and loading times of nearly everything python vanished.",">  dosen't work for me. > echo ""This should be Bazel v8.8.1"" > bazel version here I get only 8.1.1 and I get some Errors for the build. >  >  > is there any chance when tensorflow will support the 5090 on its own and I can simply use the next version of tensorflow? >  > If so, please give me a date when. >  Sorry  , I'm not a tensorflow employee. I'm just some dude. Can't guess when it will be fixed. I just got my build to work and my personal projects running fine. My tests are failing and I assume that needs resolving. If your Bazel version is wrong, try installing Bazelisk. See above for instructions. ","> wait so you got it to work 100%. it sucks my system has 2 5090's and i'm using a cpu for training. >  Yep. For my workflow (training StableDiffusion LoRAs) it works fine. The tests are failing locally, but they must be testing tensorflow components that I am not using. You could presumably try following in my footsteps and use your 5090s. ","> Try on Linux and see if that works, I don't know why you are using Windows as a senior Dev. My speeds on the 4090 doubled on render times for anything AI/ML related and loading times of nearly everything python vanished. I'm also a senior dev, and while I agree in general that Linux is better and faster, Windows is still a perfectly legit OS. In fact, Apple Silicon is quite nice for training too. There's no distinction between RAM and VRAM in arm64a. Huge models run on consumer hardware. Not near as fast as on nvidia, but OSX is a legit OS too. "," I tried following the WSL script and got this error in the final step: ""external/local_tsl/tsl/profiler/lib/nvtx_utils.cc:32:10: fatal error: 'third_party/gpus/cuda/include/cuda.h' file not found"". All previous steps were OK, such as the one building the .cu file using clang. ","  What's your HERMETIC_CUDA_VERSION? It should be 12.8.1 Apart from that, maybe try cleaning the Bazel cache? ```  Double check CUDA echo ""HERMETIC_CUDA_VERSION: $HERMETIC_CUDA_VERSION""  I have trust issues with every cache thing bazel clean expunge ```"," It is 12.8.1 correctly. I also tried cleaning the Bazel cache, the error was same: header files in 'third_party/gpus/cuda/include' cannot be found. "," Some additional info: among the error verbose text, it displayed some environmental variables such as ""LD_LIBRARY_PATH"" ""PATH"", etc, but no ""CPATH"" can be seen. Could this be related to the issue?"," find nvtx_utils. include ""cuda.h""  > [](https://github.com/maludwig) I tried following the WSL script and got this error in the final step: ""external/local_tsl/tsl/profiler/lib/nvtx_utils.cc:32:10: fatal error: 'third_party/gpus/cuda/include/cuda.h' file not found"". All previous steps were OK, such as the one building the .cu file using clang."
maludwig,Fixing bugs and old code in the tf_env script,"I was trying to make a bug report, and the Github Issues thing told me to run this script, which failed to run and produced a lot of buggy output. I didn't try to make any changes to the script, since I'm not strictly sure what all of these help with, I just tried to stabilize the script, make the code cleaner, and account for common bugs in bash scripts (like a space in the path to python). I'll make comments on the PR to explain each change.",2025-03-15T03:11:04Z,ready to pull size:M,closed,0,7,https://github.com/tensorflow/tensorflow/issues/89271,"Sorry, maybe I should have merged right away rather than making the changes. Not sure what the process is for letting you know, but I've made extremely minor modifications as suggested to the top of the file.","Sorry if I'm doing it wrong, new to contributing to this project, but I added the comment lines you suggested, can you rereview,  ?","Sorry for the delay, I was not able to do much TF reviews over the past few days.","So, now what happens. I can't seem to merge this. Who does the merging?","If you look at https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.mdcontributorlicenseagreements, there's a graph just above the section I just linked to. The PR needs to get imported internally and there are more checks and reviews there. You only see what is above the topmost line.","Oh, ok, I expected it to be like an automated thing, where once it gets approved then it rather quickly gets merged, if tests pass. So now I just wait then I guess? I just found a different PR (unrelated) that you approved and then it got merged like 2 weeks later. Is this normal? https://github.com/tensorflow/tensorflow/pull/73692","Yeah, sadly we only have one single engineer that is pinged when the internal change gets imported, plus the people that review the PR. But if I review the PR I cannot also approve the internal change, so either I need to add someone else or the pinged engineer has to review. The nice thing is that here everything seems to pass, so once we get the internal approval this should be ready to merge."
copybara-service[bot],Test changes in stubs.,Test changes in stubs.,2025-03-15T02:44:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89270
kaushikn02,A dynamic link library (DLL) initialization routine failed.," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.8  Custom code Yes  OS platform and distribution windows 11  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[3], line 1 > 1 import tensorflow as tf       2 print(tf.__version__) File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\hp\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.  Standalone code to reproduce the issue ```shell ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:73      72 try: > 73   from tensorflow.python._pywrap_tensorflow_internal import *      74  This try catch logic is because there is no bazel equivalent for py_extension.      75  Externally in opensource we must enable exceptions to load the shared object      76  by exposing the PyInit symbols with pybind. This error will only be      77  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      78       79  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[3], line 1 > 1 import tensorflow as tf       2 print(tf.__version__) File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:88      86     sys.setdlopenflags(_default_dlopen_flags)      87 except ImportError: > 88   raise ImportError(      89       f'{traceback.format_exc()}'      90       f'\n\nFailed to load the native TensorFlow runtime.\n'      91       f'See https://www.tensorflow.org/install/errors '      92       f'for some common causes and solutions.\n'      93       f'If you need help, create an issue '      94       f'at https://github.com/tensorflow/tensorflow/issues '      95       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\hp\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```  Relevant log output ```shell ```",2025-03-15T01:59:40Z,type:build/install,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89269,please update the MSVC 2019 redistributable，download links: https://download.visualstudio.microsoft.com/download/pr/285b28c73cf947fb9be801cf5323a8df/8F9FB1B3CFE6E5092CF1225ECD6659DAB7CE50B8BF935CB79BFEDE1F3C895240/VC_redist.x64.exe,Please search for duplicates before opening a new issue,Are you satisfied with the resolution of your issue? Yes No
hhoppe,"MemoryError during ""import tensorflow"" with tensorflow-cpu==2.19.0"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tensorflowcpu 2.19.0  Custom code No  OS platform and distribution WSL Ubuntu 22.04.5  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? ""import tensorflow"" crashes.  Standalone code to reproduce the issue ```shell pip install U tensorflowcpu==2.19.0 numpy keras python c ""import tensorflow""   Crashes; see log below.  It works OK with 2.18.0 and 2.18.1 ```  Relevant log output ```shell 20250314 18:21:25.978099: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. Traceback (most recent call last):   File """", line 1, in    File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/__init__.py"", line 49, in      from tensorflow._api.v2 import __internal__   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/_api/v2/__internal__/__init__.py"", line 8, in      from tensorflow._api.v2.__internal__ import autograph   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/_api/v2/__internal__/autograph/__init__.py"", line 8, in      from tensorflow.python.autograph.core.ag_ctx import control_status_ctx  line: 34   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/python/autograph/core/ag_ctx.py"", line 21, in      from tensorflow.python.autograph.utils import ag_logging   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/python/autograph/utils/__init__.py"", line 17, in      from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/python/autograph/utils/context_managers.py"", line 19, in      from tensorflow.python.framework import ops   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/python/framework/ops.py"", line 33, in      from tensorflow.core.framework import attr_value_pb2   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/core/framework/attr_value_pb2.py"", line 14, in      from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/core/framework/tensor_pb2.py"", line 14, in      from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/core/framework/resource_handle_pb2.py"", line 14, in      from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/tensorflow/core/framework/tensor_shape_pb2.py"", line 18, in      _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/google/protobuf/internal/builder.py"", line 67, in BuildMessageAndEnumDescriptors     BuildNestedDescriptors(msg_des, module_name + '_')   File ""/home/hhoppe/.local/lib/python3.10/sitepackages/google/protobuf/internal/builder.py"", line 57, in BuildNestedDescriptors     for (name, nested_msg) in msg_des.nested_types_by_name.items(): MemoryError ```",2025-03-15T01:42:02Z,stat:awaiting response type:bug wsl2 TF 2.18,closed,0,9,https://github.com/tensorflow/tensorflow/issues/89268,"Hello  , please try to install the dependencies in a virtual environment. The python version I have in my env is `3.12.8`. I ran the codes as below: ``` mamba env create n tensor mamba activate tensor pip install U tensorflowcpu==2.19.0 numpy keras python c ""import tensorflow"" ``` The result: ``` 20250327 08:19:31.700350: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. ```","Thanks  . (I used `python m venv` instead of `mamba`.) With `pip install U tensorflowcpu==2.19.0 numpy keras`, it works OK. The problem is that I also include `tensorflowdatasets`. With `pip install U tensorflowcpu==2.19.0 numpy keras tensorflowdatasets`, it fails: Package `tensorflowdatasets` depends on `tensorflowmetadata`; `tensorflowmetadata 1.16.1 requires protobuf=3.20.3; python_version < ""3.11"", but you have protobuf 5.29.4 which is incompatible.` The resulting older `protobuf3.20.3` causes the failure. So the problem for me is the dependency in `tensorflowmetadata`, which may be outdated?","(In addition, `tensorflowcpu` should somehow depend on the more recent version of `protobuf` ?)","Hello  ,  I tested the Python command you mentioned earlier with the new dependencies you provided and encountered no errors. Additionally, I’ve attached the requirments.txt file I used. The issue might be resolved by changing the Python version in your virtual environment. requirments.txt","Hi  , Apologies for the delay, and thank you for raising your concern here. I tried running your code in the same environment you mentioned, but I did not encounter any issues. I’m attaching a screenshot for your reference. !Image Please let me know if I made any mistakes. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"With `pip install U tensorflowcpu==2.19.0 numpy keras tensorflowdatasets`, I now see: `Successfully installed MarkupSafe3.0.2 abslpy2.2.2 array_record0.7.2 astunparse1.6.3 attrs25.3.0 certifi2025.4.26 charsetnormalizer3.4.2 dmtree0.1.9 docstringparser0.16 einops0.8.1 etils1.12.2 flatbuffers25.2.10 fsspec2025.3.2 gast0.6.0 googlepasta0.2.0 grpcio1.71.0 h5py3.13.0 idna3.10 immutabledict4.2.1 importlib_resources6.5.2 keras3.9.2 libclang18.1.1 markdown3.8 markdownitpy3.0.0 mdurl0.1.2 mldtypes0.5.1 namex0.0.9 numpy2.1.3 opteinsum3.4.0 optree0.15.0 packaging25.0 promise2.3 protobuf4.21.12 psutil7.0.0 pyarrow20.0.0 pygments2.19.1 requests2.32.3 rich14.0.0 simple_parsing0.1.7 six1.17.0 tensorboard2.19.0 tensorboarddataserver0.7.2 tensorflowcpu2.19.0 tensorflowdatasets4.9.8 tensorflowiogcsfilesystem0.37.1 tensorflowmetadata1.17.1 termcolor3.1.0 toml0.10.2 tqdm4.67.1 typingextensions4.13.2 urllib32.4.0 werkzeug3.1.3 wheel0.45.1 wrapt1.17.2 zipp3.21.0` Possibly the upgrade `tensorflowmetadata1.16.1 > tensorflowmetadata1.17.1` allowed the upgrade `protobuf3.20.3 > protobuf4.21.12`. And now it works fine. So we can close this issue.","Hi  , Glad to see your issue has been resolved! Please feel free to close this issue if everything is working as expected. Thank you!",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],"Remove canonicalization from stablehlo <-> hlo conversion apis. Keeping conversion as a pure conversion, without any optimizations/canonicalization. Canonicalization was anyway disabled for current users as it has side effects.","Remove canonicalization from stablehlo  hlo conversion apis. Keeping conversion as a pure conversion, without any optimizations/canonicalization. Canonicalization was anyway disabled for current users as it has side effects. There was a duplicate call to StableToMhlo function, removed.",2025-03-15T00:21:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89267
copybara-service[bot],Fix parsing for `RaggedDotDimensionNumbersAttr` to correctly parse nested attributes.,Fix parsing for `RaggedDotDimensionNumbersAttr` to correctly parse nested attributes. The issue was that the `dot_dimension_numbers` attribute was parsed like raw structlike element instead of being parsed as an `Attribute`.,2025-03-15T00:13:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89266
copybara-service[bot],Add missing parsing for MHLO attributes.,Add missing parsing for MHLO attributes. Some attributes do not parse the closing `>` and they should.,2025-03-15T00:11:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89265
copybara-service[bot],PR #23412: Additional Activations for FP8 Graph Convolutions,"PR CC(Unable to import tensorflow  after successful installation ): Additional Activations for FP8 Graph Convolutions Imported from GitHub PR https://github.com/openxla/xla/pull/23412 Extends the functionality of graphbased FP8 convolutions to include ELU, ReLU6 and Leaky ReLU activations. Also allows instructions to have multiple users in the fused graph. Copybara import of the project:  a9241a6e8b9acfaf7a2a2822fa58318ff7ce07db by Philipp Hack : Adds activations and support for instructions with multiple users in graphbased FP8 convolutions.  a9a055bcf57b082193d90aa2b7cb2e5307953770 by Philipp Hack : Adds activations and support for instructions with multiple users in graphbased FP8 convolutions.  6c3087509f5b25be6236d197ba18d64f373c0954 by Philipp Hack : Adds activations and support for instructions with multiple users in graphbased FP8 convolutions. Merging this change closes CC(Unable to import tensorflow  after successful installation ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23412 from philipphack:u_conv_activations_xla 6c3087509f5b25be6236d197ba18d64f373c0954",2025-03-14T23:42:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89264
copybara-service[bot],#sdy Minor cleanups in ShardyXlaPass,sdy Minor cleanups in ShardyXlaPass Remove unused imports and unify the naming in a couple of places.,2025-03-14T23:36:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89263
copybara-service[bot],Lower lax.ragged_dot_general to chlo.ragged_dot in some cases on tpu.,Lower lax.ragged_dot_general to chlo.ragged_dot in some cases on tpu.,2025-03-14T22:54:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89262
copybara-service[bot],Add select operator patterns where the true/false values are true/false.,Add select operator patterns where the true/false values are true/false.,2025-03-14T22:46:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89261
copybara-service[bot],Fix parameter and tile layout setup in HLO<->MLIR conversion,Fix parameter and tile layout setup in HLOMLIR conversion,2025-03-14T22:22:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89260
copybara-service[bot],Obligatory copy paste of the symlinking build extensions,Obligatory copy paste of the symlinking build extensions,2025-03-14T21:59:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89259
copybara-service[bot],Fix issues with multi-plugin application and JIT within the compiled model api.,"Fix issues with multiplugin application and JIT within the compiled model api. There are a few bugs here. * The apply plugin result doesn't actually populate `new_flatbuffer`, so the compiled model has `nullptr` for flatbuffer in JIT case. * compiled model api not correctly handling litert>tfl Changes: * Push the serialization of the compilation result into caller. * Use the tflite::Model to initialize the flatbuffer within compiled model in the case that it is valid. * Document the various details about who owns the flatbuffer alloc in different situations.",2025-03-14T21:31:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89258
copybara-service[bot],[XLA] fix use after free in HloPassPipeline,[XLA] fix use after free in HloPassPipeline There is no guarantee that module config is will not be updated by passes so any access of debug_options will be use after free.,2025-03-14T21:18:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89257
copybara-service[bot],"Create ml_build container for CUDA 12.8, CUDNN 9.8.","Create ml_build container for CUDA 12.8, CUDNN 9.8. Reverts f3cf4a3a25e94daade0cd6bf0cdf8a3bc60084bc",2025-03-14T21:10:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89256
copybara-service[bot],Add WebP dependency to the portable image library.,Add WebP dependency to the portable image library. Fixes broken build after adding WebPdecoding support to TensorFlow's decode_image op.,2025-03-14T21:01:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89255
copybara-service[bot],Add missing builders to unary operators.,Add missing builders to unary operators.,2025-03-14T19:59:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89254
copybara-service[bot],PR #23669: [NVIDIA GPU] Integrate collective-permute combiner to compiler pipeline,PR CC(Disable warnning logging by default): [NVIDIA GPU] Integrate collectivepermute combiner to compiler pipeline Imported from GitHub PR https://github.com/openxla/xla/pull/23669 This PR integrated collectivepermute combiner to compiler pipeline and added endtoend test. Copybara import of the project:  41f8b36ec244d9172ffd964dd5aec8d4dc53a16d by Terry Sun : integrate to compiler pipeline  f9d738c138b26a2cc102e5e25e9850aac09cc575 by Terry Sun : check cp count  31e30ba15dc9353c21229a4066f5db2fd0df2c72 by Terry Sun : remove symmetric test  62686cf956f792fb0c8d42904202dcdb35addbc7 by Terry Sun : fix merge issue Merging this change closes CC(Disable warnning logging by default) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23669 from terryysun:terryysun/cp_combiner_integration 62686cf956f792fb0c8d42904202dcdb35addbc7,2025-03-14T19:53:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89253
sjh0849,Inconsistent results when running tf.keras.layers.InputLayer," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? The expectation is that the API should raise a ValueError, but it does not pass the test.  Standalone code to reproduce the issue ```shell def test_input_layer_with_none_shape(self):          Test with None as input shape         with self.assertRaises(ValueError):             tf.keras.layers.InputLayer(input_shape=None) ```  Relevant log output ```shell Traceback (most recent call last):   File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/zero_shot/tf/tf.keras.layers.InputLayer.py"", line 55, in test_input_layer_with_none_shape     tf.keras.layers.InputLayer(input_shape=None) AssertionError: ValueError not raised ```",2025-03-14T19:41:05Z,stat:awaiting response type:bug stale comp:keras TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89252,"Hi  , Apologies for the delay, and thank you for your patience. I tried running your code on Colab using TensorFlow version 2.19.0 and nightly, but I did not encounter the error you are facing. I am attaching a gist for your reference. Based on my findings, this issue seems to be more related to Keras. I recommend posting this issue on the kerasteam/keras repository for better support. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Reverts e6b1c6796eebc7184b60c8762dc6247ae2d8f35b,Reverts e6b1c6796eebc7184b60c8762dc6247ae2d8f35b,2025-03-14T19:39:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89251
sjh0849,Getting AttributeError when running tf.ones_like," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux 22.04  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? Getting AttributeError when running tf.ones_like.  Standalone code to reproduce the issue ```shell def test_ones_like_with_name(self):          Test with name         input_tensor = tf.constant([1, 2, 3])         result = tf.ones_like(input_tensor, name=""test_ones_like"")         self.assertEqual(result.op.name, ""test_ones_like"") ```  Relevant log output ```shell Traceback (most recent call last):   File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.ones_like.py"", line 66, in test_ones_like_with_name     self.assertEqual(result.op.name, ""test_ones_like"")   File ""/home/user/anaconda3/lib/python3.8/sitepackages/tensorflow/python/framework/ops.py"", line 444, in __getattr__     self.__getattribute__(name)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/tensorflow/python/framework/ops.py"", line 1305, in op     raise AttributeError( AttributeError: Tensor.op is undefined when eager execution is enabled. ```",2025-03-14T19:34:37Z,type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89250,"Hi  , Apologies for the delay, and thanks for raising your concern here. Could you please provide the full code snippet? This would help debug your issue more accurately. I tried running your code on Colab using TensorFlow versions 2.18.0 and 2.19.0, but I did not encounter any issues. Please find the gist here for your reference. Let me know if you need further assistance. Thank you!","sorry for that again. here is the complete class! ```python import tensorflow as tf import numpy as np import unittest class TestTfOnesLike(unittest.TestCase):     def test_ones_like_with_name(self):          Test with name         input_tensor = tf.constant([1, 2, 3])         result = tf.ones_like(input_tensor, name=""test_ones_like"")         self.assertEqual(result.op.name, ""test_ones_like"") if __name__ == '__main__':     unittest.main() ``` many thanks!"
copybara-service[bot],Reverts f3cf4a3a25e94daade0cd6bf0cdf8a3bc60084bc,Reverts f3cf4a3a25e94daade0cd6bf0cdf8a3bc60084bc,2025-03-14T19:31:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89249
sjh0849,The API documentation of tf.no_op specifies that tf.no_op() should return a TensorFlow Operation.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution N\A  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? The API documentation of tf.no_op specifies that tf.no_op() should return a TensorFlow Operation.  The tests expect that calling tf.no_op() returns an instance of tf.Operation (and that its name property can be set by providing a name, e.g., ""my_no_op""). The error message indicates that tf.no_op() is returning None rather than an Operation (None has no attribute ""name""), causing the tests to fail. Given that the test suite is properly written according to the API's specification and the expected behavior, the issue stems from the source code implementation.  Standalone code to reproduce the issue ```shell def test_no_op_name(self):         """"""Test that a no_op can be created with a specific name.""""""         no_op = tf.no_op(name=""my_no_op"")         self.assertEqual(no_op.name, ""my_no_op"")    def test_no_op_creation(self):         """"""Test that a no_op can be created without errors.""""""         try:             no_op = tf.no_op()             self.assertIsInstance(no_op, tf.Operation)         except Exception as e:             self.fail(f""tf.no_op raised an exception: {e}"") ```  Relevant log output ```shell Traceback (most recent call last):   File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.no_op.py"", line 17, in test_no_op_name     self.assertEqual(no_op.name, ""my_no_op"") AttributeError: 'NoneType' object has no attribute 'name' Traceback (most recent call last):   File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.no_op.py"", line 12, in test_no_op_creation     self.fail(f""tf.no_op raised an exception: {e}"") AssertionError: tf.no_op raised an exception: None is not an instance of  ```",2025-03-14T19:01:19Z,type:bug comp:apis comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89248,"Hi  , Apologies for the delay, and thanks for raising your concern here. Could you please provide the full code snippet? This would help debug your issue more accurately. I tried running your code on Colab using TensorFlow versions 2.18.0 and 2.19.0, but I did not encounter any issues. Please find the gist here for your reference. Let me know if you need further assistance. Thank you!","sorry for that. here is the complete class! ```python import unittest import tensorflow as tf class TestNoOpFunctionality(unittest.TestCase):     def test_no_op_name(self):         """"""Test that a no_op can be created with a specific name.""""""         no_op = tf.no_op(name=""my_no_op"")         self.assertEqual(no_op.name, ""my_no_op"")     def test_no_op_creation(self):         """"""Test that a no_op can be created without errors.""""""         try:             no_op = tf.no_op()             self.assertIsInstance(no_op, tf.Operation)         except Exception as e:             self.fail(f""tf.no_op raised an exception: {e}"") if __name__ == '__main__':     unittest.main() ``` many thanks!"
copybara-service[bot],[documentaiton] Add README.md with description of github actions ci infra.,[documentaiton] Add README.md with description of github actions ci infra.,2025-03-14T18:55:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89247
sjh0849,tf.keras.losses.SparseCategoricalCrossentropy has logical/Doc bug," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? API documentation of tf.keras.losses.SparseCategoricalCrossentropy mentions that one of the parameters can be None, but the implementation does not check None, it checks 'none' which is a string.  Standalone code to reproduce the issue ```shell def test_reduction_none(self):          Test with reduction set to None         y_true = np.array([0, 1, 2])         y_pred = np.array([[0.9, 0.05, 0.05],                            [0.05, 0.9, 0.05],                            [0.05, 0.05, 0.9]])         loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=None)         loss = loss_fn(y_true, y_pred).numpy()         expected_loss = np.log([0.9, 0.9, 0.9])         np.testing.assert_almost_equal(loss, expected_loss, decimal=5) ```  Relevant log output ```shell Traceback (most recent call last):   File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.keras.losses.SparseCategoricalCrossentropy.py"", line 49, in test_reduction_none     loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=None)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/losses.py"", line 1026, in __init__     super().__init__(   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/losses.py"", line 262, in __init__     super().__init__(reduction=reduction, name=name)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/losses.py"", line 93, in __init__     losses_utils.ReductionV2.validate(reduction)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/utils/losses_utils.py"", line 88, in validate     raise ValueError( ValueError: Invalid Reduction Key: None. Expected keys are ""('auto', 'none', 'sum', 'sum_over_batch_size')"" ```",2025-03-14T18:50:50Z,type:bug comp:keras TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89246,"This should be opened against the keras repository (tf_keras or keras directly). From the TF side, the only change that could be done is to update the documentation of the function, make sure the docstring uses the correct phrasing. Looking at https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy I see an example where `None` is being used instead of the string `""none""`, so if the example was a doctest then some test would have failed. We should convert the examples to be doctests. Probably though all of this would happen on keras side, not TF.", I see! Thanks for the clarification. I will report this to the keras repository. Thanks!
sjh0849,I get an invalid shape error when running tf.keras.losses.BinaryCrossentropy," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux 22.04  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? I get an invalid shape error when running ```tf.keras.losses.BinaryCrossentropy```  Standalone code to reproduce the issue ```shell def test_binary_crossentropy_invalid_inputs(self):          Test with invalid inputs         y_true = np.array([0, 1, 0, 1], dtype=np.float32)         y_pred = np.array([0.1, 0.9, 0.2], dtype=np.float32)   Mismatched shape         bce = tf.keras.losses.BinaryCrossentropy()         with self.assertRaises(ValueError):             bce(y_true, y_pred) ```  Relevant log output ```shell Traceback (most recent call last):   File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.keras.losses.BinaryCrossentropy.py"", line 70, in test_binary_crossentropy_invalid_inputs     bce(y_true, y_pred)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/losses.py"", line 152, in __call__     losses = call_fn(y_true, y_pred)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/losses.py"", line 284, in call     return ag_fn(y_true, y_pred, **self._fn_kwargs)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/losses.py"", line 2176, in binary_crossentropy     backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),   File ""/home/user/anaconda3/lib/python3.8/sitepackages/keras/backend.py"", line 5688, in binary_crossentropy     bce = target * tf.math.log(output + epsilon()) tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [4] vs. [3] [Op:Mul] ```",2025-03-14T18:42:02Z,type:bug comp:keras TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89245,"Hi  , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18, 2.19, and the nightly, but I did not encounter any issues. Please find the gist attached for your reference. Thank you!","Hi  , Thanks for getting back to me! I noticed, like the other issue, that the replicable code wasn't sufficient, leading to the failure to call and execute the function. However, I reran the test, but it's not giving me the errors it used to give me. I think it's being flaky."
sjh0849,AssertionError when calling tf.keras.layers.Conv2DTranspose," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux 22.4  Mobile device N\A  Python version 3.9  Bazel version N\A  GCC/compiler version N\A  CUDA/cuDNN version N\A  GPU model and memory N\A  Current behavior? When running ```tf.keras.layers.Conv2DTranspose```, I get AssertionError. The minimum reproducing example is attached.  Standalone code to reproduce the issue ```shell def test_kernel_initializer():          Test with a custom kernel initializer         model = tf.keras.Sequential([             tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, kernel_initializer='ones', input_shape=(4, 4, 1))         ])         input_data = np.ones((1, 4, 4, 1), dtype=np.float32)         output_data = model(input_data)         expected_output = np.full((1, 6, 6, 1), 9.0)   Since kernel is initialized with ones         np.testing.assert_array_almost_equal(output_data.numpy(), expected_output) ```  Relevant log output ```shell Traceback (most recent call last):   File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.keras.layers.Conv2DTranspose.py"", line 74, in test_kernel_initializer     np.testing.assert_array_almost_equal(output_data.numpy(), expected_output)   File ""/home/user/anaconda3/lib/python3.8/sitepackages/numpy/testing/_private/utils.py"", line 1046, in assert_array_almost_equal     assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,   File ""/home/user/anaconda3/lib/python3.8/sitepackages/numpy/testing/_private/utils.py"", line 844, in assert_array_compare     raise AssertionError(msg) AssertionError:  Arrays are not almost equal to 6 decimals Mismatched elements: 32 / 36 (88.9%) Max absolute difference: 8. Max relative difference: 0.88888889  x: array([[[[1.],          [2.],          [3.],...  y: array([[[[9.],          [9.],          [9.],...  Ran 8 tests in 0.155s FAILED (failures=2) ```",2025-03-14T18:37:22Z,type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89244,"Hi  , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18, 2.19, and the nightly, but I did not encounter any issues. Please find the gist attached for your reference. Thank you!","> Hi [](https://github.com/sjh0849) , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18, 2.19, and the nightly, but I did not encounter any issues. Please find the gist attached for your reference. >  > Thank you! Hi  , Sorry, I think my replication code earlier was not sufficient. (The colab seems like it's not executing the test.) Here is the updated replicable code. ```python import tensorflow as tf import numpy as np import unittest class TestConv2DTranspose(unittest.TestCase):     def test_kernel_initializer(self):          Test with a custom kernel initializer         model = tf.keras.Sequential([             tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, kernel_initializer='ones', input_shape=(4, 4, 1))         ])         input_data = np.ones((1, 4, 4, 1), dtype=np.float32)         output_data = model(input_data)         expected_output = np.full((1, 6, 6, 1), 9.0)   Since kernel is initialized with ones         np.testing.assert_array_almost_equal(output_data.numpy(), expected_output) if __name__ == '__main__':     unittest.main() ``` > Hi [](https://github.com/sjh0849) , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow versions 2.18, 2.19, and the nightly, but I did not encounter any issues. Please find the gist attached for your reference. >  > Thank you!"
copybara-service[bot],Reland: [XLA:GPU] Enable RaggedAllToAll one shot kernel by default.,Reland: [XLA:GPU] Enable RaggedAllToAll one shot kernel by default. Reverts 2931aad06d9d11a6c028e89a5a061d60e3808193,2025-03-14T18:26:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89243
copybara-service[bot],AcquireDonation and DonationTransaction helper class for the PJRT async client.,AcquireDonation and DonationTransaction helper class for the PJRT async client.,2025-03-14T17:57:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89242
copybara-service[bot],Propagate volatility attribute when creating QConst ops during quantization,Propagate volatility attribute when creating QConst ops during quantization * This ensures that output of a qconst will have the same volatility as the source quantize op. * This will help with subsequently identify and remove unnecessary QConst>Dequantize ops after quantization.,2025-03-14T17:39:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89241
panda123dd,build from source: TypeError: 'NoneType' object is not iterable," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19.0  Custom code Yes  OS platform and distribution linux ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version 6.5.0  GCC/compiler version clang 11.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When use this command ""bazel build [config=option] //tensorflow/tools/pip_package:build_pip_package "",it can't build successfully. and I have done this ""py_binary( name = ""build_pip_package"",  changed from ""build_pip_package_py"" srcs = [""build_pip_package.py""], main = ""build_pip_package.py"","" but it can't solve the problem.  Standalone code to reproduce the issue ```shell (tensorflow2.19test) root:/frameworks/tensorflow2.19test ./bazelbin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg usage: build_pip_package.py [h] outputname OUTPUT_NAME projectname PROJECT_NAME platform                             PLATFORM [headers HEADERS] [srcs SRCS] [dests DESTS]                             [xla_aot XLA_AOT] [version VERSION] [collab COLLAB] build_pip_package.py: error: the following arguments are required: outputname, projectname, platform (tensorflow2.19test) root:/frameworks/tensorflow2.19test ./bazelbin/tensorflow/tools/pip_package/build_pip_package outputname /tmp/ttt.whl projectname ttt platform linux_86_64 Traceback (most recent call last):   File ""/frameworks/tensorflow2.19test/./bazelbin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/tools/pip_package/build_pip_package.py"", line 461, in      prepare_wheel_srcs(   File ""/frameworks/tensorflow2.19test/./bazelbin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/tools/pip_package/build_pip_package.py"", line 271, in prepare_wheel_srcs     prepare_headers(headers, os.path.join(srcs_dir, ""tensorflow/include""))   File ""/frameworks/tensorflow2.19test/./bazelbin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/tools/pip_package/build_pip_package.py"", line 142, in prepare_headers     for file in headers: TypeError: 'NoneType' object is not iterable ```  Relevant log output ```shell ```",2025-03-14T17:22:32Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89240,"Hi **** , Apologies for the delay, and thank you for bringing up this issue. I noticed a version mismatch in Clang, which could be one of the possible reasons for issue. TensorFlow 2.18+ requires Clang 17.0.6, whereas you are using Clang 11. Upgrading to the recommended version might help resolve the issue. For reference, I am attaching the official documentation please check the compatibility requirements to ensure everything aligns correctly. Additionally, here is a related issue that might provide further insights. Let me know if you still facing issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],test for new github actions,test for new github actions,2025-03-14T17:12:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89239
copybara-service[bot],[XLA:GPU][Emitters] Add a pattern to flatten vector.transfer_read.,[XLA:GPU][Emitters] Add a pattern to flatten vector.transfer_read.,2025-03-14T16:54:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89238
copybara-service[bot],[XLA:GPU] Enable peer access for all collective cliques.,"[XLA:GPU] Enable peer access for all collective cliques. To check for peer access, we need to call CUDA driver for all pairs of devices, so we want to do it only once per clique, if possible. The peer access is currently needed only for oneshot raggedalltoall kernel, but we might want to experiment with other collectives later.",2025-03-14T16:29:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89237
copybara-service[bot],Add retries to `tar` command in nccl `generated_srcs` target.,Add retries to `tar` command in nccl `generated_srcs` target.,2025-03-14T15:43:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89236
copybara-service[bot],[XLA:CPU] Add support for benchmarking AOT compiled models on CPU,[XLA:CPU] Add support for benchmarking AOT compiled models on CPU,2025-03-14T15:34:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89235
copybara-service[bot],[dlpack] Support more DLPack dtypes now that we target DLPack 1.1,[dlpack] Support more DLPack dtypes now that we target DLPack 1.1 I did not update `jax.dlpack.SUPPORTED_DTYPES` because neither NumPy nor TensorFlow currently support importing DLPack arrays with the new dtypes.,2025-03-14T14:48:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89234
copybara-service[bot],PR #23718: Add missing hlo_argument_modes to the error message,"PR CC( Failed to load the native TensorFlow runtime.): Add missing hlo_argument_modes to the error message Imported from GitHub PR https://github.com/openxla/xla/pull/23718 The `multihost_hlo_runner` supports five `hlo_argument_mode` options, but existing error message lists only three modes. This PR adds missing modes to the error message  specifically ""use_zeros_as_input"" and ""uninitialized"" Additionally, the error message now uses raw string literals (R""()"") to simplify the code and avoid unnecessary escape characters. Copybara import of the project:  f0abbde8f3cd514acfdfb68701483c73e77ef50e by Alexander Pivovarov : Add missing hlo_argument_modes to error message Merging this change closes CC( Failed to load the native TensorFlow runtime.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23718 from apivovarov:fix_err_msg f0abbde8f3cd514acfdfb68701483c73e77ef50e",2025-03-14T14:12:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89233
copybara-service[bot],[AutoPGLE] Prevent an AutoPGLE to run if user launched an external profiler.,[AutoPGLE] Prevent an AutoPGLE to run if user launched an external profiler.,2025-03-14T13:57:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89232
copybara-service[bot],[XLA:GPU][NFC] Migrate to `GetNonContractingDims` in matmul perf table gen.,[XLA:GPU][NFC] Migrate to `GetNonContractingDims` in matmul perf table gen. Simplifies the implementation a little bit.,2025-03-14T13:32:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89231
copybara-service[bot],[XLA:CPU] Add AOT compilation tests,[XLA:CPU] Add AOT compilation tests,2025-03-14T12:10:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89230
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-14T11:21:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89229
copybara-service[bot],[xla:gpu] Remove more mentions of NCCL from function names,[xla:gpu] Remove more mentions of NCCL from function names,2025-03-14T10:49:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89228
copybara-service[bot],Make `PjRtClient::DeserializeExecutable()` delegate to `PjRtClient::LoadSerializedExecutable()`.,Make `PjRtClient::DeserializeExecutable()` delegate to `PjRtClient::LoadSerializedExecutable()`.,2025-03-14T10:45:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89227
copybara-service[bot],"[NFC] Make `PjRtClient::DeserializeExecutable()` delegate to `PjRtClient::LoadSerializedExecutable()`, and replace all uses of `PjRtClient::DeserializeExecutable()` with `PjRtClient::LoadSerializedExecutable()`.","[NFC] Make `PjRtClient::DeserializeExecutable()` delegate to `PjRtClient::LoadSerializedExecutable()`, and replace all uses of `PjRtClient::DeserializeExecutable()` with `PjRtClient::LoadSerializedExecutable()`. This is to prepare for updating `PjRtClient::DeserializeExecutable()` to return an unloaded executable.",2025-03-14T10:34:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89226
copybara-service[bot],Revert default value of `print_large_constants` back to `false`.,"Revert default value of `print_large_constants` back to `false`. This causes some tests to OOM. Printing large constants is required for parsability. However, it is not clear that the default print options should produce a parsable result. Users that require parsability, can use e.g. `HloPrintOptions::ShortParsable`.",2025-03-14T10:28:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89225
Jhe-Cyuan,Getting 0 nodes delegated while using TFLite C Library with TFLite Flex Delegate," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version Tensroflow 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version 6.5.0  GCC/compiler version 13.3.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am using the TFLite C library to generate model inference C code for an edge device. In my use case, I require certain operations, such as tf.unpack with shape (0, 2), which are supported by TensorFlow but not currently available in TFLite.  How I built the TFLite C Library I found a solution article on the Google AI Edge website (here), which provides instructions on building the TFLite C library using CMake. I followed these steps and successfully built the library without any errors. As a result, I obtained `libtensorflowlite_c.so` and placed it in `/usr/local/lib`, which is the system library directory on Ubuntu.  How I built the Tensorflow Flex Delegate library Similarly, there is an article here that explains how to build `libtensorflowlite_flex.so.` I followed the instructions provided and successfully built the library without encountering any issues. To verify the build, I ran the command: ```bash strings /usr/local/lib/libtensorflowlite_flex.so | grep Unpack ``` This command displayed several symbols containing Unpack, which indicates that the build was successful, I think.  Click to show output ``` UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp TensorArrayUnpackOrScatterOp '(new gtl::FlatSet{""ArgMax"", ""ArgMin"", ""AudioSpectrogram"", ""AvgPool"", ""BatchMatMul"", ""BatchMatMulV2"", ""BatchNormWithGlobalNormalization"", ""BatchToSpace"", ""BatchToSpaceND"", ""Bincount"", ""BroadcastArgs"", ""BroadcastGradientArgs"", ""Bucketize"", ""CTCBeamSearchDecoder"", ""CTCGreedyDecoder"", ""CTCLoss"", ""CompareAndBitpack"", ""ComplexAbs"", ""Concat"", ""ConcatOffset"", ""ConcatV2"", ""Conv2D"", ""Copy"", ""CopyHost"", ""Cross"", ""CudnnRNN"", ""CudnnRNNBackprop"", ""CudnnRNNBackpropV2"", ""CudnnRNNBackpropV3"", ""CudnnRNNCanonicalToParams"", ""CudnnRNNCanonicalToParamsV2"", ""CudnnRNNParamsSize"", ""CudnnRNNParamsToCanonical"", ""CudnnRNNParamsToCanonicalV2"", ""CudnnRNNV2"", ""CudnnRNNV3"", ""CumProd"", ""CumSum"", ""DebugNanCount"", ""DebugNumericSummary"", ""DecodeProtoV2"", ""DecodeWav"", ""DeepCopy"", ""DepthToSpace"", ""Dequantize"", ""Diag"", ""DiagPart"", ""EditDistance"", ""Empty"", ""EncodeProtoV2"", ""EncodeWav"", ""ExtractImagePatches"", ""ExtractVolumePatches"", ""Fill"", ""Gather"", ""GatherNd"", ""GatherV2"", ""HistogramFixedWidth"", ""InvertPermutation"", ""IsInf"", ""IsNan"", ""Isfinite"", ""LinSpace"", ""LowerBound"", ""MatMul"", ""MatrixDiag"", ""MatrixDiagPart"", ""MatrixDiagPartV2"", ""MatrixDiagV2"", ""Mfcc"", ""Multinomial"", ""OneHot"", ""Pack"", ""ParameterizedTruncatedNormal"", ""PopulationCount"", ""RandomGamma"", ""RandomPoisson"", ""RandomPoissonV2"", ""RandomStandardNormal"", ""RandomUniform"", ""RandomUniformInt"", ""Range"", ""Rank"", ""RequantizationRange"", ""Requantize"", ""ReverseSequence"", ""Shape"", ""ShapeN"", ""Size"", ""SpaceToBatch"", ""SpaceToBatchND"", ""SpaceToDepth"", ""SparseMatMul"", ""Split"", ""SplitV"", ""TruncatedNormal"", ""Unique"", ""UniqueV2"", ""UniqueWithCounts"", ""UniqueWithCountsV2"", ""Unpack"", ""UnravelIndex"", ""UpperBound"", ""Where""})' Must be non NULL TensorArrayUnpack Unpack UnpackOp UnpackOp UnpackOp UnpackOp UnpackOp UnpackGrad tfg.Unpack N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEmEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceElEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEjEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEtEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEsEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEhEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEaEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEiEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceENS1_4halfEEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceENS1_8bfloat16EEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEfEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEdEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceESt7complexIfEEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceESt7complexIdEEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEbEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEN3tsl7tstringEEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceENS_14ResourceHandleEEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceENS_7VariantEEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEN9ml_dtypes15float8_internal11float8_e5m2EEE N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEN9ml_dtypes15float8_internal13float8_e4m3fnEEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEmLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEmLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceElLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceElLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEjLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEjLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEtLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEtLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEsLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEsLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEhLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEhLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEaLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEaLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEiLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEiLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS1_4halfELb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS1_4halfELb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS1_8bfloat16ELb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS1_8bfloat16ELb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEfLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEfLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEdLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEdLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceESt7complexIfELb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceESt7complexIfELb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceESt7complexIdELb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceESt7complexIdELb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEbLb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEbLb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEN3tsl7tstringELb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEN3tsl7tstringELb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS_14ResourceHandleELb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS_14ResourceHandleELb0EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS_7VariantELb1EEE N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS_7VariantELb0EEE Unpack ```    How I use C Library Function to load model and delegate library I have check the model that some ops do have `Flex` prefix in the op names. !Image It was converted using the following settings: ```python converter = tf.lite.TFLiteConverter.from_saved_model(model_folder) converter.optimizations = [tf.lite.Optimize.DEFAULT]  converter.target_spec.supported_types = [tf.float16] converter.target_spec.supported_ops = [     tf.lite.OpsSet.SELECT_TF_OPS,  tf.lite.OpsSet.TFLITE_BUILTINS,  ] converter.allow_custom_ops = True converter.legalize_custom_tensor_list_ops = True converter._experimental_lower_tensor_list_ops = False  converter.experimental_enable_resource_variables = True tflite_model = converter.convert() ``` Then, I use the code below to load the model and delegate operations that are not supported by TFLite to the Flex library. However, it always displays a message indicating that no nodes have been delegated by the Flex library. ```bash INFO: Created TensorFlow Lite delegate for select TF ops. 20250314 17:43:31.412379: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 623 nodes with 0 partitions. ModifyGraphWithDelegate Ok Load model successfully! ``` How should I resolve this issue? I would really appreciate any insights or suggestions. Thanks in advance for your help!  Standalone code to reproduce the issue ```shell void tflite_load(const string model_path, map& sig_map) {     // Load model     TfLiteModel* model = TfLiteModelCreateFromFile(model_path.c_str());     if (model == nullptr) {         cerr (SharedLibrary::GetLibrarySymbol(hdll, ""TF_AcquireFlexDelegate""));     if (TF_AcquireFlexDelegate == NULL) {         cerr  delegate = TF_AcquireFlexDelegate();     auto TfLiteStatus = TfLiteInterpreterModifyGraphWithDelegate(interpreter, delegate.get());     if(TfLiteStatus==0)         cout << ""ModifyGraphWithDelegate Ok"" << endl;     cout << ""Load model successfully!"" << endl; } ```  Relevant log output ```shell ```",2025-03-14T10:11:51Z,stat:awaiting tensorflower type:bug comp:lite TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89224,"Hi, Cyuan  I apologize for the delay in response, If possible could you please want to try removing `converter.optimizations = [tf.lite.Optimize.DEFAULT]` temporarily to see if that changes the behavior. Please make sure that both libraries (TFLite C library and Flex delegate) are built from the same TensorFlow version. Version mismatches can cause delegation failures even when the code is correct. Your current approach applies the delegate after creating the interpreter. The recommended way is to add the delegate to the options before creating the interpreter something like below : ``` TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate(); // Acquire and add the delegate to options auto hdll = SharedLibrary::LoadLibrary(""libtensorflowlite_flex.so""); auto TF_AcquireFlexDelegate = reinterpret_cast(     SharedLibrary::GetLibrarySymbol(hdll, ""TF_AcquireFlexDelegate"")); std::unique_ptr delegate = TF_AcquireFlexDelegate(); // Add delegate to options before creating interpreter TfLiteInterpreterOptionsAddDelegate(options, delegate.get()); // Create interpreter with configured options TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options); ``` The Flex delegate has a specific allowlist of supported TensorFlow operations only operations in this list can be delegated Could you please check and try above things and see is it resolving your issue or not ? If I have missed something here please let me know. Thank you for your cooperation and patience.","Hi,   Thank you very much for replying me. I've edit my code as you told. ```C++ auto hdll = SharedLibrary::LoadLibrary(""libtensorflowlite_flex.so""); if (hdll == nullptr) {     cerr (     SharedLibrary::GetLibrarySymbol(hdll, ""TF_AcquireFlexDelegate"")); std::unique_ptr delegate = TF_AcquireFlexDelegate(); TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate(); TfLiteInterpreterOptionsSetEnableDelegateFallback(options, true); TfLiteInterpreterOptionsAddDelegate(options, delegate.get()); TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options); TfLiteInterpreterOptionsDelete(options); ``` But it still delegate 0 node with my model. ```bash INFO: Created TensorFlow Lite delegate for select TF ops. INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 623 nodes with 0 partitions. ``` As you mentioned, I built the `TFLite C Library` and `Flex Delegate` using the commit tagged v2.18.0. It seems that from that version onwards, we need to build the external_delegate library as well, although I haven’t tried that yet. My current test aims to explore ondevice training. To do this, I’ve incorporated some loss computation designs using TensorFlow ops (not Keras, but in SavedModel format). These models contain many operations such as `IF,` `UNPACK`, and others that are not natively supported by TFLite. They also involve tensors with dynamic or zero dimensions (e.g., shape (0, 10)), which adds further complexity. Therefore, understanding how to use delegates to enable more TensorFlow ops to be executed within the TFLite environment is crucial to my study. I hope this context helps clarify my direction. Thank you again for your support and discussion."
copybara-service[bot],[XLA:CPU] Make the small while loop hoisting threshold configurable & increase default to 1024,"[XLA:CPU] Make the small while loop hoisting threshold configurable & increase default to 1024 This change also made an underlying bug apparent where dot ops would call into Eigen runtime and crash, this resolves that by not hoisting while loops which transitively call dot.",2025-03-14T10:01:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89223
copybara-service[bot],PR #23688: [ROCm] Triton performance fixes,"PR CC(remove reduce_ from established functions, make it closer to numpy): [ROCm] Triton performance fixes Imported from GitHub PR https://github.com/openxla/xla/pull/23688 Copybara import of the project:  f6998514cd08d018a313294f6974ccab674525bb by Dragan Mladjenovic : [ROCm] Apply precise block size metadata  bdcba45f58f7ad40e6ee8e8d2afd0c04956b34d5 by Dragan Mladjenovic : [ROCm] Pass correct warp size to Triton pipeline Merging this change closes CC(remove reduce_ from established functions, make it closer to numpy) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23688 from ROCm:ci_rocm_triton_perf_fixes bdcba45f58f7ad40e6ee8e8d2afd0c04956b34d5",2025-03-14T09:00:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89222
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-14T08:53:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89221
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-14T08:48:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89220
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-14T08:39:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89219
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-14T08:29:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89218
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-14T08:17:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89217
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-14T08:09:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89216
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-14T08:07:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89215
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-14T08:05:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89214
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-14T07:56:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89213
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-14T07:56:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89212
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-14T07:46:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89211
jiunkaiy,Qualcomm AI Engine Direct - Provide op optimization,Summary:  Support FC>CONV2D optimization  Add CONV2D op builder with CPU transpose Test:  [x] a8w8 Gemma3 compile and execution successfully  [x] qnn_compiler_plugin_test all pass,2025-03-14T07:08:28Z,awaiting review ready to pull size:L,closed,0,2,https://github.com/tensorflow/tensorflow/issues/89210,Any test results?,> Any test results? I have left some comments in the first conversation block. Thanks!
copybara-service[bot],PR #23494: [ROCm]  Use kernel cache per stream executor for redzone checker,PR CC(未找到相关数据): [ROCm]  Use kernel cache per stream executor for redzone checker Imported from GitHub PR https://github.com/openxla/xla/pull/23494 This is needed to load the kernel on each of the gpu device as rocm did not cache redzone_checker kernel per stream executor. Copybara import of the project:  ceb9dbf630d98013d52a3ec9d11bc8426832b72f by Harsha HS : [ROCm] Use kernel cache per stream executor for redzone checker Merging this change closes CC(未找到相关数据) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23494 from ROCm:ci_redzone_checker_obj_20250307 ceb9dbf630d98013d52a3ec9d11bc8426832b72f,2025-03-14T06:24:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89209
Venkat6871,Fix typos in documentation strings,"Hi, Team I observed few typos in the documentation strings and I have fixed those typos so please do the needful. Thank you.",2025-03-14T05:51:34Z,ready to pull size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89208
copybara-service[bot],PR #23386: [debug_options] Printing all the fields in debug options,PR CC(InvalidArgumentError: Invalid name: An op that loads optimization parameters into HBM for embedding. (ConfigureTPUEmbeddingHost)): [debug_options] Printing all the fields in debug options Imported from GitHub PR https://github.com/openxla/xla/pull/23386 This patch prints all the values of the debug options while dumping it under VLOG. This is specifically required for boolean fields which have the default value set to true in `xla/debug_options_flags.cc`. These values will not be printed in `DebugString()` if `XLA_FLAGS` overrides it to `false`. Copybara import of the project:  4746d058005f9fff6ebbcc37818a288979d05165 by Shraiysh Vaishay : [debug_options] Printing all the fields in debug options This patch prints all the values of the debug options while dumping it under VLOG. This is specifically required for boolean fields which have the default value set to true in `xla/debug_options_flags.cc`. These values will not be printed in `DebugString()` if `XLA_FLAGS` overrides it to `false`. Merging this change closes CC(InvalidArgumentError: Invalid name: An op that loads optimization parameters into HBM for embedding. (ConfigureTPUEmbeddingHost)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23386 from shraiysh:print_debug_options_full 4746d058005f9fff6ebbcc37818a288979d05165,2025-03-14T05:36:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89207
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-14T05:02:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89206
jiunkaiy,Qualcomm AI Engine Direct - Compile with PlatformInfo,Summary:  Add soc table for PlatformInfo  Fix qnn compiler tests  Compile with PlatformInfo  Align LiteRt graph and configs with QnnTFLiteDelegate,2025-03-14T02:40:25Z,size:M,open,0,0,https://github.com/tensorflow/tensorflow/issues/89205
copybara-service[bot],"Add `CreateErrorBuffer`, and unit test for `Compile()`","Add `CreateErrorBuffer`, and unit test for `Compile()`",2025-03-14T02:28:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89204
copybara-service[bot],"Avoid using constexpr virtual dispatching, which is not supported in c++17","Avoid using constexpr virtual dispatching, which is not supported in c++17",2025-03-14T01:20:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89203
copybara-service[bot],"Use the `SameOperandsAndResultType` trait for instead of `SameOperandsAndResultShape`, for applicable operators.","Use the `SameOperandsAndResultType` trait for instead of `SameOperandsAndResultShape`, for applicable operators. This trait adds missing builders to these ops, to enable graph transformation rules.",2025-03-14T00:53:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89202
copybara-service[bot],Enable `jax.device_put` to a sharding with no local devices.,Enable `jax.device_put` to a sharding with no local devices.,2025-03-14T00:39:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89201
copybara-service[bot],[XLA:GPU] sort_rewriter: Don't run SortRewriter if there's no GPU present.,"[XLA:GPU] sort_rewriter: Don't run SortRewriter if there's no GPU present. Even when the compiler is built with CUDA, the compilation may not occur on a machine with a GPU. Currently SortRewriter will always attempt to contact the driver, which will fail in such scenarios. Let's avoid that at the admitted runtime cost, especially since the error message (""invalid device ordinal"") is not particularly clear.",2025-03-14T00:02:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89200
copybara-service[bot],Print buffer-trace for host offload failure.,Print buffertrace for host offload failure.,2025-03-13T23:41:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89199
copybara-service[bot],LogScheduleStatistics is very slow. Lower it's level from 1 to 3.,LogScheduleStatistics is very slow. Lower it's level from 1 to 3.,2025-03-13T23:36:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89198
copybara-service[bot],Expose a method in spmd_partitioner for propagating sharding from module parameters and outputs to the module's sharding configs.,Expose a method in spmd_partitioner for propagating sharding from module parameters and outputs to the module's sharding configs.,2025-03-13T23:30:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89197
copybara-service[bot],Allow different scales/zero-points on inputs+outputs for uint8 w/tfl.concat.,"Allow different scales/zeropoints on inputs+outputs for uint8 w/tfl.concat. For legacy reasons which are not well understood, uint8 allows different different scales/zeropoints across its inputs and the output scale/zeropoint being the union across the different quant params. This change disables requantizing inputs and outputs to a single scale/zeropoint and replicates behavior exhibited by the now defunct toco converter in order to support some legacy clients for which this functionality was broken during migration.",2025-03-13T23:04:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89196
copybara-service[bot],Add transpose-squeeze moving pattern,Add transposesqueeze moving pattern,2025-03-13T22:36:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89195
copybara-service[bot],[MHLO] Preserve discardable attrs when canonicalizing while op,[MHLO] Preserve discardable attrs when canonicalizing while op,2025-03-13T22:27:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89194
copybara-service[bot],Support both dynamic / statically linked GPU accelerator,"Support both dynamic / statically linked GPU accelerator Define a weak function RegisterStaticLinkedAcceleratorGpu, and let the ml_drift_cl_accelerator.'s linked static.",2025-03-13T22:24:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89193
copybara-service[bot],Preserve `mhlo.frontend_attributes` when legalizing to `mhlo.ragged_dot`.,Preserve `mhlo.frontend_attributes` when legalizing to `mhlo.ragged_dot`.,2025-03-13T22:20:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89192
copybara-service[bot],litert: Change to unordered_map,litert: Change to unordered_map,2025-03-13T22:18:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89191
copybara-service[bot],[xla:gpu] Remove mentions of NCCL from CollectiveGroupThunk,[xla:gpu] Remove mentions of NCCL from CollectiveGroupThunk,2025-03-13T21:51:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89190
copybara-service[bot],[xla:gpu] Remove mentions of NCCL from Thunk::Kind,[xla:gpu] Remove mentions of NCCL from Thunk::Kind,2025-03-13T21:50:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89189
copybara-service[bot],Remove defaulted constructors which are implicitly deleted in litert_model.h,Remove defaulted constructors which are implicitly deleted in litert_model.h,2025-03-13T21:43:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89188
copybara-service[bot],Allow `make_array_from_callback` to construct nonaddressable arrays.,Allow `make_array_from_callback` to construct nonaddressable arrays.,2025-03-13T20:50:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89187
copybara-service[bot],Initial version of TfrtGpuBuffer,Initial version of TfrtGpuBuffer,2025-03-13T20:24:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89186
copybara-service[bot],Support tfl-shlo customcall carrier in older api version.,Support tflshlo customcall carrier in older api version.,2025-03-13T20:05:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89185
copybara-service[bot],Remove transfer bytes context from async dynamic-slice instruction.,Remove transfer bytes context from async dynamicslice instruction.,2025-03-13T19:45:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89184
copybara-service[bot],needed for diffbase,needed for diffbase,2025-03-13T19:44:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89183
copybara-service[bot],Handle additional patterns for FQ propagation on mean and concat ops.,"Handle additional patterns for FQ propagation on mean and concat ops. The previous assumption that you could follow a chain of tfl.dequantize < tfl.quantize < tf.FakeQuantWithMinMaxVars to find the fakequant didn't turn out to be correct as there are cases where the qdq's haven't been generated yet, so the tf.fq is the immediate input.",2025-03-13T19:28:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89182
copybara-service[bot],Use keep-sorted markers to keep the operators with `INFER_RETURN_TYPE_COMPONENTS_FROM_OPERANDS` sorted.,Use keepsorted markers to keep the operators with `INFER_RETURN_TYPE_COMPONENTS_FROM_OPERANDS` sorted.,2025-03-13T19:18:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89181
copybara-service[bot],Update PyArray::BatchedDevicePut to support zero buffers and destination devices. Use batched_device_put instead of ArrayImpl to build arrays with no local shards.,Update PyArray::BatchedDevicePut to support zero buffers and destination devices. Use batched_device_put instead of ArrayImpl to build arrays with no local shards.,2025-03-13T19:16:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89180
copybara-service[bot],Optimization patterns to promote folding of more constants.,"Optimization patterns to promote folding of more constants. This pattern optimizes:    (x + cst1) + cst2 > x + cst    (x  cst1)  cst2 > x  cst  Where: cst = cst1 + cst2 Similar patterns can be possible with other binary ewise ops. But this CL only tackles Add, Sub, Mul and Div.",2025-03-13T18:42:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89179
copybara-service[bot],Introduce safer `Shape` ctors to replace the unsafe one.,"Introduce safer `Shape` ctors to replace the unsafe one. Currently, depending on the parameters, the same `Shape` ctor can create an array shape, a tuple shape, or a token/opaque shape. This approach has several issues: 1. It doesn't enforce correct usage by construct, and thus is errorprone. For example, someone could pass in nonempty dimensions and nonempty tuple_shapes at the same time, which doesn't make sense. A good API should prevent this, ideally at compile time. 2. It makes it harder to understand a call site's intention, especially when the parameters are not literal values. The reader will have to do some research to see whether an array, a tuple, or something else is intended. In this change, we introduce 3 ctor overloads, each with a dedicated purpose, so that they can used instead of the catchall ctor. Also switch existing uses of the original ctor in XLA to the new ctors.",2025-03-13T18:37:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89178
copybara-service[bot],"Delete `pywrap.bzl`, use `pywrap.default.bzl` explicitly","Delete `pywrap.bzl`, use `pywrap.default.bzl` explicitly",2025-03-13T18:36:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89177
copybara-service[bot],[XLA:GPU] support hoisting bitcasts past broadcasts,[XLA:GPU] support hoisting bitcasts past broadcasts Only rewrites hoisting up to the parameters. Works in assumption that bitcast is a reshape and broadcast does not transpose.,2025-03-13T18:27:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89176
copybara-service[bot],Add soc_model parameter to LiteRtCompilerPluginPartition.,Add soc_model parameter to LiteRtCompilerPluginPartition.,2025-03-13T18:26:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89175
copybara-service[bot],[xla:copy_insertion] Fix a problem in handling Send feeding into a while-loop.,"[xla:copy_insertion] Fix a problem in handling Send feeding into a whileloop. Previously, we only make a copy of the operand for rotated Send/SendDone inside a whileloop. We now also make a copy of the operand for the Send feeding into a whileloop so that the buffer for the operand does not have to hold different values with live range interference.",2025-03-13T18:19:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89174
copybara-service[bot],[XLA:Python] Small optimizations to NamedSharding::NamedSharding().,[XLA:Python] Small optimizations to NamedSharding::NamedSharding(). * don't repeatedly look up the _internal_device_list attribute. * only look up the `check_pspec` function once.,2025-03-13T18:10:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89173
copybara-service[bot],Re-introduce bitmap_test and make bitmap copiable,Reintroduce bitmap_test and make bitmap copiable,2025-03-13T18:09:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89172
copybara-service[bot],Clean Up Forwarding Headers in tensorflow/core/profiler/protobuf Folder,Clean Up Forwarding Headers in tensorflow/core/profiler/protobuf Folder,2025-03-13T18:04:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89171
copybara-service[bot],Clean Up Forwarding Headers in tensorflow/core/profiler/protobuf Folder,Clean Up Forwarding Headers in tensorflow/core/profiler/protobuf Folder,2025-03-13T18:02:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89170
copybara-service[bot],Move memory_profile.proto to OSS to prepare for OSS benchmarking,Move memory_profile.proto to OSS to prepare for OSS benchmarking,2025-03-13T17:56:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89169
copybara-service[bot],litert: Access GPU environment via C API instead of direct linking,litert: Access GPU environment via C API instead of direct linking The direct linking could make multiple singleton instances with dynamic accelerator library.  Added C API LiteRtGpuGlobalEnvironmentCreate()  Renamed runtime/environment to runtime/gpu_environment  Renamed EnvironmentSingleton to GpuEnvironmentSingleton,2025-03-13T17:35:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89168
copybara-service[bot],Better formatting of some conditionals per the style guide.,Better formatting of some conditionals per the style guide.,2025-03-13T17:33:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89167
copybara-service[bot],Add user-directed fuse attributes.,Add userdirected fuse attributes.,2025-03-13T17:15:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89166
copybara-service[bot],[XLA] Use build.py to run GPU nightly jobs and refactor nightly workflows,[XLA] Use build.py to run GPU nightly jobs and refactor nightly workflows,2025-03-13T17:11:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89165
copybara-service[bot],Reorder explicit broadcast_to ops around binary ops and remove redundant broadcast_to ops.,"Reorder explicit broadcast_to ops around binary ops and remove redundant broadcast_to ops. TFL_BinaryOp(TFL_BroadcastToOp(input1, dim), input2) => TFL_BroadcastToOp(TFL_BinaryOp(input1, input2), dim)",2025-03-13T17:06:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89164
copybara-service[bot],remove transfer bytes context from async dynamic-slice instruction,remove transfer bytes context from async dynamicslice instruction,2025-03-13T17:04:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89163
copybara-service[bot],Minor change to the trace viewer ui code to handle countre messages from backend.,Minor change to the trace viewer ui code to handle countre messages from backend.,2025-03-13T16:46:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89162
copybara-service[bot],Add LiteRtGL typedefs to simplify enabling OpenGL support.,Add LiteRtGL typedefs to simplify enabling OpenGL support.,2025-03-13T16:45:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89161
copybara-service[bot],litert: Add OpenCL event support,litert: Add OpenCL event support  Added LiteRtEventType  Added OpenCL event creation methods  Added Signal method,2025-03-13T16:40:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89160
copybara-service[bot],fix: resolve issue where pre-release versions of protobuf are installed,fix: resolve issue where prerelease versions of protobuf are installed,2025-03-13T16:38:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89159
copybara-service[bot],[xla:gpu] Remove mentions of NCCL from CollectiveBroadcast and CollectivePermute,[xla:gpu] Remove mentions of NCCL from CollectiveBroadcast and CollectivePermute,2025-03-13T16:11:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89158
copybara-service[bot],Replace uses of `Shape::dimensions_size()` with `Shape::rank()`.,"Replace uses of `Shape::dimensions_size()` with `Shape::rank()`. The two functions have the same semantics, just different spellings. We decided to deprecate the former for consistency. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23654 from zml:zml/rocm/mfma c25655136987ff5b246ce9498af30daae3f20270",2025-03-13T16:02:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89157
copybara-service[bot],[xla:gpu] Remove mentions of NCCL from AllToAllThunk,[xla:gpu] Remove mentions of NCCL from AllToAllThunk,2025-03-13T15:39:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89156
copybara-service[bot],[xla:gpu] Remove mentions of NCCL from RaggedAllToAllThunk,[xla:gpu] Remove mentions of NCCL from RaggedAllToAllThunk,2025-03-13T15:36:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89155
copybara-service[bot],[xla:gpu] Rename Send/Recv thunks to HostSend/HostRecv,[xla:gpu] Rename Send/Recv thunks to HostSend/HostRecv,2025-03-13T15:35:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89154
copybara-service[bot],[xla:gpu] Remove mentions of NCCL from AllGatherThunk,[xla:gpu] Remove mentions of NCCL from AllGatherThunk,2025-03-13T15:35:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89153
copybara-service[bot],[xla:gpu] Remove mentions of NCCL from SendThunk and RecvThunk,[xla:gpu] Remove mentions of NCCL from SendThunk and RecvThunk,2025-03-13T15:22:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89152
copybara-service[bot],PR #23693: Remove HloComputation::WhileCallInstruction.,"PR CC(Dilated convolution support with nnapi): Remove HloComputation::WhileCallInstruction. Imported from GitHub PR https://github.com/openxla/xla/pull/23693 The function is deprecated and broken. I'll remove the remaining similar functions one by one. The caller API I added in the previous PR was too `const`, fixed that as well. Copybara import of the project:  48e84e69a04f4f8d7ba42cfc78cf1f9c0bd3aa36 by Johannes Reifferscheid : Remove HloComputation::WhileCallInstruction. The function is deprecated and broken. I'll remove the remaining similar functions one by one.  3f591be598370c39789fabed15640275b41ffd70 by Johannes Reifferscheid : Undo accidental changes.  9b307599fe6224ad56af3c867304c5617d7d39fd by Johannes Reifferscheid : Fixes:  add missing const  in copy insertion, don't look for a unique caller (not sure this is   actually a problem, but it seems more accurate wrt the comment above)  in collective permute valid iteration annotator, look at all the   loops. Again, this seems a bit strange, but the current logic appears   to depend on the last created loop to be found (?) Merging this change closes CC(Dilated convolution support with nnapi) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23693 from jreiffers:callerinstructions 9b307599fe6224ad56af3c867304c5617d7d39fd",2025-03-13T15:10:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89151
copybara-service[bot],[XLA:GPU] Disable the flaky `nvshmem_test`.,[XLA:GPU] Disable the flaky `nvshmem_test`.,2025-03-13T15:09:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89150
copybara-service[bot],Revert: [XLA:GPU] Enable RaggedAllToAll one shot kernel by default.,Revert: [XLA:GPU] Enable RaggedAllToAll one shot kernel by default. Breaks internal tests. Reverts a92cc3d7cff250c40fce23067b0a2e32f96a39f7,2025-03-13T14:35:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89149
copybara-service[bot],PR #23654: [ROCm] Enable mfma instructions by passing the correct arch name,"PR CC(tf.contrib.image.transform lead to a ValueError in new releases of tensorflow): [ROCm] Enable mfma instructions by passing the correct arch name Imported from GitHub PR https://github.com/openxla/xla/pull/23654 This PR enables the Triton pipeline to emit `triton_gpu.amd_mfma` annotations during the Triton to TritonGPU lowering. This is done by the `TritonAMDGPUAccelerateMatmulPass`, which checks the GFX version to do that. Correctly passing the `gfx_version` reduces our kernel runtime from **~5ms** to **~620us** on MI300X, matching the performance of the Python Triton Compiler used in Torch. We expect this change to radiate quite a bit given that this pipeline is shared by the IR Fusion Emitter used widely across XLA if `tt.dot` ops are emitted. Closes CC(Ubuntu 18.04 bazel build from source fails) Copybara import of the project:  c25655136987ff5b246ce9498af30daae3f20270 by Steeve Morin : [ROCm] Enable mfma instructions by passing the correct arch name Without this commit, mfma instructions would not be emitted by this pass. Merging this change closes CC(tf.contrib.image.transform lead to a ValueError in new releases of tensorflow) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23654 from zml:zml/rocm/mfma c25655136987ff5b246ce9498af30daae3f20270",2025-03-13T14:25:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89148
copybara-service[bot],Provides a simple task executor API on top of a fixed-size thread pool.,"Provides a simple task executor API on top of a fixedsize thread pool.  Tries to fail fast should any action from a batch fail, i.e. does not wait for all submitted actions to finish.  Allows user to enforce the number of worker threads per parallelization call, up to the maximum number of threads existing in the pool.",2025-03-13T14:20:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89147
copybara-service[bot],[xla:gpu] Remove mentions on NCCL from AllReduceThunk,[xla:gpu] Remove mentions on NCCL from AllReduceThunk,2025-03-13T14:18:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89146
copybara-service[bot],[JAX] Add missing preset for X9 dot optimization on BF16/BF16 -> F32.,[JAX] Add missing preset for X9 dot optimization on BF16/BF16 > F32. Two PRs that support this feature have been submitted to stablehlo and openxla. Now we could do the last step  enable it in JAX.,2025-03-13T12:59:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89145
copybara-service[bot],Change `c:litert_dispatch_delegate` to depend on `c:environment_options`.,Change `c:litert_dispatch_delegate` to depend on `c:environment_options`. This breaks the following dependency cycle in future CLs. ``` .> c:litert_environment    cc:litert_dispatch_delegate ` c:litert_environment ```,2025-03-13T11:39:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89144
copybara-service[bot],Bug fix: Fix the table rotation when loading from an unsharded checkpoint.,Bug fix: Fix the table rotation when loading from an unsharded checkpoint.,2025-03-13T11:23:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89143
copybara-service[bot],[XLA:GPU] Do not use SplitK cutlass kernels for unsupported cases.,"[XLA:GPU] Do not use SplitK cutlass kernels for unsupported cases. Manual testing showed that the SplitK kernels only work for contracting dimension size of 32 and 64 or 64 and 128, respectively.",2025-03-13T11:16:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89142
copybara-service[bot],[xla] Add a first iteration on the Autotuner API,[xla] Add a first iteration on the Autotuner API,2025-03-13T11:13:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89141
copybara-service[bot],PR #23320: [NVIDIA GPU] Add utility functions for multi-host fast-interconnect domain,"PR CC([Feature Request] Addition of new operation to Tensorflow Lite for ""ENet""): [NVIDIA GPU] Add utility functions for multihost fastinterconnect domain Imported from GitHub PR https://github.com/openxla/xla/pull/23320 On Blackwell and onwards, multiple hosts may be connected by NVLink thus creating a fastinterconnect domain beyond a single node. This PR leverages NVML APIs to detect the actual fastinterconnect domain. Followup PRs will update the logic in PjRt client on top of these utility functions. Copybara import of the project:  ee084714876336243fa3ba52a857d2d41124b730 by Terry Sun : pipe nvml to detect nvl  36e1f71dfabb75dbd0853355a7be8c0d6d103903 by Terry Sun : convert clusterUUID  917eb219a1bdebfc7a12f5849545615c83092c73 by Terry Sun : bake boot id into key  cbd909ab66ccf47f6e140632645fbb4d6c61c7c3 by Terry Sun : use global id and add getter  e487c4718eefe72e77708ffaf2c8cbbf6c979e8e by Terry Sun : use device ordinal and add doc string  2c761da2c04ea39d6cc79314f0dc14efef6d0380 by Terry Sun : merge fabric info into device proto  ed34dfbba07fbc5ffaaa914478381184a90f9cbb by Terry Sun : error handling and cleanup  c62f08691b7e3a59effc0f721948b30baea5b8a8 by Terry Sun : polish doc string  ab9d41e3142a0792859b9d37b99fe3ddfd9bbba8 by Terry Sun : conditional compile  29d857815a55d7df7911625cfeb35047bfbb07cb by Terry Sun : more conditional compile  524b569f1937db51babdaac04b50e9499da81a95 by Terry Sun : fix field designator order  7a0fb0f10be555c32403af4d226e687c650603d2 by Terry Sun : str buffer size  91e28b675b2f25e2b5d6c73732c8dae193b0dff3 by Terry Sun : compute capability condition  48fafb9dc4f44c451ca64bdb8ca15a1678cae398 by Terry Sun : protobuf backward compatibility  90f07569cf64d4d7d957cf4efe16fd2c9142eb4c by Terry Sun : guard cluster uuid size Merging this change closes CC([Feature Request] Addition of new operation to Tensorflow Lite for ""ENet"") FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23320 from terryysun:terryysun/detect_mnnvl 4820e4b53c9a72fe6ee8c08ab29ee640f1697c8d",2025-03-13T11:13:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89140
copybara-service[bot],PR #23681: Add an API to get a computation's caller(s).,"PR CC(Generating of wrapper functions for TensorFlow ops for Go fails with not existing framework_go_proto directory): Add an API to get a computation's caller(s). Imported from GitHub PR https://github.com/openxla/xla/pull/23681 This should replace CallGraph in most cases, and adds an alternative to the deprecated .*CallInstruction functions. Copybara import of the project:  4168abbcd70640eaa0b4a28b9add6cbfb1482470 by Johannes Reifferscheid : Add an API to get a computation's caller(s). This should replace CallGraph in most cases, and adds an alternative to the deprecated .*CallInstruction functions. Merging this change closes CC(Generating of wrapper functions for TensorFlow ops for Go fails with not existing framework_go_proto directory) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23681 from jreiffers:callerinstructions 4168abbcd70640eaa0b4a28b9add6cbfb1482470",2025-03-13T11:03:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89139
copybara-service[bot],Refactor: Move profiler/convert/trace_viewer from tensorflow to xprof,Refactor: Move profiler/convert/trace_viewer from tensorflow to xprof This change moves the `profiler/convert/trace_viewer/` directory from the TensorFlow repository to the xprof repository.,2025-03-13T10:48:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89138
copybara-service[bot],PR #23679: [GPU] Fix a manual GPU test.,PR CC(Installation of tensorflow r1.10 on windows with visual studio c++ (with gpu) help.): [GPU] Fix a manual GPU test. Imported from GitHub PR https://github.com/openxla/xla/pull/23679 Copybara import of the project:  3d6c73feed4874bdb7beaf9fead006718408b713 by Ilia Sergachev : [GPU] Fix a manual GPU test. Merging this change closes CC(Installation of tensorflow r1.10 on windows with visual studio c++ (with gpu) help.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23679 from openxla:fix_gpu_test 3d6c73feed4874bdb7beaf9fead006718408b713,2025-03-13T10:24:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89137
copybara-service[bot],Replace the uses of `PjRtClient::Compile()` with `PjRtClient::CompileAndLoad()`.,Replace the uses of `PjRtClient::Compile()` with `PjRtClient::CompileAndLoad()`. This is to prepare for updating `PjRtClient::Compile()` to return an unloaded executable [1/N],2025-03-13T09:12:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89136
copybara-service[bot],[XLA:GPU] Add debug messages to track which thunk hangs.,[XLA:GPU] Add debug messages to track which thunk hangs.,2025-03-13T09:05:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89135
copybara-service[bot],Need to stop processing if GIF is corrupt. Cannot decode if DGifSlurp fails with error eof is detected earlier. So we are returning from the function without further processing.,Need to stop processing if GIF is corrupt. Cannot decode if DGifSlurp fails with error eof is detected earlier. So we are returning from the function without further processing.,2025-03-13T08:12:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89134
copybara-service[bot],Remove determinism dep from xla_cc_test,Remove determinism dep from xla_cc_test,2025-03-13T07:32:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89133
chunhsue,Qualcomm AI Engine Direct - Convert QINT16 to QUINT16," What  Compile int16 as uint16 for better backend compatibility.  Users need to convert int16 input to uint16 in dispatch flow, demonstrates in tensorflow/lite/experimental/litert/vendors/qualcomm/dispatch/dispatch_api_qualcomm_test.cc.  Based on https://github.com/tensorflow/tensorflow/pull/88546  [ ] Fix copyright  Tests Tested with Qualcomm® AI Runtime SDK 2.32.0.250228 `dispatch_api_qualcomm_test` ``` [] Global test environment teardown [==========] 4 tests from 1 test suite ran. (890 ms total) [  PASSED  ] 4 tests. ``` `qnn_compiler_plugin_test` ``` [] Global test environment teardown [==========] 127 tests from 5 test suites ran. (4663 ms total) [  PASSED  ] 127 tests. ``` `/litert/vendors/qualcomm/core/wrappers/tests` ``` //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:op_wrapper_test (cached) PASSED in 0.0s //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:param_wrapper_test (cached) PASSED in 0.0s //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:quantize_params_wrapper_test (cached) PASSED in 0.0s //tensorflow/lite/experimental/litert/vendors/qualcomm/core/wrappers/tests:tensor_wrapper_test (cached) PASSED in 0.0s ```",2025-03-13T07:07:17Z,size:XL,open,0,1,https://github.com/tensorflow/tensorflow/issues/89132,quic    Can you guys please help review?
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-13T06:57:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89131
copybara-service[bot],Move the same result and operand type check of the tfl.batch_matmul op to runtime check.,"Move the same result and operand type check of the tfl.batch_matmul op to runtime check. This allows more varieties of ""tfl.batch_matmul"" op to pass to other nonruntime backend. converter_gen..",2025-03-13T05:56:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89130
copybara-service[bot],Override `result_shape()` for `GpuExecutable`,Override `result_shape()` for `GpuExecutable`,2025-03-13T05:46:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89129
copybara-service[bot],Introduce safer `Shape` ctors to replace the unsafe one.,Introduce safer `Shape` ctors to replace the unsafe one.,2025-03-13T04:22:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89128
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-13T04:07:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89127
copybara-service[bot],enable tfl direct lowering on shlo custom_call carrier,enable tfl direct lowering on shlo custom_call carrier,2025-03-13T03:41:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89126
copybara-service[bot],[XLA] Add a runner with 1 L4 GPU to GPU nightly workflows,[XLA] Add a runner with 1 L4 GPU to GPU nightly workflows,2025-03-13T03:38:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89125
copybara-service[bot],[xla:cpu] enable scatter fusion emitter,"[xla:cpu] enable scatter fusion emitter Known issues:  bf16 performance is poor. This is because in the emitters   we are missing an optimization that we have in XLA thunks.   We will fix this soon.  No parallel scatter. We are leaving this as future work   since the singlethreaded implementation is already bringing   significant performance improvements. Scatter microbenchmarks: ```                                                                            │ fusion_emitters  │          scatter                    │                                                                            │    cpusec/op    │  cpusec/op   vs base               │ BM_ScatterS32_R1/262144/262144/process_time                                       600.1µ ± 1%   206.5µ ±  2%  65.59% (p=0.002 n=6) BM_ScatterS32_R2/512/512/process_time                                             77.45µ ± 1%   49.88µ ±  3%  35.59% (p=0.002 n=6) BM_ScatterS32_R3/64/64/process_time                                               54.28µ ± 0%   50.49µ ±  3%   6.99% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:1/d1:64/d2:8/num_slices:1/process_time            755.0n ± 1%   702.3n ±  4%   6.98% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:50/d1:64/d2:8/num_slices:10/process_time         106.82µ ± 0%   17.66µ ±  3%  83.47% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:500/d1:64/d2:8/num_slices:100/process_time       10.301m ± 2%   2.635m ±  2%  74.42% (p=0.002 n=6) BM_SelectAndScatterF32/128/process_time                                           37.00µ ± 1%   27.76µ ±  3%  24.97% (p=0.002 n=6) BM_SelectAndScatterF32/256/process_time                                          117.86µ ± 2%   86.36µ ±  1%  26.73% (p=0.002 n=6) BM_SelectAndScatterF32/512/process_time                                           1.613m ± 2%   1.528m ± 61%        ~ (p=0.065 n=6) geomean                                                                           134.8µ        73.45µ        45.53% ``` The gap from a few days ago was wider (geomean improvement of 76%), but the recent work on improving performance of small while loops (https://github.com/openxla/xla/commit/db734148ec74) narrowed that to the numbers above. Legacy emitters (""nothunks"") compile all while loops, and are therefore a tougher baseline to compare against. Still, scatter fusion emitters are faster (all singlethreaded): ```                                                                            │ nothunks         │          scatter                    │                                                                            │      sec/op      │    sec/op     vs base               │ BM_ScatterS32_R1/262144/262144/process_time                                      360.4µ ±  1%   203.0µ ±  0%  43.67% (p=0.002 n=6) BM_ScatterS32_R2/512/512/process_time                                            50.03µ ±  2%   49.81µ ±  3%        ~ (p=0.699 n=6) BM_ScatterS32_R3/64/64/process_time                                              49.92µ ±  0%   50.41µ ±  0%   +0.96% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:1/d1:64/d2:8/num_slices:1/process_time           617.5n ±  5%   722.7n ±  1%  +17.03% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:50/d1:64/d2:8/num_slices:10/process_time         17.60µ ±  3%   17.82µ ±  4%        ~ (p=0.937 n=6) BM_SimpleScatterReduceF32_R3/d0:500/d1:64/d2:8/num_slices:100/process_time       2.708m ±  2%   2.657m ±  1%        ~ (p=0.093 n=6) BM_SelectAndScatterF32/128/process_time                                          31.26µ ±  1%   27.90µ ±  2%  10.74% (p=0.002 n=6) BM_SelectAndScatterF32/256/process_time                                         101.54µ ±  1%   86.53µ ±  4%  14.78% (p=0.002 n=6) BM_SelectAndScatterF32/512/process_time                                          850.9µ ± 60%   909.6µ ± 16%        ~ (p=0.240 n=6) geomean                                                                          74.60µ         69.60µ         6.70% ``` For the rest of the microbenchmarks, the scatter emitter does not affect performance, except when bf16 is involved due to the known issue mentioned above.  Microbenchmarks ```                                                                                     │ fusion_emitters  │           scatter                    │                                                                                     │    cpusec/op    │  cpusec/op    vs base               │ BM_ConcatenateTwoR3F32/parallel:0/batch:128/width:64/height:256/axis:0/process_time       618.9µ ±  1%    820.4µ ±  1%  +32.57% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:128/width:64/height:256/axis:1/process_time       636.3µ ±  2%    654.3µ ±  6%        ~ (p=1.000 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:128/width:64/height:256/axis:2/process_time       1.002m ±  0%    1.003m ±  1%        ~ (p=0.485 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:256/width:128/height:64/axis:0/process_time       620.9µ ±  0%    607.5µ ±  2%   2.14% (p=0.009 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:256/width:128/height:64/axis:1/process_time       618.5µ ±  1%    750.9µ ±  3%  +21.42% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:256/width:128/height:64/axis:2/process_time       1.380m ±  5%    1.285m ±  6%   6.86% (p=0.041 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:64/width:256/height:128/axis:0/process_time       627.1µ ±  2%    818.4µ ±  6%  +30.50% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:64/width:256/height:128/axis:1/process_time       620.6µ ±  2%    715.8µ ± 14%  +15.36% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:64/width:256/height:128/axis:2/process_time       1.045m ±  2%    1.020m ±  0%   2.45% (p=0.015 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:128/width:64/height:256/axis:0/process_time       1.780m ± 10%    1.779m ± 12%        ~ (p=1.000 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:128/width:64/height:256/axis:1/process_time       1.703m ±  6%    1.540m ± 25%        ~ (p=0.180 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:128/width:64/height:256/axis:2/process_time       2.573m ± 10%    2.375m ± 10%        ~ (p=0.132 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:256/width:128/height:64/axis:0/process_time       1.671m ±  8%    1.790m ± 10%        ~ (p=0.132 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:256/width:128/height:64/axis:1/process_time       1.682m ±  8%    1.792m ±  7%   +6.56% (p=0.026 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:256/width:128/height:64/axis:2/process_time       2.361m ± 14%    2.190m ± 18%        ~ (p=0.093 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:64/width:256/height:128/axis:0/process_time       1.787m ± 12%    1.787m ± 17%        ~ (p=0.699 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:64/width:256/height:128/axis:1/process_time       1.668m ± 10%    1.751m ±  8%        ~ (p=0.485 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:64/width:256/height:128/axis:2/process_time       2.286m ± 12%    2.227m ± 16%        ~ (p=0.937 n=6) BM_Conv1DStrided/1/129/process_time                                                       53.14m ±  3%    50.15m ±  3%   5.62% (p=0.002 n=6) BM_Conv1DStrided/3/129/process_time                                                       161.4m ±  7%    151.3m ±  7%   6.21% (p=0.041 n=6) BM_Conv1DTransposedStrided/129/1/process_time                                             43.65m ± 16%    50.79m ±  5%  +16.35% (p=0.026 n=6) BM_Conv1DTransposedStrided/129/3/process_time                                             139.1m ± 15%    158.1m ±  5%  +13.68% (p=0.015 n=6) BM_Conv1DTransposedStridedNonDefaultLayout/129/1/process_time                             30.11m ±  7%    32.61m ± 14%        ~ (p=0.093 n=6) BM_Conv1DTransposedStridedNonDefaultLayout/129/3/process_time                             107.6m ±  5%    123.8m ±  5%  +15.06% (p=0.002 n=6) BM_Conv2D/16/32/32/128/1/1/1024/process_time                                         121.6m ± 21%    116.0m ± 22%        ~ (p=0.310 n=6) BM_Conv2D/16/32/32/128/3/3/1024/process_time                                         962.9m ± 10%    985.1m ±  8%        ~ (p=0.589 n=6) BM_Conv2D/32/256/256/4/1/1/16/process_time                                           180.9m ±  7%    196.0m ±  5%   +8.35% (p=0.002 n=6) BM_Conv2D/32/256/256/4/3/3/16/process_time                                           434.8m ± 15%    347.8m ± 13%  20.01% (p=0.009 n=6) BM_Conv2D/32/32/32/128/1/1/1024/process_time                                         235.2m ± 25%    244.3m ± 21%        ~ (p=0.093 n=6) BM_Conv2D/32/32/32/128/3/3/1024/process_time                                          1.922 ±  9%     2.074 ±  1%   +7.92% (p=0.041 n=6) BM_Conv2D/32/32/32/96/1/1/96/process_time                                            24.70m ±  2%    22.88m ±  6%   7.37% (p=0.004 n=6) BM_Conv2D/32/32/32/96/3/3/96/process_time                                            138.0m ±  2%    145.5m ±  9%   +5.45% (p=0.002 n=6) BM_Conv2D/32/64/64/32/1/1/64/process_time                                            30.90m ±  7%    27.09m ±  6%  12.33% (p=0.002 n=6) BM_Conv2D/32/64/64/32/3/3/64/process_time                                            164.0m ±  3%    161.8m ±  2%        ~ (p=0.180 n=6) BM_Conv2D/32/64/64/4/1/1/16/process_time                                             1.232m ±  7%    1.191m ±  4%        ~ (p=0.394 n=6) BM_Conv2D/32/64/64/4/3/3/16/process_time                                             30.67m ±  7%    24.93m ± 16%  18.72% (p=0.002 n=6) BM_Conv2D/8/128/128/4/1/1/8/process_time                                             459.2µ ±  2%    434.1µ ±  3%   5.46% (p=0.002 n=6) BM_Conv2D/8/128/128/4/3/3/8/process_time                                             14.52m ±  4%    14.46m ± 10%        ~ (p=0.699 n=6) BM_Conv2D/8/32/32/128/1/1/1024/process_time                                          53.96m ± 11%    51.73m ±  9%   4.15% (p=0.041 n=6) BM_Conv2D/8/32/32/128/3/3/1024/process_time                                          435.6m ±  4%    471.7m ± 21%   +8.29% (p=0.004 n=6) BM_Conv2D/8/5/5/1/1/1/32/process_time                                                3.260µ ±  2%    3.316µ ±  2%   +1.74% (p=0.004 n=6) BM_Conv2D/8/5/5/1/3/3/32/process_time                                                13.26µ ±  1%    13.34µ ±  0%        ~ (p=0.065 n=6) BM_Conv2D/8/5/5/4/1/1/32/process_time                                                1.794µ ±  1%    1.880µ ±  1%   +4.84% (p=0.002 n=6) BM_Conv2D/8/5/5/4/3/3/32/process_time                                                17.97µ ±  2%    18.02µ ±  1%        ~ (p=0.394 n=6) BM_Conv2DStrided/process_time                                                             56.48m ±  5%    54.79m ± 10%        ~ (p=0.132 n=6) BM_Conv2DTransposedStrided/process_time                                                   43.19m ±  9%    48.14m ±  2%  +11.44% (p=0.002 n=6) BM_GroupedConv2D/1/45/45/1024/5/5/1024/1024/process_time                                  414.7m ±  7%    434.2m ± 10%        ~ (p=0.093 n=6) BM_GroupedConv2DStrided/128/128/128/process_time                                          61.65m ±  9%    59.23m ±  6%        ~ (p=0.180 n=6) BM_GroupedConv2DStrided/128/128/16/process_time                                           59.17m ±  5%    59.62m ±  6%        ~ (p=0.937 n=6) BM_GroupedConv2DTransposedStrided/128/128/128/process_time                                 4.425 ±  7%     4.522 ±  3%        ~ (p=0.240 n=6) BM_GroupedConv2DTransposedStrided/128/128/16/process_time                                  3.878 ± 11%     4.496 ± 18%        ~ (p=0.132 n=6) BM_CustomCall_16FloatBuffers/process_time                                                 1.977µ ±  7%    1.982µ ±  6%        ~ (p=0.937 n=6) BM_CustomCall_16IntAttributes/process_time                                                612.8n ±  2%    657.2n ±  3%   +7.24% (p=0.002 n=6) BM_CustomCall_Minimal/process_time                                                        522.2n ±  1%    564.4n ±  8%   +8.07% (p=0.002 n=6) BM_DagExecution/1024/process_time                                                         7.606m ±  1%    7.259m ±  3%   4.56% (p=0.002 n=6) BM_DagExecution/128/process_time                                                          684.3µ ±  4%    502.5µ ± 21%  26.57% (p=0.002 n=6) BM_DagExecution/16384/process_time                                                        174.0m ± 11%    172.3m ± 27%        ~ (p=0.937 n=6) BM_DagExecution/256/process_time                                                          1.917m ±  3%    1.767m ±  4%   7.84% (p=0.002 n=6) BM_DagExecution/512/process_time                                                          3.464m ±  0%    3.296m ±  1%   4.83% (p=0.002 n=6) BM_DagExecution/8192/process_time                                                         67.37m ± 11%    69.95m ± 15%        ~ (p=0.485 n=6) BM_BatchedDot/11/1/128/process_time                                                       37.65µ ±  0%    37.53µ ±  0%   0.31% (p=0.004 n=6) BM_BatchedDot/11/1/2/process_time                                                         540.1n ±  2%    476.2n ±  7%  11.82% (p=0.002 n=6) BM_BatchedDot/11/1/256/process_time                                                       761.4µ ±  2%    720.0µ ±  7%   5.43% (p=0.026 n=6) BM_BatchedDot/11/1/32/process_time                                                        1.804µ ±  0%    1.713µ ±  2%   5.06% (p=0.002 n=6) BM_BatchedDot/11/1/512/process_time                                                       9.297m ±  4%    9.149m ±  1%        ~ (p=0.310 n=6) BM_BatchedDot/11/1/64/process_time                                                        6.452µ ±  0%    6.385µ ±  1%   1.05% (p=0.002 n=6) BM_BatchedDot/11/2/128/process_time                                                       74.46µ ±  0%    74.51µ ±  0%        ~ (p=0.394 n=6) BM_BatchedDot/11/2/2/process_time                                                         542.4n ±  1%    479.0n ± 14%        ~ (p=0.093 n=6) BM_BatchedDot/11/2/256/process_time                                                       1.702m ±  9%    1.472m ±  4%  13.53% (p=0.002 n=6) BM_BatchedDot/11/2/32/process_time                                                        2.837µ ±  0%    2.820µ ±  1%        ~ (p=0.065 n=6) BM_BatchedDot/11/2/512/process_time                                                       21.84m ±  3%    21.67m ±  2%        ~ (p=0.394 n=6) BM_BatchedDot/11/2/64/process_time                                                        12.11µ ±  1%    12.13µ ±  1%        ~ (p=0.818 n=6) BM_BatchedDot/11/4/128/process_time                                                       148.8µ ±  0%    149.0µ ±  0%   +0.13% (p=0.004 n=6) BM_BatchedDot/11/4/2/process_time                                                         546.2n ±  0%    550.1n ±  1%   +0.72% (p=0.002 n=6) BM_BatchedDot/11/4/256/process_time                                                       3.171m ± 17%    3.177m ± 16%        ~ (p=0.937 n=6) BM_BatchedDot/11/4/32/process_time                                                        4.820µ ±  0%    4.824µ ±  0%        ~ (p=0.394 n=6) BM_BatchedDot/11/4/512/process_time                                                       47.88m ±  2%    49.00m ±  6%        ~ (p=0.394 n=6) BM_BatchedDot/11/4/64/process_time                                                        23.28µ ±  0%    23.46µ ±  1%   +0.78% (p=0.002 n=6) BM_BatchedDot/11/8/128/process_time                                                       298.1µ ±  0%    297.3µ ±  0%   0.28% (p=0.002 n=6) BM_BatchedDot/11/8/2/process_time                                                         556.4n ±  1%    557.5n ±  1%        ~ (p=0.699 n=6) BM_BatchedDot/11/8/256/process_time                                                       6.350m ±  3%    7.168m ±  6%  +12.88% (p=0.002 n=6) BM_BatchedDot/11/8/32/process_time                                                        8.760µ ±  1%    8.830µ ±  1%   +0.80% (p=0.015 n=6) BM_BatchedDot/11/8/512/process_time                                                      101.53m ±  3%    97.02m ±  2%   4.44% (p=0.002 n=6) BM_BatchedDot/11/8/64/process_time                                                        45.88µ ±  0%    45.93µ ±  0%        ~ (p=0.240 n=6) BM_BatchedDot/16/1/128/process_time                                                       40.97µ ±  0%    40.97µ ±  0%        ~ (p=0.937 n=6) BM_BatchedDot/16/1/2/process_time                                                         610.1n ±  1%    603.9n ±  2%   1.00% (p=0.026 n=6) BM_BatchedDot/16/1/256/process_time                                                       1.080m ±  2%    1.125m ± 11%        ~ (p=0.065 n=6) BM_BatchedDot/16/1/32/process_time                                                        2.089µ ±  1%    2.062µ ±  1%   1.28% (p=0.026 n=6) BM_BatchedDot/16/1/512/process_time                                                       11.21m ±  2%    11.05m ±  1%   1.40% (p=0.041 n=6) BM_BatchedDot/16/1/64/process_time                                                        7.380µ ±  1%    7.356µ ±  1%        ~ (p=0.063 n=6) BM_BatchedDot/16/2/128/process_time                                                       81.12µ ±  0%    81.06µ ±  0%        ~ (p=0.240 n=6) BM_BatchedDot/16/2/2/process_time                                                         624.0n ±  1%    613.7n ±  1%   1.65% (p=0.002 n=6) BM_BatchedDot/16/2/256/process_time                                                       2.219m ±  2%    2.085m ±  1%   6.06% (p=0.002 n=6) BM_BatchedDot/16/2/32/process_time                                                        3.371µ ±  1%    3.358µ ±  0%        ~ (p=0.132 n=6) BM_BatchedDot/16/2/512/process_time                                                       25.65m ±  3%    24.98m ±  3%        ~ (p=0.065 n=6) BM_BatchedDot/16/2/64/process_time                                                        13.79µ ±  0%    13.80µ ±  1%        ~ (p=0.310 n=6) BM_BatchedDot/16/4/128/process_time                                                       163.6µ ±  0%    163.9µ ±  0%   +0.23% (p=0.002 n=6) BM_BatchedDot/16/4/2/process_time                                                         636.3n ±  1%    627.1n ±  3%        ~ (p=0.240 n=6) BM_BatchedDot/16/4/256/process_time                                                       4.720m ±  9%    4.489m ±  8%   4.91% (p=0.041 n=6) BM_BatchedDot/16/4/32/process_time                                                        5.752µ ±  1%    5.757µ ±  1%        ~ (p=0.485 n=6) BM_BatchedDot/16/4/512/process_time                                                       53.96m ±  2%    55.37m ±  5%        ~ (p=0.310 n=6) BM_BatchedDot/16/4/64/process_time                                                        26.56µ ±  1%    26.69µ ±  1%   +0.47% (p=0.041 n=6) BM_BatchedDot/16/8/128/process_time                                                       742.6µ ±  2%    896.0µ ± 17%  +20.65% (p=0.002 n=6) BM_BatchedDot/16/8/2/process_time                                                         650.6n ±  1%    649.3n ±  1%        ~ (p=0.818 n=6) BM_BatchedDot/16/8/256/process_time                                                       9.378m ±  6%    9.013m ±  4%   3.89% (p=0.015 n=6) BM_BatchedDot/16/8/32/process_time                                                        10.49µ ±  0%    10.49µ ±  0%        ~ (p=0.937 n=6) BM_BatchedDot/16/8/512/process_time                                                       115.7m ±  4%    110.4m ±  8%        ~ (p=0.132 n=6) BM_BatchedDot/16/8/64/process_time                                                        52.45µ ±  0%    52.64µ ±  0%   +0.37% (p=0.002 n=6) BM_DynamicUpdateSliceF32/1024/process_time                                                25.79µ ±  3%    26.61µ ±  3%        ~ (p=0.093 n=6) BM_DynamicUpdateSliceF32/128/process_time                                                 3.024µ ±  2%    3.085µ ±  1%   +2.02% (p=0.004 n=6) BM_DynamicUpdateSliceF32/16384/process_time                                               788.0µ ±  6%    754.3µ ±  5%   4.28% (p=0.009 n=6) BM_DynamicUpdateSliceF32/256/process_time                                                 5.649µ ±  2%    5.743µ ±  1%   +1.67% (p=0.041 n=6) BM_DynamicUpdateSliceF32/512/process_time                                                 13.40µ ±  4%    13.63µ ±  3%   +1.68% (p=0.041 n=6) BM_DynamicUpdateSliceF32/8192/process_time                                                447.0µ ±  8%    422.9µ ±  4%   5.39% (p=0.041 n=6) BM_AddBF16/1024/process_time                                                              199.3µ ±  3%    166.9µ ±  0%  16.24% (p=0.002 n=6) BM_AddBF16/128/process_time                                                               10.16µ ±  0%    10.15µ ±  0%        ~ (p=0.132 n=6) BM_AddBF16/16384/process_time                                                             2.255m ± 12%    2.279m ± 18%        ~ (p=0.937 n=6) BM_AddBF16/256/process_time                                                               38.19µ ±  4%    31.23µ ±  1%  18.23% (p=0.002 n=6) BM_AddBF16/32768/process_time                                                             6.166m ±  5%    6.915m ± 11%  +12.15% (p=0.009 n=6) BM_AddBF16/512/process_time                                                              105.86µ ±  1%    85.75µ ±  1%  19.00% (p=0.002 n=6) BM_AddBF16/8192/process_time                                                              1.096m ±  7%    1.034m ±  9%        ~ (p=0.310 n=6) BM_AddF32/1024/process_time                                                               322.9µ ±  4%    303.1µ ±  5%   6.13% (p=0.004 n=6) BM_AddF32/128/process_time                                                                21.62µ ±  1%    19.13µ ±  2%  11.51% (p=0.002 n=6) BM_AddF32/16384/process_time                                                              7.754m ± 33%    5.547m ± 35%        ~ (p=0.093 n=6) BM_AddF32/256/process_time                                                                67.31µ ±  1%    58.99µ ±  2%  12.36% (p=0.002 n=6) BM_AddF32/32768/process_time                                                              15.90m ±  9%    15.18m ±  9%        ~ (p=0.180 n=6) BM_AddF32/512/process_time                                                                144.5µ ±  1%    131.3µ ±  1%   9.17% (p=0.002 n=6) BM_AddF32/8192/process_time                                                               1.859m ± 17%    1.755m ± 13%        ~ (p=0.180 n=6) BM_ConvertF32ToBF16/1024/process_time                                                     179.5µ ±  2%    174.6µ ±  1%   2.77% (p=0.002 n=6) BM_ConvertF32ToBF16/128/process_time                                                      5.814µ ±  1%    5.926µ ±  1%   +1.93% (p=0.004 n=6) BM_ConvertF32ToBF16/16384/process_time                                                    1.998m ±  8%    1.992m ± 13%        ~ (p=0.589 n=6) BM_ConvertF32ToBF16/256/process_time                                                      29.26µ ±  1%    28.69µ ±  2%   1.92% (p=0.004 n=6) BM_ConvertF32ToBF16/32768/process_time                                                    6.877m ± 46%    6.017m ± 12%  12.50% (p=0.004 n=6) BM_ConvertF32ToBF16/512/process_time                                                      91.14µ ±  4%    83.40µ ±  1%   8.49% (p=0.002 n=6) BM_ConvertF32ToBF16/8192/process_time                                                     1.091m ± 10%    1.066m ± 13%        ~ (p=0.818 n=6) BM_BcastFusionF32/1024/process_time                                                       294.0µ ±  2%    262.2µ ±  2%  10.80% (p=0.002 n=6) BM_BcastFusionF32/128/process_time                                                        22.16µ ±  4%    16.54µ ±  1%  25.36% (p=0.002 n=6) BM_BcastFusionF32/16384/process_time                                                      3.567m ±  7%    3.486m ± 11%        ~ (p=1.000 n=6) BM_BcastFusionF32/256/process_time                                                        57.94µ ±  3%    47.47µ ±  3%  18.08% (p=0.002 n=6) BM_BcastFusionF32/512/process_time                                                        125.8µ ±  4%    104.9µ ±  4%  16.58% (p=0.002 n=6) BM_BcastFusionF32/8192/process_time                                                       1.779m ±  9%    1.685m ± 13%        ~ (p=0.132 n=6) BM_ChainOfAddF32/1024/process_time                                                        74.55µ ±  3%    71.41µ ±  1%   4.21% (p=0.002 n=6) BM_ChainOfAddF32/128/process_time                                                         12.63µ ±  1%    12.46µ ±  1%   1.36% (p=0.002 n=6) BM_ChainOfAddF32/256/process_time                                                         20.47µ ±  2%    20.09µ ±  2%        ~ (p=0.093 n=6) BM_ChainOfAddF32/512/process_time                                                         37.49µ ±  3%    36.21µ ±  1%   3.42% (p=0.002 n=6) BM_ChainOfAddF32/64/process_time                                                          8.909µ ±  3%    8.844µ ±  2%        ~ (p=0.132 n=6) BM_DynamicUpdateSliceFusionF32/1024/process_time                                          26.50µ ±  3%    25.52µ ±  4%   3.69% (p=0.009 n=6) BM_DynamicUpdateSliceFusionF32/128/process_time                                           2.936µ ±  1%    2.895µ ±  0%   1.40% (p=0.002 n=6) BM_DynamicUpdateSliceFusionF32/16384/process_time                                         786.4µ ±  5%    785.2µ ±  4%        ~ (p=0.937 n=6) BM_DynamicUpdateSliceFusionF32/256/process_time                                           5.539µ ±  2%    5.466µ ±  1%   1.30% (p=0.002 n=6) BM_DynamicUpdateSliceFusionF32/512/process_time                                           13.49µ ±  1%    13.84µ ±  2%   +2.58% (p=0.026 n=6) BM_DynamicUpdateSliceFusionF32/8192/process_time                                          428.0µ ±  4%    409.5µ ±  6%        ~ (p=0.132 n=6) BM_FusionF32/1024/process_time                                                            339.0µ ±  3%    306.2µ ±  1%   9.67% (p=0.002 n=6) BM_FusionF32/128/process_time                                                             26.59µ ± 14%    19.27µ ±  3%  27.55% (p=0.002 n=6) BM_FusionF32/16384/process_time                                                           6.154m ± 20%    6.126m ± 11%        ~ (p=1.000 n=6) BM_FusionF32/256/process_time                                                             73.46µ ±  2%    59.92µ ±  2%  18.43% (p=0.002 n=6) BM_FusionF32/512/process_time                                                             162.4µ ±  1%    132.1µ ±  1%  18.61% (p=0.002 n=6) BM_FusionF32/8192/process_time                                                            2.007m ±  8%    2.039m ± 12%        ~ (p=0.937 n=6) BM_FusionF32_2/160/process_time                                                           1.257µ ±  1%    1.231µ ±  1%   2.05% (p=0.002 n=6) BM_FusionF32_2/240/process_time                                                           1.533µ ±  1%    1.507µ ±  1%   1.66% (p=0.002 n=6) BM_FusionF32_2/40/process_time                                                            848.2n ±  1%    830.0n ±  1%   2.15% (p=0.002 n=6) BM_FusionF32_2/80/process_time                                                            993.7n ±  2%    955.7n ±  3%   3.82% (p=0.004 n=6) BM_GatherS32/10/128/1/process_time                                                        466.3n ± 12%    463.8n ±  1%        ~ (p=0.310 n=6) BM_GatherS32/10/128/2/process_time                                                        473.0n ±  1%    470.3n ±  1%   0.57% (p=0.015 n=6) BM_GatherS32/10/128/32/process_time                                                       648.0n ±  2%    636.9n ±  1%   1.72% (p=0.026 n=6) BM_GatherS32/10/256/1/process_time                                                        473.8n ±  1%    472.5n ±  7%        ~ (p=0.699 n=6) BM_GatherS32/10/256/2/process_time                                                        484.8n ±  1%    480.4n ±  1%   0.91% (p=0.015 n=6) BM_GatherS32/10/256/64/process_time                                                       1.209µ ±  2%    1.215µ ±  3%        ~ (p=0.310 n=6) BM_GatherS32/10/3/1/process_time                                                          457.0n ±  3%    457.0n ±  1%        ~ (p=0.937 n=6) BM_GatherS32/10/3/2/process_time                                                          464.9n ±  3%    459.4n ±  1%        ~ (p=0.132 n=6) BM_GatherS32/10/3/4/process_time                                                          466.6n ±  3%    463.6n ±  1%        ~ (p=0.180 n=6) BM_GatherS32/10/32/1/process_time                                                         460.8n ±  1%    460.7n ±  1%        ~ (p=1.000 n=6) BM_GatherS32/10/32/2/process_time                                                         465.6n ±  2%    460.9n ±  5%        ~ (p=0.180 n=6) BM_GatherS32/10/32/8/process_time                                                         473.4n ±  1%    466.9n ±  0%   1.37% (p=0.002 n=6) BM_GatherS32/10/512/1/process_time                                                        485.7n ±  2%    485.8n ±  9%        ~ (p=0.699 n=6) BM_GatherS32/10/512/128/process_time                                                      3.955µ ±  2%    3.934µ ±  0%        ~ (p=0.240 n=6) BM_GatherS32/10/512/2/process_time                                                        511.5n ±  2%    507.1n ±  1%   0.86% (p=0.002 n=6) BM_GatherS32/10/64/1/process_time                                                         462.4n ±  4%    457.5n ±  3%        ~ (p=0.093 n=6) BM_GatherS32/10/64/16/process_time                                                        501.8n ±  1%    498.0n ±  1%        ~ (p=0.093 n=6) BM_GatherS32/10/64/2/process_time                                                         465.3n ±  2%    463.0n ±  2%        ~ (p=0.240 n=6) BM_GatherS32/100/128/1/process_time                                                       466.7n ±  3%    460.8n ±  1%   1.26% (p=0.026 n=6) BM_GatherS32/100/128/2/process_time                                                       477.7n ±  1%    469.9n ±  2%   1.63% (p=0.015 n=6) BM_GatherS32/100/128/32/process_time                                                      770.8n ±  1%    773.6n ±  1%        ~ (p=0.394 n=6) BM_GatherS32/100/256/1/process_time                                                       475.0n ±  8%    470.7n ±  1%        ~ (p=0.065 n=6) BM_GatherS32/100/256/2/process_time                                                       484.8n ±  2%    478.0n ±  1%   1.39% (p=0.002 n=6) BM_GatherS32/100/256/64/process_time                                                      1.600µ ±  3%    1.596µ ±  0%        ~ (p=0.240 n=6) BM_GatherS32/100/3/1/process_time                                                         460.4n ±  2%    455.0n ±  1%   1.17% (p=0.002 n=6) BM_GatherS32/100/3/2/process_time                                                         463.6n ±  2%    460.4n ±  1%        ~ (p=0.180 n=6) BM_GatherS32/100/3/4/process_time                                                         467.2n ±  1%    463.2n ±  1%   0.86% (p=0.009 n=6) BM_GatherS32/100/32/1/process_time                                                        462.3n ±  1%    460.7n ±  2%        ~ (p=0.310 n=6) BM_GatherS32/100/32/2/process_time                                                        465.3n ±  1%    461.9n ±  1%        ~ (p=0.240 n=6) BM_GatherS32/100/32/8/process_time                                                        476.2n ±  1%    469.4n ±  1%   1.43% (p=0.009 n=6) BM_GatherS32/100/512/1/process_time                                                       488.4n ±  1%    484.6n ±  1%   0.77% (p=0.041 n=6) BM_GatherS32/100/512/128/process_time                                                     5.184µ ±  3%    5.174µ ±  1%        ~ (p=0.589 n=6) BM_GatherS32/100/512/2/process_time                                                       510.9n ±  1%    506.5n ±  1%        ~ (p=0.065 n=6) BM_GatherS32/100/64/1/process_time                                                        462.8n ±  3%    458.6n ±  1%   0.90% (p=0.002 n=6) BM_GatherS32/100/64/16/process_time                                                       507.3n ±  1%    499.3n ±  1%   1.58% (p=0.002 n=6) BM_GatherS32/100/64/2/process_time                                                        466.0n ±  1%    462.6n ±  0%   0.73% (p=0.002 n=6) BM_GatherS32/3/128/1/process_time                                                         465.4n ±  1%    462.0n ±  3%        ~ (p=0.180 n=6) BM_GatherS32/3/128/2/process_time                                                         473.2n ±  2%    469.4n ±  3%        ~ (p=0.132 n=6) BM_GatherS32/3/128/32/process_time                                                        629.8n ±  2%    633.4n ±  5%        ~ (p=0.937 n=6) BM_GatherS32/3/256/1/process_time                                                         472.3n ±  0%    470.1n ±  1%        ~ (p=0.818 n=6) BM_GatherS32/3/256/2/process_time                                                         486.1n ±  1%    479.1n ±  8%        ~ (p=0.180 n=6) BM_GatherS32/3/256/64/process_time                                                        1.100µ ±  1%    1.104µ ±  5%        ~ (p=1.000 n=6) BM_GatherS32/3/3/1/process_time                                                           460.6n ±  1%    460.3n ±  2%        ~ (p=0.818 n=6) BM_GatherS32/3/3/2/process_time                                                           465.4n ±  1%    462.2n ±  7%        ~ (p=0.589 n=6) BM_GatherS32/3/3/4/process_time                                                           466.4n ±  1%    465.6n ±  2%        ~ (p=0.699 n=6) BM_GatherS32/3/32/1/process_time                                                          461.6n ±  2%    460.4n ±  1%        ~ (p=0.240 n=6) BM_GatherS32/3/32/2/process_time                                                          464.6n ±  1%    461.4n ±  5%        ~ (p=0.394 n=6) BM_GatherS32/3/32/8/process_time                                                          475.2n ±  6%    468.5n ±  1%   1.41% (p=0.009 n=6) BM_GatherS32/3/512/1/process_time                                                         484.4n ±  3%    484.6n ±  1%        ~ (p=0.937 n=6) BM_GatherS32/3/512/128/process_time                                                       3.068µ ±  2%    3.070µ ±  1%        ~ (p=0.937 n=6) BM_GatherS32/3/512/2/process_time                                                         512.9n ±  3%    508.1n ±  2%        ~ (p=0.310 n=6) BM_GatherS32/3/64/1/process_time                                                          461.5n ±  2%    457.4n ±  1%   0.88% (p=0.004 n=6) BM_GatherS32/3/64/16/process_time                                                         496.8n ±  1%    498.9n ±  1%        ~ (p=0.589 n=6) BM_GatherS32/3/64/2/process_time                                                          465.5n ±  1%    462.4n ±  1%        ~ (p=0.065 n=6) BM_Optimizer0/1024/process_time                                                           5.001m ±  1%    4.892m ± 11%        ~ (p=0.394 n=6) BM_Optimizer0/128/process_time                                                            460.4µ ± 12%    380.2µ ±  2%  17.41% (p=0.002 n=6) BM_Optimizer0/16384/process_time                                                          62.65m ±  6%    62.65m ± 12%        ~ (p=0.937 n=6) BM_Optimizer0/256/process_time                                                            1.215m ± 10%    1.066m ±  3%  12.21% (p=0.004 n=6) BM_Optimizer0/512/process_time                                                            2.436m ± 18%    2.236m ±  3%   8.21% (p=0.002 n=6) BM_Optimizer0/8192/process_time                                                           35.19m ±  5%    32.69m ±  6%        ~ (p=0.065 n=6) BM_PadF32/1024/process_time                                                               29.85m ±  8%    28.86m ± 10%        ~ (p=0.310 n=6) BM_PadF32/128/process_time                                                                340.1µ ±  1%    318.1µ ±  0%   6.48% (p=0.002 n=6) BM_PadF32/256/process_time                                                                1.178m ±  5%    1.141m ±  4%        ~ (p=0.065 n=6) BM_PadF32/4096/process_time                                                               423.3m ±  0%    423.2m ±  3%        ~ (p=0.937 n=6) BM_PadF32/512/process_time                                                                4.229m ± 10%    4.489m ±  8%        ~ (p=0.240 n=6) BM_ReduceAddBF16/1024/process_time                                                        3.387m ±  3%    3.409m ±  6%        ~ (p=0.310 n=6) BM_ReduceAddBF16/128/process_time                                                         239.0µ ±  0%    239.5µ ±  0%   +0.22% (p=0.002 n=6) BM_ReduceAddBF16/16384/process_time                                                       51.13m ±  1%    53.88m ±  6%        ~ (p=0.132 n=6) BM_ReduceAddBF16/256/process_time                                                         719.8µ ±  8%   1121.9µ ± 13%  +55.87% (p=0.002 n=6) BM_ReduceAddBF16/512/process_time                                                         1.674m ±  2%    2.152m ±  7%  +28.60% (p=0.002 n=6) BM_ReduceAddBF16/8192/process_time                                                        25.97m ±  1%    27.34m ±  8%        ~ (p=0.065 n=6) BM_ReduceAddF32/1024/process_time                                                         487.0µ ±  2%    469.0µ ±  5%   3.70% (p=0.041 n=6) BM_ReduceAddF32/128/process_time                                                          23.40µ ±  0%    23.42µ ±  0%        ~ (p=0.310 n=6) BM_ReduceAddF32/16384/process_time                                                        6.683m ±  0%    6.657m ±  0%   0.39% (p=0.002 n=6) BM_ReduceAddF32/256/process_time                                                          62.92µ ±  4%    60.37µ ±  1%   4.05% (p=0.002 n=6) BM_ReduceAddF32/512/process_time                                                          246.6µ ±  0%    238.0µ ±  0%   3.48% (p=0.002 n=6) BM_ReduceAddF32/8192/process_time                                                         3.471m ±  0%    3.459m ±  1%        ~ (p=0.132 n=6) BM_ScatterS32_R1/262144/262144/process_time                                               600.1µ ±  1%    206.5µ ±  2%  65.59% (p=0.002 n=6) BM_ScatterS32_R2/512/512/process_time                                                     77.45µ ±  1%    49.88µ ±  3%  35.59% (p=0.002 n=6) BM_ScatterS32_R3/64/64/process_time                                                       54.28µ ±  0%    50.49µ ±  3%   6.99% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:1/d1:64/d2:8/num_slices:1/process_time                    755.0n ±  1%    702.3n ±  4%   6.98% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:50/d1:64/d2:8/num_slices:10/process_time                 106.82µ ±  0%    17.66µ ±  3%  83.47% (p=0.002 n=6) BM_SimpleScatterReduceF32_R3/d0:500/d1:64/d2:8/num_slices:100/process_time               10.301m ±  2%    2.635m ±  2%  74.42% (p=0.002 n=6) BM_SelectAndScatterF32/128/process_time                                                   37.00µ ±  1%    27.76µ ±  3%  24.97% (p=0.002 n=6) BM_SelectAndScatterF32/256/process_time                                                  117.86µ ±  2%    86.36µ ±  1%  26.73% (p=0.002 n=6) BM_SelectAndScatterF32/512/process_time                                                   1.613m ±  2%    1.528m ± 61%        ~ (p=0.065 n=6) BM_TanhF16/1024/process_time                                                              687.7n ±  1%    686.2n ±  2%        ~ (p=0.589 n=6) BM_TanhF16/128/process_time                                                               439.0n ±  2%    443.6n ±  1%        ~ (p=0.180 n=6) BM_TanhF16/256/process_time                                                               481.3n ±  1%    482.0n ±  1%        ~ (p=0.240 n=6) BM_TanhF16/4096/process_time                                                              1.522µ ±  0%    1.522µ ±  1%        ~ (p=0.818 n=6) BM_TanhF16/512/process_time                                                               553.5n ±  0%    556.9n ±  2%        ~ (p=0.394 n=6) BM_TanhF32/1024/process_time                                                              751.8n ±  1%    693.2n ±  1%   7.79% (p=0.002 n=6) BM_TanhF32/128/process_time                                                               507.8n ±  0%    451.7n ±  3%  11.05% (p=0.002 n=6) BM_TanhF32/256/process_time                                                               552.0n ±  1%    494.1n ±  1%  10.49% (p=0.002 n=6) BM_TanhF32/4096/process_time                                                              1.608µ ±  2%    1.580µ ±  1%   1.75% (p=0.026 n=6) BM_TanhF32/512/process_time                                                               623.4n ±  1%    559.9n ±  1%  10.18% (p=0.002 n=6) BM_TanhF64/1024/process_time                                                              12.59µ ±  0%    12.60µ ±  0%        ~ (p=0.699 n=6) BM_TanhF64/128/process_time                                                               1.945µ ±  0%    1.947µ ±  0%        ~ (p=0.937 n=6) BM_TanhF64/256/process_time                                                               3.464µ ±  0%    3.471µ ±  1%        ~ (p=0.310 n=6) BM_TanhF64/4096/process_time                                                              49.05µ ±  0%    49.05µ ±  0%        ~ (p=0.937 n=6) BM_TanhF64/512/process_time                                                               6.506µ ±  0%    6.522µ ±  1%        ~ (p=0.132 n=6) geomean                                                                                   91.38µ          88.09µ         3.59% ``` ",2025-03-13T02:19:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89124
copybara-service[bot],[xla:cpu] flip xla_cpu_use_fusion_emitters to true,"[xla:cpu] flip xla_cpu_use_fusion_emitters to true Note, however, that no fusion emitter is enabled yet. Flipping this flag does change the HLO pipeline, and is therefore worth submitting as its own CL to get test coverage and measure performance differences. The largest perf difference can be seen in Gather:  Microbenchmarks thunks vs. fusion emitters ```                                                                                     │ thunks           │           fusion_emitters            │                                                                                     │    cpusec/op    │  cpusec/op    vs base               │ BM_ConcatenateTwoR3F32/parallel:0/batch:128/width:64/height:256/axis:0/process_time       602.3µ ±  0%    603.0µ ±  1%        ~ (p=0.485 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:128/width:64/height:256/axis:1/process_time       624.8µ ±  2%    806.0µ ±  1%  +29.00% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:128/width:64/height:256/axis:2/process_time       998.4µ ±  0%   1127.5µ ±  1%  +12.93% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:256/width:128/height:64/axis:0/process_time       605.0µ ±  4%    609.5µ ±  2%        ~ (p=0.132 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:256/width:128/height:64/axis:1/process_time       620.0µ ±  2%    854.8µ ± 28%        ~ (p=0.132 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:256/width:128/height:64/axis:2/process_time       1.284m ±  3%    1.936m ±  7%  +50.79% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:64/width:256/height:128/axis:0/process_time       602.2µ ±  3%    602.8µ ±  1%        ~ (p=0.699 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:64/width:256/height:128/axis:1/process_time       615.2µ ±  0%    828.1µ ±  8%  +34.61% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:0/batch:64/width:256/height:128/axis:2/process_time       1.031m ±  1%    1.175m ±  6%  +13.99% (p=0.002 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:128/width:64/height:256/axis:0/process_time       1.753m ± 13%    1.650m ±  8%        ~ (p=0.180 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:128/width:64/height:256/axis:1/process_time       1.711m ±  6%    1.804m ± 11%        ~ (p=0.310 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:128/width:64/height:256/axis:2/process_time       2.602m ±  8%    2.434m ±  7%   6.47% (p=0.041 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:256/width:128/height:64/axis:0/process_time       1.684m ± 12%    1.730m ± 14%        ~ (p=0.485 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:256/width:128/height:64/axis:1/process_time       1.798m ± 10%    1.668m ±  5%   7.25% (p=0.026 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:256/width:128/height:64/axis:2/process_time       2.287m ± 12%    2.251m ±  7%        ~ (p=0.180 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:64/width:256/height:128/axis:0/process_time       1.703m ±  7%    1.666m ±  6%        ~ (p=0.485 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:64/width:256/height:128/axis:1/process_time       1.727m ±  5%    1.717m ±  6%        ~ (p=0.589 n=6) BM_ConcatenateTwoR3F32/parallel:1/batch:64/width:256/height:128/axis:2/process_time       2.232m ± 11%    2.235m ± 16%        ~ (p=0.818 n=6) BM_Conv1DStrided/1/129/process_time                                                       54.79m ±  8%    54.83m ±  6%        ~ (p=0.699 n=6) BM_Conv1DStrided/3/129/process_time                                                       171.4m ±  9%    170.4m ±  6%        ~ (p=1.000 n=6) BM_Conv1DTransposedStrided/129/1/process_time                                             47.47m ±  3%    53.37m ±  7%  +12.43% (p=0.002 n=6) BM_Conv1DTransposedStrided/129/3/process_time                                             155.8m ±  5%    155.1m ±  9%        ~ (p=0.937 n=6) BM_Conv1DTransposedStridedNonDefaultLayout/129/1/process_time                             37.36m ±  9%    34.52m ±  2%        ~ (p=0.180 n=6) BM_Conv1DTransposedStridedNonDefaultLayout/129/3/process_time                             122.1m ±  5%    128.4m ±  7%        ~ (p=0.132 n=6) BM_Conv2D/16/32/32/128/1/1/1024/process_time                                         123.6m ± 21%    121.2m ± 14%        ~ (p=0.180 n=6) BM_Conv2D/16/32/32/128/3/3/1024/process_time                                         888.2m ± 10%   1032.4m ± 27%        ~ (p=0.093 n=6) BM_Conv2D/32/256/256/4/1/1/16/process_time                                           202.9m ±  8%    190.3m ±  8%   6.24% (p=0.026 n=6) BM_Conv2D/32/256/256/4/3/3/16/process_time                                           427.8m ± 15%    376.0m ±  5%  12.10% (p=0.041 n=6) BM_Conv2D/32/32/32/128/1/1/1024/process_time                                         249.9m ±  4%    249.0m ±  3%        ~ (p=0.937 n=6) BM_Conv2D/32/32/32/128/3/3/1024/process_time                                          1.948 ±  8%     1.890 ±  6%        ~ (p=0.310 n=6) BM_Conv2D/32/32/32/96/1/1/96/process_time                                            21.98m ±  8%    22.39m ±  3%        ~ (p=0.818 n=6) BM_Conv2D/32/32/32/96/3/3/96/process_time                                            141.6m ±  4%    144.9m ±  3%        ~ (p=0.310 n=6) BM_Conv2D/32/64/64/32/1/1/64/process_time                                            29.17m ±  3%    27.90m ±  4%   4.35% (p=0.015 n=6) BM_Conv2D/32/64/64/32/3/3/64/process_time                                            161.0m ±  4%    159.4m ±  3%        ~ (p=0.310 n=6) BM_Conv2D/32/64/64/4/1/1/16/process_time                                             1.202m ±  5%    1.207m ± 14%        ~ (p=0.699 n=6) BM_Conv2D/32/64/64/4/3/3/16/process_time                                             26.16m ±  4%    26.60m ±  3%        ~ (p=0.240 n=6) BM_Conv2D/8/128/128/4/1/1/8/process_time                                             457.0µ ±  4%    453.2µ ±  4%        ~ (p=0.589 n=6) BM_Conv2D/8/128/128/4/3/3/8/process_time                                             14.59m ±  4%    14.81m ±  4%        ~ (p=0.485 n=6) BM_Conv2D/8/32/32/128/1/1/1024/process_time                                          62.50m ± 10%    57.55m ±  8%        ~ (p=0.065 n=6) BM_Conv2D/8/32/32/128/3/3/1024/process_time                                          449.5m ±  6%    451.0m ±  6%        ~ (p=0.937 n=6) BM_Conv2D/8/5/5/1/1/1/32/process_time                                                3.269µ ±  4%    3.262µ ±  1%        ~ (p=0.589 n=6) BM_Conv2D/8/5/5/1/3/3/32/process_time                                                13.38µ ±  9%    13.31µ ±  0%   0.52% (p=0.009 n=6) BM_Conv2D/8/5/5/4/1/1/32/process_time                                                1.821µ ±  1%    1.842µ ±  0%   +1.10% (p=0.041 n=6) BM_Conv2D/8/5/5/4/3/3/32/process_time                                                17.93µ ±  4%    18.00µ ±  2%        ~ (p=0.310 n=6) BM_Conv2DStrided/process_time                                                             57.50m ±  5%    59.22m ±  6%        ~ (p=0.394 n=6) BM_Conv2DTransposedStrided/process_time                                                   47.48m ±  2%    48.09m ±  4%        ~ (p=0.310 n=6) BM_GroupedConv2D/1/45/45/1024/5/5/1024/1024/process_time                                  439.2m ±  5%    425.9m ±  3%        ~ (p=0.093 n=6) BM_GroupedConv2DStrided/128/128/128/process_time                                          65.37m ±  8%    60.74m ±  6%        ~ (p=0.132 n=6) BM_GroupedConv2DStrided/128/128/16/process_time                                           57.54m ±  7%    60.12m ± 11%        ~ (p=0.485 n=6) BM_GroupedConv2DTransposedStrided/128/128/128/process_time                                 4.373 ±  4%     4.342 ±  5%        ~ (p=0.937 n=6) BM_GroupedConv2DTransposedStrided/128/128/16/process_time                                  4.997 ±  5%     4.696 ±  7%   6.02% (p=0.026 n=6) BM_CustomCall_16FloatBuffers/process_time                                                 2.039µ ±  4%    1.916µ ±  4%   5.99% (p=0.004 n=6) BM_CustomCall_16IntAttributes/process_time                                                677.2n ±  2%    642.6n ±  1%   5.10% (p=0.002 n=6) BM_CustomCall_Minimal/process_time                                                        590.2n ±  1%    557.3n ±  1%   5.57% (p=0.002 n=6) BM_DagExecution/1024/process_time                                                         7.481m ±  1%    7.649m ±  9%   +2.24% (p=0.009 n=6) BM_DagExecution/128/process_time                                                          682.5µ ±  5%    689.5µ ±  2%        ~ (p=0.065 n=6) BM_DagExecution/16384/process_time                                                        171.4m ±  9%    197.4m ± 13%  +15.17% (p=0.015 n=6) BM_DagExecution/256/process_time                                                          1.926m ±  4%    1.939m ±  1%        ~ (p=0.699 n=6) BM_DagExecution/512/process_time                                                          3.449m ±  1%    3.467m ±  2%        ~ (p=0.240 n=6) BM_DagExecution/8192/process_time                                                         59.51m ± 15%    62.34m ± 15%        ~ (p=0.310 n=6) BM_BatchedDot/11/1/128/process_time                                                       37.62µ ±  0%    37.50µ ±  0%   0.33% (p=0.002 n=6) BM_BatchedDot/11/1/2/process_time                                                         488.7n ± 11%    458.3n ±  3%   6.21% (p=0.002 n=6) BM_BatchedDot/11/1/256/process_time                                                       766.9µ ±  3%    769.9µ ±  3%        ~ (p=0.818 n=6) BM_BatchedDot/11/1/32/process_time                                                        1.770µ ±  2%    1.716µ ±  0%   3.05% (p=0.002 n=6) BM_BatchedDot/11/1/512/process_time                                                       9.177m ±  1%    9.440m ±  2%   +2.86% (p=0.004 n=6) BM_BatchedDot/11/1/64/process_time                                                        6.431µ ±  2%    6.376µ ±  0%   0.85% (p=0.002 n=6) BM_BatchedDot/11/2/128/process_time                                                       74.44µ ±  0%    74.38µ ±  0%        ~ (p=0.310 n=6) BM_BatchedDot/11/2/2/process_time                                                         495.0n ±  1%    462.9n ±  4%   6.49% (p=0.002 n=6) BM_BatchedDot/11/2/256/process_time                                                       1.623m ± 26%    1.575m ±  3%        ~ (p=1.000 n=6) BM_BatchedDot/11/2/32/process_time                                                        2.804µ ±  0%    2.750µ ±  0%   1.92% (p=0.002 n=6) BM_BatchedDot/11/2/512/process_time                                                       26.11m ±  3%    22.17m ±  2%  15.08% (p=0.002 n=6) BM_BatchedDot/11/2/64/process_time                                                        12.10µ ±  1%    12.00µ ±  1%   0.81% (p=0.015 n=6) BM_BatchedDot/11/4/128/process_time                                                       148.8µ ±  0%    148.6µ ±  0%   0.13% (p=0.002 n=6) BM_BatchedDot/11/4/2/process_time                                                         500.0n ±  1%    465.5n ±  2%   6.91% (p=0.002 n=6) BM_BatchedDot/11/4/256/process_time                                                       3.716m ±  2%    3.192m ±  4%  14.11% (p=0.002 n=6) BM_BatchedDot/11/4/32/process_time                                                        4.818µ ±  1%    4.788µ ±  4%        ~ (p=0.394 n=6) BM_BatchedDot/11/4/512/process_time                                                       47.46m ±  3%    46.98m ±  1%        ~ (p=0.065 n=6) BM_BatchedDot/11/4/64/process_time                                                        23.36µ ±  0%    23.26µ ±  0%   0.45% (p=0.002 n=6) BM_BatchedDot/11/8/128/process_time                                                       298.0µ ±  0%    297.8µ ±  0%   0.05% (p=0.041 n=6) BM_BatchedDot/11/8/2/process_time                                                         514.2n ±  2%    475.1n ±  1%   7.60% (p=0.002 n=6) BM_BatchedDot/11/8/256/process_time                                                       7.223m ±  7%    6.327m ±  4%  12.40% (p=0.002 n=6) BM_BatchedDot/11/8/32/process_time                                                        8.787µ ±  0%    8.739µ ±  0%   0.55% (p=0.002 n=6) BM_BatchedDot/11/8/512/process_time                                                      101.90m ±  5%    98.02m ±  3%        ~ (p=0.180 n=6) BM_BatchedDot/11/8/64/process_time                                                        46.00µ ±  0%    45.93µ ±  0%        ~ (p=0.065 n=6) BM_BatchedDot/16/1/128/process_time                                                       40.95µ ±  0%    40.84µ ±  0%   0.27% (p=0.002 n=6) BM_BatchedDot/16/1/2/process_time                                                         557.7n ±  5%    523.7n ±  6%   6.10% (p=0.004 n=6) BM_BatchedDot/16/1/256/process_time                                                       1.113m ±  4%    1.078m ±  3%        ~ (p=0.093 n=6) BM_BatchedDot/16/1/32/process_time                                                        2.059µ ±  1%    2.001µ ±  1%   2.85% (p=0.002 n=6) BM_BatchedDot/16/1/512/process_time                                                       11.57m ±  2%    11.10m ±  1%   4.02% (p=0.002 n=6) BM_BatchedDot/16/1/64/process_time                                                        7.363µ ±  1%    7.293µ ±  1%   0.95% (p=0.002 n=6) BM_BatchedDot/16/2/128/process_time                                                       81.31µ ±  0%    81.00µ ±  0%   0.38% (p=0.002 n=6) BM_BatchedDot/16/2/2/process_time                                                         594.0n ±  4%    530.4n ±  2%  10.71% (p=0.002 n=6) BM_BatchedDot/16/2/256/process_time                                                       2.270m ±  1%    2.212m ±  4%        ~ (p=0.180 n=6) BM_BatchedDot/16/2/32/process_time                                                        3.348µ ±  1%    3.297µ ±  1%   1.54% (p=0.002 n=6) BM_BatchedDot/16/2/512/process_time                                                       25.68m ±  2%    25.40m ±  2%        ~ (p=0.485 n=6) BM_BatchedDot/16/2/64/process_time                                                        13.89µ ±  1%    13.73µ ±  0%   1.19% (p=0.002 n=6) BM_BatchedDot/16/4/128/process_time                                                       163.8µ ±  0%    163.6µ ±  0%        ~ (p=0.093 n=6) BM_BatchedDot/16/4/2/process_time                                                         607.9n ±  1%    542.8n ±  1%  10.71% (p=0.002 n=6) BM_BatchedDot/16/4/256/process_time                                                       4.782m ±  1%    4.668m ±  2%   2.39% (p=0.009 n=6) BM_BatchedDot/16/4/32/process_time                                                        5.754µ ±  1%    5.675µ ±  2%   1.38% (p=0.041 n=6) BM_BatchedDot/16/4/512/process_time                                                       56.58m ±  3%    55.00m ±  1%   2.78% (p=0.015 n=6) BM_BatchedDot/16/4/64/process_time                                                        26.67µ ±  0%    26.59µ ±  0%        ~ (p=0.093 n=6) BM_BatchedDot/16/8/128/process_time                                                       749.6µ ± 16%    753.7µ ±  4%        ~ (p=0.180 n=6) BM_BatchedDot/16/8/2/process_time                                                         632.7n ±  4%    636.7n ±  1%        ~ (p=0.699 n=6) BM_BatchedDot/16/8/256/process_time                                                       9.479m ±  2%    9.313m ±  2%   1.75% (p=0.009 n=6) BM_BatchedDot/16/8/32/process_time                                                        10.52µ ±  0%    10.53µ ±  0%        ~ (p=0.699 n=6) BM_BatchedDot/16/8/512/process_time                                                       114.5m ±  7%    113.0m ±  4%        ~ (p=0.240 n=6) BM_BatchedDot/16/8/64/process_time                                                        52.56µ ±  0%    52.50µ ±  0%   0.13% (p=0.015 n=6) BM_DynamicUpdateSliceF32/1024/process_time                                                25.78µ ±  1%    25.95µ ±  3%        ~ (p=0.065 n=6) BM_DynamicUpdateSliceF32/128/process_time                                                 3.151µ ±  3%    3.026µ ±  0%   3.97% (p=0.002 n=6) BM_DynamicUpdateSliceF32/16384/process_time                                               798.0µ ±  7%    764.4µ ±  4%   4.22% (p=0.015 n=6) BM_DynamicUpdateSliceF32/256/process_time                                                 5.810µ ±  1%    5.680µ ±  3%        ~ (p=0.065 n=6) BM_DynamicUpdateSliceF32/512/process_time                                                 13.70µ ±  3%    13.65µ ±  3%        ~ (p=0.240 n=6) BM_DynamicUpdateSliceF32/8192/process_time                                                435.5µ ±  2%    427.8µ ±  5%        ~ (p=0.180 n=6) BM_AddBF16/1024/process_time                                                              198.4µ ±  1%    183.6µ ±  2%   7.45% (p=0.002 n=6) BM_AddBF16/128/process_time                                                               10.23µ ±  0%    10.17µ ±  0%   0.56% (p=0.002 n=6) BM_AddBF16/16384/process_time                                                             2.231m ±  4%    2.164m ± 27%        ~ (p=1.000 n=6) BM_AddBF16/256/process_time                                                               37.87µ ±  3%    33.62µ ±  1%  11.23% (p=0.002 n=6) BM_AddBF16/32768/process_time                                                             6.420m ± 15%    5.611m ± 16%  12.61% (p=0.015 n=6) BM_AddBF16/512/process_time                                                              110.95µ ±  1%    95.40µ ±  1%  14.02% (p=0.002 n=6) BM_AddBF16/8192/process_time                                                              1.050m ±  6%    1.022m ±  7%        ~ (p=0.589 n=6) BM_AddF32/1024/process_time                                                               343.5µ ±  5%    320.5µ ±  4%   6.70% (p=0.002 n=6) BM_AddF32/128/process_time                                                                21.95µ ±  1%    21.79µ ±  2%        ~ (p=0.240 n=6) BM_AddF32/16384/process_time                                                              6.284m ± 15%    5.409m ± 11%  13.91% (p=0.002 n=6) BM_AddF32/256/process_time                                                                67.42µ ±  1%    67.63µ ±  1%        ~ (p=0.485 n=6) BM_AddF32/32768/process_time                                                              15.24m ±  6%    14.96m ±  5%        ~ (p=0.394 n=6) BM_AddF32/512/process_time                                                                164.0µ ± 14%    146.4µ ±  1%        ~ (p=0.394 n=6) BM_AddF32/8192/process_time                                                               1.967m ± 19%    1.792m ± 24%   8.90% (p=0.041 n=6) BM_ConvertF32ToBF16/1024/process_time                                                     154.1µ ±  5%    185.0µ ±  1%  +20.04% (p=0.002 n=6) BM_ConvertF32ToBF16/128/process_time                                                      5.918µ ±  1%    5.861µ ±  1%        ~ (p=0.093 n=6) BM_ConvertF32ToBF16/16384/process_time                                                    1.901m ± 10%    1.973m ±  7%        ~ (p=0.589 n=6) BM_ConvertF32ToBF16/256/process_time                                                      25.28µ ±  1%    30.70µ ±  1%  +21.43% (p=0.002 n=6) BM_ConvertF32ToBF16/32768/process_time                                                    5.758m ± 45%    6.767m ± 18%        ~ (p=0.310 n=6) BM_ConvertF32ToBF16/512/process_time                                                      73.36µ ±  1%    91.98µ ±  1%  +25.38% (p=0.002 n=6) BM_ConvertF32ToBF16/8192/process_time                                                     977.4µ ± 10%   1046.1µ ±  9%        ~ (p=0.394 n=6) BM_BcastFusionF32/1024/process_time                                                       279.5µ ±  3%    274.0µ ±  2%   1.96% (p=0.015 n=6) BM_BcastFusionF32/128/process_time                                                        20.29µ ±  2%    19.91µ ±  2%   1.90% (p=0.015 n=6) BM_BcastFusionF32/16384/process_time                                                      3.623m ±  7%    3.459m ±  8%        ~ (p=0.093 n=6) BM_BcastFusionF32/256/process_time                                                        52.64µ ±  3%    52.06µ ±  9%        ~ (p=0.394 n=6) BM_BcastFusionF32/512/process_time                                                        113.9µ ±  1%    117.3µ ±  1%   +2.92% (p=0.002 n=6) BM_BcastFusionF32/8192/process_time                                                       1.739m ±  9%    1.657m ±  6%        ~ (p=0.240 n=6) BM_ChainOfAddF32/1024/process_time                                                        72.65µ ±  1%    71.52µ ±  2%   1.56% (p=0.041 n=6) BM_ChainOfAddF32/128/process_time                                                         12.72µ ±  8%    12.43µ ±  1%        ~ (p=0.394 n=6) BM_ChainOfAddF32/256/process_time                                                         20.55µ ±  1%    20.06µ ±  0%   2.39% (p=0.002 n=6) BM_ChainOfAddF32/512/process_time                                                         37.05µ ±  2%    36.13µ ±  2%   2.50% (p=0.004 n=6) BM_ChainOfAddF32/64/process_time                                                          8.835µ ±  1%    8.864µ ±  1%        ~ (p=0.394 n=6) BM_DynamicUpdateSliceFusionF32/1024/process_time                                          26.06µ ±  2%    25.82µ ±  2%        ~ (p=0.310 n=6) BM_DynamicUpdateSliceFusionF32/128/process_time                                           2.891µ ±  0%    2.952µ ±  2%   +2.12% (p=0.002 n=6) BM_DynamicUpdateSliceFusionF32/16384/process_time                                         780.4µ ±  4%    774.4µ ±  7%        ~ (p=0.394 n=6) BM_DynamicUpdateSliceFusionF32/256/process_time                                           5.538µ ±  1%    5.766µ ±  3%   +4.11% (p=0.002 n=6) BM_DynamicUpdateSliceFusionF32/512/process_time                                           13.23µ ±  2%    13.42µ ±  2%   +1.45% (p=0.041 n=6) BM_DynamicUpdateSliceFusionF32/8192/process_time                                          431.9µ ±  4%    431.0µ ±  5%        ~ (p=0.818 n=6) BM_FusionF32/1024/process_time                                                            322.1µ ±  1%    319.2µ ±  1%        ~ (p=0.310 n=6) BM_FusionF32/128/process_time                                                             22.09µ ±  2%    21.97µ ±  1%        ~ (p=0.240 n=6) BM_FusionF32/16384/process_time                                                           5.473m ± 13%    6.237m ± 10%  +13.95% (p=0.015 n=6) BM_FusionF32/256/process_time                                                             67.53µ ±  2%    68.25µ ±  8%   +1.07% (p=0.009 n=6) BM_FusionF32/512/process_time                                                             146.9µ ±  7%    147.0µ ±  7%        ~ (p=0.818 n=6) BM_FusionF32/8192/process_time                                                            1.855m ± 10%    1.989m ± 16%   +7.25% (p=0.041 n=6) BM_FusionF32_2/160/process_time                                                           1.227µ ±  4%    1.283µ ±  1%   +4.55% (p=0.002 n=6) BM_FusionF32_2/240/process_time                                                           1.501µ ±  3%    1.560µ ±  1%   +3.90% (p=0.002 n=6) BM_FusionF32_2/40/process_time                                                            827.2n ±  2%    873.4n ±  1%   +5.60% (p=0.002 n=6) BM_FusionF32_2/80/process_time                                                            963.5n ±  2%   1009.0n ±  1%   +4.73% (p=0.002 n=6) BM_GatherS32/10/128/1/process_time                                                        564.5n ± 19%    541.5n ±  2%        ~ (p=0.394 n=6) BM_GatherS32/10/128/2/process_time                                                        466.5n ±  3%    549.0n ±  2%  +17.69% (p=0.002 n=6) BM_GatherS32/10/128/32/process_time                                                       639.6n ±  5%    718.4n ±  2%  +12.31% (p=0.002 n=6) BM_GatherS32/10/256/1/process_time                                                        469.1n ±  5%    552.1n ±  0%  +17.70% (p=0.002 n=6) BM_GatherS32/10/256/2/process_time                                                        477.7n ±  1%    562.1n ±  1%  +17.66% (p=0.002 n=6) BM_GatherS32/10/256/64/process_time                                                       1.219µ ±  1%    1.286µ ±  1%   +5.50% (p=0.002 n=6) BM_GatherS32/10/3/1/process_time                                                          558.5n ±  0%    454.7n ±  3%  18.58% (p=0.002 n=6) BM_GatherS32/10/3/2/process_time                                                          565.3n ±  1%    454.6n ±  1%  19.59% (p=0.002 n=6) BM_GatherS32/10/3/4/process_time                                                          566.3n ±  0%    462.2n ±  1%  18.38% (p=0.002 n=6) BM_GatherS32/10/32/1/process_time                                                         561.1n ±  1%    458.6n ±  1%  18.26% (p=0.002 n=6) BM_GatherS32/10/32/2/process_time                                                         561.6n ±  5%    457.5n ± 21%  18.53% (p=0.002 n=6) BM_GatherS32/10/32/8/process_time                                                         573.4n ±  9%    560.8n ±  2%   2.19% (p=0.004 n=6) BM_GatherS32/10/512/1/process_time                                                        489.2n ±  3%    576.0n ±  2%  +17.74% (p=0.002 n=6) BM_GatherS32/10/512/128/process_time                                                     18.982µ ±  2%    4.127µ ±  3%  78.26% (p=0.002 n=6) BM_GatherS32/10/512/2/process_time                                                        506.1n ±  2%    587.1n ±  2%  +15.99% (p=0.002 n=6) BM_GatherS32/10/64/1/process_time                                                         563.9n ±  4%    534.6n ±  1%   5.20% (p=0.002 n=6) BM_GatherS32/10/64/16/process_time                                                        603.1n ±  2%    582.6n ±  1%   3.41% (p=0.002 n=6) BM_GatherS32/10/64/2/process_time                                                         567.9n ±  1%    548.6n ±  1%   3.40% (p=0.002 n=6) BM_GatherS32/100/128/1/process_time                                                       460.2n ±  0%    550.9n ±  1%  +19.71% (p=0.002 n=6) BM_GatherS32/100/128/2/process_time                                                       469.1n ±  3%    562.0n ±  1%  +19.80% (p=0.002 n=6) BM_GatherS32/100/128/32/process_time                                                      768.6n ±  1%    849.3n ±  1%  +10.51% (p=0.002 n=6) BM_GatherS32/100/256/1/process_time                                                       468.1n ±  1%    561.8n ±  1%  +20.02% (p=0.002 n=6) BM_GatherS32/100/256/2/process_time                                                       479.0n ±  1%    567.1n ±  2%  +18.38% (p=0.002 n=6) BM_GatherS32/100/256/64/process_time                                                      1.604µ ±  0%    1.674µ ±  0%   +4.33% (p=0.002 n=6) BM_GatherS32/100/3/1/process_time                                                         452.1n ±  2%    533.6n ±  1%  +18.02% (p=0.002 n=6) BM_GatherS32/100/3/2/process_time                                                         455.0n ±  1%    556.9n ±  3%  +22.40% (p=0.002 n=6) BM_GatherS32/100/3/4/process_time                                                         463.5n ±  4%    537.0n ±  1%  +15.85% (p=0.002 n=6) BM_GatherS32/100/32/1/process_time                                                        460.3n ±  1%    535.5n ±  3%  +16.35% (p=0.002 n=6) BM_GatherS32/100/32/2/process_time                                                        461.6n ±  1%    549.1n ±  1%  +18.96% (p=0.002 n=6) BM_GatherS32/100/32/8/process_time                                                        467.6n ±  0%    566.6n ±  1%  +21.15% (p=0.002 n=6) BM_GatherS32/100/512/1/process_time                                                       479.8n ±  4%    567.8n ±  2%  +18.35% (p=0.002 n=6) BM_GatherS32/100/512/128/process_time                                                    20.158µ ±  3%    5.464µ ±  3%  72.89% (p=0.002 n=6) BM_GatherS32/100/512/2/process_time                                                       507.9n ±  3%    585.8n ±  1%  +15.34% (p=0.002 n=6) BM_GatherS32/100/64/1/process_time                                                        458.4n ±  4%    544.9n ±  3%  +18.87% (p=0.002 n=6) BM_GatherS32/100/64/16/process_time                                                       496.1n ±  2%    594.7n ±  3%  +19.88% (p=0.002 n=6) BM_GatherS32/100/64/2/process_time                                                        461.6n ±  1%    554.9n ±  1%  +20.21% (p=0.002 n=6) BM_GatherS32/3/128/1/process_time                                                         567.6n ±  1%    463.7n ±  5%  18.32% (p=0.002 n=6) BM_GatherS32/3/128/2/process_time                                                         569.7n ±  2%    470.7n ±  1%  17.38% (p=0.002 n=6) BM_GatherS32/3/128/32/process_time                                                        722.9n ±  0%    625.5n ±  2%  13.47% (p=0.002 n=6) BM_GatherS32/3/256/1/process_time                                                         573.1n ±  2%    473.1n ±  2%  17.44% (p=0.002 n=6) BM_GatherS32/3/256/2/process_time                                                         582.2n ±  1%    480.9n ±  2%  17.41% (p=0.002 n=6) BM_GatherS32/3/256/64/process_time                                                        1.202µ ±  1%    1.107µ ±  1%   7.91% (p=0.002 n=6) BM_GatherS32/3/3/1/process_time                                                           566.0n ±  1%    460.1n ± 16%  18.70% (p=0.002 n=6) BM_GatherS32/3/3/2/process_time                                                           564.1n ±  0%    456.7n ±  3%  19.05% (p=0.002 n=6) BM_GatherS32/3/3/4/process_time                                                           567.6n ±  3%    461.9n ±  0%  18.63% (p=0.002 n=6) BM_GatherS32/3/32/1/process_time                                                          563.9n ±  1%    459.4n ±  1%  18.53% (p=0.002 n=6) BM_GatherS32/3/32/2/process_time                                                          562.9n ±  0%    459.0n ±  5%  18.46% (p=0.002 n=6) BM_GatherS32/3/32/8/process_time                                                          572.5n ±  1%    469.5n ±  1%  18.01% (p=0.002 n=6) BM_GatherS32/3/512/1/process_time                                                         585.8n ±  1%    482.0n ±  1%  17.73% (p=0.002 n=6) BM_GatherS32/3/512/128/process_time                                                      18.549µ ±  3%    3.101µ ±  1%  83.28% (p=0.002 n=6) BM_GatherS32/3/512/2/process_time                                                         612.2n ±  7%    505.5n ±  1%  17.42% (p=0.002 n=6) BM_GatherS32/3/64/1/process_time                                                          562.2n ±  3%    457.4n ±  1%  18.63% (p=0.002 n=6) BM_GatherS32/3/64/16/process_time                                                         604.1n ±  4%    494.6n ±  0%  18.13% (p=0.002 n=6) BM_GatherS32/3/64/2/process_time                                                          567.1n ±  1%    463.8n ±  1%  18.22% (p=0.002 n=6) BM_Optimizer0/1024/process_time                                                           5.125m ± 43%    5.046m ±  1%        ~ (p=0.132 n=6) BM_Optimizer0/128/process_time                                                            468.4µ ±  7%    449.4µ ± 12%   4.07% (p=0.026 n=6) BM_Optimizer0/16384/process_time                                                          65.03m ±  5%    62.68m ±  7%        ~ (p=0.485 n=6) BM_Optimizer0/256/process_time                                                            1.219m ±  5%    1.208m ±  7%        ~ (p=0.485 n=6) BM_Optimizer0/512/process_time                                                            3.541m ± 32%    2.392m ±  3%  32.45% (p=0.015 n=6) BM_Optimizer0/8192/process_time                                                           33.71m ±  2%    32.97m ±  5%        ~ (p=0.132 n=6) BM_PadF32/1024/process_time                                                               28.24m ±  9%    28.63m ±  6%        ~ (p=0.485 n=6) BM_PadF32/128/process_time                                                                339.8µ ±  1%    341.0µ ±  0%        ~ (p=0.310 n=6) BM_PadF32/256/process_time                                                                1.192m ±  4%    1.195m ±  3%        ~ (p=0.589 n=6) BM_PadF32/4096/process_time                                                               422.4m ±  1%    423.9m ±  0%        ~ (p=0.310 n=6) BM_PadF32/512/process_time                                                                4.339m ±  6%    4.170m ±  7%        ~ (p=0.065 n=6) BM_ReduceAddBF16/1024/process_time                                                        3.435m ±  2%    3.401m ±  9%        ~ (p=0.180 n=6) BM_ReduceAddBF16/128/process_time                                                         239.5µ ±  0%    239.0µ ±  0%   0.20% (p=0.002 n=6) BM_ReduceAddBF16/16384/process_time                                                       51.15m ±  4%    50.94m ±  2%        ~ (p=0.818 n=6) BM_ReduceAddBF16/256/process_time                                                         914.1µ ± 11%    777.5µ ±  8%  14.94% (p=0.004 n=6) BM_ReduceAddBF16/512/process_time                                                         1.704m ±  6%    1.680m ±  1%        ~ (p=0.093 n=6) BM_ReduceAddBF16/8192/process_time                                                        26.21m ±  3%    25.97m ±  7%        ~ (p=0.699 n=6) BM_ReduceAddF32/1024/process_time                                                         488.1µ ±  1%    487.1µ ±  0%        ~ (p=0.093 n=6) BM_ReduceAddF32/128/process_time                                                          23.36µ ±  0%    23.42µ ±  0%   +0.26% (p=0.026 n=6) BM_ReduceAddF32/16384/process_time                                                        6.756m ±  1%    6.672m ±  0%   1.23% (p=0.002 n=6) BM_ReduceAddF32/256/process_time                                                          62.44µ ±  1%    62.76µ ±  5%        ~ (p=0.132 n=6) BM_ReduceAddF32/512/process_time                                                          246.5µ ±  3%    246.9µ ±  0%        ~ (p=0.310 n=6) BM_ReduceAddF32/8192/process_time                                                         3.479m ±  0%    3.462m ±  0%   0.47% (p=0.004 n=6) BM_ScatterS32_R1/262144/262144/process_time                                               592.3µ ±  1%    590.2µ ±  1%        ~ (p=0.240 n=6) BM_ScatterS32_R2/512/512/process_time                                                     77.23µ ±  2%    77.87µ ±  2%        ~ (p=0.394 n=6) BM_ScatterS32_R3/64/64/process_time                                                       54.41µ ±  0%    54.79µ ±  2%        ~ (p=0.180 n=6) BM_SimpleScatterReduceF32_R3/d0:1/d1:64/d2:8/num_slices:1/process_time                    727.3n ±  1%    720.6n ±  2%   0.92% (p=0.026 n=6) BM_SimpleScatterReduceF32_R3/d0:50/d1:64/d2:8/num_slices:10/process_time                  108.0µ ±  1%    107.3µ ±  3%        ~ (p=0.394 n=6) BM_SimpleScatterReduceF32_R3/d0:500/d1:64/d2:8/num_slices:100/process_time                10.38m ±  1%    10.33m ±  1%        ~ (p=0.132 n=6) BM_SelectAndScatterF32/128/process_time                                                   35.65µ ±  1%    35.72µ ±  4%        ~ (p=0.699 n=6) BM_SelectAndScatterF32/256/process_time                                                   118.7µ ±  1%    117.9µ ±  3%        ~ (p=0.310 n=6) BM_SelectAndScatterF32/512/process_time                                                   1.657m ±  5%    1.640m ±  6%        ~ (p=0.240 n=6) BM_TanhF16/1024/process_time                                                              684.4n ±  1%    716.9n ±  3%   +4.75% (p=0.002 n=6) BM_TanhF16/128/process_time                                                               435.9n ±  2%    466.8n ±  7%   +7.07% (p=0.002 n=6) BM_TanhF16/256/process_time                                                               474.4n ±  2%    504.8n ±  1%   +6.41% (p=0.002 n=6) BM_TanhF16/4096/process_time                                                              1.521µ ±  0%    1.545µ ±  1%   +1.56% (p=0.002 n=6) BM_TanhF16/512/process_time                                                               547.4n ±  1%    578.6n ±  1%   +5.70% (p=0.002 n=6) BM_TanhF32/1024/process_time                                                              688.6n ±  0%    716.3n ±  1%   +4.02% (p=0.002 n=6) BM_TanhF32/128/process_time                                                               446.9n ±  3%    466.9n ±  2%   +4.48% (p=0.002 n=6) BM_TanhF32/256/process_time                                                               476.8n ±  1%    507.2n ±  1%   +6.36% (p=0.002 n=6) BM_TanhF32/4096/process_time                                                              1.580µ ±  0%    1.616µ ±  1%   +2.31% (p=0.002 n=6) BM_TanhF32/512/process_time                                                               556.7n ±  1%    584.9n ±  1%   +5.06% (p=0.002 n=6) BM_TanhF64/1024/process_time                                                              12.60µ ±  2%    12.61µ ±  0%        ~ (p=0.310 n=6) BM_TanhF64/128/process_time                                                               1.944µ ±  0%    1.973µ ±  0%   +1.49% (p=0.002 n=6) BM_TanhF64/256/process_time                                                               3.463µ ±  1%    3.493µ ±  1%   +0.86% (p=0.002 n=6) BM_TanhF64/4096/process_time                                                              49.11µ ±  0%    49.10µ ±  0%        ~ (p=0.394 n=6) BM_TanhF64/512/process_time                                                               6.510µ ±  0%    6.527µ ±  0%   +0.25% (p=0.002 n=6) geomean                                                                                   94.44µ          92.38µ         2.19% ``` ",2025-03-13T02:17:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89123
copybara-service[bot],[xla:cpu] add dylib_index test case with fusion emitters,[xla:cpu] add dylib_index test case with fusion emitters Now that the scatter emitter is enabled we can test that there is a reserved dylib_index for it.,2025-03-13T02:15:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89122
copybara-service[bot],[xla:cpu] ir_emitter2: allow per-kernel compilation flag overrides (JIT only),"[xla:cpu] ir_emitter2: allow perkernel compilation flag overrides (JIT only) This is needed by the upcoming fusion emitters. Without this, the scatter emitter is very slow (~10x slowdown). Note that this is for JIT compilation only. Support for thunks in AOT is currently in progress, so we will wait until that fully lands to port this feature to it. Testing this is tricky because very little of it is exposed to users, and there won't be any users until the scatter fusion emitter lands. (Adding a fake user via tests is too complicated.) Until that lands, we can still do some testing by exposing xla_dylib_index as an LLVM module flag that then we can inspect via tests. That is what I'm doing in this change.",2025-03-13T01:46:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89121
copybara-service[bot],Add single op f32 models to integration tests for most of the target ops for qnn,Add single op f32 models to integration tests for most of the target ops for qnn Reverts f3cf4a3a25e94daade0cd6bf0cdf8a3bc60084bc,2025-03-13T01:11:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89120
copybara-service[bot],"Nit, ifrt_proxy: Remove declaration of unused `HandleReshardRequest()` function.","Nit, ifrt_proxy: Remove declaration of unused `HandleReshardRequest()` function.",2025-03-13T00:45:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89119
sjh0849,API Doc Error for tf.train.ClusterSpec API," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Mac  Mobile device N/A  Python version 3.9  Bazel version N/A  GCC/compiler version N/A  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? The test conversion expects the resulting ClusterDef to have its first job named ""worker"" (matching the insertion order of the original dictionary). However, the test reveals that the first job ends up being ""ps"", which deviates from the API’s expected behavior (as evidenced in the provided examples API doc).  Standalone code to reproduce the issue ```shell import unittest import tensorflow as tf class TestClusterSpec(unittest.TestCase):     def test_cluster_def_conversion(self):          Test conversion to ClusterDef         cluster_dict = {             ""worker"": [""worker0.example.com:2222"", ""worker1.example.com:2222""],             ""ps"": [""ps0.example.com:2222""]         }         cluster_spec = tf.train.ClusterSpec(cluster_dict)         cluster_def = cluster_spec.as_cluster_def()         self.assertEqual(len(cluster_def.job), 2)         self.assertEqual(cluster_def.job[0].name, ""worker"")         self.assertEqual(cluster_def.job[1].name, ""ps"") if __name__ == '__main__':     unittest.main() ```  Relevant log output ```shell ====================================================================== FAIL: test_cluster_def_conversion (__main__.TestClusterSpec)  Traceback (most recent call last):   File ""/Users/sjh/test.py"", line 16, in test_cluster_def_conversion     self.assertEqual(cluster_def.job[0].name, ""worker"") AssertionError: 'ps' != 'worker'  ps + worker  Ran 1 test in 0.000s FAILED (failures=1) ```",2025-03-13T00:34:44Z,type:bug comp:apis comp:ops TF 2.18,open,0,4,https://github.com/tensorflow/tensorflow/issues/89118,I was able to reproduce the same issue on Colab using TensorFlow 2.18 and the nightly version. Please find the gist attached for your reference. Thank you!, I have added this colablink in which I have provided two solutions or approach for now to handle this error please go through it.,"  Let me know If there is anything else, I can do to work on this issue.",Thank you for getting back to me with a solution!
copybara-service[bot],[IFRT] Introduce `Client::MakeArraysFromHostBufferShards()`,"[IFRT] Introduce `Client::MakeArraysFromHostBufferShards()` `Client::MakeArraysFromHostBufferShards()` is a new IFRT Client API that lets creating multidevice arrays in bulk without breaking down the process to many IFRT API calls. This API makes it easy to skip the intermediate step of creating singledevice arrays and allows using the same host buffer for replicated shards within an array, which reduces redundant resource use regardless of the array sharding. Similar to `Client::MakeArrayFromHostBuffer()`, it allows using a different host buffer layout from the IFRT Array layout, which the runtime is responsible for relayouting the buffer content as needed. To faciliate transition, an implementationagnostic static method `ifrt::Client::MakeArraysFromHostBufferShards()` is provided.",2025-03-13T00:23:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89117
copybara-service[bot],[XLA:Python] Remove references to xla_client.bfloat16.,"[XLA:Python] Remove references to xla_client.bfloat16. This is an alias to ml_dtypes.bfloat16 and has been for a long time. Use that name (or jax.numpy.bfloat16, which is another alias) instead.",2025-03-12T23:47:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89116
copybara-service[bot],Pull the genrule for mlir->tflite into a macro so it can be reused.,Pull the genrule for mlir>tflite into a macro so it can be reused.,2025-03-12T23:40:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89115
copybara-service[bot],Check and log target existence for excluded targets,Check and log target existence for excluded targets,2025-03-12T23:39:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89114
copybara-service[bot],Extending op coverage for TFL,Extending op coverage for TFL,2025-03-12T23:34:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89113
copybara-service[bot],Fix grammar mistake and rewrite paragraph,Fix grammar mistake and rewrite paragraph,2025-03-12T23:07:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89112
copybara-service[bot],[XLA] Run gpu_hlo_backend.hlo in the nightly job to be consistent with presubmit and postsubmit workflows,[XLA] Run gpu_hlo_backend.hlo in the nightly job to be consistent with presubmit and postsubmit workflows,2025-03-12T23:02:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89111
adamvvu,Gradients are zero when clipping values in function definition," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18; 2.10  Custom code Yes  Python version 3.10  Current behavior? I encountered a strange issue where gradients are zero when clippingrelated TensorFlow Ops are defined in functions. For context, I was implementing a numerically stable version of sigmoid where the inputs are clipped to `[10, 10]`. However this bug can be reproduced with other functions.  In the provided sample code, I define a truncated version of f(x) = x^2 where the function `truncate_domain` is used to clip the input tensors. However gradients only work as expected when the clipping occurs inside `GradientTape` and not when it's part of the function definition. Reproduced on TensorFlow 2.10 with GPU, and latest 2.18 with CPU  Standalone code to reproduce the issue ```shell import tensorflow as tf def square1(x):     return x**2 def truncate_domain(x, lb, ub):      Truncate for numerical stability     x = tf.clip_by_value(x, lb, ub)     x = tf.math.maximum(x, lb)     x = tf.math.minimum(x, ub)     return x def square2(x):     x = truncate_domain(x, 10., 10.)     return x**2 def square3(x):     x = truncate_domain(x, 10., 10.)     return square1(x)  Note: All 3 square functions are exactly mathematically equivalent when x in [10, 10].  Checking gradients x = tf.constant(15., dtype=tf.float32) with tf.GradientTape() as tape:     tape.watch(x)     x = truncate_domain(x, 10., 10.)     l = square1(x) grad1 = tape.gradient(l, x) print(l) print(grad1)  >> tf.Tensor(100.0, shape=(), dtype=float32)  >> tf.Tensor(20.0, shape=(), dtype=float32) x = tf.constant(15., dtype=tf.float32) with tf.GradientTape() as tape:     tape.watch(x)     l = square2(x) grad2 = tape.gradient(l, x) print(l) print(grad2)  >> tf.Tensor(100.0, shape=(), dtype=float32)  >> tf.Tensor(0.0, shape=(), dtype=float32) x = tf.constant(15., dtype=tf.float32) with tf.GradientTape() as tape:     tape.watch(x)     l = square3(x) grad3 = tape.gradient(l, x) print(l) print(grad3)  >> tf.Tensor(100.0, shape=(), dtype=float32)  >> tf.Tensor(0.0, shape=(), dtype=float32)  Note: Gradients for square2 and square3 are 0, even though they are mathematically and (Python) syntactically equivalent to square1's expected behavior ```  Relevant log output ```shell ```",2025-03-12T22:33:15Z,stat:awaiting tensorflower type:bug comp:ops comp:autograph TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/89110,I was able to reproduce the same issue on Colab using TensorFlow 2.18 and the nightly version. Please find the gist attached for your reference. Thank you!
copybara-service[bot],Support raw buffers on GPU.,Support raw buffers on GPU. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22563 from Inteltensorflow:akhil/conv_scratch b9d29252882118391bdc91f0c4f5ccc5d6c96c5c,2025-03-12T21:56:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89109
copybara-service[bot],Fuse Sum -> Mul into Mean pattern,Fuse Sum > Mul into Mean pattern,2025-03-12T21:47:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89108
sjh0849,Error in tf.convert_to_tensor API," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Ubuntu  Mobile device N/A  Python version 3.9  Bazel version N/A  GCC/compiler version N/A  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? The two values (one from numpy and one from tf) have the same value, but the assertion equality is not working as expected.  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np import unittest class TestConvertToTensor(unittest.TestCase):     def test_convert_float(self):          Test conversion of a float to a tensor         float_data = 3.14         tensor = tf.convert_to_tensor(float_data)         self.assertTrue(isinstance(tensor, tf.Tensor))         self.assertEqual(tensor.dtype, tf.float32)         self.assertEqual(tensor.numpy(), float_data) if __name__ == '__main__':     unittest.main() ```  Relevant log output ```shell Traceback (most recent call last):   File ""/sjh/tf.convert_to_tensor.py"", line 45, in test_convert_float     self.assertEqual(tensor.numpy(), float_data) AssertionError: 3.14 != 3.14 ```",2025-03-12T21:40:29Z,stat:awaiting response type:bug comp:apis TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89107,"Hi  , Apologies for the delay, and thanks for bringing this up! I tried running your code on Colab using TensorFlow versions 2.18.0, 2.19.0, and the nightly build. While I did not encounter the exact issue you mentioned, I did face a different problem related to unittest. After adjusting the unittest setup, the code worked fine for me. I have attached a gist for your reference, it might help resolve the issue on your end. Let me know if you need further assistance! Thank you!","Hi  , Thanks for the assistance! It resolves the issues.",Are you satisfied with the resolution of your issue? Yes No
sjh0849,Doc Error: tf.boolean_mask API," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Ubuntu  Mobile device _No response_  Python version 3.9  Bazel version N/A  GCC/compiler version N/A  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? The API documentation states that the mask parameter must be a Boolean tensor, but the implementation does not fully enforce this. In the test test_mask_with_different_dtype, an int32 mask is passed, and while a TypeError is expected, no error is raised. The expectation is that the documentation should mention that nonBoolean can be handled in the API implementation.  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np import unittest class TestBooleanMask(unittest.TestCase):     def test_mask_with_different_dtype(self):          Test with a mask of different dtype (int)         tensor = tf.constant([1, 2, 3, 4, 5])         mask = tf.constant([1, 0, 1, 0, 1], dtype=tf.int32)         with self.assertRaises(TypeError):             tf.boolean_mask(tensor, mask) if __name__ == '__main__':     unittest.main() ```  Relevant log output ```shell ```",2025-03-12T21:23:42Z,type:docs-bug type:bug comp:apis comp:ops awaiting PR merge TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89106,"Hi  , Apologies for the delay, and thanks for raising your issue here. I have submitted a PR addressing your issue. Once it gets merged, the issue should be resolved. Thank you!","Hi  , Thanks for getting back to me! Awesome!"
shinjh0849,Doc Error: tf.boolean_mask API," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Ubuntu   Mobile device _No response_  Python version 3.9  Bazel version N/A  GCC/compiler version N/A  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? The API documentation states that the mask parameter must be a Boolean tensor, but the implementation does not fully enforce this. In the test test_mask_with_different_dtype, an int32 mask is passed, and while a TypeError is expected, no error is raised. The expectation is that the documentation should mention that nonBoolean can be handled in the API implementation.  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np import unittest class TestBooleanMask(unittest.TestCase):     def test_mask_with_different_dtype(self):          Test with a mask of different dtype (int)         tensor = tf.constant([1, 2, 3, 4, 5])         mask = tf.constant([1, 0, 1, 0, 1], dtype=tf.int32)         with self.assertRaises(TypeError):             tf.boolean_mask(tensor, mask) if __name__ == '__main__':     unittest.main() ```  Relevant log output ```shell ```",2025-03-12T21:14:51Z,type:docs-bug type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/89105,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Legalize All Sin to TFL,Legalize All Sin to TFL,2025-03-12T21:06:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89104
copybara-service[bot],Add NVIDIA A4 B200 224 vCPU runner to OpenXLA nightly workflows,Add NVIDIA A4 B200 224 vCPU runner to OpenXLA nightly workflows,2025-03-12T21:02:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89103
copybara-service[bot],Add GPU and MTK mobile device tests.,Add GPU and MTK mobile device tests.,2025-03-12T21:02:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89102
copybara-service[bot],[XLA:GPU] Generate simpler control dependencies in collective permute decomposer,[XLA:GPU] Generate simpler control dependencies in collective permute decomposer,2025-03-12T20:56:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89101
copybara-service[bot],Eliminate std::is_pod.,"Eliminate std::is_pod. std::is_pod is deprecated, use more specific alternatives",2025-03-12T20:39:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89100
copybara-service[bot],[easy] [XLA] Rewrite reshape(bitcast(op)) to reshape(op),[easy] [XLA] Rewrite reshape(bitcast(op)) to reshape(op),2025-03-12T20:33:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89099
copybara-service[bot],"[XLA:GPU] Fix control dependency generation for async collectives, send/recv","[XLA:GPU] Fix control dependency generation for async collectives, send/recv",2025-03-12T19:54:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89098
copybara-service[bot],PR #23386: [debug_options] Printing all the fields in debug options,PR CC(InvalidArgumentError: Invalid name: An op that loads optimization parameters into HBM for embedding. (ConfigureTPUEmbeddingHost)): [debug_options] Printing all the fields in debug options Imported from GitHub PR https://github.com/openxla/xla/pull/23386 This patch prints all the values of the debug options while dumping it under VLOG. This is specifically required for boolean fields which have the default value set to true in `xla/debug_options_flags.cc`. These values will not be printed in `DebugString()` if `XLA_FLAGS` overrides it to `false`. Copybara import of the project:  4746d058005f9fff6ebbcc37818a288979d05165 by Shraiysh Vaishay : [debug_options] Printing all the fields in debug options This patch prints all the values of the debug options while dumping it under VLOG. This is specifically required for boolean fields which have the default value set to true in `xla/debug_options_flags.cc`. These values will not be printed in `DebugString()` if `XLA_FLAGS` overrides it to `false`. Merging this change closes CC(InvalidArgumentError: Invalid name: An op that loads optimization parameters into HBM for embedding. (ConfigureTPUEmbeddingHost)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23386 from shraiysh:print_debug_options_full 4746d058005f9fff6ebbcc37818a288979d05165,2025-03-12T19:08:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89097
copybara-service[bot],Makes `Layout::CreateFromProto()` not crash on invalid input.,"Makes `Layout::CreateFromProto()` not crash on invalid input. This function may be called on an invalid proto (e.g. intentionally in `shape_util_fuzzer`, or when loading an incorrect proto). In such cases, instead of crashing, we prefer to return an invalid `Layout` object, which can be caught in validation later.",2025-03-12T18:53:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89096
copybara-service[bot],Migrate AddLegalizeTFToStablehloPasses to tensorflow/compiler/mlir/stablehlo,Migrate AddLegalizeTFToStablehloPasses to tensorflow/compiler/mlir/stablehlo,2025-03-12T18:25:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89095
copybara-service[bot],Google-internal comment change.,Googleinternal comment change.,2025-03-12T18:22:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89094
copybara-service[bot],Add support for WebP in DecodeImage,"Add support for WebP in DecodeImage WebP offers both lossless and lossy compression that can be superior to JPEG or PNG. Requests for WebP support directly in TF have come up before (e.g., https://github.com/tensorflow/tensorflow/issues/18250) but has been kept out of tree in the TFIO (https://github.com/tensorflow/io/pull/43). Unfortunately, the TFIO implementation hardcodes channels_ = 4, and doesn't have the same ShapeInferenceFn style of support that DecodeImage does. Let's just bring native WebP decode support into TF for tf.data directly. Animation is supported, and similar to decode_gif, decode_webp produces a tensor of [num_frames, height, width, channels], even for a still frame (num_frames = 1). To get 3D tensors instead, use decode_image with expand_animations = False. Note: libwebp (and maybe the WebP format?) only supports 4channel animation, so channels will always be 4 for animations.",2025-03-12T18:09:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89093
copybara-service[bot],Migrate :legalize_tf used by AddLegalizeTFToStablehloPasses to tensorflow/compiler/mlir/stablehlo,Migrate :legalize_tf used by AddLegalizeTFToStablehloPasses to tensorflow/compiler/mlir/stablehlo,2025-03-12T18:07:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89092
copybara-service[bot],Add async tests to CC Compiled Model.,Add async tests to CC Compiled Model.,2025-03-12T17:34:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89091
copybara-service[bot],Split the environment options map out from the `LiteRtEnvironmentT`.,Split the environment options map out from the `LiteRtEnvironmentT`.  Add a C function to get the environment option map from the environment.  Add a C function to get environment options values from the option map.  Add a header only target for `c/litert_environment_options` to avoid a cycle   between `c/litert_environment_options` and `core/environment_options`. litert,2025-03-12T16:53:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89090
copybara-service[bot],[XLA:CPU] Rename CallKernelEmitter to ComputationKernelEmitter,[XLA:CPU] Rename CallKernelEmitter to ComputationKernelEmitter,2025-03-12T16:23:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89089
chuntl,Qualcomm AI Engine Direct - Enable QNN profiler in LiteRt,Summary:  Use kProfilingOff profiling_level as default for now  Implement QNN profiler with simple format  Can be tested with dispatch_api_qualcomm_test  Need to add into dispatch option after header interface merged,2025-03-12T16:16:12Z,size:M,open,0,3,https://github.com/tensorflow/tensorflow/issues/89088,"**Description** For now, the profiling is disabled by setting profiling_level in the code, you could simply change the profiling_level to kBasicProfiling (equal to 1) or kDetailedProfiling (equal to 2) in the tensorflow/lite/experimental/litert/vendors/qualcomm/dispatch/litert_dispatch_invocation_context.(TensorBoard logdir path, if relative, is relative to $HOME) **TODO**  Add profiling_level as dispatch option after header interface merged  Add viewer tools for profiling result to format **Sample Result** The sample of profiling result with DETAILED: !image The sample of profiling result with BASIC: !image",Can we have a test that ensures graph execute success when profile is on (basic/detailed)?,"> Can we have a test that ensures graph execute success when profile is on (basic/detailed)? Discussed internally, will add some tests based on the incoming option interface PR merged, thanks!"
copybara-service[bot],Bumped dlpack to v1.1,Bumped dlpack to v1.1,2025-03-12T16:06:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89087
copybara-service[bot],Use fingerprint options when converting the HloModule to a string.,"Use fingerprint options when converting the HloModule to a string. We use the string to compute a fingerprint, so we should also use the fingerprint options.",2025-03-12T15:23:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89086
copybara-service[bot],PR #23650: Fix EvaluateWithSubstitutions when the evaluated thing is substituted.,"PR CC(Exclude the log_dir from the metadata path): Fix EvaluateWithSubstitutions when the evaluated thing is substituted. Imported from GitHub PR https://github.com/openxla/xla/pull/23650 Currently, this results in an error, which is surprising. There seems no benefit to the current behavior, so let's just return the substitued value for this case. Copybara import of the project:  e14bec1faac9586b3eac65bc4244d09d7d805d44 by Johannes Reifferscheid : Fix EvaluateWithSubstitutions when the evaluated thing is substituted. Currently, this results in an error, which is surprising. There seems no benefit to the current behavior, so let's just return the substitued value for this case. Merging this change closes CC(Exclude the log_dir from the metadata path) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23650 from jreiffers:evaluator e14bec1faac9586b3eac65bc4244d09d7d805d44",2025-03-12T15:10:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89085
t-kalinowski,Update Docs to latest release," Issue type Documentation Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The website at https://www.tensorflow.org/api_docs/python currently hosts docs for TensorFlow version 2.16.1, released March 8, 2024 (over 1 year ago) The current release version of TensorFlow is 2.19.0. Will the website be updated for the current TensorFlow release? Or is it possible to build the docs locally with only opensource tools?  Standalone code to reproduce the issue ```shell Visit https://www.tensorflow.org/api_docs/python ```  Relevant log output ```shell ```",2025-03-12T14:52:21Z,stat:awaiting tensorflower type:feature type:docs-feature TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/89084,toplay this is step that's being skipped during latest releases...,Thank you very much for flagging!   Can you take a look and address?
novamcbo,from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version TENSORFLOW 2.19  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? :\Users\ray_v\PycharmProjects\pythonProject\.venv1\Scripts\python.exe C:\Users\ray_v\PycharmProjects\pythonProject\backend\ai\train_ml_model.py  Traceback (most recent call last):   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\Users\ray_v\PycharmProjects\pythonProject\backend\ai\train_ml_model.py"", line 10, in      import tensorflow as tf   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. Process finished with exit code 1  Standalone code to reproduce the issue ```shell :\Users\ray_v\PycharmProjects\pythonProject\.venv1\Scripts\python.exe C:\Users\ray_v\PycharmProjects\pythonProject\backend\ai\train_ml_model.py  Traceback (most recent call last):   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\Users\ray_v\PycharmProjects\pythonProject\backend\ai\train_ml_model.py"", line 10, in      import tensorflow as tf   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\ray_v\PycharmProjects\pythonProject\.venv1\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. Process finished with exit code 1 ```  Relevant log output ```shell ```",2025-03-12T14:38:51Z,type:bug TF 2.18,closed,1,4,https://github.com/tensorflow/tensorflow/issues/89083,"Hi **** , Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:  You need to install the MSVC 2019 redistributable  Your CPU does not support AVX2 instructions  Your CPU/Python is on 32 bits  There is a library that is in a different location/not installed on your system that cannot be loaded. Also kindly provide the environment details and the steps followed to install the tensorflow. CC(Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.) Also this is a duplicate of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) Thank you!",please update the MSVC 2019 redistributable，download links: https://download.visualstudio.microsoft.com/download/pr/285b28c73cf947fb9be801cf5323a8df/8F9FB1B3CFE6E5092CF1225ECD6659DAB7CE50B8BF935CB79BFEDE1F3C895240/VC_redist.x64.exe,Please search for duplicates before opening new issues ,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[xla] Cleanup warnings in rendezvous,[xla] Cleanup warnings in rendezvous,2025-03-12T13:51:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89082
copybara-service[bot],[XLA:GPU] Refactor communication type from interpolation code out.,[XLA:GPU] Refactor communication type from interpolation code out.,2025-03-12T13:36:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89081
copybara-service[bot],[XLA:GPU] additional tests for nest gemm fusions pass,"[XLA:GPU] additional tests for nest gemm fusions pass broadcasts and compare are not fully supported yet, adding tests to track that",2025-03-12T13:29:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89080
copybara-service[bot],[dlpack] `tf.Tensor` now implements `__dlpack__` and `__dlpack_device__`,[dlpack] `tf.Tensor` now implements `__dlpack__` and `__dlpack_device__` Note that this change only affects exporting `tf.Tensor`s via DLPack. The DLPack import machinery does not currently support the new DLPack protocol. See https://dmlc.github.io/dlpack/latest/python_spec.html and CC(AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '__dlpack__').,2025-03-12T13:10:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89079
copybara-service[bot],Do not print large constants when calling ToString() on them.,"Do not print large constants when calling ToString() on them. They could be really large, and we use ToString() in many places.",2025-03-12T12:54:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89078
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23506 from olupton:removecubhacks 6867fab8638d8f69d88d9a37ae141f72d0cd6278,2025-03-12T11:16:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89077
copybara-service[bot],Make GPU accelerator registration function return a LiteRtStatus.,Make GPU accelerator registration function return a LiteRtStatus.,2025-03-12T11:16:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89076
copybara-service[bot],[XLA:GPU] Account for batch dim in the flop count for matmul sweep generation.,"[XLA:GPU] Account for batch dim in the flop count for matmul sweep generation. Current flop count didn't include batch dim, which meant for batch matmuls, it was considering batch dim as a contracting dim, leading to incorrect flop count.",2025-03-12T10:52:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89075
copybara-service[bot],PR #23506: Do not reach inside cub:: namespace,"PR CC(After Installing Tensorflow when I TRy to verify I getting Follwing error can some body help on this): Do not reach inside cub:: namespace Imported from GitHub PR https://github.com/openxla/xla/pull/23506 https://github.com/NVIDIA/cccl/commit/a4508f6f8c28871db0763c68f61238a3d79470f7 will stop this code compiling, something similar happened before with https://github.com/openxla/xla/pull/16095. As far as I can see, this code is not actually used [anymore] in OSS. Copybara import of the project:  6867fab8638d8f69d88d9a37ae141f72d0cd6278 by Olli Lupton : Do not reach inside cub:: namespace. Merging this change closes CC(After Installing Tensorflow when I TRy to verify I getting Follwing error can some body help on this) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23506 from olupton:removecubhacks 6867fab8638d8f69d88d9a37ae141f72d0cd6278",2025-03-12T10:39:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89074
copybara-service[bot],Avoid running tests on GPU that don't need a GPU.,Avoid running tests on GPU that don't need a GPU.,2025-03-12T10:28:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89073
GaganaMD,ImportError: DLL load failed while importing _pywrap_tensorflow_internal on Windows (TensorFlow CPU)," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.9  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am facing an issue while trying to import TensorFlow on a CPU machine. The error traceback suggests that a DLL load failed while importing _pywrap_tensorflow_internal, which prevents TensorFlow from initializing properly. Traceback (most recent call last):   File ""C:\Users\gagan\Desktop\EGU_replication\venv\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\Users\gagan\Desktop\EGU_replication\IEEE_TNNLS_EGUNet\EGUNetpw.py"", line 5, in      import tensorflow as tf   File ""C:\Users\gagan\Desktop\EGU_replication\venv\lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport   File ""C:\Users\gagan\Desktop\EGU_replication\venv\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 88, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\gagan\Desktop\EGU_replication\venv\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 73, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for common causes and solutions.  Standalone code to reproduce the issue ```shell Please test this on your local windows machine https://colab.research.google.com/drive/1PPNyc5LP5ngeVadUCx6vBxmRQN5MhiVq?usp=sharing ```  Relevant log output ```shell ```",2025-03-12T10:23:53Z,type:bug,closed,0,4,https://github.com/tensorflow/tensorflow/issues/89072,TensorFlow version: 2.18.0,Please search for duplicates before opening a new issue,Are you satisfied with the resolution of your issue? Yes No,Update the MSVC like this download link ：https://download.visualstudio.microsoft.com/download/pr/285b28c73cf947fb9be801cf5323a8df/8F9FB1B3CFE6E5092CF1225ECD6659DAB7CE50B8BF935CB79BFEDE1F3C895240/VC_redist.x64.exe
copybara-service[bot],PR #23610: Replace dynamic-slice fusions with memcpy.,"PR CC(Causal Convolutions in Tensorflow): Replace dynamicslice fusions with memcpy. Imported from GitHub PR https://github.com/openxla/xla/pull/23610 Fusiondispatch pattern matches these fusions, after which fusions./thunk type. This also adds an integration test. Copybara import of the project:  ae81151c58e21c7290b2f38b33c56311aaa0a302 by Johannes Reifferscheid : Replace dynamicslice fusions with memcpy. Fusiondispatch pattern matches these fusions, after which fusions./thunk type. This also adds an integration test.  f36b10830e2b6c8f0f67f1e0bd6245e2572ceb5e by Johannes Reifferscheid : Add an integration test for noncontiguous memcpys. This was already handled in the matcher code, but we had no test to verify the behavior.  905854e1150945dc23793e9deca88cf57d2e9c62 by Johannes Reifferscheid : Address comment.  9e9fa203c9084400f4459f5754e0cd5ecf83687f by Johannes Reifferscheid : Fix gpu_compiler_test. I think the fusiondispatch test passed somewhat coincidentally before. This change makes overlapping ranges for pass order tests explicitly OK (instead of only allowing it if the overlap is exactly one pass).  db9540b7c209ea87b469f1809e74855428918e74 by Johannes Reifferscheid : Fix gpu_compiler_test_again. ?? Merging this change closes CC(Causal Convolutions in Tensorflow) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23610 from jreiffers:memcpyenable db9540b7c209ea87b469f1809e74855428918e74",2025-03-12T10:20:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89071
copybara-service[bot],Change `SharedLibrary::Load()` overloads for special handles.,"Change `SharedLibrary::Load()` overloads for special handles. Remove the flags from the function parameters, those are useless for these cases.",2025-03-12T10:05:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89070
copybara-service[bot],[XLA:GPU] Enable RaggedAllToAll one-shot kernel by default.,[XLA:GPU] Enable RaggedAllToAll oneshot kernel by default. Oneshot kernel is the most performant and stable implementation of RaggedAllToAll that we have right now. NCCL version is currently unstable and sometimes causes deadlocks. Enabling the flag will make singlehost usage of RaggedAllToAll stable by default. Multihost case is currently not well supported.,2025-03-12T09:59:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89069
copybara-service[bot],Use the NVML from the CUDA.,Use the NVML from the CUDA.,2025-03-12T09:54:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89068
copybara-service[bot],PR #23613: [ROCm] Pass correct warp size to Triton pipeline,PR CC(ERROR: Config value opt is not defined in any .rc file): [ROCm] Pass correct warp size to Triton pipeline Imported from GitHub PR https://github.com/openxla/xla/pull/23613 Copybara import of the project:  dc43a7518d690038398ae3cf301de477d1ca715f by Dragan Mladjenovic : [ROCm] Pass correct warp size to Triton pipeline Merging this change closes CC(ERROR: Config value opt is not defined in any .rc file) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23613 from ROCm:triton_wrap_size dc43a7518d690038398ae3cf301de477d1ca715f,2025-03-12T09:48:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89067
copybara-service[bot],Remove deprecated `ReadModuleFromString` and `CreateModuleFromString`.,Remove deprecated `ReadModuleFromString` and `CreateModuleFromString`.,2025-03-12T09:38:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89066
AntonBondarenkoArm,Build error when building benchmark_model from TensorFlow Lite using CMake," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version master (tf 2.20)  Custom code No  OS platform and distribution MacOS 15.3.1  Mobile device _No response_  Python version _No response_  Bazel version CMake 3.31.6  GCC/compiler version Apple clang version 16.0.0 (clang1600.0.26.6)  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Build error when building benchmark_model from TensorFlow Lite using CMake Failing on source code  commit d4aca8a0919cc01cae8855de7d83a8e7e9848cf7 (HEAD, origin/master)  Standalone code to reproduce the issue ```shell cmake S tensorflow/lite B buildxxx DXNNPACK_ENABLE_ARM_I8MM=TRUE DTFLITE_ENABLE_XNNPACK=ON DTFLITE_ENABLE_GPU=OFF DTFLITE_ENABLE_METAL=OFF cmake build buildxxx j1 t benchmark_model ```  Relevant log output ```shell [ 82%] Building CXX object CMakeFiles/tensorflowlite.dir/core/c/c_api.cc.o /Library/Developer/CommandLineTools/usr/bin/c++ DCPUINFO_SUPPORTED_PLATFORM=1 DEIGEN_MPL2_ONLY DNOMINMAX=1 DPTHREADPOOL_NO_DEPRECATED_API=1 DXNN_LOG_LEVEL=0 Ithird_party/xla/third_party/tsl Ithird_party/xla Ibuildxxx/pthreadpoolsource/include Ibuildxxx/FP16source/include Ibuildxxx/xnnpack/include Ibuildxxx/cpuinfo I/Users/antbon01/work/ml/kai/third_party/tensorflow Ibuildxxx/eigen Ibuildxxx/abseilcpp Ibuildxxx/farmhash/src Ibuildxxx/flatbuffers/include Ibuildxxx/gemmlowp Ibuildxxx/ml_dtypes Ibuildxxx/ml_dtypes/ml_dtypes Ibuildxxx/ruy Ibuildxxx/cpuinfo/include O3 DNDEBUG std=gnu++20 arch arm64 isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX15.2.sdk fPIC DEIGEN_NEON_GEBP_NR=4 DTFLITE_KERNEL_USE_XNNPACK DTFLITE_BUILD_WITH_XNNPACK_DELEGATE DXNNPACK_DELEGATE_ENABLE_QS8 DXNNPACK_DELEGATE_ENABLE_QU8 DXNNPACK_DELEGATE_USE_LATEST_OPS DXNNPACK_DELEGATE_ENABLE_SUBGRAPH_RESHAPING DTFL_STATIC_LIBRARY_BUILD Wnodeprecateddeclarations MD MT CMakeFiles/tensorflowlite.dir/core/c/c_api.cc.o MF CMakeFiles/tensorflowlite.dir/core/c/c_api.cc.o.d o CMakeFiles/tensorflowlite.dir/core/c/c_api.cc.o c tensorflow/lite/core/c/c_api./lite/core/c/c_api.cc:67:38: error: expected ')'    67    ^ 2 errors generated. make[3]: *** [CMakeFiles/tensorflowlite.dir/core/c/c_api.cc.o] Error 1 make[2]: *** [CMakeFiles/tensorflowlite.dir/all] Error 2 make[1]: *** [tools/benchmark/CMakeFiles/benchmark_model.dir/rule] Error 2 make: *** [benchmark_model] Error 2 ```",2025-03-12T09:35:46Z,type:build/install comp:lite subtype:macOS,open,0,6,https://github.com/tensorflow/tensorflow/issues/89065,The build issue seems to be introduced with commit 805775fcb5f9272e4c52dce751b00cf7f70364f2.,"I got the same issue when crosscompiling for arm with GCC8 with commit 83283e1. From the comment above, I can imagine that it should be something like:  ``` // e.g. ""0.5.0"" or ""0.6.0alpha"". define TF_VERSION_STRING (_TF_STR(TF_MAJOR_VERSION) ""."" _TF_STR(TF_MINOR_VERSION) ""."" _TF_STR(TF_PATCH_VERSION) """" _TF_STR(TF_VERSION_SUFFIX)) ``` ?","> I got the same issue when crosscompiling for arm with GCC8 with commit 83283e1. >  > From the comment above, I can imagine that it should be something like: >  > ``` > // e.g. ""0.5.0"" or ""0.6.0alpha"". > define TF_VERSION_STRING (_TF_STR(TF_MAJOR_VERSION) ""."" _TF_STR(TF_MINOR_VERSION) ""."" _TF_STR(TF_PATCH_VERSION) """" _TF_STR(TF_VERSION_SUFFIX)) > ``` >  > ? Your solution works for me 👍 ","Hi, ,   Hi,  Thank you for providing the workaround for this issue I have submitted a PR to take care of this issue https://github.com/tensorflow/tensorflow/pull/91562 Thank you for your cooperation and patience.","Hi, , ,   Please refer these comments https://github.com/tensorflow/tensorflow/pull/91562issuecomment2813863024 and https://github.com/tensorflow/tensorflow/issues/90533issuecomment2813855694 If issue has been resolved please feel free to close this issue from your end.  Thank you for your cooperation and understanding.", Could you please clarify which additional variables must be specified? I've tried Bazel ones with ML_WHEEL_TYPE and it does not work. In the commit mentioned it seems like we need to provide version defines directly via C/CXX flags. If so this way is quite fragile and does not give us any compatibility as these defines could be changed at any time and I see no value in giving them instead of using a default ones provided by the project.
takutarou,"Error: Unable to register cuFFT, cuDNN, and cuBLAS factories in TensorFlow with CUDA"," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0gpu（docker hub）  Custom code Yes  OS platform and distribution Ubuntu 22.04 LTS (WSL2)  Mobile device _No response_  Python version 3.11.0  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 12.8, cuDNN 8.x (inside TensorFlow container)  GPU model and memory nvidia GeForce RTX4080 SUPER（16GB）  Current behavior? ・Summary I can’t resolve this error, so I need help. I am running an Ubuntu environment on WSL2 and using Docker via CLI. I am using TensorFlow 2.18.0GPU from Docker Hub. The GPU driver I am using is listed below. To prevent any issues from previous paths, I have reset my PC once, and there are no other development environments installed. As shown in the log below, I am encountering an error that I cannot resolve, and I would like to know the solution. ・NVIDIA Studio driver version 572.60 ・Error logs 20250305 17:54:21.254014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20250305 17:54:21.333890: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20250305 17:54:21.358644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250305 17:54:21.492257: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250305 17:54:23.035426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT  Standalone code to reproduce the issue ```shell docker pull tensorflow/tensorflow:2.18.0gpu ```  Relevant log output ```shell ```",2025-03-12T09:35:46Z,stat:awaiting tensorflower type:support comp:gpu TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/89064,"Hi **** , Thanks for raising your concern here. A similar issue ( CC(cuDNN, cuFFT, and cuBLAS Errors)) is still open. Please follow it for further updates and details. Thank you!","Hi  ,  Thank you, I overlooked a similar question. Now I have a much better understanding of the situation. I’ll refer to everyone’s comments while also looking for a solution myself. If I find any useful information, I’ll share it in CC(cuDNN, cuFFT, and cuBLAS Errors). With that, I’ll close this issue. Appreciate all the hard work from the development team—thank you for your dedication!",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Also generate TPU and CPU tests by default.,"Also generate TPU and CPU tests by default. The previous logic assumed that configcudaonly tag meant that we want to generate just GPU tests. That was a misunderstanding of the tag name. The tag means the test should only be run with config=cuda, as that is required by the generated GPU tests.",2025-03-12T09:30:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89063
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-12T08:57:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89062
copybara-service[bot],PR #23632: [ROCM] Fix vector bias add fusion into BLAS call,"PR CC(Incorrect masking in keras.backend.rnn): [ROCM] Fix vector bias add fusion into BLAS call Imported from GitHub PR https://github.com/openxla/xla/pull/23632 If RHS operand of GEMM has no noncontracting dims, broadcast bias vector to guarantee its length will still match matrix D rows with HIPBLASLT_EPILOGUE_BIAS epilogue required by (https://rocm.docs.amd.com/projects/hipBLASLt/en/latest/datatypes.html). Copybara import of the project:  7ae5ecbb89e6bb0e798cce3834dd3e094c47930a by Jian Li : [ROCM] Fix vector bias add fusion into BLAS call If RHS operand of GEMM has no noncontracting dims, broadcast bias vector to guarantee its length will still match matrix D rows with HIPBLASLT_EPILOGUE_BIAS epilogue required by (https://rocm.docs.amd.com/projects/hipBLASLt/en/latest/datatypes.html). Merging this change closes CC(Incorrect masking in keras.backend.rnn) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23632 from ROCm:ci_fix_bias_vector_add_v2 7ae5ecbb89e6bb0e798cce3834dd3e094c47930a",2025-03-12T08:52:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89061
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-12T08:34:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89060
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-12T08:22:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89059
copybara-service[bot],Add comment to explain why code is correct (NFC).,Add comment to explain why code is correct (NFC).,2025-03-12T07:36:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89058
copybara-service[bot],Remove GPU runtime dependencies from xla_cc_test,Remove GPU runtime dependencies from xla_cc_test xla_cc_test should only be used for backendagnostic tests. If you need the GPU backend to be linked in either use `xla_test` if the tests requires a GPU or explicitly add the dependencies otherwise. The latter should be rare and mainly needed for XLA runtime tests.,2025-03-12T07:15:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89057
copybara-service[bot],litert model utils: use pybind_extension for core_pybind build,litert model utils: use pybind_extension for core_pybind build,2025-03-12T05:41:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89056
copybara-service[bot],Add `TrackedTfrtGpuDeviceBuffer` and its helper class `MaybeOwningGpuMemory`.,Add `TrackedTfrtGpuDeviceBuffer` and its helper class `MaybeOwningGpuMemory`. This `MaybeOwningGpuMemory` will be refactored and merged with `MaybeOwningDeviceMemory` when basic implementation of TfrtGpuClient is done.,2025-03-12T04:11:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89055
copybara-service[bot],Improve documentation of the `Shape` class and `ShapeProto`.,Improve documentation of the `Shape` class and `ShapeProto`.,2025-03-12T03:48:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89054
copybara-service[bot],Add dynamic compiled model test case generation for integration tests.,"Add dynamic compiled model test case generation for integration tests. This will generate cc_test cases on the fly based on a configuration. The configuration consists of one or many tflite models and a accelerator setting (npu, cpu ...). This should be pretty flexible and will handle linux/android environment differences gracefull via GTEST_SKIP. The idea is for this to be the single C++ binary capable of testing all of the compiled models functionality (JIT, invoke precompiled, invoke cpu/gpu).",2025-03-12T02:58:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89053
copybara-service[bot],Internal change only,Internal change only,2025-03-12T01:45:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89052
copybara-service[bot],Fork quantization_config into Lite,Fork quantization_config into Lite,2025-03-12T01:15:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89051
copybara-service[bot],Change the `const vector<bool>&` parameters in `ShapeUtil` to `Span<const bool>` for consistency as flexibility.,"Change the `const vector&` parameters in `ShapeUtil` to `Span` for consistency as flexibility. Several functions in `ShapeUtil` takes both `Span` and `const vector&` parameters, which is inconsistent. `Span` is preferred as it can be trivially created from `vector`, but not the other way around. This is analogous to replacing `const string&` with `string_view`.",2025-03-12T00:22:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89050
copybara-service[bot],Add user-directed fuse attributes.,Add userdirected fuse attributes.,2025-03-12T00:09:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89049
copybara-service[bot],Add compiler and linker flags for dispatch API shared libraries.,Add compiler and linker flags for dispatch API shared libraries.,2025-03-11T23:57:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89048
copybara-service[bot],Integrate LLVM at llvm/llvm-project@262a7755822c,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 262a7755822c,2025-03-11T22:37:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89047
copybara-service[bot],Remove err print left in on accident,Remove err print left in on accident,2025-03-11T21:30:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89046
copybara-service[bot],Add small helper function for getting build stamp directly from the model,Add small helper function for getting build stamp directly from the model,2025-03-11T21:27:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89045
copybara-service[bot],Clean up version checks and acceleration compilation options ,"Clean up version checks and acceleration compilation options  Drivers for Dispatch API and Compiler Plugins must ensure that the loaded library have the same exact version, since we expect them to be distributed together with the LiteRT runtime.",2025-03-11T21:08:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89044
copybara-service[bot],Add RAII helper class MarkEventReadyOnExit for GpuEvent,Add RAII helper class MarkEventReadyOnExit for GpuEvent,2025-03-11T20:56:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89043
copybara-service[bot],Add CC Tensor Buffer Event tests.,Add CC Tensor Buffer Event tests.,2025-03-11T20:25:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89042
copybara-service[bot],Deprecate `Shape::dimensions_size()` in favor of `Shape::rank()`.,Deprecate `Shape::dimensions_size()` in favor of `Shape::rank()`. The two has the same behavior. The latter is a standard term and terse.,2025-03-11T20:19:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89041
copybara-service[bot],Make some OpKernelContext functions virtual,Make some OpKernelContext functions virtual,2025-03-11T19:57:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89040
copybara-service[bot],Switches `Shape::dimensions_size()` calls to `Shape::rank()` in OpenXLA for consistency.,"Switches `Shape::dimensions_size()` calls to `Shape::rank()` in OpenXLA for consistency. `Shape::dimensions_size()` and `Shape::rank()` have the same semantics/behavior. There's no need to have both. Switch uses of the former to the latter, which is more concise, so that we can remove the former later.",2025-03-11T19:57:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89039
copybara-service[bot],[XLA] upload to GCS bucket,[XLA] upload to GCS bucket,2025-03-11T19:31:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89038
copybara-service[bot],PR #23476: Skip CSE for Instructions That Are Not Safely Removable,"PR CC(Python Tensorflow crashes on saving the data): Skip CSE for Instructions That Are Not Safely Removable Imported from GitHub PR https://github.com/openxla/xla/pull/23476 **Description:**   Currently, CSE performs cleanup on replaced instructions using `computation.RemoveInstructionAndUnusedOperands(instr)`, which internally calls: ```c TF_RET_CHECK(IsSafelyRemovable(instruction, ignore_control_dependencies)) ``` If `IsSafelyRemovable` returns false, this causes a crash. To prevent this, I've added an `IsSafelyRemovable` check to the existing CSE prechecks.  To Reproduce model.hlo to reproduce the issue: ``` HloModule m ENTRY main {   %p419 = f32[512,768]{1,0} parameter(0)   %p420 = f32[512,768]{1,0} parameter(1)   %constant.3834 = f32[] constant(0)   %broadcast.7223 = f32[512,768]{1,0} broadcast(f32[] %constant.3834), dimensions={}   %copy.2311 = f32[512,768]{1,0} copy(f32[512,768]{1,0} %broadcast.7223), controlpredecessors={%p419}   %copy.2312 = f32[512,768]{1,0} copy(f32[512,768]{1,0} %broadcast.7223), controlpredecessors={%p420}   ROOT %tuple.17 = (f32[512,768]{1,0}, f32[512,768]{1,0}) tuple(f32[512,768]{1,0} %copy.2311, f32[512,768]{1,0} %copy.2312) } ``` cmd to repro the crash on CPU ``` xla/tools/run_hlo_module platform=CPU model.hlo ``` Error: ``` INTERNAL: RET_CHECK failure (xla/hlo/ir/hlo_computation.cc:435) IsSafelyRemovable(instruction, ignore_control_dependencies) Cannot remove instruction: %copy.2312 = f32[512,768]{1,0} copy(f32[512,768]{1,0} %broadcast.7223), controlpredecessors={%p420} 	Failed to execute on Host ```  Unit Tests I've added both positive and negative test cases:  CopyOpCSE – Verifies that CSE successfully removes safely removable operations.  DontCSE_NonSafelyRemovableOp – Ensures that CSE is skipped when copyOp is not safely removable. Copybara import of the project:  82f22bb7fbc724e905bffa594e2f4f1d716f8d71 by Alexander Pivovarov : Skip CSE for Instructions That Are Not Safely Removable Merging this change closes CC(Python Tensorflow crashes on saving the data) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23476 from apivovarov:skip_cse_noSafelyRemovable 82f22bb7fbc724e905bffa594e2f4f1d716f8d71",2025-03-11T19:16:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89037
copybara-service[bot],#litert Fix `litert::Expected` assignment operators.,"litert Fix `litert::Expected` assignment operators. This is quite a nasty bug. It would only trigger when copy/move assigning an `Expected` from another `Expected` with a different value state (`HasValue()`). For instance, in the `Holds an error` = `Holds a value` case: 1. `~Expected()` would destroy `unexpected_`. 2. Then we would call the `value_` assignment operator on some memory when a    `T` had never been constructed. This breaks the assignment operator    precondition that an instance of the class already exists and has a _valid_    internal state.",2025-03-11T19:04:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89036
copybara-service[bot],Add weight only quant support in FC op builder.,Add weight only quant support in FC op builder.,2025-03-11T18:46:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89035
copybara-service[bot],Add CC LiteRT Event test.,Add CC LiteRT Event test.,2025-03-11T18:43:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89034
copybara-service[bot],PR #88546: Qualcomm AI Engine Direct - Wrapper tests & Refactor tensor wrapper & Fix rms norm,PR CC(Qualcomm AI Engine Direct  Wrapper tests & Refactor tensor wrapper & Fix rms norm): Qualcomm AI Engine Direct  Wrapper tests & Refactor tensor wrapper & Fix rms norm Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/88546  What 1. Refactor tensor wrapper      Safer tensor data getter and setter. 2. Add wrapper tests. 3. Fix rms norm builder      Change 0 beta tensor type to float32 for float32 input and uint8 for other input data types. See op support types here.  Tests `qnn_compiler_plugin_test` ``` [] Global test environment teardown [==========] 99 tests from 5 test suites ran. (3995 ms total) [  PASSED  ] 99 tests. ``` Copybara import of the project:  c9511b8677d7a565416db5a317bfa239ee189d08 by chunhsue : refine tensor wrapper  a86a562667edb6fa81afff73b09d785dabfb5e8c by chunhsue : add wrapper tests  04116197eaa5c802c980198dcca675a15b60c2fd by chunhsue : fix rms norm builder Merging this change closes CC(Qualcomm AI Engine Direct  Wrapper tests & Refactor tensor wrapper & Fix rms norm) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88546 from jiunkaiy:dev/chunhsue/wrapper_tests 04116197eaa5c802c980198dcca675a15b60c2fd,2025-03-11T18:36:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89033
elfprince13,Allow empty configuration values to be supplied from environment,For example it may be desirable for `CC_OPT_FLAGS` to be empty.,2025-03-11T18:07:48Z,size:XS,closed,0,6,https://github.com/tensorflow/tensorflow/issues/89032,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , Can you please sign the CLA? Many thanks!","> Hi , Can you please sign the CLA? Many thanks! Hi, I actually already signed it and the CI bot just didn't rerun afterward =)","So the bot requires someone (contributor or reviewer) to retrigger a scan. From the check page, there's a link for that, when the CLA check fails.", anything remaining before it can be merged?,"Sorry, I was away while you sent the last message, but the delay here was caused by the need to have this PR imported into the internal systems, more CI to run on the result of the import and a different set of approvers. The process is documented somewhat in CONTRIBUTING.md"
copybara-service[bot],Make some OpKernelContext functions virtual,Make some OpKernelContext functions virtual,2025-03-11T17:57:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89031
copybara-service[bot],Refactor to use type,Refactor to use type,2025-03-11T17:51:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89030
copybara-service[bot],[XLA:CPU] Add SmallWhileLoopHoistingPass,[XLA:CPU] Add SmallWhileLoopHoistingPass,2025-03-11T17:50:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89029
copybara-service[bot],[XLA:CPU] Add CallKernelEmitter and use it to emit small while loops,[XLA:CPU] Add CallKernelEmitter and use it to emit small while loops Reverts 0ee8097d8e62aea9a59b1e5b935f7d0b2b963cf4,2025-03-11T17:48:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89028
copybara-service[bot],[XLA] Upload HLO test output to GCS during presubmit.,[XLA] Upload HLO test output to GCS during presubmit.,2025-03-11T17:41:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89027
copybara-service[bot],Remove stale TODO,Remove stale TODO,2025-03-11T17:21:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89026
copybara-service[bot],1. Change to the internal group spec.  2. Correcting a typo.,1. Change to the internal group spec.  2. Correcting a typo.,2025-03-11T16:21:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89025
copybara-service[bot],Simplify `hlo_parser_test`.,Simplify `hlo_parser_test`. * Factor out duplicated logic. * Remove unused `enable_verification`.,2025-03-11T15:46:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89024
copybara-service[bot],Allow appending a custom error message when using error checking macros & other improvements.,"Allow appending a custom error message when using error checking macros & other improvements.  Rename and move `ErrorStatusReturnHelper` to `litert::ErrorStatusBuilder`.  Add extra logging capabilities to `litert::ErrorStatusBuilder`.   It is now possible to stream data to the builder. This creates an extra   message that is appended to the original error message.   ```cpp   LITERT_RETURN_IF_ERROR(expr) << ""Failed while trying to ..."";   ```  Refactor `LITERT_RETURN_IF_ERROR` so that the default `ErrorStatusBuilder`   can be used (see example above).  Refactor `LITERT_ASSIGN_OR_RETURN` so that the return expression can   reference a variable called `_` that holds an `ErrorStatusBuilder` built with   the result of the expression.   ```cpp   LITERT_ASSIGN_OR_RETURN(auto var, expr, _ << ""Failed while trying to ..."");   ```  In functions returning a `LiteRtStatus`, this logs the message automatically   upon conversion.   This makes it possible to easily log messages.   ```cpp   LiteRtStatus LiteRtCFunction(LiteRtEnvironment environment, ...) {     LITERT_RETURN_IF_ERROR(environment, litert::Error(kLiteRtStatusErrorInvalidArgument,                                         ""`environment` handle must not be null.""));   }   ```  The log severity upon conversion can be adjusted with the `Log*()` functions   and silenced with `NoLog()`.",2025-03-11T15:45:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89023
copybara-service[bot],Remove default ctor for CC Tensor Buffer and Handle.,Remove default ctor for CC Tensor Buffer and Handle.,2025-03-11T15:38:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89022
copybara-service[bot],Add litert namespace to cc/litert_tensor_buffer_test.,Add litert namespace to cc/litert_tensor_buffer_test.,2025-03-11T15:14:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89021
copybara-service[bot],Add synchronization test cases for GL AHWB interop.,Add synchronization test cases for GL AHWB interop.,2025-03-11T14:40:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89020
copybara-service[bot],[xla:gpu] Remove mentions on NCCL from base CollectiveThunk,[xla:gpu] Remove mentions on NCCL from base CollectiveThunk,2025-03-11T14:23:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89019
18marie05,gemma2-2b SavedModel to tflite conversion,"Hello,  I am trying to convert a Gemma Saved Model in tflite format to run on an edge device.  1. System information I am running my code on a Jupyter Notebook. My tensorflow version is 2.18.0  2. Code  Code Here is the code I used once I have my Saved Model :  Since it is too large to compress and I can't share my Saved Model folder, here is how I got it:  I downloaded the Keras model from Kaggle  Here is the code I used to get de Saved Model Colab link ``` converter = tf.lite.TFLiteConverter.from_saved_model(""gemma22b_model"") converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]  Convert the model tflite_model = converter.convert()  Save the converted model with open('gemma22b_model/gemma2_2b.tflite', 'wb') as f:     f.write(tflite_model) ``` I tried different solutions:   with the converter.target_spec.supported_ops  without the converter.target_spec.supported_ops   only with TFLITE_BUILTINS and only with SELECT_TF_OPS  with an optimization like this converter.optimizations = [tf.lite.Optimize.DEFAULT] And I also tried this code with the method I saw on another resolved issue: ``` converter = tf.lite.TFLiteConverter.from_saved_model(""gemma22b_model"") converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.target_spec.supported_types = [tf.float16] converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS,     tf.lite.OpsSet.SELECT_TF_OPS] converter.experimental_new_converter = True converter.allow_custom_ops = True tflite_model = converter.convert()  Save the model. with open('gemma22b_model/test_ops_gemma2_2b.tflite', 'wb') as f:   f.write(tflite_model) ```  3. Failure after conversion After running it, the code produces a .tflite file but the StridedSlice is not supported, making the tflite model impossible to use. Here are the logs:  ``` W0000 00:00:1741686810.500306   38515 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format. W0000 00:00:1741686810.500796   38515 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency. 20250311 10:53:30.505166: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: gemma22b_model 20250311 10:53:30.526521: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve } 20250311 10:53:30.526654: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: gemma22b_model 20250311 10:53:30.741628: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle. 20250311 10:53:36.765240: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: gemma22b_model 20250311 10:53:37.149345: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 6644559 microseconds. 20250311 10:53:39.223577: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3825] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s): Flex ops: FlexStridedSlice Details: 	tf.StridedSlice(tensor, tensor, tensor, tensor) > (tensor) : {begin_mask = 7 : i64, ellipsis_mask = 0 : i64, end_mask = 7 : i64, new_axis_mask = 8 : i64, shrink_axis_mask = 0 : i64} 	tf.StridedSlice(tensor, tensor, tensor, tensor) > (tensor) : {begin_mask = 25 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 25 : i64, new_axis_mask = 6 : i64, shrink_axis_mask = 0 : i64} 	tf.StridedSlice(tensor, tensor, tensor, tensor) > (tensor) : {begin_mask = 25 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 25 : i64, new_axis_mask = 6 : i64, shrink_axis_mask = 0 : i64} See instructions: https://www.tensorflow.org/lite/guide/ops_select ```  I still tried to run the ```tf.lite.Interpreter``` with this code: ``` import numpy as np import tensorflow as tf  Load the TFLite model interpreter = tf.lite.Interpreter(model_path=""gemma22btflite/test_ops_gemma2_2b.tflite"") interpreter.allocate_tensors()  Get input and output details input_details = interpreter.get_input_details() output_details = interpreter.get_output_details()  Print the input and output details print(""Input Details:"", input_details) print(""Output Details:"", output_details)  Prepare input data (match the shape and dtype) input_data = np.array([[1]], dtype=np.int32)   Scalar value as input, matching the shape [1, 1] and dtype int32  Set the input tensor interpreter.set_tensor(input_details[0]['index'], input_data)  Run inference interpreter.invoke()  Get the output tensor output_data = interpreter.get_tensor(output_details[0]['index'])  Print the output print(""Output data:"", output_data ``` and this is what I got :  ``` INFO: Created TensorFlow Lite delegate for select TF ops. INFO: TfLiteFlexDelegate delegate: 52 nodes delegated out of 5669 nodes with 26 partitions. INFO: Created TensorFlow Lite XNNPACK delegate for CPU.  RuntimeError                              Traceback (most recent call last) Cell In[11], line 23      20 interpreter.set_tensor(input_details[0]['index'], input_data)      22  Run inference > 23 interpreter.invoke()      25  Get the output tensor      26 output_data = interpreter.get_tensor(output_details[0]['index']) File ~/Documents/.venv/lib/python3.11/sitepackages/tensorflow/lite/python/interpreter.py:965, in Interpreter.invoke(self)     953 """"""Invoke the interpreter.     954      955 Be sure to set the input sizes, allocate tensors and fill values before    (...)     962   ValueError: When the underlying interpreter fails raise ValueError.     963 """"""     964 self._ensure_safe() > 965 self._interpreter.Invoke() RuntimeError: tensorflow/lite/kernels/read_variable.cc:67 variable != nullptr was not true.Node number 297 (READ_VARIABLE) failed to invoke. ``` I tried to resolve my issue by consulting a few issues like this one  CC(error while converting to tflite) but none of the codes seem to work for my use case. I would be so grateful for your help. Thank you!",2025-03-11T13:38:13Z,stat:awaiting response stale comp:lite TFLiteConverter TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/89018,"Hi, Is there a solution to fix this conversion issue  ? Thank you for your answer","Hi,   I aplogize for the delay in my response, if possible could you please give it try with below conversion script by removing experimental flag and optimizations and see is it resolving your issue or not ? ``` converter = tf.lite.TFLiteConverter.from_saved_model(""gemma22b_model"") converter.target_spec.supported_types = [tf.float16] converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS,     tf.lite.OpsSet.SELECT_TF_OPS] tflite_model = converter.convert()  Save the model. with open('gemma22b_model/test_ops_gemma2_2b.tflite', 'wb') as f:   f.write(tflite_model) ``` Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,"i am having the same issue. I tried  's advice, unfortunately it doesn't work. My TF version is 2.19.0.  could you tell us how you solved this?"
copybara-service[bot],[XLA] Modify HloModule/HloComputation to maintain computations in an online topological sort at all times.,"[XLA] Modify HloModule/HloComputation to maintain computations in an online topological sort at all times. Rather than repeatedly computing HloModule::MakeComputationPostOrder() (i.e., an offline topological sort), we can simply maintain the HloComputations belonging to each HloModule in a topological order at all times using an online topological sort. This improves HLO compilation time, since many HLO passes need to operate in a postorder of computations, and it is cheaper to compute online as we update the HLO rather than to repeatedly recompute it from scratch.",2025-03-11T12:41:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89017
copybara-service[bot],[XLA:GPU]: Some cleanups in PriorityFusion (NFC),[XLA:GPU]: Some cleanups in PriorityFusion (NFC) Remove unused using declaration and private member variable. Apply ClangTidy header include fixes.,2025-03-11T12:32:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89016
copybara-service[bot],#sdy add temporary pass in XLA SDY import to not allow meshes with different axis shapes.,"sdy add temporary pass in XLA SDY import to not allow meshes with different axis shapes. Longer term we should either error during propagation or just support propagating through different meshes, but until this is done www.github.com/jaxml/jax/issues/26914 we can't look at doing this. As JAX first wants to make sure the lowered module never even has different meshes (but this is just a restriction JAX is imposing, that we don't have to impose in Shardy).",2025-03-11T12:29:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89015
copybara-service[bot],Introduce an advanced configuration option for the profiler.,Introduce an advanced configuration option for the profiler.,2025-03-11T12:12:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89014
copybara-service[bot],Add missing dependencies.,Add missing dependencies. litert,2025-03-11T11:02:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89013
copybara-service[bot],[XLA:GPU] extract code with regular dot op emission to a standalone function,[XLA:GPU] extract code with regular dot op emission to a standalone function noop refactoring The goal is to reduce the complexity of the individual functions and make it easier to see what we use in the code.,2025-03-11T10:28:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89012
copybara-service[bot],[XLA:GPU] extract for_loop body generator lambda out of EmitMatMul function,[XLA:GPU] extract for_loop body generator lambda out of EmitMatMul function noop refactoring It makes clear the data dependency between the functions.,2025-03-11T10:24:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89011
copybara-service[bot],[XLA:GPU] Refactor default triton gemm config handling.,[XLA:GPU] Refactor default triton gemm config handling. noop refactoring Extract function that infer the triton config.,2025-03-11T10:23:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89010
copybara-service[bot],[XLA:GPU] extract helper methods. NOOP change,[XLA:GPU] extract helper methods. NOOP change,2025-03-11T10:17:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89009
copybara-service[bot],"Refactor: Move profiler/convert/{google,oss} from tensorflow to xprof","Refactor: Move profiler/convert/{google,oss} from tensorflow to xprof This change moves the `profiler/convert/google/` and `profiler/convert/oss/`  directories from the TensorFlow repository to the xprof repository.",2025-03-11T08:58:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89008
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-11T08:56:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89007
copybara-service[bot],PR #23549: Add dynamic memcpy thunk and fusion.,"PR CC(Can the ops in pb files be fully supported by android tensorflow  Mobile ?): Add dynamic memcpy thunk and fusion. Imported from GitHub PR https://github.com/openxla/xla/pull/23549 These are not yet used, one more PR is needed for that. For now, this thunk only supports dynamicslicelike operations. Dynamicupdateslice will be supported in the future. There appears to be no good way to support these operations with command buffers enabled. We therefore disable command buffer support for them. We're investigating ways to make this work. Copybara import of the project:  333e077fed4c2a791a553dd8581cfc446bfd1170 by Johannes Reifferscheid : Add dynamic memcpy thunk and fusion. These are not yet used, see the next commit for that. For now, this thunk only supports dynamicslicelike operations. Dynamicupdateslice will be supported in the future. There appears to be no good way to support these operations with command buffers enabled. We therefore disable command buffer support for them. We're investigating ways to make this work.  e92b5d028a05694b0ce7d9ce7d2000491bf211d8 by Johannes Reifferscheid : Address review comments.  df6e2248d9d0dbb81faaabe920dafe64d25bc9ba by Johannes Reifferscheid : Address another comment, fix a stray BUILD change.  d03c59fffc53ebc682e31816a5a9bf3e07f8104a by Johannes Reifferscheid : Appease the linter.  5ade76c7ad60efa285fd145416511fb7214faf81 by Johannes Reifferscheid : Fix broken VLOG.  2f386d97f818c5fd067f26caa7cec471b780278a by Johannes Reifferscheid : Add missing dependencies. Merging this change closes CC(Can the ops in pb files be fully supported by android tensorflow  Mobile ?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23549 from jreiffers:memcpythunk 2f386d97f818c5fd067f26caa7cec471b780278a",2025-03-11T08:36:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89006
weilhuan-quic,Qualcomm AI Engine Direct - Two step TensorWrapper," WHAT Original flow: 1. Convert LiteRt tensor to TensorWrapper 2. Add TensorWrapper into Qnn graph 3. Convert LiteRt op to OpWrapper by op builder 4. Add OpWrapper into Qnn graph But in some op builder, we may modify some TensorWrapper inside. And these modification won't work due to the original flow. i.e. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/litert/vendors/qualcomm/core/builders/conv2d_op_builder.ccL78L81 Proposed flow: 1. Convert LiteRt tensor to TensorWrapper 2. **Convert LiteRt op to OpWrapper by op builder** 3. **Add TensorWrapper into Qnn graph** 4. Add OpWrapper into Qnn graph  TEST  qnn_compiler_plugin_test ``` [] Global test environment teardown [==========] 115 tests from 5 test suites ran. (3463 ms total) [  PASSED  ] 115 tests. ```",2025-03-11T05:49:54Z,size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89005
copybara-service[bot],API cleanup for CompilationOptions,"API cleanup for CompilationOptions With this change, the caller owns litert::CompilationOptions or LiteRtCompilationOptions when calling litert::CompileModel::Create or LiteRtCompiledModelCreate. This behavior is consistent with other LiteRT APIs.",2025-03-11T05:22:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89004
copybara-service[bot],Use correct namespaces for internal code,Use correct namespaces for internal code No functional change,2025-03-11T04:48:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89003
copybara-service[bot],Opaque pointers for callbacks.,Opaque pointers for callbacks.,2025-03-11T04:35:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/89002
copybara-service[bot],Refactor: Replace transient TensorFlow dependencies with TSL in `profiler/convert/`,Refactor: Replace transient TensorFlow dependencies with TSL in `profiler/convert/` This change replaces all instances of `tensorflow::` with `tsl::` within the `profiler/convert/` directory. This refactor aims to remove transient dependencies on TensorFlow and enhance modularity.,2025-03-11T04:03:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89001
copybara-service[bot],Remove TF_CUDNN_USE_FRONTEND env var.,"Remove TF_CUDNN_USE_FRONTEND env var. This environment variable has defaulted to true for over three years, and no known users set it to false. There is a lot of code maintaining the nonfrontend legacy cuDNN API which we'd like to delete. Removing the env var is the first step. I don't remove any code relying on the env var yet, in order to make this CL easier to rollback if necessary.",2025-03-11T02:42:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/89000
copybara-service[bot],litert: Update CompiledModel::Create() comments,litert: Update CompiledModel::Create() comments  Clearly mentioned that options are used for JIT compilation of the model.  Also mentioned that JIT options are meaningless for fully AOT compiled models.,2025-03-11T02:41:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88999
copybara-service[bot],Fixing PushAddedConstraints() which makes wrong assumptions,Fixing PushAddedConstraints() which makes wrong assumptions,2025-03-11T02:02:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88998
copybara-service[bot],Improve the partitioner for the dot pattern where,"Improve the partitioner for the dot pattern where 1. LHS and output is fully replicated 2. RHS is fully partitioned along the noncontracting dimensions 3. ShapeSize(RHS) >= ShapeSize(result) Two improvements. 1. Reduce the size of dot operation since we partition along noncontracting dimensions. 2. Reduce or keep the size of allgather due to Condition 3 ShapeSize(RHS) >= ShapeSize(result). Given the following input HLO ``` %lhs = bf16[64,32] parameter(0), sharding={devices=[2,1]<=[2]} %rhs = bf16[32,16] parameter(1), sharding={replicated} %dot = bf16[64,16] dot(%lhs, %rhs), lhs_contracting_dims={1}, rhs_contracting_dims={0}, sharding={replicated} ``` Previously, we (1) replicate LHS through allgather and (2) do the dot operation. ``` %param = bf16[32,32]{1,0} parameter(0), sharding={devices=[2,1]0,1} %allgather = bf16[64,32]{1,0} allgather(bf16[32,32]{1,0} %param), channel_id=1, replica_groups={{0,1}}, dimensions={0}, use_global_device_ids=true %param.1 = bf16[32,16]{1,0} parameter(1), sharding={replicated} %dot.1 = bf16[64,16]{1,0} dot(bf16[64,32]{1,0} %allgather, bf16[32,16]{1,0} %param.1), lhs_contracting_dims={1}, rhs_contracting_dims={0} ``` With this change, we (1) do the dot operation, and (2) replicate the result through allgather. We reduce the size of dot operation on each device. ``` %param = bf16[32,32]{1,0} parameter(0), sharding={devices=[2,1]<=[2]} %param.1 = bf16[32,16]{1,0} parameter(1), sharding={replicated} %dot.1 = bf16[32,16]{1,0} dot(bf16[32,32]{1,0} %param, bf16[32,16]{1,0} %param.1), lhs_contracting_dims={1}, rhs_contracting_dims={0} %allgather = bf16[64,16]{1,0} allgather(bf16[32,16]{1,0} %dot.1), channel_id=1, replica_groups=[1,2]<=[2], dimensions={0}, use_global_device_ids=true ```",2025-03-11T01:52:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88997
copybara-service[bot],Remove flag --xla_gpu_enable_cudnn_frontend.,"Remove flag xla_gpu_enable_cudnn_frontend. This flag has defaulted to true for over three years, and no known users set it to false. There is a lot of code maintaining the nonfrontend legacy cuDNN API which we'd like to delete. Removing the flag is the first step. I don't remove any code relying on the flag yet, in order to make this CL easier to rollback if necessary.",2025-03-11T00:46:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88996
copybara-service[bot],Fix model rountripping to make sure any buffers that were initially appended to the flatbuffer stay that way.,Fix model rountripping to make sure any buffers that were initially appended to the flatbuffer stay that way.,2025-03-11T00:33:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88995
copybara-service[bot],Add plane name regex matching util functions,Add plane name regex matching util functions,2025-03-11T00:32:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88994
copybara-service[bot],Add a little bit more test for `BoundedStreamPool` return,Add a little bit more test for `BoundedStreamPool` return,2025-03-11T00:20:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88993
copybara-service[bot],"[XLA] Rename file host_memory_offload_annotations.h to memory_annotations.h, rename build rules, relevant code pointers and namespace.","[XLA] Rename file host_memory_offload_annotations.h to memory_annotations.h, rename build rules, relevant code pointers and namespace.",2025-03-11T00:04:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88992
copybara-service[bot],Refactor `PartitionBaseCase` for spmd dot handler.,Refactor `PartitionBaseCase` for spmd dot handler. This CL is a pure refactor and does not affect the functionality of the SPMD partitioner.,2025-03-10T23:28:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88991
copybara-service[bot],litert: Share CL environment,litert: Share CL environment,2025-03-10T23:27:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88990
copybara-service[bot],"Directly call GpuExecutable::ExecuteThunks rather than RunAsync. This allows us to take a vector<ShapeTree<pair<raw_buffer, is_donated_bool>>> and return ShapeTree<raw_buffer> and handle aliasing properly for aliased types.","Directly call GpuExecutable::ExecuteThunks rather than RunAsync. This allows us to take a vector>> and return ShapeTree and handle aliasing properly for aliased types. Before this CL, aliased types would be converted to and from ScopedShapedBuffer which means that foreign memory types would lose their attached cleanup callback. Recently this was changed to simply error if the user tries to donate foreign memory types (created via CreateViewOfDeviceBuffer), but this CL makes it supported. This is also needed to properly track ownership for raw buffer support. (rollforward with fixes.) Reverts f68a041fd54892822dec9cd842541d2394685fc5",2025-03-10T23:09:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88989
copybara-service[bot],Optimize utility function for getting op options. Single dispatch has a lot of overhead,Optimize utility function for getting op options. Single dispatch has a lot of overhead,2025-03-10T22:58:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88988
wonjeon,[mlir][tosa] Update control flow variable names and Fix lit tests for max_pool2d and pad op,We've been pushing TOSA v1.0 LLVM patches to upstream. This PR includes commit(s) to keep the following operator(s) to be aligned with LLVM: [mlir][tosa] Remove optional for pad_const and remove input_zp attr for PadOp https://github.com/llvm/llvmproject/pull/129336 [mlir][tosa] Add more verifiers for the following operators https://github.com/llvm/llvmproject/pull/127923 This PR addresses the current issue of broken code and lit tests.,2025-03-10T22:35:58Z,kokoro:force-run ready to pull size:S comp:lite-tosa,closed,0,2,https://github.com/tensorflow/tensorflow/issues/88987,"Local testing done successfully: INFO: Analyzed 33 targets (1 packages loaded, 33392 targets configured). INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 4.039s, Critical Path: 2.52s INFO: 18 processes: 34 local. INFO: Build completed successfully, 18 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test         PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test             PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test      PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test                PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test   PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test           PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test  PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test      PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test         PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 2.1s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test     PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test        PASSED in 0.1s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test   PASSED in 0.2s Executed 17 out of 17 tests: 17 tests pass.",Updated the description to include the matching llvm patches.
copybara-service[bot],Integrate LLVM at llvm/llvm-project@262a7755822c,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 262a7755822c,2025-03-10T22:34:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88986
copybara-service[bot],[XLA:CollectiveSinking] Pass the move_info index map by reference to the function that merges move_infos.,[XLA:CollectiveSinking] Pass the move_info index map by reference to the function that merges move_infos.,2025-03-10T22:18:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88985
copybara-service[bot],Delete unused IFRT proxy backend factory in `ifrt_proxy_internal.py`,Delete unused IFRT proxy backend factory in `ifrt_proxy_internal.py`,2025-03-10T22:17:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88984
copybara-service[bot],Fix a crash introduced by a prior change,Fix a crash introduced by a prior change,2025-03-10T22:14:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88983
copybara-service[bot],Fix documentation on shapes and layouts.,"Fix documentation on shapes and layouts. The documentation on XLA shapes and layouts are outdated:  `Shape` and `Layout` have been renamed `ShapeProto` and `LayoutProto`.  The `padded_dimensions` and `padding_value` fields were removed long ago. Also clarify:  how to interpret the `tail_padding_alignment_in_elements` field in `LayoutProto` if it's unset or set to 0,  the invariant that `tail_padding_alignment_in_elements_` is >= 1 in the `Layout` class (also added CHECK to enforce this invariant). Also move `minor_to_major` to the beginning of `LayoutProto` as it's the most important field.",2025-03-10T21:55:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88982
copybara-service[bot],[xla:cpu] Check if some other thread created communicator for given rank,[xla:cpu] Check if some other thread created communicator for given rank,2025-03-10T21:55:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88981
amdjebb,Fix lack of error handling in device_lib and gpu_device_name issue##88288,"This is a pr for issue CC(error handling in list_devices() not present) (Fixing gpu device errors) Currently, calling `device_lib.list_local_devices()` or `test_util.gpu_device_name()` without proper error handling causes TensorFlow to crash when GPU drivers are missing, misconfigured, or devices are improperly queried. Changed: Added tryexcept block to handle device listing failures gracefully Added more checks for GPU detection function",2025-03-10T21:34:47Z,stat:awaiting response stale ready to pull size:S python,closed,0,9,https://github.com/tensorflow/tensorflow/issues/88980,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , CAn you please sign the CLA, Thank you !","I've signed the CLA 3 times, still get the error: !image","Please make sure it is signed with your `` email, as that seems to be the email used to make the commits","!image It is, not sure what the issue is. > Please make sure it is signed with your `` email, as that seems to be the email used to make the commits","Sorry, that was on our side, we also needed to retrigger the scanning. Thank you!","The internal tests are failing with a linter rule: > Please use mock.patch.object() instead of mock.patch() which is hostile towards static analysis of imports Can you do that, please?",This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,This PR was closed because it has been inactive for 14 days since being marked as stale. Please reopen if you'd like to work on this further.
copybara-service[bot],Install jq package in the ml build container,Install jq package in the ml build container,2025-03-10T21:31:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88979
copybara-service[bot],Add absl::Span<char> overload for tsl::RandomAccessFile::Read,Add absl::Span overload for tsl::RandomAccessFile::Read,2025-03-10T21:30:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88978
copybara-service[bot],Enable rewriting max_enqueued_batches to 0,Enable rewriting max_enqueued_batches to 0,2025-03-10T21:12:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88977
copybara-service[bot],Define CAPI and Python API for chlo.ragged_dot.,Define CAPI and Python API for chlo.ragged_dot.,2025-03-10T21:00:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88976
copybara-service[bot],Fix operand_batching_dims vs start_indices_batching_dims mix-up in ConvertGatherOp,Fix operand_batching_dims vs start_indices_batching_dims mixup in ConvertGatherOp The `slice_sizes` compatibility check was incorrectly using `start_indices_batching_dims` instead of `operand_batching_dims`. Updated the associated test cases so that `start_indices_batching_dims` and `operand_batching_dims` contain different dimensions.,2025-03-10T20:36:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88975
copybara-service[bot],[XLA] Simplify the DFS-based implementation of HloModule::MakeComputationPostOrder.,"[XLA] Simplify the DFSbased implementation of HloModule::MakeComputationPostOrder. The current implementation has a couple of strange features: * it goes to great lengths to separate root from nonroot computations. But we don't need to do this, a depthfirst search post order will happily identify the roots for us, without traversing the instructions a second time. * it calls HloComputation::MakeEmbeddedComputations(). But this doesn't just return the direct children of a computation, it also returns the transitive children. If we have a deeply nested computation, we might looking at the instructions to find transitive callees in a way that is quadratic with the depth of nested computations.",2025-03-10T20:24:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88974
copybara-service[bot],Clean Up Forwarding Headers in tensorflow/core/profiler/protobuf Folder,Clean Up Forwarding Headers in tensorflow/core/profiler/protobuf Folder,2025-03-10T20:12:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88973
copybara-service[bot],Reverts d7d98846ac7e582ef25e3de156ea9efaefeaa94b,Reverts d7d98846ac7e582ef25e3de156ea9efaefeaa94b,2025-03-10T20:02:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88972
copybara-service[bot],[HLO-OPT] Minor documentation update,[HLOOPT] Minor documentation update,2025-03-10T19:45:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88971
copybara-service[bot],Fix a bug where aggregating post allocation update information would result in wrong results.,Fix a bug where aggregating post allocation update information would result in wrong results. This happened where an allocation has a use that is getting updated by more than one instruction.,2025-03-10T19:28:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88970
copybara-service[bot],[XLA] Compute GPU stats in postsubmit and update run schedule for nightly jobs,[XLA] Compute GPU stats in postsubmit and update run schedule for nightly jobs,2025-03-10T19:11:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88969
copybara-service[bot],[XLA:MSA] Fix an iterator invalidation bug in MSA.,[XLA:MSA] Fix an iterator invalidation bug in MSA. We should not be removing and iterating the uses of an allocation at the same time.,2025-03-10T18:38:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88968
copybara-service[bot],Avoid creating large constants in ConvertTFLBroadcastToMulOp optimization pass.,Avoid creating large constants in ConvertTFLBroadcastToMulOp optimization pass. This pattern is inherited from before and has proved to be increasing the model size due the introduction of large splat const. In its current form this pattern replaces a tfl.broadcast_to op (with rank<4) to a tfl.mul with allones tensor. This change will keep the broadcast_to ops as is because its clear that introducing MUL is not an optimization.,2025-03-10T18:32:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88967
copybara-service[bot],Update MacOS Kokoro build script to invoke `build.py` correctly,Update MacOS Kokoro build script to invoke `build.py` correctly,2025-03-10T18:31:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88966
copybara-service[bot],Upgrade the Bazel apple_support to version 1.18.1.,Upgrade the Bazel apple_support to version 1.18.1. To get https://github.com/bazelbuild/apple_support/pull/372 to removes `Os` from the 't supported by old Xcode versions.,2025-03-10T17:59:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88965
copybara-service[bot],PR #23441: Up HERMETIC_CUDA_VERSION default value to 12.6.3,PR CC(Fix the compute_output_shape issue in tf.keras.layers.Bidirectional): Up HERMETIC_CUDA_VERSION default value to 12.6.3 Imported from GitHub PR https://github.com/openxla/xla/pull/23441 Copybara import of the project:  caacac1e24c0f72354b80d1b3e836c4c35c194de by Shraiysh Vaishay : Up HERMETIC_CUDA_VERSION default value to 12.6.3 Merging this change closes CC(Fix the compute_output_shape issue in tf.keras.layers.Bidirectional) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23441 from shraiysh:uphermeticcudadefault caacac1e24c0f72354b80d1b3e836c4c35c194de,2025-03-10T17:34:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88964
copybara-service[bot],[XLA] Add a pass for moving slice uses closer to their operands.,[XLA] Add a pass for moving slice uses closer to their operands.,2025-03-10T17:08:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88963
copybara-service[bot],API cleanup: litert::Layout,API cleanup: litert::Layout No functional change,2025-03-10T17:00:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88962
copybara-service[bot],Add CreateFromGlBuffer unit test to CC Tensor Buffer.,Add CreateFromGlBuffer unit test to CC Tensor Buffer.,2025-03-10T16:39:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88961
nassimus26,could not find registered transfer manager for platform Host -- check target linkage 	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_33145]," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.8  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi since I added some conv2D to my model I am getting this error on the Colab TPU  :  ``` NotFoundError: Graph execution error: Detected at node StatefulPartitionedCall defined at (most recent call last):   File """", line 198, in _run_module_as_main   File """", line 88, in _run_code   File ""/usr/local/lib/python3.11/distpackages/colab_kernel_launcher.py"", line 37, in    File ""/usr/local/lib/python3.11/distpackages/traitlets/config/application.py"", line 992, in launch_instance   File ""/usr/local/lib/python3.11/distpackages/ipykernel/kernelapp.py"", line 712, in start   File ""/usr/local/lib/python3.11/distpackages/tornado/platform/asyncio.py"", line 205, in start   File ""/usr/lib/python3.11/asyncio/base_events.py"", line 608, in run_forever   File ""/usr/lib/python3.11/asyncio/base_events.py"", line 1936, in _run_once   File ""/usr/lib/python3.11/asyncio/events.py"", line 84, in _run   File ""/usr/local/lib/python3.11/distpackages/ipykernel/kernelbase.py"", line 510, in dispatch_queue   File ""/usr/local/lib/python3.11/distpackages/ipykernel/kernelbase.py"", line 499, in process_one   File ""/usr/local/lib/python3.11/distpackages/ipykernel/kernelbase.py"", line 406, in dispatch_shell   File ""/usr/local/lib/python3.11/distpackages/ipykernel/kernelbase.py"", line 730, in execute_request   File ""/usr/local/lib/python3.11/distpackages/ipykernel/ipkernel.py"", line 383, in do_execute   File ""/usr/local/lib/python3.11/distpackages/ipykernel/zmqshell.py"", line 528, in run_cell   File ""/usr/local/lib/python3.11/distpackages/IPython/core/interactiveshell.py"", line 2975, in run_cell   File ""/usr/local/lib/python3.11/distpackages/IPython/core/interactiveshell.py"", line 3030, in _run_cell   File ""/usr/local/lib/python3.11/distpackages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner   File ""/usr/local/lib/python3.11/distpackages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async   File ""/usr/local/lib/python3.11/distpackages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes   File ""/usr/local/lib/python3.11/distpackages/IPython/core/interactiveshell.py"", line 3553, in run_code   File """", line 48, in    File """", line 47, in train   File """", line 82, in prep   File """", line 78, in train_step   File ""/usr/local/lib/python3.11/distpackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler   File ""/usr/local/lib/python3.11/distpackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit   File ""/usr/local/lib/python3.11/distpackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function   File ""/usr/local/lib/python3.11/distpackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator could not find registered transfer manager for platform Host  check target linkage 	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_33145] ```  Standalone code to reproduce the issue Here's the Conv2D which I added before getting this Error :  ``` cnn = layers.Conv2D(300, (3, 3), strides=(3, 3), activation=""relu"")(input) cnn = layers.MaxPooling2D((2, 2))(cnn) cnn = layers.Conv2D(120, (3, 3), strides=(3, 3), activation=""relu"")(cnn) cnn = layers.AveragePooling2D((2, 2))(cnn) cnn = layers.Conv2D(64, (2, 2), strides=(2, 2), activation=""sigmoid"")(cnn) ```",2025-03-10T16:27:36Z,stat:awaiting response type:bug stale comp:dist-strat TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88960,"Hi **** , Apologies for the delay, and thank you for raising your concern here. Could you please share the complete code snippet? This would help us investigate the issue more effectively. Additionally, I noticed that you are using an older version of TensorFlow (2.8). I recommend upgrading to the latest version and checking if the issue persists. If the problem still occurs, please let us know, and we will be happy to assist further. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Gracefully handles absent departure costs & allows for a non-deterministic timeout.,Gracefully handles absent departure costs & allows for a nondeterministic timeout.,2025-03-10T16:05:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88959
copybara-service[bot],[XLA:GPU] Make sure to delete the `TritonGemmKey` from fusions as we nest them.,"[XLA:GPU] Make sure to delete the `TritonGemmKey` from fusions as we nest them. Previously, both a `TritonGemmKey` and a `BlockLevelFusionConfig` would be present in the backend config of a fusion after the application of `NestGemmFusion`.",2025-03-10T16:04:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88958
copybara-service[bot],[XLA:GPU] Adapt `EmitReduce`'s masking logic to `dot`s.,[XLA:GPU] Adapt `EmitReduce`'s masking logic to `dot`s.,2025-03-10T15:46:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88957
copybara-service[bot],[xla:gpu] Support empty nested fusions.,[xla:gpu] Support empty nested fusions.,2025-03-10T15:35:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88956
copybara-service[bot],PR #23502: Add gfx1101 support,PR CC(未找到相关数据): Add gfx1101 support Imported from GitHub PR https://github.com/openxla/xla/pull/23502 Copybara import of the project:  be7af2aaf124f59d780de8135e94fd8fda4b48bd by Jehandad Khan : add gfx1101 support Merging this change closes CC(未找到相关数据) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23502 from ROCm:jdgfx11 be7af2aaf124f59d780de8135e94fd8fda4b48bd,2025-03-10T15:21:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88955
copybara-service[bot],PR #23553: Remove unused CUPTI/NVTX #include,"PR CC(Relationships between Grappler, GraphOptimizer and GraphOptimizationPass): Remove unused CUPTI/NVTX include Imported from GitHub PR https://github.com/openxla/xla/pull/23553 All the other lines are insisted upon by the `clangformat` presubmit check. Copybara import of the project:  152733177dd35876b6c0614917e31bab188fc54c by Olli Lupton : Remove unused CUPTI/NVTX include  970f55f476a9b4bf2dcb829152d4104f21864436 by Olli Lupton : clangformat Merging this change closes CC(Relationships between Grappler, GraphOptimizer and GraphOptimizationPass) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23553 from olupton:removeunusedinclude 970f55f476a9b4bf2dcb829152d4104f21864436",2025-03-10T15:18:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88954
VTaPo,Saving and Reloading custom model," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution MacOS Apple Sillicon  Mobile device _No response_  Python version 3.11.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I have my custom model like this: () class CategoricalPreprocessor(tf.keras.Model):     def __init__(self, categorical_features, num_oov_indices=1, vocabularies=None, **kwargs):         """"""         A class for preprocessing categorical features using StringLookup.         Args:             categorical_features (list): List of categorical feature names.             num_oov_indices (int): Number of outofvocabulary (OOV) indices for unknown values.             **kwargs: Additional arguments passed to the parent class.         """"""         super().__init__(**kwargs)         self.categorical_features = categorical_features          Initialize InputLayers for each categorical feature.         self.input_layers = {             feature: tf.keras.Input(shape=(None,), dtype=tf.string, name=f""{feature}_input"")             for feature in categorical_features         }          Initialize StringLookup layers for each feature with OOV handling.         self.string_lookups = {             feature: tf.keras.layers.StringLookup(num_oov_indices=num_oov_indices, name=f""{feature}_lookup"")             for feature in categorical_features         }         if vocabularies!=None:             for feature in categorical_features:                 self.string_lookups[feature].set_vocabulary(vocabularies[feature])     def adapt(self, data):         """"""         Prepares the StringLookup layers with the vocabulary from the given data.         Args:             data (dict): Dictionary where keys are feature names and values are numpy arrays.         """"""         for feature in self.categorical_features:              Adapt each StringLookup layer to the corresponding feature's data.             self.string_lookups[feature].adapt(data[feature])     def call(self, inputs):         """"""         Applies StringLookup transformation to each categorical feature.         Args:             inputs (dict): A dictionary where keys are feature names and values are tensors or numpy arrays.         Returns:             dict: A dictionary where keys are feature names and values are tensors transformed by StringLookup.         """"""         return {             feature: self.string_lookupsfeature             for feature in self.categorical_features         }     def get_config(self):         config = super().get_config()         vocabularies = {}         for feature in self.categorical_features:             vocabularies[feature] = self.string_lookups[feature].get_vocabulary()         config.update({             ""num_oov_indices"": num_oov_indices,             ""categorical_features"": categorical_features,             ""vocabularies"": vocabularies         })         return config          def from_config(cls, config):         return cls(**config) I saved my model by model.save('preprocessor.keras') Then, I reloaded it for inference, but I got an error ""Unknown layer 'CategoricalPreprocessor' if I didn't redefine my code about CategoricalPreprocessor in the inference file. Is there a way to save and load the custom model so that when I use it for the inference phase, I can just reload the checkpoint and use it without having to redefine the class (without having to import my class into the inference file)? Please help me, thank you very much!  Standalone code to reproduce the issue ```shell I can't share full code. ```  Relevant log output ```shell ```",2025-03-10T15:04:16Z,stat:awaiting response type:support stale TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/88953,PLease tell me any way that I can reload model without access model definition?,"All custom operations and custom layers need to be accessible to the side that uses the model. Otherwise, the graph cannot be interpretted. You don't need to share the full model architecture/definition, just make sure the code that implements the custom addition is available.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],PR #23542: [ROCm] Support clang19 as host compiler,PR CC(No C++ symbols exported after built libtensorflow_cc with bazel on windows): [ROCm] Support clang19 as host compiler Imported from GitHub PR https://github.com/openxla/xla/pull/23542 Pass nocanonicalprefixes to clang19 to get old InstalledDir behavior. Copybara import of the project:  03a6958a7ef6fde43ec6c20c8eb984b4afa181ff by Dragan Mladjenovic : [ROCm] Support clang19 as host compiler Merging this change closes CC(No C++ symbols exported after built libtensorflow_cc with bazel on windows) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23542 from ROCm:ci_clang19 03a6958a7ef6fde43ec6c20c8eb984b4afa181ff,2025-03-10T14:05:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88952
copybara-service[bot],Fix DMABUF preprocessor directive.,Fix DMABUF preprocessor directive.,2025-03-10T14:04:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88951
copybara-service[bot],PR #23509: [GPU] Add cuDNN 9.8.0.,PR CC(Does keras saved Model can output tensor?): [GPU] Add cuDNN 9.8.0. Imported from GitHub PR https://github.com/openxla/xla/pull/23509 Copybara import of the project:  913743e282a952ca475678a5c713bd1ac502dd2f by Ilia Sergachev : [GPU] Add cuDNN 9.8.0. Merging this change closes CC(Does keras saved Model can output tensor?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23509 from openxla:cudnn_980 913743e282a952ca475678a5c713bd1ac502dd2f,2025-03-10T14:02:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88950
copybara-service[bot],PR #23552: [ROCm] Add rocm deps for ragged_all_to_all_kernel,"PR CC(if i set the inputshape with (None, None, featdim),how can I split the input by second dimention(axis=1)?): [ROCm] Add rocm deps for ragged_all_to_all_kernel Imported from GitHub PR https://github.com/openxla/xla/pull/23552 Kernel added in https://github.com/openxla/xla/commit/be68e80894862fe97757ea2b6110958ef4244c21. For ROCm build is currently failing with: ``` [20250309T01:01:48.040Z] ERROR: /tf/xla/xla/service/gpu/kernels/BUILD:291:13: Compiling xla/service/gpu/kernels/ragged_all_to_all_kernel.cu.: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing CppCompile command (from target //xla/service/gpu/kernels:ragged_all_to_all_kernel_gpu) external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc U_FORTIFY_SOURCE fstackprotector Wall Wunusedbutsetparameter Wnofreenonheapobject fnoomitframepointer ... (remaining 148 arguments skipped) [20250309T01:01:48.040Z] /root/.cache/bazel/_bazel_root/217377b0e928b171b843eb11ea7bc36e/execroot/xla/external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc:23: DeprecationWarning: 'pipes' is deprecated and slated for removal in Python 3.13 [20250309T01:01:48.040Z]   import pipes [20250309T01:01:48.040Z] xla/service/gpu/kernels/ragged_all_to_all_kernel.cu.cc:52:1: error: ‘__global__’ does not name a type [20250309T01:01:48.040Z]    52     ``` This PR just adds necessary deps for rocm  Copybara import of the project:  80f96ff15de134bf14ff00a4483f7b6707744445 by Milica Makevic : Add rocm deps for ragged_all_to_all_kernel Merging this change closes CC(if i set the inputshape with (None, None, featdim),how can I split the input by second dimention(axis=1)?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23552 from ROCm:hotfix_250310 80f96ff15de134bf14ff00a4483f7b6707744445",2025-03-10T14:00:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88949
copybara-service[bot],[XLA:GPU] Log an error when path can't be used for dump.,[XLA:GPU] Log an error when path can't be used for dump.,2025-03-10T13:50:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88948
copybara-service[bot],Allow to replace an instruction in the HloDfsReachability data structure.,"Allow to replace an instruction in the HloDfsReachability data structure. This will be needed when we want to use HloDfsReachability in PriorityFusion. When doing a regular (nonmultioutput) fusion, we don't need to rebuild the whole data structure and can just replace a single instruction with the created fusion instruction if needed.",2025-03-10T13:06:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88947
copybara-service[bot],API cleanup: litert::Layout,API cleanup: litert::Layout No functional change,2025-03-10T12:52:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88946
copybara-service[bot],Dump Shardy dumps only when shardy-xla pass is registered on xla_dump_hlo_pass_re.,Dump Shardy dumps only when shardyxla pass is registered on xla_dump_hlo_pass_re.,2025-03-10T12:45:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88945
copybara-service[bot],Make MultiOutputFusionCreatesCycle a standalone method in anonymous namespace (NFC).,Make MultiOutputFusionCreatesCycle a standalone method in anonymous namespace (NFC). The method is not called outside of instruction_fusion.cc,2025-03-10T12:35:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88944
copybara-service[bot],PR #23551: [XLA:GPU] Warn if tensor memory usage is too high,"PR CC([Resolved] Inconsistent output shape for tf.image.resize_images() when preserve_aspect_ratio=True): [XLA:GPU] Warn if tensor memory usage is too high Imported from GitHub PR https://github.com/openxla/xla/pull/23551 Provide a better error message if the selected Triton gemm config uses too much TMEM (tensor memory) on Blackwell, similar to the shared memory warning. Copybara import of the project:  45f3580601098a8a1af489dccb54803acd315143 by Sergey Kozub : [XLA:GPU] Warn if tensor memory usage is too high Merging this change closes CC([Resolved] Inconsistent output shape for tf.image.resize_images() when preserve_aspect_ratio=True) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23551 from openxla:skozub/warn_tmem 45f3580601098a8a1af489dccb54803acd315143",2025-03-10T12:34:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88943
copybara-service[bot],[XLA:GPU] Add a version field to the autotuning cache key.,[XLA:GPU] Add a version field to the autotuning cache key. This field is used to invalidate the cache in case of changes that would affect the autotuner results (e.g. Triton upgrades).,2025-03-10T11:03:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88942
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-10T10:54:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88941
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-10T09:36:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88940
copybara-service[bot],PR #23468: Ensure PTX version compatibility w/ Clang & ptxas,"PR CC(Add option for inferring op attributes from inputs): Ensure PTX version compatibility w/ Clang & ptxas Imported from GitHub PR https://github.com/openxla/xla/pull/23468 Using the flag `cudafeature=+ptx`, Clang can be instructed to emit a specific PTX version from the NVPTX backend. If this flag is omitted, then Clang might emit a newer version of PTX than what ptxas from Hermetic CUDA can recognize which can lead to compilation errors. This commit adds a mapping from Clang & CUDA version to PTX version in `third_party/gpus/cuda/hermetic/cuda_redist_versions.bzl` which will need to be updated over time. If either the version for Clang or CUDA cannot be mapped to a PTX version, then configuration will fail. Resolves openxla/xla CC(Dataset shard index automatic change in Estimator DistributionStrategy) Copybara import of the project:  49c5940498f608b82539243b286431a74cdfc0dd by Jack Wolfard : Ensure PTX version compatibility w/ Clang & ptxas Using the flag `cudafeature=+ptx`, Clang can be instructed to emit a specific PTX version from the NVPTX backend. If this flag is omitted, then Clang might emit a newer version of PTX than what ptxas from Hermetic CUDA can recognize which can lead to compilation errors. This commit adds a mapping from Clang & CUDA version to PTX version in `third_party/gpus/cuda/hermetic/cuda_redist_versions.bzl` which will need to be updated over time. If either the version for Clang or CUDA cannot be mapped to a PTX version, then configuration will fail. Resolves openxla/xla CC(Dataset shard index automatic change in Estimator DistributionStrategy) Merging this change closes CC(Add option for inferring op attributes from inputs) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23468 from JackWolfard:clangptxasversion 49c5940498f608b82539243b286431a74cdfc0dd",2025-03-10T07:38:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88939
copybara-service[bot],Pass empty string to per op data as the southbound API expects an empty function name.,Pass empty string to per op data as the southbound API expects an empty function name.,2025-03-10T06:48:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88938
copybara-service[bot],Internal change only.,Internal change only.,2025-03-10T06:42:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88937
jimwang118,[pcl] build failure," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 1.15.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? E:\vcpkg\buildtrees\pcl\src\pcl1.15.0cf88c69527.clean\simulation\src\glsl_shader.cpp(163): error C3861: 'gluErrorString': identifier not found E:\vcpkg\buildtrees\pcl\src\pcl1.15.0cf88c69527.clean\simulation\src\glsl_shader.cpp(163): error C2593: 'operator <<' is ambiguous  Standalone code to reproduce the issue ```shell vcpkg install pcl[core,opengl,qt,simulation,surfaceonnurbs,tools,visualization,vtk]:x64windows ```  Relevant log output ```shell E:\vcpkg\buildtrees\pcl\src\pcl1.15.0cf88c69527.clean\simulation\src\glsl_shader.cpp(163): error C3861: 'gluErrorString': identifier not found E:\vcpkg\buildtrees\pcl\src\pcl1.15.0cf88c69527.clean\simulation\src\glsl_shader.cpp(163): error C2593: 'operator <<' is ambiguous ```",2025-03-10T06:42:15Z,stat:awaiting response type:bug stale TF 1.15,closed,0,3,https://github.com/tensorflow/tensorflow/issues/88936,"Hi **** , Apologies for the delay, and thanks for raising your concern here. This issue does not appear to be related to TensorFlow. It seems to be associated with PCL (Point Cloud Library) and vcpkg. I recommend raising the issue in the appropriate repository for a quicker resolution. Additionally, I noticed that you are using an older version of TensorFlow (1.15.0), which is no longer supported. Please consider upgrading to the latest version for better compatibility and support. Here is the official TensorFlow upgrade documentation, and windows for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Are you satisfied with the resolution of your issue? Yes No
chunhsue,Qualcomm AI Engine Direct - Convert QINT16 to QUINT16, [ ] Rebase  [ ] Add fastrpc test,2025-03-10T05:22:04Z,size:XL,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88935
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-10T05:05:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88934
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-10T05:03:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88933
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-10T04:54:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88932
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-10T04:48:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88931
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-10T04:46:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88930
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-10T04:42:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88929
copybara-service[bot],Improve mypy stubs,Improve mypy stubs These stubs are generated by cherrypicking in https://github.com/python/mypy/pull/18762 to mypy 1.13.0 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23553 from olupton:removeunusedinclude 970f55f476a9b4bf2dcb829152d4104f21864436,2025-03-10T03:39:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88928
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-10T02:39:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88927
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-10T02:25:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88926
copybara-service[bot],Move xprof code in tensorflow/python/profiler to xprof.,"Move xprof code in tensorflow/python/profiler to xprof. As part of an ongoing effort to decouple our ml profiler code from tensorflow, this PR migrates code from tensorflow/tensorflow into tensorflow/profiler. The existing tensorflow/python/profiler directory contains a mixture of the old tensorflow profiler v1 code, as well as the more XLA focused library. This PR touches code within tensorflow/tensorflow and tensorflow/profiler. Here is an outline of the changes for each repo. tensorflow/tensorflow: * Extract implementation of python/profiler/internal/pywrap_profiler_plugin./profiler: * Create xprof/pywrap BUILD * Use python/profiler/internal/pywrap_profiler_plugin. * Remove dependency on tensorflow/python/profiler/internal FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23947 from yliu120:fix_inliner 83afbae34a3f054dc77c12db886c61084b0b7ae3",2025-03-10T01:36:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88925
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-09T22:27:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88924
copybara-service[bot],[xla:cpu:onednn] Minor build fix,"[xla:cpu:onednn] Minor build fix In rule `onednn_graph_cc_test`, the dependency `gunit_main` must only be added when Graph API is disabled. Otherwise, the test will have two `main`s when Graph API is enabled. + Remove unnecessary header include.",2025-03-09T16:42:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88923
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-09T14:29:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88922
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-09T13:48:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88921
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-09T06:33:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88920
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-09T05:22:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88919
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-09T05:21:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88918
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-09T05:20:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88917
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-09T05:13:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88916
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-09T04:18:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88915
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-09T03:59:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88914
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-09T03:27:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88913
copybara-service[bot],"Add `PjRtClient::CompileAndLoad()` variant, and make `PjRtClient::Compile()` delegate to it.","Add `PjRtClient::CompileAndLoad()` variant, and make `PjRtClient::Compile()` delegate to it. This is to prepare for updating `PjRtClient::Compile()` to return a unloaded executable.",2025-03-08T20:58:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88912
copybara-service[bot],Fix assertion error on scatter fold,Fix assertion error on scatter fold,2025-03-08T20:17:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88911
copybara-service[bot],[XLA:GPU] Fix test.,[XLA:GPU] Fix test. `channel_id` doesn't work with the partitioning scheme used in the test. The test fails with: ``` E0000 00:00:1741425380.494901 2701535 status_macros.cc:57] INTERNAL: RET_CHECK failure (xla/service/collective_ops_utils.cc:553) 0 <= partition_id && partition_id < partition_count 4 1 ```,2025-03-08T19:55:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88910
copybara-service[bot],[pjrt] Add PjRtClient::CompileAndLoad variant,[pjrt] Add PjRtClient::CompileAndLoad variant,2025-03-08T10:54:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88909
copybara-service[bot],"Add `PjRtClient::CompileAndLoad()` variant, and make `PjRtClient::Compile()` delegate to it.","Add `PjRtClient::CompileAndLoad()` variant, and make `PjRtClient::Compile()` delegate to it. This is to prepare for updating `PjRtClient::Compile()` to return an unloaded executable.",2025-03-08T10:40:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88908
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T09:46:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88907
pureayu,Non-quantized MobileNetV2 model gives unexpectedly low accuracy in TensorFlow Lite evaluation," 1. **System Information**  **Device**: Xiaomi 14 (Android)  **TensorFlow Installation**: Built from source  2. **Link**  **Evaluation Task Link**: [TensorFlow Lite ImageNet Classification Evaluation](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification)  3. **Problem** After following the instructions on the page, I successfully compiled and ran `run_eval`, but the accuracy is very low. **Here is the output after running the evaluation:**  **Tested 10 images**  **Accuracy**: 0 for all metrics ```bash INFO: Num evaluation runs: 10 INFO: Preprocessing latency: avg=5540(us), std_dev=0(us) INFO: Inference latency: avg=96199(us), std_dev=91761(us) INFO: Top1 Accuracy: 0 INFO: Top2 Accuracy: 0 INFO: Top3 Accuracy: 0 INFO: Top4 Accuracy: 0 INFO: Top5 Accuracy: 0 INFO: Top6 Accuracy: 0 INFO: Top7 Accuracy: 0 INFO: Top8 Accuracy: 0 INFO: Top9 Accuracy: 0 INFO: Top10 Accuracy: 0 ``` **Then I tested with `mobilenet_v2.tflite` (nonquantized model):** ```bash INFO: Num evaluation runs: 10 INFO: Preprocessing latency: avg=5101.7(us), std_dev=0(us) INFO: Inference latency: avg=17079.5(us), std_dev=12222(us) INFO: Top1 Accuracy: 0.1 INFO: Top2 Accuracy: 0.1 INFO: Top3 Accuracy: 0.2 INFO: Top4 Accuracy: 0.2 INFO: Top5 Accuracy: 0.2 INFO: Top6 Accuracy: 0.2 INFO: Top7 Accuracy: 0.2 INFO: Top8 Accuracy: 0.2 INFO: Top9 Accuracy: 0.2 INFO: Top10 Accuracy: 0.2 ``` These results are not consistent with the demo accuracy shown on the page. **Demo output (with `mobilenet_v1_quant.tflite` model)**: ```bash INFO: Num evaluation runs: 300  Total images evaluated INFO: Preprocessing latency: avg=13772.5(us), std_dev=0(us) INFO: Inference latency: avg=76578.4(us), std_dev=600(us) INFO: Top1 Accuracy: 0.733333 INFO: Top2 Accuracy: 0.826667 INFO: Top3 Accuracy: 0.856667 INFO: Top4 Accuracy: 0.87 INFO: Top5 Accuracy: 0.89 INFO: Top6 Accuracy: 0.903333 INFO: Top7 Accuracy: 0.906667 INFO: Top8 Accuracy: 0.913333 INFO: Top9 Accuracy: 0.92 INFO: Top10 Accuracy: 0.923333 ```  4. **Steps to Reproduce** 1. Follow the steps outlined in the evaluation task documentation. 2. Compile and run the `run_eval` binary with the respective model (`mobilenet_v2.tflite` or `mobilenet_v1_quant.tflite`). 3. Observe the output accuracy.  5. **Expected Behavior** I expect that the accuracy for the nonquantized `mobilenet_v2.tflite` model should be higher and closer to the demo output shown in the example above.  6. **Actual Behavior** The nonquantized `mobilenet_v2.tflite` model is giving very low accuracy, and the results are inconsistent with the demo output.  7. **Environment**  **TensorFlow Lite version tf 2.18.0  **Device**: Xiaomi 14, Android version 11  **Model**: `mobilenet_v2.tflite` (nonquantized)  8. **Additional Information**  I have verified that the images and labels are correctly set up and placed.  I followed the exact steps provided in the official documentation, but the results are still lower than expected.",2025-03-08T09:17:55Z,TFLiteConverter,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88906
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T08:28:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88905
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f01e760c0836,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f01e760c0836,2025-03-08T08:06:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88904
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T07:31:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88903
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T07:12:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88902
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T07:03:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88901
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T07:03:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88900
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T07:02:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88899
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T07:01:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88898
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:57:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88897
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:55:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88896
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:55:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88895
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:48:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88894
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:45:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88893
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:37:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88892
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:36:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88891
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:35:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88890
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:34:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88889
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:34:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88888
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:32:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88887
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:31:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88886
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:17:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88885
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:13:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88884
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:10:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88883
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:09:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88882
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T06:03:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88881
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T05:51:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88880
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T05:44:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88879
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T05:25:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88878
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T05:23:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88877
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T05:22:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88876
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T04:17:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88875
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-08T04:14:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88874
copybara-service[bot],Reverts 205cf0e3fb2ca704d51ebedd1a9c11f27dfef8f9,Reverts 205cf0e3fb2ca704d51ebedd1a9c11f27dfef8f9,2025-03-08T02:10:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88873
copybara-service[bot],Make MHLO tests less sensitive to doc strings,Make MHLO tests less sensitive to doc strings Needed for integ of https://github.com/openxla/stablehlo/pull/2738,2025-03-08T01:24:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88872
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@e34c3f6e,Integrate StableHLO at openxla/stablehlo,2025-03-08T01:19:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88871
copybara-service[bot],Add `.bazel` suffix to some `BUILD` files in TSL,Add `.bazel` suffix to some `BUILD` files in TSL,2025-03-08T00:56:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88870
copybara-service[bot],OSS NOOP change,OSS NOOP change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23485 from JackWolfard:clang19upb 42567b5f1fb339adae9ea574749513c62c0287f6,2025-03-08T00:54:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88869
copybara-service[bot],PR #23427: [XLA:GPU] Set command buffer CONDITIONAL op default enabled,PR CC(Build failing with ` ERROR: Config value cuda is not defined in any .rc file`): [XLA:GPU] Set command buffer CONDITIONAL op default enabled Imported from GitHub PR https://github.com/openxla/xla/pull/23427 Copybara import of the project:  3f2b148d6627756c7d151bbcc32ea5064017f708 by Shawn Wang : enable command buffer conditional op default enabled Merging this change closes CC(Build failing with ` ERROR: Config value cuda is not defined in any .rc file`) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23427 from shawnwang18:shawnw/set_conditional_default_command_buffer 3f2b148d6627756c7d151bbcc32ea5064017f708,2025-03-07T23:24:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88868
copybara-service[bot],litert: Share CL environment,litert: Share CL environment Reverts d7d98846ac7e582ef25e3de156ea9efaefeaa94b FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23542 from ROCm:ci_clang19 03a6958a7ef6fde43ec6c20c8eb984b4afa181ff,2025-03-07T23:10:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88867
copybara-service[bot],test to move stablhlo_passes.td req for previous target,test to move stablhlo_passes.td req for previous target,2025-03-07T23:09:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88866
copybara-service[bot],Add internal experimental library,Add internal experimental library,2025-03-07T23:01:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88865
copybara-service[bot],"Move tfl_quantization_driver, originally from quantization_lib, to the rest of the quantization_lib fork in lite","Move tfl_quantization_driver, originally from quantization_lib, to the rest of the quantization_lib fork in lite",2025-03-07T22:27:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88864
copybara-service[bot],Run build_cleaner --action_spec=fix_deps on third_party/tensorflow/compiler/xla/...,Run build_cleaner action_spec=fix_deps on third_party/tensorflow/compiler/xla/... FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23414 from openxla:fix_compilation_with_workspace c5d96a27927699979f09e93248ba389d54795b8e,2025-03-07T22:05:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88863
copybara-service[bot],Remove reference to deleted file in common.bara.sky,Remove reference to deleted file in common.bara.sky FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23414 from openxla:fix_compilation_with_workspace c5d96a27927699979f09e93248ba389d54795b8e,2025-03-07T22:00:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88862
cousteaulecommandant,Fixed-point Softmax() calls exp_on_negative_values() twice,"Fixedpoint `Softmax()` calls `exp_on_negative_values()` twice per element: once to calculate the value of the exp of each element https://github.com/tensorflow/tensorflow/blob/4266e9ab53bb3f7fca0dd816b965b82862b69b4e/tensorflow/lite/kernels/internal/reference/softmax.hL131 and another one a few lines earlier to calculate the sum of all exps https://github.com/tensorflow/tensorflow/blob/4266e9ab53bb3f7fca0dd816b965b82862b69b4e/tensorflow/lite/kernels/internal/reference/softmax.hL109L110 The `exp_on_negative_values()` function can be rather slow, taking most of the time of `Softmax()` (which can be especially concerning in embedded implementations), so calling it twice per element makes the whole `Softmax()` function be about twice as slow as necessary. Could this be optimized so that it is only called once per element?  Store the exp results in a temporary array (for example, reusing `output_data`), computing the sum as they're stored, and then applying the scaling to the precomputed exp results rather than computing them again. (`SoftmaxInt16()` seems to follow a similar approach.)",2025-03-07T21:48:15Z,comp:lite awaiting PR merge,open,0,0,https://github.com/tensorflow/tensorflow/issues/88861
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f01e760c0836,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f01e760c0836,2025-03-07T21:47:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88860
copybara-service[bot],integrate rules into CI,integrate rules into CI,2025-03-07T21:28:03Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/88859,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Make the `IsArrayType()` definition safer.,"Make the `IsArrayType()` definition safer. The definition of this function used to be ```   ... && primitive_type > PRIMITIVE_TYPE_INVALID &&          primitive_type < PrimitiveType_ARRAYSIZE; ``` However, this assumes that: 1. `PRIMITIVE_TYPE_INVALID` is the smallest value in the enum. 2. The enum values starts at 0 and is contiguous. These assumptions are unreliable and can be unintentionally broken (e.g. if we remove a deprecated enum value). Also, that definition is not forward compatible with new enum values: if the code reads an enum value written by a newer version of the code, it may get a value it doesn't know about, which will be outside the range of the enum. In that case, the code will think the type is not an array type, which is most likely incorrect. As a result, we explicitly check for the types that are *not* array types.",2025-03-07T21:24:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88858
copybara-service[bot],Plumbing for has-a between LoadedExec and Exec,"Plumbing for hasa between LoadedExec and Exec A loaded executable should havean executable. The inheritance relationship as it exists today leads to unnecessary complexity in the API design, and plumbing through the same behavior on the LoadedExecutable as in the Executable. This separates out the functionality, delegating calls to the executable contained by LoadedExecutable via the GetExecutable() call.",2025-03-07T21:13:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88857
copybara-service[bot],[XLA] Directly track callers and callees of an HloComputation.,"[XLA] Directly track callers and callees of an HloComputation. Currently there's no direct way to navigate from an HloComputation to its callers and callees. To determine the callees, one must iterate through all of the instructions in a computation, which can be slow. And there is no way to navigate to the callers of a computation other than iterating over all the computations in a module. This change adds absl::btree_map data structures that allow one to navigate from an HloComputation to its callers and callees. For each neighbor, we keep a count of the number of references.",2025-03-07T21:08:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88856
copybara-service[bot],[XLA] Add an online topological sort.,"[XLA] Add an online topological sort. This PR adds an online topological sort implementation based on: Bender, M.A., Fineman, J.T., Gilbert, S. and Tarjan, R.E., 2015. A new approach to incremental cycle detection and related problems. ACM Transactions on Algorithms (TALG), 12(2), pp.122. (https://dl.acm.org/doi/abs/10.1145/2756553). XLA uses (offline) topological sorts for both computations (HloModule::MakeComputationPostOrder) and instructions (HloComputation::MakeInstructionPostOrder), and these are significant time consumers during the HLO pipeline. It is very common for an HLO pass to want to iterate over either computations or instructions in postorder or reverse postorder, and so we repeatedly compute an offline topological sort in many HLO passes. Rather than repeatedly recomputing the topological ordering using an offline topological sort, we can save time if we instead maintain instructions and computations in a topological ordering at all times using an online topological sort. Future PRs will change the HLO data structures to use an online topological sort. The implementation in this PR uses an intrusive data structure that can be embedded into other objects (e.g., HloInstruction or HloComputation).",2025-03-07T21:06:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88855
copybara-service[bot],copy  tensorflow/compiler/mlir/lite:validators to mlir/utils:validators,copy  tensorflow/compiler/mlir/lite:validators to mlir/utils:validators,2025-03-07T20:56:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88854
copybara-service[bot],[XLA] Add Operation Semantics doc for TopK,[XLA] Add Operation Semantics doc for TopK,2025-03-07T20:53:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88853
copybara-service[bot],OSS NOOP change,OSS NOOP change,2025-03-07T20:43:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88852
copybara-service[bot],Deprecate `HloTestBase`.,"Deprecate `HloTestBase`. New tests should utilize `HloPjRtTestBase` where possible. If `RunAndCompare` functionality is required, tests can instantiate `HloPjRtInterpreterReferenceMixin`. Please see `HloRunnerAgnosticTestBase` and `HloRunnerAgnosticReferenceMixin` for generic implementations that can be subclassed for different use cases, if your `HloPjRtTestBase` and/or `HloPjRtInterpreterReferenceMixin` do not meet your needs.",2025-03-07T19:59:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88851
copybara-service[bot],Changes to internal tests.,Changes to internal tests.,2025-03-07T19:55:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88850
copybara-service[bot],PR #23340: [ROCm] Add -Wno-stringop-truncation to build flags,"PR CC(TFLite object detection toco error(the size of detect.tflite is 0kb)): [ROCm] Add Wnostringoptruncation to build flags Imported from GitHub PR https://github.com/openxla/xla/pull/23340 Build error emerged after https://github.com/openxla/xla/commit/bd281e6ecdf5ede173e39f7402e547bbb9e1dc90. Added a workaround until the issue is fixed in upb. Log: ``` ERROR: /root/.cache/bazel/_bazel_root/217377b0e928b171b843eb11ea7bc36e/external/upb/BUILD:57:11: Compiling upb/upb.c failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing CppCompile command (from target @//:upb) external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc U_FORTIFY_SOURCE fstackprotector Wall Wunusedbutsetparameter Wnofreenonheapobject fnoomitframepointer ... (remaining 33 arguments skipped) In file included from /usr/include/string.h:535,                  from external/upb/upb/upb.h:16,                  from external/upb/upb/upb.c:2: In function ‘strncpy’,     inlined from ‘upb_status_seterrmsg’ at external/upb/upb/upb.c:40:3: /usr/include/x86_64linuxgnu/bits/string_fortified.h:95:10: error: ‘__builtin_strncpy’ specified bound 127 equals destination size [Werror=stringoptruncation]    95                                    ~~~~~~~~~~~~~~~~~~~~~~~~~ ``` There is a similar workaround in Jax as well > https://github.com/jaxml/jax/pull/4974/files Copybara import of the project:  bb1d0bbb7913e1feb79f4e9cb0a748a030ad0fd2 by Milica Makevic : Add Wnostringoptruncation to build flags for ROCm Merging this change closes CC(TFLite object detection toco error(the size of detect.tflite is 0kb)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23340 from ROCm:ci_hotfix_250304 bb1d0bbb7913e1feb79f4e9cb0a748a030ad0fd2",2025-03-07T19:46:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88849
belitskiy,Backport Windows builds into 2.18 for 2.18.1,"This is being backported because Windows build jobs were added after 2.18, whereas 2.18 wheels were built and uploaded by Intel. Intel is no longer in charge of building any wheels, so this backport is necessary to be able to provide 2.18.1 wheels for Windows.",2025-03-07T19:44:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88848
copybara-service[bot],"Allows combiner names starting with ""custom"".","Allows combiner names starting with ""custom"".",2025-03-07T19:01:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88847
copybara-service[bot],Add `HloRunnerInterface::ExecutablesAreEquivalent`.,Add `HloRunnerInterface::ExecutablesAreEquivalent`. This patch adds a new method `HloRunnerInterface::ExecutablesAreEquivalent` that tells us if two executables match based on a runnerspecific equivalence criteria. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/24059 from dimvar:fixgpucompilertestforblackwell 4297c120c719c87e9607f3eaf7a0298eead048b2,2025-03-07T18:52:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88846
copybara-service[bot],"Get rid of call to deprecated method in Eigen. The method was only called by this test, and not used in actual code.","Get rid of call to deprecated method in Eigen. The method was only called by this test, and not used in actual code.",2025-03-07T18:50:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88845
copybara-service[bot],PR #23485: Add `-Wno-c23-extensions` flag with Clang >= 19,"PR CC(未找到相关数据): Add `Wnoc23extensions` flag with Clang >= 19 Imported from GitHub PR https://github.com/openxla/xla/pull/23485 XLA's version of gRPC depends on an ancient version of upb which is unable to compile on Clang >= 19 without this flag. Also, added clarification for why the `Wnognuoffsetofextensions` is used for Clang 16 > 18. To test this change, run the following locally: ```sh $ docker run rm it \     name xla \     v $PWD:/xla \     w /tmp \     silkeh/clang:19 $ wget https://github.com/bazelbuild/bazelisk/releases/download/v1.25.0/bazeliskamd64.deb $ dpkg i bazeliskamd64.deb $ cd /xla $ ./configure.py backend=CPU $ bazel \     build \     repo_env=HERMETIC_PYTHON_VERSION=3.11 \     spawn_strategy=sandboxed \     //:upb ``` Copybara import of the project:  42567b5f1fb339adae9ea574749513c62c0287f6 by Jack Wolfard : Add `Wnoc23extensions` flag with Clang >= 19 XLA's version of gRPC depends on an ancient version of upb which is unable to compile on Clang >= 19 without this flag. Also, added clarification for why the `Wnognuoffsetofextensions` is used for Clang 16 > 18. Merging this change closes CC(未找到相关数据) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23485 from JackWolfard:clang19upb 42567b5f1fb339adae9ea574749513c62c0287f6",2025-03-07T18:04:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88844
copybara-service[bot],#litert Detect memory sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`.,"litert Detect memory sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`. Trying to load a library using `RTLD_DEEPBIND` is not supported by memory sanitizers. In an effort to enable testing we strip the flag. If this leads to unintended behaviour, either remove the `RTLD_DEEPBIND` flag or run without a memory sanitizer. See https://github.com/google/sanitizers/issues/611 for more information.",2025-03-07T17:47:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88843
copybara-service[bot],#litert Fix a dangling C string pointer.,"litert Fix a dangling C string pointer. When `lib_qnn_htp_so_path` is defined, `auto` resolves to a `const char*`. The following block has creates a `vector` of `string`s and, when the library path is found, assigns the first `string::c_str()` in the vector to `lib_qnn_htp_so_path`. At the end of that block, the vector goes out of scope and `lib_qnn_htp_so_path` becomes a dangling pointer.",2025-03-07T17:39:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88842
copybara-service[bot],[XLA:GPU] Add one-shot kernel implementation to RaggedAllToAll.,[XLA:GPU] Add oneshot kernel implementation to RaggedAllToAll. The kernel uses a CUDA kernel for an efficient implementation of ra2a on single host when direct peer access between GPUs is available.,2025-03-07T17:37:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88841
copybara-service[bot],#litert Have helper functions in `qnn_manager.cc` return an expected.,litert Have helper functions in `qnn_manager.cc` return an expected. This allows passing the error messages up to the callers.,2025-03-07T17:28:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88840
copybara-service[bot],Override LITERT_HAS_OPENCL_SUPPORT to be disabled if new define is set,Override LITERT_HAS_OPENCL_SUPPORT to be disabled if new define is set,2025-03-07T17:22:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88839
copybara-service[bot],[PJRT] Support CHLO<->VHLO roundtripping of select CLHOs with HW support.,[PJRT] Support CHLOVHLO roundtripping of select CLHOs with HW support. This also makes PJRT serialization and lowering to HLO unidirectional. I.e. serialization is now `CHLO>StableHLO` instead of `CHLO>MHLO>StableHLO`.,2025-03-07T17:22:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88838
copybara-service[bot],Log the error message when an `ErrorStatusReturnHelper` is converted to `LiteRtStatus`.,"Log the error message when an `ErrorStatusReturnHelper` is converted to `LiteRtStatus`. This is useful in `LITERT_ASSIGN_OR_RETURN` and `LITERT_RETURN_IF_ERROR`, when returning an `Unexpected` or an `Error` in a function that has C linkage. It avoids dropping the error explanation when it is propagated.",2025-03-07T17:14:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88837
copybara-service[bot],PR #23477: [Hermetic CUDA] Skip arch if not in redistrib json,"PR CC(Tensorflow's Estimator stops training): [Hermetic CUDA] Skip arch if not in redistrib json Imported from GitHub PR https://github.com/openxla/xla/pull/23477 Given a custom CUDA redistribution which only specifies a distribution for `linuxx86_64` and not `linuxsbsa` nor `linuxaarch64`, a key error occurs due to `_get_redistribution_urls` expecting all architectures to be present for each subproject. ```json // fragment of the custom CUDA redistribution JSON {     ""cuda_cccl"": {         ""linuxx86_64"": {             ""relative_path"": ""..."",             ""sha256"": ""...""         },     }, } ``` This can be resolved by adding a key for each arch mapping to an empty dict if there is no artifact for that arch. However, this solution is awkward and adds clutter to the custom CUDA redistribution. ```json {   ""cuda_cccl"": {       ""linuxx86_64"": {           ""relative_path"": ""..."",           ""sha256"": ""...""       },       ""linuxaarch64"": {},       ""linuxsbsa"": {},   }, ``` Instead, allow for `_get_redistribution_urls` to skip the arch if it is not present in the redistribution. Copybara import of the project:  2a00755a0aba650588b2ba22340990980f18b472 by Jack Wolfard : [Hermetic CUDA] Skip arch if not in redistrib json Given a custom CUDA redistribution which only specifies a distribution for `linuxx86_64` and not `linuxsbsa` nor `linuxaarch64`, a key error occurs due to `_get_redistribution_urls` expecting all architectures to be present for each subproject. ```json // fragment of the custom CUDA redistribution JSON {     ""cuda_cccl"": {         ""linuxx86_64"": {             ""relative_path"": ""..."",             ""sha256"": ""...""         },     }, } ``` This can be resolved by adding a key for each arch mapping to an empty dict if there is no artifact for that arch. However, this solution is awkward and adds clutter to the custom CUDA redistribution. ```json {   ""cuda_cccl"": {       ""linuxx86_64"": {           ""relative_path"": ""..."",           ""sha256"": ""...""       },       ""linuxaarch64"": {},       ""linuxsbsa"": {},   }, ``` Instead, allow for `_get_redistribution_urls` to skip the arch if it is not present in the redistribution. Merging this change closes CC(Tensorflow's Estimator stops training) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23477 from JackWolfard:hermeticcudaarch 2a00755a0aba650588b2ba22340990980f18b472",2025-03-07T16:38:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88836
wonjeon,[mlir][tosa] Add Rescale Attribute changes to TOSA legalizations and Switch zero point of avgpool2d to input variable type,We've been pushing TOSA v1.0 LLVM patches to upstream. This PR includes commit(s) to keep the following operator(s) to be aligned with LLVM: [mlir][tosa] Make RESCALE op input_unsigned and output_unsigned attributes required https://github.com/llvm/llvmproject/pull/129339 [mlir][tosa] Switch zero point of avgpool2d to input variable type https://github.com/llvm/llvmproject/pull/128983,2025-03-07T16:31:09Z,kokoro:force-run ready to pull size:M,closed,0,1,https://github.com/tensorflow/tensorflow/issues/88835,"Local testing done successfully: INFO: Analyzed 33 targets (2 packages loaded, 33382 targets configured). INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 142.222s, Critical Path: 96.99s INFO: 426 processes: 1 internal, 425 local. INFO: Build completed successfully, 426 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test         PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test             PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test      PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test                PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test   PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test           PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test  PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test      PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test         PASSED in 0.1s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 2.5s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test     PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test        PASSED in 0.1s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test   PASSED in 0.2s Executed 17 out of 17 tests: 17 tests pass."
copybara-service[bot],"[XLA] Make HloComputation::set_parent() private, and make HloModule a friend of HloComputation.","[XLA] Make HloComputation::set_parent() private, and make HloModule a friend of HloComputation. We want to enforce the invariant that an HloComputation has a parent if and only if it is a member of a module. The easiest way to do that is to forbid anyone from calling set_parent() other than HloModule itself. Making HloModule a friend of HloComputation is perhaps a bit unfortunate because it grants visibility to many other things, but I'd much prefer that to granting access to set_parent() to everything else in XLA.",2025-03-07T15:56:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88834
copybara-service[bot],[XLA:CPU] Multithreaded NanoRt Execute blocks until execute thunks are finished.,[XLA:CPU] Multithreaded NanoRt Execute blocks until execute thunks are finished.,2025-03-07T15:36:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88833
copybara-service[bot],Fix saving location info on function arguments HLO->MLR.,Fix saving location info on function arguments HLO>MLR.,2025-03-07T15:31:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88832
copybara-service[bot],Integrate LLVM at llvm/llvm-project@94c937d32195,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 94c937d32195,2025-03-07T15:28:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88831
copybara-service[bot],Integrate LLVM at llvm/llvm-project@94c937d32195,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 94c937d32195,2025-03-07T15:07:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88830
copybara-service[bot],Remove axes sized 1 in the sharding but not the mesh itself,Remove axes sized 1 in the sharding but not the mesh itself,2025-03-07T15:01:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88829
copybara-service[bot],PR #22541: [ROCm] Cleanup atomics support,"PR CC(Java process crashes during model loading): [ROCm] Cleanup atomics support Imported from GitHub PR https://github.com/openxla/xla/pull/22541 Weaken the ordering barriers to match what atomicAdd does on rocm. Emulate fp16 atomic on top of packed fp16 atomic where possible. Also for bfloat16 atomics, albeit those don't get matched right now due to FloatNormalization. Left in support for fp16 and bfloat16 vector atomics. We might enable the vectorization for them in the future if we can prove the access satisfies 4byte aligment. Copybara import of the project:  06907ef930c76c824788e86db8d3b30eeb141175 by Dragan Mladjenovic : [ROCm] Cleanup atomics support Merging this change closes CC(Java process crashes during model loading) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22541 from ROCm:atomics_cleanup 06907ef930c76c824788e86db8d3b30eeb141175",2025-03-07T14:45:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88828
copybara-service[bot],"Fix ""NOLINT"" comment","Fix ""NOLINT"" comment",2025-03-07T13:49:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88827
copybara-service[bot],lite: Add `flags` to LiteRtCompilerPluginT,lite: Add `flags` to LiteRtCompilerPluginT This is used to handle custom compiler flags.  apply_plugin.sh enabled the compilers flags to be passed as cli parameter  adapter.h made changes to the compile function signature to support the passing of flags to CompileFlatbuffer  adapter_test.  compiler_plugin.. Updated the plugin to hold the flags as state 2. Implemented the `LiteRtCompilerPluginSetFlags` function 3. Used the flags for creating the compiled artifacts,2025-03-07T13:13:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88826
copybara-service[bot],[XLA:CPU] Run simplification pipeline after scatter expansion,[XLA:CPU] Run simplification pipeline after scatter expansion,2025-03-07T12:03:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88825
copybara-service[bot],[XLA:GPU] run new nest_get_fusion pass when experimental flag is enabled,[XLA:GPU] run new nest_get_fusion pass when experimental flag is enabled,2025-03-07T11:34:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88824
copybara-service[bot],Change FusionDeduplicationCache to use HloInstruction pointers.,"Change FusionDeduplicationCache to use HloInstruction pointers. This makes it more flexible as we can then also call UpdateFusedInstructionId() if one of the ""old"" HloInstructions has been deleted already.",2025-03-07T11:08:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88823
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-07T10:29:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88822
copybara-service[bot],[XLA] add details to operands number failure,[XLA] add details to operands number failure,2025-03-07T10:23:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88821
copybara-service[bot],"Add back MHLO dialect where needed, as we still use mhlo::CopyOp","Add back MHLO dialect where needed, as we still use mhlo::CopyOp",2025-03-07T10:20:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88820
copybara-service[bot],Refactor: Replace transient TensorFlow dependencies with TSL in `profiler/convert/trace_viewer/`,Refactor: Replace transient TensorFlow dependencies with TSL in `profiler/convert/trace_viewer/` This change replaces all instances of `tensorflow::` with `tsl::` within the `profiler/convert/trace_viewer/` directory. This refactor aims to remove transient dependencies on TensorFlow and enhance modularity.,2025-03-07T10:19:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88819
copybara-service[bot],PR #23349: Add helper to analyze induction var dependencies.,"PR CC(my CNN models print NAN. what is reason??? i don't know what is it.): Add helper to analyze induction var dependencies. Imported from GitHub PR https://github.com/openxla/xla/pull/23349 This adds a helper function that checks if an HLO instruction is a pure functional dependency of a  while loop's induction variable. See the comment on `ResolveFunctionalDependencyOnInductionVariable` in `ir_emission_utils.h` for more details. For additional context, see https://github.com/openxla/xla/compare/main...jreiffers:xla:memcpy and the companion document https://docs.google.com/document/d/1E2_Jt_Dw4VbPXPVktNWhtsDEtIpNkurCMGGyvMV4JA. Copybara import of the project:  c38098afc10e0ff51b9262a06257638a489f4bdc by Johannes Reifferscheid : Add helper to analyze induction var dependencies. This adds a helper function that checks if an HLO instruction is a pure functional dependency of a  while loop's induction variable. See the comment on `ResolveFunctionalDependencyOnInductionVariable` in `ir_emission_utils.h` for more details. For additional context, see https://github.com/openxla/xla/compare/main...jreiffers:xla:memcpy and the companion document https://docs.google.com/document/d/1E2_Jt_Dw4VbPXPVktNWhtsDEtIpNkurCMGGyvMV4JA.  3d5a80b16c38c563342db1db8bfea0771c625a90 by Johannes Reifferscheid : Add a note about while_loop_trip_count_annotator. Merging this change closes CC(my CNN models print NAN. what is reason??? i don't know what is it.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23349 from jreiffers:functionaldep 3d5a80b16c38c563342db1db8bfea0771c625a90",2025-03-07T10:08:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88818
yjiangling,The CTC loss is Numeric Instability inTensorFlow2.x when use sparse tensor as input," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf2.9，tf2.12，tf2.15  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.2  Mobile device _No response_  Python version 3.8, 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA11.6，CUDA11.8，CUDA12.0  GPU model and memory RTX3080，A10，A100，L40，......  Current behavior? Hi, all,         I find when train CTC model in the TF2.x environment，if the dense of the labels are feed into the function `tf.nn.ctc_loss`，it will assume a lot of GPU memory and the train speed is very slow; when the labels are convert to sparse tensor and feed to the function, the train speed will be much faster, but it is very easy to get NAN during the training, resulting the failure of the train. When compare the reult of ctc loss for this two type input, as well as use ctc loss function in TF1.x (tf.compat.v1.nn.ctc_loss), it seems OK. That is to say, when use sparse tensor as input for tf.nn.ctc_loss function in TF2.x, the result is numeric unstable? Or there are some problem in the gradient computation for it? Which leading the gradient explosion. And it seems that it's a common problem in TensorFlow2, because we have tried it in TensorFlow2.9，TensorFlow2.12 and TensorFlow2.15, both of them have this issue. All the  train ard conducted with the same network, same training parameters and same train dataset.   Standalone code to reproduce the issue When use sparse tensor as input in tf.nn.ctc_loss function, the output of the Conformer network (ctc_logits) will become **nan** after a few train steps, which lead to the train loss become **nan** and the failure of the train... Here is the code we used: 	ctc_logits, the final output of the conformer network, with shape [frames, batch_size, num_labels] 	xlen, length of input sequence in ctc_logits, with shape [batch_size] 	ys, the labels of each train sample, with shape [batch_size, max_label_seq_length]  	ylen, length of reference label sequence in ys, with shape [batch_size] 	ys_sparse = tf.sparse.from_dense(ys)  convert dense tensor to sparse tensor 	ctc_loss = tf.compat.v1.nn.ctc_loss(labels=ys_sparse, inputs=ctc_logits, sequence_length=xlen, ignore_longer_outputs_than_inputs=True)  train with ctc loss function in TF1.x, the labels use dense or sparse tensor are all fine 	ctc_loss = tf.nn.ctc_loss(labels=ys, logits=ctc_logits, label_length=ylen, logit_length=xlen, blank_index=1)  train with ctc loss function in TF2.x, use dense as input for labels, it OK 	ctc_loss = tf.nn.ctc_loss(labels=ys_sparse, logits=ctc_logits, label_length=None, logit_length=xlen, blank_index=1)  train with ctc loss function in TF2.x, use sparse tensor as input for labels will leadingt to gradient explosion 1. Why this happend? Is it a common phenomenon in TF2.x? 2. How can I solve it? By the way, I also tried to add gradient clip in optimizer function like `optimizer = tf.keras.optimizers.Adam(learning_rate=1e4, clipnorm=1.0)` or `optimizer = tf.keras.optimizers.Adam(learning_rate=1e4, clipvalue=1.0)`, but does't work. I will be great appreciate if anyone can give some suggestions. kindly help...  Relevant log output Even when we use a pretrained model with ctc loss function in TF1.x or dense input in TF2,x, and then convert to use sparse tensor as input in TF2.x, the gradient will explode soon... !Image",2025-03-07T09:38:27Z,stat:awaiting response type:bug stale comp:ops TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88817,"Hi **** , Apologies for the delay, and thanks for raising your concern here. Could you please provide the complete code snippet? This would help us investigate further. Additionally, I noticed that you are using an older version of TensorFlow. Could you try updating to the latest version and let us know if the issue still persists? Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Integrate LLVM at llvm/llvm-project@35842f354ecc,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 35842f354ecc,2025-03-07T09:02:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88816
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-07T08:58:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88815
copybara-service[bot],[XLA:GPU] Fix msan in recently added tests.,[XLA:GPU] Fix msan in recently added tests.,2025-03-07T08:40:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88814
copybara-service[bot],PR #23414: [GPU] Let cuDNN fusion compiler process graphs with assigned workspace. ,PR CC(Improve shape function of tf.sparse_reduce_sum): [GPU] Let cuDNN fusion compiler process graphs with assigned workspace.  Imported from GitHub PR https://github.com/openxla/xla/pull/23414 This enables running optimized HLO. Copybara import of the project:  2933f6f84e612d0e47efb6875ea7c272c719edff by Ilia Sergachev : [GPU][NFC] Cleanup a test.  c5d96a27927699979f09e93248ba389d54795b8e by Ilia Sergachev : [GPU] Let cuDNN fusion compiler process graphs with assigned workspace. This enables running optimized HLO. Merging this change closes CC(Improve shape function of tf.sparse_reduce_sum) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23414 from openxla:fix_compilation_with_workspace c5d96a27927699979f09e93248ba389d54795b8e,2025-03-07T08:31:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88813
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-07T08:09:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88812
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-07T08:09:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88811
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-07T08:09:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88810
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-07T08:08:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88809
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-07T08:07:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88808
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-07T08:06:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88807
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-07T08:04:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88806
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-07T07:48:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88804
gaikwadrahul8,Fix 03 broken links in object_detection.md,"Hi, Team I found 03 broken documentation links in this file object_detection.md so I have updated those broken links to functional link. Please review and merge this change as appropriate. Thank you for your consideration.",2025-03-07T07:36:31Z,comp:lite ready to pull size:XS,closed,0,3,https://github.com/tensorflow/tensorflow/issues/88803,"Hi,   I apologize for the delay in my response, I am finding broken links sometimes while going through Github documentations and sometimes via website documentations manually. Thank you","The issue is that GitHub links want to have `.md` at the end, since they point to a file. But this likely breaks on the website, where the link must be to an `.html`. Rather than then having to create churn and undo these changes, can you first check that a `.md` link from one of the previous PRs is working on the website? If not, let's revert all these changes.","Hi,   I cross verified this PR https://github.com/tensorflow/tensorflow/pull/86234 changes against the corresponding website page and it seems like working as expected if I am missing something here please let me know. Thank you"
copybara-service[bot],"Folds dynamic_slice(cst, *) where cst is a splat.","Folds dynamic_slice(cst, *) where cst is a splat.",2025-03-07T07:05:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88802
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-07T06:35:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88801
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-07T06:22:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88800
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-07T05:31:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88799
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-07T05:16:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88798
copybara-service[bot],Fold mhlo.reduce with an empty body and constant return value.,"Fold mhlo.reduce with an empty body and constant return value. This change targets reduce ops with an empty body and constant return values (or inputs). In such a scenario, the reduce operation is simply creating a new constant with the initial values provided by the second argument. This new function is very similar in nature to `tryFoldOutsideValuesReduction`, with the difference being that we go a step further in the case that the return value is a constant.",2025-03-07T04:32:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88797
xxHn-pro,cuSteamSynchronize take tons of time," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.14  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.6  GPU model and memory 4070S  Current behavior? I am trying to benchmark the inference.  A simple code as  ``` // Configure a `CallableHandle` that feeds from and fetches to a device. Status SetupCallable(std::unique_ptr& session,                      std::vector& input_info,                      std::vector& output_info,                      const string& device_name,                      bool input_from_device,                      bool output_to_host,                      tensorflow::Session::CallableHandle* handle) {   tensorflow::CallableOptions opts;   for (const auto& info : input_info) {     const string& name = info.name();     opts.add_feed(name);     if (input_from_device) {       opts.mutable_feed_devices()>insert({name, device_name});     }   }   for (const auto& info : output_info) {     const string& name = info.name();     opts.add_fetch(name);     if (!output_to_host) {       opts.mutable_fetch_devices()>insert({name, device_name});     }   }   opts.set_fetch_skip_sync(true);   return session>MakeCallable(opts, handle); } ``` The inference is run by ``` start_time = std::chrono::steady_clock::now();       TFTRT_ENSURE_OK(         bundle.session>RunCallable(handle, inputs_device, &outputs, nullptr));       // Sync, as `set_fetch_skip_sync(false)` is currently not implemented       TFTRT_ENSURE_OK(device>Sync());       end_time = std::chrono::steady_clock::now(); ``` The profile is collected by `nsys profile w true t cuda,nvtx,cudnn,cublas f true x true o profile_c /opt/tensorflow/tensorflowsource/bazelbin/tensorflow/examples/image_classification/MiniBatch/mini_tftrt model_path=""./resnet50_saved_model_RT"" batch_size=64 output_to_host=False` And I found that cuSteamSynchronize takes most of time, as shown below: profile_c.zip !Image I think the real computation is done and the GPU is wasting its time. Is that right? I don't see any other kernel working. So I try to skip that Sync by setting `opts.set_fetch_skip_sync(true);`. However, the  cuSteamSynchronize  is still on there. No mater whether `device>Sync()` is used or not.  The time consumption and cuSteamSynchronize  are always unchanged even I set the output to host. Here is the code and readme to reproduce the issue. MiniBatch2.zip  Standalone code to reproduce the issue ```shell See the zip file at the end. ```  Relevant log output ```shell ```",2025-03-07T04:27:50Z,type:bug TF2.14,open,0,2,https://github.com/tensorflow/tensorflow/issues/88796,"Hi **pro** , Apologies for the delay, and thanks for raising your concern here.I noticed a version compatibility issue. I am attaching the official documentation for your reference, please verify all compatibility requirements. Additionally, I see that you are using an older version of TensorFlow (2.14). Could you please try updating to the latest version for better results? Thank you!","Hi  , Thanks for your reply. Actually I try the same code at other machine. And there are more information from the new test, which indicate that CUDA kernel is running all the time during the waiting of tensorRT kernel. I think the tensorflow is running asynchronously. It turns out that is not a problem of cuSteamSynchronize !Image PS, the new machine with better GPU uses singularity to run the docker. It runs real CentOS Linux instead of WSL. It is still not clear why I can not get all information at my first try. Anyway, Thanks you. If there is any misunderstand above, pls correct it."
copybara-service[bot],Integrate LLVM at llvm/llvm-project@35842f354ecc,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 35842f354ecc,2025-03-07T03:49:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88795
copybara-service[bot],"Do not add min_runtime_version metadata twice when exporting to flatbuffer, if this attribute exists in the mlir::Module, because the metadata is generated at flatbuffer export time.","Do not add min_runtime_version metadata twice when exporting to flatbuffer, if this attribute exists in the mlir::Module, because the metadata is generated at flatbuffer export time. In the exporter, we generate flatbuffer first and then edit this `min_runtime_version` field. Before this CL, we may generate invalid flatbuffer if we have this attribute in the model and the value is not 16 bytes.",2025-03-07T03:04:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88794
tytyr12,I'm a spammer,"**System information**  Android Device information (use `adb shell getprop ro.build.fingerprint`   if possible):  TensorFlow Lite in Play Services SDK version (found in `build.gradle`):  Google Play Services version   (`Settings` > `Apps` > `Google Play Services` > `App details`): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to or attach code demonstrating the problem. **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2025-03-07T02:50:10Z,invalid,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88793
copybara-service[bot],PR #23423: Skip constant folding for instructions that cannot be safely removed,"PR CC(Should use '_train_distribution' because Estimator doesn't have attribute '_distribution'): Skip constant folding for instructions that cannot be safely removed Imported from GitHub PR https://github.com/openxla/xla/pull/23423 **Description:**   The constant folding pass fails for one of our real models during internal testing.   `HloConstantFolding` removes replaced instructions using `computation.RemoveInstruction(instr)`. Internally, `RemoveInstruction` calls `RemoveInstructionImpl` without forcing removal (`ignore_safety_check=false`). If the instruction is not **safely removable**, `RemoveInstructionImpl` crashes the application.   To prevent this crash, I added an `IsSafelyRemovable` check to the existing list of Constant Folding prechecks.  To Reproduce model.hlo to reproduce the issue ``` HloModule m %AddComputation.27823 (x.735: f32[], y.735: f32[]) > f32[] {   %x.735 = f32[] parameter(0)   %y.735 = f32[] parameter(1)   ROOT %add.715 = f32[] add(f32[] %x.735, f32[] %y.735) } ENTRY main {   %p.0 = f32[8,8]{1,0} parameter(0), frontend_attributes={input_name=""input0""}   %p821 = f32[] parameter(1), frontend_attributes={input_name=""input821""}   %copy.1104 = f32[] copy(f32[] %p821)   %constant.3010 = f32[] constant(0)   %reduce.734 = f32[] reduce(%p.0, f32[] %constant.3010), dimensions={0,1}, to_apply=%AddComputation.27823   %add.1649 = f32[] add(f32[] %copy.1104, f32[] %reduce.734)   %constant.3833 = f32[] constant(0)   %copy.2310 = f32[] copy(f32[] %constant.3833), controlpredecessors={%copy.1104}   ROOT %tuple.17 = (f32[], f32[]) tuple(f32[] %add.1649, f32[] %copy.2310) } ``` cmd to repro the crash on CPU ``` xla/tools/run_hlo_module platform=CPU model.hlo ``` Error: ``` E0000 00:00:1741222471.912307    5276 status_macros.cc:57] INTERNAL: RET_CHECK failure  (xla/hlo/ir/hlo_computation.cc:503) ignore_safety_check  IsSafelyRemovable(instruction) cannot remove instruction:  %copy.2310 = f32[] copy(f32[] %constant.3833), controlpredecessors={%p821} ``` Copybara import of the project:  c0c3cac1ebf28623ca20c31993fc98da93ae27b8 by Alexander Pivovarov : Skip constant folding for instructions that cannot be safely removed Merging this change closes CC(Should use '_train_distribution' because Estimator doesn't have attribute '_distribution') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23423 from apivovarov:skip_constant_folding_if_not_IsSafelyRemovable c0c3cac1ebf28623ca20c31993fc98da93ae27b8",2025-03-07T02:44:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88792
copybara-service[bot],PR #23427: [XLA:GPU] Set command buffer CONDITIONAL op default enabled,PR CC(Build failing with ` ERROR: Config value cuda is not defined in any .rc file`): [XLA:GPU] Set command buffer CONDITIONAL op default enabled Imported from GitHub PR https://github.com/openxla/xla/pull/23427 Copybara import of the project:  3f2b148d6627756c7d151bbcc32ea5064017f708 by Shawn Wang : enable command buffer conditional op default enabled Merging this change closes CC(Build failing with ` ERROR: Config value cuda is not defined in any .rc file`) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23427 from shawnwang18:shawnw/set_conditional_default_command_buffer 3f2b148d6627756c7d151bbcc32ea5064017f708,2025-03-07T02:34:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88791
copybara-service[bot],PR #23426: [XLA:GPU] set CUBLASLT command buffer as default enabled,PR CC(problems about tf.nn.softmax): [XLA:GPU] set CUBLASLT command buffer as default enabled Imported from GitHub PR https://github.com/openxla/xla/pull/23426 Copybara import of the project:  3d2f748916dc6a9c72c681f9bd9fc46f03d396ee by Shawn Wang : set CUBLASLT command buffer as default enabled Merging this change closes CC(problems about tf.nn.softmax) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23426 from shawnwang18:shawnwn/enable_command_buffer_cublaslt_default 3d2f748916dc6a9c72c681f9bd9fc46f03d396ee,2025-03-07T02:12:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88790
copybara-service[bot],Run HLO tests on GPU T4 on presubmit,Run HLO tests on GPU T4 on presubmit,2025-03-07T02:01:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88789
copybara-service[bot],Mark delegate_provider_hdr and its dependencies compatible with non_prod.,Mark delegate_provider_hdr and its dependencies compatible with non_prod.,2025-03-07T01:57:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88788
copybara-service[bot],Add tensorflow/profiler as a dependency for tensorflow,"Add tensorflow/profiler as a dependency for tensorflow As part of an ongoing effort to decouple our ml profiler code from tensorflow, this PR lays the foundation for moving code into tensorflow/profiler. This PR touches code within tensorflow/tensorflow and openxla/xla. Here is an outline of the changes for each repo. tensorflow/tensorflow: * Grant Visibility to ///xprof/(pywrap) openxla/xla: * Functionally a noop. Grants visibility to xprof, however all uses are wrapped in `internal_visibility`.",2025-03-07T01:54:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88787
copybara-service[bot],keep presubmits happy,keep presubmits happy,2025-03-07T01:50:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88786
copybara-service[bot],Create xprof/ subdirectory for bazel builds,Create xprof/ subdirectory for bazel builds,2025-03-07T01:46:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88785
copybara-service[bot],keep presubmits happy,keep presubmits happy,2025-03-07T01:26:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88784
copybara-service[bot],[XLA] Allow sharding to propagate across pin to device memory and pin to vmem custom calls.,[XLA] Allow sharding to propagate across pin to device memory and pin to vmem custom calls.,2025-03-07T00:59:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88783
copybara-service[bot],Internal code change,Internal code change,2025-03-07T00:51:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88782
copybara-service[bot],Switch `cc_test` to `xla_cc_test` to enforce best practices in OpenXLA.,Switch `cc_test` to `xla_cc_test` to enforce best practices in OpenXLA.,2025-03-07T00:19:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88781
jsuj1th,import error," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.8  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior?  ImportError                               Traceback (most recent call last) File c:\Users\SUJITH\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[1], line 3       1  !pip install tensorflow > 3 import tensorflow as tf       4 from tensorflow.keras.applications import MobileNetV3Small       5 from tensorflow.keras.models import Model File c:\Users\SUJITH\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File c:\Users\SUJITH\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:85      83     sys.setdlopenflags(_default_dlopen_flags)      84 except ImportError: > 85   raise ImportError(      86       f'{traceback.format_exc()}'      87       f'\n\nFailed to load the native TensorFlow runtime.\n'      88       f'See https://www.tensorflow.org/install/errors '      89       f'for some common causes and solutions.\n'      90       f'If you need help, create an issue '      91       f'at https://github.com/tensorflow/tensorflow/issues '      92       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""c:\Users\SUJITH\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.   Standalone code to reproduce the issue ```shell pip install tensorflow ```  Relevant log output ```shell ```",2025-03-06T23:45:28Z,stat:awaiting response type:build/install stale TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88780,"Hi ****, Apologies for the delay, and thanks for raising your concern here. I noticed that you are using an older version of TensorFlow (2.8). Could you please check with the latest version for better results? Also, please verify all compatibility requirements. I am attaching the official documentation for your reference. Additionally, a similar issue is currently being discussed, please follow that thread for further updates. CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Please always search for duplicate issues. Please do a minimum of effort for that and for properly formatting the issue.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Internal only change,Internal only change,2025-03-06T23:42:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88779
copybara-service[bot],Switch `cc_test` to `strict_cc_test` in internal XLA to enforce best practices.,Switch `cc_test` to `strict_cc_test` in internal XLA to enforce best practices. Also fix bugs revealed by this change.,2025-03-06T23:38:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88778
copybara-service[bot],Internal change only.,Internal change only.,2025-03-06T23:30:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88777
copybara-service[bot],Handle the `pos` attr for composite builders.,Handle the `pos` attr for composite builders.,2025-03-06T23:12:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88776
copybara-service[bot],dispatch_api: Fix nullptr reference,dispatch_api: Fix nullptr reference Assigning nullptr to std::string is not recommended.,2025-03-06T22:55:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88775
copybara-service[bot],Internal change only.,Internal change only. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88221 from jiunkaiy:dev/weilhuan/more_op_builders 542a108226dcb1c31abc30578cba2b74b20e8e0b,2025-03-06T22:44:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88774
copybara-service[bot],Add `.bazel` suffix to some `third_party` files,Add `.bazel` suffix to some `third_party` files,2025-03-06T21:45:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88773
copybara-service[bot],litert: Store async execution mode in ExternalLiteRtBufferContext,litert: Store async execution mode in ExternalLiteRtBufferContext User's intention is store by SetAsyncExecutionMode(). And DelegateKernel refers it with IsAsyncExecutionMode().,2025-03-06T21:45:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88772
copybara-service[bot],Integrate LLVM at llvm/llvm-project@87976ca45f4f,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 87976ca45f4f,2025-03-06T21:36:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88771
copybara-service[bot],[XLA:CPU] Add CallKernelEmitter and use it to emit small while loops,[XLA:CPU] Add CallKernelEmitter and use it to emit small while loops,2025-03-06T21:27:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88770
copybara-service[bot],Implement PjRtClient::Compile in TFRT GPU.,Implement PjRtClient::Compile in TFRT GPU.,2025-03-06T21:02:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88769
copybara-service[bot],PR #21638: Add the hlo verifier before host offloader to check host memory space,"PR CC([XLA] ResourceExhaustedError when trying to define a Sequential model in Keras under jit_scope context manager): Add the hlo verifier before host offloader to check host memory space Imported from GitHub PR https://github.com/openxla/xla/pull/21638 Ensure No Instructions Have Host Memory Space S(5) Before Host Offloader This change verifies that no instruction possesses host memory space S(5) prior to the host offloader pass. It addresses an issue where the HLO passes before the host offloader could inadvertently leak memory space annotations from the entry computation layout to the graph. In PR https://github.com/openxla/xla/pull/20426, the layout assignment pass was corrected to prevent instructions from inheriting memory space S(5) from the entry computation layout. This commit further ensures that such annotations are not propagated, keeping host memory space not changed until the host offloader pass. Copybara import of the project:  a785e186e919d3921a2922caca5fbca1f6eb0f37 by Jane Liu : Add the hlo verifier before host offloader to check host memory space  bebf8b97743e34b59114c1c0966b9cf1c5877b90 by Jane Liu : remove extra std::move and change InvalidArgumentError to Internal  9ac2bf59d93759f67d9a41578cd70dba78498b0f by Jane Liu : use ForEachSubshapeWithStatus Merging this change closes CC([XLA] ResourceExhaustedError when trying to define a Sequential model in Keras under jit_scope context manager) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21638 from zhenyingliu:verifier 9ac2bf59d93759f67d9a41578cd70dba78498b0f",2025-03-06T20:33:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88768
copybara-service[bot],[XLA] Remove unnecessary uses of set_parent() when constructing fusion and call instructions.,"[XLA] Remove unnecessary uses of set_parent() when constructing fusion and call instructions. We'd like to maintain the invariant that an instruction has a parent if and only if it has been added to a computation. It only seems needed because of some code in CloneAndAppendInstructionIntoCalledComputation needs it to find the enclosing module, but we can just look at a different instruction for that.",2025-03-06T20:21:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88767
copybara-service[bot],Fix some Android emulator based tests.,Fix some Android emulator based tests.,2025-03-06T20:00:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88766
copybara-service[bot],PR #23395: [ROCm] gfx950 support,PR CC(deploying the Tensorflow model in Python): [ROCm] gfx950 support Imported from GitHub PR https://github.com/openxla/xla/pull/23395 rotation please have a look Thanks Copybara import of the project:  6c6bfad5a896154c1a21c263cda433253e9f8597 by Chao Chen : gfx950 support Merging this change closes CC(deploying the Tensorflow model in Python) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23395 from ROCm:ci_gfx950 6c6bfad5a896154c1a21c263cda433253e9f8597,2025-03-06T19:48:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88765
copybara-service[bot],PR #88546: Qualcomm AI Engine Direct - Wrapper tests & Refactor tensor wrapper & Fix rms norm,PR CC(Qualcomm AI Engine Direct  Wrapper tests & Refactor tensor wrapper & Fix rms norm): Qualcomm AI Engine Direct  Wrapper tests & Refactor tensor wrapper & Fix rms norm Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/88546  What 1. Refactor tensor wrapper      Safer tensor data getter and setter. 2. Add wrapper tests. 3. Fix rms norm builder      Change 0 beta tensor type to float32 for float32 input and uint8 for other input data types. See op support types here.  Tests `qnn_compiler_plugin_test` ``` [] Global test environment teardown [==========] 99 tests from 5 test suites ran. (3995 ms total) [  PASSED  ] 99 tests. ``` Copybara import of the project:  c4a8ccebc910318f6afaaccd552129fa92cfa29f by chunhsue : refine tensor wrapper  9ef17b23072f7d50e76d7c7cef37fa4fbf0ad577 by chunhsue : add wrapper tests  0feee34f0192e726ab40192a32c551ed396896d3 by chunhsue : fix rms norm builder Merging this change closes CC(Qualcomm AI Engine Direct  Wrapper tests & Refactor tensor wrapper & Fix rms norm) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88546 from jiunkaiy:dev/chunhsue/wrapper_tests 0feee34f0192e726ab40192a32c551ed396896d3,2025-03-06T19:27:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88764
copybara-service[bot],Create a benchmark presubmit workflow to track performance regression and rename existing benchmark workflows to differentiate nightly from presubmit,Create a benchmark presubmit workflow to track performance regression and rename existing benchmark workflows to differentiate nightly from presubmit,2025-03-06T19:25:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88763
rtg0795,Update scripts/configs for Windows nightly/release builds.,"`set u` (does not allow unbound variables) has been removed from all scripts. This is due to Docker on Windows treating variables in an env file, set to an empty value (`MY_VAR=`), as unbound variables. Consequently, these variables, even though they are ""set"", do not make it into the Docker container at all, and various checks for those variables fail outright. PiperOriginRevId: 713717958",2025-03-06T19:24:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88762
copybara-service[bot],Expose profiler_data submodule from XLA to Jaxlib.,Expose profiler_data submodule from XLA to Jaxlib.,2025-03-06T19:24:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88761
copybara-service[bot],Define lax.ragged_dot_general and express lax.ragged_dot in terms of it.,Define lax.ragged_dot_general and express lax.ragged_dot in terms of it.,2025-03-06T19:18:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88760
copybara-service[bot],Include correct command to regenerate goldens for the `diff_test`,Include correct command to regenerate goldens for the `diff_test` Also add it to the docstring of `build.py` itself,2025-03-06T19:18:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88759
copybara-service[bot],[XLA:GPU] Add RaggedAllToAll CUDA kernel.,"[XLA:GPU] Add RaggedAllToAll CUDA kernel. The kernel will be used in RaggedAllToAll thunk for singlehost collectives. Runtime is responsible for communication, synchronization and exchange of pointer. The kernel only need to move the data.",2025-03-06T19:02:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88758
psunn,[TOSA] fix test_mul_with_unequal_ranks_qi8 LIT test,fix LIT test,2025-03-06T18:57:08Z,kokoro:force-run ready to pull size:S comp:lite-tosa,closed,0,1,https://github.com/tensorflow/tensorflow/issues/88757,"INFO: Analyzed 33 targets (1 packages loaded, 33380 targets configured). INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 6.475s, Critical Path: 3.46s INFO: 35 processes: 1 internal, 34 local. INFO: Build completed successfully, 35 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test        PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test         PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test             PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test      PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test                PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test   PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test        PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test           PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test  PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test      PASSED in 0.8s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test         PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 3.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test     PASSED in 0.6s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test        PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test   PASSED in 0.5s Executed 17 out of 17 tests: 17 tests pass."
copybara-service[bot],Add program_id to op profile Node.xla,Add program_id to op profile Node.xla,2025-03-06T18:41:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88756
copybara-service[bot],[XLA] Have WhileLoopInvariantCodeMotion cleanup (more) after itself,[XLA] Have WhileLoopInvariantCodeMotion cleanup (more) after itself The tuple mess after narrowing/widening confuses follow up passes down the road that rely on pattern matching.,2025-03-06T18:41:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88755
copybara-service[bot],StableHLOAggressiveFolderPass : Don't fold iota op if number of elements is large (> 65536),StableHLOAggressiveFolderPass : Don't fold iota op if number of elements is large (> 65536),2025-03-06T18:32:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88754
copybara-service[bot],[XLA:CPU] Add CallKernelEmitter and use it to emit small while loops,[XLA:CPU] Add CallKernelEmitter and use it to emit small while loops FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23502 from ROCm:jdgfx11 be7af2aaf124f59d780de8135e94fd8fda4b48bd,2025-03-06T18:30:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88753
copybara-service[bot],Add all `BUILD.bazel` files to `_XLA_MOVE_ONLY_FILES`,Add all `BUILD.bazel` files to `_XLA_MOVE_ONLY_FILES` FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22799 from shraiysh:ds_fusion_rs_multiple_buffers 817c274ad4b715b28ae60cfabe88ee86bb6e1671,2025-03-06T18:29:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88752
copybara-service[bot],Copy tf/compiler/mlir/quantization/common/quantization_lib/quantization.td to Lite,Copy tf/compiler/mlir/quantization/common/quantization_lib/quantization.td to Lite,2025-03-06T18:26:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88751
copybara-service[bot],remove a workflow,remove a workflow FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22799 from shraiysh:ds_fusion_rs_multiple_buffers 817c274ad4b715b28ae60cfabe88ee86bb6e1671,2025-03-06T17:33:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88750
copybara-service[bot],PR #22808: [ds-fusion] Enable GpuReduceScatterCombiner before ds-fusion,PR CC([Documentation] Format example list.): [dsfusion] Enable GpuReduceScatterCombiner before dsfusion Imported from GitHub PR https://github.com/openxla/xla/pull/22808 This PR depends on CC(1.12rc0 cherrypick request: Mark tensorflow/contrib/tpu:datasets_test flaky). This patch runs GpuReduceScatterCombiner before dsfusion so that the overhead of multiple nccl thunks is avoided in `gpu_compiler`. Copybara import of the project:  9334995a20cd7fcc501d873349475135eda726d1 by Shraiysh Vaishay : [dsfusion] Allow multiple buffers in reducescatter for dsfusion This patch adds support for multibuffer reducescatter in dynamicslicefusion. All of these will be a part of the same NCCL reducescatter thunk to avoid overheads of multiple NCCL thunks.  b3e6d0ee0ed54d122a5fcc9958beed898aac8324 by Shraiysh Vaishay : Address comments  d8a34e545c61f7bc1a44ee20da4efe8198738617 by Shraiysh Vaishay : Address comments  817c274ad4b715b28ae60cfabe88ee86bb6e1671 by Shraiysh Vaishay : Address comment  00743cdcf02a9a213519cbb89be55ea76376b4a0 by Shraiysh Vaishay : Move reducescatter combiner before dynamicslicefusion Merging this change closes CC([Documentation] Format example list.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22808 from shraiysh:gpu_compiler_enable_ds_fusion_rs_multiple_buffers 00743cdcf02a9a213519cbb89be55ea76376b4a0,2025-03-06T17:12:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88749
copybara-service[bot],PR #22799: [ds-fusion] Allow multiple buffers in reduce-scatter for ds-fusion,PR CC(1.12rc0 cherrypick request: Mark tensorflow/contrib/tpu:datasets_test flaky): [dsfusion] Allow multiple buffers in reducescatter for dsfusion Imported from GitHub PR https://github.com/openxla/xla/pull/22799 This patch adds support for multibuffer reducescatter in dynamicslicefusion. We also now call the reducescatter combiner pass before the dynamicslicefusion rewriter to ensure that we do not incur the overheads of multiple nccl calls. Copybara import of the project:  9334995a20cd7fcc501d873349475135eda726d1 by Shraiysh Vaishay : [dsfusion] Allow multiple buffers in reducescatter for dsfusion This patch adds support for multibuffer reducescatter in dynamicslicefusion. All of these will be a part of the same NCCL reducescatter thunk to avoid overheads of multiple NCCL thunks.  b3e6d0ee0ed54d122a5fcc9958beed898aac8324 by Shraiysh Vaishay : Address comments  d8a34e545c61f7bc1a44ee20da4efe8198738617 by Shraiysh Vaishay : Address comments  817c274ad4b715b28ae60cfabe88ee86bb6e1671 by Shraiysh Vaishay : Address comment Merging this change closes CC(1.12rc0 cherrypick request: Mark tensorflow/contrib/tpu:datasets_test flaky) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22799 from shraiysh:ds_fusion_rs_multiple_buffers 817c274ad4b715b28ae60cfabe88ee86bb6e1671,2025-03-06T17:04:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88748
copybara-service[bot],Revert VhloToVersion change from the StableHLO integrate cl/733942948,Revert VhloToVersion change from the StableHLO integrate cl/733942948,2025-03-06T17:04:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88747
LakshmiKalaKadali,Updated `scan` as `tf.scan` in  functional_ops.py,"The function `scan(lambda a, x: a + x, elems)` causes  NameError: name 'scan' is not defined. It is to be  `tf.scan(lambda a, x: a + x, elems)`.  Here is the [gist].(https://colab.sandbox.google.com/gist/LakshmiKalaKadali/18151b51142f0bec3060f144e014b1e1/api_tf_scan.ipynbscrollTo=p4TfDoaoUw1U)",2025-03-06T16:46:18Z,stat:awaiting response stale comp:ops size:XS,closed,0,3,https://github.com/tensorflow/tensorflow/issues/88746,"> While here, can you change the examples to doctests? This way the code would get tested and future breakages such as this one would get caught earlier. Thanks for the comment. Done the changes. Thank You",This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,This PR was closed because it has been inactive for 14 days since being marked as stale. Please reopen if you'd like to work on this further.
copybara-service[bot],PR #23433: Update README.md,PR CC([nGraph] Updated to nGraph version v0.9.1 and ngraphtf version v0.7.0): Update README.md Imported from GitHub PR https://github.com/openxla/xla/pull/23433 Fix broken link to C++ API Copybara import of the project:  80d06aadde20f14d2bc4bd863f864159255974a8 by Iman Hosseini : Update README.md Fix broken link to C++ API  b073b066763dd0d76ba82cad855500a5ae198eec by Iman Hosseini : Update README.md Merging this change closes CC([nGraph] Updated to nGraph version v0.9.1 and ngraphtf version v0.7.0) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23433 from ImanHosseini:patch1 b073b066763dd0d76ba82cad855500a5ae198eec,2025-03-06T16:28:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88745
copybara-service[bot],Automatically register the NPU accelerator.,Automatically register the NPU accelerator.  Removes the creation and manual application of the Dispatch delegate in   `LiteRtCompiledModelT::Create`. It is now applied through the NPU   accelerator.  Adds the allocation file descriptor to the `ModelCompilationData` structure   and retrieves it in the `NpuAccelerator::CreateDelegate()` function.  Registers the NPU accelerator in `TriggerAcceleratorAutomaticRegistration()`.,2025-03-06T16:08:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88744
copybara-service[bot],#litert Detect address sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`.,"litert Detect address sanitizers in `cc:litert_shared_library` to disable `RTLD_DEEPBIND`. Trying to load a library using `RTLD_DEEPBIND` is not supported by address sanitizers. In an effort to enable testing we strip the flag. If this leads to unintended behaviour, either remove the `RTLD_DEEPBIND` flag or run without an address sanitizer. See https://github.com/google/sanitizers/issues/611 for more information.",2025-03-06T15:46:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88743
copybara-service[bot],[XLA:CPU] Add support for AOT compilation for thunk mode.,[XLA:CPU] Add support for AOT compilation for thunk mode.,2025-03-06T15:33:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88742
felix-johnny,Update version of KleidiAI,KleidiAI version is updated to v1.4.0,2025-03-06T14:11:31Z,ready to pull size:S,closed,0,1,https://github.com/tensorflow/tensorflow/issues/88741,  Here is the update at TF as well. 
copybara-service[bot],[XLA:CPU] Update kernel API builder to enable more flexible buffer arguments,[XLA:CPU] Update kernel API builder to enable more flexible buffer arguments,2025-03-06T12:05:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88740
copybara-service[bot],#litert Don't fail compilation of `cc:litert_shared_library` on Windows.,litert Don't fail compilation of `cc:litert_shared_library` on Windows. Note: Windows is still an unsupported platform to load shared libraries.,2025-03-06T11:51:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88739
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T11:37:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88738
Venkat6871,Fix typos in documentation strings,"Hi, Team I observed few typos in the documentation strings and I have fixed those typos so please do the needful. Thank you.",2025-03-06T11:12:18Z,ready to pull size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88737
copybara-service[bot],[XLA:GPU] Emit tiled dots in generic triton emitter,[XLA:GPU] Emit tiled dots in generic triton emitter Only basic cases are supported at the moment.,2025-03-06T11:11:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88736
copybara-service[bot],Don't shard autotuning when autotuning is disabled.,"Don't shard autotuning when autotuning is disabled.  Otherwise the compilation fails when we disable autotuning on multihost, as the xla_gpu_shard_autotuning is set to True by default.",2025-03-06T10:57:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88735
copybara-service[bot],[XLA:GPU] Propagate `element_size_in_bits` when building a `transpose` in `gather_scatter_utils.cc`.,"[XLA:GPU] Propagate `element_size_in_bits` when building a `transpose` in `gather_scatter_utils.cc`. The `ScatterSimplifier` would previously fail to propagate it, which could cause packing mismatches & lead to crashes further down the line.",2025-03-06T10:54:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88734
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T09:59:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88733
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T09:50:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88732
JiahuaZhao,tf.experimental.dlpack.to_dlpack() becomes performance bottleneck," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.9.0  Custom code Yes  OS platform and distribution Rocky Linux 8.6  Mobile device _No response_  Python version 3.9.7  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA11.7.0/cuDNN8.4.1.50  GPU model and memory NVIDIA A100SXM440GB  Current behavior? Hi, my code uses CuPy and TensorFlow, where CuPy is used for data preprocessing and postprocessing, and TensorFlow is responsible for loading models and inference. For the conversion between CuPy arrays and TF tensors, we use DLpack: https://docs.cupy.dev/en/stable/user_guide/interoperability.htmldlpack. However, it seems that `tf.experimental.dlpack.to_dlpack()` has become a performance bottleneck because I need to convert the inference result (TF tensor) into a CuPy array for postprocessing. And a single conversion takes about **0.13 seconds** but converting from a CuPy array to a TF tensor (`tf.experimental.dlpack.from_dlpack()`) takes less than **0.01 seconds**. The entire calculation process needs to be carried out in a loop, which usually requires hundreds of iterations, so the data conversion time will be magnified. So I want to know how to reduce the time taken by `tf.experimental.dlpack.to_dlpack()` if there are any suggestions. Thanks in advance :)  Standalone code to reproduce the issue ```shell tf_tensor= tf_model(tf.experimental.dlpack.from_dlpack(CuPy_array.toDlpack()), training=False) cp.from_dlpack(tf.experimental.dlpack.to_dlpack(tf_tensor)) ```  Relevant log output ```shell ```",2025-03-06T09:31:12Z,comp:gpu type:performance TF 2.9,open,0,3,https://github.com/tensorflow/tensorflow/issues/88731,"I get the same problem. But my goal is to get the pointer of the tensor. Time for `tf.experimental.dlpack.from_dlpack` is much longer than the inference itself, while it is extremely fast to get an pointer in C++ API.","Hi **** , Apologies for the delay, and thanks for raising your concern here. I noticed that you are using an older version of TensorFlow (12.9) and found a compatibility mismatch. Could you please check with the latest version? Ensuring version compatibility can help achieve better results. I am attaching the official documentation for your reference. If you are still facing the same issue with the latest version, please let us know so we can investigate further. Thank you!","> Hi **[](https://github.com/JiahuaZhao)** , Apologies for the delay, and thanks for raising your concern here. I noticed that you are using an older version of TensorFlow (12.9) and found a compatibility mismatch. Could you please check with the latest version? Ensuring version compatibility can help achieve better results. I am attaching the official documentation for your reference. If you are still facing the same issue with the latest version, please let us know so we can investigate further. >  > Thank you! Hi , thank you for your reply. I have tried TensorFlow up to version 2.16 (due to environmental restrictions, I cannot use the latest version), but this problem still occurs. But I found more details. When I don’t need to use TF inference in my code, both `tf.experimental.dlpack.from_dlpack()` and `tf.experimental.dlpack.to_dlpack()` are fast (both <0.01 sec, even in the loop body), for example: ``` for i in range(num):   a = cp.random.randn(248, 288, 112, 2).astype(cp.float32)   a_tf = tf.experimental.dlpack.from_dlpack(a.toDlpack())   cap = cp.from_dlpack(tf.experimental.dlpack.to_dlpack(a_tf)) ``` As soon as I used TF inference in it, the previous situation appeared (`tf.experimental.dlpack.to_dlpack()` has become a performance bottleneck), for example: ``` for i in range(num):   a = cp.random.randn(248, 288, 112, 2).astype(cp.float32)   a_tf = tf.experimental.dlpack.from_dlpack(a.toDlpack())   pred = model(a_tf, training=False)   cap = cp.from_dlpack(tf.experimental.dlpack.to_dlpack(pred)) ``` So I suspect that there is blocking, and I haven't done any professional performance analysis yet. Do you have any suggestions? By the way, CuPy version is 13.3. Thanks!"
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T09:19:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88730
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T09:18:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88729
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T09:10:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88728
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T09:07:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88727
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T09:04:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88726
copybara-service[bot],TopkRewriter/TopkDecomposer: ensure metadata is kept.,TopkRewriter/TopkDecomposer: ensure metadata is kept. Also slightly refactor the code (nonfunctional changes). Replace confusing CreateSort call that passes {input>shape()} with just input>shape(). This compiled before because it is effectively calling the copy constructor of shape. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23232 from jreiffers:whilethunk ace5f2ec052bc6d10ad8cbe76b27716b7b6c19d2,2025-03-06T09:01:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88725
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T08:56:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88724
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T08:55:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88723
copybara-service[bot],Integrate LLVM at llvm/llvm-project@03da07996884,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 03da07996884,2025-03-06T08:54:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88722
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T08:53:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88721
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T08:53:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88720
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T08:50:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88719
copybara-service[bot],PR #23307: [ROCm] Support multiple ROCm paths,"PR CC(Missing Documentation: Multiple links return 404 in Python API Guide): [ROCm] Support multiple ROCm paths Imported from GitHub PR https://github.com/openxla/xla/pull/23307 This PR provides support for multiple ROCm paths.  Usually ROCm is installed in a single location like ""/opt/rocm"" but that isn't always the case (ex. when using spack). To build with multiple ROCm paths set the environment variable `TF_ROCM_MULTIPLE_PATHS` to a "":"" seperated list of all the ROCm component paths and `LLVM_PATH` to the ROCm LLVM path. ~~To set the rpaths of the multiple ROCm lib directories use:~~ ~~`//rocm:multiple_rocm_rpath=True`~~ Edit: To set the rpaths of the multiple ROCm lib directories use: `//rocm:rocm_path_type=multiple` To set the rpaths of the hermetic libs use: `//rocm:rocm_path_type=hermetic` Copybara import of the project:  310f9cbdd2773e4aa3330c5c18e69a0bb845bbcf by Afzal Patel : Support multiple ROCm paths  572a3d46f54f7686915327f038d9a80927b7a081 by Afzal Patel : replace bool flags with rocm_path_type  f672353d47c990b0222801fc53ee589579e559c5 by Afzal Patel : set hermetic in tensorflow.bazelrc Merging this change closes CC(Missing Documentation: Multiple links return 404 in Python API Guide) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23307 from ROCm:multiplerocmpaths dc60ee34af499fc8c54913c297fba906086d7467",2025-03-06T08:49:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88718
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T08:48:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88717
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T08:46:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88716
copybara-service[bot],Add LiteRT digit classifier android example.,Add LiteRT digit classifier android example.,2025-03-06T08:46:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88715
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T08:40:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88714
copybara-service[bot],[LiteRT] Make `dispatch_api_shared_lib` public to allow APK packaging.,[LiteRT] Make `dispatch_api_shared_lib` public to allow APK packaging.,2025-03-06T08:08:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88713
copybara-service[bot],[tf.data] Store the processing time in an atomic in `SimpleStepStatsCollector`.,"[tf.data] Store the processing time in an atomic in `SimpleStepStatsCollector`. Previously, this value was protected by a mutex. In very finegrained tf.data function execution, this mutex can become contended. The atomic provides adequate semantics for keeping a running total of processing time.",2025-03-06T08:07:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88712
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T07:49:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88711
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T07:11:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88710
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T07:00:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88709
priyanshujiiii,Graph Execution Error," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0dev20240917  Custom code Yes  OS platform and distribution Rocky Linux  Mobile device Rocky Linux  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 9.3.0.  GPU model and memory NIVIDIA SMI 560.28.03  Current behavior? The code is showing a Graph execution error while training a model  Standalone code to reproduce the issue ```shell import os import numpy as np import tensorflow as tf from sklearn.model_selection import train_test_split def load_and_preprocess_data(base_path, batch_size=2, val_split=0.2):     noise_files = []     fullstack_files = []      Collect file paths     for subdir, _, files in os.walk(base_path):         for file in files:             if file.endswith("".npy""):                 full_path = os.path.join(subdir, file)                 if ""noise"" in file:                     noise_files.append(full_path)                 elif ""fullstack"" in file:                     fullstack_files.append(full_path)      Ensure files are in correct order     noise_files.sort()     fullstack_files.sort()      Split data into train and validation sets     noise_train, noise_val, fullstack_train, fullstack_val = train_test_split(         noise_files, fullstack_files, test_size=val_split, random_state=42     )     def create_generator(noise_list, fullstack_list):         def generator():             for noise_path, fullstack_path in zip(noise_list, fullstack_list):                  Load files in memorymapped mode                 noise_data = np.load(noise_path, mmap_mode='r', allow_pickle=True)                 fullstack_data = np.load(fullstack_path, mmap_mode='r', allow_pickle=True)                 for i in range(300):                     x_sample = noise_data[:, :, i]   Only loads this slice into RAM                     y_sample = fullstack_data[:, :, i]                     yield np.expand_dims(x_sample, axis=1), np.expand_dims(y_sample, axis=1)         return generator      Create TensorFlow datasets     train_dataset = tf.data.Dataset.from_generator(         create_generator(noise_train, fullstack_train),         output_signature=(             tf.TensorSpec(shape=(1259, 300, 1), dtype=tf.float32),             tf.TensorSpec(shape=(1259, 300, 1), dtype=tf.float32)         )     ).batch(batch_size).prefetch(tf.data.AUTOTUNE)     val_dataset = tf.data.Dataset.from_generator(         create_generator(noise_val, fullstack_val),         output_signature=(             tf.TensorSpec(shape=(1259, 300, 1), dtype=tf.float32),             tf.TensorSpec(shape=(1259, 300, 1), dtype=tf.float32)         )     ).batch(batch_size).prefetch(tf.data.AUTOTUNE)     return train_dataset, val_dataset  Example Usage base_path = ""/home/simlab120/Denoise_comp/Pragyant/imageimpeccabletraindatapart1"" train_dataset, val_dataset = load_and_preprocess_data(base_path)  Display a sample batch from the training set for x_batch, y_batch in train_dataset.take(1):     import matplotlib.pyplot as plt     plt.subplot(1, 2, 1)     plt.imshow(x_batch[0, :, :, 0], cmap='gray')   Removing channel dimension for visualization     plt.title(""Noisy Image"")     plt.subplot(1, 2, 2)     plt.imshow(y_batch[0, :, :, 0], cmap='gray')     plt.title(""Fullstack Image"")     plt.show()     break Output:  20250306 11:47:58.814180: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250306 11:47:58.827279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741241878.841060  263395 cuda_dnn.cc:8321] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741241878.844820  263395 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250306 11:47:58.859322: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. I0000 00:00:1741241880.476719  263395 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5694 MB memory:  > device: 0, name: NVIDIA T1000 8GB, pci bus id: 0000:52:00.0, compute capability: 7.5 Input:  import tensorflow as tf from tensorflow.keras.layers import Conv2D, Conv2DTranspose, ZeroPadding2D, Cropping2D, BatchNormalization, MaxPooling2D, UpSampling2D from tensorflow.keras import models def build_encdec(input_shape=(1259, 300, 1)):   Ensure it's divisible by 2^6     model = models.Sequential(name=""encoder_decoder"")      Encoder     model.add(ZeroPadding2D(padding=((21, 0), (10, 10)), input_shape=input_shape))   Add extra padding     model.add(Conv2D(16, (3, 3), activation='swish', padding=""same"", strides=1))      model.add(BatchNormalization())     model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))     model.add(UpSampling2D((2, 2)))     model.add(Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same', strides=1))   Change activation to sigmoid     model.add(BatchNormalization())      Remove padding added in encoder     model.add(Cropping2D(cropping=((21, 0), (10, 10))))       return model     def ssim_metric(y_true, y_pred):      Ensure y_true has a channel dimension if missing.     y_true = y_true if len(y_true.shape) == 4 else tf.expand_dims(y_true, 1)     return tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))  Create the autoencoder model instance autoencoder = build_encdec()  Compile the model autoencoder.compile(optimizer=""adam"", loss=""mse"", metrics=[ssim_metric])  Build the model by specifying an input shape (optional) autoencoder.build((None, 1259, 300, 1)) autoencoder.summary() output: Model: ""encoder_decoder"" ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ ┃ Layer (type)                    ┃ Output Shape           ┃       Param  ┃ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ │ zero_padding2d_1                │ (None, 1280, 320, 1)   │             0 │ │ (ZeroPadding2D)                 │                        │               │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ conv2d_6 (Conv2D)               │ (None, 1280, 320, 16)  │           160 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ batch_normalization_12          │ (None, 1280, 320, 16)  │            64 │ │ (BatchNormalization)            │                        │               │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ max_pooling2d_6 (MaxPooling2D)  │ (None, 640, 160, 16)   │             0 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ up_sampling2d_6 (UpSampling2D)  │ (None, 1280, 320, 16)  │             0 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ conv2d_transpose_6              │ (None, 1280, 320, 1)   │           145 │ │ (Conv2DTranspose)               │                        │               │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ batch_normalization_13          │ (None, 1280, 320, 1)   │             4 │ │ (BatchNormalization)            │                        │               │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ cropping2d_1 (Cropping2D)       │ (None, 1259, 300, 1)   │             0 │ └─────────────────────────────────┴────────────────────────┴───────────────┘  Total params: 373 (1.46 KB)  Trainable params: 339 (1.32 KB)  Nontrainable params: 34 (136.00 B)  import os os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""  history = autoencoder.fit(     train_dataset,   Use the dataset directly     validation_data=val_dataset,   Use validation dataset     epochs=1000,     verbose=1, ) ```  Relevant log output ```shell W0000 00:00:1741242567.222588  263744 assert_op.cc:38] Ignoring Assert operator SSIM/Assert/Assert W0000 00:00:1741242567.222702  263744 assert_op.cc:38] Ignoring Assert operator SSIM/Assert_1/Assert W0000 00:00:1741242567.222812  263744 assert_op.cc:38] Ignoring Assert operator SSIM/Assert_2/Assert W0000 00:00:1741242567.222874  263744 assert_op.cc:38] Ignoring Assert operator SSIM/Assert_3/Assert E0000 00:00:1741242567.262839  263744 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. E0000 00:00:1741242567.282689  263744 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. 20250306 11:59:27.290025: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at xla_ops.cc:577 : FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.  FailedPreconditionError                   Traceback (most recent call last) Cell In[8], line 3       1 import os       2 os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""  > 3 history = autoencoder.fit(       4     train_dataset,   Use the dataset directly       5     validation_data=val_dataset,   Use validation dataset       6     epochs=1000,       7     verbose=1,       8 ) File ~/anaconda3/envs/tf/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py:122, in filter_traceback..error_handler(*args, **kwargs)     119     filtered_tb = _process_traceback_frames(e.__traceback__)     120      To get the full stack trace, call:     121      `keras.config.disable_traceback_filtering()` > 122     raise e.with_traceback(filtered_tb) from None     123 finally:     124     del filtered_tb File ~/anaconda3/envs/tf/lib/python3.12/sitepackages/tensorflow/python/eager/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)      51 try:      52   ctx.ensure_initialized() > 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,      54                                       inputs, attrs, num_outputs)      55 except core._NotOkStatusException as e:      56   if name is not None: FailedPreconditionError: Graph execution error: Detected at node StatefulPartitionedCall defined at (most recent call last):   File """", line 198, in _run_module_as_main   File """", line 88, in _run_code   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel_launcher.py"", line 18, in    File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/traitlets/config/application.py"", line 1075, in launch_instance   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/kernelapp.py"", line 739, in start   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/tornado/platform/asyncio.py"", line 205, in start   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/asyncio/base_events.py"", line 641, in run_forever   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/asyncio/base_events.py"", line 1986, in _run_once   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/asyncio/events.py"", line 88, in _run   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/kernelbase.py"", line 545, in dispatch_queue   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/kernelbase.py"", line 534, in process_one   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/kernelbase.py"", line 437, in dispatch_shell   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/ipkernel.py"", line 362, in execute_request   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/kernelbase.py"", line 778, in execute_request   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/ipkernel.py"", line 449, in do_execute   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/ipykernel/zmqshell.py"", line 549, in run_cell   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"", line 3075, in run_cell   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"", line 3130, in _run_cell   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/IPython/core/async_helpers.py"", line 128, in _pseudo_sync_runner   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"", line 3334, in run_cell_async   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"", line 3517, in run_ast_nodes   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"", line 3577, in run_code   File ""/tmp/ipykernel_263395/279222490.py"", line 3, in    File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 320, in fit   File ""/home/simlab120/anaconda3/envs/tf/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 121, in one_step_on_iterator DNN library initialization failed. Look at the errors above for more details. 	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_iterator_14559] ```",2025-03-06T06:42:15Z,stat:awaiting response stale type:performance TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88708,"Hi **** , Apologies for the delay, and thanks for raising your concern here. Could you please provide a Colab gist for troubleshooting this issue more accurately? Alternatively, you can share the specific code where you are facing the issue. I attempted to replicate the provided code but encountered a different issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-06T05:50:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88707
copybara-service[bot],Fix dependency on absl/strings:str_format for tool_options.h,Fix dependency on absl/strings:str_format for tool_options.h,2025-03-06T02:51:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88706
copybara-service[bot],Enable `import_api_packages_test` on windows builds.,Enable `import_api_packages_test` on windows builds.,2025-03-06T02:44:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88705
copybara-service[bot],Cleanup: Fix code warnings.,Cleanup: Fix code warnings.,2025-03-06T02:18:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88704
copybara-service[bot],#tf-data Removes min parallelism logging in `parallel_batch_dataset_op.cc` since there are users reporting log spamming,tfdata Removes min parallelism logging in `parallel_batch_dataset_op.cc` since there are users reporting log spamming Reverts f6439c5f481f2769d9be994995bf2c176a99ea8a FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22383 from ROCm:ci_enable_hiphostMemRegister_20250205 0452d2b73a9c3145e53b08eb186e0fa9ec10bbb4,2025-03-06T02:01:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88703
copybara-service[bot],Update xplane_to_op_stats visibility,Update xplane_to_op_stats visibility,2025-03-06T01:56:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88702
copybara-service[bot],Add `stream_pool`. It tracks gpu streams to be used in data transfer.,Add `stream_pool`. It tracks gpu streams to be used in data transfer.,2025-03-06T01:53:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88701
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-06T01:52:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88700
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-06T01:51:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88699
copybara-service[bot],Integrate LLVM at llvm/llvm-project@e739ce2e10e6,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match e739ce2e10e6 Reverts f6439c5f481f2769d9be994995bf2c176a99ea8a FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22383 from ROCm:ci_enable_hiphostMemRegister_20250205 0452d2b73a9c3145e53b08eb186e0fa9ec10bbb4,2025-03-06T01:50:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88698
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-06T00:56:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88697
copybara-service[bot],#tf-data Removes min parallelism logging in `parallel_batch_dataset_op.cc` since there are users reporting log spamming,tfdata Removes min parallelism logging in `parallel_batch_dataset_op.cc` since there are users reporting log spamming,2025-03-06T00:51:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88696
copybara-service[bot],Remove extraneous transform in XLA's Copybara config,Remove extraneous transform in XLA's Copybara config,2025-03-06T00:39:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88695
copybara-service[bot],Reverts c6568686bc302726af21693ef3919918dd3dcb2a,Reverts c6568686bc302726af21693ef3919918dd3dcb2a FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22383 from ROCm:ci_enable_hiphostMemRegister_20250205 0452d2b73a9c3145e53b08eb186e0fa9ec10bbb4,2025-03-06T00:33:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88694
copybara-service[bot],PR #22968: Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline,"PR CC(remove noisy warning in StepCounterHook): Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline Imported from GitHub PR https://github.com/openxla/xla/pull/22968 This PR adds ScheduleAwareCollectiveOpsCSE to the GPU pipeline before scheduling, as the pass is not scheduleaware. This is a second attempt, as the previous PR was reverted due to an outofmemory error. This PR introduces a flag to control the distance threshold. Copybara import of the project:  249c6a3ef59c2bdc798e265a22fce7827bf8eb6f by Sevin Varoglu : Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline  96400cb93fb430fec2566e122feb1ca693de106a by Sevin Varoglu : Add review feedback  96124692de811106462cea5408d71bce63d5c125 by Sevin Varoglu : Add local var Merging this change closes CC(remove noisy warning in StepCounterHook) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22968 from sfvaroglu:sevin/collective_cse_flag 96124692de811106462cea5408d71bce63d5c125",2025-03-06T00:07:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88693
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-06T00:01:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88692
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-06T00:00:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88691
copybara-service[bot],PR #22383: [ROCm] Add hiphostMemRegister for intra-node all-to-all memcpy,PR CC(Inconsistency in tf.nn.top_k when using sorted=False and tf.float32 tensor and GPU placement): [ROCm] Add hiphostMemRegister for intranode alltoall memcpy Imported from GitHub PR https://github.com/openxla/xla/pull/22383 Enable nccl memcpy for intranode alltoall for ROCm extending work done in https://github.com/openxla/xla/pull/15144 Copybara import of the project:  0452d2b73a9c3145e53b08eb186e0fa9ec10bbb4 by Harsha HS : [ROCm] Add hiphostMemRegister for intranode alltoall memcpy Enable nccl memcpy for intranode alltoall for ROCm extending work done in https://github.com/openxla/xla/pull/15144 Merging this change closes CC(Inconsistency in tf.nn.top_k when using sorted=False and tf.float32 tensor and GPU placement) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22383 from ROCm:ci_enable_hiphostMemRegister_20250205 0452d2b73a9c3145e53b08eb186e0fa9ec10bbb4,2025-03-05T23:43:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88690
copybara-service[bot],Test tpu job,Test tpu job,2025-03-05T23:28:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88689
copybara-service[bot],Remove redundant log message.,Remove redundant log message.,2025-03-05T23:27:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88688
copybara-service[bot],[XLA] Have WhileLoopInvariantCodeMotion cleanup (more) after itself,[XLA] Have WhileLoopInvariantCodeMotion cleanup (more) after itself The tuple mess after narrowing/widening confuses follow up passes down the road that rely on pattern matching. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23433 from ImanHosseini:patch1 b073b066763dd0d76ba82cad855500a5ae198eec,2025-03-05T22:55:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88687
copybara-service[bot],[XLA] Add a utility function to query if a collective is exclusively cross replica or not.,[XLA] Add a utility function to query if a collective is exclusively cross replica or not.,2025-03-05T22:20:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88686
copybara-service[bot],[XLA:GPU] Move rendezvous calls before and after the kernel into separate functions.,[XLA:GPU] Move rendezvous calls before and after the kernel into separate functions.,2025-03-05T22:19:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88685
copybara-service[bot],No longer use `opensource_only.files` in TSL,No longer use `opensource_only.files` in TSL,2025-03-05T22:18:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88684
copybara-service[bot],Reverts f6439c5f481f2769d9be994995bf2c176a99ea8a,Reverts f6439c5f481f2769d9be994995bf2c176a99ea8a,2025-03-05T22:17:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88683
copybara-service[bot],[xla:cpu] Add TraceMe producer consumer to profile Thunk execution,[xla:cpu] Add TraceMe producer consumer to profile Thunk execution,2025-03-05T22:08:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88682
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T22:06:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88681
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T22:05:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88680
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T22:00:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88679
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T22:00:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88678
chandrasekhard2,Update RELEASE.md,,2025-03-05T21:59:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88677
Corey4005,Tensorflow does not recognize GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0  Custom code No  OS platform and distribution WSL Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.0  Bazel version _No response_  GCC/compiler version 13.3.0  CUDA/cuDNN version _No response_  GPU model and memory RTX A4000 16GB  Current behavior? First, I installed Anaconda32025.10.1Linux using wget and the shell script.   1.  Install Anaconda Commands: ``` wget https://repo.anaconda.com/archive/Anaconda32024.101Linuxx86_64.sh bash Anaconda32024.101Linuxx86_64.sh ``` I selected yes on the location in which it generally installs. Anaconda was installed in /home/user/anaconda3.  I also allowed anaconda to update my shell profile by selecting yes.  I rebooted the shell and went to step 2.   2. I made sure my drivers were up to date.  I ran the nvidiasmi command to check this and saw the latest driver for my NVIDIA RTX A4000 GPUs. The image below shows the output with the most up to date drivers.  !Image  3. I installed the latest cuda toolkit using the following commands.  ``` sudo aptkey del 7fa2af80 wget https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda_12.8.0_570.86.10_linux.run sudo sh cuda_12.8.0_570.86.10_linux.run ```  4. I set my path to the cuda toolkit and the lib64 in my .bashrc file I put the at the bottom of the file and rebooted the shell  ``` export PATH=/usr/local/cuda12.8/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda12.8/lib64:$PATH ```  5. I install cuDNN I downloaded the tar file for linux here. It does seem that there is a file for 24.04, but the tar file did work when I installed it.  ``` tar xvf cudnnlinuxx86_648.9.7.29_cuda12archive.tar.xz ``` This unraveled a lot of files on my machine. I followed the installation instructions here specifically, I did the following:  ``` sudo cp cudnn*archive/include/cudnn*.h /usr/local/cuda/include  sudo cp P cudnn*archive/lib/libcudnn* /usr/local/cuda/lib64  sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn* ``` When I run nvcc V, I get:  !Image  6 I install a virtual environment and activate it.  ``` conda create n tf python==3.12 conda activate tf python3 m pip install tensorflow[andcuda] ``` Then, when I check for GPU support, I get errors and my GPU is not recognized:  7 The errors are shown below:  ``` python3 c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"" ``` Errors:  ``` 20250305 15:55:46.327193: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250305 15:55:46.338383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741211746.350376   17653 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741211746.354146   17653 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250305 15:55:46.365820: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250305 15:55:48.179254: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: RESOURCE_EXHAUSTED: Failed call to cuInit: CUDA_ERROR_OUT_OF_MEMORY: out of memory ``` What am I doing wrong?   Standalone code to reproduce the issue ```shell I am getting a CUDA_ERROR_OUT_OF_MEMORY upon install and my GPU is not being listed. ```  Relevant log output ```shell ```",2025-03-05T21:57:51Z,type:bug comp:gpu TF 2.18,open,0,12,https://github.com/tensorflow/tensorflow/issues/88676,"The highest version of CUDA supported by your drivers is 12.3, whereas the version of CUDA you installed is 12.8 Try downgrading CUDA to <=12.3 and try again.  Look up here to find compatible versions of CUDA, cuDNN, gcc and Python for your installed Tensorflow version.","Hi ****, thanks for raising your concern here. Hi **AT**, thanks for your response. The latest CUDA version is not compatible with your machines. When installing TensorFlow with GPU support, the CUDA version is installed automatically by using the following command: ``` python3 m pip install tensorflow[andcuda] ``` There is no need to manually install the CUDA Toolkit. I am attaching the official documentation1, documentation2 for installing TensorFlow using pip and checking compatibility versions. Based on this documentation, I installed it, and it worked fine for me. Please find the details below for your reference. ``` (tf_env) maayaragpu1:~$ python3 Python 3.9.21 (main, Dec 11 2024, 16:24:11)  [GCC 11.2.0] :: Anaconda, Inc. on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf >>> print(tf.__version__) 2.18.0 >>> print(tf.config.list_physical_devices(""GPU"")) [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')] >>> exit() (tf_env) maayaragpu1:~$ nvidiasmi Thu Mar  6 05:55:43 2025        ++  ++ ``` Thank you!","> Hi **[](https://github.com/Corey4005)**, thanks for raising your concern here. Hi **AT**, thanks for your response. The latest CUDA version is not compatible with your machines. When installing TensorFlow with GPU support, the CUDA version is installed automatically by using the following command: >  > ``` > python3 m pip install tensorflow[andcuda] > ``` >  > There is no need to manually install the CUDA Toolkit. I am attaching the official documentation1, documentation2 for installing TensorFlow using pip and checking compatibility versions. Based on this documentation, I installed it, and it worked fine for me. Please find the details below for your reference. >  > ``` > (tf_env) maayaragpu1:~$ python3 > Python 3.9.21 (main, Dec 11 2024, 16:24:11)  > [GCC 11.2.0] :: Anaconda, Inc. on linux > Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. > >>> import tensorflow as tf > >>> print(tf.__version__) > 2.18.0 > >>> print(tf.config.list_physical_devices(""GPU"")) > [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')] > >>> exit() > (tf_env) maayaragpu1:~$ nvidiasmi > Thu Mar  6 05:55:43 2025        > ++ >  > ++ > ``` >  > Thank you! Hi, . Thank you for the reply. I am going to provide exact steps I am following after seeing your recommendation that I do not install cuda manually and provide outputs for your review.  First, I wiped WSL and reinstalled to get a fresh vm.   1. In powershell ```  wsl unregister Ubuntu  wsl install ``` I now have a new install.   2. Opening WSL and looking at my version  ``` lsb_release a  ``` The following image shows the output.  !Image  3. I pull the latest conda and install ``` wget https://repo.anaconda.com/archive/Anaconda32024.101Linuxx86_64.sh bash Anaconda32024.101Linuxx86_64.sh ``` The installation location is set by default to `/home/user/anaconda3`.  I also allow conda to initialize by selecting `yes` when prompted. I then restart wsl and see that conda environments are now available by seeing `(base)` in my shell.   4. I create a conda environment with python 3.12 as suggested for tensorflow 2.18.0 in the documentation you provided and activate.  ``` conda create n tf python==3.12 conda activate tf ```  5. I install tensorflow and cuda using:  ``` python3 m pip install tensorflow[andcuda] ``` python version shows:  ``` python3 version ``` !Image pip version shows:  ``` pip version ``` !Image  7 I run the following command and get: ``` python3 c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"" ``` !Image nvidiasmi shows:  !Image Am I installing the wrong versions? ","> The highest version of CUDA supported by your drivers is 12.3, whereas the version of CUDA you installed is 12.8 Try downgrading CUDA to <=12.3 and try again. Look up here to find compatible versions of CUDA, cuDNN, gcc and Python for your installed Tensorflow version. How would I downgrade? Is there a command I can provide pip to get 12.3 instead?","Hello   AT.  I am still unable to get Tensorflow to work. I am going to walk through some new developments, however in an effort to work towards a transparent solution that others can follow. To just start over again with a fresh install, I just unregistered WSL as I had done in prior steps and reinstalled ubuntu.  !Image First, I have double checked my driver. I looked online and found that the RTX A4000 has driver 572.60 available. I installed this driver such that nvidiasmi now lists CUDA 12.8, shown below.  !Image Could ,  or AT please verify I have installed the correct driver for my GPUs?  Then, I updated and upgraded using the following commands:  ``` sudo apt update sudo apt upgrade ``` Afterwards, I installed anaconda as prior, which installed anaconda in /home/user/anaconda3. I also let it initialize conda by changing my bashrc file shown below: ``` wget https://repo.anaconda.com/archive/Anaconda32024.101Linuxx86_64.sh bash Anaconda32024.101Linuxx86_64.sh ``` Does this `.bashrc` file look ok?  ```  ~/.bashrc: executed by bash(1) for nonlogin shells.  see /usr/share/doc/bash/examples/startupfiles (in the package bashdoc)  for examples  If not running interactively, don't do anything case $ in     *i*) ;;       *) return;; esac  don't put duplicate lines or lines starting with space in the history.  See bash(1) for more options HISTCONTROL=ignoreboth  append to the history file, don't overwrite it shopt s histappend  for setting history length see HISTSIZE and HISTFILESIZE in bash(1) HISTSIZE=1000 HISTFILESIZE=2000  check the window size after each command and, if necessary,  update the values of LINES and COLUMNS. shopt s checkwinsize  If set, the pattern ""**"" used in a pathname expansion context will  match all files and zero or more directories and subdirectories. shopt s globstar  make less more friendly for nontext input files, see lesspipe(1) [ x /usr/bin/lesspipe ] && eval ""$(SHELL=/bin/sh lesspipe)""  set variable identifying the chroot you work in (used in the prompt below) if [ z ""${debian_chroot:}"" ] && [ r /etc/debian_chroot ]; then     debian_chroot=$(cat /etc/debian_chroot) fi  set a fancy prompt (noncolor, unless we know we ""want"" color) case ""$TERM"" in     xtermcolor]\s*alert$//'\'')""'  Alias definitions.  You may want to put all your additions into a separate file like  ~/.bash_aliases, instead of adding them here directly.  See /usr/share/doc/bashdoc/examples in the bashdoc package. if [ f ~/.bash_aliases ]; then     . ~/.bash_aliases fi  enable programmable completion features (you don't need to enable  this, if it's already enabled in /etc/bash.bashrc and /etc/profile  sources /etc/bash.bashrc). if ! shopt oq posix; then   if [ f /usr/share/bashcompletion/bash_completion ]; then     . /usr/share/bashcompletion/bash_completion   elif [ f /etc/bash_completion ]; then     . /etc/bash_completion   fi fi  >>> conda initialize >>>  !! Contents within this block are managed by 'conda init' !! __conda_setup=""$('/home/user/anaconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)"" if [ $? eq 0 ]; then     eval ""$__conda_setup"" else     if [ f ""/home/user/anaconda3/etc/profile.d/conda.sh"" ]; then         . ""/home/user/anaconda3/etc/profile.d/conda.sh""     else         export PATH=""/home/user/anaconda3/bin:$PATH""     fi fi unset __conda_setup  >> import tensorflow as tf 20250310 15:55:25.474408: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250310 15:55:25.484407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741640125.496393    4606 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741640125.500028    4606 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250310 15:55:25.512041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. >>> tf.config.list_physical_devices('GPU') 20250310 15:55:37.500622: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: RESOURCE_EXHAUSTED: Failed call to cuInit: CUDA_ERROR_OUT_OF_MEMORY: out of memory [] ```","Also , I install python 3.9.21 and used pip install tensorflow[andcuda] and got the same result: ``` python Python 3.9.21 (main, Dec 11 2024, 16:24:11) [GCC 11.2.0] :: Anaconda, Inc. on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf 20250310 16:15:17.136808: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250310 16:15:17.146938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741641317.158879    5197 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741641317.162466    5197 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250310 16:15:17.174416: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. >>> print(tf.__version__) 2.18.0 >>> print(tf.config.list_physical_devices(""GPU"")) 20250310 16:15:49.531676: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: RESOURCE_EXHAUSTED: Failed call to cuInit: CUDA_ERROR_OUT_OF_MEMORY: out of memory [] >>> exit() nvidiasmi Mon Mar 10 16:16:00 2025 ++  ++ ``` so, I installed the same python and the same tensorflow versions as you and it seems I am still getting the same error.  There must be something wrong with my system that I am not understanding.  Exact packages I installed to replicate your terminal outputs:  ```  packages in environment at /home/user/anaconda3/envs/tf:   Name                    Version                   Build  Channel _libgcc_mutex             0.1                        main _openmp_mutex             5.1                       1_gnu abslpy                   2.1.0                    pypi_0    pypi astunparse                1.6.3                    pypi_0    pypi cacertificates           2025.2.25            h06a4308_0 certifi                   2025.1.31                pypi_0    pypi charsetnormalizer        3.4.1                    pypi_0    pypi flatbuffers               25.2.10                  pypi_0    pypi gast                      0.6.0                    pypi_0    pypi googlepasta              0.2.0                    pypi_0    pypi grpcio                    1.71.0                   pypi_0    pypi h5py                      3.13.0                   pypi_0    pypi idna                      3.10                     pypi_0    pypi importlibmetadata        8.6.1                    pypi_0    pypi keras                     3.9.0                    pypi_0    pypi ld_impl_linux64          2.40                 h12ee557_0 libclang                  18.1.1                   pypi_0    pypi libffi                    3.4.4                h6a678d5_1 libgccng                 11.2.0               h1234567_1 libgomp                   11.2.0               h1234567_1 libstdcxxng              11.2.0               h1234567_1 markdown                  3.7                      pypi_0    pypi markdownitpy            3.0.0                    pypi_0    pypi markupsafe                3.0.2                    pypi_0    pypi mdurl                     0.1.2                    pypi_0    pypi mldtypes                 0.4.1                    pypi_0    pypi namex                     0.0.8                    pypi_0    pypi ncurses                   6.4                  h6a678d5_0 numpy                     2.0.2                    pypi_0    pypi nvidiacublascu12        12.5.3.2                 pypi_0    pypi nvidiacudacupticu12    12.5.82                  pypi_0    pypi nvidiacudanvcccu12     12.5.82                  pypi_0    pypi nvidiacudanvrtccu12    12.5.82                  pypi_0    pypi nvidiacudaruntimecu12  12.5.82                  pypi_0    pypi nvidiacudnncu12         9.3.0.75                 pypi_0    pypi nvidiacufftcu12         11.2.3.61                pypi_0    pypi nvidiacurandcu12        10.3.6.82                pypi_0    pypi nvidiacusolvercu12      11.6.3.83                pypi_0    pypi nvidiacusparsecu12      12.5.1.3                 pypi_0    pypi nvidiancclcu12          2.21.5                   pypi_0    pypi nvidianvjitlinkcu12     12.5.82                  pypi_0    pypi openssl                   3.0.16               h5eee18b_0 opteinsum                3.4.0                    pypi_0    pypi optree                    0.14.1                   pypi_0    pypi packaging                 24.2                     pypi_0    pypi pip                       25.0             py39h06a4308_0 protobuf                  5.29.3                   pypi_0    pypi pygments                  2.19.1                   pypi_0    pypi python                    3.9.21               he870216_1 readline                  8.2                  h5eee18b_0 requests                  2.32.3                   pypi_0    pypi rich                      13.9.4                   pypi_0    pypi setuptools                75.8.0           py39h06a4308_0 six                       1.17.0                   pypi_0    pypi sqlite                    3.45.3               h5eee18b_0 tensorboard               2.18.0                   pypi_0    pypi tensorboarddataserver   0.7.2                    pypi_0    pypi tensorflow                2.18.0                   pypi_0    pypi tensorflowiogcsfilesystem 0.37.1                   pypi_0    pypi termcolor                 2.5.0                    pypi_0    pypi tk                        8.6.14               h39e8969_0 typingextensions         4.12.2                   pypi_0    pypi tzdata                    2025a                h04d1e81_0 urllib3                   2.3.0                    pypi_0    pypi werkzeug                  3.1.3                    pypi_0    pypi wheel                     0.45.1           py39h06a4308_0 wrapt                     1.17.2                   pypi_0    pypi xz                        5.6.4                h5eee18b_1 zipp                      3.21.0                   pypi_0    pypi zlib                      1.2.13               h5eee18b_1 ```", What is the output of `nvcc version`,"> [](https://github.com/Corey4005) What is the output of `nvcc version` Hi AT. Good morning to you! When I run `nvcc version`, I get: ``` $ nvcc version Command 'nvcc' not found, but can be installed with: sudo apt install nvidiacudatoolkit ``` Is it supposed to be installed when `pip install tensorflow[andcuda]` is ran?  If I am understanding  correctly, he stated:  ""There is no need to manually install the CUDA Toolkit. I am attaching the official documentation1, documentation2 for installing TensorFlow using pip and checking compatibility versions. Based on this documentation, I installed it, and it worked fine for me. Please find the details below for your reference."" Should I have to install it with `sudo apt install nvidiacudatoolkit`? After I run that command I get: !Image I rerun the commands as above after nvcc is installed:  ```  python Python 3.9.21 (main, Dec 11 2024, 16:24:11) [GCC 11.2.0] :: Anaconda, Inc. on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf 20250312 09:26:58.433368: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250312 09:26:58.442961: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741789618.454737    2772 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741789618.458211    2772 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250312 09:26:58.469874: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. >>> print(tf.__version__) 2.18.0 >>> print(tf.config.list_physical_devices(""GPU"")) \20250312 09:27:29.037109: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: RESOURCE_EXHAUSTED: Failed call to cuInit: CUDA_ERROR_OUT_OF_MEMORY: out of memory [] >>> ```"," Based on your output of `nvidiasmi`, the highest supported version of CUDA on your machine is 12.3, for which the corresponding TensorFlow version is 2.17.  I would suggest deleting the current CUDA toolkit  ``` sudo apt purge remove nvidiacudatoolkit sudo apt autoremove ``` Then installing CUDA 12.3 using the runfile option  ``` wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda_12.3.0_545.23.06_linux.run sudo sh cuda_12.3.0_545.23.06_linux.run silent toolkit ``` Copy over the include and lib files to /usr/local as mentioned in the postinstall screen `nvcc version` now should report 12.3 Then install tensorflow 2.17","> [](https://github.com/Corey4005) Based on your output of `nvidiasmi`, the highest supported version of CUDA on your machine is 12.3, for which the corresponding TensorFlow version is 2.17. I would suggest deleting the current CUDA toolkit >  > ``` > sudo apt purge remove nvidiacudatoolkit > sudo apt autoremove > ``` >  > Then installing CUDA 12.3 using the runfile option >  > ``` > wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda_12.3.0_545.23.06_linux.run > sudo sh cuda_12.3.0_545.23.06_linux.run silent toolkit > ``` >  > Copy over the include and lib files to /usr/local as mentioned in the postinstall screen `nvcc version` now should report 12.3 >  > Then install tensorflow 2.17 Hi, AT. Just to clarify... I need to install CUDA? It seems like it was suggested I do not need to install it per 's comments:  ""There is no need to manually install the CUDA Toolkit. I am attaching the official documentation1, documentation2 for installing TensorFlow using pip and checking compatibility versions. Based on this documentation, I installed it, and it worked fine for me. Please find the details below for your reference."" When I run nvidiasmi, I get 12.8 now after installing this driver Also see the output: !Image To follow the documentation, it seems like it is suggesting I need to install CUDA 12.5, and provided the image above, it looks like I can support up to 12.8. Therefore, Tensorflow 2.18 should run per the `nvidiasmi` output I am showing above, correct?  I ran the following commands to get 12.5:  ``` wget https://developer.download.nvidia.com/compute/cuda/12.5.0/local_installers/cuda_12.5.0_555.42.02_linux.run sudo sh cuda_12.5.0_555.42.02_linux.run ```  I should note that it does not give me any output to copy files to any directory. However, the ouptut suggest that I set my paths. I do that here:  ``` export PATH=/usr/local/cuda12.5/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda12.5/lib64:$PATH ``` Then, when I run `nvcc version` I get: !Image I am still getting the same error even after nvcc is installed.  ``` Python 3.9.21 (main, Dec 11 2024, 16:24:11) [GCC 11.2.0] :: Anaconda, Inc. on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf 20250314 12:52:00.348718: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250314 12:52:00.358413: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741974720.370189    1088 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741974720.373698    1088 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250314 12:52:00.387155: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. >>> tf.__version__ '2.18.0' >>> print(tf.config.list_physical_devices('GPU')) 20250314 12:52:25.012176: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: RESOURCE_EXHAUSTED: Failed call to cuInit: CUDA_ERROR_OUT_OF_MEMORY: out of memory [] ``` Can you help me understand what files I am supposed to be copying over? The only output I got from the cuda runfile was to set my paths. ","Hi,  AT. Have there been any updates here? Is there any information you need me to provide further? I am beginning to think I just wont be able to use tensorflow for my project. Thank you for your time.","Good morning  and AT. Just wanted to share that pytorch works when I swap from WSL2 to anaconda prompt. See the following issue where I show that installing pytorch in WSL2 causes a memory error similar to what I am seeing in this issue thread. However, when I install pytorch in anaconda prompt, it is able to recognize the GPU and run. However, I tried to install tensorflow using anaconda prompt using  ``` pip install tensorflow[andcuda] ``` and recieved a bunch of errors. I assume this is because it can only be installed on WSL2 in a windows environment. If this is the case, then I am not sure how I can solve the issue because there is only one method of installing tensorflow on windows. "
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T21:57:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88675
copybara-service[bot],This is for testing purpose,This is for testing purpose,2025-03-05T21:55:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88674
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T21:55:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88673
copybara-service[bot],"Jax memories api prefers ""vmem"" to ""device_sram"", update the host memory offload annotations accordingly.","Jax memories api prefers ""vmem"" to ""device_sram"", update the host memory offload annotations accordingly.",2025-03-05T21:54:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88672
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T21:50:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88671
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T21:48:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88670
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T21:47:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88669
copybara-service[bot],Allow users to specify the ADSP_LIBRARY_PATH env variable.,Allow users to specify the ADSP_LIBRARY_PATH env variable.,2025-03-05T21:40:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88668
copybara-service[bot],Export stablehlo ops in the TAC exporting function.,Export stablehlo ops in the TAC exporting function.,2025-03-05T21:38:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88667
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T21:36:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88666
copybara-service[bot],Add Google Tensor test for CC Compiled Model.,Add Google Tensor test for CC Compiled Model.,2025-03-05T21:29:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88665
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T21:26:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88664
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T21:21:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88663
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T21:18:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88662
copybara-service[bot],#litert Add a function that triggers built-in accelerators registration.,litert Add a function that triggers builtin accelerators registration.,2025-03-05T21:11:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88661
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T21:09:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88660
copybara-service[bot],[xla] Do not parallelize scalar index traversal,[xla] Do not parallelize scalar index traversal,2025-03-05T21:01:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88659
tensorflow-jenkins,"r2.19 cherry-pick: 3a79ec99874 ""Bump libtpu versions to pick correct versioned nightlies""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/3a79ec9987413e0824a3d243eeb538e51bf3132a,2025-03-05T20:26:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88658
copybara-service[bot],[XLA:Python] Fix use-after-free in pjit.,"[XLA:Python] Fix useafterfree in pjit. The following useafterfree was observed in a freethreaded build of JAX: https://gist.githubusercontent.com/vfdev5/d08f1939a560b54b6d34f6c38efdad18/raw/26455d2aa75ff92cff9af9bb2a8e3e38c16b47ac/tests_export_back_compat_test.313.log I believe what is happening here is that we are attempting to access the function that forms part of the cache key to hash it, but after we've freed the function in question. Precompute the hash instead, which avoids the problem.",2025-03-05T20:18:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88657
copybara-service[bot],[tsl:thread] Sprinkle deprecation warnings in tsl::thread::ThreadPool API in preparation for cleanup,[tsl:thread] Sprinkle deprecation warnings in tsl::thread::ThreadPool API in preparation for cleanup Reverts f6439c5f481f2769d9be994995bf2c176a99ea8a,2025-03-05T20:15:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88656
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T20:08:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88655
copybara-service[bot],testing file for ci stuff,testing file for ci stuff,2025-03-05T20:08:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88654
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T20:04:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88653
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T20:03:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88652
copybara-service[bot],[tsl:thread] Remove deprecated ParallelForWithWorkerId,[tsl:thread] Remove deprecated ParallelForWithWorkerId,2025-03-05T20:03:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88651
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T20:02:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88650
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T19:57:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88649
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T19:56:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88648
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-03-05T19:54:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88647
copybara-service[bot],Allow for viewing of memory_viewer trace properties by memory type.,Allow for viewing of memory_viewer trace properties by memory type.,2025-03-05T19:46:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88646
copybara-service[bot],[XLA] Don't implicitly derive offload attributes,[XLA] Don't implicitly derive offload attributes Offloading is very sensitive. Normally this is done by the user or by exclusive offloading passes.,2025-03-05T19:43:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88645
copybara-service[bot],Move some common code under ifndef for static memory builds.,Move some common code under ifndef for static memory builds.,2025-03-05T19:21:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88644
VHursevich,Wrong list of labels in an introduction notebook," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? https://github.com/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb x_labels = ['no', 'yes', 'down', 'go', 'left', 'up', 'right', 'stop'] !Image Correct list of labels depends on system. For Ubuntu 22.04 correct labels:  x_labels = ['down', 'go', 'left', 'no', 'right', 'stop', 'up', 'yes']  !Image  Standalone code to reproduce the issue ```shell https://github.com/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb ```  Relevant log output ```shell ```",2025-03-05T19:20:25Z,type:bug TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/88643,"Is the problem that the label order changes for different operatiing systems? Or that you want the order of the labels to change with the operating system. If its the first option, you could explicitly by adding  `sorted(os.listdir(data_dir))`",Are you satisfied with the resolution of your issue? Yes No,"The issue stems from how labels are handled in the original code. I've fixed this issue by ensuring consistent label ordering throughout the pipeline: Python :        def main():            Get commands (classes)           commands = []           for item in data_dir.iterdir():               if item.is_dir() and item.name not in ['README.md', '.DS_Store']:                   commands.append(item.name)           commands = sorted(commands)  Sort for consistency        Load and preprocess dataset       train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(           directory=data_dir,           batch_size=64,           validation_split=0.2,           seed=0,           output_sequence_length=16000,           subset='both'       )        Get label names       label_names = np.array(train_ds.class_names)        Save label names properly       labels_list = label_names.tolist() if hasattr(label_names, 'tolist') else label_names       with open(""label_names.json"", ""w"") as f:           json.dump(labels_list, f) And for prediction, I've implemented proper preprocessing and consistent label handling:  Proper prediction code     def preprocess_audiobuffer(waveform):         """"""Preprocess audio buffer for model prediction""""""          Normalize from [32768, 32767] to [1, 1]         waveform = waveform / 32768         waveform = tf.convert_to_tensor(waveform, dtype=tf.float32)         spectrogram = get_spectrogram(waveform)          Add batch dimension         spectrogram = tf.expand_dims(spectrogram, 0)         return spectrogram  Test prediction with consistent label handling     prediction = trained_model(spec, training=False)     probabilities = tf.nn.softmax(prediction[0])     predicted_index = tf.argmax(probabilities).numpy()     predicted_word = label_names[predicted_index]"
copybara-service[bot],Delete `PjRtClient.Defragment`.,"Delete `PjRtClient.Defragment`. The `Defragment` implementation for GPU is in `py_client.cc`, so this should be a noop.",2025-03-05T19:08:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88642
copybara-service[bot],Remove old genrules in favor of direct dep on jni.,Remove old genrules in favor of direct dep on jni.,2025-03-05T18:58:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88641
copybara-service[bot],Fix GitHub workflow feedback results return to critique,Fix GitHub workflow feedback results return to critique,2025-03-05T18:00:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88640
copybara-service[bot],Fix Win2022 RBE builds.,Fix Win2022 RBE builds.,2025-03-05T17:55:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88639
copybara-service[bot],Exclude v1only tests and targets from `windows_x86_cpu_2022_pycpp_test_filters`.,Exclude v1only tests and targets from `windows_x86_cpu_2022_pycpp_test_filters`.,2025-03-05T17:52:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88638
copybara-service[bot],Exclude `//tensorflow/python/kernel_tests/...` from TensorFlow builds,Exclude `//tensorflow/python/kernel_tests/...` from TensorFlow builds,2025-03-05T17:46:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88637
copybara-service[bot],Integrate LLVM at llvm/llvm-project@e739ce2e10e6,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match e739ce2e10e6,2025-03-05T17:41:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88636
copybara-service[bot],LiteRT: Fix a bug on input type checking,LiteRT: Fix a bug on input type checking,2025-03-05T17:28:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88635
copybara-service[bot],Remove `CUDA` dependencies from `xla_extension.so` in OSS.,Remove `CUDA` dependencies from `xla_extension.so` in OSS. `xla_extension.so` shouldn't depend on any `CUDA` headers and libraries in OSS because it's used in `jaxlib` for CPU operations only.,2025-03-05T17:15:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88634
copybara-service[bot],Migrate fake_quant_utils from Lite to TF QuantOps dialect,Migrate fake_quant_utils from Lite to TF QuantOps dialect,2025-03-05T16:59:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88633
copybara-service[bot],Copy QuantizeUtils component from lite directory to TF common,Copy QuantizeUtils component from lite directory to TF common,2025-03-05T16:41:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88632
copybara-service[bot],[onednn] Fix tsan issue in oneDNN when initializing setting_t,[onednn] Fix tsan issue in oneDNN when initializing setting_t As reported in https://github.com/openxla/xla/issues/20686,2025-03-05T16:35:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88631
copybara-service[bot],[XLA:GPU] Move rendezvous calls before and after the kernel into separate functions.,[XLA:GPU] Move rendezvous calls before and after the kernel into separate functions.,2025-03-05T16:14:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88630
copybara-service[bot],[XLA:GPU] Fix multi host profiling for collective perf tables.,[XLA:GPU] Fix multi host profiling for collective perf tables. Previous solution initialized buffers on devices arguments multiple times and results included this overhead.,2025-03-05T15:47:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88629
copybara-service[bot],PR #20399: Elementwise Ops in Collective Pipeliner,"PR CC(Feature request: A Layer object wrapping multiple Layer objects): Elementwise Ops in Collective Pipeliner Imported from GitHub PR https://github.com/openxla/xla/pull/20399 Enables support for elementwise ops ahead of the dynamicupdateslice, such as in scaled FP8 GEMMs, in the collective pipeliner. Copybara import of the project:  d3cbc62d158427ac41d712e41ff9db3743e1e112 by Philipp Hack : Enables support for elementwise ops in the collective pipeliner. Merging this change closes CC(Feature request: A Layer object wrapping multiple Layer objects) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20399 from philipphack:u_pipeliner_elementwise_xla d3cbc62d158427ac41d712e41ff9db3743e1e112",2025-03-05T15:20:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88628
copybara-service[bot],Reverts 7d8800f302ca1f6360d9b9569c8ef3ca9766f989,Reverts 7d8800f302ca1f6360d9b9569c8ef3ca9766f989,2025-03-05T15:19:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88627
copybara-service[bot],[xla:gpu] Tiling analysis: test empty fusions.,[xla:gpu] Tiling analysis: test empty fusions. Also fix 'M' and 'N' in the test documentation.,2025-03-05T15:05:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88626
copybara-service[bot],[XLA:GPU] Simplify logic to emit tt.load in the legacy emitter.,[XLA:GPU] Simplify logic to emit tt.load in the legacy emitter.,2025-03-05T13:18:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88625
copybara-service[bot],Enable apple toolchain also for ios build,Enable apple toolchain also for ios build,2025-03-05T11:18:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88624
copybara-service[bot],[xla:gpu] Expect fully-ranked output_tiles sizes in nested fusions.,[xla:gpu] Expect fullyranked output_tiles sizes in nested fusions. This makes it consistent with the spec (rank corresponds to the rank of the roots) and what we produce in NestGemmFusion pass. It drops the 'rid_x' from the output tile offset indexing map and replaces them with ordinary `pid_x`. I don't think the variable names provide significant value. This allows us to drop the `RootIndexing::num_reduction_dims`.,2025-03-05T08:57:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88623
copybara-service[bot],[XLA:GPU] Collect perf table data for further analysis.,[XLA:GPU] Collect perf table data for further analysis.,2025-03-05T08:57:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88622
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T08:51:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88621
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T08:42:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88620
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T08:19:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88619
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T08:06:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88618
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T08:02:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88617
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T07:55:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88616
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T07:55:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88615
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T07:52:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88614
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T07:48:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88613
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T07:45:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88612
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T07:40:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88611
copybara-service[bot],Avoid signed int overflow (NFC).,Avoid signed int overflow (NFC). We can rewrite the expression into an equivalent expression that doesn't cause overflow.,2025-03-05T06:57:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88610
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T06:17:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88608
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T06:08:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88607
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T04:48:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88606
copybara-service[bot],Reverts f83deb5e24c8871146e7f407d87151962c78cee8,Reverts f83deb5e24c8871146e7f407d87151962c78cee8,2025-03-05T04:46:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88605
copybara-service[bot],Do not quantized unannotated constants in strict mode,Do not quantized unannotated constants in strict mode PrepareQuantizePass no longer quantizes constants that are not annotated when QDQConversionMode is strict.,2025-03-05T04:33:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88604
copybara-service[bot],Expose qdq_conversion_mode to PrepareQuantizePass,Expose qdq_conversion_mode to PrepareQuantizePass This would allow additional configuration of QSV propagation based on the conversion mode.,2025-03-05T04:29:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88603
copybara-service[bot],Fork quantization driver in tflite codebase,Fork quantization driver in tflite codebase This would allow tflite specific changes to QSV propagation logic.,2025-03-05T04:19:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88602
copybara-service[bot],Make HloRunner thread-safe.,"Make HloRunner threadsafe. This fixes tests in conditional_test..://github.com/openxla/xla/commit/c930cee33d6b21bb5b65e8ae264618745fbb6fe9. The tests failed under tsan. This removes the entry_computation_layout_ field, and now passes around an entry_computation_layout to functions, to avoid race conditions. Additionally, the entry computation layout is only updated once per module to avoid concurrent updates, or reads from it while its being updated.  The class now also has a mutex. The backend() method no longer initializes backend_ if its nullptr, since backend_ is always set in the constructor.",2025-03-05T04:15:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88601
copybara-service[bot],Exclude `v1only` tests and targets from Windows 2022 CI jobs.,Exclude `v1only` tests and targets from Windows 2022 CI jobs.,2025-03-05T02:59:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88600
copybara-service[bot],Integrate LLVM at llvm/llvm-project@e739ce2e10e6,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match e739ce2e10e6,2025-03-05T02:21:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88599
copybara-service[bot],Update uses of `@tsl//third_party` to `@xla//third_party` after https://github.com/openxla/xla/commit/933649f19ef0e7fa13b6b515985f9cbbc570606e,Update uses of `//third_party` to `//third_party` after https://github.com/openxla/xla/commit/933649f19ef0e7fa13b6b515985f9cbbc570606e,2025-03-05T01:49:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88598
copybara-service[bot],Migrate createUnfuseBatchNormPass to tensorflow/compiler/mlir/stablehlo,Migrate createUnfuseBatchNormPass to tensorflow/compiler/mlir/stablehlo,2025-03-05T01:14:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88597
copybara-service[bot],PR #23355: [ROCm] Add rocm_base and rocm_gcc configs,"PR CC(Failed to load): [ROCm] Add rocm_base and rocm_gcc configs Imported from GitHub PR https://github.com/openxla/xla/pull/23355 Changed `rocm` config to `rocm_base`, and added `rocm_gcc`  so we can have gcc specific options. Since a lot of scripts still depend on `config=rocm` I have temporarily left it in .bazelrc. It will be removed once I transition everything to use `rocm_gcc` Copybara import of the project:  eaf35d55cb1b7700b4607678a9e31913f5a226b6 by Milica Makevic : Change rocm config to rocm_base and add rocm_gcc Merging this change closes CC(Failed to load) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23355 from ROCm:addrocmbase eaf35d55cb1b7700b4607678a9e31913f5a226b6",2025-03-05T00:49:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88596
copybara-service[bot],[XLA:GPU] Assign latency multipliers for P2P collectives,[XLA:GPU] Assign latency multipliers for P2P collectives,2025-03-05T00:48:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88595
copybara-service[bot],Migrate CreateRenameEntrypointToMainPass to tensorflow/compiler/mlir/stablehlo,Migrate CreateRenameEntrypointToMainPass to tensorflow/compiler/mlir/stablehlo,2025-03-05T00:48:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88594
copybara-service[bot],PR #23060: Refacting xla/tests/client_library_test_base to remove duplicated codes.,PR CC(Attributes values not inferred by TFE C API (eager mode)): Refacting xla/tests/client_library_test_base to remove duplicated codes. Imported from GitHub PR https://github.com/openxla/xla/pull/23060 Copybara import of the project:  217e312c7f5bd5834f0b4a2d4206aa735b72476a by Shawn Wang : code refacotring  c62b8f587da2886287376246086c9bf550fe7fcb by Shawn Wang : fix  b98044083e6dd6f2a37152e9638b93ac7cee6a06 by Shawn Wang : fix  a291d6e332ab1cc3fd11ff5ff19d414ac9973896 by Shawn Wang : fix: Merging this change closes CC(Attributes values not inferred by TFE C API (eager mode)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23060 from shawnwang18:shawnw/options_iterator a291d6e332ab1cc3fd11ff5ff19d414ac9973896,2025-03-05T00:46:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88593
copybara-service[bot],PR #88221: Qualcomm AI Engine Direct - Op Builders for 1P Models,"PR CC(Qualcomm AI Engine Direct  Op Builders for 1P Models): Qualcomm AI Engine Direct  Op Builders for 1P Models Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/88221  WHAT 1. Conv2d 2. DepthwiseConv2d 3. AveragePool 4. MaxPool 5. DepthToSpace 6. SpaceToDepth 7. HardSwish 8. LeakyRelu 9. ResizeBilinear 10. Litert options for these op builders, unit test 11. update qnn_compiler_plugin_test with these op builders  TEST  qnn_compiler_plugin_test ``` [] Global test environment teardown [==========] 115 tests from 5 test suites ran. (3489 ms total) [  PASSED  ] 115 tests. ```  litert_options_test ``` [] Global test environment teardown [==========] 22 tests from 1 test suite ran. (0 ms total) [  PASSED  ] 22 tests ``` Copybara import of the project:  542a108226dcb1c31abc30578cba2b74b20e8e0b by weilhuanquic : Qualcomm AI Engine Direct  Op Builders for 1P Models 1. Conv2d 2. DepthwiseConv2d 3. AveragePool 4. MaxPool 5. DepthToSpace 6. SpaceToDepth 7. HardSwish 8. LeakyRelu 9. ResizeBilinear 10. Litert options for these op builders, unit test 11. update qnn_compiler_plugin_test with these op builders Merging this change closes CC(Qualcomm AI Engine Direct  Op Builders for 1P Models) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88221 from jiunkaiy:dev/weilhuan/more_op_builders 542a108226dcb1c31abc30578cba2b74b20e8e0b",2025-03-05T00:33:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88592
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-05T00:22:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88591
copybara-service[bot],Initialize TfLiteQuantization before parsing,Initialize TfLiteQuantization before parsing If the tensor has no quantization this makes sure that the params field is initialized to null.,2025-03-04T23:53:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88590
copybara-service[bot],Guard inclusion of opencl header with LITERT_HAS_OPENCL_SUPPORT header,Guard inclusion of opencl header with LITERT_HAS_OPENCL_SUPPORT header,2025-03-04T23:33:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88589
copybara-service[bot],Placeholder for github public description due to base CL requirement.  This could be removed once base CL is submitted.,Placeholder for github public description due to base CL requirement.  This could be removed once base CL is submitted.,2025-03-04T23:32:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88588
copybara-service[bot],Updated the LiteRT Google implementation to dlopen libedgetpu_litert.so when,Updated the LiteRT Google implementation to dlopen libedgetpu_litert.so when possible.,2025-03-04T23:31:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88587
copybara-service[bot],[XLA:SchedulingAnnoations] Delete the redundant resource limit violation check. ,"[XLA:SchedulingAnnoations] Delete the redundant resource limit violation check.  `SchedulingAnnotationCrossesOverlapLimit` takes the annotated group as a whole, analyzes its accumulated resource usage and decides whether scheduling the group as a whole crosses any resource overlap limits. After a recent bug fix in this function, the instructionlevel check became redundant because we know that the individual instructions cannot cross the overlap limit anymore if `SchedulingAnnotationCrossesOverlapLimit` returned false.",2025-03-04T23:30:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88586
copybara-service[bot],[XLA] Enforce that instructions do not reference computations in other modules.,"[XLA] Enforce that instructions do not reference computations in other modules. Remove a call to `HloComputation::set_parent()` in the `HloInstruction` cloning code that at least introduces some ephemeral crossmodule edges. The `set_parent()` call seems misguided from an invariant point of view, since the cloned instruction has not yet been added to a computation and hence does not have a parent. This causes a number of downstream test failures, which we fix. We also make a small refactoring so called computations are set more consistently via a `HloInstruction::set_called_computation()` helper.",2025-03-04T23:08:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88585
copybara-service[bot],Fix Win2022 RBE builds.,Fix Win2022 RBE builds.,2025-03-04T22:50:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88584
copybara-service[bot],"litert: Define C APIs with `extern ""C""`.","litert: Define C APIs with `extern ""C""`.",2025-03-04T22:43:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88583
copybara-service[bot],Add `GpuEvent`. It will be used as a component of `TrackedTfrtGpuDeviceBuffer`.,Add `GpuEvent`. It will be used as a component of `TrackedTfrtGpuDeviceBuffer`. Reverts f6439c5f481f2769d9be994995bf2c176a99ea8a FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22383 from ROCm:ci_enable_hiphostMemRegister_20250205 0452d2b73a9c3145e53b08eb186e0fa9ec10bbb4,2025-03-04T22:37:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88582
copybara-service[bot],Set `--color` for all builds in `build.py`,Set `color` for all builds in `build.py`,2025-03-04T22:30:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88581
copybara-service[bot],PR #74506: Adds missing datatype support for various tflite operations,"PR CC(Adds missing datatype support for various tflite operations): Adds missing datatype support for various tflite operations Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/74506  Adds bf16, f16 support for tfl.atan2  Adds bf16,f16,int8,int16 for tfl.neg  Adds bf16, f16 support for tfl.min, tfl.max  Adds bf16, f16 support for tfl.slice  Adds bf16, f16 support for tfl.round  Adds bf16, f16 support for tfl.reverse  Adds bf16, f16 support for tfl.pad  Adds bf16, f16 support for tfl.tanh and tfl.logistic  Adds bf16, f16 support for tfl.floor Copybara import of the project:  775ae2b6aec166a107f41070cad1ed9bea25b696 by swatheeshmcw : TfLite neg_op add type support ( CC(1st Class Windows Support)) * TfLite neg_op add type support  Adds type support for bfloat16  Adds type supoort for float16  Adds type support for Int8  Adds type support for Int16 Coauthoredby: prathammcw   e108d4654398e7eb6eea17d38362001e0f36e054 by swatheeshmcw : Tflite tanh and logistic adds missing datatype support ( CC(cuDNN v2 (6.5) not available anymore)) * Tflite tanh and logistic adds missing datatype support Adds f16,bf16 for tanh and logistic Adds f16,bf16 for tanh and logistic unit test Coauthoredby: nitheshsrikanthmcw   18581a6cb30cfb0b66b01b5a4925cb7fb84a65cf by swatheeshmcw : Tflite floor adds missing datatype support ( CC(No module named copy_reg  Installation Issue)) * Tflite floor missing datatype support Adds f16,bf16 for floor Adds f16,bf16 for floor unit test Coauthoredby: nitheshsrikanthmcw   e0aaea195d9a0927cbaddff9cc9e5953bc4d5b72 by swatheeshmcw : TfLite Slice missing datatype support ( CC(Error while installing tensorflow using pip on Ubuntu 14.04 32bit system)) ( CC(Cannot run the android example on Android 5.1.1)) * TfLite Slice missing datatype support Adds bf16, f16 support for slice Adds bf16, f16 slice unit tests  458c6a4063c9b7c345d5cf68240e1f0a8f3ba277 by swatheeshmcw : TfLite Round missing datatype support ( CC(Connectionist Temporal Classification example)) ( CC(No plan for official doc of any other languages than English?)) * TfLite Round missing datatype support Adds bf16, f16 support for round Adds bf16, f16 round unit tests  5c6def1d96f7f60dac83501870670ba63308b983 by swatheeshmcw : Tflite min max missing datatypes support ( CC(Updated links in documentation.)) ( CC(Target //tensorflow/tools/pip_package:build_pip_package failed to build on OSX)) Adds bf16,f16 for tflite min max operations Adds bf16,f16 min max unit tests  baea4f1b7c53d1bb4f31ccdac5fcb1e75143c8b9 by swatheeshmcw : TfLite Reverse missing datatype support ( CC(Slack Channel)) ( CC(Can't install from source if I don't have a GPU? )) * TfLite Reverse missing datatype support  Adds bf16, f16 support for reverse  Adds bf16, f16 reverse unit tests  96ccf49a5534f3462f29572a492ec3ecb360d7a0 by Swatheesh Muralidharan : Addressed redefinition of TFLITE_TENSOR_TYPE_ASSOC error in test_util.h file  2ff70a8c0a6be4eabb5f6c49faabd98299afe918 by RahulSudarMCW : Include nonquantized int8 & int16 type support for tfl.pad Merging this change closes CC(Adds missing datatype support for various tflite operations) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/74506 from MCWDev:master 2ff70a8c0a6be4eabb5f6c49faabd98299afe918",2025-03-04T22:15:11Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/88580,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],[XLA:GPU] Fix incorrect buffer aliasing in ir_emitter_unnested for RaggedAllToAll.,[XLA:GPU] Fix incorrect buffer aliasing in ir_emitter_unnested for RaggedAllToAll.,2025-03-04T21:45:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88579
copybara-service[bot],#litert Fix `cc:litert_shared_library` when `RTLD_DI_LMID` is not defined.,litert Fix `cc:litert_shared_library` when `RTLD_DI_LMID` is not defined.,2025-03-04T21:34:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88578
copybara-service[bot],Remove unused `ifrt_proxy` library,Remove unused `ifrt_proxy` library,2025-03-04T21:33:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88577
copybara-service[bot],testing file for ci stuff,testing file for ci stuff,2025-03-04T21:04:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88576
copybara-service[bot],Integrate LLVM at llvm/llvm-project@22d8ba3dbc73,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 22d8ba3dbc73,2025-03-04T20:59:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88575
copybara-service[bot],litert: Update litert/c/litert_dispatch_delegate.h,litert: Update litert/c/litert_dispatch_delegate.h Align functions in dispatch_delegate.. Also removed the dependency to litert/c/litert_environment.h,2025-03-04T20:51:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88574
copybara-service[bot],[XLA] Optimize computation of computation postorders by directly representing the HloComputation dependency graph.,"[XLA] Optimize computation of computation postorders by directly representing the HloComputation dependency graph. Previously to compute a postorder of computations we had to look at every instruction to find HloComputations that called subcomputations. This is an expensive thing to do. But if we instead simply maintain the computation graph explicitly then computing a postorder of computations becomes a cheap things to do. The previous code also had another efficiency bug. It used `MakeEmbeddedComputationsList` to determine the child computations of a computation for the purposes of a depthfirst search. But that function computes the *transitive* callees, not just the direct children, so it did redundant work.",2025-03-04T20:33:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88573
copybara-service[bot],"Bump linux_arm64 whl size limit from 245M to 250M, since it's failing due to over-the-limit.","Bump linux_arm64 whl size limit from 245M to 250M, since it's failing due to overthelimit.",2025-03-04T20:13:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88572
copybara-service[bot],#litert Fix building `vendors/mediatek/dispatch:dispatch_api`.,litert Fix building `vendors/mediatek/dispatch:dispatch_api`.,2025-03-04T19:57:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88571
copybara-service[bot],#litert Add a function to load accelerators distributed as shared libraries.,litert Add a function to load accelerators distributed as shared libraries.,2025-03-04T19:46:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88570
copybara-service[bot],add genrules for pip wheel,add genrules for pip wheel,2025-03-04T19:24:27Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/88569,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],PR #23351: [ROCm] Fix build break in external udp due to stringop-truncation,PR CC(fake_quant_min_max_args/vars works unexpected): [ROCm] Fix build break in external udp due to stringoptruncation Imported from GitHub PR https://github.com/openxla/xla/pull/23351 Copybara import of the project:  d039829ba6a45807a13a2230cfb35e17590cd497 by Harsha HS : [ROCm] Fix build break in external udp due to stringoptruncation Merging this change closes CC(fake_quant_min_max_args/vars works unexpected) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23351 from ROCm:ci_fix_stringop_trunc_20250304 d039829ba6a45807a13a2230cfb35e17590cd497,2025-03-04T19:23:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88568
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@7b7d6ad4,Integrate StableHLO at openxla/stablehlo Reverts f6439c5f481f2769d9be994995bf2c176a99ea8a FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22383 from ROCm:ci_enable_hiphostMemRegister_20250205 0452d2b73a9c3145e53b08eb186e0fa9ec10bbb4,2025-03-04T19:13:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88567
copybara-service[bot],Make JAX pipeline parallelism example shorter,Make JAX pipeline parallelism example shorter,2025-03-04T19:02:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88566
copybara-service[bot],Fix ios compilation issue,Fix ios compilation issue,2025-03-04T18:47:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88565
copybara-service[bot],Only compile egl_environment for Android in litert_tensor_buffer_test.,Only compile egl_environment for Android in litert_tensor_buffer_test. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88412 from tensorflow:dependabot/docker/tensorflow/tools/tf_sig_build_dockerfiles/ubuntued1544e 4b6ba37c2667bdfded35b1774ed8ee438bc5fb25,2025-03-04T18:05:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88564
copybara-service[bot],PR #23355: [ROCm] Add rocm_base and rocm_gcc configs,"PR CC(Failed to load): [ROCm] Add rocm_base and rocm_gcc configs Imported from GitHub PR https://github.com/openxla/xla/pull/23355 Changed `rocm` config to `rocm_base`, and added `rocm_gcc`  so we can have gcc specific options. Since a lot of scripts still depend on `config=rocm` I have temporarily left it in .bazelrc. It will be removed once I transition everything to use `rocm_gcc` Copybara import of the project:  eaf35d55cb1b7700b4607678a9e31913f5a226b6 by Milica Makevic : Change rocm config to rocm_base and add rocm_gcc Merging this change closes CC(Failed to load) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23355 from ROCm:addrocmbase eaf35d55cb1b7700b4607678a9e31913f5a226b6",2025-03-04T17:49:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88563
copybara-service[bot],[NFC] Fix a comment.,[NFC] Fix a comment. Change ShapeUtil::Hash() to Shape::Hash() to match the code.,2025-03-04T16:23:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88562
copybara-service[bot],PR #23232: Add while iteration index API for HLOs.,"PR CC(Broken distributed training with tf.estimator.Estimator): Add while iteration index API for HLOs. Imported from GitHub PR https://github.com/openxla/xla/pull/23232 Currently, we have an API that retrieves a running while loop's iteration number using the depth of the loop in the current call stack. This is hard to use if you only have the HLO instruction of the loop and want to get its index. For the larger context,  see https://github.com/openxla/xla/compare/main...jreiffers:xla:memcpy and the companion document https://docs.google.com/document/d/1E2_Jt_Dw4VbPXPVktNWhtsDEtIpNkurCMGGyvMV4JA. Copybara import of the project:  193e731471e3609cd0b96b27ff23af8bd32bfb20 by Johannes Reifferscheid : Add while iteration index API for HLOs. Currently, we have an API that retrieves a running while loop's iteration number using the index of nested loops. This is hard to use if you only have the HLO instruction of the loop and want to get its index. For the larger context, see https://github.com/openxla/xla/compare/main...jreiffers:xla:memcpy and the companion document https://docs.google.com/document/d/1E2_Jt_Dw4VbPXPVktNWhtsDEtIpNkurCMGGyvMV4JA.  ace5f2ec052bc6d10ad8cbe76b27716b7b6c19d2 by Johannes Reifferscheid : Fix for_all_thunks_test. Merging this change closes CC(Broken distributed training with tf.estimator.Estimator) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23232 from jreiffers:whilethunk ace5f2ec052bc6d10ad8cbe76b27716b7b6c19d2",2025-03-04T16:12:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88561
copybara-service[bot],[XLA:GPU] Support async opcode in collective interpolator.,[XLA:GPU] Support async opcode in collective interpolator.,2025-03-04T15:47:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88560
copybara-service[bot],Integrate LLVM at llvm/llvm-project@22d8ba3dbc73,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 22d8ba3dbc73,2025-03-04T15:47:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88559
copybara-service[bot],[XLA] Enforce that instructions do not reference computations in other modules.,"[XLA] Enforce that instructions do not reference computations in other modules. Remove a call to `HloComputation::set_parent()` in the `HloInstruction` cloning code that at least introduces some ephemeral crossmodule edges. The `set_parent()` call seems misguided from an invariant point of view, since the cloned instruction has not yet been added to a computation and hence does not have a parent. This causes a number of downstream test failures, which we fix. We also make a small refactoring so called computations are set more consistently via a `HloInstruction::set_called_computation()` helper. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23395 from ROCm:ci_gfx950 6c6bfad5a896154c1a21c263cda433253e9f8597",2025-03-04T15:38:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88558
copybara-service[bot],[XLA:GPU] Interpolate collectives from perf tables.,[XLA:GPU] Interpolate collectives from perf tables.,2025-03-04T14:52:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88557
copybara-service[bot],PR #22897: Multihost HLO runner: enable partitioning when num_partitions > 1.,PR CC(Conversion from pb to tflite fails): Multihost HLO runner: enable partitioning when num_partitions > 1. Imported from GitHub PR https://github.com/openxla/xla/pull/22897 Copybara import of the project:  759005eca2e09eeae4adb5ad1358a8318b38a5b3 by Ilia Sergachev : [GPU] Multihost HLO runner: assume use_spmd_partitioning when num_partitions > 1. Merging this change closes CC(Conversion from pb to tflite fails) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22897 from openxla:use_spmd 759005eca2e09eeae4adb5ad1358a8318b38a5b3,2025-03-04T14:27:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88556
copybara-service[bot],Reverts 28a5480c25b88ebe9133241f99062f3089eb66b9,Reverts 28a5480c25b88ebe9133241f99062f3089eb66b9,2025-03-04T14:22:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88555
copybara-service[bot],[pjrt] Inlined deprecated `PjRtBuffer::BlockUntilReady`,[pjrt] Inlined deprecated `PjRtBuffer::BlockUntilReady`,2025-03-04T13:59:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88554
copybara-service[bot],[XLA:GPU] Add RaggedAllToAllCanonicalizer pass.,[XLA:GPU] Add RaggedAllToAllCanonicalizer pass. In too many places we need to assume or work around the element type of offset and size operands of raggedalltoall. It's much easier to do an HLO rewrite. The added converts are tiny and will likely be fused with other operation. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88412 from tensorflow:dependabot/docker/tensorflow/tools/tf_sig_build_dockerfiles/ubuntued1544e 4b6ba37c2667bdfded35b1774ed8ee438bc5fb25,2025-03-04T13:31:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88553
copybara-service[bot],[XLA:CPU][tfcompile] Use thunk runtime for tfcompiled models.,[XLA:CPU][tfcompile] Use thunk runtime for tfcompiled models.,2025-03-04T13:26:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88552
copybara-service[bot],[LiteRT] Skip passing the dispatch op name as the southbound API expects an empty function name.,[LiteRT] Skip passing the dispatch op name as the southbound API expects an empty function name.,2025-03-04T12:53:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88551
copybara-service[bot],[XLA:GPU] Add interpolation base for collectives.,[XLA:GPU] Add interpolation base for collectives.,2025-03-04T12:26:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88550
Liu-Jitai,[RNN] tensorflow/lite/kernels/transpose.cc:62 op_context->perm->dims->data[0] != dims (3 != 2)Node number 3 (TRANSPOSE) failed to prepare," 1. System information  Window 10  PyCharm 2024.2.3  TensorFlow 2.13  Python 3.8  2. Code Keras model convert to TFLite model fail, please help find the debug method.  TensorFlow Model Colab (Train a TensorFlow Keras LSTM Model for SIN funtion regression using random generated dataset) ([TensorFlow Model Colab]: (https://colab.research.google.com/gist/LiuJitai/08faad02c37315eb09576e85f6df44eb/tensorflowdatasets.ipynb)  Keras model convert to TFLite model Code [Keras model convert to TFLite model]  Fail at tflite_model_quant = converter.convert()  Import libraries import logging import tensorflow as tf from tensorflow import keras import numpy as np from tensorflow.keras.models import load_model from sklearn.preprocessing import MinMaxScaler  Helper functions def generate_data(seq_length, sequences_num):     x = []     y = []     for _ in range(sequences_num):         start = np.random.rand() * 2 * np.pi         sequence = np.sin(np.linspace(start, start + 3 * np.pi, seq_length + 1))         x.append(sequence[:1])         y.append(sequence[1:])   shift sequence by one to predict next value     return np.array(x), np.array(y) def representative_data_gen():     for k in model_input:         yield [k]  Generate dataset seq_length = 50 num_sequences = 100 model_input, model_output = generate_data(seq_length, num_sequences)  Normalization scaler = MinMaxScaler(feature_range=(0, 1)) model_input = scaler.fit_transform(model_input) model_output = scaler.transform(model_output)    Reshape the data to fit the LSTM input. The LSTM model expects input of shape (batch_size, time_steps, features), which here corresponds to (num_sequences, seq_length, 1). model_input = model_input.reshape((model_input.shape[0], model_input.shape[1], 1)) model_input = np.array(model_input, dtype=np.float32)  Load Keras model model = load_model('LSTM_Sin_model.keras')  coverter converter = tf.lite.TFLiteConverter.from_keras_model(model)  set the ops configuration converter.optimizations = [tf.lite.Optimize.DEFAULT]  converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS_INT8,       tf.lite.OpsSet.SELECT_TF_OPS       ] converter.inference_input_type = tf.int8  converter.inference_output_type = tf.int8  converter.representative_dataset = representative_data_gen tflite_model_quant = converter.convert() ** **fail at this step: tensorflow/lite/kernels/transpose.cc:62 op_context>perm>dims>data[0] != dims (3 != 2)Node number 3 (TRANSPOSE) failed to prepare****  save .tflite model tflite_model_path = 'LSTM_sin_model.tflite' with open(tflite_model_path, 'wb') as f:     f.write(tflite_model_quant) print(f""Model successfully converted to TFLite format and saved to {tflite_model_path}"")",2025-03-04T12:10:24Z,comp:lite TFLiteConverter TF 2.13,closed,0,3,https://github.com/tensorflow/tensorflow/issues/88549,"Hi, Jitai  Thank you for bringing this issue to our attention, I have been able to replicate the same behavior from my end with your provided code snippet but I think there was issue with directly yields samples without a batch dimension: `yield [k]` This produces data with shape (50, 1) causing a dimension mismatch. The model expects 3D inputs but receives 2D data leading to the TRANSPOSE node error so I've modified your provided code for missing batch dimension in the representative dataset and it seems like working as expected please refer this gistfile. Please give it try from your end and see is it working as expected or not ? If I have missed something here please let me know. Thank you for your cooperation and understanding. ``` import tensorflow as tf import numpy as np from tensorflow.keras.models import load_model from sklearn.preprocessing import MinMaxScaler def generate_data(seq_length, sequences_num):     x, y = [], []     for _ in range(sequences_num):         start = np.random.rand() * 2 * np.pi         sequence = np.sin(np.linspace(start, start + 3 * np.pi, seq_length + 1))         x.append(sequence[:1])         y.append(sequence[1:])     return np.array(x), np.array(y) def representative_data_gen():     for input_value in model_input:         yield [np.array([input_value], dtype=np.float32)]   seq_length = 50 num_sequences = 100 model_input, model_output = generate_data(seq_length, num_sequences) scaler = MinMaxScaler(feature_range=(0, 1)) model_input = scaler.fit_transform(model_input) model_output = scaler.transform(model_output) model_input = model_input.reshape((1, seq_length, 1)).astype(np.float32) model = load_model('/content/LSTM_Sin_model.keras') converter = tf.lite.TFLiteConverter.from_keras_model(model) converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS_INT8,       tf.lite.OpsSet.SELECT_TF_OPS      ] converter.representative_dataset = representative_data_gen converter.inference_input_type = tf.int8 converter.inference_output_type = tf.int8 tflite_model_quant = converter.convert() tflite_model_path = '/content/LSTM_sin_model.tflite' with open(tflite_model_path, 'wb') as f:     f.write(tflite_model_quant) print(f""Model successfully converted to TFLite format and saved to {tflite_model_path}"") ```",您好，您所发的邮件我已经收到，我会尽快阅读，祝您工作愉快！,The error was solved. Thank you so much for your help! 
pemeliya,[ROCM][NFC] ExecuteOnStream interface change as part of GpuBlasLt refactoring,This PR is to be merged together with this XLA PR which required small interface change: https://github.com/openxla/xla/pull/23315,2025-03-04T10:48:15Z,size:XS comp:core,closed,0,3,https://github.com/tensorflow/tensorflow/issues/88548,", the main XLA PR (https://github.com/openxla/xla/pull/23315) is already merged. As the rewriver suggested, I added a shim to make sure it won't break TF build. Now, once this TF PR is merged, we can remove the shim from XLA's gpu_blas_lt interface.",", I wonder if this PR is still blocked because Google internal checks are failed ?  I am not able to see what's wrong with it..","There was a stuck job, but should be good now"
copybara-service[bot],[XLA:GPU] Add ragged-all-to-all tests with multiple replica groups.,"[XLA:GPU] Add raggedalltoall tests with multiple replica groups. As a side effect, we can also support tests where devices in a replica group are not in increasing order.",2025-03-04T10:20:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88547
chunhsue,Qualcomm AI Engine Direct - Wrapper tests & Refactor tensor wrapper & Fix rms norm, What 1. Refactor tensor wrapper      Safer tensor data getter and setter. 2. Add wrapper tests. 3. Fix rms norm builder      Change 0 beta tensor type to float32 for float32 input and uint8 for other input data types. See op support types here.   Tests `qnn_compiler_plugin_test` ``` [] Global test environment teardown [==========] 99 tests from 5 test suites ran. (3995 ms total) [  PASSED  ] 99 tests. ```,2025-03-04T10:17:17Z,comp:lite size:XL,closed,0,1,https://github.com/tensorflow/tensorflow/issues/88546,", quic,   Can you also help review? "
copybara-service[bot],Reverts b70de1e127eace867cf6823d95bd1b4a83e238e2,Reverts b70de1e127eace867cf6823d95bd1b4a83e238e2,2025-03-04T09:49:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88545
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-04T09:27:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88544
copybara-service[bot],Add new BF16_BF16_F32_X9 to known dot algorithm,Add new BF16_BF16_F32_X9 to known dot algorithm,2025-03-04T09:03:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88543
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-04T08:45:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88542
copybara-service[bot],Handle broadcast result shapes with zero elements.,Handle broadcast result shapes with zero elements.,2025-03-04T08:44:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88541
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-04T08:34:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88540
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-04T08:33:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88539
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-04T08:33:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88538
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-04T08:30:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88537
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-04T08:23:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88536
copybara-service[bot],Integrate LLVM at llvm/llvm-project@fd9a882ce31c,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match fd9a882ce31c,2025-03-04T07:54:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88535
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-04T07:53:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88534
copybara-service[bot],Handle npu call composites in partitioning.,Handle npu call composites in partitioning.,2025-03-04T07:47:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88533
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-04T07:42:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88532
copybara-service[bot],Add value_str() function in TableCell for csv dump,Add value_str() function in TableCell for csv dump FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23351 from ROCm:ci_fix_stringop_trunc_20250304 d039829ba6a45807a13a2230cfb35e17590cd497,2025-03-04T06:56:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88531
wonjeon,[mlir][tosa] Fix strided_slice legalization when input_dim % stride != 0, Add padding to round an input dim up to the nearest multiple of the corresponding stride value ChangeId: I1b9677c80e7ba704897637a3c44545d24cdae892,2025-03-04T06:01:41Z,size:M comp:lite-tosa,closed,0,2,https://github.com/tensorflow/tensorflow/issues/88530,"Local testing successfully done: INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 3.623s, Critical Path: 2.02s INFO: 2 processes: 2 local. INFO: Build completed successfully, 2 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test    (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test       (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test  (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test (cached) PASSED in 0.1s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 1.9s Executed 1 out of 17 tests: 17 tests pass.",This should be reviewed by MLIR / compiler folks
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-04T05:54:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88529
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-04T05:50:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88528
copybara-service[bot],Clean up deprecated param.,Clean up deprecated param.,2025-03-04T05:43:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88527
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-04T05:08:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88526
copybara-service[bot],Add ml_drift_cl_litert for LiteRT GPU Accelerator,Add ml_drift_cl_litert for LiteRT GPU Accelerator Added DelegateKernelLiteRt to serve TensorBuffer integration.  Publish TensorBufferRequirements in kLiteRtTensorBufferTypeOpenCl  Bind TensorBuffer by BindTensorBuffers()  Simpler Invoke() implementation FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23426 from shawnwang18:shawnwn/enable_command_buffer_cublaslt_default 3d2f748916dc6a9c72c681f9bd9fc46f03d396ee,2025-03-04T05:00:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88525
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23051 from openxla:cleanup_pipeline_def 0f0ce7ee36498d41c60f6128f9cd58a0908c506e,2025-03-04T04:42:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88524
copybara-service[bot],PR #23271: [XLA] Improve GPU memory limit handling and shape size calculation,"PR CC(Improve shape function of tf.sparse_reduce_sum): [XLA] Improve GPU memory limit handling and shape size calculation Imported from GitHub PR https://github.com/openxla/xla/pull/23271 When running MaxText Llama27b with FP16 optimizer state offloading, an out of memory failure occurred. The root cause was the int64_t memory limit being incorrectly calculated and interpreted as a large uint64_t memory limit close to UINT64_MAX, leading to overly aggressive buffer allocation by the LHS. This changelist addresses the out of memory failure by correctly handling the uint64_t memory limit and accurately calculating the device memory limit, excluding host memory space.  Change memory limit type from int64_t to uint64_t to prevent negative values and better represent memory sizes.  Add memory space filtering to exclude host memory when calculating input/output sizes that impact GPU memory usage. This ensures we only count buffers that actually reside in device memory.  Replace direct GetSizeOfShape() calls with ShapeSizeBytesFunction() wrapper, which provides:   * Consistent shape size calculation across the codebase   * Optional memory space filtering capability   * Proper handling of dynamic shapes and their metadata Copybara import of the project:  8b3184065c78f98f62e97e9a94e7d49e1797bd7b by Jane Liu : [XLA] Improve GPU memory limit handling and shape size calculation Merging this change closes CC(Improve shape function of tf.sparse_reduce_sum) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23271 from zhenyingliu:lhsoom 8b3184065c78f98f62e97e9a94e7d49e1797bd7b",2025-03-04T03:53:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88523
copybara-service[bot],PR #23051: [GPU][NFC] Cleanup GPU compiler pipeline definition.,PR CC([Experiment] Reformat issue template. ): [GPU][NFC] Cleanup GPU compiler pipeline definition. Imported from GitHub PR https://github.com/openxla/xla/pull/23051 Rename the pipeline and update/remove misleading comments as it runs before scheduling and all the related passes thus well before IR emission. Copybara import of the project:  0f0ce7ee36498d41c60f6128f9cd58a0908c506e by Ilia Sergachev : [GPU][NFC] Cleanup GPU compiler pipeline definition. Merging this change closes CC([Experiment] Reformat issue template. ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23051 from openxla:cleanup_pipeline_def 0f0ce7ee36498d41c60f6128f9cd58a0908c506e,2025-03-04T03:48:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88522
copybara-service[bot],Change a Q/DQ->Q to a Requant op,"Change a Q/DQ>Q to a Requant op When DQ>Q have different scales, they can be squashed into one quantize op with different quantized types for input and output.",2025-03-04T02:43:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88521
copybara-service[bot],Extend `DirectSession::Finalize()` to finalize the resource managers,Extend `DirectSession::Finalize()` to finalize the resource managers,2025-03-04T02:12:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88520
copybara-service[bot],Use the full contracting dimensions instead of the partitioned contracting dimensions in `GetDotGroupPartitionContractingOutputShardings`.,Use the full contracting dimensions instead of the partitioned contracting dimensions in `GetDotGroupPartitionContractingOutputShardings`.,2025-03-04T01:50:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88519
copybara-service[bot],Use plugin's `GpuClientOptions` to replace `TfrtGpuClient::Options` when initialize TfrtGpuClient,Use plugin's `GpuClientOptions` to replace `TfrtGpuClient::Options` when initialize TfrtGpuClient,2025-03-04T01:25:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88518
copybara-service[bot],Integrate LLVM at llvm/llvm-project@fd9a882ce31c,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match fd9a882ce31c,2025-03-03T23:58:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88517
copybara-service[bot],Fix pytree registrations on experimental kvcache class to properly round trip derived types. Also add a flatten/unflatten for the entry types.,Fix pytree registrations on experimental kvcache class to properly round trip derived types. Also add a flatten/unflatten for the entry types.,2025-03-03T23:55:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88516
copybara-service[bot],PR #23237: [ROCm] Remove gfx940 and gfx941,PR CC(1.12.0rc2 cherrypick request: Fix string comparison in `configured`): [ROCm] Remove gfx940 and gfx941 Imported from GitHub PR https://github.com/openxla/xla/pull/23237 https://github.com/llvm/llvmproject/commit/8615f9aaffd4337a33ea979f010c4d6410ba6125 This PR removes all occurrences of gfx940/gfx941 Copybara import of the project:  4bab9ba12920c95bf35d050da6fc162fc519e994 by Milica Makevic : Remove gfx940 and gfx941 Merging this change closes CC(1.12.0rc2 cherrypick request: Fix string comparison in `configured`) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23237 from ROCm:remove_gfx940_941 4bab9ba12920c95bf35d050da6fc162fc519e994,2025-03-03T23:52:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88515
copybara-service[bot],Open visibility for ComputePerTpuStepDataAcrossCores.,Open visibility for ComputePerTpuStepDataAcrossCores.,2025-03-03T23:48:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88514
copybara-service[bot],PR #23301: Uses absl::string_view in RegisterCustomCallPartitioner,PR CC(Fix Tdistribution sampling parameter): Uses absl::string_view in RegisterCustomCallPartitioner Imported from GitHub PR https://github.com/openxla/xla/pull/23301 Copybara import of the project:  3cf40c14097619973533fad244b054b6a4ea4b3a by Yunlong Liu : Uses absl::string_view  c8c1a80c537be6c7f33448c92fe00dbd7605043a by Yunlong Liu : Update custom_call_sharding_helper. : Update BUILD Merging this change closes CC(Fix Tdistribution sampling parameter) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23301 from yliu120:patch2 6d9c2be1ed28e55dce6afb71bbb9663a138a89c6,2025-03-03T23:46:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88513
copybara-service[bot],PR #22391: [XLA:GPU] Reduce CUDA plugin registration messages to info,"PR CC(Fixed broken links): [XLA:GPU] Reduce CUDA plugin registration messages to info Imported from GitHub PR https://github.com/openxla/xla/pull/22391 This pull request is to reduce the CUDA plugin registration log level to avoid user confusing confusion in https://github.com/openxla/xla/issues/20803. Actually, no functionality errors happened with CUDA plugin registration. Copybara import of the project:  6b3aedfe9146189c8c551e5984ce27a216fc2768 by johnnkp : cuda_blas.cc: reduce log level to info  e36fbd77b0dd5b2ce6a10a0d0e85af868b404e1a by johnnkp : cuda_dnn.cc: reduce log level to info  b40156a6e3fb9c8ed2c97d0520ed19622f4a5ce0 by johnnkp : cuda_fft.cc: reduce log level to info Merging this change closes CC(Fixed broken links) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22391 from johnnkp:main b40156a6e3fb9c8ed2c97d0520ed19622f4a5ce0",2025-03-03T23:45:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88512
copybara-service[bot],Add handling of XlaOps to GPU KernelReports,Add handling of XlaOps to GPU KernelReports,2025-03-03T23:44:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88511
copybara-service[bot],PR #23271: [XLA] Improve GPU memory limit handling and shape size calculation,"PR CC(Improve shape function of tf.sparse_reduce_sum): [XLA] Improve GPU memory limit handling and shape size calculation Imported from GitHub PR https://github.com/openxla/xla/pull/23271 When running MaxText Llama27b with FP16 optimizer state offloading, an out of memory failure occurred. The root cause was the int64_t memory limit being incorrectly calculated and interpreted as a large uint64_t memory limit close to UINT64_MAX, leading to overly aggressive buffer allocation by the LHS. This changelist addresses the out of memory failure by correctly handling the uint64_t memory limit and accurately calculating the device memory limit, excluding host memory space.  Change memory limit type from int64_t to uint64_t to prevent negative values and better represent memory sizes.  Add memory space filtering to exclude host memory when calculating input/output sizes that impact GPU memory usage. This ensures we only count buffers that actually reside in device memory.  Replace direct GetSizeOfShape() calls with ShapeSizeBytesFunction() wrapper, which provides:   * Consistent shape size calculation across the codebase   * Optional memory space filtering capability   * Proper handling of dynamic shapes and their metadata Copybara import of the project:  8b3184065c78f98f62e97e9a94e7d49e1797bd7b by Jane Liu : [XLA] Improve GPU memory limit handling and shape size calculation Merging this change closes CC(Improve shape function of tf.sparse_reduce_sum) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23271 from zhenyingliu:lhsoom 8b3184065c78f98f62e97e9a94e7d49e1797bd7b",2025-03-03T23:28:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88510
copybara-service[bot],Bump minimum protobuf version to 4.21.6,Bump minimum protobuf version to 4.21.6,2025-03-03T23:24:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88509
copybara-service[bot],"Delete obsolete tools, dcn collective stats and tf data bottleneck analysis.","Delete obsolete tools, dcn collective stats and tf data bottleneck analysis.",2025-03-03T22:52:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88508
copybara-service[bot],Add GetGlBuffer for AHWB Tensor Buffers.,Add GetGlBuffer for AHWB Tensor Buffers.,2025-03-03T22:51:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88507
copybara-service[bot],Move the make dispatch op function to model header so it can be shared,Move the make dispatch op function to model header so it can be shared,2025-03-03T22:42:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88506
copybara-service[bot],Add visibility constraints to compiler/xla/... *.bzl files to prevent them from being used outside of XLA.,Add visibility constraints to compiler/xla/... *.bzl files to prevent them from being used outside of XLA.,2025-03-03T22:36:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88505
copybara-service[bot],Update the IR allocator to handle two way transfers and specified inds. Make sure it properly reindexes composites left in the graph.,Update the IR allocator to handle two way transfers and specified inds. Make sure it properly reindexes composites left in the graph.,2025-03-03T22:35:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88504
copybara-service[bot],Stablize torch composite outlining order for nested composites,Stablize torch composite outlining order for nested composites,2025-03-03T22:20:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88503
copybara-service[bot],[MPMD][NFC] Make MPMD device agonostic by replacing `tpu::TpuTopology` with `PjRtTopologyDescriptionProto` in the stack.,[MPMD][NFC] Make MPMD device agonostic by replacing `tpu::TpuTopology` with `PjRtTopologyDescriptionProto` in the stack.,2025-03-03T22:16:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88502
copybara-service[bot],Modify lock/unlock for AHWB-backed GL buffers.,Modify lock/unlock for AHWBbacked GL buffers.,2025-03-03T22:11:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88501
copybara-service[bot],Added `WatchJobState` to coordination service agent.,Added `WatchJobState` to coordination service agent.,2025-03-03T22:09:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88500
copybara-service[bot],Remove redundant BUILD_TAG from the repository rule.,Remove redundant BUILD_TAG from the repository rule. Reverts b70de1e127eace867cf6823d95bd1b4a83e238e2,2025-03-03T21:09:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88499
copybara-service[bot],"Nit, ifrt proxy: additional logging","Nit, ifrt proxy: additional logging",2025-03-03T21:04:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88498
copybara-service[bot],PR #23314: [debug_options] Dump DebugOptions while compiling.,PR CC(Changing learning rate of the optimizer in eager mode): [debug_options] Dump DebugOptions while compiling. Imported from GitHub PR https://github.com/openxla/xla/pull/23314 This patch adds VLOG(2) for dumping debug options while compiling. This is a reliable way to retrieve the debug options based on the log. Copybara import of the project:  414e97757ea299d10f90dfad987d1898c9f30feb by Shraiysh Vaishay : [debug_options] Dump DebugOptions while compiling. This patch adds VLOG(2) for dumping debug options while compiling. This is a reliable way to retrieve the debug options based on the log. Merging this change closes CC(Changing learning rate of the optimizer in eager mode) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23314 from shraiysh:dump_debug_options 414e97757ea299d10f90dfad987d1898c9f30feb,2025-03-03T20:55:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88497
copybara-service[bot],Use arguments rather than environment variables to specify the `Build` to run in `build.py`,Use arguments rather than environment variables to specify the `Build` to run in `build.py` We no longer use Kokoro so we should not rely on the existence of the `KOKORO_JOB_NAME` env var,2025-03-03T20:34:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88496
copybara-service[bot],Integrate LLVM at llvm/llvm-project@17857d92416d,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 17857d92416d,2025-03-03T20:17:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88495
copybara-service[bot],Bump libtpu versions to pick correct versioned nightlies,Bump libtpu versions to pick correct versioned nightlies,2025-03-03T20:04:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88494
copybara-service[bot],Do not eliminate DQ-Q if the quant params are not the same.,Do not eliminate DQQ if the quant params are not the same. The pattern in the optimize pass was overly permissive. The removed pattern used to eliminate DQ>Q even when the input and output types were different as long as the defining op of DQ is not a Q.,2025-03-03T19:54:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88493
wonjeon,[mlir][tosa] Remove out_shape from transpose_conv2d and Legalize Pad op's pad_const to be rank-1,We've been pushing TOSA v1.0 LLVM patches to upstream. This PR includes commits to keep the following operators to be aligned with LLVM: [mlir][tosa] Remove out_shape from transpose_conv2d https://github.com/llvm/llvmproject/pull/129133 [mlir][tosa] Require PadOp's pad_const to be rank1 https://github.com/llvm/llvmproject/pull/129156,2025-03-03T19:43:11Z,kokoro:force-run ready to pull size:L,closed,0,3,https://github.com/tensorflow/tensorflow/issues/88492,"Local testing successfully done: INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 1.758s, Critical Path: 0.05s INFO: 1 process: 1 internal. INFO: Build completed successfully, 1 total action //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test    (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test       (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test  (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test (cached) PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test (cached) PASSED in 2.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test (cached) PASSED in 0.1s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test (cached) PASSED in 0.3s Executed 0 out of 17 tests: 17 tests pass.",Thanks Ge  for your review. Some unrelated test cases were removed., 
copybara-service[bot],Disable warning about changed files in the tar operation.,Disable warning about changed files in the tar operation.,2025-03-03T18:42:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88491
copybara-service[bot],Disable warning about changed files in the tar operation.,Disable warning about changed files in the tar operation.,2025-03-03T18:05:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88490
copybara-service[bot],Refactor visibility rules for xla/mlir,Refactor visibility rules for xla/mlir,2025-03-03T17:57:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88489
copybara-service[bot],[xla:sdy] Fix c++20 compilation error,"[xla:sdy] Fix c++20 compilation error captured structured bindings are a C++20 extension [Werror,Wc++20extensions]",2025-03-03T17:16:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88488
copybara-service[bot],[XLA:GPU] Use rendezvous to exchange pointers and CUDA events in memcpy ragged-all-to-all.,"[XLA:GPU] Use rendezvous to exchange pointers and CUDA events in memcpy raggedalltoall. The previous implementation used NCCL AllGather to exchange target device pointers and synchronize streams at the start of the kernel, but it didn't synchronize streams at the end of the kernel. Since we use push model for memcpy, it could be that one stream progresses before all updates have arrived. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23351 from ROCm:ci_fix_stringop_trunc_20250304 d039829ba6a45807a13a2230cfb35e17590cd497",2025-03-03T17:12:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88487
copybara-service[bot],Move `--verbose_failures` option to the command that builds the first wheel.,Move `verbose_failures` option to the command that builds the first wheel.,2025-03-03T17:04:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88486
copybara-service[bot],[XLA:GPU] Fix merge utility not saving network throughput.,[XLA:GPU] Fix merge utility not saving network throughput.,2025-03-03T16:52:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88485
copybara-service[bot],Specify java runtime as mentioned in https://github.com/bazelbuild/bazel/releases/tag/7.0.0,Specify java runtime as mentioned in https://github.com/bazelbuild/bazel/releases/tag/7.0.0,2025-03-03T16:20:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88484
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f6212c1cd3d8,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f6212c1cd3d8,2025-03-03T16:16:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88483
copybara-service[bot],PR #23302: Use OSS-friend test macros in GPU PJRT client test.,"PR CC(Autotune seems to be ""forgetting"" after a short delay within the same session.): Use OSSfriend test macros in GPU PJRT client test. Imported from GitHub PR https://github.com/openxla/xla/pull/23302 Copybara import of the project:  692eac6426cae82f2cec76eead8908e7b47b4101 by Yunlong Liu : Use OSSfriend test macros. Merging this change closes CC(Autotune seems to be ""forgetting"" after a short delay within the same session.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23302 from yliu120:patch3 692eac6426cae82f2cec76eead8908e7b47b4101",2025-03-03T15:24:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88482
vwrewsge,`Floating point exception` in `tf.nn.max_pool`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In tfnightly (2.20.0dev20250302), a floating point exception occurs, but this issue does not occur in the current stable version.  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np input_tensor = tf.constant(np.random.rand(1, 1, 1, 3), dtype=tf.float32) output = tf.nn.max_pool(input_tensor,                         ksize=[1, 2, 2, 1],                         strides=[1, 1, 1, 1],                         padding='VALID')  Floating point exception ```  Relevant log output ```shell Floating point exception ```",2025-03-03T14:10:23Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88481,"Hi **** , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly version, but I did not encounter any issues. Please find the gist attached for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f6212c1cd3d8,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f6212c1cd3d8,2025-03-03T14:02:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88480
copybara-service[bot],#sdy Fix bug in Shardy deduplication pass when an empty mesh exists and add it into `addSdyRoundTripExportPipeline`.,sdy Fix bug in Shardy deduplication pass when an empty mesh exists and add it into `addSdyRoundTripExportPipeline`.,2025-03-03T14:02:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88479
copybara-service[bot],#sdy Add extra `export_test.py` tests for using different meshes..,"sdy Add extra `export_test.py` tests for using different meshes.. Under Shardy, we can:  use the same mesh on save and load  use one mesh on save and another mesh on load with different axis names  use one mesh on save and another mesh on load with different axis names and sizes. For this case Shardy propagation may not be optimal if the module doesn't specify out shardings. This is very hard to write a unit test for, and is rare to happen, and is something we have been considering adding in Shardy b/399957785. This will be something we can allow for during Shardy propagation. This can be a standalone fix in Shardy without making any changes to JAX or XLA.",2025-03-03T13:58:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88478
copybara-service[bot],"Remove uses of the --apple_bitcode flag, which is no longer supported with Bazel 7","Remove uses of the apple_bitcode flag, which is no longer supported with Bazel 7 See https://github.com/bazelbuild/bazel/commit/37b8e1b37d5676b3d047b528149546bf2bfefdd1",2025-03-03T13:52:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88477
vwrewsge,`Aborted` error when using XLA with JIT compiled conv2d function on XLA_CPU device," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.20.0dev20250302  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When running the following code with TensorFlow, an `Aborted` error occurs.  Standalone code to reproduce the issue ```shell import os os.environ[""XLA_FLAGS""] = ""xla_cpu_use_thunk_runtime=false tf_xla_enable_xla_devices"" import tensorflow as tf .function(jit_compile=True) def conv_fn(x, filt):     return tf.nn.conv2d(x, filt, strides=[1, 2, 2, 1], padding=""SAME"") input_tensor = tf.random.normal([1, 224, 224, 3]) filter_tensor = tf.random.normal([7, 7, 3, 64]) with tf.device(""/device:XLA_CPU:0""):     _ = conv_fn(input_tensor, filter_tensor) ```  Relevant log output ```shell Aborted ```",2025-03-03T11:46:38Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/88476,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[XLA:GPU] Generate literals based on primitive types from the Hlo module.,[XLA:GPU] Generate literals based on primitive types from the Hlo module. Small refactoring to make it easier to write tests with different primitive types. Before we needed to have consistent native type in the template argument and primitive type in the literal shape.,2025-03-03T11:00:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88475
copybara-service[bot],Integrate LLVM at llvm/llvm-project@cb7030dbe7f3,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match cb7030dbe7f3,2025-03-03T10:53:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88474
copybara-service[bot],[XLA:GPU] Add support for nested fusions in symbolic tiling.,[XLA:GPU] Add support for nested fusions in symbolic tiling.,2025-03-03T09:29:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88473
copybara-service[bot],Integrate LLVM at llvm/llvm-project@a085da66783e,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match a085da66783e,2025-03-03T08:26:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88472
971766904,"build tensorflow 2.15 from source code successfully,but some support doesn't build"," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.15  Custom code Yes  OS platform and distribution linux  Mobile device _No response_  Python version 3.12  Bazel version 6.10  GCC/compiler version 9.1  CUDA/cuDNN version 12.2/8.1  GPU model and memory _No response_  Current behavior? the tensorflow 2.15 gpu is successfully built, but some support is not include like absl, eigen and unsupported. The libtensorflow_framework.so didn't build as well.  Standalone code to reproduce the issue ```shell main.cpp include  include ""tensorflow/cc/client/client_session.h"" ```  Relevant log output ```shell /usr/local/include/tensorflow/core/framework/tensor.h:25:10: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory    25 | include ""unsupported/Eigen/CXX11/Tensor""  // from  ```",2025-03-03T06:18:31Z,type:build/install subtype: ubuntu/linux TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88471,Try manually install the missing components (Eigen Headers) as well as build and install the libtensorflow_framework.so shared library.,"Try something like this...  ``` !/bin/bash if [ ! d ""tensorflow"" ]; then   echo ""Error: Please run this script from the TensorFlow source root directory""   exit 1 fi echo ""Installing build dependencies..."" sudo aptget update sudo aptget install y python3dev python3pip python3numpy echo ""Configuring TensorFlow build..."" python3 m pip install U pip numpy wheel python3 m pip install U keras_preprocessing ./configure echo ""Building TensorFlow with framework library and C++ API..."" bazel build config=opt config=cuda \     //tensorflow:libtensorflow_cc.so \     //tensorflow:libtensorflow_framework.so \     //tensorflow:install_headers echo ""Installing TensorFlow libraries and headers..."" sudo mkdir p /usr/local/include/tensorflow sudo mkdir p /usr/local/lib sudo cp r bazelbin/tensorflow/include/* /usr/local/include/ sudo mkdir p /usr/local/include/third_party sudo cp r bazeltensorflow/external/eigen_archive/Eigen /usr/local/include/ sudo cp r bazeltensorflow/external/eigen_archive/unsupported /usr/local/include/ sudo cp bazelbin/tensorflow/libtensorflow_cc.so /usr/local/lib/ sudo cp bazelbin/tensorflow/libtensorflow_framework.so /usr/local/lib/ sudo ldconfig echo ""Verifying installation..."" ls la /usr/local/include/unsupported/Eigen/CXX11/Tensor ls la /usr/local/lib/libtensorflow_framework.so echo ""Installation completed. You can now build C++ applications with TensorFlow."" echo ""Creating and building a test program..."" cat > test.cpp  include ""tensorflow/cc/client/client_session.h"" include ""tensorflow/cc/ops/standard_ops.h"" include ""tensorflow/core/framework/tensor.h"" int main() {     tensorflow::Scope scope = tensorflow::Scope::NewRootScope();     auto a = tensorflow::ops::Const(scope, 3.0f);     auto b = tensorflow::ops::Const(scope, 2.0f);     auto c = tensorflow::ops::Add(scope, a, b);     tensorflow::ClientSession session(scope);     std::vector outputs;     session.Run({c}, &outputs);     std::cout () << std::endl;     return 0; } EOF g++ std=c++14 test.cpp o test_tensorflow \     I/usr/local/include \     L/usr/local/lib \     ltensorflow_cc ltensorflow_framework echo ""If compilation succeeded, you can run the test with: ./test_tensorflow"" ```"," Thank you for your answer.  ``` bazel build config=opt config=cuda \     //tensorflow:libtensorflow_cc.so \     //tensorflow:libtensorflow_framework.so \     //tensorflow:install_headers ``` this works for me. ``` sudo cp r bazelbin/tensorflow/include/* /usr/local/include/ ```  copying the include folder will help. ``` sudo cp r bazeltensorflow/external/eigen_archive/Eigen /usr/local/include/ sudo cp r bazeltensorflow/external/eigen_archive/unsupported /usr/local/include/ ```  these are not necessory. Anyway, thanks a lot.",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Add ResultAccuracy to StableHLO for unary functions:,Add ResultAccuracy to StableHLO for unary functions:   * Cbrt   * Cos   * Expm1   * Log   * Log1p   * Logistic   * Rsqrt   * Sin   * Sqrt   * Tan   * Tanh,2025-03-03T06:06:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88470
971766904,build tensorflow from source code, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf r2.10  Custom code Yes  OS platform and distribution linux  Mobile device _No response_  Python version 3.8  Bazel version 5.11  GCC/compiler version 9.1  CUDA/cuDNN version 11.8/8.1  GPU model and memory _No response_  Current behavior? FAILED: Build did NOT complete successfully I want to finish the building  Standalone code to reproduce the issue ```shell bazel build config=opt config=cuda //tensorflow:libtensorflow_cc.so ```  Relevant log output ```shell /mypool/zy/tf_compile/tf210file/tensorflowr2.10/tensorflow/cc/BUILD:780:22: Executing genrule //tensorflow/cc:resource_variable_ops_genrule failed: (Exit 127): bash failed: error executing command /bin/bash c ... (remaining 1 argument skipped) bazelout/k8opt/bin/tensorflow/cc/ops/resource_variable_ops_gen_cc: symbol lookup error: bazelout/k8opt/bin/tensorflow/cc/ops/resource_variable_ops_gen_cc: undefined symbol: _ZN10tensorflow12OpDefBuilder10SetShapeFnESt8functionIFNS_6StatusEPNS_15shape_inference16InferenceContextEEE Target //tensorflow:libtensorflow_cc.so failed to build Use verbose_failures to see the command lines of failed build steps. ```,2025-03-03T06:02:55Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88469,"Hi **** , Apologies for the delay, and thanks for raising your concern. It looks like a version compatibility issue might be causing the problem. I am attaching the official documentation for your reference, please review it to check the compatibility of different versions for better results. Additionally, I noticed that you are using an older version. I recommend updating to the latest version to avoid potential issues. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-03T05:50:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88468
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-03T05:40:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88467
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-03T05:31:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88466
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-03T05:30:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88465
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-03T05:30:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88464
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-03T05:22:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88463
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-03T05:20:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88462
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-03T05:14:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88461
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-03T05:14:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88460
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-03T05:10:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88459
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-03T04:04:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88458
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-03T03:19:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88457
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-03T03:04:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88456
copybara-service[bot],[XLA] Fix the CPU benchmark workflow by removing expensive models using interpreter,[XLA] Fix the CPU benchmark workflow by removing expensive models using interpreter,2025-03-03T01:46:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88455
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-03T01:08:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88454
copybara-service[bot],[xla] Literal: optimize broadcasts,"[xla] Literal: optimize broadcasts ``` name                                                                       old cpu/op   new cpu/op   delta BM_BroadcastScalarToMatrix/16/16     [s64[] to s64[16,16]               ]  3.77µs ± 1%  0.60µs ± 4%  84.05%  (p=0.000 n=37+37) BM_BroadcastScalarToMatrix/16/1024   [s64[] to s64[16,1024]             ]   153µs ± 1%     3µs ± 2%  97.75%  (p=0.000 n=38+37) BM_BroadcastScalarToMatrix/1024/1024 [s64[] to s64[1024,1024]           ]  9.70ms ± 1%  0.18ms ± 1%  98.17%  (p=0.000 n=39+39) BM_BroadcastVectorToMatrix/16/16     [s64[16] to s64[16,16]             ]  4.31µs ± 1%  3.87µs ± 2%  10.14%  (p=0.000 n=35+39) BM_BroadcastVectorToMatrix/16/1024   [s64[16] to s64[16,1024]           ]   177µs ± 2%   150µs ± 2%  15.21%  (p=0.000 n=37+40) BM_BroadcastVectorToMatrix/1024/1024 [s64[1024] to s64[1024,1024]       ]  11.2ms ± 2%   9.5ms ± 1%  15.23%  (p=0.000 n=36+40) BM_BroadcastMatrixToTensor/16/16     [s64[16,16] to s64[4,16,16]        ]  17.0µs ± 1%  15.9µs ± 2%   6.10%  (p=0.000 n=37+37) BM_BroadcastMatrixToTensor/16/1024   [s64[16,1024] to s64[4,16,1024]    ]   943µs ± 2%   925µs ± 2%   1.83%  (p=0.000 n=38+39) BM_BroadcastMatrixToTensor/1024/1024 [s64[1024,1024] to s64[4,1024,1024]]  60.3ms ± 1%  59.4ms ± 2%   1.54%  (p=0.000 n=36+40) ```",2025-03-02T19:50:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88453
copybara-service[bot],[xla] Literal: optimize scalar broadcasts,[xla] Literal: optimize scalar broadcasts FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23302 from yliu120:patch3 692eac6426cae82f2cec76eead8908e7b47b4101,2025-03-02T18:51:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88452
dicotom,Tensorflow with C++ Builder 12," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version libtensorflowcpuwindowsx86_64.zip  Custom code Yes  OS platform and distribution windows 10  Mobile device N/A  Python version N/A  Bazel version N/A  GCC/compiler version N/A  CUDA/cuDNN version _No response_  GPU model and memory N/A  Current behavior? this simple c file should compile: bcc64 test6.c D__NO_INLINE DWIN64 I ""N:\DOWNLOADS\FASTEST\26WORKS\TOTEST\include"" L ""N:\DOWNLOADS\FASTEST\26WORKS\TOTEST\lib"" l""tensorflow"" v ``` include  //pragma comment(lib, ""tensorflow.lib"")  //pragma link ""tensorflow.lib"" int main() {     // Your code using TensorFlow's C API functions     TF_Graph* graph = TF_NewGraph();     // More TensorFlow code...     return 0; } ``` but it does not: Embarcadero C++ 7.70 for Win64 Copyright (c) 20122024 Embarcadero Technologies, Inc. test6.c: Turbo Incremental Link64 6.99 Copyright (c) 19972024 Embarcadero Technologies, Inc. Error: Unresolved external 'TF_NewGraph' referenced from C:\USERS\USER\APPDATA\LOCAL\TEMP\TEST6AF147F.O if I add: pragma comment(lib, ""tensorflow.lib"")  I got invalid object file tensorflow.dll  Standalone code to reproduce the issue ```shell include  //pragma comment(lib, ""tensorflow.lib"") ; DOES NOT WORK //pragma link ""tensorflow.lib""          ; DOES NOT WORK int main() {     // Your code using TensorFlow's C API functions     TF_Graph* graph = TF_NewGraph();     // More TensorFlow code...     return 0; } ```  Relevant log output ```shell Embarcadero C++ 7.70 for Win64 Copyright (c) 20122024 Embarcadero Technologies, Inc. test6.c: Turbo Incremental Link64 6.99 Copyright (c) 19972024 Embarcadero Technologies, Inc. Error: Unresolved external 'TF_NewGraph' referenced from C:\USERS\USER\APPDATA\LOCAL\TEMP\TEST6AF147F.O ```",2025-03-02T16:05:15Z,stat:awaiting tensorflower type:bug subtype:windows comp:core,open,0,0,https://github.com/tensorflow/tensorflow/issues/88451
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T14:39:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88450
copybara-service[bot],The Google_Tensor plugin in LiTert was only supporting a subset of operations for the prototype. These limitations have been relaxed to support all operations. This will enable more testing and exploration of the LiTert ecosystem with a larger range of models.,The Google_Tensor plugin in LiTert was only supporting a subset of operations for the prototype. These limitations have been relaxed to support all operations. This will enable more testing and exploration of the LiTert ecosystem with a larger range of models.,2025-03-02T13:09:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88449
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T07:50:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88448
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T07:24:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88447
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T07:24:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88446
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T06:54:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88445
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T06:45:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88444
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T06:43:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88443
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T06:10:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88442
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T06:10:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88441
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T06:09:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88440
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T06:09:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88439
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T06:08:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88438
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T06:04:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88437
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T06:02:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88436
xtrizeShino,Failed to parse TfLiteSettingsJsonParser on TensorFlow Lite C++," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.1  Custom code No  OS platform and distribution Debian aarch64   Mobile device VIA VAB5000  Python version Only C++  Bazel version 6.5.0  GCC/compiler version gcc version 12.2.0 (Debian 12.2.014) / Debian clang version 14.0.6  CUDA/cuDNN version None  GPU model and memory MediaTek Genio 700 MDLA  Current behavior? I am trying to use Delegate on VIA VAB5000 (aarch64) by referring to the following site. However, an error occurs when loading the json file to be used for Delegate, and I cannot use Delegate. Do you have any good ideas? I apologize for bothering you during your busy schedule. Thank you in advance. https://mediatek.gitlab.io/genio/doc/tao/npu_acceleration.html Below is the json file I am trying to parse. ``` { 	""stable_delegate_loader_settings"": { 		""delegate_path"": ""/usr/lib/libneuron_stable_delegate.so"" 	}, 	""neuron_delegate_settings"": { 		""execution_priority"": NEURON_PRIORITY_HIGH, 		""optimization_hint"": NEURON_OPTIMIZATION_NONE, 		""execution_preference"": NEURON_FAST_SINGLE_ANSWER, 		""allow_fp16"": true, 		""use_ahwb"": true 	} } ```  Standalone code to reproduce the issue ```shell  move to /home/debian $ cd ~   get TensorFlow v2.16.1 $ wget https://github.com/tensorflow/tensorflow/archive/refs/tags/v2.16.1.zip $ unzip v2.16.1.zip $ mv tensorflow2.16.1 tensorflow   get my source codes $ git.com:xtrizeShino/peoplenet_onnx_to_tflite.git $ cd peoplenet_onnx_to_tflite $ cd cpp_infer_vab5000  remove old source  $ rm rf abseilcpp $ rm rf flatbuffers  get abseil $ wget https://github.com/abseil/abseilcpp/archi ve/refs/tags/20230802.3.zip $ unzip 20230802.3.zip $ mv abseilcpp20230802.3 abseilcpp  get flatbuffers and build  $ wget https://github.com/google/flatbuffers/archive/refs/tags/v23.5.26.zip $ unzip v23.5.26.zip $ mv flatbuffers23.5.26 flatbuffers  $ cd flatbuffers $ mkdir build $ cd build $ cmake .. $ make  $ cd ../../  build my source codes $ mkdir build $ cd build  $ cmake .. $ make  exec $ ./PeopleNetInfer ```  Relevant log output ```shell $ ./PeopleNetInfer  original width=596, height=336 model file name : /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/build/resource/resnet34_peoplenet_int8.tflite ERROR: Failed to parse the delegate settings file (/home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/build/resource/stable_delegate_settings.json). Error at /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/peoplenet_main.cpp:235 ```",2025-03-02T05:56:37Z,type:support comp:lite TF 2.16,closed,0,7,https://github.com/tensorflow/tensorflow/issues/88435,"The TensorFlow Lite ""*.so"" files were crosscompiled using the following steps. ``` target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libtensorflowlite.so) target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libdelegate_loader.so) target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libtflite_settings_json_parser.so) target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libandroid_info.so) ``` ``` $ bazel build config=elinux_aarch64 c opt //tensorflow/lite/c:libtensorflowlite_c.so $ bazel build config=elinux_aarch64 c opt //tensorflow/lite:libtensorflowlite.so $ bazel build config=elinux_aarch64 c opt //tensorflow/lite/experimental/acceleration/compatibility:android_info $ bazel build config=elinux_aarch64 c opt //tensorflow/lite/delegates/utils/experimental/stable_delegate:tflite_settings_json_parser $ bazel build config=elinux_aarch64 c opt //tensorflow/lite/delegates/utils/experimental/stable_delegate:delegate_loader ``` And I am building it with cmake using the following CMakeLists.txt: ``` cmake_minimum_required(VERSION 2.8) project(PeopleNetInfer)  Create Main project add_executable(PeopleNetInfer 	peoplenet_main.cpp )  For OpenCV find_package(OpenCV REQUIRED) if(OpenCV_FOUND) 	target_include_directories(PeopleNetInfer PUBLIC ${OpenCV_INCLUDE_DIRS}) 	target_link_libraries(PeopleNetInfer ${OpenCV_LIBS}) endif()  Avseil.io (absl) add_subdirectory(abseilcpp) set(protobuf_ABSL_USED_TARGETS absl::absl_check absl::absl_log absl::algorithm absl::base absl::bind_front absl::bits absl::btree absl::cleanup absl::cord absl::core_headers absl::debugging absl::die_if_null absl::dynamic_annotations absl::flags absl::flat_hash_map absl::flat_hash_set absl::function_ref absl::hash absl::layout absl::log_initialize absl::log_severity absl::memory absl::node_hash_map absl::node_hash_set absl::optional absl::span absl::status absl::statusor absl::strings absl::synchronization absl::time absl::type_traits absl::utility absl::variant ) target_link_libraries(PeopleNetInfer ${protobuf_ABSL_USED_TARGETS})  For Tensorflow Lite target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libtensorflowlite.so) target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libdelegate_loader.so) target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libtflite_settings_json_parser.so) target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/crosstf/libandroid_info.so) target_link_libraries(PeopleNetInfer /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/flatbuffers/build/libflatbuffers.a) target_include_directories(PeopleNetInfer PUBLIC /home/debian/tensorflow) target_include_directories(PeopleNetInfer PUBLIC /home/debian/tensorflow/tensorflow) target_include_directories(PeopleNetInfer PUBLIC /home/debian/tensorflow/tensorflow/lite) target_include_directories(PeopleNetInfer PUBLIC /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/flatbuffers/include) set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS}  std=c++17 lstdc++"")  Copy resouce file(COPY ${CMAKE_SOURCE_DIR}/resource/ DESTINATION ${PROJECT_BINARY_DIR}/resource/) add_definitions(DRESOURCE_DIR=""${PROJECT_BINARY_DIR}/resource/"") ```","Hi,   I apologize for the delay in my response, I see after analyzing the error you're encountering with the `TfLiteSettingsJsonParser` maybe the values `NEURON_PRIORITY_HIGH, NEURON_OPTIMIZATION_NONE` and `NEURON_FAST_SINGLE_ANSWER` are not valid JSON as they're not enclosed in quotes. In proper JSON, string values must be enclosed in double quotes. If these are meant to be string constants: ``` { 	""stable_delegate_loader_settings"": { 		""delegate_path"": ""/usr/lib/libneuron_stable_delegate.so"" 	}, 	""neuron_delegate_settings"": { 		""execution_priority"": ""NEURON_PRIORITY_HIGH"", 		""optimization_hint"": ""NEURON_OPTIMIZATION_NONE"", 		""execution_preference"": ""NEURON_FAST_SINGLE_ANSWER"", 		""allow_fp16"": true, 		""use_ahwb"": true 	} } ``` JSON parsing errors can also occur due to file encoding problems. The parser might be failing due to hidden characters or incorrect encoding so create a new file with a text editor (like Notepad++ if available) save it with `UTF8` encoding (not UTF8BOM) and use this new file and see is it resolving your error or not ? Thank you for your cooperation and patience.","Thank you for your reply!! However, I tried the method you taught me, but unfortunately the result did not change. The output is as follows.  When I open the file in Notepad++, it shows as UTF8. ``` $ cat /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/build/resource/stable_delegate_settings.json  libneuron_stable_delegate.so setting {         ""stable_delegate_loader_settings"": {                 ""delegate_path"": ""/usr/lib/libneuron_stable_delegate.so""         },         ""neuron_delegate_settings"": {                 ""execution_priority"": ""NEURON_PRIORITY_HIGH"",                 ""optimization_hint"": ""NEURON_OPTIMIZATION_NONE"",                 ""execution_preference"": ""NEURON_FAST_SINGLE_ANSWER"",                 ""allow_fp16"": true,                 ""use_ahwb"": true         } } $ ./PeopleNetInfer original width=596, height=336 model file name : /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/build/resource/resnet34_peoplenet_int8.tflite ERROR: Failed to parse the delegate settings file (/home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/build/resource/stable_delegate_settings.json). Error at /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/peoplenet_main.cpp:235 ``` Since the character code contains only alphabets, the nkf command displayed it as ""ASCII"". ``` $ sudo apt install nkf $ nkf g /home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/build/resource/stable_delegate_settings.json ASCII ``` https://github.com/xtrizeShino/peoplenet_onnx_to_tflite/blob/main/cpp_infer_vab5000/peoplenet_main.cppL232 ``` define TFLITE_MINIMAL_CHECK(x) \     if (!(x)) {  \         fprintf(stderr, ""Error at %s:%d\n"", __FILE__, __LINE__); \         exit(1);  \     } ... constexpr char kSettingsPath[] = ""/home/debian/peoplenet_onnx_to_tflite/cpp_infer_vab5000/build/resource/stable_delegate_settings.json"";  ... /* Load settings */ TfLiteSettingsJsonParser parser; const tflite::TFLiteSettings* settings = parser.Parse(kSettingsPath); TFLITE_MINIMAL_CHECK(settings != nullptr); ```","Hi, Did you find a fix for this by anychance? ",Not yet.,"The embedded environment I'm using may be special. This issue occurs on the Debian OS of the VAB5000, but the JSON file seems to be read normally from TensorFlow Lite on the Yocto OS. If you can't reproduce this issue on your end, I'll ask the vendor.",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T05:56:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88434
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T05:53:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88433
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T05:53:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88432
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T05:27:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88431
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T04:57:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88430
copybara-service[bot],[xla] HloEvaluator: use ParallelFor to parallize loops,[xla] HloEvaluator: use ParallelFor to parallize loops ``` name                           old cpu/op   new cpu/op   delta BM_UnaryOp/64/process_time      243µs ± 6%   332µs ± 2%  +36.79%  (p=0.000 n=36+38) BM_UnaryOp/128/process_time     502µs ± 5%   522µs ± 2%   +3.97%  (p=0.000 n=39+38) BM_UnaryOp/512/process_time    4.89ms ± 3%  4.13ms ± 2%  15.48%  (p=0.000 n=39+40) BM_UnaryOp/1024/process_time   18.4ms ± 3%  15.4ms ± 2%  16.46%  (p=0.000 n=38+39) BM_BinaryOp/64/process_time     238µs ± 7%   318µs ± 3%  +33.79%  (p=0.000 n=40+40) BM_BinaryOp/128/process_time    454µs ± 9%   485µs ± 1%   +6.98%  (p=0.000 n=40+38) BM_BinaryOp/512/process_time   4.73ms ± 6%  4.03ms ± 2%  14.74%  (p=0.000 n=40+40) BM_BinaryOp/1024/process_time  18.0ms ± 5%  15.0ms ± 2%  16.65%  (p=0.000 n=40+36) name                           old time/op          new time/op          delta BM_UnaryOp/64/process_time     56.0µs ± 8%          43.7µs ± 3%  22.06%  (p=0.000 n=36+39) BM_UnaryOp/128/process_time    86.0µs ± 8%          59.3µs ± 3%  31.01%  (p=0.000 n=37+40) BM_UnaryOp/512/process_time     501µs ± 4%           351µs ± 2%  30.05%  (p=0.000 n=39+40) BM_UnaryOp/1024/process_time   1.80ms ± 6%          1.30ms ± 2%  27.59%  (p=0.000 n=38+39) BM_BinaryOp/64/process_time    54.6µs ± 8%          42.1µs ± 2%  22.96%  (p=0.000 n=40+39) BM_BinaryOp/128/process_time   76.0µs ±11%          55.8µs ± 2%  26.58%  (p=0.000 n=40+38) BM_BinaryOp/512/process_time    480µs ± 7%           347µs ± 2%  27.64%  (p=0.000 n=40+39) BM_BinaryOp/1024/process_time  1.77ms ± 9%          1.29ms ± 1%  27.01%  (p=0.000 n=40+36) ```,2025-03-02T04:03:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88429
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-02T03:49:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88428
copybara-service[bot],[xla] HloEvaluator: optimize evaluating scatter with trivial update computation,[xla] HloEvaluator: optimize evaluating scatter with trivial update computation,2025-03-02T03:14:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88427
cybersupersoap,"`tf.compat.v1.linalg.set_diag` aborts with ""Check failed: d < dims() (2 vs. 2)"""," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.20.0dev20250225  Custom code Yes  OS platform and distribution Ubuntu 20.04 LTS   Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an aborted issue in TensorFlow when I used API tf.compat.v1.linalg.set_diag . I have confirmed that below code would crash on tfnightly 2.20.0dev20250225 (nightlybuild).  Standalone code to reproduce the issue ```shell import tensorflow as tf input_tensor = tf.random.uniform([5, 4, 4, 4], dtype=tf.float32) input = tf.identity(input_tensor) diagonal_0_0 = 2.0 diagonal_0_1 = 3.0 diagonal_0_2 = 4.0 diagonal_0_3 = 5.0 diagonal_0 = [diagonal_0_0, diagonal_0_1, diagonal_0_2, diagonal_0_3] diagonal_1_0 = 1.0 diagonal_1_1 = 2.0 diagonal_1_2 = 3.0 diagonal_1_3 = 4.0 diagonal_1 = [diagonal_1_0, diagonal_1_1, diagonal_1_2, diagonal_1_3] diagonal = [diagonal_0, diagonal_1] name = 'set_diag' k_0 = 0 k_1 = 1 k = [k_0, k_1] align = 'LEFT_RIGHT' out = tf.compat.v1.linalg.set_diag(input=input, diagonal=diagonal, name=name, k=k, align=align) ```  Relevant log output ```shell 20250302 02:25:05.721633: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (2 vs. 2) Aborted (core dumped) ```",2025-03-02T02:21:06Z,type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/88426,I was able to reproduce this issue on Colab using TensorFlow 2.18 and the nightly version. Please find the gist attached for your reference. Thank you!
copybara-service[bot],[xla] HloEvaluator: keep per evaluation state in EvaluationState and use move assignment to efficiently return evaluated results to callers,"[xla] HloEvaluator: keep per evaluation state in EvaluationState and use move assignment to efficiently return evaluated results to callers 1. Protected data members are banned by Google C++ style, make all data members private 2. Extract per evaluation state in a separate class with a clear ownership and lifetime documentation, and use std::move to extract evaluated results to avoid redundant Literal::Clone on every call to Evaluate",2025-03-01T22:22:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88425
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T15:15:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88423
cybersupersoap,`tf.config.threading.set_intra_op_parallelism_threads` can cause a crash," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.20.0dev20250225  Custom code Yes  OS platform and distribution Ubuntu 20.04 LTS  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `aborted issue` in TensorFlow when I used API `tf.config.threading.set_intra_op_parallelism_threads` . I have confirmed that below code would crash on `tfnightly 2.20.0dev20250225` (nightlybuild).  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np num_streams = 1 tf.config.threading.set_intra_op_parallelism_threads(num_streams) x = np.array([[1, 2], [3, 4]], dtype=np.float32) y = tf.cast(x, dtype=tf.int32) ```  Relevant log output ```shell 20250226 14:41:57.365578: F external/local_xla/xla/tsl/platform/threadpool.cc:126] Check failed: num_threads >= 1 (1 vs. 1) Aborted (core dumped) ```",2025-03-01T14:22:02Z,type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/88422,I was able to reproduce this issue on Colab using TensorFlow 2.18 and the nightly version. Please find the gist attached for your reference. Thank you!
copybara-service[bot],Internal change to fix tests,Internal change to fix tests,2025-03-01T14:06:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88421
wokron,Fix wrong time unit handling for node scheduled time in executor,"The `nodestats::SetScheduled` function used the wrong time unit. The scheduled time is obtained through the `NowInNsec` function, but this value was treated as microseconds when passed to `nodestats::SetScheduled`.",2025-03-01T11:24:47Z,ready to pull size:XS comp:core,closed,0,1,https://github.com/tensorflow/tensorflow/issues/88420,> Would it be possible to add a test for this? I have added a test in tensorflow/core/common_runtime/executor_test.cc. This test would fail without the bug fix.
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T10:35:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88419
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T09:32:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88418
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T09:20:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88417
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T09:17:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88416
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T09:16:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88415
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T08:51:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88414
dependabot[bot],Bump ubuntu from `80dd3c3` to `7229784` in /tensorflow/tools/gcs_test,"Bumps ubuntu from `80dd3c3` to `7229784`. ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2025-03-01T08:46:43Z,ready to pull size:XS dependencies docker,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88413
dependabot[bot],Bump ubuntu from `0e5e4a5` to `ed1544e` in /tensorflow/tools/tf_sig_build_dockerfiles,"Bumps ubuntu from `0e5e4a5` to `ed1544e`. ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2025-03-01T08:31:11Z,ready to pull size:XS dependencies docker,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88412
dependabot[bot],Bump the github-actions group with 6 updates,"Bumps the githubactions group with 6 updates:  Updates `peterevans/createpullrequest` from 7.0.6 to 7.0.7  Release notes Sourced from peterevans/createpullrequest's releases.  Create Pull Request v7.0.7 ⚙️ Fixes an issue with commit signing where modifications to the same file in multiple commits squash into the first commit. What's Changed  build(deps): bump @​octokit/core from 6.1.2 to 6.1.3 by @​dependabot in peterevans/createpullrequest CC(How to run custom encoderdecoder in Tensorflow using available APIs?) build(depsdev): bump @​types/node from 18.19.68 to 18.19.70 by @​dependabot in peterevans/createpullrequest CC(Moved eightbit graph trimming to before output_nodes definition) Update distribution by @​actionsbot in peterevans/createpullrequest CC(RC 0.10 3X Slower than 0.9 and Error Compiling From Source Under Certain Conditions) build(depsdev): bump typescript from 5.7.2 to 5.7.3 by @​dependabot in peterevans/createpullrequest CC(Graph optimization and other features) build(deps): bump octokit dependencies by @​peterevans in peterevans/createpullrequest CC(gcc: error: unrecognized command line option 'fnocanonicalsystemheaders') docs: add workflow tip for showing message via workflow command by @​ybiquitous in peterevans/createpullrequest CC(contrib/makefile:  error: conflicting return type) build(depsdev): bump eslintpluginprettier from 5.2.1 to 5.2.3 by @​dependabot in peterevans/createpullrequest CC(Unable to import frozen graph with batchnorm) build(deps): bump nodefetchnative from 1.6.4 to 1.6.6 by @​dependabot in peterevans/createpullrequest CC(Tensorflow inability to kill processes using more than 1 GPU) build(depsdev): bump undici from 6.21.0 to 6.21.1 by @​dependabot in peterevans/createpullrequest CC(TF works in python3.4 for some users but not for others under RedHat7 ) build(depsdev): bump @​types/node from 18.19.70 to 18.19.71 by @​dependabot in peterevans/createpullrequest CC(Problems in ""Implement the gradient in Python"" docs) Update distribution by @​actionsbot in peterevans/createpullrequest CC(Mismatch in gradient of 'abs' for complex values) build(depsdev): bump @​types/node from 18.19.71 to 18.19.74 by @​dependabot in peterevans/createpullrequest CC(0.10.0rc0: Contrib distributions crash when sampling ""n"" is scalar) build(depsdev): bump @​types/node from 18.19.74 to 18.19.75 by @​dependabot in peterevans/createpullrequest CC(Error when using TensorArray and variables in nested loops) build(deps): bump @​octokit/pluginrestendpointmethods from 13.3.0 to 13.3.1 by @​dependabot in peterevans/createpullrequest CC(Update CUDA/cuDNN in Dockerfiles) build(depsdev): bump prettier from 3.4.2 to 3.5.0 by @​dependabot in peterevans/createpullrequest CC(Add layer_norm op to contrib.layers.) Update distribution by @​actionsbot in peterevans/createpullrequest CC(error: can't copy 'tensorflow/models/embedding/gen_word2vec.py': doesn't exist) build(deps): bump @​octokit/requesterror from 6.1.6 to 6.1.7 by @​dependabot in peterevans/createpullrequest CC(Unable to generate signed APK for project based on Android demo) build(deps): bump @​octokit/pluginpaginaterest from 11.4.0 to 11.4.1 by @​dependabot in peterevans/createpullrequest CC(Error when trying to run tensorboard logdir=some_path) build(deps): bump @​octokit/endpoint from 10.1.2 to 10.1.3 by @​dependabot in peterevans/createpullrequest CC(Cuda8) Update distribution by @​actionsbot in peterevans/createpullrequest CC(Benchmarking example for iOS profiling) build(depsdev): bump prettier from 3.5.0 to 3.5.1 by @​dependabot in peterevans/createpullrequest CC(random_uniform for int32 broken on GPU) build(depsdev): bump eslintimportresolvertypescript from 3.7.0 to 3.8.1 by @​dependabot in peterevans/createpullrequest CC(function.Defun can't be applied to tf.Variable) build(deps): bump @​octokit/pluginpaginaterest from 11.4.1 to 11.4.2 by @​dependabot in peterevans/createpullrequest CC(Explanation of blank label in ctc_loss) build(depsdev): bump @​types/node from 18.19.75 to 18.19.76 by @​dependabot in peterevans/createpullrequest CC(gather_nd not working with API examples) build(deps): bump @​octokit/core from 6.1.3 to 6.1.4 by @​dependabot in peterevans/createpullrequest CC(mnist_with_summaries.py error 'module' object has no attribute 'DT_HALF') Update distribution by @​actionsbot in peterevans/createpullrequest CC(Fix typo in TensorFlow Linear Model Tutorial) Use showFileAtRefBase64 to read percommit file contents by @​grahamc in peterevans/createpullrequest CC(extract element from list when py_func's output type is a single tensorflow type)  New Contributors  @​ybiquitous made their first contribution in peterevans/createpullrequest CC(contrib/makefile:  error: conflicting return type) @​grahamc made their first contribution in peterevans/createpullrequest CC(extract element from list when py_func's output type is a single tensorflow type)  Full Changelog: https://github.com/peterevans/createpullrequest/compare/v7.0.6...v7.0.7    Commits  dd2324f fix: use showFileAtRefBase64 to read percommit file contents ( CC(extract element from list when py_func's output type is a single tensorflow type)) 367180c ci: remove testv5 cmd 25575a1 build: update distribution ( CC(Fix typo in TensorFlow Linear Model Tutorial)) a56e7a5 build(deps): bump @​octokit/core from 6.1.3 to 6.1.4 ( CC(mnist_with_summaries.py error 'module' object has no attribute 'DT_HALF')) eac17dc build(depsdev): bump @​types/node from 18.19.75 to 18.19.76 ( CC(gather_nd not working with API examples)) a2e685f build(deps): bump @​octokit/pluginpaginaterest from 11.4.1 to 11.4.2 ( CC(Explanation of blank label in ctc_loss)) 6cfd146 build(depsdev): bump eslintimportresolvertypescript ( CC(function.Defun can't be applied to tf.Variable)) b38e8d3 build(depsdev): bump prettier from 3.5.0 to 3.5.1 ( CC(random_uniform for int32 broken on GPU)) 8a41570 build: update distribution ( CC(Benchmarking example for iOS profiling)) 2e9b4cc build(deps): bump @​octokit/endpoint from 10.1.2 to 10.1.3 ( CC(Cuda8)) Additional commits viewable in compare view    Updates `ossf/scorecardaction` from 2.4.0 to 2.4.1  Release notes Sourced from ossf/scorecardaction's releases.  v2.4.1 What's Changed  This update bumps the Scorecard version to the v5.1.1 release. For a complete list of changes, please refer to the v5.1.0 and v5.1.1 release notes. Publishing results now uses half the API quota as before. The exact savings depends on the repository in question.  use Scorecard library entrypoint instead of Cobra hooking by @​spencerschrock in ossf/scorecardaction CC(Need force_gpu_if_available for tests)   Some errors were made into annotations to make them more visible  Make default branch error more prominent by @​jsoref in ossf/scorecardaction CC(partial_run segfault)   There is now an optional file_mode input which controls how repository files are fetched from GitHub. The default is archive, but git produces the most accurate results for repositories with .gitattributes files at the cost of analysis speed.  add input for specifying filemode by @​spencerschrock in ossf/scorecardaction CC(Fix python3 b)   The underlying container for the action is now hosted on GitHub Container Registry. There should be no functional changes.  :seedling: publish docker images to GitHub Container Registry by @​spencerschrock in ossf/scorecardaction CC(Multidimensional RNN)    Docs  Installation docs update by @​JeremiahAHoward in ossf/scorecardaction CC(ci_build  debian jessie)  New Contributors  @​JeremiahAHoward made their first contribution in ossf/scorecardaction CC(ci_build  debian jessie) @​jsoref made their first contribution in ossf/scorecardaction CC(partial_run segfault) Full Changelog: https://github.com/ossf/scorecardaction/compare/v2.4.0...v2.4.1     Commits  f49aabe bump docker to ghcr v2.4.1 ( CC(Hardcoded bash path)) 30a595b :seedling: Bump github.com/sigstore/cosign/v2 from 2.4.2 to 2.4.3 ( CC(rnn.bidirectional_rnn  cause a problem)) 69ae593 omit vcs info from build ( CC(Bugfix to test/run_and_gather_logs.)) 6a62a1c add input for specifying filemode ( CC(Fix python3 b)) 2722664 :seedling: Bump the githubactions group with 2 updates ( CC(Delete useless directory)) ae0ef31 :seedling: Bump github.com/spf13/cobra from 1.8.1 to 1.9.1 ( CC(some learning decays from Stanford CS231n Karpathy lecture 6)) 3676bbc :seedling: Bump golang from 1.23.6 to 1.24.0 in the dockerimages group ( CC(seems issues with softmax_cross_entropy_with_logits)) ae7548a Limit codeQL push trigger to main branch ( CC(quick python3 fix)) 9165624 upgrade scorecard to v5.1.0 ( CC(Fix python3 breakage (oldstyle exception block))) 620fd28 :seedling: Bump the githubactions group with 2 updates ( CC(typos fix and ign temp files in gitignore)) Additional commits viewable in compare view    Updates `actions/uploadartifact` from 4.6.0 to 4.6.1  Release notes Sourced from actions/uploadartifact's releases.  v4.6.1 What's Changed  Update to use artifact 2.2.2 package by @​yacaovsnc in actions/uploadartifact CC(PoolAlloc: Remove div by zero, demote WARN>INFO)  Full Changelog: https://github.com/actions/uploadartifact/compare/v4...v4.6.1    Commits  4cec3d8 Merge pull request  CC(PoolAlloc: Remove div by zero, demote WARN>INFO) from actions/yacaovsnc/artifact_2.2.2 e9fad96 license cache update for artifact b26fd06 Update to use artifact 2.2.2 package See full diff in compare view    Updates `github/codeqlaction` from 3.28.8 to 3.28.10  Release notes Sourced from github/codeqlaction's releases.  v3.28.10 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.10  21 Feb 2025  Update default CodeQL bundle version to 2.20.5.  CC(Please consider adding flatten) Address an issue where the CodeQL Bundle would occasionally fail to decompress on macOS.  CC(Checkpoint Restore blocked by changed default bias variable name)  See the full CHANGELOG.md for more information. v3.28.9 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.9  07 Feb 2025  Update default CodeQL bundle version to 2.20.4.  CC(C++ compilation of rule '//:sip_hash' failed (Tensorflow serving on Android))  See the full CHANGELOG.md for more information.    Changelog Sourced from github/codeqlaction's changelog.  CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. [UNRELEASED] No user facing changes. 3.28.10  21 Feb 2025  Update default CodeQL bundle version to 2.20.5.  CC(Please consider adding flatten) Address an issue where the CodeQL Bundle would occasionally fail to decompress on macOS.  CC(Checkpoint Restore blocked by changed default bias variable name)  3.28.9  07 Feb 2025  Update default CodeQL bundle version to 2.20.4.  CC(C++ compilation of rule '//:sip_hash' failed (Tensorflow serving on Android))  3.28.8  29 Jan 2025  Enable support for Kotlin 2.1.10 when running with CodeQL CLI v2.20.3.  CC(Fix for build issue 2742;)  3.28.7  29 Jan 2025 No user facing changes. 3.28.6  27 Jan 2025  Reenable debug artifact upload for CLI versions 2.20.3 or greater.  CC(Modifying MNIST example to distributed version: could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR)  3.28.5  24 Jan 2025  Update default CodeQL bundle version to 2.20.3.  CC(Branch 124290852)  3.28.4  23 Jan 2025 No user facing changes. 3.28.3  22 Jan 2025  Update default CodeQL bundle version to 2.20.2.  CC(Update roadmap.md) Fix an issue downloading the CodeQL Bundle from a GitHub Enterprise Server instance which occurred when the CodeQL Bundle had been synced to the instance using the CodeQL Action sync tool and the Actions runner did not have Zstandard installed.  CC(Branch 124251558) Uploading debug artifacts for CodeQL analysis is temporarily disabled.  CC(Tensorflow with Pyinstaller)  3.28.2  21 Jan 2025 No user facing changes. 3.28.1  10 Jan 2025  CodeQL Action v2 is now deprecated, and is no longer updated or supported. For better performance, improved security, and new features, upgrade to v3. For more information, see this changelog post.  CC(Import error)    ... (truncated)   Commits  b56ba49 Merge pull request  CC(Isn't current tensorflowgit r0.9? ) from github/updatev3.28.109856c48b1 60c9c77 Update changelog for v3.28.10 9856c48 Merge pull request  CC(Segmentation fault on tensorflow 0.9.0) from github/redsun82/rust 9572e09 Rust: fix log string 1a52936 Rust: special case default setup cf7e909 Merge pull request  CC(Please consider adding flatten) from github/updatebundle/codeqlbundlev2.20.5 b7006aa Merge branch 'main' into updatebundle/codeqlbundlev2.20.5 cfedae7 Rust: throw configuration errors if requested and not correctly enabled 3971ed2 Merge branch 'main' into redsun82/rust d38c6e6 Merge pull request  CC(Bazel fail to resolve submodule tensorflow) from github/angelapwen/bumpoctokit Additional commits viewable in compare view    Updates `docker/setupbuildxaction` from 3.8.0 to 3.10.0  Release notes Sourced from docker/setupbuildxaction's releases.  v3.10.0  Bump @​docker/actionstoolkit from 0.54.0 to 0.56.0 in docker/setupbuildxaction CC(Can't find pngwutil.c building tensorflow)  Full Changelog: https://github.com/docker/setupbuildxaction/compare/v3.9.0...v3.10.0 v3.9.0  Bump @​docker/actionstoolkit from 0.48.0 to 0.54.0 in docker/setupbuildxaction CC(More details of Inception model?) docker/setupbuildxaction CC(gcc4.8.1 is unhappy with usage of auto* in conv_grad_ops.cc)  Full Changelog: https://github.com/docker/setupbuildxaction/compare/v3.8.0...v3.9.0    Commits  b5ca514 Merge pull request  CC(Can't find pngwutil.c building tensorflow) from docker/dependabot/npm_and_yarn/docker/actionsto... 1418a4e chore: update generated content 93acf83 build(deps): bump @​docker/actionstoolkit from 0.54.0 to 0.56.0 f7ce87c Merge pull request  CC(gcc4.8.1 is unhappy with usage of auto* in conv_grad_ops.cc) from docker/dependabot/npm_and_yarn/docker/actionsto... aa1e2a0 chore: update generated content 673e008 build(deps): bump @​docker/actionstoolkit from 0.53.0 to 0.54.0 ba31df4 Merge pull request  CC(More details of Inception model?) from docker/dependabot/npm_and_yarn/docker/actionsto... 5475af1 chore: update generated content acacad9 build(deps): bump @​docker/actionstoolkit from 0.48.0 to 0.53.0 6a25f98 Merge pull request  CC(Cifar10 example bug (batch 5 not loading)) from crazymax/bakev6 Additional commits viewable in compare view    Updates `docker/buildpushaction` from 6.13.0 to 6.15.0  Release notes Sourced from docker/buildpushaction's releases.  v6.15.0  Bump @​docker/actionstoolkit from 0.55.0 to 0.56.0 in docker/buildpushaction CC(Problematic links in official website)  Full Changelog: https://github.com/docker/buildpushaction/compare/v6.14.0...v6.15.0 v6.14.0  Bump @​docker/actionstoolkit from 0.53.0 to 0.55.0 in docker/buildpushaction CC(GLIBC error)  Full Changelog: https://github.com/docker/buildpushaction/compare/v6.13.0...v6.14.0    Commits  471d1dc Merge pull request  CC(Problematic links in official website) from docker/dependabot/npm_and_yarn/docker/actionst... b89ff0a chore: update generated content 1e3ae3a chore(deps): Bump @​docker/actionstoolkit from 0.55.0 to 0.56.0 b16f42f Merge pull request  CC(tf.get_variable() cannot recognize existing variables) from crazymax/buildxedge dc0fea5 ci: update buildx to edge and buildkit to latest 0adf995 Merge pull request  CC(GLIBC error) from docker/dependabot/npm_and_yarn/docker/actionst... d88cd28 chore: update generated content 3d09a6b chore(deps): Bump @​docker/actionstoolkit from 0.53.0 to 0.55.0 See full diff in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore  major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)  ` ignore  minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)  ` ignore ` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)  ` unignore ` will remove all of the ignore conditions of the specified dependency  ` unignore  ` will remove the ignore condition of the specified dependency and ignore conditions ",2025-03-01T08:27:20Z,ready to pull size:S dependencies github_actions,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88411
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T07:04:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88410
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T07:00:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88409
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T06:55:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88408
copybara-service[bot],Add Memory and Device for TfrtGpuClient,Add Memory and Device for TfrtGpuClient,2025-03-01T06:46:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88407
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T06:34:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88406
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T06:24:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88405
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T06:12:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88404
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T05:57:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88403
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T05:50:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88402
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T05:33:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88401
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T04:53:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88400
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T04:29:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88399
copybara-service[bot],Automated Code Change,Automated Code Change,2025-03-01T04:16:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88398
copybara-service[bot],Add Deserialize function to HloRunnerInterface.,"Add Deserialize function to HloRunnerInterface. This function consumes a runnerspecific protobuf message, which it uses to construct a runnerinternal `OpaqueExecutable`. Where available, this function delegates to preexisting deserialization functionality, such as what exists in the PjRt API.",2025-03-01T02:35:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88397
copybara-service[bot],litert: LiteRT changes for GPU support,litert: LiteRT changes for GPU support  Update CheckCpuTensors() to check nodes with execution_plan() instead of   checking all nodes_and_registration(). This aligns with SubGraph::Invoke()   and works well after applying a Delegate.  Added ExternalLiteRtBufferContext::RegisterLiteRtBufferRequirement()   to register with C type LiteRtTensorBufferRequirements.  Update visibility,2025-03-01T01:30:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88396
copybara-service[bot],litert: Load OpenCL library with Alloc() method,litert: Load OpenCL library with Alloc() method OpenclBuffer::IsSupported() should be called to load OpenCL library in Alloc() method.,2025-03-01T01:19:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88395
copybara-service[bot],Use consistent naming in `build_tools/ci/build.py`,Use consistent naming in `build_tools/ci/build.py` In preparation to stop using the `KOKORO_JOB_NAME` environment variable,2025-03-01T01:03:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88394
copybara-service[bot],[xla] Literal: improve Literal::Clone performance for tiny literals,[xla] Literal: improve Literal::Clone performance for tiny literals Cloning a scalar improved from 140ns to 60ns.,2025-03-01T00:39:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88393
copybara-service[bot],[XLA:MSA] Fixes an issue that leads to sync mem op replacements not respecting the auxiliary control dependencies while converting sync mem ops to async,"[XLA:MSA] Fixes an issue that leads to sync mem op replacements not respecting the auxiliary control dependencies while converting sync mem ops to async MSA assumes all control dependencies (predesseccors/successors) are met in earlier passes by the scheduler. Therefore, it traditionally only respects the data flow dependencies (i.e., instruction.uses()). This became problematic when we replaced sync copies with async ones: the async copies where not aware of indirect control dependencies when we were determining their latest allowed copydone time.",2025-03-01T00:22:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88392
copybara-service[bot],Reverts 19aa387f08b12381c38b5076168ff9ed07c987f9,Reverts 19aa387f08b12381c38b5076168ff9ed07c987f9,2025-02-28T23:55:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88391
copybara-service[bot],Part2: Remove moved code createLegalizeTFXlaCallModuleToStablehloPass from tensorflow/compiler/mlir/lite/stablehlo,Part2: Remove moved code createLegalizeTFXlaCallModuleToStablehloPass from tensorflow/compiler/mlir/lite/stablehlo,2025-02-28T23:45:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88390
copybara-service[bot],Extend rpc_helper lifetime to avoid errors when making executable destruction call as the rpc_helper destruction could lead to early RPC session destruction before individual executable destruction completes.,Extend rpc_helper lifetime to avoid errors when making executable destruction call as the rpc_helper destruction could lead to early RPC session destruction before individual executable destruction completes.,2025-02-28T23:41:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88389
rtg0795,Update RELEASE.md with minor fix to indentation,,2025-02-28T23:23:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88388
copybara-service[bot],Resolve share c library loading for mobile device tests.,Resolve share c library loading for mobile device tests.,2025-02-28T23:12:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88387
copybara-service[bot],test pjrt compilation2,test pjrt compilation2,2025-02-28T23:09:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88386
tensorflow-jenkins,Update version numbers for TensorFlow 2.19.0,"Before merging this PR, please double check that it has correctly updated `core/public/version.h`, `tools/pip_package/setup.py`, and `tensorflow/tensorflow.bzl`. Also review the execution notes below: ``` Major: 2 > 2 Minor: 19 > 19 Patch: 0 > 0 No lingering old version strings ""2.19.0rc0"" found in source directory  ""tensorflow/"". Good. No lingering old version strings ""2.19.0rc0"" found in source directory  ""tensorflow/"". Good. ```",2025-02-28T22:56:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88385
copybara-service[bot],Make c header files in compiler/mlir/lite/core/c accessible publicly.,Make c header files in compiler/mlir/lite/core/c accessible publicly.,2025-02-28T22:45:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88384
copybara-service[bot],Define CAPI and Python API for chlo.ragged_dot.,Define CAPI and Python API for chlo.ragged_dot.,2025-02-28T22:44:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88383
copybara-service[bot],Do not infer return type of chlo.ragged_dot.,Do not infer return type of chlo.ragged_dot.,2025-02-28T22:35:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88382
copybara-service[bot],Implement Jax CPU/GPU callbacks with XLA's FFI.,"Implement Jax CPU/GPU callbacks with XLA's FFI.  Change 4 of 4 addressing CC(JVM, .NET Language Support) in https://github.com/jaxml/jax/issues/25842.",2025-02-28T22:04:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88381
copybara-service[bot],Upgrade to latest version of shardy.,Upgrade to latest version of shardy.,2025-02-28T22:02:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88380
copybara-service[bot],Add constraint for the DQD removal pattern in the quantize pass.,"Add constraint for the DQD removal pattern in the quantize pass. Add a constraint that requires the input and the result types to be the same. This prevents the quantize pass from emitting invalid IR when the requantization is actually necessary for the surrounding ops, e.g. ""tfl.reshape"" often require the input to the requantized to the same scale as its output's.",2025-02-28T21:44:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88379
copybara-service[bot],Part1: Migrate createLegalizeTFXlaCallModuleToStablehloPass to tensorflow/compiler/mlir/stablehlo,Part1: Migrate createLegalizeTFXlaCallModuleToStablehloPass to tensorflow/compiler/mlir/stablehlo,2025-02-28T21:37:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88378
copybara-service[bot],Add iter tools for ops/subgraphs,Add iter tools for ops/subgraphs,2025-02-28T21:37:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88377
copybara-service[bot],Bump XNNPACK version to remove executable stack from xla_extension.so.,Bump XNNPACK version to remove executable stack from xla_extension.so. Fix https://github.com/jaxml/jax/issues/26781.,2025-02-28T21:23:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88376
copybara-service[bot],Reverts f5f347c47e5b2fa3576b698ba53adefdf7001790,Reverts f5f347c47e5b2fa3576b698ba53adefdf7001790,2025-02-28T21:20:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88375
copybara-service[bot],Clean Up Forwarding Headers in tensorflow/core/profiler/protobuf Folder,Clean Up Forwarding Headers in tensorflow/core/profiler/protobuf Folder,2025-02-28T21:18:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88374
copybara-service[bot],Fix Jax XLA FFI callback handlers for OSS GPU.,"Fix Jax XLA FFI callback handlers for OSS GPU. OSS Jax builds for GPU backends split `jaxlib` into three wheels and since we cannot expect a stable C++ ABI among the shared libraries, we refactor to ensure: 1. C++ objects are not created/consumed by different shared libraries. 2. Static objects are declared and defined appropriately. This PR: 1. Migrates Jax XLA FFI callback handlers from XLA's Internal FFI API to the External FFI API. Note that we update both CPU and GPU handlers because we cannot mix Internal and External APIs. 2. Updates how FFI GPU handlers are registered, now analogous to how the original GPU custom call was registered. 3. Adds an `xla::ffi::ExecutionContext` member to `ifrt::PjRtLoadedExectuable` holding opaque pointers to callbacks. 4. Updates Jax `callback.py` to call the new FFI callback handlers.",2025-02-28T21:13:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88373
copybara-service[bot],Test clangtidy blocking export to github,Test clangtidy blocking export to github,2025-02-28T21:11:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88372
copybara-service[bot],Clean up dependencies in core/profiler/convert,Clean up dependencies in core/profiler/convert,2025-02-28T21:09:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88371
copybara-service[bot],Clean up dependencies in core/profiler/convert,Clean up dependencies in core/profiler/convert,2025-02-28T21:05:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88370
copybara-service[bot],Clean up dependencies in core/profiler/convert,Clean up dependencies in core/profiler/convert,2025-02-28T21:04:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88369
copybara-service[bot],Clean up dependencies in core/profiler/convert,Clean up dependencies in core/profiler/convert,2025-02-28T21:02:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88368
copybara-service[bot],Clean up dependencies in core/profiler/convert,Clean up dependencies in core/profiler/convert FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88412 from tensorflow:dependabot/docker/tensorflow/tools/tf_sig_build_dockerfiles/ubuntued1544e 4b6ba37c2667bdfded35b1774ed8ee438bc5fb25,2025-02-28T21:01:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88367
copybara-service[bot],Disable shuffling for gpu_compiler_test.,Disable shuffling for gpu_compiler_test. This test depends on the order of its test cases. This change unbreaks the build. We should fix this dependency separately.,2025-02-28T21:00:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88366
copybara-service[bot],Disable failing test CompiledProgramsCount.,Disable failing test CompiledProgramsCount.,2025-02-28T20:47:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88365
copybara-service[bot],Clean up `third_party/tensorflow/core/profiler/lib` headers,Clean up `third_party/tensorflow/core/profiler/lib` headers,2025-02-28T20:42:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88364
copybara-service[bot],[XLA] Increase the time limit for cpu_benchmarks.yml to 9 hours (540 minutes).,[XLA] Increase the time limit for cpu_benchmarks.yml to 9 hours (540 minutes).,2025-02-28T20:38:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88363
copybara-service[bot],Add helpers to check if a composite op is indicating DRQ.,Add helpers to check if a composite op is indicating DRQ.,2025-02-28T20:37:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88362
nassimus26,ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors,"for this code  (on colab https://colab.research.google.com/drive/1WKSgxQUSZp4Q5dHeq2HgJvjjVzxJUoqA?usp=sharing) :  ``` cnn = tf.keras.applications.EfficientNetV2B3(       include_top=False,       weights='imagenet',       input_tensor=None,       include_preprocessing=True,       input_shape = (300, 300, 3),       pooling=None,       classes = 2   ) def representative_dataset_gen():     for i in range(0):         yield [] converter = tf.lite.TFLiteConverter.from_keras_model(cnn) converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] converter.target_spec.supported_types = [tf.int8]   extra line missing converter.experimental_new_quantizer = True converter.experimental_new_dynamic_range_quantizer = True converter.inference_output_type = tf.uint8 converter.experimental_new_converter = True converter.experimental_mlir_quantizer = True converter.experimental_enable_resource_variables = False converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.representative_dataset = representative_dataset_gen tflite_model = converter.convert() open(""cnn.tflite"", ""wb"").write(tflite_model) ``` And then  `!edgetpu_compiler s cnn.tflite` I am getting :  ``` Edge TPU Compiler version 16.0.384591198 Started a compilation timeout timer of 180 seconds. ERROR: Attempting to use a delegate that only supports staticsized tensors with a graph that has dynamicsized tensors. Compilation failed: Model failed in Tflite interpreter. Please ensure model can be loaded/run in Tflite interpreter. Compilation child process completed within timeout period. Compilation failed!  ``` I am fully aware that the Coral Dev Board team release the EdgeTPU Model version (efficientnetedgetpuL_quant_edgetpu.tflite) But I am not interested in the default generated EdgeTPU file, indeed I want to do something like this :  ``` cnn = tf.keras.applications.EfficientNetV2B3(..pooling=None,..) // no pooling  x = cnn(cnn.input) x = layers.AveragePooling2D(pool_size=(5, 5), strides=(5, 5), padding=""same"")(x) x = layers.Flatten()(x) mdl = models.Model(inputs = cnn.input, outputs =x) ``` And then build the TFLite and the EdgeTPU. Unfortunately it doesn't seems possible to do this directly on a TFLite or TFliteEdgeTPU model :        For example : how to apply the code above with this : EfficientB3 EDGE_TPU version ? And it seems the Coral Dev Board Team has rewrite the EfficientNet model from scratch, but they don't explain why or how to import their model implemented here TPU repo  By import I mean how to replace the pretrainded **tf.keras.applications.EfficientNetV2B3** with their model ?",2025-02-28T20:35:22Z,stat:awaiting response stale comp:lite TFLiteConverter,closed,0,5,https://github.com/tensorflow/tensorflow/issues/88361,"Hi,   I apologize for the delay in my response, I have been able to replicate the similar issue with your provided code and I'm also getting same error message please refer this gistfile so we will have to dig more into this issue and will update you. **Here is error log output for reference :** ``` Edge TPU Compiler version 16.0.384591198 Started a compilation timeout timer of 180 seconds. ERROR: Attempting to use a delegate that only supports staticsized tensors with a graph that has dynamicsized tensors. Compilation failed: Model failed in Tflite interpreter. Please ensure model can be loaded/run in Tflite interpreter. Compilation child process completed within timeout period. Compilation failed!  ``` Thank you for your cooperation and patience.","Hi  , thank you, what do you mean by : please refer this [gistfile] ?  What do you want me to do about it ?, I already write you a test case and publish it on this page. And I hope that the Edge compiler project is not dead, most of their converted models has been written with TF 1.x, and the compiler has not been updated since many years, and it seems not Open source.","Hi,  I apologize for the delayed response, sorry for the confusion I mean to say I am able to replicate the same behavior which you reported from my end so for further investigation from our end as reference I added gistfile As per the official documentation of TensorFlow models on the Edge TPU specifically Model requirements section the model must meet these basic requirements:  Tensor parameters are quantized (8bit fixedpoint numbers; int8 or uint8).  Tensor sizes are constant at compiletime (no dynamic sizes).  Model parameters (such as bias tensors) are constant at compiletime.  Tensors are either 1, 2, or 3dimensional. If a tensor has more than 3 dimensions, then only the 3 innermost dimensions may have a size greater than 1.  The model uses only the operations supported by the Edge TPU (see table 1 below). Please use tools like Netron to visualize your model and check for operations that produce dynamicshaped tensors. The Edge TPU requires models to be quantized to (8bit fixedpoint numbers; int8 or uint8). Make sure that the representative_dataset is correctly set and that the model is fully quantized. some layers or operations might not be supported by the Edge TPU. Please make sure that all layers in your model are compatible with Edge TPU. You can refer to the coral documentation for a list of supported operations. In some cases there are no dynamic size tensors in the graph but it has a control flow op, While op which classifies graph as dynamic graph that might be the cause for this. Thank you for your cooperation and understanding.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
copybara-service[bot],[PJRT:CPU] Implement `PjRtDevice::PoisonExecution()` API.,"[PJRT:CPU] Implement `PjRtDevice::PoisonExecution()` API. `PjRtDevice::PoisonExecution()` API provides a mechanism to mark output buffers of a pending asynchronous execution with an error, where the error is constructed above PjRt and potentially reflect a broader view on the system state. This change implements this API for the PJRT CPU client to improve support for cancelling pending executions. The state of poisonable executions is owned by `TfrtCpuDevice` and accessed from `TfrtCpuExecutable` as via a new class `TfrtCpuAsyncExecutionTracker`.",2025-02-28T20:09:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88360
copybara-service[bot],Visibility update for profiler_impl,Visibility update for profiler_impl,2025-02-28T19:55:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88359
copybara-service[bot],Return error when failing to find kernel runner instead of letting it fall through,Return error when failing to find kernel runner instead of letting it fall through and cause crash.,2025-02-28T19:51:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88358
AD-lite24,Disabling GPU delegate for particular tflite nodes, Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17.1  Custom code Yes  OS platform and distribution Linux Ubuntu 18.04 ARM  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The OpenCL GPU delegate on our device doesn't support floor operations causing the delegation for the entire graph to fail.  ``` INFO: Loaded OpenCL library with dlopen. ERROR: No selector for floor ERROR: Falling back to OpenGL ``` Is there anyway I can disable delegation specifically for the nodes involving floor ops so that it doesn't tank my entire delegation?   Standalone code to reproduce the issue ```shell N/A ```  Relevant log output ```shell ```,2025-02-28T19:45:40Z,type:feature comp:lite 2.17,open,0,6,https://github.com/tensorflow/tensorflow/issues/88357,"Hi, lite24 I apologize for the delay in my response, As far I know to resolve delegation failures caused by unsupported `floor` operations in TFLite's OpenCL GPU delegate use custom delegate with Op filtering implement a delegate that skips `floor` operations during GPU delegation, force floor ops to CPU via explicit partitioning or replace floor with a GPU compatible approximation during model conversion if precision allows.  Please refer official documentation of Implementing a Custom Delegate  and GPU ML operations support Thank you for your cooperation and understanding."," I went through the docs but unfortunately none of the delegates seem to implement the interface and all the invoking API does not work with the interface either. Regardless, I figured out on my own how to disable delegating the floor op to GPU, but then I have another set of problems ``` GATHER_ND: Operation is not supported. GREATER: Not supported logical op case. LESS: Not supported logical op case. LOGICAL_NOT: Operation is not supported. LOGICAL_OR: Operation is not supported. MUL: MUL requires one tensor that not less than second in all dimensions. RESHAPE: OP is supported, but tensor type/shape isn't compatible. SCATTER_ND: Operation is not supported. TOPK_V2: Operation is not supported. TRANSPOSE: OP is supported, but tensor type/shape isn't compatible. 28 operations will run on the GPU, and the remaining 164 operations will run on the CPU. VERBOSE: Replacing 28 out of 192 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 3 partitions for the whole graph. ERROR: TfLiteGpuDelegate Init: MUL: Batch size is not equal to 1. INFO: Created 0 GPU delegate kernels. ERROR: TfLiteGpuDelegate Prepare: delegate is not initialized ERROR: Node number 192 (TfLiteGpuDelegateV2) failed to prepare. ERROR: Restored original execution plan after delegate application failure. GPU delegate error. INFO: Applying 1 TensorFlow Lite delegate(s) lazily. INFO: Created TensorFlow Lite XNNPACK delegate for CPU. INFO: XNNPack weight cache not enabled ``` What I fail to understand is why the delegate fails to apply to the entire graph every time a singular node fails"," Please find the output from the tensorflow analyzer. I find it rather odd that the MUL op has a problem with the batch dimension not being 1 (why else would I have a batch dimension in that case) and more importantly the exception is thrown during delegate init and not during checking, causing the entire delegate to fail. ``` === TFLite ModelAnalyzer === Your TFLite model has '1' subgraph(s). In the subgraph description below, T represents the Tensor numbers. For example, in Subgraph CC(未找到相关数据), the DEQUANTIZE op takes tensor CC(Specify output tensor for ops from python) as input and produces tensor CC(Mac: OSError: [Errno 1] Operation not permitted: '/tmp/pipXcfgD6uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six1.4.1py2.7.egginfo') as output. Subgraph CC(未找到相关数据) main(T CC(未找到相关数据)) > [T CC(No documentation for Saver class), T CC(error in computation graph tutorials)]   Op CC(未找到相关数据) DEQUANTIZE(T CC(Specify output tensor for ops from python)) > [T CC(Mac: OSError: [Errno 1] Operation not permitted: '/tmp/pipXcfgD6uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six1.4.1py2.7.egginfo')]   Op CC(Add support for Python 3.x) DEQUANTIZE(T CC(CUDNN error on ""import tensorflow as tf"" for gpu version)) > [T CC(FR: Change TensorBoard image interpolation method)]   Op CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"") DEQUANTIZE(T CC(ImportError: No module named core.framework.graph_pb2)) > [T CC(compilation error)]   Op CC(JVM, .NET Language Support) DEQUANTIZE(T CC(Try to convert to readable latex)) > [T CC(no such package '//': Error downloading from ijg.org)]   Op CC(Installation over pip fails to import with protobuf 2.6.1) DEQUANTIZE(T CC(Error in final step of installation)) > [T CC(Typo)]   Op CC(Java interface) DEQUANTIZE(T CC(No gradient defined for the Reverse op)) > [T CC(models/rnn/ptb not included in pip package)]   Op CC(Pretrained models) DEQUANTIZE(T CC(virtualenv python 2.7.6 import tensorflow. TypeError: __init__() got an unexpected keyword argument 'syntax')) > [T CC(Human Consumable Language Independent DSL)]   Op CC(API docs does not list RNNs) DEQUANTIZE(T CC(pip error: No such file or directory: '/tmp/pip...build/setup.py')) > [T CC(Can't install from source if I don't have a GPU? )]   Op CC(Setting lower gcc version for cuda) DEQUANTIZE(T CC(Javascript > JavaScript)) > [T CC(Target //tensorflow/tools/pip_package:build_pip_package failed to build on OSX)]   Op CC(Typo in getting started guide) DEQUANTIZE(T CC(Can't install on ubuntu 12.04.5 LTS)) > [T CC(No plan for official doc of any other languages than English?)]   Op CC(Go API) DEQUANTIZE(T CC(Problem running RNN example)) > [T CC(Cannot run the android example on Android 5.1.1)]   Op CC(0.5.0 wheel install on Mac OS X using Homebrew python broken) DEQUANTIZE(T CC(error __init__() got an unexpected keyword argument 'syntax')) > [T CC(No module named copy_reg  Installation Issue)]   Op CC(Remote worker configuration) DEQUANTIZE(T CC(OSX PIP Install: Setup.py missing)) > [T CC(cuDNN v2 (6.5) not available anymore)]   Op CC([doc] typo) DEQUANTIZE(T CC(Pretrained models)) > [T CC(1st Class Windows Support)]   Op CC(g3doc format) DEQUANTIZE(T CC(Error in the Getting started/Variables section of the website)) > [T CC(Scalar Equation is incorrect ? Documentation)]   Op CC(Quantized ops?) DEQUANTIZE(T CC(is a Python 3 support coming soon ?)) > [T CC(segmentation fault when running convolutional.py)]   Op CC(iOS Support and Example) DEQUANTIZE(T CC(Updated links in documentation.)) > [T CC(tensorboard gulp  analytics.js missing)]   Op CC(Windows Support and Documentation) DEQUANTIZE(T CC(Node.js (JavaScript) Wrapper API)) > [T CC(Installed from source; unable to import tensorflow)]   Op CC(C api) DEQUANTIZE(T CC(No module named tensorflow.python.platform)) > [T CC(bazel compile error)]   Op CC(Swift API) DEQUANTIZE(T CC(Can't run TensorBoard on El Captain)) > [T CC(can't install on ubuntu 12.04)]   Op CC(CUDA 7.5 fails with pip install and docker (Ubuntu 14.04)) DEQUANTIZE(T CC(Error when run docker image on Mac OS X 10.11.1)) > [T CC(Septation of Generalised DAG / Data Flow Programming Framework and ML Components)]   Op CC(GPU_Base dockerfile image not found) DEQUANTIZE(T CC(Error while installing tensorflow using pip on Ubuntu 14.04 32bit system)) > [T CC(C++ API neural net examples)]   Op CC(OpenCL support) DEQUANTIZE(T CC(Connectionist Temporal Classification example)) > [T CC(GPU implementations for more ops)]   Op CC(Distributed Version) DEQUANTIZE(T CC(Slack Channel)) > [T CC(fixed link to tutorial and some typos)]   Op CC(Problems running the image example (Python 2.7.10, PyEnv, Xubuntu 14.04 64bit)) DEQUANTIZE(T CC(Go API)) > [T CC(Unable to run tensorboard)]   Op CC(Cuda 3.0?) DEQUANTIZE(T CC(minimum req: Cuda compute capability 3.5)) > [T CC(Typo in `/tutorials/mnist/beginners/index.md`)]   Op CC(simplify contributing process) DEQUANTIZE(T CC(Could port to OpenCL?)) > [T CC(Fix 89)]   Op CC(Warning while creating Session on Mac OS X: can't determine number of CPU cores) DEQUANTIZE(T CC(Warning while creating Session on Mac OS X: can't determine number of CPU cores)) > [T CC(import six.moves.copyreg as copyreg error)]   Op CC(Could port to OpenCL?) DEQUANTIZE(T CC(simplify contributing process)) > [T CC(do we have plans for java api?)]   Op CC(minimum req: Cuda compute capability 3.5) DEQUANTIZE(T CC(CUDA 7.5 fails with pip install and docker (Ubuntu 14.04))) > [T CC( build failed!  File ""/usr/lib/python2.7/encodings/__init__.py"", line 123       raise CodecRegistryError,\)]   Op CC(Go API) DEQUANTIZE(T CC(Swift API)) > [T CC(Abandon gerrit and use github for everything)]   Op CC(Slack Channel) DEQUANTIZE(T CC(C api)) > [T CC(Greedy heuristics may not find the optimal node placement)]   Op CC(Connectionist Temporal Classification example) DEQUANTIZE(T CC(Windows Support and Documentation)) > [T CC(tensorflow0.5.0cp27nonelinux_x86_64.whl is not a supported wheel on this platform.)]   Op CC(Error while installing tensorflow using pip on Ubuntu 14.04 32bit system) DEQUANTIZE(T CC(iOS Support and Example)) > [T CC(How to extract predictions)]   Op CC(Error when run docker image on Mac OS X 10.11.1) DEQUANTIZE(T CC(Quantized ops?)) > [T CC(MAC pip install operation not permitted)]   Op CC(Can't run TensorBoard on El Captain) DEQUANTIZE(T CC(Go API)) > [T CC(Fix when installation on OSX)]   Op CC(No module named tensorflow.python.platform) DEQUANTIZE(T CC(Setting lower gcc version for cuda)) > [T CC(Installing from source  problem with bazel)]   Op CC(Node.js (JavaScript) Wrapper API) DEQUANTIZE(T CC(Quantized ops?)) > [T CC(Neural Translation Model example fails due to missing EN tokens )]   Op CC(Updated links in documentation.) DEQUANTIZE(T CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"")) > [T CC(Fix 'Fetches' example in basic_usage)]   Op CC(is a Python 3 support coming soon ?) DEQUANTIZE(T CC(Add support for Python 3.x)) > [T CC(Alpine Linux: __isnanf: symbol not found )]   Op CC(Error in the Getting started/Variables section of the website) CONV_2D(T CC(未找到相关数据), T CC(Fix 'Fetches' example in basic_usage), T CC(Alpine Linux: __isnanf: symbol not found )) > [T CC(Support for python 3)]   Op CC(Pretrained models) CONV_2D(T CC(Support for python 3), T CC(fixed link to tutorial and some typos), T CC(Target //tensorflow/tools/pip_package:build_pip_package failed to build on OSX)) > [T CC(Can't build from source?)]   Op CC(Windows support) MAX_POOL_2D(T CC(Can't build from source?)) > [T CC(Ubuntu installation error using pip)]   Op CC(Cannot import after installing with pip) CONV_2D(T CC(Ubuntu installation error using pip), T CC(GPU implementations for more ops), T CC(Can't install from source if I don't have a GPU? )) > [T CC(Truncated backprop docs are confusing)]   Op CC(Can't install on El Capitan  probably python again :() CONV_2D(T CC(Truncated backprop docs are confusing), T CC(C++ API neural net examples), T CC(Human Consumable Language Independent DSL)) > [T CC(Building a shared libary)]   Op CC(OSX PIP Install: Setup.py missing) MAX_POOL_2D(T CC(Building a shared libary)) > [T CC( C++ compilation of rule '//tensorflow/python:tf_session_helper' failed)]   Op CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"") CONV_2D(T CC( C++ compilation of rule '//tensorflow/python:tf_session_helper' failed), T CC(Septation of Generalised DAG / Data Flow Programming Framework and ML Components), T CC(models/rnn/ptb not included in pip package)) > [T CC(Support for Redhat, Centos and many superclusters)]   Op CC(Integration with blaze ecosystem numba python to llvm compiler?) CONV_2D(T CC(Support for Redhat, Centos and many superclusters), T CC(can't install on ubuntu 12.04), T CC(Typo)) > [T CC(configure script hardcodes location of cuda that makes it fail on OSX)]   Op CC(Object Detection) MAX_POOL_2D(T CC(configure script hardcodes location of cuda that makes it fail on OSX)) > [T CC(EC2 g2.2xlarge: Ignoring gpu device (GRID K520) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.)]   Op CC(error __init__() got an unexpected keyword argument 'syntax') CONV_2D(T CC(EC2 g2.2xlarge: Ignoring gpu device (GRID K520) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.), T CC(bazel compile error), T CC(no such package '//': Error downloading from ijg.org)) > [T CC(AlexNet with FC layers: backward is very slow?)]   Op CC(Ruby API) CONV_2D(T CC(AlexNet with FC layers: backward is very slow?), T CC(Installed from source; unable to import tensorflow), T CC(compilation error)) > [T CC(Matrix multiplication in softmax documentation carried out incorrectly)]   Op CC(Ubuntu ImportError: No module named core.framework.graph_pb2) CONV_2D(T CC(Matrix multiplication in softmax documentation carried out incorrectly), T CC(tensorboard gulp  analytics.js missing), T CC(FR: Change TensorBoard image interpolation method)) > [T CC(Incorrect matrix math in tutorial at:  http://www.tensorflow.org/tutorials/mnist/beginners/index.md)]   Op CC(Problem running RNN example) CONV_2D(T CC(Incorrect matrix math in tutorial at:  http://www.tensorflow.org/tutorials/mnist/beginners/index.md), T CC(segmentation fault when running convolutional.py), T CC(Cannot run the android example on Android 5.1.1)) > [T CC(When will you have a version of TensorFlow for Win10/8/7?)]   Op CC(Can't install on ubuntu 12.04.5 LTS) SOFTMAX(T CC(When will you have a version of TensorFlow for Win10/8/7?)) > [T CC(TensorBoard logdir path, if relative, is relative to $HOME)]   Op CC(Support cuda 7.5 and cudnn 7.0) CONV_2D(T CC(Matrix multiplication in softmax documentation carried out incorrectly), T CC(Scalar Equation is incorrect ? Documentation), T CC(Mac: OSError: [Errno 1] Operation not permitted: '/tmp/pipXcfgD6uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six1.4.1py2.7.egginfo')) > [T CC(""help wanted"" I cannot get the TensorBoard working, I am following the given tutorial)]   Op CC(Javascript > JavaScript) CONV_2D(T CC(""help wanted"" I cannot get the TensorBoard working, I am following the given tutorial), T CC(1st Class Windows Support), T CC(No plan for official doc of any other languages than English?)) > [T CC(Bazel can't build protobuf)]   Op CC(pip error: No such file or directory: '/tmp/pip...build/setup.py') MUL(T CC(Bazel can't build protobuf), T CC(Bazel can't build protobuf)) > [T CC(TensorFlow session.run() overhead for graphs with few flops)]   Op CC(virtualenv python 2.7.6 import tensorflow. TypeError: __init__() got an unexpected keyword argument 'syntax') SUM(T CC(TensorFlow session.run() overhead for graphs with few flops), T CC(0.5.0 wheel install on Mac OS X using Homebrew python broken)[3]) > [T CC(ImportError: undefined symbol: clock_gettime)]   Op CC(No gradient defined for the Reverse op) SQRT(T CC(ImportError: undefined symbol: clock_gettime)) > [T CC(Windows Installation?)]   Op CC(Error in final step of installation) MAXIMUM(T CC(Windows Installation?), T CC(MAC pip install operation not permitted)) > [T CC(Cant install, Mac  El Capitan  not a supported wheel)]   Op CC(Try to convert to readable latex) MUL(T CC(Cant install, Mac  El Capitan  not a supported wheel), T CC(How to extract predictions)) > [T CC(tools/jdk: BUILD file not found on package path.)]   Op CC(ImportError: No module named core.framework.graph_pb2) DIV(T CC(Bazel can't build protobuf), T CC(tools/jdk: BUILD file not found on package path.)) > [T CC(Can anyone install it with cuda7.5 and cudnn 7.0?)]   Op CC(CUDNN error on ""import tensorflow as tf"" for gpu version) STRIDED_SLICE(T CC(TensorBoard logdir path, if relative, is relative to $HOME), T CC(Windows support)[0, 0, 0, 0], T CC(Cannot import after installing with pip)[0, 0, 0, 64], T CC(Can't install on El Capitan  probably python again :()[1, 1, 1, 1]) > [T CC(cpu version Installed successfully, but cannot import tensorflow in python.)]   Op CC(Specify output tensor for ops from python) TRANSPOSE(T CC(cpu version Installed successfully, but cannot import tensorflow in python.), T CC(Remote worker configuration)[0, 3, 1, 2]) > [T CC(cannot use bazel to compile tensorflow example codes)]   Op CC(Mac: OSError: [Errno 1] Operation not permitted: '/tmp/pipXcfgD6uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six1.4.1py2.7.egginfo') RESHAPE(T CC(cannot use bazel to compile tensorflow example codes), T CC(Problems running the image example (Python 2.7.10, PyEnv, Xubuntu 14.04 64bit))[2, 8, 8, 64, 64]) > [T CC(Lots of C++ compiletime warnings)]   Op CC(FR: Change TensorBoard image interpolation method) TRANSPOSE(T CC(Lots of C++ compiletime warnings), T CC([doc] typo)[0, 3, 1, 4, 2]) > [T CC(Update docs to instruct to use Bazel 0.1.1 installer)]   Op CC(compilation error) RESHAPE(T CC(Update docs to instruct to use Bazel 0.1.1 installer), T CC(Support cuda 7.5 and cudnn 7.0)[2, 512, 512, 1]) > [T CC(Use Bazel 0.1.1)]   Op CC(no such package '//': Error downloading from ijg.org) MAX_POOL_2D(T CC(Use Bazel 0.1.1)) > [T CC(CUDA 7.0 is hardcoded in `configure` script for Linux)]   Op CC(Typo) EQUAL(T CC(Use Bazel 0.1.1), T CC(CUDA 7.0 is hardcoded in `configure` script for Linux)) > [T CC(Communicating channels: gitter.im + discourse)]   Op CC(models/rnn/ptb not included in pip package) CAST(T CC(Communicating channels: gitter.im + discourse)) > [T CC(Linux installation problem with VirtualEnv)]   Op CC(Human Consumable Language Independent DSL) MAX_POOL_2D(T CC(Linux installation problem with VirtualEnv)) > [T CC(Linux installation issue for GPUenabled version)]   Op CC(Can't install from source if I don't have a GPU? ) CAST(T CC(Linux installation issue for GPUenabled version)) > [T CC(osx 10.11 installation issues)]   Op CC(Target //tensorflow/tools/pip_package:build_pip_package failed to build on OSX) LOGICAL_NOT(T CC(osx 10.11 installation issues)) > [T CC(ResourceExhaustedError in CNN/MNIST example (with GPU))]   Op CC(No plan for official doc of any other languages than English?) SELECT(T CC(osx 10.11 installation issues), T CC(Fix 89), T CC(Use Bazel 0.1.1)) > [T CC(bazel always redownloaded the dependency libraries)]   Op CC(Cannot run the android example on Android 5.1.1) MAX_POOL_2D(T CC(bazel always redownloaded the dependency libraries)) > [T CC(Out of Memory in mnist?)]   Op CC(No module named copy_reg  Installation Issue) EQUAL(T CC(bazel always redownloaded the dependency libraries), T CC(Out of Memory in mnist?)) > [T CC(unable to use nn.moments when the dimension of the axis is None)]   Op CC(cuDNN v2 (6.5) not available anymore) LOGICAL_AND(T CC(unable to use nn.moments when the dimension of the axis is None), T CC(ResourceExhaustedError in CNN/MNIST example (with GPU))) > [T CC(Questions about using LSTM )]   Op CC(1st Class Windows Support) LOGICAL_OR(T CC(Communicating channels: gitter.im + discourse), T CC(Questions about using LSTM )) > [T CC(typo in decaying the learning rate example)]   Op CC(Scalar Equation is incorrect ? Documentation) CAST(T CC(typo in decaying the learning rate example)) > [T CC(TF not compatible with AWS GPU instances?)]   Op CC(segmentation fault when running convolutional.py) MAX_POOL_2D(T CC(TF not compatible with AWS GPU instances?)) > [T CC(Truncated backdrop with max pooling over time)]   Op CC(tensorboard gulp  analytics.js missing) CAST(T CC(Truncated backdrop with max pooling over time)) > [T CC(tutorial GPU issue)]   Op CC(Installed from source; unable to import tensorflow) LOGICAL_NOT(T CC(tutorial GPU issue)) > [T CC(unable to install Inside the virtualenv, install TensorFlow:)]   Op CC(bazel compile error) SELECT(T CC(tutorial GPU issue), T CC(Fix 89), T CC(Use Bazel 0.1.1)) > [T CC(Wrong multiplication in MNIST beginner tutorial)]   Op CC(can't install on ubuntu 12.04) MAX_POOL_2D(T CC(Wrong multiplication in MNIST beginner tutorial)) > [T CC(add multiplemachine support)]   Op CC(Septation of Generalised DAG / Data Flow Programming Framework and ML Components) EQUAL(T CC(Wrong multiplication in MNIST beginner tutorial), T CC(add multiplemachine support)) > [T CC(Wrong link in the ""common problems"" docs)]   Op CC(C++ API neural net examples) LOGICAL_AND(T CC(Wrong link in the ""common problems"" docs), T CC(unable to install Inside the virtualenv, install TensorFlow:)) > [T CC(Official Tensorflow Docker Image)]   Op CC(GPU implementations for more ops) LOGICAL_OR(T CC(typo in decaying the learning rate example), T CC(Official Tensorflow Docker Image)) > [T CC(Is there 3D ConvNets support ? )]   Op CC(fixed link to tutorial and some typos) SELECT(T CC(Is there 3D ConvNets support ? ), T CC(Use Bazel 0.1.1), T CC(Fix 89)) > [T CC(Typo in reshape documentation)]   Op CC(Unable to run tensorboard) GATHER(T CC(Typo in reshape documentation), T CC(API docs does not list RNNs)[0]) > [T CC(Segmentation fault when GPUs are already used)]   Op CC(Typo in `/tutorials/mnist/beginners/index.md`) TRANSPOSE(T CC(Segmentation fault when GPUs are already used), T CC(Typo in getting started guide)[0, 2, 1]) > [T CC(Library not loaded: /usr/lib/libc++.1.dylib)]   Op CC(Fix 89) SCATTER_ND(T CC(Integration with blaze ecosystem numba python to llvm compiler?)[0, 0, 0, 1, 0, ...], T CC(cuDNN v2 (6.5) not available anymore), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(Changes to word2vec_basic.py)]   Op CC(import six.moves.copyreg as copyreg error) SUB(T CC(Fix when installation on OSX), T CC(Changes to word2vec_basic.py)) > [T CC(Complete the loop before returning the words)]   Op CC(do we have plans for java api?) MUL(T CC(Complete the loop before returning the words), T CC(Library not loaded: /usr/lib/libc++.1.dylib)) > [T CC(RuntimeError: Broken toolchain: cannot link a simple C program)]   Op CC( build failed!  File ""/usr/lib/python2.7/encodings/__init__.py"", line 123       raise CodecRegistryError,\) SCATTER_ND(T CC(Integration with blaze ecosystem numba python to llvm compiler?)[0, 0, 0, 1, 0, ...], T CC(do we have plans for java api?), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(Change test set in mnist demo to use batches to avoid being OOM (>4GB) on gpu.)]   Op CC(Abandon gerrit and use github for everything) ADD(T CC(RuntimeError: Broken toolchain: cannot link a simple C program), T CC(Change test set in mnist demo to use batches to avoid being OOM (>4GB) on gpu.)) > [T CC(Switch int to uint to remove some warnings)]   Op CC(Greedy heuristics may not find the optimal node placement) TRANSPOSE(T CC(Switch int to uint to remove some warnings), T CC(Typo in getting started guide)[0, 2, 1]) > [T CC(tensorflow binary image for ARM architecture)]   Op CC(tensorflow0.5.0cp27nonelinux_x86_64.whl is not a supported wheel on this platform.) SCATTER_ND(T CC(Object Detection)[0, 508, 0, 509, 0, ...], T CC(cuDNN v2 (6.5) not available anymore), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(doc for install from source of pip+gpu missing config=cuda and use_gpu)]   Op CC(How to extract predictions) SUB(T CC(Fix when installation on OSX), T CC(doc for install from source of pip+gpu missing config=cuda and use_gpu)) > [T CC(Can't install tensorflow on OS X  problem with virtualenv)]   Op CC(MAC pip install operation not permitted) MUL(T CC(Can't install tensorflow on OS X  problem with virtualenv), T CC(tensorflow binary image for ARM architecture)) > [T CC(Any Roadmap Available?)]   Op CC(Fix when installation on OSX) SCATTER_ND(T CC(Object Detection)[0, 508, 0, 509, 0, ...], T CC(do we have plans for java api?), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(Published Roadmap)]   Op CC(Installing from source  problem with bazel) ADD(T CC(Any Roadmap Available?), T CC(Published Roadmap)) > [T CC(Direct Native Code / LLVM IR generation / JIT Compilation / Staging / Incremental Computing)]   Op CC(Neural Translation Model example fails due to missing EN tokens ) TRANSPOSE(T CC(Direct Native Code / LLVM IR generation / JIT Compilation / Staging / Incremental Computing), T CC(Typo in getting started guide)[0, 2, 1]) > [T CC(in_top_k op does not work with int64 labels)]   Op CC(Fix 'Fetches' example in basic_usage) SCATTER_ND(T CC(Ruby API)[0, 0, 0, 0, 0, ...], T CC(No module named copy_reg  Installation Issue), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(Does TensorFlow support temporal convolution)]   Op CC(Alpine Linux: __isnanf: symbol not found ) SUB(T CC(Fix when installation on OSX), T CC(Does TensorFlow support temporal convolution)) > [T CC(g3doc is not installed when using pip )]   Op CC(Support for python 3) MUL(T CC(g3doc is not installed when using pip ), T CC(in_top_k op does not work with int64 labels)) > [T CC(tensor flow does not support operator.__truediv__)]   Op CC(Can't build from source?) SCATTER_ND(T CC(Ruby API)[0, 0, 0, 0, 0, ...], T CC(import six.moves.copyreg as copyreg error), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(Small typo in Deep MNIST Tutorial)]   Op CC(Ubuntu installation error using pip) ADD(T CC(tensor flow does not support operator.__truediv__), T CC(Small typo in Deep MNIST Tutorial)) > [T CC(Not a gzipped file)]   Op CC(Truncated backprop docs are confusing) TRANSPOSE(T CC(Not a gzipped file), T CC(Typo in getting started guide)[0, 2, 1]) > [T CC(Tensorboard creates unecessary loops in graph)]   Op CC(Building a shared libary) SCATTER_ND(T CC(Ubuntu ImportError: No module named core.framework.graph_pb2)[0, 0, 508, 0, 0, ...], T CC(No module named copy_reg  Installation Issue), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(Wrong Logistic Loss)]   Op CC( C++ compilation of rule '//tensorflow/python:tf_session_helper' failed) SUB(T CC(Fix when installation on OSX), T CC(Wrong Logistic Loss)) > [T CC(Unable to restore trained models on the enfr translate model:  tensorflow.python.framework.errors.NotFoundError: Tensor name ""embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Matrix"" not found)]   Op CC(Support for Redhat, Centos and many superclusters) MUL(T CC(Unable to restore trained models on the enfr translate model:  tensorflow.python.framework.errors.NotFoundError: Tensor name ""embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Matrix"" not found), T CC(Tensorboard creates unecessary loops in graph)) > [T CC(run from script only instead of compiling with bazel?)]   Op CC(configure script hardcodes location of cuda that makes it fail on OSX) SCATTER_ND(T CC(Ubuntu ImportError: No module named core.framework.graph_pb2)[0, 0, 508, 0, 0, ...], T CC(import six.moves.copyreg as copyreg error), T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")[2, 512, 512]) > [T CC(einsumlike function?)]   Op CC(EC2 g2.2xlarge: Ignoring gpu device (GRID K520) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.) ADD(T CC(run from script only instead of compiling with bazel?), T CC(einsumlike function?)) > [T CC(Split a tensor with Tensor 0D int32 as num_split argument)]   Op CC(AlexNet with FC layers: backward is very slow?) TRANSPOSE(T CC(Split a tensor with Tensor 0D int32 as num_split argument), T CC(Typo in getting started guide)[0, 2, 1]) > [T CC(ImportError: /lib64/libc.so.6: version `GLIBC_2.17')]   Op CC(Matrix multiplication in softmax documentation carried out incorrectly) RESHAPE(T CC(ImportError: /lib64/libc.so.6: version `GLIBC_2.17'), T CC(Cuda 3.0?)[2, 262144]) > [T CC(Installation error)]   Op CC(Incorrect matrix math in tutorial at:  http://www.tensorflow.org/tutorials/mnist/beginners/index.md) TOPK_V2(T CC(Installation error), T CC(GPU_Base dockerfile image not found)[512]) > [T CC(Typo in TensorBoard: Visualizing Learning docs), T CC('state' is not defined )]   Op CC(When will you have a version of TensorFlow for Win10/8/7?) CAST(T CC('state' is not defined )) > [T CC(Typo in tf.Graph.name_scope(name) docs)]   Op CC(TensorBoard logdir path, if relative, is relative to $HOME) RESHAPE(T CC(Typo in tf.Graph.name_scope(name) docs), T CC(OpenCL support)[2, 512, 1]) > [T CC(The icon is assymetric)]   Op CC(""help wanted"" I cannot get the TensorBoard working, I am following the given tutorial) CAST(T CC(The icon is assymetric)) > [T CC(does tensorboard allow for building/editing models?)]   Op CC(Bazel can't build protobuf) MUL(T CC(does tensorboard allow for building/editing models?), T CC(Typo in `/tutorials/mnist/beginners/index.md`)) > [T CC(Dropout Loses Shape Inference Information)]   Op CC(TensorFlow session.run() overhead for graphs with few flops) CAST(T CC(Dropout Loses Shape Inference Information)) > [T CC(Incorrect matrix in beginner tutorial on www.tensorflow.org)]   Op CC(ImportError: undefined symbol: clock_gettime) FLOOR_MOD(T CC(Incorrect matrix in beginner tutorial on www.tensorflow.org), T CC(g3doc format)) > [T CC(No documentation for Saver class)]   Op CC(Windows Installation?) CAST(T CC(No documentation for Saver class)) > [T CC(Make Python/Numpy include paths configurable)]   Op CC(Cant install, Mac  El Capitan  not a supported wheel) SUB(T CC(Make Python/Numpy include paths configurable), T CC( build failed!  File ""/usr/lib/python2.7/encodings/__init__.py"", line 123       raise CodecRegistryError,\)) > [T CC(Extra exp in softmax formula?)]   Op CC(tools/jdk: BUILD file not found on package path.) MUL(T CC(Extra exp in softmax formula?), T CC(tensorflow0.5.0cp27nonelinux_x86_64.whl is not a supported wheel on this platform.)) > [T CC(No model directory)]   Op CC(Can anyone install it with cuda7.5 and cudnn 7.0?) MUL(T CC(No model directory), T CC(Unable to run tensorboard)) > [T CC(when install from sources, I encounter ERROR: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed)]   Op CC(cpu version Installed successfully, but cannot import tensorflow in python.) SUB(T CC(when install from sources, I encounter ERROR: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed), T CC(Fix when installation on OSX)) > [T CC(Unrecognized option: data_dir)]   Op CC(cannot use bazel to compile tensorflow example codes) ADD(T CC(Unrecognized option: data_dir), T CC(Fix when installation on OSX)) > [T CC(ImportError: No module named copyreg)]   Op CC(Lots of C++ compiletime warnings) MUL(T CC(ImportError: No module named copyreg), T CC(Greedy heuristics may not find the optimal node placement)) > [T CC(Building from source for BSD)]   Op CC(Update docs to instruct to use Bazel 0.1.1 installer) RESHAPE(T CC(Building from source for BSD), T CC(Pretrained models)[2, 1, 512, 2]) > [T CC(g3doc tutorial mnist.py tf.range missing argument)]   Op CC(Use Bazel 0.1.1) SPLIT(T CC(Installation over pip fails to import with protobuf 2.6.1)[1], T CC(g3doc tutorial mnist.py tf.range missing argument)) > [T CC(Maybe the website in other languages?), T CC(Padding type definition is swapped in the documentation.)]   Op CC(CUDA 7.0 is hardcoded in `configure` script for Linux) FLOOR(T CC(Maybe the website in other languages?)) > [T CC(Small typo in Beginners MNIST Tutorial)]   Op CC(Communicating channels: gitter.im + discourse) MINIMUM(T CC(Small typo in Beginners MNIST Tutorial), T CC(Installing from source  problem with bazel)) > [T CC(cumulative longer epochs)]   Op CC(Linux installation problem with VirtualEnv) RELU(T CC(cumulative longer epochs)) > [T CC(a Makefile would be supremely helpful)]   Op CC(Linux installation issue for GPUenabled version) ADD(T CC(a Makefile would be supremely helpful), T CC(Fix when installation on OSX)) > [T CC(Broken link on beginner's tutorial page)]   Op CC(osx 10.11 installation issues) MINIMUM(T CC(Broken link on beginner's tutorial page), T CC(Installing from source  problem with bazel)) > [T CC(How To doc for tensor indexing and assigning ops)]   Op CC(ResourceExhaustedError in CNN/MNIST example (with GPU)) RELU(T CC(How To doc for tensor indexing and assigning ops)) > [T CC(Better error message for tf.assign)]   Op CC(bazel always redownloaded the dependency libraries) SUB(T CC(Maybe the website in other languages?), T CC(a Makefile would be supremely helpful)) > [T CC(Promote usage of docker above all other installation methods)]   Op CC(Out of Memory in mnist?) SUB(T CC(Fix when installation on OSX), T CC(Promote usage of docker above all other installation methods)) > [T CC(Using 3d Input for seq2seq Models  Word Vector Input)]   Op CC(unable to use nn.moments when the dimension of the axis is None) FLOOR(T CC(Padding type definition is swapped in the documentation.)) > [T CC(Remove unnecessary null pointer checks)]   Op CC(Questions about using LSTM ) MINIMUM(T CC(Remove unnecessary null pointer checks), T CC(Installing from source  problem with bazel)) > [T CC(Generalize slicing and slice assignment ops (including gather and scatter))]   Op CC(typo in decaying the learning rate example) RELU(T CC(Generalize slicing and slice assignment ops (including gather and scatter))) > [T CC(confused by device placement on Amazon AWS)]   Op CC(TF not compatible with AWS GPU instances?) ADD(T CC(confused by device placement on Amazon AWS), T CC(Fix when installation on OSX)) > [T CC(Symbolic loops (like ""scan"" in Theano))]   Op CC(Truncated backdrop with max pooling over time) MINIMUM(T CC(Symbolic loops (like ""scan"" in Theano)), T CC(Installing from source  problem with bazel)) > [T CC(Yann LeCun's web page is not available  cannot download data from there is there a mirror?)]   Op CC(tutorial GPU issue) RELU(T CC(Yann LeCun's web page is not available  cannot download data from there is there a mirror?)) > [T CC(feature request: softmax target axes / multidimensional softmax)]   Op CC(unable to install Inside the virtualenv, install TensorFlow:) CONCATENATION(T CC(feature request: softmax target axes / multidimensional softmax), T CC(a Makefile would be supremely helpful)) > [T CC(Add gpu support for LRN)]   Op CC(Wrong multiplication in MNIST beginner tutorial) CAST(T CC(Add gpu support for LRN)) > [T CC(Multiple models in one session)]   Op CC(add multiplemachine support) CONCATENATION(T CC(JVM, .NET Language Support), T CC(Multiple models in one session)) > [T CC(tensorflow howto for sharing variable is confusing)]   Op CC(Wrong link in the ""common problems"" docs) GATHER_ND(T CC(Can anyone install it with cuda7.5 and cudnn 7.0?), T CC(tensorflow howto for sharing variable is confusing)) > [T CC(Beam Search)]   Op CC(Official Tensorflow Docker Image) CONCATENATION(T CC(feature request: softmax target axes / multidimensional softmax), T CC(Better error message for tf.assign)) > [T CC(Question:  Example on how can TensorFlow be used for Text classification?)]   Op CC(Is there 3D ConvNets support ? ) CAST(T CC(Question:  Example on how can TensorFlow be used for Text classification?)) > [T CC(MatMul Broadcasting / tensordot)]   Op CC(Typo in reshape documentation) CONCATENATION(T CC(JVM, .NET Language Support), T CC(MatMul Broadcasting / tensordot)) > [T CC(Is pep8 compatibility necessary right now for tensorflow?)]   Op CC(Segmentation fault when GPUs are already used) GATHER_ND(T CC(Can anyone install it with cuda7.5 and cudnn 7.0?), T CC(Is pep8 compatibility necessary right now for tensorflow?)) > [T CC(bazel run error)]   Op CC(Library not loaded: /usr/lib/libc++.1.dylib) CONCATENATION(T CC(confused by device placement on Amazon AWS), T CC(a Makefile would be supremely helpful)) > [T CC(ImportError:No module named setuptools after exectued the step for bazelbin)]   Op CC(Changes to word2vec_basic.py) CAST(T CC(ImportError:No module named setuptools after exectued the step for bazelbin)) > [T CC(how can I export androidtensorflow into my android stuido)]   Op CC(Complete the loop before returning the words) CONCATENATION(T CC(JVM, .NET Language Support), T CC(how can I export androidtensorflow into my android stuido)) > [T CC(tf.matrix_inverse() is slow compared to numpy.linalg.inv)]   Op CC(RuntimeError: Broken toolchain: cannot link a simple C program) GATHER_ND(T CC(Can anyone install it with cuda7.5 and cudnn 7.0?), T CC(tf.matrix_inverse() is slow compared to numpy.linalg.inv)) > [T CC(Error in api_docs/images/Gather.png )]   Op CC(Change test set in mnist demo to use batches to avoid being OOM (>4GB) on gpu.) CONCATENATION(T CC(confused by device placement on Amazon AWS), T CC(Better error message for tf.assign)) > [T CC(extra cpus not recognized when using docker on windows 10)]   Op CC(Switch int to uint to remove some warnings) CAST(T CC(extra cpus not recognized when using docker on windows 10)) > [T CC(TensorFlow for Jetson TK1 (ARM + Cuda))]   Op CC(tensorflow binary image for ARM architecture) CONCATENATION(T CC(JVM, .NET Language Support), T CC(TensorFlow for Jetson TK1 (ARM + Cuda))) > [T CC(Cannot get TensorBoard example working)]   Op CC(doc for install from source of pip+gpu missing config=cuda and use_gpu) GATHER_ND(T CC(Can anyone install it with cuda7.5 and cudnn 7.0?), T CC(Cannot get TensorBoard example working)) > [T CC(`from tensorflow.g3doc...` is Broken)]   Op CC(Can't install tensorflow on OS X  problem with virtualenv) SUB(T CC(Padding type definition is swapped in the documentation.), T CC(confused by device placement on Amazon AWS)) > [T CC(Compute capability  [T CC(AttributeError in Tensor)]   Op CC(Published Roadmap) MUL(T CC(AttributeError in Tensor), T CC(Beam Search)) > [T CC(from __future__ import division gives error in word2vec_basic when dividing tensors.Just comment out ""from __future__...."")]   Op CC(Direct Native Code / LLVM IR generation / JIT Compilation / Staging / Incremental Computing) MUL(T CC(Compute capability  [T CC(document error)]   Op CC(in_top_k op does not work with int64 labels) MUL(T CC(document error), T CC(bazel run error)) > [T CC(cannot enable peer access from device ordinal 0 to device ordinal 1)]   Op CC(Does TensorFlow support temporal convolution) SUB(T CC(Fix when installation on OSX), T CC(Compute capability  [T CC(NameError: name 'init' is not defined)]   Op CC(g3doc is not installed when using pip ) MUL(T CC(NameError: name 'init' is not defined), T CC(Using 3d Input for seq2seq Models  Word Vector Input)) > [T CC(Need a way to ask users what version of tensorflow they are running)]   Op CC(tensor flow does not support operator.__truediv__) MUL(T CC(Need a way to ask users what version of tensorflow they are running), T CC(Error in api_docs/images/Gather.png )) > [T CC(translate example is missing '' in its 'run' command)]   Op CC(Small typo in Deep MNIST Tutorial) ADD(T CC(translate example is missing '' in its 'run' command), T CC(from __future__ import division gives error in word2vec_basic when dividing tensors.Just comment out ""from __future__...."")) > [T CC(the tutorial Sequence to Sequence Models has errors)]   Op CC(Not a gzipped file) ADD(T CC(the tutorial Sequence to Sequence Models has errors), T CC(cannot enable peer access from device ordinal 0 to device ordinal 1)) > [T CC(Getting Started variable name mismatch)]   Op CC(Tensorboard creates unecessary loops in graph) MUL(T CC(NameError: name 'init' is not defined), T CC(Promote usage of docker above all other installation methods)) > [T CC(Error when running code from seq2seq translate model)]   Op CC(Wrong Logistic Loss) MUL(T CC(Error when running code from seq2seq translate model), T CC(`from tensorflow.g3doc...` is Broken)) > [T CC(Empty input to conv2d causes floating point exception)]   Op CC(Unable to restore trained models on the enfr translate model:  tensorflow.python.framework.errors.NotFoundError: Tensor name ""embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Matrix"" not found) ADD(T CC(Getting Started variable name mismatch), T CC(Empty input to conv2d causes floating point exception)) > [T CC(Basic Usage  Variables: incorrect variable name)]   Op CC(run from script only instead of compiling with bazel?) GREATER(T CC(Maybe the website in other languages?), T CC(Installing from source  problem with bazel)) > [T CC(Mistake in matrix multiplication(http://tensorflow.org/tutorials/mnist/beginners/index.md))]   Op CC(einsumlike function?) GREATER(T CC(Padding type definition is swapped in the documentation.), T CC(Installing from source  problem with bazel)) > [T CC(Minor change for read consistency. (Highlight MD Fix))]   Op CC(Split a tensor with Tensor 0D int32 as num_split argument) LESS(T CC(Maybe the website in other languages?), T CC(Neural Translation Model example fails due to missing EN tokens )) > [T CC(Minor change for tutorial.)]   Op CC(ImportError: /lib64/libc.so.6: version `GLIBC_2.17') LOGICAL_OR(T CC(Minor change for tutorial.), T CC(Mistake in matrix multiplication(http://tensorflow.org/tutorials/mnist/beginners/index.md))) > [T CC(Wheel for binary installation is outdated; consider automating release process)]   Op CC(Installation error) LESS(T CC(Padding type definition is swapped in the documentation.), T CC(Neural Translation Model example fails due to missing EN tokens )) > [T CC(Example word2vec.py)]   Op CC(Typo in TensorBoard: Visualizing Learning docs) LOGICAL_OR(T CC(Example word2vec.py), T CC(Minor change for read consistency. (Highlight MD Fix))) > [T CC(word2vec tutorial plot labels are incorrect)]   Op CC('state' is not defined ) LOGICAL_OR(T CC(Wheel for binary installation is outdated; consider automating release process), T CC(word2vec tutorial plot labels are incorrect)) > [T CC(Examples for loop control flow ops (Enter/Leave/NextIteration))]   Op CC(Typo in tf.Graph.name_scope(name) docs) SELECT_V2(T CC(Examples for loop control flow ops (Enter/Leave/NextIteration)), T CC(Neural Translation Model example fails due to missing EN tokens ), T CC(Basic Usage  Variables: incorrect variable name)) > [T CC(translate module not present in binary pip installation)]   Op CC(The icon is assymetric) TRANSPOSE(T CC(translate module not present in binary pip installation), T CC(Remote worker configuration)[0, 3, 1, 2]) > [T CC(typo in placeholder docs)]   Op CC(does tensorboard allow for building/editing models?) RESHAPE(T CC(typo in placeholder docs), T CC(Distributed Version)[2, 256, 512]) > [T CC(Documentation: 'typo' in softmax explanation image)]   Op CC(Dropout Loses Shape Inference Information) MUL(T CC(typo in placeholder docs), T CC(typo in placeholder docs)) > [T CC(Outofsource builds)]   Op CC(Incorrect matrix in beginner tutorial on www.tensorflow.org) RESHAPE(T CC(Outofsource builds), T CC(Distributed Version)[2, 256, 512]) > [T CC(Install tensorflow from source )]   Op CC(No documentation for Saver class) SUM(T CC(Install tensorflow from source ), T CC(Java interface)[1]) > [T CC(Make TensorFlow compatible with PyPy)]   Op CC(Make Python/Numpy include paths configurable) SQRT(T CC(Make TensorFlow compatible with PyPy)) > [T CC(Need help, how to choose one column of a tensor)]   Op CC(Extra exp in softmax formula?) MAXIMUM(T CC(Need help, how to choose one column of a tensor), T CC(MAC pip install operation not permitted)) > [T CC(Tensorflow on Raspberry Pi)]   Op CC(No model directory) MUL(T CC(Tensorflow on Raspberry Pi), T CC(Abandon gerrit and use github for everything)) > [T CC(CUDA_ERROR_NO_DEVICE)]   Op CC(when install from sources, I encounter ERROR: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed) DIV(T CC(Documentation: 'typo' in softmax explanation image), T CC(CUDA_ERROR_NO_DEVICE)) > [T CC(Transpose convolution layer for tensorflow (was deconvolution))]   Op CC(Unrecognized option: data_dir) TRANSPOSE(T CC(Transpose convolution layer for tensorflow (was deconvolution)), T CC(Typo in getting started guide)[0, 2, 1]) > [T CC(error in computation graph tutorials)] Tensors of Subgraph CC(未找到相关数据)   T CC(未找到相关数据)(images) shape:[2, 512, 512, 1], type:FLOAT32   T CC(Add support for Python 3.x)(arith.constant) shape:[64], type:FLOAT16 RO 128 bytes, buffer: 2, data:[??, ??, ??, ??, ??, ...]   T CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"")(arith.constant1) shape:[64, 3, 3, 1], type:FLOAT16 RO 1152 bytes, buffer: 3, data:[??, ??, ??, ??, ??, ...]   T CC(JVM, .NET Language Support)(arith.constant2) shape:[2, 1, 512, 1], type:INT64 RO 8192 bytes, buffer: 4, data:[??, ??, ??, ??, ??, ...]   T CC(Installation over pip fails to import with protobuf 2.6.1)(arith.constant3) shape:[], type:INT32 RO 4 bytes, buffer: 5, data:[1]   T CC(Java interface)(arith.constant4) shape:[1], type:INT32 RO 4 bytes, buffer: 6, data:[1]   T CC(Pretrained models)(arith.constant5) shape:[4], type:INT32 RO 16 bytes, buffer: 7, data:[2, 1, 512, 2]   T CC(API docs does not list RNNs)(arith.constant6) shape:[], type:INT32 RO 4 bytes, buffer: 8, data:[0]   T CC(Setting lower gcc version for cuda)(arith.constant7) shape:[], type:FLOAT16 RO 2 bytes, buffer: 9, data:[??, ??]   T CC(Typo in getting started guide)(arith.constant8) shape:[3], type:INT32 RO 12 bytes, buffer: 10, data:[0, 2, 1]   T CC(Go API)(arith.constant9) shape:[], type:FLOAT16 RO 2 bytes, buffer: 11, data:[??, ??]   T CC(0.5.0 wheel install on Mac OS X using Homebrew python broken)(arith.constant10) shape:[1], type:INT32 RO 4 bytes, buffer: 12, data:[3]   T CC(Remote worker configuration)(arith.constant11) shape:[4], type:INT32 RO 16 bytes, buffer: 13, data:[0, 3, 1, 2]   T CC([doc] typo)(arith.constant12) shape:[5], type:INT32 RO 20 bytes, buffer: 14, data:[0, 3, 1, 4, 2]   T CC(g3doc format)(arith.constant13) shape:[1, 1, 2], type:INT64 RO 16 bytes, buffer: 15, data:[??, ??, ??, ??, ??, ...]   T CC(Quantized ops?)(arith.constant14) shape:[], type:FLOAT16 RO 2 bytes, buffer: 16, data:[??, ??]   T CC(iOS Support and Example)(arith.constant15) shape:[2, 64, 64, 256], type:FLOAT16 RO 4194304 bytes, buffer: 17, data:[??, ??, ??, ??, ??, ...]   T CC(Windows Support and Documentation)(arith.constant16) shape:[], type:FLOAT16 RO 2 bytes, buffer: 18, data:[??, ??]   T CC(C api)(arith.constant17) shape:[2], type:FLOAT16 RO 4 bytes, buffer: 19, data:[??, ??, ??, ??]   T CC(Swift API)(arith.constant18) shape:[2, 256, 512], type:FLOAT16 RO 524288 bytes, buffer: 20, data:[??, ??, ??, ??, ??, ...]   T CC(CUDA 7.5 fails with pip install and docker (Ubuntu 14.04))(arith.constant19) shape:[], type:FLOAT16 RO 2 bytes, buffer: 21, data:[??, ??]   T CC(GPU_Base dockerfile image not found)(arith.constant20) shape:[], type:INT32 RO 4 bytes, buffer: 22, data:[512]   T CC(OpenCL support)(arith.constant21) shape:[3], type:INT32 RO 12 bytes, buffer: 23, data:[2, 512, 1]   T CC(Distributed Version)(arith.constant22) shape:[3], type:INT32 RO 12 bytes, buffer: 24, data:[2, 256, 512]   T CC(Problems running the image example (Python 2.7.10, PyEnv, Xubuntu 14.04 64bit))(arith.constant23) shape:[5], type:INT32 RO 20 bytes, buffer: 25, data:[2, 8, 8, 64, 64]   T CC(Cuda 3.0?)(arith.constant24) shape:[2], type:INT32 RO 8 bytes, buffer: 26, data:[2, 262144]   T CC(simplify contributing process)(arith.constant25) shape:[2, 4, 512], type:FLOAT16 RO 8192 bytes, buffer: 27, data:[??, ??, ??, ??, ??, ...]   T CC(Warning while creating Session on Mac OS X: can't determine number of CPU cores)(arith.constant26) shape:[2, 512, 4], type:FLOAT16 RO 8192 bytes, buffer: 27, data:[??, ??, ??, ??, ??, ...]   T CC(Could port to OpenCL?)(arith.constant27) shape:[2, 512, 512, 1], type:FLOAT16 RO 1048576 bytes, buffer: 29, data:[??, ??, ??, ??, ??, ...]   T CC(minimum req: Cuda compute capability 3.5)(arith.constant28) shape:[1, 1, 2], type:FLOAT16 RO 4 bytes, buffer: 30, data:[??, ??, ??, ??]   T CC(Go API)(arith.constant29) shape:[1, 1, 2], type:FLOAT16 RO 4 bytes, buffer: 31, data:[??, ??, ??, ??]   T CC(Slack Channel)(arith.constant30) shape:[64, 3, 3, 64], type:FLOAT16 RO 73728 bytes, buffer: 32, data:[??, ??, ??, ??, ??, ...]   T CC(Connectionist Temporal Classification example)(arith.constant31) shape:[64, 3, 3, 64], type:FLOAT16 RO 73728 bytes, buffer: 33, data:[??, ??, ??, ??, ??, ...]   T CC(Error while installing tensorflow using pip on Ubuntu 14.04 32bit system)(arith.constant32) shape:[64, 3, 3, 64], type:FLOAT16 RO 73728 bytes, buffer: 34, data:[??, ??, ??, ??, ??, ...]   T CC(Error when run docker image on Mac OS X 10.11.1)(arith.constant33) shape:[128, 3, 3, 64], type:FLOAT16 RO 147456 bytes, buffer: 35, data:[??, ??, ??, ??, ??, ...]   T CC(Can't run TensorBoard on El Captain)(arith.constant34) shape:[128, 3, 3, 128], type:FLOAT16 RO 294912 bytes, buffer: 36, data:[??, ??, ??, ??, ??, ...]   T CC(No module named tensorflow.python.platform)(arith.constant35) shape:[128, 3, 3, 128], type:FLOAT16 RO 294912 bytes, buffer: 37, data:[??, ??, ??, ??, ??, ...]   T CC(Node.js (JavaScript) Wrapper API)(arith.constant36) shape:[128, 3, 3, 128], type:FLOAT16 RO 294912 bytes, buffer: 38, data:[??, ??, ??, ??, ??, ...]   T CC(Updated links in documentation.)(arith.constant37) shape:[256, 3, 3, 128], type:FLOAT16 RO 589824 bytes, buffer: 39, data:[??, ??, ??, ??, ??, ...]   T CC(is a Python 3 support coming soon ?)(arith.constant38) shape:[65, 1, 1, 256], type:FLOAT16 RO 33280 bytes, buffer: 40, data:[??, ??, ??, ??, ??, ...]   T CC(Error in the Getting started/Variables section of the website)(arith.constant39) shape:[256, 3, 3, 128], type:FLOAT16 RO 589824 bytes, buffer: 41, data:[??, ??, ??, ??, ??, ...]   T CC(Pretrained models)(arith.constant40) shape:[256, 1, 1, 256], type:FLOAT16 RO 131072 bytes, buffer: 42, data:[??, ??, ??, ??, ??, ...]   T CC(Windows support)(arith.constant41) shape:[4], type:INT32 RO 16 bytes, buffer: 43, data:[0, 0, 0, 0]   T CC(Cannot import after installing with pip)(arith.constant42) shape:[4], type:INT32 RO 16 bytes, buffer: 44, data:[0, 0, 0, 64]   T CC(Can't install on El Capitan  probably python again :()(arith.constant43) shape:[4], type:INT32 RO 16 bytes, buffer: 45, data:[1, 1, 1, 1]   T CC(OSX PIP Install: Setup.py missing)(arith.constant44) shape:[2, 4, 512], type:FLOAT16 RO 8192 bytes, buffer: 46, data:[??, ??, ??, ??, ??, ...]   T CC(Missing ""pip install upgrade pip"" in instructions leads to bogus error ""No such file or directory ... setup.py"")(arith.constant45) shape:[3], type:INT32 RO 12 bytes, buffer: 47, data:[2, 512, 512]   T CC(Integration with blaze ecosystem numba python to llvm compiler?)(arith.constant46) shape:[2, 4, 2], type:INT32 RO 64 bytes, buffer: 48, data:[0, 0, 0, 1, 0, ...]   T CC(Object Detection)(arith.constant47) shape:[2, 4, 2], type:INT32 RO 64 bytes, buffer: 49, data:[0, 508, 0, 509, 0, ...]   T CC(error __init__() got an unexpected keyword argument 'syntax')(arith.constant48) shape:[2, 512, 4], type:FLOAT16 RO 8192 bytes, buffer: 46, data:[??, ??, ??, ??, ??, ...]   T CC(Ruby API)(arith.constant49) shape:[2, 512, 4, 3], type:INT32 RO 49152 bytes, buffer: 51, data:[0, 0, 0, 0, 0, ...]   T CC(Ubuntu ImportError: No module named core.framework.graph_pb2)(arith.constant50) shape:[2, 512, 4, 3], type:INT32 RO 49152 bytes, buffer: 52, data:[0, 0, 508, 0, 0, ...]   T CC(Problem running RNN example)(arith.constant51) shape:[65], type:FLOAT16 RO 130 bytes, buffer: 53, data:[??, ??, ??, ??, ??, ...]   T CC(Can't install on ubuntu 12.04.5 LTS)(arith.constant52) shape:[256], type:FLOAT16 RO 512 bytes, buffer: 54, data:[??, ??, ??, ??, ??, ...]   T CC(Support cuda 7.5 and cudnn 7.0)(arith.constant53) shape:[4], type:INT32 RO 16 bytes, buffer: 55, data:[2, 512, 512, 1]   T CC(Javascript > JavaScript)(arith.constant54) shape:[64], type:FLOAT16 RO 128 bytes, buffer: 56, data:[??, ??, ??, ??, ??, ...]   T CC(pip error: No such file or directory: '/tmp/pip...build/setup.py')(arith.constant55) shape:[64], type:FLOAT16 RO 128 bytes, buffer: 57, data:[??, ??, ??, ??, ??, ...]   T CC(virtualenv python 2.7.6 import tensorflow. TypeError: __init__() got an unexpected keyword argument 'syntax')(arith.constant56) shape:[64], type:FLOAT16 RO 128 bytes, buffer: 58, data:[??, ??, ??, ??, ??, ...]   T CC(No gradient defined for the Reverse op)(arith.constant57) shape:[128], type:FLOAT16 RO 256 bytes, buffer: 59, data:[??, ??, ??, ??, ??, ...]   T CC(Error in final step of installation)(arith.constant58) shape:[128], type:FLOAT16 RO 256 bytes, buffer: 60, data:[??, ??, ??, ??, ??, ...]   T CC(Try to convert to readable latex)(arith.constant59) shape:[128], type:FLOAT16 RO 256 bytes, buffer: 61, data:[??, ??, ??, ??, ??, ...]   T CC(ImportError: No module named core.framework.graph_pb2)(arith.constant60) shape:[128], type:FLOAT16 RO 256 bytes, buffer: 62, data:[??, ??, ??, ??, ??, ...]   T CC(CUDNN error on ""import tensorflow as tf"" for gpu version)(arith.constant61) shape:[256], type:FLOAT16 RO 512 bytes, buffer: 63, data:[??, ??, ??, ??, ??, ...]   T CC(Specify output tensor for ops from python)(arith.constant62) shape:[256], type:FLOAT16 RO 512 bytes, buffer: 64, data:[??, ??, ??, ??, ??, ...]   T CC(Mac: OSError: [Errno 1] Operation not permitted: '/tmp/pipXcfgD6uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six1.4.1py2.7.egginfo')(tfl.dequantize) shape:[256], type:FLOAT32   T CC(FR: Change TensorBoard image interpolation method)(tfl.dequantize1) shape:[256], type:FLOAT32   T CC(compilation error)(tfl.dequantize2) shape:[128], type:FLOAT32   T CC(no such package '//': Error downloading from ijg.org)(tfl.dequantize3) shape:[128], type:FLOAT32   T CC(Typo)(tfl.dequantize4) shape:[128], type:FLOAT32   T CC(models/rnn/ptb not included in pip package)(tfl.dequantize5) shape:[128], type:FLOAT32   T CC(Human Consumable Language Independent DSL)(tfl.dequantize6) shape:[64], type:FLOAT32   T CC(Can't install from source if I don't have a GPU? )(tfl.dequantize7) shape:[64], type:FLOAT32   T CC(Target //tensorflow/tools/pip_package:build_pip_package failed to build on OSX)(tfl.dequantize8) shape:[64], type:FLOAT32   T CC(No plan for official doc of any other languages than English?)(tfl.dequantize9) shape:[256], type:FLOAT32   T CC(Cannot run the android example on Android 5.1.1)(tfl.dequantize10) shape:[65], type:FLOAT32   T CC(No module named copy_reg  Installation Issue)(tfl.dequantize11) shape:[2, 512, 4], type:FLOAT32   T CC(cuDNN v2 (6.5) not available anymore)(tfl.dequantize12) shape:[2, 4, 512], type:FLOAT32   T CC(1st Class Windows Support)(tfl.dequantize13) shape:[256, 1, 1, 256], type:FLOAT32   T CC(Scalar Equation is incorrect ? Documentation)(tfl.dequantize14) shape:[256, 3, 3, 128], type:FLOAT32   T CC(segmentation fault when running convolutional.py)(tfl.dequantize15) shape:[65, 1, 1, 256], type:FLOAT32   T CC(tensorboard gulp  analytics.js missing)(tfl.dequantize16) shape:[256, 3, 3, 128], type:FLOAT32   T CC(Installed from source; unable to import tensorflow)(tfl.dequantize17) shape:[128, 3, 3, 128], type:FLOAT32   T CC(bazel compile error)(tfl.dequantize18) shape:[128, 3, 3, 128], type:FLOAT32   T CC(can't install on ubuntu 12.04)(tfl.dequantize19) shape:[128, 3, 3, 128], type:FLOAT32   T CC(Septation of Generalised DAG / Data Flow Programming Framework and ML Components)(tfl.dequantize20) shape:[128, 3, 3, 64], type:FLOAT32   T CC(C++ API neural net examples)(tfl.dequantize21) shape:[64, 3, 3, 64], type:FLOAT32   T CC(GPU implementations for more ops)(tfl.dequantize22) shape:[64, 3, 3, 64], type:FLOAT32   T CC(fixed link to tutorial and some typos)(tfl.dequantize23) shape:[64, 3, 3, 64], type:FLOAT32   T CC(Unable to run tensorboard)(tfl.dequantize24) shape:[1, 1, 2], type:FLOAT32   T CC(Typo in `/tutorials/mnist/beginners/index.md`)(tfl.dequantize25) shape:[1, 1, 2], type:FLOAT32   T CC(Fix 89)(tfl.dequantize26) shape:[2, 512, 512, 1], type:FLOAT32   T CC(import six.moves.copyreg as copyreg error)(tfl.dequantize27) shape:[2, 512, 4], type:FLOAT32   T CC(do we have plans for java api?)(tfl.dequantize28) shape:[2, 4, 512], type:FLOAT32   T CC( build failed!  File ""/usr/lib/python2.7/encodings/__init__.py"", line 123       raise CodecRegistryError,\)(tfl.dequantize29) shape:[], type:FLOAT32   T CC(Abandon gerrit and use github for everything)(tfl.dequantize30) shape:[2, 256, 512], type:FLOAT32   T CC(Greedy heuristics may not find the optimal node placement)(tfl.dequantize31) shape:[2], type:FLOAT32   T CC(tensorflow0.5.0cp27nonelinux_x86_64.whl is not a supported wheel on this platform.)(tfl.dequantize32) shape:[], type:FLOAT32   T CC(How to extract predictions)(tfl.dequantize33) shape:[2, 64, 64, 256], type:FLOAT32   T CC(MAC pip install operation not permitted)(tfl.dequantize34) shape:[], type:FLOAT32   T CC(Fix when installation on OSX)(tfl.dequantize35) shape:[], type:FLOAT32   T CC(Installing from source  problem with bazel)(tfl.dequantize36) shape:[], type:FLOAT32   T CC(Neural Translation Model example fails due to missing EN tokens )(tfl.dequantize37) shape:[], type:FLOAT32   T CC(Fix 'Fetches' example in basic_usage)(model_26/tf.nn.convolution/convolution) shape:[64, 3, 3, 1], type:FLOAT32   T CC(Alpine Linux: __isnanf: symbol not found )(model_26/tf.nn.relu/Relu;model_26/tf.math.add/Add;model_26/tf.nn.convolution/convolution) shape:[64], type:FLOAT32   T CC(Support for python 3)(model_26/tf.nn.relu/Relu;model_26/tf.math.add/Add;model_26/tf.nn.convolution/convolution1) shape:[2, 512, 512, 64], type:FLOAT32   T CC(Can't build from source?)(model_26/tf.nn.relu_1/Relu;model_26/tf.math.add_1/Add;model_26/tf.nn.convolution_1/convolution) shape:[2, 512, 512, 64], type:FLOAT32   T CC(Ubuntu installation error using pip)(model_26/tf.nn.max_pool2d/MaxPool2d) shape:[2, 256, 256, 64], type:FLOAT32   T CC(Truncated backprop docs are confusing)(model_26/tf.nn.relu_2/Relu;model_26/tf.math.add_2/Add;model_26/tf.nn.convolution_2/convolution) shape:[2, 256, 256, 64], type:FLOAT32   T CC(Building a shared libary)(model_26/tf.nn.relu_3/Relu;model_26/tf.math.add_3/Add;model_26/tf.nn.convolution_3/convolution) shape:[2, 256, 256, 64], type:FLOAT32   T CC( C++ compilation of rule '//tensorflow/python:tf_session_helper' failed)(model_26/tf.nn.max_pool2d_1/MaxPool2d) shape:[2, 128, 128, 64], type:FLOAT32   T CC(Support for Redhat, Centos and many superclusters)(model_26/tf.nn.relu_4/Relu;model_26/tf.math.add_4/Add;model_26/tf.nn.convolution_4/convolution) shape:[2, 128, 128, 128], type:FLOAT32   T CC(configure script hardcodes location of cuda that makes it fail on OSX)(model_26/tf.nn.relu_5/Relu;model_26/tf.math.add_7/Add;model_26/tf.nn.convolution_7/convolution) shape:[2, 128, 128, 128], type:FLOAT32   T CC(EC2 g2.2xlarge: Ignoring gpu device (GRID K520) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.)(model_26/tf.nn.max_pool2d_2/MaxPool2d) shape:[2, 64, 64, 128], type:FLOAT32   T CC(AlexNet with FC layers: backward is very slow?)(model_26/tf.nn.relu_6/Relu;model_26/tf.math.add_8/Add;model_26/tf.nn.convolution_8/convolution) shape:[2, 64, 64, 128], type:FLOAT32   T CC(Matrix multiplication in softmax documentation carried out incorrectly)(model_26/tf.nn.relu_7/Relu;model_26/tf.math.add_9/Add;model_26/tf.nn.convolution_9/convolution) shape:[2, 64, 64, 128], type:FLOAT32   T CC(Incorrect matrix math in tutorial at:  http://www.tensorflow.org/tutorials/mnist/beginners/index.md)(model_26/tf.nn.relu_8/Relu;model_26/tf.math.add_10/Add;model_26/tf.nn.convolution_10/convolution) shape:[2, 64, 64, 256], type:FLOAT32   T CC(When will you have a version of TensorFlow for Win10/8/7?)(model_26/tf.math.add_12/Add;model_26/tf.nn.convolution_12/convolution) shape:[2, 64, 64, 65], type:FLOAT32   T CC(TensorBoard logdir path, if relative, is relative to $HOME)(model_26/tf.nn.softmax_1/wa/extractor/Softmax) shape:[2, 64, 64, 65], type:FLOAT32   T CC(""help wanted"" I cannot get the TensorBoard working, I am following the given tutorial)(model_26/tf.nn.relu_9/Relu;model_26/tf.math.add_11/Add;model_26/tf.nn.convolution_11/convolution) shape:[2, 64, 64, 256], type:FLOAT32   T CC(Bazel can't build protobuf)(model_26/tf.math.add_13/Add;model_26/tf.nn.convolution_13/convolution) shape:[2, 64, 64, 256], type:FLOAT32   T CC(TensorFlow session.run() overhead for graphs with few flops)(model_26/tf.compat.v1.norm_1/norm/mul) shape:[2, 64, 64, 256], type:FLOAT32   T CC(ImportError: undefined symbol: clock_gettime)(model_26/tf.compat.v1.norm_1/norm/Sum) shape:[2, 64, 64, 1], type:FLOAT32   T CC(Windows Installation?)(model_26/tf.compat.v1.norm_1/norm/Sqrt) shape:[2, 64, 64, 1], type:FLOAT32   T CC(Cant install, Mac  El Capitan  not a supported wheel)(model_26/tf.math.maximum/Maximum) shape:[2, 64, 64, 1], type:FLOAT32   T CC(tools/jdk: BUILD file not found on package path.)(model_26/tf.math.multiply/Mul) shape:[2, 64, 64, 256], type:FLOAT32   T CC(Can anyone install it with cuda7.5 and cudnn 7.0?)(model_26/tf.math.divide/truediv) shape:[2, 64, 64, 256], type:FLOAT32   T CC(cpu version Installed successfully, but cannot import tensorflow in python.)(model_26/tf.strided_slice/StridedSlice) shape:[2, 64, 64, 64], type:FLOAT32   T CC(cannot use bazel to compile tensorflow example codes)(model_26/tf.compat.v1.transpose_3/transpose) shape:[2, 64, 64, 64], type:FLOAT32   T CC(Lots of C++ compiletime warnings)(model_26/tf.reshape_2/Reshape) shape:[2, 8, 8, 64, 64], type:FLOAT32   T CC(Update docs to instruct to use Bazel 0.1.1 installer)(model_26/tf.compat.v1.transpose_6/transpose) shape:[2, 64, 8, 64, 8], type:FLOAT32   T CC(Use Bazel 0.1.1)(model_26/tf.compat.v1.transpose_12/transpose) shape:[2, 512, 512, 1], type:FLOAT32   T CC(CUDA 7.0 is hardcoded in `configure` script for Linux)(model_26/tf.compat.v1.nn.pool/max_pool) shape:[2, 512, 512, 1], type:FLOAT32   T CC(Communicating channels: gitter.im + discourse)(model_26/tf.math.equal/Equal) shape:[2, 512, 512, 1], type:BOOL   T CC(Linux installation problem with VirtualEnv)(model_26/tf.cast/Cast) shape:[2, 512, 512, 1], type:FLOAT32   T CC(Linux installation issue for GPUenabled version)(model_26/tf.compat.v1.nn.pool_1/max_pool) shape:[2, 512, 512, 1], type:FLOAT32   T CC(osx 10.11 installation issues)(model_26/tf.cast_1/Cast) shape:[2, 512, 512, 1], type:BOOL   T CC(ResourceExhaustedError in CNN/MNIST example (with GPU))(model_26/tf.math.logical_not/LogicalNot) shape:[2, 512, 512, 1], type:BOOL   T CC(bazel always redownloaded the dependency libraries)(model_26/tf.where/SelectV2) shape:[2, 512, 512, 1], type:FLOAT32   T CC(Out of Memory in mnist?)(model_26/tf.compat.v1.nn.pool_2/max_pool) shape:[2, 512, 512, 1], type:FLOAT32   T CC(unable to use nn.moments when the dimension of the axis is None)(model_26/tf.math.equal_1/Equal) shape:[2, 512, 512, 1], type:BOOL   T CC(Questions about using LSTM )(model_26/tf.math.logical_and/LogicalAnd) shape:[2, 512, 512, 1], type:BOOL   T CC(typo in decaying the learning rate example)(model_26/tf.math.logical_or/LogicalOr) shape:[2, 512, 512, 1], type:BOOL   T CC(TF not compatible with AWS GPU instances?)(model_26/tf.cast_2/Cast) shape:[2, 512, 512, 1], type:FLOAT32   T CC(Truncated backdrop with max pooling over time)(model_26/tf.compat.v1.nn.pool_3/max_pool) shape:[2, 512, 512, 1], type:FLOAT32   T CC(tutorial GPU issue)(model_26/tf.cast_3/Cast) shape:[2, 512, 512, 1], type:BOOL   T CC(unable to install Inside the virtualenv, install TensorFlow:)(model_26/tf.math.logical_not_1/LogicalNot) shape:[2, 512, 512, 1], type:BOOL   T CC(Wrong multiplication in MNIST beginner tutorial)(model_26/tf.where_1/SelectV2) shape:[2, 512, 512, 1], type:FLOAT32   T CC(add multiplemachine support)(model_26/tf.compat.v1.nn.pool_4/max_pool) shape:[2, 512, 512, 1], type:FLOAT32   T CC(Wrong link in the ""common problems"" docs)(model_26/tf.math.equal_2/Equal) shape:[2, 512, 512, 1], type:BOOL   T CC(Official Tensorflow Docker Image)(model_26/tf.math.logical_and_1/LogicalAnd) shape:[2, 512, 512, 1], type:BOOL   T CC(Is there 3D ConvNets support ? )(model_26/tf.math.logical_or_1/LogicalOr) shape:[2, 512, 512, 1], type:BOOL   T CC(Typo in reshape documentation)(model_26/tf.where_2/SelectV2) shape:[2, 512, 512, 1], type:FLOAT32   T CC(Segmentation fault when GPUs are already used)(model_26/tf.compat.v1.gather/GatherV2;model_26/tf.compat.v1.gather/GatherV2/axis) shape:[2, 512, 512], type:FLOAT32   T CC(Library not loaded: /usr/lib/libc++.1.dylib)(model_26/tf.compat.v1.transpose_17/transpose) shape:[2, 512, 512], type:FLOAT32   T CC(Changes to word2vec_basic.py)(model_26/tf.compat.v1.transpose_17/transpose1) shape:[2, 512, 512], type:FLOAT32   T CC(Complete the loop before returning the words)(model_26/tf.tensor_scatter_nd_update/TensorScatterUpdate) shape:[2, 512, 512], type:FLOAT32   T CC(RuntimeError: Broken toolchain: cannot link a simple C program)(model_26/tf.tensor_scatter_nd_update/TensorScatterUpdate1) shape:[2, 512, 512], type:FLOAT32   T CC(Change test set in mnist demo to use batches to avoid being OOM (>4GB) on gpu.)(model_26/tf.compat.v1.transpose_17/transpose2) shape:[2, 512, 512], type:FLOAT32   T CC(Switch int to uint to remove some warnings)(model_26/tf.tensor_scatter_nd_update/TensorScatterUpdate2) shape:[2, 512, 512], type:FLOAT32   T CC(tensorflow binary image for ARM architecture)(model_26/tf.compat.v1.transpose_18/transpose) shape:[2, 512, 512], type:FLOAT32   T CC(doc for install from source of pip+gpu missing config=cuda and use_gpu)(model_26/tf.compat.v1.transpose_18/transpose1) shape:[2, 512, 512], type:FLOAT32   T CC(Can't install tensorflow on OS X  problem with virtualenv)(model_26/tf.tensor_scatter_nd_update_1/TensorScatterUpdate) shape:[2, 512, 512], type:FLOAT32   T CC(Any Roadmap Available?)(model_26/tf.tensor_scatter_nd_update_1/TensorScatterUpdate1) shape:[2, 512, 512], type:FLOAT32   T CC(Published Roadmap)(model_26/tf.compat.v1.transpose_18/transpose2) shape:[2, 512, 512], type:FLOAT32   T CC(Direct Native Code / LLVM IR generation / JIT Compilation / Staging / Incremental Computing)(model_26/tf.tensor_scatter_nd_update_1/TensorScatterUpdate2) shape:[2, 512, 512], type:FLOAT32   T CC(in_top_k op does not work with int64 labels)(model_26/tf.compat.v1.transpose_19/transpose) shape:[2, 512, 512], type:FLOAT32   T CC(Does TensorFlow support temporal convolution)(model_26/tf.compat.v1.transpose_19/transpose1) shape:[2, 512, 512], type:FLOAT32   T CC(g3doc is not installed when using pip )(model_26/tf.tensor_scatter_nd_update_2/TensorScatterUpdate) shape:[2, 512, 512], type:FLOAT32   T CC(tensor flow does not support operator.__truediv__)(model_26/tf.tensor_scatter_nd_update_2/TensorScatterUpdate1) shape:[2, 512, 512], type:FLOAT32   T CC(Small typo in Deep MNIST Tutorial)(model_26/tf.compat.v1.transpose_19/transpose2) shape:[2, 512, 512], type:FLOAT32   T CC(Not a gzipped file)(model_26/tf.tensor_scatter_nd_update_2/TensorScatterUpdate2) shape:[2, 512, 512], type:FLOAT32   T CC(Tensorboard creates unecessary loops in graph)(model_26/tf.compat.v1.transpose_20/transpose) shape:[2, 512, 512], type:FLOAT32   T CC(Wrong Logistic Loss)(model_26/tf.compat.v1.transpose_20/transpose1) shape:[2, 512, 512], type:FLOAT32   T CC(Unable to restore trained models on the enfr translate model:  tensorflow.python.framework.errors.NotFoundError: Tensor name ""embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Matrix"" not found)(model_26/tf.tensor_scatter_nd_update_3/TensorScatterUpdate) shape:[2, 512, 512], type:FLOAT32   T CC(run from script only instead of compiling with bazel?)(model_26/tf.tensor_scatter_nd_update_3/TensorScatterUpdate1) shape:[2, 512, 512], type:FLOAT32   T CC(einsumlike function?)(model_26/tf.compat.v1.transpose_20/transpose2) shape:[2, 512, 512], type:FLOAT32   T CC(Split a tensor with Tensor 0D int32 as num_split argument)(model_26/tf.tensor_scatter_nd_update_3/TensorScatterUpdate2) shape:[2, 512, 512], type:FLOAT32   T CC(ImportError: /lib64/libc.so.6: version `GLIBC_2.17')(model_26/tf.compat.v1.transpose_23/transpose) shape:[2, 512, 512], type:FLOAT32   T CC(Installation error)(model_26/tf.reshape_9/Reshape) shape:[2, 262144], type:FLOAT32   T CC(Typo in TensorBoard: Visualizing Learning docs)(model_26/tf.math.top_k/TopKV2) shape:[2, 512], type:FLOAT32   T CC('state' is not defined )(model_26/tf.math.top_k/TopKV21) shape:[2, 512], type:INT32   T CC(Typo in tf.Graph.name_scope(name) docs)(model_26/tf.cast_4/Cast) shape:[2, 512], type:INT64   T CC(The icon is assymetric)(model_26/tf.reshape_10/Reshape) shape:[2, 512, 1], type:INT64   T CC(does tensorboard allow for building/editing models?)(model_26/tf.cast_5/Cast) shape:[2, 512, 1], type:FLOAT32   T CC(Dropout Loses Shape Inference Information)(model_26/tf.math.divide_1/truediv) shape:[2, 512, 2], type:FLOAT32   T CC(Incorrect matrix in beginner tutorial on www.tensorflow.org)(model_26/tf.cast_6/Cast) shape:[2, 512, 2], type:INT64   T CC(No documentation for Saver class)(Identity) shape:[2, 512, 2], type:INT64   T CC(Make Python/Numpy include paths configurable)(model_26/tf.cast_7/Cast) shape:[2, 512, 2], type:FLOAT32   T CC(Extra exp in softmax formula?)(model_26/tf.math.subtract_2/Sub) shape:[2, 512, 2], type:FLOAT32   T CC(No model directory)(model_26/tf.math.multiply_10/Mul) shape:[2, 512, 2], type:FLOAT32   T CC(when install from sources, I encounter ERROR: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed)(model_26/tf.math.divide_2/truediv) shape:[2, 512, 2], type:FLOAT32   T CC(Unrecognized option: data_dir)(model_26/tf.math.subtract_5/Sub) shape:[2, 512, 2], type:FLOAT32   T CC(ImportError: No module named copyreg)(model_26/tf.__operators__.add_7/AddV2;model_26/tf.reshape_11/Reshape) shape:[2, 512, 2], type:FLOAT32   T CC(Building from source for BSD)(model_26/tf.math.multiply_22/Mul;model_26/tf.__operators__.add_7/AddV2;model_26/tf.reshape_11/Reshape) shape:[2, 512, 2], type:FLOAT32   T CC(g3doc tutorial mnist.py tf.range missing argument)(model_26/tf.math.multiply_22/Mul;model_26/tf.__operators__.add_7/AddV2;model_26/tf.reshape_11/Reshape1) shape:[2, 1, 512, 2], type:FLOAT32   T CC(Maybe the website in other languages?)(model_26/tf.split_1/split) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Padding type definition is swapped in the documentation.)(model_26/tf.split_1/split1) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Small typo in Beginners MNIST Tutorial)(model_26/tf.math.floor_2/Floor) shape:[2, 1, 512, 1], type:FLOAT32   T CC(cumulative longer epochs)(model_26/tf.clip_by_value_4/clip_by_value/Minimum) shape:[2, 1, 512, 1], type:FLOAT32   T CC(a Makefile would be supremely helpful)(model_26/tf.clip_by_value_4/clip_by_value) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Broken link on beginner's tutorial page)(model_26/tf.__operators__.add_8/AddV2) shape:[2, 1, 512, 1], type:FLOAT32   T CC(How To doc for tensor indexing and assigning ops)(model_26/tf.clip_by_value_6/clip_by_value/Minimum) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Better error message for tf.assign)(model_26/tf.clip_by_value_6/clip_by_value) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Promote usage of docker above all other installation methods)(model_26/tf.math.subtract_12/Sub) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Using 3d Input for seq2seq Models  Word Vector Input)(model_26/tf.math.subtract_15/Sub) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Remove unnecessary null pointer checks)(model_26/tf.math.floor_3/Floor) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Generalize slicing and slice assignment ops (including gather and scatter))(model_26/tf.clip_by_value_5/clip_by_value/Minimum) shape:[2, 1, 512, 1], type:FLOAT32   T CC(confused by device placement on Amazon AWS)(model_26/tf.clip_by_value_5/clip_by_value) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Symbolic loops (like ""scan"" in Theano))(model_26/tf.__operators__.add_9/AddV2) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Yann LeCun's web page is not available  cannot download data from there is there a mirror?)(model_26/tf.clip_by_value_7/clip_by_value/Minimum) shape:[2, 1, 512, 1], type:FLOAT32   T CC(feature request: softmax target axes / multidimensional softmax)(model_26/tf.clip_by_value_7/clip_by_value) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Add gpu support for LRN)(model_26/tf.concat_5/concat) shape:[2, 1, 512, 2], type:FLOAT32   T CC(Multiple models in one session)(model_26/tf.cast_13/Cast) shape:[2, 1, 512, 2], type:INT64   T CC(tensorflow howto for sharing variable is confusing)(model_26/tf.compat.v1.gather_nd_5/BatchGatherND/concat_3) shape:[2, 1, 512, 3], type:INT64   T CC(Beam Search)(model_26/tf.compat.v1.gather_nd_5/BatchGatherND/GatherNd) shape:[2, 1, 512, 256], type:FLOAT32   T CC(Question:  Example on how can TensorFlow be used for Text classification?)(model_26/tf.concat_6/concat) shape:[2, 1, 512, 2], type:FLOAT32   T CC(MatMul Broadcasting / tensordot)(model_26/tf.cast_14/Cast) shape:[2, 1, 512, 2], type:INT64   T CC(Is pep8 compatibility necessary right now for tensorflow?)(model_26/tf.compat.v1.gather_nd_6/BatchGatherND/concat_3) shape:[2, 1, 512, 3], type:INT64   T CC(bazel run error)(model_26/tf.compat.v1.gather_nd_6/BatchGatherND/GatherNd) shape:[2, 1, 512, 256], type:FLOAT32   T CC(ImportError:No module named setuptools after exectued the step for bazelbin)(model_26/tf.concat_4/concat) shape:[2, 1, 512, 2], type:FLOAT32   T CC(how can I export androidtensorflow into my android stuido)(model_26/tf.cast_12/Cast) shape:[2, 1, 512, 2], type:INT64   T CC(tf.matrix_inverse() is slow compared to numpy.linalg.inv)(model_26/tf.compat.v1.gather_nd_4/BatchGatherND/concat_3) shape:[2, 1, 512, 3], type:INT64   T CC(Error in api_docs/images/Gather.png )(model_26/tf.compat.v1.gather_nd_4/BatchGatherND/GatherNd) shape:[2, 1, 512, 256], type:FLOAT32   T CC(extra cpus not recognized when using docker on windows 10)(model_26/tf.concat_7/concat) shape:[2, 1, 512, 2], type:FLOAT32   T CC(TensorFlow for Jetson TK1 (ARM + Cuda))(model_26/tf.cast_15/Cast) shape:[2, 1, 512, 2], type:INT64   T CC(Cannot get TensorBoard example working)(model_26/tf.compat.v1.gather_nd_7/BatchGatherND/concat_3) shape:[2, 1, 512, 3], type:INT64   T CC(`from tensorflow.g3doc...` is Broken)(model_26/tf.compat.v1.gather_nd_7/BatchGatherND/GatherNd) shape:[2, 1, 512, 256], type:FLOAT32   T CC(Compute capability < 3.5)(model_26/tf.math.subtract_13/Sub) shape:[2, 1, 512, 1], type:FLOAT32   T CC(AttributeError in Tensor)(model_26/tf.math.multiply_24/Mul) shape:[2, 1, 512, 1], type:FLOAT32   T CC(from __future__ import division gives error in word2vec_basic when dividing tensors.Just comment out ""from __future__...."")(model_26/tf.math.multiply_28/Mul) shape:[2, 1, 512, 256], type:FLOAT32   T CC(document error)(model_26/tf.math.multiply_25/Mul) shape:[2, 1, 512, 1], type:FLOAT32   T CC(cannot enable peer access from device ordinal 0 to device ordinal 1)(model_26/tf.math.multiply_29/Mul) shape:[2, 1, 512, 256], type:FLOAT32   T CC(NameError: name 'init' is not defined)(model_26/tf.math.subtract_14/Sub) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Need a way to ask users what version of tensorflow they are running)(model_26/tf.math.multiply_23/Mul) shape:[2, 1, 512, 1], type:FLOAT32   T CC(translate example is missing '' in its 'run' command)(model_26/tf.math.multiply_27/Mul) shape:[2, 1, 512, 256], type:FLOAT32   T CC(the tutorial Sequence to Sequence Models has errors)(model_26/tf.__operators__.add_10/AddV2) shape:[2, 1, 512, 256], type:FLOAT32   T CC(Getting Started variable name mismatch)(model_26/tf.__operators__.add_11/AddV2) shape:[2, 1, 512, 256], type:FLOAT32   T CC(Error when running code from seq2seq translate model)(model_26/tf.math.multiply_26/Mul) shape:[2, 1, 512, 1], type:FLOAT32   T CC(Empty input to conv2d causes floating point exception)(model_26/tf.math.multiply_30/Mul) shape:[2, 1, 512, 256], type:FLOAT32   T CC(Basic Usage  Variables: incorrect variable name)(model_26/tf.__operators__.add_12/AddV2) shape:[2, 1, 512, 256], type:FLOAT32   T CC(Mistake in matrix multiplication(http://tensorflow.org/tutorials/mnist/beginners/index.md))(model_26/tf.math.greater_2/Greater) shape:[2, 1, 512, 1], type:BOOL   T CC(Minor change for read consistency. (Highlight MD Fix))(model_26/tf.math.greater_3/Greater) shape:[2, 1, 512, 1], type:BOOL   T CC(Minor change for tutorial.)(model_26/tf.math.less_2/Less) shape:[2, 1, 512, 1], type:BOOL   T CC(Wheel for binary installation is outdated; consider automating release process)(model_26/tf.math.logical_or_5/LogicalOr) shape:[2, 1, 512, 1], type:BOOL   T CC(Example word2vec.py)(model_26/tf.math.less_3/Less) shape:[2, 1, 512, 1], type:BOOL   T CC(word2vec tutorial plot labels are incorrect)(model_26/tf.math.logical_or_6/LogicalOr) shape:[2, 1, 512, 1], type:BOOL   T CC(Examples for loop control flow ops (Enter/Leave/NextIteration))(model_26/tf.math.logical_or_7/LogicalOr) shape:[2, 1, 512, 1], type:BOOL   T CC(translate module not present in binary pip installation)(model_26/tf.where_4/SelectV2) shape:[2, 1, 512, 256], type:FLOAT32   T CC(typo in placeholder docs)(model_26/tf.compat.v1.transpose_34/transpose) shape:[2, 256, 1, 512], type:FLOAT32   T CC(Documentation: 'typo' in softmax explanation image)(model_26/tf.reshape_14/Reshape) shape:[2, 256, 512], type:FLOAT32   T CC(Outofsource builds)(model_26/tf.compat.v1.norm_3/norm/mul;model_26/tf.reshape_14/Reshape) shape:[2, 256, 1, 512], type:FLOAT32   T CC(Install tensorflow from source )(model_26/tf.compat.v1.norm_3/norm/mul;model_26/tf.reshape_14/Reshape1) shape:[2, 256, 512], type:FLOAT32   T CC(Make TensorFlow compatible with PyPy)(model_26/tf.compat.v1.norm_3/norm/Sum) shape:[2, 1, 512], type:FLOAT32   T CC(Need help, how to choose one column of a tensor)(model_26/tf.compat.v1.norm_3/norm/Sqrt) shape:[2, 1, 512], type:FLOAT32   T CC(Tensorflow on Raspberry Pi)(model_26/tf.math.maximum_1/Maximum) shape:[2, 1, 512], type:FLOAT32   T CC(CUDA_ERROR_NO_DEVICE)(model_26/tf.math.multiply_31/Mul) shape:[2, 256, 512], type:FLOAT32   T CC(Transpose convolution layer for tensorflow (was deconvolution))(model_26/tf.math.divide_3/truediv) shape:[2, 256, 512], type:FLOAT32   T CC(error in computation graph tutorials)(Identity_1) shape:[2, 512, 256], type:FLOAT32 Your model looks compatible with GPU delegate on TFLite runtime version 2.18.0. This does not guarantee that your model will work well with GPU delegate because there could still be runtime incompatibililties.                Model size:    8527712 bytes     Nondata buffer size:      35425 bytes (00.42 %)   Total data buffer size:    8492287 bytes (99.58 %)     (Zero value buffers):    1048598 bytes (12.30 %) * Buffers of TFLite model are mostly used for constant tensors.   And zero value buffers are buffers filled with zeros.   (Consider use `converter._experimental_unfold_large_splat_constant` to save the model size.)   Nondata buffers area are used to store operators, subgraphs and etc.   You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs ```","Hi,   Please take a look into this issue. Thank you.","Hi lite24, what is the ideal behavior you are expecting? I'm trying to understand is this a feature request or did you want the error behavior to behave differently?, Or some combination of both.", I am not sure why the entire delegation has to fail and why the op failure is detected much later. If an op cannot be applied shouldn't the graph just be partitioned there. Also if there is any documentation on the exact nature of tensors all ops support that would be pretty useful as well so that I can account for all of them in my model beforehand. So basically yeah it is a combination of a bug fix or a feature request (the way I understand it currently anyway)
copybara-service[bot],Allow combiner names that start with `custom`.,Allow combiner names that start with `custom`.,2025-02-28T19:41:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88356
copybara-service[bot],Integrate LLVM at llvm/llvm-project@992b451f0837,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 992b451f0837,2025-02-28T19:29:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88355
wonjeon,[mlir][tosa] Rename Tosa ReduceProd to ReduceProduct,This adjusts TF/TFL legalization code for renaming Tosa ReduceProd operator to ReduceProduct ChangeId: I3df7bd7cd95064656b01b0e694e23d0d03a97840,2025-02-28T19:00:28Z,kokoro:force-run ready to pull size:S comp:lite-tosa,closed,0,2,https://github.com/tensorflow/tensorflow/issues/88354,We've been pushing TOSA v1.0 LLVM patches to upstream. This PR includes commit(s) to keep the following operator(s) to be aligned with LLVM: [mlir][tosa] Change zero points of convolution ops to required inputs https://github.com/llvm/llvmproject/pull/127679,"Local testing successfully done: INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 165.923s, Critical Path: 104.09s INFO: 980 processes: 68 internal, 912 local. INFO: Build completed successfully, 980 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test         PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test             PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test      PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test                PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test   PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test           PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test  PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test      PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test         PASSED in 0.1s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 2.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test     PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test        PASSED in 0.1s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test   PASSED in 0.2s Executed 17 out of 17 tests: 17 tests pass."
copybara-service[bot],Rollback of se_gpu changes to allow proper aliasing.,Rollback of se_gpu changes to allow proper aliasing. Reverts f050a2f9e4cbee4bc9c5db3399ab74add8bbab4f,2025-02-28T19:00:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88353
copybara-service[bot],Deprecate use of int64,Deprecate use of int64 Reverts f050a2f9e4cbee4bc9c5db3399ab74add8bbab4f,2025-02-28T18:40:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88352
copybara-service[bot],Use `int64_t` rather than `uint64_t` for execution stream id,Use `int64_t` rather than `uint64_t` for execution stream id,2025-02-28T18:39:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88351
copybara-service[bot],[XLA:GPU] Fix RaggedAllToAllDecomposer when input and output buffers have different sizes.,"[XLA:GPU] Fix RaggedAllToAllDecomposer when input and output buffers have different sizes. We were doubling the size of the buffer to be able to use dynamicupdateslice, because by HLO semantics, if the update goes out of bound of the result, the update is not applied at all. The correct solution is to pad to `input_size + output_size`.",2025-02-28T18:21:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88350
copybara-service[bot],Internal change to fix tests,Internal change to fix tests,2025-02-28T17:46:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88349
jasonleekungfu,Could you kindly update CUDA version in official Tensorflow containers?," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.18.0, 2.19.0rc0  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Can you please update official Tensorflow Docker containers starting version 2.18.0? It is supposed to require CUDA > 12.5. However, the official containers still use 12.3 and does not work. We can technically pull the container and modify. But official containers should include the correct dependencies. Thank you!  Standalone code to reproduce the issue ```shell import tensorflow as tf tf.config.list_physical_devices() ```  Relevant log output ```shell WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1740763678.575642 2218885 gpu_device.cc:2340] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')] ```",2025-02-28T17:31:07Z,type:feature comp:gpu TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/88348,Hi! Just checking. Any updates on this issue? Thank you!
copybara-service[bot],[XLA:CPU] Add support for AOT compilation for thunk mode.,[XLA:CPU] Add support for AOT compilation for thunk mode.,2025-02-28T17:21:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88347
copybara-service[bot],Proof-of-concept: A JAX callback that operates directly on device buffers.,Proofofconcept: A JAX callback that operates directly on device buffers.,2025-02-28T16:11:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88346
copybara-service[bot],[XLA:CPU] Move xla compiled function library from tf2xla,[XLA:CPU] Move xla compiled function library from tf2xla,2025-02-28T16:05:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88345
copybara-service[bot],Integrate LLVM at llvm/llvm-project@9e2eb95c238d,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 9e2eb95c238d,2025-02-28T16:05:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88344
copybara-service[bot],"#litert Replace `OpenLib`, `CloseLib`, `ResolveLibSymbol`, `DLLInfo` with `ShareLibrary`.","litert Replace `OpenLib`, `CloseLib`, `ResolveLibSymbol`, `DLLInfo` with `ShareLibrary`.",2025-02-28T16:00:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88343
copybara-service[bot],[XLA:GPU/TMA] Adding TMA related attributes to the function arguments.,[XLA:GPU/TMA] Adding TMA related attributes to the function arguments.,2025-02-28T15:59:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88342
copybara-service[bot],[XLA:CPU] Refactor AOT compilation result outside of cpu_compiler.,[XLA:CPU] Refactor AOT compilation result outside of cpu_compiler.,2025-02-28T15:58:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88341
copybara-service[bot],[XLA:GPU] Constrain ragged-all-to-all layout.,[XLA:GPU] Constrain raggedalltoall layout. We can only support cases when ragged dimension (dim 0) is the most major dimension. This was all update are contiguous in memory. Layout of other dimensions doesn't matter.,2025-02-28T15:53:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88340
copybara-service[bot],Fix macOS builds,"Fix macOS builds  Use ://github.com/bazelbuild/bazel/pull/16619  Upgrade pthreadspool to get https://github.com/google/pthreadpool/commit/b1aee199d54003fb557076a201bcac3398af580, which is needed for Bazel 7",2025-02-28T15:36:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88339
copybara-service[bot],Fix tensorflow/lite/github/macos/ios_tests/continuous after upgrading to Bazel 7,"Fix tensorflow/lite/github/macos/ios_tests/continuous after upgrading to Bazel 7 Cause of breakage is https://github.com/bazelbuild/bazel/pull/16619, where apple toolchains are moved to apple_support See also: https://github.com/bazelbuild/apple_support/tree/1.15.1?tab=readmeovfiletoolchainsetup",2025-02-28T15:17:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88338
copybara-service[bot],Reverts 0dc9eb6a2e1f0a9c86442a9c6992856223d46eee,Reverts 0dc9eb6a2e1f0a9c86442a9c6992856223d46eee,2025-02-28T15:12:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88337
copybara-service[bot],Reverts c0562df1a2e3e0c26be622a75bb3682fbe1859ed,Reverts c0562df1a2e3e0c26be622a75bb3682fbe1859ed,2025-02-28T15:11:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88336
copybara-service[bot],[XLA:GPU] Add support for nested fusions in symbolic tiling.,[XLA:GPU] Add support for nested fusions in symbolic tiling.,2025-02-28T14:48:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88335
copybara-service[bot],[XLA:GPU] Move implementation details to anonymous namespace.,[XLA:GPU] Move implementation details to anonymous namespace.,2025-02-28T14:38:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88334
copybara-service[bot],[XLA:GPU] Cover RaggedAllToAllDecomposer in collective e2e tests.,[XLA:GPU] Cover RaggedAllToAllDecomposer in collective e2e tests.,2025-02-28T13:49:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88333
copybara-service[bot],Make the sequence length dimension dynamic for reshapes in SDPA op.,Make the sequence length dimension dynamic for reshapes in SDPA op. XNNPack can infer this dimension.,2025-02-28T13:33:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88332
copybara-service[bot],PR #22788: Implement GetCompiledMemoryStats for GPU AOT executables,"PR CC(TFTRT User provided INT8 quantization scales): Implement GetCompiledMemoryStats for GPU AOT executables Imported from GitHub PR https://github.com/openxla/xla/pull/22788 This implements `GetCompiledMemoryStats` for aheadoftime compiled executables. With this patch, one can estimate memory consumption of a JAX function even without access to a GPU. Unfortunately, the patch duplicates code between unloaded and loaded GPU executables and between `GpuThunkAotCompilationResult::GetBufferAssignment()`  and `Compiler::BufferSizeBytesFunction()`+`GpuCompiler::ShapeSizeBytesFunction()`. This could be perhaps improved by exposing the relevant compiler code as static methods, but that does not seem worth the extra complexity. The patch also threads `pointer_size` from `GpuCompiler` to `GpuThunkAotCompilationResult` so that we can get buffer allocation sizes without direct access to the compiler. Another option would be embedding `pointer_size` within `CompilationResultProto`. Also note that this still does not set `generated_code_size_in_bytes` correctly  that would require duplicating some code from `GpuExecutable::SizeOfGeneratedCodeInBytes()`. Copybara import of the project:  39015570fab3e32bae2a54eb4ae7bf428a3c0e3b by Jaroslav Sevcik : Implement GetCompiledMemoryStats for GPU AOT executables  a3e60c7e7ae724d89ae670c14eb0cee8226b66d4 by Jaroslav Sevcik : Check more stats Merging this change closes CC(TFTRT User provided INT8 quantization scales) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22788 from jarosevcik:memstatsforunloadedexecutable a3e60c7e7ae724d89ae670c14eb0cee8226b66d4",2025-02-28T13:25:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88331
ghost,I'm a spammer," Previously, the AnyTensor() function directly constructed tensors based on fuzzerprovided vector sizes. A very large or maliciously crafted size could lead to outofmemory errors or other shaperelated crashes. We've introduced explicit shape validation using shape.IsValid() These improvements were implemented to prevent crashes due to invalid tensor shapes etc.  We implement a std::mutex called init_mutex_ to protect the initialization block inside InitIfNeeded(). This ensures that only one thread can initialize the session at a time, this implementation protects against concurrent access issues during initialization.  The FuzzImpl method can throw exceptions if the data provided is malformed or leads to invalid tensor operations. We implemented a try...catch block around the call to FuzzImpl inside the Fuzz() method. This catches any exceptions thrown by FuzzImpl and logs them as errors, allowing the fuzzing process to continue even if individual fuzzing iterations encounter exceptions.",2025-02-28T13:24:34Z,size:M invalid,closed,1,1,https://github.com/tensorflow/tensorflow/issues/88330,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],#litert Add an RAII wrapper around shared library management.,"litert Add an RAII wrapper around shared library management. This aims to replace the existing use of `OpenLib`, `CloseLib`, `ResolveLibSymbol`, `DLLInfo`, `dlsym`, `dlopen` while providing a harder to misuse, more idiomatic C++ interface.",2025-02-28T13:03:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88329
copybara-service[bot],Example QNN accelerator options link.,Example QNN accelerator options link. This is distributed as a part of LiteRt (or vendor specific SDK package). **The header needs to be public.** The implementation can be distributed either as the source or as a precompiled binary.  `set_options_example.cc` is an example of C API use for the high level   bindings. End users are not expected to use this API.,2025-02-28T12:54:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88328
copybara-service[bot],[XLA:GPU] Add the triton and cublas implementation for the BF16_X9 dot algorithm.,[XLA:GPU] Add the triton and cublas implementation for the BF16_X9 dot algorithm. The approach is identical to X3 and X6. The difference between X6 and X9 is that for X9 we also do the following dots: low @ low + low @ med + med @ low the rest dots are the same as for X6,2025-02-28T12:16:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88327
copybara-service[bot],Integrate LLVM at llvm/llvm-project@39c6c8be2f3f,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 39c6c8be2f3f,2025-02-28T11:46:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88326
copybara-service[bot],[XLA:CPU] NanoRt exposes ExecutionOptions.,[XLA:CPU] NanoRt exposes ExecutionOptions. This can be used to set the thread pool device. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88412 from tensorflow:dependabot/docker/tensorflow/tools/tf_sig_build_dockerfiles/ubuntued1544e 4b6ba37c2667bdfded35b1774ed8ee438bc5fb25,2025-02-28T11:34:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88325
copybara-service[bot],Rename input format flag option from `input_snapshot_proto_binary` to `unoptimized_snapshot_proto_binary` before it's too late!,"Rename input format flag option from `input_snapshot_proto_binary` to `unoptimized_snapshot_proto_binary` before it's too late! Make it consistent and unified with all other places, it used to be called input snapshots on early stages of development.",2025-02-28T11:14:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88324
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T10:00:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88323
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T09:59:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88322
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T09:57:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88321
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T09:56:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88320
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T09:52:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88319
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T09:51:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88318
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T09:51:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88317
copybara-service[bot],[XLA:GPU] Simplify bf16_x3 and bf16_x6 matmuls code in fusion emitter.,[XLA:GPU] Simplify bf16_x3 and bf16_x6 matmuls code in fusion emitter. NOOP refactoring. This implementation is shorter and much easier to follow.,2025-02-28T09:42:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88316
copybara-service[bot],Plumb layout through the creation of IFRT Arrays (roll-forward with fix).,Plumb layout through the creation of IFRT Arrays (rollforward with fix). Reverts c0e21e1a1964c7dcabeb821c2c023b42050b1eb6,2025-02-28T06:06:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88315
Venkat6871,Fix typos in documentation strings,"Hi, Team I observed few typos in the documentation strings and I have fixed those typos so please do the needful. Thank you.",2025-02-28T05:56:19Z,ready to pull size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88314
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T05:13:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88313
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T05:09:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88312
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T05:07:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88311
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T05:06:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88310
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T05:03:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88309
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T05:01:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88308
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:57:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88307
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:56:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88306
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:56:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88305
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:55:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88304
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:55:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88303
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:53:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88302
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:53:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88301
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:52:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88300
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:52:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88299
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:52:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88298
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:52:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88297
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:49:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88296
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:49:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88295
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:47:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88294
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:44:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88293
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:42:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88292
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-28T04:41:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88291
copybara-service[bot],Templatize C++ options parsing and move to public api.,Templatize C++ options parsing and move to public api.,2025-02-28T04:04:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88290
copybara-service[bot],Fix control dep crash in DoubleBufferLoopUnrolling.,"Fix control dep crash in DoubleBufferLoopUnrolling. Some code handling control dependencies initialized a vector with N null pointers, then added N nonnull pointers, when it intended to just add N nonnull pointers.",2025-02-28T03:49:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88289
amdjebb,error handling in list_devices() not present," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I noticed that there is a lack of error handling in the test_utils.py or device_lib.py to handle improper device query failures.  If _pywrap_device_lib.list_devices() is called when GPU drivers are not updated/missing, it could lead to Tensorflow crashing out without handling the error.  This can also happen for undefined device_lib in gpu_device_name() or if no GPU is detected.   If viable, I would like to be assigned this issue to work on.  Standalone code to reproduce the issue ```shell import tensorflow as tf from tensorflow.python.client import device_lib def list_local_devices():     """"""List the available devices""""""     from tensorflow.core.protobuf import device_attributes_pb2     def _convert(pb_str):         """"""Convert serialized device attribute protobuf to a readable format.""""""         m = device_attributes_pb2.DeviceAttributes()         m.ParseFromString(pb_str)         return m     return [_convert(s) for s in tf.compat.v1.Session().list_devices()]  Might return none def gpu_device_name():     """"""Returns the name of a GPU device""""""     for x in device_lib.list_local_devices():   Might return none         if x.device_type == ""GPU"":             return tf.compat.as_str(x.name)   Might return none     return """" def test_gpu():     """"""Test the GPU functions""""""     devices = list_local_devices()   Where problem might occur     for device in devices:         print(f""Device found: {device}"")     gpu_name = gpu_device_name()   Where problem might occur     print(f""GPU Name: {gpu_name}"") if __name__ == ""__main__"":     test_gpu() ```  Relevant log output ```shell ImportError: cannot import name 'device_attributes_pb2' from 'tensorflow.core.protobuf' ```",2025-02-28T03:11:13Z,type:bug TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/88288,I was able to reproduce the same issue using TensorFlow 2.18 and the nightly version. Please find the gist here for reference. Thank you!
copybara-service[bot],[xla:hlo] Overwirte Literal::Get/GetLinear to avoid vtable dispatch overheads,[xla:hlo] Overwirte Literal::Get/GetLinear to avoid vtable dispatch overheads ``` name                           old cpu/op   new cpu/op   delta BM_UnaryOp/64/process_time      240µs ± 9%   234µs ± 6%   2.33%  (p=0.000 n=40+39) BM_UnaryOp/128/process_time     500µs ± 4%   468µs ± 7%   6.45%  (p=0.000 n=33+39) BM_UnaryOp/512/process_time    4.92ms ± 4%  4.30ms ± 7%  12.48%  (p=0.000 n=40+40) BM_UnaryOp/1024/process_time   18.8ms ± 8%  16.0ms ± 4%  14.72%  (p=0.000 n=40+40) BM_BinaryOp/64/process_time     237µs ± 7%   222µs ± 9%   6.31%  (p=0.000 n=40+40) BM_BinaryOp/128/process_time    460µs ± 8%   415µs ± 8%   9.70%  (p=0.000 n=38+36) BM_BinaryOp/512/process_time   4.72ms ± 6%  3.45ms ±12%  26.93%  (p=0.000 n=40+35) BM_BinaryOp/1024/process_time  18.0ms ± 5%  12.8ms ±11%  29.02%  (p=0.000 n=40+34) name                           old time/op          new time/op          delta BM_UnaryOp/64/process_time     55.2µs ±12%          54.8µs ± 8%     ~     (p=0.109 n=40+40) BM_UnaryOp/128/process_time    85.8µs ± 6%          83.0µs ±11%   3.29%  (p=0.000 n=35+40) BM_UnaryOp/512/process_time     505µs ± 3%           460µs ± 6%   8.99%  (p=0.000 n=40+40) BM_UnaryOp/1024/process_time   1.85ms ±11%          1.64ms ± 5%  11.34%  (p=0.000 n=40+40) BM_BinaryOp/64/process_time    54.7µs ± 8%          52.8µs ±10%   3.47%  (p=0.000 n=40+40) BM_BinaryOp/128/process_time   77.1µs ±12%          75.5µs ± 8%     ~     (p=0.063 n=40+36) BM_BinaryOp/512/process_time    479µs ± 6%           396µs ± 6%  17.31%  (p=0.000 n=40+36) BM_BinaryOp/1024/process_time  1.77ms ± 8%          1.45ms ± 9%  18.11%  (p=0.000 n=40+40) ```,2025-02-28T02:57:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88287
copybara-service[bot],[xla:hlo] Templetize HloEvaluator on a hot path to remove std::function overheads,[xla:hlo] Templetize HloEvaluator on a hot path to remove std::function overheads ``` name                           old cpu/op   new cpu/op   delta BM_UnaryOp/64/process_time      244µs ±11%   230µs ±10%   5.76%  (p=0.000 n=40+40) BM_UnaryOp/128/process_time     538µs ±13%   491µs ±11%   8.84%  (p=0.000 n=40+40) BM_UnaryOp/512/process_time    5.93ms ± 6%  5.08ms ± 9%  14.33%  (p=0.000 n=39+40) BM_UnaryOp/1024/process_time   24.6ms ± 6%  19.4ms ± 9%  21.21%  (p=0.000 n=32+40) BM_BinaryOp/64/process_time     236µs ±10%   233µs ±12%     ~     (p=0.187 n=40+40) BM_BinaryOp/128/process_time    487µs ±11%   457µs ±11%   6.31%  (p=0.000 n=39+38) BM_BinaryOp/512/process_time   5.28ms ± 6%  4.66ms ± 9%  11.65%  (p=0.000 n=38+40) BM_BinaryOp/1024/process_time  21.4ms ± 9%  18.3ms ± 8%  14.32%  (p=0.000 n=38+40) name                           old time/op          new time/op          delta BM_UnaryOp/64/process_time     54.7µs ±15%          52.9µs ±12%     ~     (p=0.077 n=40+40) BM_UnaryOp/128/process_time    83.8µs ±15%          81.8µs ±16%     ~     (p=0.165 n=40+40) BM_UnaryOp/512/process_time     584µs ± 7%           516µs ±11%  11.60%  (p=0.000 n=39+40) BM_UnaryOp/1024/process_time   2.41ms ± 8%          1.93ms ±12%  19.63%  (p=0.000 n=36+40) BM_BinaryOp/64/process_time    52.7µs ±13%          53.3µs ±14%     ~     (p=0.198 n=39+40) BM_BinaryOp/128/process_time   77.9µs ±17%          77.7µs ±15%     ~     (p=0.874 n=39+39) BM_BinaryOp/512/process_time    524µs ± 9%           473µs ±12%   9.73%  (p=0.000 n=40+40) BM_BinaryOp/1024/process_time  2.14ms ±15%          1.84ms ±13%  14.03%  (p=0.000 n=39+40) ```,2025-02-28T02:49:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88286
copybara-service[bot],[xla] HloEvaluator: Use PopulateLinear for Unary/Binary/Ternary and Compare ops,[xla] HloEvaluator: Use PopulateLinear for Unary/Binary/Ternary and Compare ops ``` name                           old cpu/op   new cpu/op   delta BM_UnaryOp/64/process_time      314µs ± 7%   261µs ± 8%  16.71%  (p=0.000 n=40+40) BM_UnaryOp/128/process_time     759µs ± 5%   566µs ±10%  25.46%  (p=0.000 n=40+40) BM_UnaryOp/512/process_time    8.60ms ± 3%  5.80ms ± 3%  32.55%  (p=0.000 n=40+39) BM_UnaryOp/1024/process_time   33.5ms ± 6%  22.1ms ± 2%  34.06%  (p=0.000 n=40+35) BM_BinaryOp/64/process_time     337µs ± 5%   258µs ±10%  23.46%  (p=0.000 n=38+39) BM_BinaryOp/128/process_time    834µs ± 3%   531µs ± 8%  36.35%  (p=0.000 n=40+40) BM_BinaryOp/512/process_time   10.0ms ± 5%   5.6ms ± 7%  44.19%  (p=0.000 n=39+40) BM_BinaryOp/1024/process_time  37.6ms ±11%  19.6ms ± 4%  47.98%  (p=0.000 n=40+39) name                           old time/op          new time/op          delta BM_UnaryOp/64/process_time     82.5µs ±11%          59.8µs ±11%  27.52%  (p=0.000 n=39+39) BM_UnaryOp/128/process_time     117µs ±10%            99µs ±16%  15.61%  (p=0.000 n=40+40) BM_UnaryOp/512/process_time     961µs ± 6%           678µs ± 8%  29.48%  (p=0.000 n=40+39) BM_UnaryOp/1024/process_time   3.77ms ± 9%          2.73ms ±10%  27.71%  (p=0.000 n=39+39) BM_BinaryOp/64/process_time    89.7µs ± 7%          61.0µs ± 8%  31.99%  (p=0.000 n=40+38) BM_BinaryOp/128/process_time    133µs ± 8%           104µs ±15%  21.49%  (p=0.000 n=40+40) BM_BinaryOp/512/process_time   1.27ms ±16%          0.83ms ±20%  34.30%  (p=0.000 n=40+40) BM_BinaryOp/1024/process_time  4.85ms ±15%          3.17ms ± 9%  34.56%  (p=0.000 n=40+35) ```,2025-02-28T02:48:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88285
copybara-service[bot],[xla] Literal: cache buffer pointer to avoid branches on a hot path,[xla] Literal: cache buffer pointer to avoid branches on a hot path ``` name                           old cpu/op   new cpu/op   delta BM_UnaryOp/64/process_time      257µs ± 9%   251µs ±10%  2.09%  (p=0.015 n=40+39) BM_UnaryOp/128/process_time     573µs ±12%   560µs ±11%  2.28%  (p=0.025 n=40+40) BM_UnaryOp/512/process_time    6.22ms ± 9%  6.15ms ± 9%    ~     (p=0.244 n=40+40) BM_UnaryOp/1024/process_time   26.0ms ± 3%  24.9ms ±11%  4.16%  (p=0.000 n=31+40) BM_BinaryOp/64/process_time     266µs ±11%   258µs ± 9%  2.79%  (p=0.003 n=40+39) BM_BinaryOp/128/process_time    551µs ± 6%   532µs ± 8%  3.47%  (p=0.000 n=38+36) BM_BinaryOp/512/process_time   6.00ms ± 8%  5.70ms ±12%  5.05%  (p=0.000 n=38+39) BM_BinaryOp/1024/process_time  23.4ms ± 6%  21.9ms ± 8%  6.36%  (p=0.000 n=31+40) name                           old time/op          new time/op          delta BM_UnaryOp/64/process_time     58.9µs ±12%          57.6µs ±11%   2.31%  (p=0.023 n=39+39) BM_UnaryOp/128/process_time    96.9µs ±15%          95.8µs ±19%     ~     (p=0.575 n=40+40) BM_UnaryOp/512/process_time     722µs ±13%           717µs ± 9%     ~     (p=0.409 n=40+39) BM_UnaryOp/1024/process_time   3.23ms ±11%          3.18ms ±19%     ~     (p=0.539 n=32+40) BM_BinaryOp/64/process_time    62.7µs ±11%          60.8µs ± 9%   3.00%  (p=0.004 n=39+38) BM_BinaryOp/128/process_time    109µs ±17%           105µs ±20%   3.58%  (p=0.002 n=40+39) BM_BinaryOp/512/process_time    894µs ±20%           854µs ±19%   4.46%  (p=0.003 n=37+38) BM_BinaryOp/1024/process_time  3.87ms ±22%          3.72ms ±18%   3.90%  (p=0.042 n=40+39) ``` Benchmark numbers taken without benchmark changes in this CL. Reverts f050a2f9e4cbee4bc9c5db3399ab74add8bbab4f,2025-02-28T02:48:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88284
copybara-service[bot],"run_hlo_module, hlo-opt tools : Identify it is a MLIR input based on input filename.","run_hlo_module, hloopt tools : Identify it is a MLIR input based on input filename. fixes issue raised at https://github.com/openxla/xla/pull/22827",2025-02-28T02:04:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88283
copybara-service[bot],Add an option for TfAllocatorAdapter to disallow asynchronous_deallocation,Add an option for TfAllocatorAdapter to disallow asynchronous_deallocation,2025-02-28T02:02:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88282
copybara-service[bot],Add output_shape() to the xla::Executable base class.,Add output_shape() to the xla::Executable base class.,2025-02-28T01:55:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88281
copybara-service[bot],Update test config.,Update test config.,2025-02-28T01:36:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88280
copybara-service[bot],Create a minimum stub for tfrt_gpu_client,Create a minimum stub for tfrt_gpu_client,2025-02-28T01:20:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88279
copybara-service[bot],Remove redundant CUDA includes from crosstool toolchain.,Remove redundant CUDA includes from crosstool toolchain.,2025-02-28T00:57:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88278
copybara-service[bot],Experimentally remove argmaxpool and unpooling,Experimentally remove argmaxpool and unpooling,2025-02-27T23:54:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88277
copybara-service[bot],The `topology` field in `RunEnvironment` is deleted and there is no read reference to this field,The `topology` field in `RunEnvironment` is deleted and there is no read reference to this field,2025-02-27T23:38:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88276
copybara-service[bot],Make Pathways IFRT client get GPU topology as well.,Make Pathways IFRT client get GPU topology as well.,2025-02-27T23:29:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88275
copybara-service[bot],Add `--verbose_failures` option for building the wheel,Add `verbose_failures` option for building the wheel,2025-02-27T23:25:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88274
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-27T23:16:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88273
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-27T23:10:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88272
copybara-service[bot],Internal change to fix tests,Internal change to fix tests,2025-02-27T22:50:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88271
copybara-service[bot],Move TensorFlow GPU CI to the single GPU pool,Move TensorFlow GPU CI to the single GPU pool,2025-02-27T22:27:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88270
copybara-service[bot],"PR #23181: Annotate loops: start, step, induction variable.","PR CC(how to get the Projective transform matrix ,which will be used to tf.contrib.image.transform??): Annotate loops: start, step, induction variable. Imported from GitHub PR https://github.com/openxla/xla/pull/23181 Currently, we annotate while loops with a known trip count accordingly. This PR adds the start, step and induction variable so it's easy to see when a while loop is actually a for loop. For the larger context see https://github.com/openxla/xla/compare/main...jreiffers:xla:memcpy and the companion document https://docs.google.com/document/d/1E2_Jt_Dw4VbPXPVktNWhtsDEtIpNkurCMGGyvMV4JA/edit?tab=t.0. Copybara import of the project:  a5f8f4e7e43cb660c7198db85e923bfc9411c555 by Johannes Reifferscheid : Annotate loops: start, step, induction variable. Currently, we annotate while loops with a known trip count accordingly. This PR adds the start, step and induction variable so it's easy to see when a while loop is actually a for loop. For the larger context see https://github.com/openxla/xla/compare/main...jreiffers:xla:memcpy and the companion document https://docs.google.com/document/d/1E2_Jt_Dw4VbPXPVktNWhtsDEtIpNkurCMGGyvMV4JA/edit?tab=t.0. Merging this change closes CC(how to get the Projective transform matrix ,which will be used to tf.contrib.image.transform??) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23181 from jreiffers:whileannotator a5f8f4e7e43cb660c7198db85e923bfc9411c555",2025-02-27T22:21:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88269
copybara-service[bot],Disable `//xla/python/ifrt_proxy/integration_tests:executable_impl_test_tfrt_cpu` on ARM64,Disable `//xla/python/ifrt_proxy/integration_tests:executable_impl_test_tfrt_cpu` on ARM64 This test occasionally times out on ARM builds.,2025-02-27T21:30:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88268
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-27T21:18:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88267
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-27T21:17:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88266
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-27T21:13:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88265
copybara-service[bot],Create build rules for building wheels,Create build rules for building wheels,2025-02-27T20:51:03Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/88264,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-27T20:50:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88263
copybara-service[bot],Internal change only.,Internal change only.,2025-02-27T20:44:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88262
copybara-service[bot],[XLA:GPU] Remove GetAsyncStreamKind override,[XLA:GPU] Remove GetAsyncStreamKind override The override is a copy of the base implementation.,2025-02-27T20:25:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88261
copybara-service[bot],Fix tenstorrent doc link,Fix tenstorrent doc link,2025-02-27T20:07:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88260
copybara-service[bot],Rollback of Bazel updgrade,Rollback of Bazel updgrade Reverts 5c289f5ba22711a296a216100cf9816c6077d85d,2025-02-27T19:58:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88259
copybara-service[bot],Merge _check_mesh_resource_axis and _check_axis_type_consistency into 1 function.,Merge _check_mesh_resource_axis and _check_axis_type_consistency into 1 function.,2025-02-27T19:55:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88258
copybara-service[bot],Fix internal build breakage,Fix internal build breakage FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/87246 from tensorflow:gaikwadrahul8patch2 2887f4e1474e753ee455157e88fc7872deec4c54,2025-02-27T19:52:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88257
copybara-service[bot],[transfer-lib]: Fix bug where ids were getting reset.,[transferlib]: Fix bug where ids were getting reset.,2025-02-27T19:49:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88256
Khushisawalkar,"git commit -m ""Correct zstandard version format in requirements.txt (…","…use == instead of =)""",2025-02-27T19:41:37Z,ready to pull size:XS,open,0,1,https://github.com/tensorflow/tensorflow/issues/88255,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
calcuttj,Error regarding nodes colocated with other unknown nodes, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.20  Custom code No  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Can't load model stored in pb file get an error regarding colocated nodes. This happens for some models but not others. Bad file: https://www.dropbox.com/scl/fi/4pmvsihcv4f5eak7bxw5g/cnn_emtrkmichl_pitch_5_wire_48_drift_48_down_6_mean_notes_protoduneBeamAndCosmicsMCC11.pb?rlkey=dj28ktmqmp2kvi9idrnmq0j3e&st=tqft1tdy&dl=0 Good file: https://www.dropbox.com/scl/fi/1q8ace49r2zsbputg3b7q/dune_cvn_hd_2x6_anu_2023_tf26_v03_00_00.pb?rlkey=p3cx01gc0aurs6ok6j8gs1adn&st=2a2up2si&dl=0  Standalone code to reproduce the issue ```shell https://www.dropbox.com/scl/fi/35nt291vp8fnb40is58s8/colocate_error.ipynb?rlkey=2m0tr7tq57lolin8eh61g6ffq&st=x42ktldf&dl=0 ```  Relevant log output ```shell ValueError: Node 'em_trk_none_netout/kernel/read' expects to be colocated with unknown node 'em_trk_none_out/kernel' ```,2025-02-27T19:40:11Z,stat:awaiting response type:bug stale TF 2.18,closed,0,10,https://github.com/tensorflow/tensorflow/issues/88254,"Hi **** , Apologies for the delay, and thanks for raising your concern here. Could you please provide a Colab gist to reproduce the issue? The link you shared is inaccessible to me. Thank you!",My apologies. Here's a colab link https://colab.research.google.com/drive/1FM_B4imfuYHUTlDw1252vnMDaIc3WJpF?usp=sharing please request access,"Hi ****, Could you please provide access to the shared Colab? The link you shared is currently inaccessible to me. To ensure I can access it, please create a Colab gist instead. I am attaching a screenshot for your reference on how to create and share a Colab gist. Thank you!","Apologies, I was unfamiliar with gist. Here's a link to one that should work if you have those files https://gist.github.com/calcuttj/df4c081421e336842fd0f4ffa2283c82 If you're unable to retrieve the input files in the original post, please let me know how I can best provide them",Were you able to use the above gist?,Can I please have an update on this?,"Hi  , Apologies for the delay, and thanks for your patience. I tried running your code on Colab using TensorFlow 2.19.0, but I encountered the following error: ``` NotFoundError: dune_cvn_hd_2x6_anu_2023_tf26_v03_00_00.pb; No such file or directory ``` It seems that the required file is missing or inaccessible. I attempted to download the attached files, but I don't have access to them. Could you please provide access so I can investigate further? Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Bring back precision mode pick logic because f16 is not supported on some V68 SoCs.,Bring back precision mode pick logic because f16 is not supported on some V68 SoCs.,2025-02-27T19:35:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88253
copybara-service[bot],[XLA] Make tensorflow/core/profiler visible to tensorflow/compiler/xla/tools.,[XLA] Make tensorflow/core/profiler visible to tensorflow/compiler/xla/tools. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/87246 from tensorflow:gaikwadrahul8patch2 2887f4e1474e753ee455157e88fc7872deec4c54,2025-02-27T19:31:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88252
copybara-service[bot],Add a test for rewriting BF16 convolutions to OneDNN; remove an `#ifdef`.,"Add a test for rewriting BF16 convolutions to OneDNN; remove an `ifdef`. Allow tests to cover a platformdependent behavior detail regardless of build configuration. In order to support this (and also to improve general code health), replace an `ifdef` with a runtime `if` checking a new member of `AlgebraicSimplifierOptions`.",2025-02-27T19:29:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88251
copybara-service[bot],PR #23019: Bump actions/upload-artifact from 4.6.0 to 4.6.1,"PR CC(`tf.layers.Conv2D`'s padding doesn't allow variable reuse in some cases): Bump actions/uploadartifact from 4.6.0 to 4.6.1 Imported from GitHub PR https://github.com/openxla/xla/pull/23019 Bumps actions/uploadartifact from 4.6.0 to 4.6.1.  Release notes Sourced from actions/uploadartifact's releases.  v4.6.1 What's Changed  Update to use artifact 2.2.2 package by @​yacaovsnc in actions/uploadartifact CC(PoolAlloc: Remove div by zero, demote WARN>INFO)  Full Changelog: https://github.com/actions/uploadartifact/compare/v4...v4.6.1    Commits  4cec3d8 Merge pull request  CC(PoolAlloc: Remove div by zero, demote WARN>INFO) from actions/yacaovsnc/artifact_2.2.2 e9fad96 license cache update for artifact b26fd06 Update to use artifact 2.2.2 package See full diff in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)  Copybara import of the project:  7cb47206e44222d93a2c1efc5525c31190e046b1 by dependabot[bot] : Bump actions/uploadartifact from 4.6.0 to 4.6.1 Bumps actions/uploadartifact from 4.6.0 to 4.6.1.  Release notes  Commits  updateddependencies:  dependencyname: actions/uploadartifact   dependencytype: direct:production   updatetype: versionupdate:semverpatch ... Signedoffby: dependabot[bot]  Merging this change closes CC(`tf.layers.Conv2D`'s padding doesn't allow variable reuse in some cases) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23019 from openxla:dependabot/github_actions/actions/uploadartifact4.6.1 7cb47206e44222d93a2c1efc5525c31190e046b1",2025-02-27T19:24:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88250
copybara-service[bot],[XLA] Remove extraneous `gemma2_2b_keras_jax.hlo` argument in wget.,[XLA] Remove extraneous `gemma2_2b_keras_jax.hlo` argument in wget.,2025-02-27T19:18:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88249
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f8cc509b69cc,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f8cc509b69cc FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/87655 from tensorflow:tilakrayalpatch2 a10923d1dea2e45aed2a8cd26670e90cf981bcee,2025-02-27T19:08:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88248
copybara-service[bot],Simplify multi-process tests calls,Simplify multiprocess tests calls,2025-02-27T19:03:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88247
copybara-service[bot],Use string_view for version strings.,Use string_view for version strings.,2025-02-27T19:01:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88246
copybara-service[bot],"Remove `nogcp` config, remove warnings associated with the `linux` config as XLA has its own warnings.bazelrc","Remove `nogcp` config, remove warnings associated with the `linux` config as XLA has its own warnings.bazelrc FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/87655 from tensorflow:tilakrayalpatch2 a10923d1dea2e45aed2a8cd26670e90cf981bcee",2025-02-27T18:59:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88245
copybara-service[bot],[XLA:GPU] Run annotated collectives on dedicated P2P stream,[XLA:GPU] Run annotated collectives on dedicated P2P stream This is tested extensively through the pipeline parallelism tests,2025-02-27T18:45:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88244
copybara-service[bot],[XLA:GPU] Move collective attributes and constants together in collective utils.,[XLA:GPU] Move collective attributes and constants together in collective utils.,2025-02-27T18:44:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88243
copybara-service[bot],[XLA:CPU] NanoRt is visible from tf2xla and supports inputs without a specified type.,[XLA:CPU] NanoRt is visible from tf2xla and supports inputs without a specified type.,2025-02-27T18:24:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88242
copybara-service[bot],[StableHLO] Check for FileLineColLoc in location as part of lowering to older versions,[StableHLO] Check for FileLineColLoc in location as part of lowering to older versions,2025-02-27T18:23:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88241
copybara-service[bot],[XLA:CPU] Factor out constant allocation utilities from cpu compiler.,"[XLA:CPU] Factor out constant allocation utilities from cpu compiler. Future plans are to remove AOT compilation results from cpu compiler, this will enable both modules to share the code.",2025-02-27T18:21:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88240
copybara-service[bot],[XLA:CPU] Add consume function library to CPU executable,[XLA:CPU] Add consume function library to CPU executable Useful when we the CPU executable will be destroyed but we want to preserve the function library (e.x. for exporting).,2025-02-27T18:21:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88239
copybara-service[bot],Add accessor function for the TensorBuffer size.,Add accessor function for the TensorBuffer size.,2025-02-27T17:45:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88238
copybara-service[bot],[XLA] Add logic to calculate and print peak device memory Usage.,[XLA] Add logic to calculate and print peak device memory Usage. Print the peak device memory usage as a result of the tool. This is used in the future to evaluate the memory performance on GPU.,2025-02-27T17:44:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88237
copybara-service[bot],[xla:cpu] fix name of dumped object file in thunkless mode,"[xla:cpu] fix name of dumped object file in thunkless mode While at it, remove code duplication.",2025-02-27T16:48:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88236
copybara-service[bot],[xla] HloEvaluator: Add benchmarks for Unary and Binary op evaluation,[xla] HloEvaluator: Add benchmarks for Unary and Binary op evaluation,2025-02-27T16:38:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88235
copybara-service[bot],[XLA:CPU][tfcompile] Function library that can link functions from a given symbol map.,[XLA:CPU][tfcompile] Function library that can link functions from a given symbol map. This is used will be used for tfcompile to link against functions defined in object files.,2025-02-27T16:37:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88234
copybara-service[bot],#sdy Remove the handling of SPMDFullToShardShape and SPMDShardToFullShape.,sdy Remove the handling of SPMDFullToShardShape and SPMDShardToFullShape.,2025-02-27T16:08:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88233
copybara-service[bot],PR #23165: [XLA:GPU] fix case command buffer operator data corruption issue when index is bool type,"PR CC(TensorRT engine binding error): [XLA:GPU] fix case command buffer operator data corruption issue when index is bool type Imported from GitHub PR https://github.com/openxla/xla/pull/23165 XLA misinterprets that the first branch in conditional_thunk's branch vector is true branch, while the 2nd branch is false branch.  Copybara import of the project:  db66863cf131268af020c2347fb7f90c142163a6 by Shawn Wang : fix case command buffer operator fault when index is bool type Merging this change closes CC(TensorRT engine binding error) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23165 from shawnwang18:shawnw/conditional_test db66863cf131268af020c2347fb7f90c142163a6",2025-02-27T15:41:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88232
copybara-service[bot],[xla:cpu] Remove ENABLE_ONEDNN_V3 checks.,[xla:cpu] Remove ENABLE_ONEDNN_V3 checks. We only build with oneDNN version 3 now. (Not supporting v2 anymore.),2025-02-27T15:09:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88231
copybara-service[bot],Integrate LLVM at llvm/llvm-project@0e3ba99ad65f,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 0e3ba99ad65f,2025-02-27T14:50:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88230
copybara-service[bot],[XLA:CPU] Replace colons with underscores in ConvertToCName.,[XLA:CPU] Replace colons with underscores in ConvertToCName.,2025-02-27T14:14:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88229
tilakrayal,Fixed the typos in flatbuffer_translate.cc,,2025-02-27T14:11:15Z,comp:lite ready to pull size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88228
copybara-service[bot],"[XLA:GPU/TMA] Add TMA optionality and lowering to triton_xla tile, extract, and insert.","[XLA:GPU/TMA] Add TMA optionality and lowering to triton_xla tile, extract, and insert.",2025-02-27T13:43:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88227
Unknownuserfrommars,Tensorflow Website Out-of-date," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2  Custom code Yes  OS platform and distribution Windows 11 24H2  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In tensorflow.org/install, everything is just so out of date. It started when i'm reading the chinese version of the install page (https://www.tensorflow.org/install?hl=zhcn): which is:   我们在以下 64 位系统上测试过 TensorFlow 并且这些系统支持 TensorFlow： Python 3.6–3.9     See the problem? it said: TensorFlow is tested and supported on the following 64bit systems: Python 3.63.9 But 3.6 is EOS like millions of years ago, so i scrolled down and saw the page last updated date: 20210825. But this may be that the people are lazy to translate them to chinese, so i changed my language to english. Better but it said TF supports Python 3.83.11, which is also not true. The date last updated for this is 20230324, almost two years ago. So, my question is: why is this so out of date? Like this is supposed to be the official website for tensorflow! In the main page (tensorflow.org), it said: TF 2.18 released (which is true), but the API docs version is actually v2.16.1, with a datelastupdated of 20240930. This is just so annoying. When people use a google search on 'Tensorflow', the first thing that they see is going to be the website, not the Github repo itself, so please update it. (Okay, if you're just too lazy to stay updated just post a message on the screen saying ""This page is no longer maintained so see the repo (link to repo)"" something like that)  Standalone code to reproduce the issue ```shell No code. ```  Relevant log output ```shell ```",2025-02-27T13:07:47Z,type:docs-bug stat:awaiting tensorflower type:bug,open,0,1,https://github.com/tensorflow/tensorflow/issues/88226,"The Chinese installation document translation has not been updated for a long time, see CC(Remove or update zhcn translation from installation instructions). "
copybara-service[bot],#litert Fix LiteRT build,litert Fix LiteRT build,2025-02-27T11:37:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88225
copybara-service[bot],[XLA:CPU] Add support to build HLO computations programmatically in kernel_emitter testlib,[XLA:CPU] Add support to build HLO computations programmatically in kernel_emitter testlib,2025-02-27T11:23:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88224
copybara-service[bot],[XLA:GPU] Add missing bounds check for Scatter.,[XLA:GPU] Add missing bounds check for Scatter. We didn't check whether the linear index derived from block id and thread id is within bounds of the scatter indices.,2025-02-27T11:19:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88223
copybara-service[bot],Make Windows RBE build fast again,"Make Windows RBE build fast again Due to https://github.com/bazelbuild/bazel/commit/24ba4fa60cc82380196f03ad978717da84b9984b, nobuild_runfile_links is no longer implied by remote_download_minimal, which caused performance regression on Windows after upgrading Bazel to 7.4.1",2025-02-27T11:03:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88222
weilhuan-quic,Qualcomm AI Engine Direct - Op Builders for 1P Models," WHAT 1. Conv2d 2. DepthwiseConv2d 3. AveragePool 4. MaxPool 5. DepthToSpace 6. SpaceToDepth 7. HardSwish 8. LeakyRelu 9. ResizeBilinear 10. Litert options for these op builders, unit test 11. update qnn_compiler_plugin_test with these op builders  TEST  qnn_compiler_plugin_test ``` [] Global test environment teardown [==========] 115 tests from 5 test suites ran. (3489 ms total) [  PASSED  ] 115 tests. ```  litert_options_test ``` [] Global test environment teardown [==========] 22 tests from 1 test suite ran. (0 ms total) [  PASSED  ] 22 tests ```",2025-02-27T10:49:15Z,awaiting review comp:lite ready to pull size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88221
copybara-service[bot],[XLA:GPU] Fix ra2a thunk kind and add to the list of collective thunks.,[XLA:GPU] Fix ra2a thunk kind and add to the list of collective thunks.,2025-02-27T10:43:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88220
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T09:45:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88219
copybara-service[bot],[XLA:GPU] use eraseNamedMetadata instead of eraseNamedMDNode,[XLA:GPU] use eraseNamedMetadata instead of eraseNamedMDNode eraseNamedMDNode does not clean the hash map that getNamedMetadata method uses. As a result the hash map keeps pointer to a deleted node and this leads to UseAfterFree crash.,2025-02-27T09:34:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88218
copybara-service[bot],[XLA:GPU] Add NVSHMEM library and initialization test,"[XLA:GPU] Add NVSHMEM library and initialization test This is a port of the PR from NVIDIA: https://github.com/openxla/xla/pull/20395 Copy of openxla CC(Update eager notebooks with buttons, licenses, and change filenames)",2025-02-27T09:22:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88217
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T09:16:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88216
copybara-service[bot],xformer on device test model.,xformer on device test model.,2025-02-27T07:43:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88215
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T07:05:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88214
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T06:53:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88213
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T06:14:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88212
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88128 from jiunkaiy:dev/chunhsue/embeddingLookupWorkAround 9d36f30080c36ff0f22f0e78fea34d7ee8e6baa8,2025-02-27T06:12:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88211
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T06:04:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88210
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T05:58:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88209
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T05:56:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88208
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T05:40:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88207
copybara-service[bot],Add ResultAccuracy to xla builder for the remaining unary functions:,Add ResultAccuracy to xla builder for the remaining unary functions:   * Cbrt   * Cos   * Erf   * Expm1   * Log   * Log1p   * Logistic   * Rsqrt   * Sin   * Sqrt   * Tan   * Tanh,2025-02-27T05:26:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88206
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T05:23:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88205
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T05:21:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88204
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T05:13:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88203
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T05:12:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88202
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T05:11:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88201
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T05:08:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88200
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T05:06:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88199
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T04:59:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88198
copybara-service[bot],[xla] Literal: convert Literal::Populate functions to templates,[xla] Literal: convert Literal::Populate functions to templates ``` name                                         old cpu/op   new cpu/op   delta BM_Populate/64/process_time                  49.9µs ± 2%  44.1µs ± 1%  11.48%  (p=0.000 n=38+33) BM_Populate/128/process_time                  194µs ± 1%   171µs ± 2%  12.12%  (p=0.000 n=37+37) BM_Populate/512/process_time                 3.06ms ± 1%  2.67ms ± 2%  12.63%  (p=0.000 n=38+38) BM_Populate/1024/process_time                12.2ms ± 1%  10.7ms ± 2%  12.69%  (p=0.000 n=36+38) BM_PopulateParallel/64/process_time           796µs ±16%   772µs ±12%   2.94%  (p=0.030 n=39+38) BM_PopulateParallel/128/process_time         1.89ms ± 9%  1.84ms ±16%   3.05%  (p=0.004 n=37+39) BM_PopulateParallel/512/process_time         10.8ms ± 8%  10.4ms ± 8%   3.29%  (p=0.000 n=39+39) BM_PopulateParallel/1024/process_time        27.8ms ± 6%  25.7ms ± 5%   7.51%  (p=0.000 n=39+39) BM_PopulateLinear/64/process_time            18.6µs ± 2%   9.3µs ± 1%  50.17%  (p=0.000 n=40+36) BM_PopulateLinear/128/process_time           70.8µs ± 2%  33.4µs ± 1%  52.89%  (p=0.000 n=38+37) BM_PopulateLinear/512/process_time           1.11ms ± 1%  0.51ms ± 1%  53.82%  (p=0.000 n=39+38) BM_PopulateLinear/1024/process_time          4.45ms ± 1%  2.05ms ± 1%  53.95%  (p=0.000 n=37+37) BM_PopulateLinearParallel/64/process_time     331µs ±11%   299µs ±11%   9.76%  (p=0.000 n=39+39) BM_PopulateLinearParallel/128/process_time    440µs ± 8%   364µs ± 6%  17.29%  (p=0.000 n=39+35) BM_PopulateLinearParallel/512/process_time   2.13ms ± 2%  1.05ms ± 3%  50.86%  (p=0.000 n=40+37) BM_PopulateLinearParallel/1024/process_time  7.77ms ± 2%  4.02ms ± 2%  48.22%  (p=0.000 n=39+39) name                                         old time/op          new time/op          delta BM_Populate/64/process_time                  49.9µs ± 2%          44.1µs ± 1%  11.55%  (p=0.000 n=39+38) BM_Populate/128/process_time                  195µs ± 2%           171µs ± 2%  12.19%  (p=0.000 n=39+39) BM_Populate/512/process_time                 3.06ms ± 1%          2.67ms ± 2%  12.76%  (p=0.000 n=39+37) BM_Populate/1024/process_time                12.2ms ± 1%          10.7ms ± 2%  12.67%  (p=0.000 n=39+40) BM_PopulateParallel/64/process_time           268µs ±16%           262µs ±21%     ~     (p=0.374 n=37+39) BM_PopulateParallel/128/process_time          590µs ±10%           586µs ±10%     ~     (p=0.311 n=39+35) BM_PopulateParallel/512/process_time         2.39ms ±10%          2.37ms ±13%     ~     (p=0.277 n=37+39) BM_PopulateParallel/1024/process_time        4.93ms ± 7%          4.79ms ± 7%   2.80%  (p=0.000 n=37+39) BM_PopulateLinear/64/process_time            18.7µs ± 1%           9.3µs ± 1%  50.20%  (p=0.000 n=39+35) BM_PopulateLinear/128/process_time           70.9µs ± 2%          33.4µs ± 1%  52.92%  (p=0.000 n=39+37) BM_PopulateLinear/512/process_time           1.11ms ± 1%          0.52ms ± 1%  53.79%  (p=0.000 n=39+38) BM_PopulateLinear/1024/process_time          4.46ms ± 2%          2.05ms ± 1%  53.98%  (p=0.000 n=40+37) BM_PopulateLinearParallel/64/process_time     115µs ±12%           113µs ±12%     ~     (p=0.127 n=39+37) BM_PopulateLinearParallel/128/process_time    125µs ±10%           120µs ±10%   4.58%  (p=0.000 n=38+35) BM_PopulateLinearParallel/512/process_time    198µs ± 2%           157µs ± 5%  20.65%  (p=0.000 n=39+35) BM_PopulateLinearParallel/1024/process_time   686µs ± 2%           354µs ± 2%  48.39%  (p=0.000 n=39+39) ``` FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/87655 from tensorflow:tilakrayalpatch2 a10923d1dea2e45aed2a8cd26670e90cf981bcee,2025-02-27T04:58:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88197
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T04:39:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88196
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T04:38:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88195
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-27T04:36:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88194
clickbaron,Fix TOSA bytecode conversion and improve code readability," Bug Fix This PR fixes a critical bug in the TOSA conversion pipeline where `experimental_tflite_to_tosa_bytecode()` was trying to call `ExperimentalTFLiteToTosaBytecode` directly instead of through the `_pywrap_mlir` module.  Code Improvements Additionally, this PR improves code readability and reduces complexity by:  Adding a helper function for string encoding  Simplifying parameter defaults  Making code more consistent across functions  Testing Verified the fix resolves the original NameError by importing and using the function. Fixes CC([TOSA] NameError: name 'ExperimentalTFLiteToTosaBytecode' is not defined) ",2025-02-27T02:38:09Z,size:M comp:lite-tosa,closed,0,6,https://github.com/tensorflow/tensorflow/issues/88193,> [!IMPORTANT] > The terms of service for this installation has not been accepted. Please ask the Organization owners to visit the Gemini Code Assist Admin Console to sign it.,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , Can you please sign CLA, Thank you !","hi  , are you still working on this PR? should we close this? ","Oh you can close it because I didn't know g workspaces couldn't be connected  will contribute later on with my Gmail 👍🏻 Founder of CloudTap.com On Mon, Apr 14, 2025 at 2:43 PM JerryGe ***@***.***> wrote: > hi   , are you still working on > this PR? should we close this? > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> > *JerryGe* left a comment (tensorflow/tensorflow CC(Fix TOSA bytecode conversion and improve code readability)) >  > > hi   , are you still working on > this PR? should we close this? > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","Based on the last comment, I'm closing this PR."
copybara-service[bot],Create `PjrtCApiGpuExecutableTest`.,"Create `PjrtCApiGpuExecutableTest`. Executable functions going through the PJRT C API are not tested on GPU. There doesn't seem to be any particular reason for this, and we should in principle be testing all relevant code paths.",2025-02-27T01:41:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88192
copybara-service[bot],[XLA:GPU] Fix post-optimization pipeline parallelism tests,[XLA:GPU] Fix postoptimization pipeline parallelism tests These tests passed earlier (by chance). The underlying issue is the same as for the test fixed in cl/730568729.,2025-02-27T01:14:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88191
copybara-service[bot],[XLA:GPU] Enable latency-hiding scheduler in pipeline parallleism tests,[XLA:GPU] Enable latencyhiding scheduler in pipeline parallleism tests Disable tests that fail in combination with latencyhiding scheduling.,2025-02-27T01:12:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88190
copybara-service[bot],[XLA:GPU] Add copy insertion test for directly shifing data with send/recv,[XLA:GPU] Add copy insertion test for directly shifing data with send/recv This test case caused nondeterministic behavior in pipeline parallelism tests. The root cause was invalid postoptimization HLO in the test case. Adding this test case is an improvement either way.,2025-02-27T01:12:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88189
copybara-service[bot],[XLA:GPU] Fix post-optimization pipeline parallelism tests,"[XLA:GPU] Fix postoptimization pipeline parallelism tests The test does not run HLO passes. In particular, copy insertion does not run on this input. This means the input must guarantee nonconflicting live ranges of all buffers. The new copies and control dependencies enforce this guarantee. With HLO passes enabled, this would be enforced by the copy insertion pass.",2025-02-27T01:12:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88188
copybara-service[bot],[XLA:GPU] Log when skipping NCCL send/recv,[XLA:GPU] Log when skipping NCCL send/recv,2025-02-27T01:11:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88187
copybara-service[bot],[XLA:GPU] Resolve conflicts between send/recv in trailing loop iteration and surrounding collectives when pipelining,[XLA:GPU] Resolve conflicts between send/recv in trailing loop iteration and surrounding collectives when pipelining Send/recv operations in the peeled off last loop iteration must not slip in between conflicting collectives to avoid deadlocks. Add control dependencies from send/recv ops in trailing loop iteration to conflicting collectives surrounding the new loop.,2025-02-27T01:10:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88186
copybara-service[bot],Remove parsed_pspec from NamedSharding constructor,Remove parsed_pspec from NamedSharding constructor,2025-02-27T01:03:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88185
copybara-service[bot],Allow PJRT GPU compilation and load to be done on a client that does not run the executable.,Allow PJRT GPU compilation and load to be done on a client that does not run the executable.,2025-02-27T01:00:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88184
copybara-service[bot],migrate createFuseConvolutionPass from mlir/lite/stablehlo/transforms to mlir/stablehlo/transforms,migrate createFuseConvolutionPass from mlir/lite/stablehlo/transforms to mlir/stablehlo/transforms,2025-02-27T00:44:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88183
copybara-service[bot],[XLA:GPU] point control dependencies in gpu_p2p_pipeliner to send/recv-done instead of send/recv,[XLA:GPU] point control dependencies in gpu_p2p_pipeliner to send/recvdone instead of send/recv,2025-02-27T00:38:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88182
copybara-service[bot],Add test for Model sigature runner,Add test for Model sigature runner,2025-02-26T23:41:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88181
copybara-service[bot],PR #23020: Bump github/codeql-action from 3.24.9 to 3.28.10,"PR CC(Error in C++ Tensorflow to load model trained by python): Bump github/codeqlaction from 3.24.9 to 3.28.10 Imported from GitHub PR https://github.com/openxla/xla/pull/23020 Bumps github/codeqlaction from 3.24.9 to 3.28.10.  Release notes Sourced from github/codeqlaction's releases.  v3.28.10 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.10  21 Feb 2025  Update default CodeQL bundle version to 2.20.5.  CC(Please consider adding flatten) Address an issue where the CodeQL Bundle would occasionally fail to decompress on macOS.  CC(Checkpoint Restore blocked by changed default bias variable name)  See the full CHANGELOG.md for more information. v3.28.9 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.9  07 Feb 2025  Update default CodeQL bundle version to 2.20.4.  CC(C++ compilation of rule '//:sip_hash' failed (Tensorflow serving on Android))  See the full CHANGELOG.md for more information. v3.28.8 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.8  29 Jan 2025  Enable support for Kotlin 2.1.10 when running with CodeQL CLI v2.20.3.  CC(Fix for build issue 2742;)  See the full CHANGELOG.md for more information. v3.28.7 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.7  29 Jan 2025 No user facing changes. See the full CHANGELOG.md for more information. v3.28.6 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs.   ... (truncated)   Changelog Sourced from github/codeqlaction's changelog.  CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. [UNRELEASED] No user facing changes. 3.28.10  21 Feb 2025  Update default CodeQL bundle version to 2.20.5.  CC(Please consider adding flatten) Address an issue where the CodeQL Bundle would occasionally fail to decompress on macOS.  CC(Checkpoint Restore blocked by changed default bias variable name)  3.28.9  07 Feb 2025  Update default CodeQL bundle version to 2.20.4.  CC(C++ compilation of rule '//:sip_hash' failed (Tensorflow serving on Android))  3.28.8  29 Jan 2025  Enable support for Kotlin 2.1.10 when running with CodeQL CLI v2.20.3.  CC(Fix for build issue 2742;)  3.28.7  29 Jan 2025 No user facing changes. 3.28.6  27 Jan 2025  Reenable debug artifact upload for CLI versions 2.20.3 or greater.  CC(Modifying MNIST example to distributed version: could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR)  3.28.5  24 Jan 2025  Update default CodeQL bundle version to 2.20.3.  CC(Branch 124290852)  3.28.4  23 Jan 2025 No user facing changes. 3.28.3  22 Jan 2025  Update default CodeQL bundle version to 2.20.2.  CC(Update roadmap.md) Fix an issue downloading the CodeQL Bundle from a GitHub Enterprise Server instance which occurred when the CodeQL Bundle had been synced to the instance using the CodeQL Action sync tool and the Actions runner did not have Zstandard installed.  CC(Branch 124251558) Uploading debug artifacts for CodeQL analysis is temporarily disabled.  CC(Tensorflow with Pyinstaller)  3.28.2  21 Jan 2025 No user facing changes. 3.28.1  10 Jan 2025  CodeQL Action v2 is now deprecated, and is no longer updated or supported. For better performance, improved security, and new features, upgrade to v3. For more information, see this changelog post.  CC(Import error)    ... (truncated)   Commits  b56ba49 Merge pull request  CC(Isn't current tensorflowgit r0.9? ) from github/updatev3.28.109856c48b1 60c9c77 Update changelog for v3.28.10 9856c48 Merge pull request  CC(Segmentation fault on tensorflow 0.9.0) from github/redsun82/rust 9572e09 Rust: fix log string 1a52936 Rust: special case default setup cf7e909 Merge pull request  CC(Please consider adding flatten) from github/updatebundle/codeqlbundlev2.20.5 b7006aa Merge branch 'main' into updatebundle/codeqlbundlev2.20.5 cfedae7 Rust: throw configuration errors if requested and not correctly enabled 3971ed2 Merge branch 'main' into redsun82/rust d38c6e6 Merge pull request  CC(Bazel fail to resolve submodule tensorflow) from github/angelapwen/bumpoctokit Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)  Copybara import of the project:  62ccec95e0caa348151e04d97d944e4bdee3dd58 by dependabot[bot] : Bump github/codeqlaction from 3.24.9 to 3.28.10 Bumps github/codeqlaction from 3.24.9 to 3.28.10.  Release notes  Changelog  Commits  updateddependencies:  dependencyname: github/codeqlaction   dependencytype: direct:production   updatetype: versionupdate:semverminor ... Signedoffby: dependabot[bot]  Merging this change closes CC(Error in C++ Tensorflow to load model trained by python) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23020 from openxla:dependabot/github_actions/github/codeqlaction3.28.10 62ccec95e0caa348151e04d97d944e4bdee3dd58",2025-02-26T23:40:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88180
copybara-service[bot],[xla] Optimize accidental copies in HloEvaluator::Postprocess,[xla] Optimize accidental copies in HloEvaluator::Postprocess,2025-02-26T23:39:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88179
copybara-service[bot],PR #23018: Bump ossf/scorecard-action from 2.3.1 to 2.4.1,"PR CC(.): Bump ossf/scorecardaction from 2.3.1 to 2.4.1 Imported from GitHub PR https://github.com/openxla/xla/pull/23018 Bumps ossf/scorecardaction from 2.3.1 to 2.4.1.  Release notes Sourced from ossf/scorecardaction's releases.  v2.4.1 What's Changed  This update bumps the Scorecard version to the v5.1.1 release. For a complete list of changes, please refer to the v5.1.0 and v5.1.1 release notes. Publishing results now uses half the API quota as before. The exact savings depends on the repository in question.  use Scorecard library entrypoint instead of Cobra hooking by @​spencerschrock in ossf/scorecardaction CC(Need force_gpu_if_available for tests)   Some errors were made into annotations to make them more visible  Make default branch error more prominent by @​jsoref in ossf/scorecardaction CC(partial_run segfault)   There is now an optional file_mode input which controls how repository files are fetched from GitHub. The default is archive, but git produces the most accurate results for repositories with .gitattributes files at the cost of analysis speed.  add input for specifying filemode by @​spencerschrock in ossf/scorecardaction CC(Fix python3 b)   The underlying container for the action is now hosted on GitHub Container Registry. There should be no functional changes.  :seedling: publish docker images to GitHub Container Registry by @​spencerschrock in ossf/scorecardaction CC(Multidimensional RNN)    Docs  Installation docs update by @​JeremiahAHoward in ossf/scorecardaction CC(ci_build  debian jessie)  New Contributors  @​JeremiahAHoward made their first contribution in ossf/scorecardaction CC(ci_build  debian jessie) @​jsoref made their first contribution in ossf/scorecardaction CC(partial_run segfault) Full Changelog: https://github.com/ossf/scorecardaction/compare/v2.4.0...v2.4.1  v2.4.0 What's Changed This update bumps the Scorecard version to the v5 release. For a complete list of changes, please refer to the v5.0.0 release notes. Of special note to Scorecard Action is the Maintainer Annotation feature, which can be used to suppress some Code Scanning false positives. Alerts will not be generated for any Scorecard Check with an annotation.  :seedling: Bump github.com/ossf/scorecard/v5 from v5.0.0rc2 to v5.0.0 by @​spencerschrock in ossf/scorecardaction CC(tensorflor cond(pred, fn1, fn2) evaluate fn1 and fn2 together regardless of pred) :bug: lower license sarif alert threshold to 9 by @​spencerschrock in ossf/scorecardaction CC(Mac OS;import error ""ImportError: cannot import name _message"")  Documentation  docs: dogfooding badge by @​jkowalleck in ossf/scorecardaction CC(Error using tf.image.random._ : 'numpy.ndarray' object has no attribute 'get_shape')  New Contributors  @​jkowalleck made their first contribution in ossf/scorecardaction CC(Error using tf.image.random._ : 'numpy.ndarray' object has no attribute 'get_shape')  Full Changelog: https://github.com/ossf/scorecardaction/compare/v2.3.3...v2.4.0 v2.3.3  [!NOTE] There is no v2.3.2 release as a step was skipped in the release process. This was fixed and rereleased under the v2.3.3 tag  What's Changed  :seedling: Bump github.com/ossf/scorecard/v4 (v4.13.1) to github.com/ossf/scorecard/v5 (v5.0.0rc1) by @​spencerschrock in ossf/scorecardaction CC(Implement the Special Functions incbet,igam,igamc for CPU+GPU for float & double.) :seedling: Bump github.com/ossf/scorecard/v5 from v5.0.0rc1 to v5.0.0rc2 by @​spencerschrock in ossf/scorecardaction CC(Document trick with slash in scope names) :seedling: Bump github.com/ossf/scorecard/v5 from v5.0.0rc2 to v5.0.0rc2.0.202405091827347ce860946928 by @​spencerschrock in ossf/scorecardaction CC(Tensorflow Website : Missing Mathjax results in broken equations)  For a full changelist of what these include, see the v5.0.0rc1 and v5.0.0rc2 release notes. Documentation  :book: Move token discussion out of main README. by @​spencerschrock in ossf/scorecardaction CC(Arch doesn't support it)    ... (truncated)   Commits  f49aabe bump docker to ghcr v2.4.1 ( CC(Hardcoded bash path)) 30a595b :seedling: Bump github.com/sigstore/cosign/v2 from 2.4.2 to 2.4.3 ( CC(rnn.bidirectional_rnn  cause a problem)) 69ae593 omit vcs info from build ( CC(Bugfix to test/run_and_gather_logs.)) 6a62a1c add input for specifying filemode ( CC(Fix python3 b)) 2722664 :seedling: Bump the githubactions group with 2 updates ( CC(Delete useless directory)) ae0ef31 :seedling: Bump github.com/spf13/cobra from 1.8.1 to 1.9.1 ( CC(some learning decays from Stanford CS231n Karpathy lecture 6)) 3676bbc :seedling: Bump golang from 1.23.6 to 1.24.0 in the dockerimages group ( CC(seems issues with softmax_cross_entropy_with_logits)) ae7548a Limit codeQL push trigger to main branch ( CC(quick python3 fix)) 9165624 upgrade scorecard to v5.1.0 ( CC(Fix python3 breakage (oldstyle exception block))) 620fd28 :seedling: Bump the githubactions group with 2 updates ( CC(typos fix and ign temp files in gitignore)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)  Copybara import of the project:  c459d962aa2a508b6d8c33dafe7cbf7ed8e8a034 by dependabot[bot] : Bump ossf/scorecardaction from 2.3.1 to 2.4.1 Bumps ossf/scorecardaction from 2.3.1 to 2.4.1.  Release notes  Changelog  Commits  updateddependencies:  dependencyname: ossf/scorecardaction   dependencytype: direct:production   updatetype: versionupdate:semverminor ... Signedoffby: dependabot[bot]  Merging this change closes CC(.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23018 from openxla:dependabot/github_actions/ossf/scorecardaction2.4.1 c459d962aa2a508b6d8c33dafe7cbf7ed8e8a034",2025-02-26T23:37:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88178
copybara-service[bot],Remove warning about tokens in HostOffloader.,Remove warning about tokens in HostOffloader.,2025-02-26T23:28:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88177
copybara-service[bot],[xla:cpu] Literal: Add APIs for accessing and populating literals using linear indexing,[xla:cpu] Literal: Add APIs for accessing and populating literals using linear indexing ```  Benchmark                                            Time             CPU   Iterations  BM_Populate/1024/process_time                 10186003 ns     10164037 ns           67 BM_PopulateParallel/1024/process_time          5690373 ns     29945649 ns           23 BM_PopulateLinear/1024/process_time            3292270 ns      3286792 ns          211 BM_PopulateLinearParallel/1024/process_time     810066 ns      5736881 ns          117 ```,2025-02-26T23:25:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88176
copybara-service[bot],Enable hiding ACF parameters node that are implementation details in the hlo graph.,Enable hiding ACF parameters node that are implementation details in the hlo graph.,2025-02-26T23:22:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88175
copybara-service[bot],Apply PostQuantize pass after TFL Optimization pipeline to aid constant folding on some quantized ops.,Apply PostQuantize pass after TFL Optimization pipeline to aid constant folding on some quantized ops.,2025-02-26T23:12:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88174
copybara-service[bot],Clean up dependencies in core/profiler/convert,Clean up dependencies in core/profiler/convert,2025-02-26T23:05:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88173
wonjeon,[mlir][tosa] Remove specialized 5d reduction legalization and Change Transpose perms operand to attribute,We've been pushing TOSA v1.0 LLVM patches to upstream. This PR includes commits to keep the following operators to be aligned with LLVM: [mlir][tosa] Change Transpose perms operand to attribute https://github.com/llvm/llvmproject/pull/128115,2025-02-26T22:59:59Z,size:L comp:lite-tosa,closed,0,3,https://github.com/tensorflow/tensorflow/issues/88172,> [!IMPORTANT] > The terms of service for this installation has not been accepted. Please ask the Organization owners to visit the Gemini Code Assist Admin Console to sign it.,"Local testing successfully done: INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 4.015s, Critical Path: 2.31s INFO: 2 processes: 2 local. INFO: Build completed successfully, 2 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test    (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test       (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test  (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test (cached) PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test (cached) PASSED in 0.1s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test (cached) PASSED in 0.1s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 2.2s Executed 1 out of 17 tests: 17 tests pass.", 
copybara-service[bot],Revert to Bazel 6.5.0.,Revert to Bazel 6.5.0. The 7.4.1 switch made Windows presubmits extremely slow Reverts 5c289f5ba22711a296a216100cf9816c6077d85d,2025-02-26T22:57:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88171
copybara-service[bot],[XLA:GPU] Fix RaggedAllToAllDecomposer pass.,[XLA:GPU] Fix RaggedAllToAllDecomposer pass. There were two issues that are happening: 1. Update slice logic was not correct and would overwrite values outside of the update. 2. RaggedAllToAll API was extended to allow multiple updates per replica.,2025-02-26T22:46:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88170
copybara-service[bot],Revert program_id to string to enable seeing the hlo stats table.,Revert program_id to string to enable seeing the hlo stats table.,2025-02-26T22:45:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88169
copybara-service[bot],Fork StrictQuantizationPattern from QuantizationPattern,Fork StrictQuantizationPattern from QuantizationPattern This will allow us to have tflite specific quantization logic which will only depend on quant annotations.,2025-02-26T22:13:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88168
copybara-service[bot],Skip processing SyncFlag events while grouping of events for Xspace conversion.,Skip processing SyncFlag events while grouping of events for Xspace conversion.,2025-02-26T22:10:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88167
copybara-service[bot],Enable BatchMatMul->FC optimization for i8.,Enable BatchMatMul>FC optimization for i8.,2025-02-26T21:49:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88166
copybara-service[bot],#tf-data Adds `min_parallelism` option to allow advanced users to force set minimum parallelism for autotuning to reduce warm up time,tfdata Adds `min_parallelism` option to allow advanced users to force set minimum parallelism for autotuning to reduce warm up time,2025-02-26T21:47:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88165
copybara-service[bot],Update cpu_benchmarks.yml and gpu_benchmarks.yml to run the Gemma2-2B HLO.,Update cpu_benchmarks.yml and gpu_benchmarks.yml to run the Gemma22B HLO.,2025-02-26T21:40:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88164
copybara-service[bot],Add an AllGatherSimplifier pass that detects and removes unnecessary AllGather Ops.,Add an AllGatherSimplifier pass that detects and removes unnecessary AllGather Ops. Examples: * A trivial allgather where the input and output shapes are compatible will be replaced by its operand. * An allgather with a single consumer that is a dynamicslice such that the output of the dynamicslice is the same as the input of the allgather will be replaced by the operand.,2025-02-26T21:31:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88163
copybara-service[bot],Enable optimize_batch_matmul_pass to convert BMM with int8 RHS to FC.,Enable optimize_batch_matmul_pass to convert BMM with int8 RHS to FC.,2025-02-26T21:22:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88162
copybara-service[bot],[xla:cpu] Fix a deadlock in JitCompiler::TaskDispatcher,[xla:cpu] Fix a deadlock in JitCompiler::TaskDispatcher,2025-02-26T21:09:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88161
copybara-service[bot],[xla] Use shape partitioner to decide the number of tasks in ShapeUtil::ForEachIndexInternalParallel,[xla] Use shape partitioner to decide the number of tasks in ShapeUtil::ForEachIndexInternalParallel,2025-02-26T20:20:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88160
copybara-service[bot],Avoid an expensive blocking call to NumElements.,Avoid an expensive blocking call to NumElements.,2025-02-26T20:13:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88159
copybara-service[bot],[XlaCallModule] Add better error message to computation deserialization failures.,[XlaCallModule] Add better error message to computation deserialization failures.,2025-02-26T20:11:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88158
copybara-service[bot],"Move TSL's `third_party` to XLA, update users","Move TSL's `third_party` to XLA, update users Mostly updating `WORKSPACE` and related files from `//third_party` > `//third_party`. Please tag me if this change breaks you.",2025-02-26T20:09:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88157
copybara-service[bot],[XLA:MSA] Correctly account for already inserted instructions while fixing the schedule in MSA.,"[XLA:MSA] Correctly account for already inserted instructions while fixing the schedule in MSA. There is no need to error out if the instruction is already inserted, it just means that a previous instruction already added it to the schedule so we ignore it.",2025-02-26T19:55:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88156
copybara-service[bot],Add non-generic functions to HloRunnerPjRt that separate arg load + execution.,Add nongeneric functions to HloRunnerPjRt that separate arg load + execution. In select circumstances we want to use a HloRunnerPjRt implementation but have better control of the lifetime of the inputs and outputs. This functionality is not supported by the HloRunnerInterface. This patch provides some functions to allow the loading of inputs once and multiple execution on the same inputs.,2025-02-26T19:48:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88155
copybara-service[bot],Streamline and reduce profiler/lib imports,Streamline and reduce profiler/lib imports,2025-02-26T19:41:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88154
copybara-service[bot],Move PJRT plugin linker scripts to `additional_linker_inputs`,Move PJRT plugin linker scripts to `additional_linker_inputs` Part 2 of https://github.com/openxla/xla/pull/16696 The final change will be the namechange of the target and add the `install_name` option.,2025-02-26T19:31:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88153
copybara-service[bot],Rolling back a commit that caused a 50-90% performance regression in most MaxText workloads.,Rolling back a commit that caused a 5090% performance regression in most MaxText workloads. Reverts 1fe54335f208c63c8b7b3df12ae745249427adcb,2025-02-26T19:14:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88152
copybara-service[bot],Streamline and reduce profiler/utils imports,Streamline and reduce profiler/utils imports,2025-02-26T19:06:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88151
copybara-service[bot],Fix tensor CloneTo() not coping per-channel quantization parameters.,Fix tensor CloneTo() not coping perchannel quantization parameters.,2025-02-26T19:05:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88150
copybara-service[bot],Remove Deprecate Usage of CodeDef Proto Fields,Remove Deprecate Usage of CodeDef Proto Fields FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23019 from openxla:dependabot/github_actions/actions/uploadartifact4.6.1 7cb47206e44222d93a2c1efc5525c31190e046b1,2025-02-26T19:02:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88149
copybara-service[bot],Integrate LLVM at llvm/llvm-project@c0abae33d6e7,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match c0abae33d6e7,2025-02-26T19:00:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88148
copybara-service[bot],Fix `PadOpBroadcastEmptyTensor` to handle dynamic operand types.,Fix `PadOpBroadcastEmptyTensor` to handle dynamic operand types. Fixes https://github.com/openxla/stablehlo/issues/2721,2025-02-26T18:20:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88147
copybara-service[bot],Extend `DirectSession::Finalize()` to finalize the function library runtime,Extend `DirectSession::Finalize()` to finalize the function library runtime,2025-02-26T17:51:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88144
copybara-service[bot],Delete 'profiler' and 'profiler_client' from 'third_party/tensorflow/python/eager'.,Delete 'profiler' and 'profiler_client' from 'third_party/tensorflow/python/eager'.,2025-02-26T17:28:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88141
copybara-service[bot],Try PR 86840.,Try PR 86840.,2025-02-26T16:21:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88140
copybara-service[bot],[XLA:GPU][NFC] Move the information about the root indexing to the special struct.,[XLA:GPU][NFC] Move the information about the root indexing to the special struct.,2025-02-26T15:08:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88139
copybara-service[bot],Add CHECK to enforce the inst->operands_->parent_ == inst->parent_ invariant,Add CHECK to enforce the inst>operands_>parent_ == inst>parent_ invariant,2025-02-26T15:05:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88138
copybara-service[bot],[XLA:CPU] Entry points are valid C names prefixed with HLO module name,[XLA:CPU] Entry points are valid C names prefixed with HLO module name Names are now prefixed with the name of the HLO module so if the use wants to ensure uniqueness of names they just have to ensure the uniqueness of HLO module names. This also enforces a fairly simplified conversion of names to C style names which will be useful for AOT generation.,2025-02-26T14:52:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88137
gaikwadrahul8,Fix 04 broken links in audio_classification.md,"Hi, Team I found 04 broken documentation links in this file audio_classification.md so I have updated those broken links to functional link. Please review and merge this change as appropriate. Thank you for your consideration.",2025-02-26T14:52:45Z,awaiting review comp:lite size:S,open,0,0,https://github.com/tensorflow/tensorflow/issues/88136
copybara-service[bot],Fix variable type and add comments for PriorityFusion (NFC).,Fix variable type and add comments for PriorityFusion (NFC).,2025-02-26T13:22:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88135
copybara-service[bot],Integrate LLVM at llvm/llvm-project@a98c2940dbc0,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match a98c2940dbc0,2025-02-26T12:57:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88134
copybara-service[bot],[xla:cpu] Add new pthreadpool APIs to xnn thread pool integration,[xla:cpu] Add new pthreadpool APIs to xnn thread pool integration,2025-02-26T12:16:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88133
copybara-service[bot],Use std::string_view instead of StringRef::str,Use std::string_view instead of StringRef::str,2025-02-26T11:45:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88132
copybara-service[bot],Passing device information and TMA flag to TritonXLA Extract/Insert pass. This will be needed when adding TMA support which will only be available for Hopper+.,Passing device information and TMA flag to TritonXLA Extract/Insert pass. This will be needed when adding TMA support which will only be available for Hopper+.,2025-02-26T10:34:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88131
copybara-service[bot],Hook up the approximate GELU implementation.,Hook up the approximate GELU implementation.,2025-02-26T10:27:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88130
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T10:01:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88129
chunhsue,Qualcomm AI Engine Direct - Cast int8 embedding lookup table to int16 and set vendor logger to INFO, What  Some models contain embedding lookup ops with int8 table and int16 output. Provided a solution.  Set default qualcomm vendor logger to INFO.  Tests Passed `qnn_compiler_plugin_test` ``` [] Global test environment teardown [==========] 97 tests from 5 test suites ran. (3450 ms total) [  PASSED  ] 97 tests. ```,2025-02-26T09:06:22Z,ready to pull size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88128
weilhuan-quic,Qualcomm AI Engine Direct - Graph O3 Optimization,1. Use O3 as default 2. Use relax precision for float as default because QNN tends to always enable it.  Test ```      0.0ms [  INFO ]   QnnContext_getBinary done successfully. context = 0x1 INFO: [tensorflow/lite/experimental/litert/vendors/qualcomm/qnn_manager.cc:247] Serialized a context bin of size (bytes): 644400 INFO: [tensorflow/lite/experimental/litert/vendors/qualcomm/compiler/qnn_compiler_plugin.cc:437] Context binary 0 generated      0.0ms [  INFO ]   QnnContext_free started. context = 0x1      0.0ms [  INFO ]   QnnContext_free done successfully.      0.0ms [  INFO ]   QnnDevice_free started. device = 0x1      0.0ms [  INFO ]   QnnDevice_free done. status 0x0      0.0ms [  INFO ]   QnnBackend_free started. backend = 0x1      0.0ms [  INFO ] Closing environment: 0x55aab477e5f0 [       OK ] SupportedOpsTest/QnnPluginOpCompatibilityTest.SupportedOpsTest/41 (666 ms) [] 42 tests from SupportedOpsTest/QnnPluginOpCompatibilityTest (2995 ms total) [] Global test environment teardown [==========] 97 tests from 5 test suites ran. (3196 ms total) [  PASSED  ] 97 tests. ```,2025-02-26T08:56:08Z,awaiting review comp:lite ready to pull size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88127
copybara-service[bot],PR #23092: [NFC] Switch absl::optional to std::optional and cleanup.,PR CC(add interpreter debug func): [NFC] Switch absl::optional to std::optional and cleanup. Imported from GitHub PR https://github.com/openxla/xla/pull/23092 Copybara import of the project:  5d8333a8df3417e7769eb2b0b1143bb9834a0368 by Ilia Sergachev : [NFC] Cleanup uses of optional.  a2d4ce205ec499e592429ae873eba68baf2b1a9c by Ilia Sergachev : [NFC] Switch most remaining uses of absl::optional to std::optional. Merging this change closes CC(add interpreter debug func) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23092 from openxla:optional_cleanup a2d4ce205ec499e592429ae873eba68baf2b1a9c,2025-02-26T08:50:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88126
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T07:57:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88125
saisindhuri91,Fix build error in tensorflow/lite/delegates/xnnpack/flexbuffers_util.h on s390x,This PR has been raised to resolve the following error occured in s390x: ``` ./tensorflow/lite/delegates/xnnpack/flexbuffers_util.h:51:10: error: no viable conversion from returned value of type 'std::nullptr_t' to function return type 'tflite::xnnpack::FloatPointer'   51       ^~~~~~~ ``` NOTE: This change is also working in intel.,2025-02-26T07:31:13Z,awaiting review comp:lite size:XS,open,0,1,https://github.com/tensorflow/tensorflow/issues/88124,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Introduce `Finalize()` for `FunctionLibraryRuntimeImpl` and `ProcessFunctionLibraryRuntime`,Introduce `Finalize()` for `FunctionLibraryRuntimeImpl` and `ProcessFunctionLibraryRuntime`,2025-02-26T07:11:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88123
ywangwxd,"tensorflow cannot detect GPU with cuda 12.8, torch can"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.13  Custom code No  OS platform and distribution Linux CentOS 7  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuda 12.8 cudnn 9.7.1  GPU model and memory v100  Current behavior? I have tried python 3.8, 3.11, none of the can detect GPU, but pytorch and tfnightly is working well. Whenever I ask tensorflow to list GPU available, it gave me this message: > [GCC 11.2.0] :: Anaconda, Inc. on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf  "", len(tf.config.list_physical_devices('GPU')))20250226 14:28:49.474758: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used. 20250226 14:28:49.528867: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.  Standalone code to reproduce the issue ```shell import tensorflow as tf print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) ```  Relevant log output ```shell ```",2025-02-26T06:33:16Z,stat:awaiting tensorflower type:support comp:gpu TF 2.13,open,3,5,https://github.com/tensorflow/tensorflow/issues/88122,"I'm having trouble getting TensorFlow 2.13 to detect my GPU, even though PyTorch and tfnightly work just fine. Things I’ve Tried Checked CUDA installation – nvidiasmi confirms the GPU is fine. Verified TensorFlow compatibility – Seems like TF 2.13 only supports CUDA 11.8, but I have CUDA 12.8. Reinstalled TensorFlow with GPU support – No change. Set environment variables correctly – Still not working. Workarounds tfnightly works, so it looks like TensorFlow 2.13 doesn’t support CUDA 12.8. Potential Fixes: Use tfnightly: pip install tfnightly Downgrade to CUDA 11.8 (since TF 2.13 officially supports that). Would be great to know if stable TensorFlow will support CUDA 12.8 soon. Thanks!",">  Issue type >  > Bug >  >  Have you reproduced the bug with TensorFlow Nightly? >  > No >  >  Source >  > binary >  >  TensorFlow version >  > tf 2.13 >  >  Custom code >  > No >  >  OS platform and distribution >  > Linux CentOS 7 >  >  Mobile device >  > _No response_ >  >  Python version >  > 3.8 >  >  Bazel version >  > _No response_ >  >  GCC/compiler version >  > _No response_ >  >  CUDA/cuDNN version >  > cuda 12.8 cudnn 9.7.1 >  >  GPU model and memory >  > v100 >  >  Current behavior? >  > I have tried python 3.8, 3.11, none of the can detect GPU, but pytorch and tfnightly is working well. > Whenever I ask tensorflow to list GPU available, it gave me this message: >  > > [GCC 11.2.0] :: Anaconda, Inc. on linux > Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. > >>> import tensorflow as tf >  "", len(tf.config.list_physical_devices('GPU')))20250226 14:28:49.474758: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used. > 20250226 14:28:49.528867: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used. >  >  Standalone code to reproduce the issue >  > ```shell > import tensorflow as tf > print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) > ``` >  >  Relevant log output >  > ```shell >  > ``` On it","Check GPU Availability – Ensure your system detects the GPU using (nvidiasmi). If not, install/update NVIDIA drivers. Verify TensorFlow's CUDA Requirements – Different TensorFlow versions require specific CUDA and cuDNN versions. Install the correct versions. Install/Update NVIDIA Drivers – If no GPU is detected, install the latest NVIDIA driver. Install CUDA & cuDNN – Download and install the correct versions required by TensorFlow. Set Environment Variables – Ensure CUDA and cuDNN paths are correctly set in the system’s PATH. Restart the System – After installation, reboot to apply changes. Test TensorFlow GPU Support – Run>( tf.config.list_physical_devices('GPU')) to check if TensorFlow detects the GPU.","`import tensorflow as tf print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) if len(tf.config.list_physical_devices('GPU')) > 0:     print(""TensorFlow is using GPU"") else:     print(""TensorFlow is using CPU"")  If still error occurs , reinstall tensorflow pip uninstall  y tensorflow pip install tensorflow==2.10  `","Please support CUDA 12.8 , NVIDIA RTX 5090, Blackwell architecture GPU as PyTorch, at least nightly version of TensorFlow."
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T06:14:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88121
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T06:10:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88120
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T06:06:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88119
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T06:05:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88118
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T06:03:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88117
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T06:02:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88116
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T05:59:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88115
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T05:59:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88114
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T05:59:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88113
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T05:59:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88112
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T05:58:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88111
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T05:58:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88110
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T05:56:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88109
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T05:48:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88108
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T05:46:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88107
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-26T05:32:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88106
copybara-service[bot],#sdy Add test for the cloned round_trip_import pipeline in Shardy github.,sdy Add test for the cloned round_trip_import pipeline in Shardy github.,2025-02-26T04:04:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88105
copybara-service[bot],Make external_litert_buffer_context library visible to TPU.,Make external_litert_buffer_context library visible to TPU.,2025-02-26T02:42:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88104
neuropilot-captain,Supports more legalizations and multi-partition,1. more legalizations     a. Batch_MatMul    b. Mul    c. Fully_Connected 3. multipartition supports      introduce utils for supporting multipartition,2025-02-26T02:15:34Z,comp:lite size:L,closed,0,2,https://github.com/tensorflow/tensorflow/issues/88103,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi captain , Can you please sign and resolve the conflicts? Thank you!"
copybara-service[bot],Fix SparseCore input pipeline analysis.,Fix SparseCore input pipeline analysis.,2025-02-26T01:45:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88102
copybara-service[bot],Clear out XLA computations from compilation caches upon resource manager finalization.,"Clear out XLA computations from compilation caches upon resource manager finalization. After compiling a TF Graph into an XLA HLO program and after compiling the HLO into an executable, we keep around a `std::shared_ptrcomputation`. When the compiled HLO contains many constants, its heap memory consumption is significant and otherwise unreferenced after initialization. This CL adds an entrypoint `DeviceCompilationCache::Finalize`, which is exposed as `DeviceCompiler::Finalize`, which is an implementation of the virtual function `ResourceBase::Finalize`.",2025-02-26T01:19:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88101
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@03597b1e,Integrate StableHLO at openxla/stablehlo,2025-02-26T01:05:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88100
shashank-mg,spam commit,,2025-02-26T00:32:40Z,size:XS invalid,closed,0,1,https://github.com/tensorflow/tensorflow/issues/88099,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],[XLA:GPU] add heuristic to collective permute decomposer and make it only decompose one CP,[XLA:GPU] add heuristic to collective permute decomposer and make it only decompose one CP,2025-02-26T00:28:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88098
copybara-service[bot],PR #22437: Added frontend attribute handling to explicit_stream_annotation_async_wrapper ,"PR CC(tensorflow1.11.0rc1 fails to build): Added frontend attribute handling to explicit_stream_annotation_async_wrapper  Imported from GitHub PR https://github.com/openxla/xla/pull/22437 This is a small change that ensures the frontend attributes are correctly passed to both the `asyncstart` and `asyncdone` created pairs. This also clears the scheduling attributes that are directly on the call operation and inner ops. The specific goal of this change is to have stable support combining the scheduling group ids with stream annotation in JAX.  ```python with set_xla_metadata(_scheduling_group_id=1):    result = compute_on(""gpu_stream:1"")(jitted_func)(...) ``` Currently, the issue stems from the `set_xla_metadata` context manager, which will apply the frontend attribute to all operations, including the ones within our `jitted_func`. When the same scheduling annotations is found in two `HloComputation`s, an error is raised in `LegalizeSchedulingAnnotations`. This is intended to avoid hitting this check, and cleaning up the annotations on the wrapped streamed computation. Copybara import of the project:  994c2eee3c946102270587681f5c17b994cbb6a9 by chaser : Added frontend attributed handling  9db58b2b988dc2288d42126271223f924aac19f9 by chaser : Added clearing of scheduling annotations  a83e32a34ba5d64a29c7f01b03536f27decd8125 by chaser : Added HloInstruction.erase_frontend_attribute Merging this change closes CC(tensorflow1.11.0rc1 fails to build) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22437 from chaserileyroberts:chase/frontend_forward_async_wrapper a83e32a34ba5d64a29c7f01b03536f27decd8125",2025-02-26T00:25:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88097
copybara-service[bot],Add zstd support to DecodeCompressedOp,"Add zstd support to DecodeCompressedOp This only allows for zstd decompression of data produced by other tools, and does so directly inline. It would be nice to add full zstd support for reading and writing streams of data like https://github.com/tensorflow/tensorflow/pull/58871 did, but DecodeCompressedOp only needs a straightforward decompress implementation (even without streaming!) since the end result is going to be an inmemory tensor anyway.",2025-02-26T00:15:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88096
copybara-service[bot],Add single op model test cases for classic models.,Add single op model test cases for classic models.,2025-02-26T00:07:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88095
copybara-service[bot],PR #23060: Refacting xla/tests/client_library_test_base to remove duplicated codes.,PR CC(Attributes values not inferred by TFE C API (eager mode)): Refacting xla/tests/client_library_test_base to remove duplicated codes. Imported from GitHub PR https://github.com/openxla/xla/pull/23060 Copybara import of the project:  217e312c7f5bd5834f0b4a2d4206aa735b72476a by Shawn Wang : code refacotring  c62b8f587da2886287376246086c9bf550fe7fcb by Shawn Wang : fix  b98044083e6dd6f2a37152e9638b93ac7cee6a06 by Shawn Wang : fix  a291d6e332ab1cc3fd11ff5ff19d414ac9973896 by Shawn Wang : fix: Merging this change closes CC(Attributes values not inferred by TFE C API (eager mode)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23060 from shawnwang18:shawnw/options_iterator a291d6e332ab1cc3fd11ff5ff19d414ac9973896,2025-02-26T00:02:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88094
copybara-service[bot],Latency hiding scheduler: Allow DoesReleaseResource API to take a resource index directly.,Latency hiding scheduler: Allow DoesReleaseResource API to take a resource index directly.,2025-02-25T23:46:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88093
copybara-service[bot],Fix graph output tensor not recognized as output in QC plugin.,Fix graph output tensor not recognized as output in QC plugin.,2025-02-25T23:41:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88092
copybara-service[bot],Reverts changelist 723349025,Reverts changelist 723349025,2025-02-25T23:29:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88091
copybara-service[bot],Introduce `FunctionBody::Finalize()` to populate `AllocatorAttribute`s for arg/ret nodes and release unnecessary resources,Introduce `FunctionBody::Finalize()` to populate `AllocatorAttribute`s for arg/ret nodes and release unnecessary resources,2025-02-25T23:09:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88090
copybara-service[bot],[XLA:GPU/TMA] Rewriting func.func into tt.func during lowering to TTIR.,[XLA:GPU/TMA] Rewriting func.func into tt.func during lowering to TTIR.,2025-02-25T22:58:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88089
copybara-service[bot],Test failing change,Test failing change,2025-02-25T22:52:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88088
copybara-service[bot],[transfer library]: Add 2s connection retries.,[transfer library]: Add 2s connection retries.,2025-02-25T22:42:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88087
copybara-service[bot],Add support for AHWB-backed GL Buffers in LiteRT runtime.,Add support for AHWBbacked GL Buffers in LiteRT runtime. Reverts b70de1e127eace867cf6823d95bd1b4a83e238e2,2025-02-25T22:38:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88086
copybara-service[bot],[xla:cpu] Add alwayslink=1 to `:py_client_cpu.cc` so external builds link in CPU callback handler.,[xla:cpu] Add alwayslink=1 to `:py_client_cpu.cc` so external builds link in CPU callback handler.,2025-02-25T22:36:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88085
copybara-service[bot],[easy] [XLA:TPU] [Simplifier] Remove redundant strides for slices,[easy] [XLA:TPU] [Simplifier] Remove redundant strides for slices FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88412 from tensorflow:dependabot/docker/tensorflow/tools/tf_sig_build_dockerfiles/ubuntued1544e 4b6ba37c2667bdfded35b1774ed8ee438bc5fb25,2025-02-25T22:18:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88084
copybara-service[bot],Fuse Mul followed by Conv/FC with QDQs,Fuse Mul followed by Conv/FC with QDQs This commit extends the existing pattern to fuse a Mul into the kernel of an affine op. The tailing mul appears as part of a common pattern which is a BatchNorm following a Conv.,2025-02-25T22:08:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88083
copybara-service[bot],Register TFQuantDialect in tf_quant_opt.cc,Register TFQuantDialect in tf_quant_opt.cc,2025-02-25T22:04:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88082
copybara-service[bot],Add serialization options to the public API for alignment for bytecode.,Add serialization options to the public API for alignment for bytecode.,2025-02-25T22:00:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88081
copybara-service[bot],[XLA:OSS] Add CI connection step to the ci workflows.,[XLA:OSS] Add CI connection step to the ci workflows.,2025-02-25T21:57:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88080
copybara-service[bot],Enable folding of quantized reshape with per-axis scales,Enable folding of quantized reshape with peraxis scales,2025-02-25T21:55:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88079
copybara-service[bot],PR #84932: Qualcomm AI Engine Direct - Add dispatch options for QC,PR CC(Qualcomm AI Engine Direct  Add dispatch options for QC): Qualcomm AI Engine Direct  Add dispatch options for QC Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/84932 Summary:  Add htp runtime options  Add log level settings dispatch_delegate_qualcomm_test !image Copybara import of the project:  15aa2288c9491a5e4a7bbab13c019fea99f11855 by jiunkaiy : Qualcomm AI Engine Direct  Add dispatch options for QC Summary:  Add htp runtime options  Add log level settings Merging this change closes CC(Qualcomm AI Engine Direct  Add dispatch options for QC) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/84932 from jiunkaiy:dev/jiunkaiy/qualcomm_dispatch_config 15aa2288c9491a5e4a7bbab13c019fea99f11855,2025-02-25T21:50:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88078
copybara-service[bot],Make the dispatch api .so target depends on the litert runtime c share library,"Make the dispatch api .so target depends on the litert runtime c share library This would reduce the .so file size from 1.2M => 600K Fixed cycular dependency between dispatch_delegate, vendors/c/dispatch_api and runtime dispatch api implementation.",2025-02-25T21:26:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88076
copybara-service[bot],[XLA:OSS] Add CI connection step to the ci workflows.,[XLA:OSS] Add CI connection step to the ci workflows.,2025-02-25T21:11:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88075
copybara-service[bot],Register TFQuantDialect with tf/compiler/mlir/quantization/tensorflow/passes/preprocess_op.cc,Register TFQuantDialect with tf/compiler/mlir/quantization/tensorflow/passes/preprocess_op.cc,2025-02-25T20:39:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88074
copybara-service[bot],[XLA:CPU] Add option to always fold constants,[XLA:CPU] Add option to always fold constants,2025-02-25T20:00:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88073
copybara-service[bot],Directly overwrite ADSP_LIBRARY_PATH if shared lib path is provided to qnn manager.,Directly overwrite ADSP_LIBRARY_PATH if shared lib path is provided to qnn manager. Fix the issue where existing ADSP_LIBRARY_PATH contains other versions QNN lib files.,2025-02-25T19:51:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88072
copybara-service[bot],[IFRT] Add short form for specifying platform_names for IFRT IR passes.,"[IFRT] Add short form for specifying platform_names for IFRT IR passes. Some IFRT IR passes require a list of platform names to be given as an option. Currently, a platform names list requires an entry for each device, which makes  manually running the passes on modules with many devices tedious. This change introduces a short form for specifying platform names with the format  platform_name:number_of_occurrences. For example, tpu:2,cpu:3 is expanded to tpu,tpu,cpu,cpu,cpu.",2025-02-25T19:48:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88071
copybara-service[bot],Update TODO with bug number.,Update TODO with bug number.,2025-02-25T19:28:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88070
copybara-service[bot],Reverts 1e0f639a26b2aafd2732ae0e64817b7cdd387c81,Reverts 1e0f639a26b2aafd2732ae0e64817b7cdd387c81,2025-02-25T19:25:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88069
copybara-service[bot],[XLA:Python] Lock down visibilities.,"[XLA:Python] Lock down visibilities. Currently most libraries have wide visibility to friends of XLA. But most of these targets should not be used directly. This change sets the default visibility to private and uses "":friends"" visibility where needed only.",2025-02-25T19:20:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88068
copybara-service[bot],Fix HLO stats table to use int types as ints (instead of strings).,Fix HLO stats table to use int types as ints (instead of strings).,2025-02-25T19:13:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88067
copybara-service[bot],migrate createFoldBroadcastPass under third_party/tensorflow/compiler/mlir/stablehlo,migrate createFoldBroadcastPass under third_party/tensorflow/compiler/mlir/stablehlo,2025-02-25T19:13:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88066
copybara-service[bot],"Remove `nogcp` config, remove warnings associated with the `linux` config as XLA has its own warnings.bazelrc","Remove `nogcp` config, remove warnings associated with the `linux` config as XLA has its own warnings.bazelrc",2025-02-25T18:49:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88065
copybara-service[bot],Introduce light-weight `strict_cc_test` to enforce good practices without adding lots of dependencies.,Introduce lightweight `strict_cc_test` to enforce good practices without adding lots of dependencies. Also:  refactor existing test rules in terms of `strict_cc_test` to reduce duplication.  replace all uses of `native.cc_test` in XLA `.bzl` files with `strict_cc_test`.,2025-02-25T18:37:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88063
copybara-service[bot],[XLA:SchedulingAnnotations] Switch from the root readiness model to the scheduled successors model to launch an annotated scheduling group.,"[XLA:SchedulingAnnotations] Switch from the root readiness model to the scheduled successors model to launch an annotated scheduling group. Root readiness does not cover the newly added test case where a producerconsumer computation pair is annotated with the same group id and the producer also feeds into some other op. We previously identified the consumer as the root and tried to schedule the annotation as soon as the consumer was ready (assuming the async collective is also ready). However, the nonannotated consumer of the producer might have yet to be scheduled. In the new model, we make sure all (nonannotated) successors of the group are scheduled before we attempt to schedule it. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/87759 from tensorflow:gaikwadrahul8patch1 adaffbab3e9cc85b362614065e6ae9a022256f02",2025-02-25T18:23:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88062
copybara-service[bot],Integrate LLVM at llvm/llvm-project@9889de834b0a,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 9889de834b0a,2025-02-25T18:20:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88061
copybara-service[bot],litert: Fix broken Dispatch API tests,litert: Fix broken Dispatch API tests Provide valid DispatchOption to LiteRtDispatchInitialize(),2025-02-25T18:20:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88060
copybara-service[bot],Introduce `TPUDummyInput` as a specialization of `Fill` for ICI weight distribution.,"Introduce `TPUDummyInput` as a specialization of `Fill` for ICI weight distribution. The new op has a few benefits over the previous version: * We can generate a single op instead of three ops for each dummy input. * The new op is marked as `DoNotOptimize` and `TF_NoConstantFold`, so it will never be accidentally constantfolded to a large memory footprint.",2025-02-25T18:07:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88059
copybara-service[bot],Fix Android ARM64 build for hlo_to_mhlo.,Fix Android ARM64 build for hlo_to_mhlo. See also commit ce2bae2.,2025-02-25T18:00:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88058
copybara-service[bot],[XLA:GPU] Fix xspace.pb path,[XLA:GPU] Fix xspace.pb path,2025-02-25T17:56:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88057
copybara-service[bot],PR #23061: [XlA:GPU] Fix GpuCommandBuffer NotifyExecDestroyed assert failure,"PR CC(Xla and Ignite support always true from configure.py): [XlA:GPU] Fix GpuCommandBuffer NotifyExecDestroyed assert failure Imported from GitHub PR https://github.com/openxla/xla/pull/23061 This PR fixes the NotifyExecDestroy assertion failure, due to NotifyExecCreate is not called due to not enabling VLOG dump for the matching file. Copybara import of the project:  45ca4e27626b6dfea5abf5b2bf70623a7e76f69a by Shawn Wang : Fix GpuCommandBuffer NotifyExecDestroyed assert failure  da576f093c5a2b7b1c024cc23ca093bd47f8285b by Shawn Wang : format Merging this change closes CC(Xla and Ignite support always true from configure.py) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23061 from shawnwang18:shawnw/notify_cmd_buffer da576f093c5a2b7b1c024cc23ca093bd47f8285b",2025-02-25T17:44:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88056
copybara-service[bot],Cleanup CC Tensor Buffer and Buffer Requirements.,Cleanup CC Tensor Buffer and Buffer Requirements. Reverts b70de1e127eace867cf6823d95bd1b4a83e238e2,2025-02-25T17:14:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88054
Kuree,[TOSA] NameError: name 'ExperimentalTFLiteToTosaBytecode' is not defined," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.19.0rc  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? This function call should not return a `NameError`. It should either do not exist, or should be removed.  Standalone code to reproduce the issue ```shell from tensorflow.python.pywrap_mlir import experimental_tflite_to_tosa_bytecode experimental_tflite_to_tosa_bytecode("""", """") ```  Relevant log output ```shell Traceback (most recent call last):   File """", line 1, in    File ""/tmp/env/lib/python3.12/sitepackages/tensorflow/python/pywrap_mlir.py"", line 123, in experimental_tflite_to_tosa_bytecode     return ExperimentalTFLiteToTosaBytecode(            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ NameError: name 'ExperimentalTFLiteToTosaBytecode' is not defined. Did you mean: 'experimental_tflite_to_tosa_bytecode'? ```",2025-02-25T16:59:24Z,stat:awaiting tensorflower type:bug comp:lite TF 2.18,open,0,4,https://github.com/tensorflow/tensorflow/issues/88053, on it  putting in a PR ,"Hi, ,   I apologize for the delayed response, I have been able to replicate the same issue with `tensorflow==2.19.0rc0` but when I tried with `tensorflow 2.18.0` it's working as expected for reference please refer this gistfile and I see  has already submitted PR to take care of this issue but CLA has not signed so  could you please sign CLA for PR ? Thank you for your cooperation and patience."," 's PR does not fix the issue  the python binding is deleted by this commit: https://github.com/tensorflow/tensorflow/commit/95def861c766fd78dd673d7b8fdebd3c695dfb50. Based on the commit message, this python API is deprecated and should not exist.",I saw that the original code wasn't clean as well. Is there something better we can do?
copybara-service[bot],Clean Up Forwarding Headers in `tensorflow/core/profiler/rpc folder`,Clean Up Forwarding Headers in `tensorflow/core/profiler/rpc folder`,2025-02-25T16:59:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88052
copybara-service[bot],Add GlTexture buffer type to Tensor Buffer.,Add GlTexture buffer type to Tensor Buffer. Reverts b70de1e127eace867cf6823d95bd1b4a83e238e2,2025-02-25T16:52:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88051
copybara-service[bot],Flip default of _experimental_enable_composite_direct_lowering flag to True,Flip default of _experimental_enable_composite_direct_lowering flag to True,2025-02-25T16:31:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88050
copybara-service[bot],PR #23068: [GPU] Fix missing cuDNN symbols.,PR CC(TFLite benchmark_model cannot be compiled successfully): [GPU] Fix missing cuDNN symbols. Imported from GitHub PR https://github.com/openxla/xla/pull/23068 This fixes JAX builds with cuDNN 9.5.0+ after https://github.com/openxla/xla/commit/65b4b8874b659d7f11523f7b1d6df1613cfc8984. Copybara import of the project:  3aa286e5a849e2187ef3d44c22c54d518dd168ec by Ilia Sergachev : [GPU] Fix missing cuDNN symbols. Merging this change closes CC(TFLite benchmark_model cannot be compiled successfully) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23068 from openxla:fix_cudnn_symbols 3aa286e5a849e2187ef3d44c22c54d518dd168ec,2025-02-25T16:00:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88049
copybara-service[bot],PR #22898: [GPU] GEMM fusion autotuner: dump unoptimized fusions before profiling them.,PR CC(Error building on Windows // no such package '): [GPU] GEMM fusion autotuner: dump unoptimized fusions before profiling them. Imported from GitHub PR https://github.com/openxla/xla/pull/22898 This helps debugging failures during profiling. Copybara import of the project:  e63f7865126281a7eb5b410394424826275037a8 by Ilia Sergachev : [GPU] GEMM fusion autotuner: dump unoptimized fusions before profiling them. This helps debugging failures during profiling. Merging this change closes CC(Error building on Windows // no such package ') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22898 from openxla:dump_before_profiling e63f7865126281a7eb5b410394424826275037a8,2025-02-25T15:54:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88048
gaikwadrahul8,Improve Makefile Shell Specification for Portability To Support More Systems,"Hi, Team The current Makefile uses `SHELL := /bin/bash` while this works on many systems it can cause issues on systems where bash is not located at `/bin/bash`. This Pull Request changes the shell specification to `SHELL := /usr/bin/env bash`. `env` searches the user's `$PATH` for the bash executable ensuring that the Makefile uses the correct bash installation regardless of its location. This significantly improves portability and compatibility across different operating systems and configurations.  Thank you for your consideration.",2025-02-25T15:32:13Z,size:XL,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88047
copybara-service[bot],[xla:cpu:onednn] Support basic MatMul in oneDNN fusion thunk.,[xla:cpu:onednn] Support basic MatMul in oneDNN fusion thunk. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23078 from openxla:revert_pr_22292 f2cc964f5b849b149626a007045cccc32778ee27,2025-02-25T15:25:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88046
copybara-service[bot],[xla:cpu:onednn] Support elementwise Add and Mul in oneDNN fusion thunk,[xla:cpu:onednn] Support elementwise Add and Mul in oneDNN fusion thunk,2025-02-25T15:21:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88045
copybara-service[bot],[XLA:GPU/TMA] Adding boundary checks to Loads/Stores.,[XLA:GPU/TMA] Adding boundary checks to Loads/Stores.,2025-02-25T15:20:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88044
copybara-service[bot],#litert Fix `LITERT_RETURN_IF_ERROR` when checking bool return values.,"litert Fix `LITERT_RETURN_IF_ERROR` when checking bool return values.  `false` return values are errors.  Add `kLiteRtStatusErrorUnknown` for unknown errors.  When converting a boolean error to a `LiteRtStatus`/`litert::Expected`, the   error value is `kLiteRtStatusErrorUnknown`.",2025-02-25T15:19:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88043
shaoyuyoung,[XLA] can't compile `tf.sparse.from_dense-tf.sparse.minimum`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version nightly 20250225  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? pass the compilation  Standalone code to reproduce the issue ```python import os import tensorflow import tensorflow as tf import numpy as np os.environ[""CUDA_VISIBLE_DEVICES""] = ""1"" class ComplexModel(tf.keras.Model):     def __init__(self):         super(ComplexModel, self).__init__()     def call(self, x):         x_sparse = tf.sparse.from_dense(x)         x = tf.sparse.minimum(x_sparse, tf.sparse.from_dense(tf.ones_like(x)))         return x model = ComplexModel() x = tf.constant([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32) inputs = [x] model(*inputs) print(""succeed on eager"") class ComplexModel(tf.keras.Model):     def __init__(self):         super(ComplexModel, self).__init__()     .function(jit_compile=True)     def call(self, x):         x_sparse = tf.sparse.from_dense(x)         x = tf.sparse.minimum(x_sparse, tf.sparse.from_dense(tf.ones_like(x)))         return x model = ComplexModel() model(*inputs) print(""succeed on XLA"") ```  Relevant log output ``` succeed on eager tf2xla conversion failed while converting __inference_call_33[_XlaMustCompile=true,config_proto=13561319589895757934,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_call_33] ```",2025-02-25T15:19:14Z,type:bug comp:xla TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88042,Hey tensorflow team! Just wanted to chime in on this XLA sparse tensor issue. I've been hitting the same problem with `tf.sparse.from_dense` and `tf.sparse.minimum` not working with XLA  I went into the cuda ask and messed around; The operations run perfectly in eager mode but fail when using `jit_compile=True`. This seems to be a known limitation with XLA not fully supporting these sparse tensor operations yet. Workaround until there is a fix  I'll try to add in my end btw: 1. Simplest workaround: Remove `jit_compile=True` when using sparse ops 2. Alternative: Rewrite your code to use dense operations if performance allows 3. Hybrid approach: Split your function into XLA and nonXLA parts Is there anyone currently working on XLA sparse tensor support who could use an extra hand? any guidance on where to start if I wanted to contribute to implementing this functionality? Peace!,"I was able to reproduce the same issue using TensorFlow 2.18 and the nightly version. Please find the gist here for reference. Additionally, I have provided an alternative solution in the gist, which might be helpful. Thank you!",Thank you for your confirmation!,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Testing a temporary code change.,Testing a temporary code change.,2025-02-25T15:17:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88041
shaoyuyoung,[XLA] can't compile `tf.linalg.inv` when input tensor dtype is `tf.complex64`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version nightly 20250223  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? eager can pass the check for `tf.linalg.inv` when input dtype is `tf.complex64`. So XLA should also pass the compilation but it fails  Standalone code to reproduce the issue ```python import os import tensorflow import tensorflow as tf import numpy as np os.environ[""CUDA_VISIBLE_DEVICES""] = ""1"" class FFTInverseModel(tf.keras.Model):     def __init__(self):         super().__init__()     def call(self, x):         inv = tf.linalg.inv(x)         return inv model = FFTInverseModel() input_shape = (2, 2) x = tf.constant([[1.0, 2.0], [3.0, 4.0]], dtype=tf.complex64, shape=input_shape)   tf.complex64 is the trigger condition inputs = [x] model(*inputs) print(""succeed on eager"") class FFTInverseModel(tf.keras.Model):     def __init__(self):         super().__init__()     .function(jit_compile=True)     def call(self, x):         inv = tf.linalg.inv(x)         return inv model = FFTInverseModel() model(*inputs) print(""succeed on XLA"") ```  Relevant log output ``` succeed on eager tf2xla conversion failed while converting __inference_call_7[_XlaMustCompile=true,config_proto=13561319589895757934,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_call_7] ```",2025-02-25T15:04:36Z,type:bug comp:xla TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/88040,I was able to reproduce the same issue using TensorFlow 2.18 and the nightly version. Please find the gist here for reference. Thank you!,Thank you for your confirmation!,Are you satisfied with the resolution of your issue? Yes No,I'm having the same issue and not sure how this was resolved.
misterBart,tfLite. Select fastest available GPU ,"TfLite. Code comment in lite/delegates/gpu/delegate.h states that 'fastest available GPU' is selected. That is not the case. Currently, the first GPU of the first CL platform is selected. The current code also fails to select a GPU if that GPU resides on another CL platform than the first CL platform. My PR selects the fastest available GPU of all available CL platforms. My PR also resolves somewhat issue 44332",2025-02-25T15:01:25Z,awaiting review comp:lite size:M,open,0,1,https://github.com/tensorflow/tensorflow/issues/88039,Any news? It's been two months ago.
shaoyuyoung,[XLA] crashes when compiling `tf.raw_ops.AssignVariableOp`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version nightly 20250223  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? `tf.raw_ops.AssignVariableOp` should pass the compliation  Standalone code to reproduce the issue ```python import os import tensorflow import tensorflow as tf import numpy as np os.environ[""CUDA_VISIBLE_DEVICES""] = ""1"" class VariableModel(tf.keras.Model):     def __init__(self):         super(VariableModel, self).__init__()         self.variable = tf.Variable(initial_value=0.0)     def call(self, x):         return tf.raw_ops.AssignVariableOp(resource=self.variable.handle, value=tf.add(self.variable, x)) model = VariableModel() input_shape = [1] x = tf.constant([1.0], shape=input_shape) inputs = [x] model(*inputs) print(""succeed on eager"") class VariableModel(tf.keras.Model):     def __init__(self):         super(VariableModel, self).__init__()         self.variable = tf.Variable(initial_value=0.0)     .function(jit_compile=True)     def call(self, x):         return tf.raw_ops.AssignVariableOp(resource=self.variable.handle, value=tf.add(self.variable, x)) model = VariableModel() model(*inputs) print(""succeed on XLA"") ```  Relevant log output ``` succeed on eager ValueError: Shapes must be equal rank, but are 0 and 1 for '{{node AssignVariableOp}} = AssignVariableOpdtype=DT_FLOAT, validate_shape=false' with input shapes: [], [1]. ```",2025-02-25T14:54:24Z,type:bug comp:xla TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/88038,"I was able to reproduce the same issue using TensorFlow 2.18 and the nightly version. Please find the gist here for reference. Additionally, I have provided an alternative solution in the gist, which might be helpful. Thank you!",Thank you for your confirmation!,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[XLA] update indexing map docs,[XLA] update indexing map docs,2025-02-25T14:38:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88037
copybara-service[bot],[XLA] Do not reset AUTO layout in entry_computation_layout to default.,[XLA] Do not reset AUTO layout in entry_computation_layout to default. That restores functionality broken by https://github.com/openxla/xla/pull/22640.,2025-02-25T14:23:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88036
copybara-service[bot],[pjrt:triton] Re-use the XLA GPU pipeline instead of duplicating it,[pjrt:triton] Reuse the XLA GPU pipeline instead of duplicating it,2025-02-25T14:13:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88035
copybara-service[bot],Install tbp-testing to support mlprofiler.,Install tbptesting to support mlprofiler.,2025-02-25T13:40:40Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/88034,"Vào 20:49 Th 3, 25 thg 2, 2025 copybaraservice[bot]  đã viết: > Install tbptesting to support mlprofiler. >  > You can view, comment on, or merge this pull request online at: > >   https://github.com/tensorflow/tensorflow/pull/88034 > Commit Summary > >     748b22f >     >    Install tbptesting to support mlprofiler. > > File Changes > > (20 files ) > >     *M* >    ci/official/requirements_updater/numpy1_requirements/requirements.in >     >    (1) >     *M* >    ci/official/requirements_updater/numpy1_requirements/requirements_lock_3_10.txt >     >    (39) >     *M* >    ci/official/requirements_updater/numpy1_requirements/requirements_lock_3_11.txt >     >    (39) >     *M* >    ci/official/requirements_updater/numpy1_requirements/requirements_lock_3_12.txt >     >    (39) >     *M* >    ci/official/requirements_updater/numpy1_requirements/requirements_lock_3_9.txt >     >    (39) >     *M* ci/official/requirements_updater/requirements.in >     >    (1) >     *M* tensorflow/core/profiler/convert/BUILD >     >    (8) >     *M* tensorflow/core/profiler/rpc/BUILD >     >    (7) >     *M* tensorflow/python/profiler/internal/BUILD >     >    (87) >     *A* tensorflow/python/profiler/internal/_pywrap_profiler_plugin.py >     >    (48) >     *R* tensorflow/python/profiler/internal/_pywrap_test.py >     >    (18) >     *D* tensorflow/python/profiler/internal/pywrap_profiler_plugin.cc >     >    (210) >     *M* tensorflow/python/profiler/profiler_wrapper_test.py >     >    (5) >     *M* tensorflow/tools/pip_package/setup.py >     >    (1) >     *M* tensorflow/workspace2.bzl >     >    (7) >     *M* third_party/xla/xla/tsl/framework/BUILD >     >    (2) >     *M* third_party/xla/xla/tsl/lib/io/BUILD >     >    (1) >     *M* third_party/xla/xla/tsl/lib/monitoring/BUILD >     >    (4) >     *M* third_party/xla/xla/tsl/profiler/rpc/BUILD >     >    (4) >     *M* third_party/xla/xla/tsl/profiler/rpc/client/BUILD >     >    (7) > > Patch Links: > >     https://github.com/tensorflow/tensorflow/pull/88034.patch >     https://github.com/tensorflow/tensorflow/pull/88034.diff > > — > Reply to this email directly, view it on GitHub > , or unsubscribe >  > . > You are receiving this because you are subscribed to this thread.Message > ID: ***@***.***> >"
copybara-service[bot],"Log which ""test case"" we are running in TritonAndBlasSupport... Regular2DDot","Log which ""test case"" we are running in TritonAndBlasSupport... Regular2DDot It is not quite ideal that we have a test that in effect consists of several testcases, since it's difficult to figure out which one failed when one of them crashes. I do understand the idea that we want an easy to see support matrix, and splitting it up into individual tests would prevent us from doing that. As a middle ground, adding some logging so it's easy to tell what failed from the log.",2025-02-25T12:39:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88033
copybara-service[bot],Upgrade Bazel to 7.4.1,Upgrade Bazel to 7.4.1,2025-02-25T12:36:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88032
copybara-service[bot],[XLA:GPU/TMA] Adding verification for triton_xla ops and custom type.,[XLA:GPU/TMA] Adding verification for triton_xla ops and custom type.,2025-02-25T12:31:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88031
copybara-service[bot],"PR #23078: Revert ""PR #22292: [GPU] Support cuDNN explicit CUDA graph construction.""","PR CC(Building the pip package on Windows): Revert ""PR CC(Docker with python 3.6): [GPU] Support cuDNN explicit CUDA graph construction."" Imported from GitHub PR https://github.com/openxla/xla/pull/23078 This reverts commit 65b4b8874b659d7f11523f7b1d6df1613cfc8984. Copybara import of the project:  f2cc964f5b849b149626a007045cccc32778ee27 by Ilia Sergachev : Revert ""PR CC(Docker with python 3.6): [GPU] Support cuDNN explicit CUDA graph construction."" This reverts commit 65b4b8874b659d7f11523f7b1d6df1613cfc8984. Merging this change closes CC(Building the pip package on Windows) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23078 from openxla:revert_pr_22292 f2cc964f5b849b149626a007045cccc32778ee27",2025-02-25T12:29:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88030
johnnynunez,Cuda 12.8 support, Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19.0  Custom code Yes  OS platform and distribution Ubuntu 24.04  Mobile device _No response_  Python version 3.11  Bazel version 0.6.5  GCC/compiler version _No response_  CUDA/cuDNN version 12.8/9.7.0  GPU model and memory 5090  Current behavior? Not supported. Hermetic cuda download 12.5  Standalone code to reproduce the issue ```shell compile with hermetic cuda. XLA is not updated in tensorflow ```  Relevant log output ```shell ```,2025-02-25T12:00:33Z,type:feature comp:gpu TF 2.18,open,3,1,https://github.com/tensorflow/tensorflow/issues/88029,"Hey, I'm also affected by this issue with CUDA 12.8 and TensorFlow 2.19.0 on Ubuntu CLI within the same OS. While waiting for official support, I found a temporary workaround that might help others:  I manually installed CUDA 12.8 and cuDNN 9.7.0 on my system.  I disabled hermetic CUDA during the TensorFlow build process and pointed it to my local CUDA installation.  However, XLA still has compatibility issues, so I had to disable XLA for now. This is not a complete solution, but it allowed me to use TensorFlow with CUDA 12.8 for basic tasks. I'm still hoping for official support and XLA updates. Let me know if anyone else has tried this or found better workarounds.  Oh and one other thing: within the CUDA update, is there documentation which can help fix this compatibility issue regarding the tensorflow commits ? "
copybara-service[bot],[XLA:CPU] Kernel renamer function should be an optional member of the CPU compiler,[XLA:CPU] Kernel renamer function should be an optional member of the CPU compiler,2025-02-25T11:44:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88028
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T11:25:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88027
copybara-service[bot],Let FusionDeduplicationCache handle ProducerConsumer multi-output fusions.,Let FusionDeduplicationCache handle ProducerConsumer multioutput fusions. This will be needed when we want to allow such fusions in PriorityFusion.,2025-02-25T11:20:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88026
copybara-service[bot],[XLA] Remove the `device_util` build rule from XLA,[XLA] Remove the `device_util` build rule from XLA The header file for this build rule doesn't exist anymore.,2025-02-25T11:12:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88025
copybara-service[bot],[XLA:CPU] Introduce kernel entry renamer to kernel_emitter,[XLA:CPU] Introduce kernel entry renamer to kernel_emitter,2025-02-25T11:09:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88024
copybara-service[bot],#sdy Handle the case where an input to `sdy.manual_computation` has a dimension of size zero.,"sdy Handle the case where an input to `sdy.manual_computation` has a dimension of size zero. In this case, HLO will replace the corresponding `.sdy.GlobalToLocalShape` result with a constant of the same shape (see test case). The solution is to find another input that is produced by the custom call instead. We also mark the two custom calls as side effecting so they won't get CSEd.",2025-02-25T10:25:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88023
copybara-service[bot],[XLA:GPU] Fix thunk emitter for degenerate ops.,"[XLA:GPU] Fix thunk emitter for degenerate ops. The condition to get index of the output buffer wasn't always correct. It's possible to have an op with 1 operand and result with a tuple of 1 element. For example, a degenerate a2a will look like: ``` a2a = (u32[2]) alltoall(u32[2] a1), replica_groups={{0},{1}} ``` It's better to check that output is a tuple.",2025-02-25T10:07:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88022
copybara-service[bot],[XLA:GPU] Init output data with -1.,[XLA:GPU] Init output data with 1. Makes it easier to detect cases when we overwrite data out of the update range.,2025-02-25T09:32:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88021
misterBart,TfLite. Fix of issue 54269,TfLite. Fix of issue 54269. Code will now try to load libOpenCL.so.1 if libOpenCL.so is absent.,2025-02-25T09:23:50Z,awaiting review comp:lite size:XS,open,0,4,https://github.com/tensorflow/tensorflow/issues/88020,!speechrecognitionblockdiagramsystemVChapaneri2012_Q320, What point are you trying to make with that image?,Any news? It's been two months ago and my PR is simple.,"Hi , Can you please review this PR? Thanks !"
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T09:20:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88019
copybara-service[bot],Change int4 packing from big-endian to little-endian,"Change int4 packing from bigendian to littleendian LLVM uses littleendian format for int4 packing. To avoid converting between these formats, we should also use littleendian in XLA.",2025-02-25T09:10:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88018
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T09:09:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88017
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T09:02:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88016
copybara-service[bot],[xla:cpu] Move dot_kernel_emitter under codegen/dot,[xla:cpu] Move dot_kernel_emitter under codegen/dot FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23078 from openxla:revert_pr_22292 f2cc964f5b849b149626a007045cccc32778ee27,2025-02-25T08:37:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88015
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T08:13:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88014
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T08:09:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88013
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T08:04:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88012
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T08:03:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88011
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T08:02:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88010
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T08:02:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88009
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T08:01:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88008
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T08:00:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88007
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T08:00:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88006
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T07:59:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88005
copybara-service[bot],PR #22901: Remove transpose from reshape builder,"PR CC(We have removed this restriction. Please check out the code at head, or in the upcoming 1.4 RC. Please reopen if there are issues with it.): Remove transpose from reshape builder Imported from GitHub PR https://github.com/openxla/xla/pull/22901 Reshape builder unnecessarily accepts two arguments:  1. `dimensions` which permutation of the  layoyt of the shape.  2. `new_sizes` which are the new dimensions of the shape. This is confusing, and the we can simply remove the support for permutation of layout  as it is just a transpose and such a transpose can be easily constructed wherever required. Copybara import of the project:  b44dc317188053cd51b7af4e90598ba357ba1197 by Shraiysh Vaishay : Remove transpose from reshape builder Reshape builder unnecessarily accepts two arguments:  1. `dimensions` which is the ""layout"" of the shape.  2. `new_sizes` which is the ""dimensions"" of the shape. This is confusing, and the we can simply remove the support for layout  as it is just a transpose and such a transpose can be easily constructed wherever required.  a7100cfc2185f5b8fd9dcf6dfa52c221b195021c by Shraiysh Vaishay : Fix unrelated clangformat issues in xla/python/ops. : Fix semantics doc Merging this change closes CC(We have removed this restriction. Please check out the code at head, or in the upcoming 1.4 RC. Please reopen if there are issues with it.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22901 from shraiysh:fix_reshape_and_shape_inference b25041292522834a544b2c7c463909fc576c4ba9",2025-02-25T07:43:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/88004
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T06:58:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88003
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T06:58:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88002
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T06:48:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88001
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-25T06:42:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/88000
copybara-service[bot],Fixes sub key generation for the stacked variable.,Fixes sub key generation for the stacked variable.,2025-02-25T05:11:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87999
copybara-service[bot],Add HLO adapter,Add HLO adapter,2025-02-25T05:03:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87998
copybara-service[bot],Bump the priority of CHLO->MHLO ragged dot pass to highest.,Bump the priority of CHLO>MHLO ragged dot pass to highest.,2025-02-25T04:28:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87997
copybara-service[bot],Update `hlo_sharding_util::CanonicalizeLayoutAfterShardingPropagation` to support partial parameter/result is updated while the other is not allowed to be updated.,"Update `hlo_sharding_util::CanonicalizeLayoutAfterShardingPropagation` to support partial parameter/result is updated while the other is not allowed to be updated. Before this change, this function takes two boolean variables `update_output_layout` and `update_parameters_layout` and applies it to all results/parameters. We need finegrained control on each element of results/parameters. This change replaces them with two `std::vector`.",2025-02-25T04:19:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87996
copybara-service[bot],Fix race condition in the predicate in GPU thunks.,"Fix race condition in the predicate in GPU thunks. WhileThunk and ConditionalThunk stored CUDA host memory that would store the predicate. The thunks would transfer the predicate from device to host into the CUDA host memory. But if the thunks were called multiple times in parallel, each call would use the same memory, causing a race condition which could result in incorrect predicate values. Now a pool of host memory is used so different calls to the thunk get different pointers to host memory. The pool has a fixed size of 128, so if there are more parallel callers than that, an error will be raised. I think it's unlikely there will be that many parallel calls in practice.",2025-02-25T02:56:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87995
copybara-service[bot],litert: Keep the copy of string values on Environment,litert: Keep the copy of string values on Environment The Environment should keep the copy of string so the API caller can free the string.,2025-02-25T02:23:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87994
copybara-service[bot],Replace `tensorflow::profiler::ProfilerInterface` with `tsl::profiler::ProfilerInterface`.,Replace `tensorflow::profiler::ProfilerInterface` with `tsl::profiler::ProfilerInterface`.,2025-02-25T01:44:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87993
copybara-service[bot],PR #22953: Fix const qualifier on status prevents automatic move semantics,"PR CC(Merge branch r1.12 into master.): Fix const qualifier on status prevents automatic move semantics Imported from GitHub PR https://github.com/openxla/xla/pull/22953 reason for change  const qualifier on `status` prevents automatic move semantics in return. When return status; is executed, the compiler cannot invoke the move constructor of `absl::Status` because status is const. Copybara import of the project:  b1722312a9e697d9e55d8758eb1c083005fefcda by Alexander Pivovarov : Fix const qualifier on status prevents automatic move semantics Merging this change closes CC(Merge branch r1.12 into master.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22953 from apivovarov:fix_rnvo b1722312a9e697d9e55d8758eb1c083005fefcda",2025-02-25T01:43:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87992
copybara-service[bot],PR #22956: vlog device id in while_thunk.,PR CC(r1.12rc1 cherrypick request: Explicitly set jdk8 in ci_parameterized_build.sh): vlog device id in while_thunk. Imported from GitHub PR https://github.com/openxla/xla/pull/22956 Copybara import of the project:  d4623150b29e8c3960a1839c3da2234eae71adac by Yunlong Liu : vlog device id in while_thunk. Merging this change closes CC(r1.12rc1 cherrypick request: Explicitly set jdk8 in ci_parameterized_build.sh) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22956 from yliu120:patch1 d4623150b29e8c3960a1839c3da2234eae71adac,2025-02-25T01:40:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87991
copybara-service[bot],PR #22930: Initialize num_slices_ to 0 in Heap Simulator,PR CC(absl not installed as it is included in stringpiece.h): Initialize num_slices_ to 0 in Heap Simulator Imported from GitHub PR https://github.com/openxla/xla/pull/22930 Ensure `num_slices_` class member is explicitly initialized to 0 in `SliceTimeAllPermutationIterator` and `SliceTimePreferredPermutationIterator` to prevent potential uninitialized variable issues. Copybara import of the project:  53a76f188330d4e72171e3b5349e79aafa68132c by Alexander Pivovarov : Initialize num_slices_ to 0 in Heap Simulator Merging this change closes CC(absl not installed as it is included in stringpiece.h) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22930 from apivovarov:init_num_slices_ 53a76f188330d4e72171e3b5349e79aafa68132c,2025-02-25T01:36:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87990
copybara-service[bot],Make XLA C++ tests fail if no test case is linked in. This prevents the mistake of linking the test wrong.,Make XLA C++ tests fail if no test case is linked in. This prevents the mistake of linking the test wrong. Also fix tests that have no test case linked in.,2025-02-25T00:58:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87989
copybara-service[bot],Add support for aligned byte code in internal model serialize API,Add support for aligned byte code in internal model serialize API,2025-02-25T00:42:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87988
copybara-service[bot],Migrate callers from tensorflow::profiler math_utils to tsl/profiler/utils/math_utils.h. No functional changes expected.,Migrate callers from tensorflow::profiler math_utils to tsl/profiler/utils/math_utils.h. No functional changes expected.,2025-02-25T00:30:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87987
copybara-service[bot],IFRT Proxy: Support fetching an array with a sub-byte dtype,IFRT Proxy: Support fetching an array with a subbyte dtype,2025-02-25T00:21:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87986
copybara-service[bot],update SafeDivide() function to reference the correct lib from tsl::profiler,update SafeDivide() function to reference the correct lib from tsl::profiler Internal change,2025-02-25T00:03:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87985
copybara-service[bot],[XLA] Create compute_gpu_device_stats tool to compute the time spent on the GPU and the memcpy time from an XSpace protobuf.,[XLA] Create compute_gpu_device_stats tool to compute the time spent on the GPU and the memcpy time from an XSpace protobuf.,2025-02-24T23:45:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87984
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f1252f539ca2,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f1252f539ca2 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22822 from apivovarov:fix_source_target_pairs f97c38d47c8373ec609fdfbaedff3856f123fc33,2025-02-24T23:36:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87983
copybara-service[bot],Always set use_global_scheduler/rank_queues with priority_merge policy.,Always set use_global_scheduler/rank_queues with priority_merge policy.,2025-02-24T23:30:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87982
copybara-service[bot],Support setting up global prioritized batching via the batch op rewriter.,Support setting up global prioritized batching via the batch op rewriter.,2025-02-24T23:26:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87981
copybara-service[bot],litert: Enable GPU Accelerator on Android,litert: Enable GPU Accelerator on Android  Added build target for libLiteRtGpuAccelerator.so,2025-02-24T23:24:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87980
copybara-service[bot],Remove TensorFlow specific configs in `tensorflow.bazelrc`,Remove TensorFlow specific configs in `tensorflow.bazelrc`,2025-02-24T23:12:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87979
copybara-service[bot],Switch from tsl::Mutex to absl::Mutex,Switch from tsl::Mutex to absl::Mutex,2025-02-24T22:58:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87978
copybara-service[bot],"Build absl::string_view(data, length) (instead of StringRef::str) explicitly since the llvm::StringRef to absl::string_view converter is not (always?) available on","Build absl::string_view(data, length) (instead of StringRef::str) explicitly since the llvm::StringRef to absl::string_view converter is not (always?) available on Android. END_PUBLIC",2025-02-24T22:58:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87977
wonjeon,[mlir][tosa] Fix lit tests for resize,"ChangeId: I8cb88a0b6344259d57a37d6ddd2c0810bb7a61e7 This patch fixes the lit tests for resize op from the merged PR, https://github.com/tensorflow/tensorflow/commit/200be965419f2548bfbdb16a826d2547841479ec.",2025-02-24T22:46:23Z,kokoro:force-run ready to pull size:M,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87976,"Confirmed the test locally. INFO: Analyzed 33 targets (0 packages loaded, 33389 targets configured). INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 3.966s, Critical Path: 2.25s INFO: 2 processes: 2 local. INFO: Build completed successfully, 2 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test    (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test       (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test  (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 2.1s Executed 1 out of 17 tests: 17 tests pass."
copybara-service[bot],"Directly call GpuExecutable::ExecuteThunks rather than RunAsync. This allows us to take a vector<ShapeTree<pair<raw_buffer, is_donated_bool>>> and return ShapeTree<raw_buffer> and handle aliasing properly for aliased types.","Directly call GpuExecutable::ExecuteThunks rather than RunAsync. This allows us to take a vector>> and return ShapeTree and handle aliasing properly for aliased types. Before this CL, aliased types would be converted to and from ScopedShapedBuffer which means that foreign memory types would lose their attached cleanup callback. Recently this was changed to simply error if the user tries to donate foreign memory types (created via CreateViewOfDeviceBuffer), but this CL makes it supported. This is also needed to properly track ownership for raw buffer support.",2025-02-24T22:45:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87975
copybara-service[bot],PR #22822: Fix ambiguous constructor call in SourceTargetPairs initialization,"PR CC([INTEL MKL] Fix bug in MklSlice op when allocating output tensor.): Fix ambiguous constructor call in SourceTargetPairs initialization Imported from GitHub PR https://github.com/openxla/xla/pull/22822  Description Resolve a build failure (with GCC11) in `collective_permute_cycle_test` caused by an ambiguous constructor call when initializing `SourceTargetPairs` with an empty list (`{{}}`).    Issue   When calling `SourceTargetPairs({{}})`, the compiler could not determine whether to use the `std::vector>` constructor or the default copy/move constructors, leading to an error:   ``` xla/service/collective_permute_cycle_test.cc:130:48: error: call of overloaded 'SourceTargetPairs()' is ambiguous   130 |   EXPECT_EQ(GetCycleType(SourceTargetPairs({{}})), CycleType::kNone); ```  Solution 1. Explicitly define an `initializer_list` constructor for `SourceTargetPairs` to properly handle `{}` and `{{src, tgt}}` initializations.   2. Update the test case to use default ctor `SourceTargetPairs()` instead of `SourceTargetPairs({{}})`, ensuring clarity and correctness.   This fix ensures proper initialization and eliminates ambiguity Tested with GCC11 Copybara import of the project:  f97c38d47c8373ec609fdfbaedff3856f123fc33 by Alexander Pivovarov : Fix ambiguous constructor call in SourceTargetPairs initialization Merging this change closes CC([INTEL MKL] Fix bug in MklSlice op when allocating output tensor.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22822 from apivovarov:fix_source_target_pairs f97c38d47c8373ec609fdfbaedff3856f123fc33",2025-02-24T22:24:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87974
copybara-service[bot],[XLA:GPU] Give descriptive names to test case parameters.,[XLA:GPU] Give descriptive names to test case parameters. By default the parameterized test suites have numbers as names.,2025-02-24T21:52:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87973
t-kalinowski,`tf.random.stateless_uniform()` example broken," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.18, 2.19.0rc0  Custom code No  OS platform and distribution macOS  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Simple usage of `tf.random.stateless_uniform()` raises an exception if `tensorflowmetal` is installed and `minval` and `maxval` are supplied. https://www.tensorflow.org/api_docs/python/tf/random/stateless_uniform   Standalone code to reproduce the issue ```shell uv run python 3.11 \  with 'tensorflow==2.19.0rc0' \  with 'tensorflowmetal' \  python c \  'import tensorflow as tf; x=tf.random.stateless_uniform([10], seed = [2, 3], minval=tf.constant(0), maxval=tf.constant(10), dtype=tf.int32); print(x)' ```  Relevant log output ```shell tomaszWQVX ~ % uv run python 3.11 with 'tensorflow==2.19.0rc0' with 'tensorflowmetal' python c 'import tensorflow as tf; x=tf.random.stateless_uniform([10], seed = [2, 3], minval=tf.constant(0), maxval=tf.constant(10), dtype=tf.int32); print(x)' 20250224 16:35:02.793541: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Max 20250224 16:35:02.793566: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 128.00 GB 20250224 16:35:02.793573: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 48.00 GB WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1740432902.793592 6966705 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. I0000 00:00:1740432902.793860 6966705 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) > physical PluggableDevice (device: 0, name: METAL, pci bus id: ) 20250224 16:35:02.800931: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: minval must be 0D, got shape [2] Traceback (most recent call last):   File """", line 1, in    File ""/Users/tomasz/.cache/uv/archivev0/2UOghugvqv1rQRmVCTGy/lib/python3.11/sitepackages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/Users/tomasz/.cache/uv/archivev0/2UOghugvqv1rQRmVCTGy/lib/python3.11/sitepackages/tensorflow/python/framework/ops.py"", line 6006, in raise_from_not_ok_status     raise core._status_to_exception(e) from None   pylint: disable=protectedaccess     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StatelessRandomUniformIntV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} minval must be 0D, got shape [2] [Op:StatelessRandomUniformIntV2] name: ```",2025-02-24T21:36:49Z,stat:awaiting response type:bug stale TF 2.18,closed,0,6,https://github.com/tensorflow/tensorflow/issues/87972,"Hi **kalinowski** , Apologies for the delay, and thanks for raising your concern here. I tried running your code on tensorflowmetal using TensorFlow 2.18.0 and encountered the same issue. I am attaching the results here for your reference. ``` (tfenv) maayaramacbookpro:~ maayara$ python Python 3.11.11 (main, Dec  3 2024, 17:20:40) [Clang 16.0.0 (clang1600.0.26.4)] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf >>> print(tf.__version__) 2.18.0 >>> print(tf.config.list_physical_devices()) [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] >>> import tensorflow as tf; x=tf.random.stateless_uniform([10], seed = [2, 3], minval=tf.constant(0), maxval=tf.constant(10), dtype=tf.int32); print(x) 20250226 12:55:07.301577: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro 20250226 12:55:07.301700: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB 20250226 12:55:07.301729: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1740554707.302295 12433048 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. I0000 00:00:1740554707.302777 12433048 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) > physical PluggableDevice (device: 0, name: METAL, pci bus id: ) 20250226 12:55:07.337793: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: minval must be 0D, got shape [2] Traceback (most recent call last):   File """", line 1, in    File ""/Users/maayara/tfenv/lib/python3.11/sitepackages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/Users/maayara/tfenv/lib/python3.11/sitepackages/tensorflow/python/framework/ops.py"", line 6002, in raise_from_not_ok_status     raise core._status_to_exception(e) from None   pylint: disable=protectedaccess     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StatelessRandomUniformIntV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} minval must be 0D, got shape [2] [Op:StatelessRandomUniformIntV2] name:  ``` Since this issue is not related to the TensorFlow repository, I recommend raising it on the Apple Developer Forums for further assistance. Thank you!","Hi, I am unable to post on the apple forums.  Is there any coordination between the tensorflow and tensorflowmetal projects?","Hi **kalinowski** , Apologies for the delay, To post the issue on apple developer forum you will have to create apple account then only you are able to post the issue so I recommend you to please create apple account if you don't have it to get faster resolution for your issue : Apple Developer  TensorFlow Metal Plugin. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Fix Android ARM64 build.,"Fix Android ARM64 build. The llvm::StringRef to absl::string_view converter is not (always?) available on Android, so inserting StringRef::str() calls where necessary.",2025-02-24T21:02:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87971
copybara-service[bot],"Fix ""ops w/o operand and followed by quant accidentally matching dq-op-q patter""","Fix ""ops w/o operand and followed by quant accidentally matching dqopq patter""",2025-02-24T20:49:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87970
copybara-service[bot],[XLA] Use built-in environment variable to find paths,[XLA] Use builtin environment variable to find paths,2025-02-24T20:11:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87969
copybara-service[bot],Flag guard the option to disable embedding pipelining when summary ops are present,Flag guard the option to disable embedding pipelining when summary ops are present,2025-02-24T20:10:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87968
copybara-service[bot],Use tsl::profiler instead of tensorflow::profiler,Use tsl::profiler instead of tensorflow::profiler,2025-02-24T19:51:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87967
copybara-service[bot],Added GetJobState RPC to the coordination service.,Added GetJobState RPC to the coordination service.,2025-02-24T19:36:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87966
copybara-service[bot],Add TensorBuffer GL to CL conversion.,Add TensorBuffer GL to CL conversion.,2025-02-24T19:16:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87965
copybara-service[bot],"[hlo-opt] remove convert tool, all conversion options are already supported by hlo-opt.","[hloopt] remove convert tool, all conversion options are already supported by hloopt. hloproto > hlotext hloproto binary > hlotext hlotext > hloproto",2025-02-24T19:14:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87964
copybara-service[bot],"Make xla_test, etc, shuffle tests by default.","Make xla_test, etc, shuffle tests by default. This helps catch test order dependencies at presubmit time.",2025-02-24T18:59:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87963
copybara-service[bot],Reverts 52fc64b538c7291b8caa0de7b0bfdcf7762376e8,Reverts 52fc64b538c7291b8caa0de7b0bfdcf7762376e8,2025-02-24T18:53:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87962
copybara-service[bot],Move `immutabledict` install to the Dockerfile,Move `immutabledict` install to the Dockerfile,2025-02-24T18:52:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87961
copybara-service[bot],hlo/tools : move tests to dedicated /tests directory.,hlo/tools : move tests to dedicated /tests directory. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/83372 from cybersupersoap:transposecrashfix e9009cefdb30c243ae9b1c4b59c751756584063d,2025-02-24T18:35:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87960
copybara-service[bot],[xla:cpu] InProcessCommunicator: compute collective operations in parallel using all ranks,[xla:cpu] InProcessCommunicator: compute collective operations in parallel using all ranks,2025-02-24T18:30:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87959
copybara-service[bot],Move artifact packaging in the Docker image to fix dpkg error in android,Move artifact packaging in the Docker image to fix dpkg error in android,2025-02-24T18:27:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87958
copybara-service[bot],[XLA:GPU] Reset `CodedInputStream` after parsing each literal in the serialization of large snapshots.,"[XLA:GPU] Reset `CodedInputStream` after parsing each literal in the serialization of large snapshots. `CodedInputStream` has an internal int32 counter for total bytes read, limiting the bytes read by a single instance to 2 GiB. I've changed the deserialization implementation to parse each literal with a separate `CodedInputStream`. This fix still limits the *size of each literal* to 2 GiB.",2025-02-24T17:38:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87957
copybara-service[bot],Refactors FindDynamicIndex and adds some more tests.,Refactors FindDynamicIndex and adds some more tests.,2025-02-24T17:36:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87956
copybara-service[bot],Overwrite xla_dump_as_* options in raw_options only if raw_options.xla_dump_to is set. Otherwise keep debug_options settings.,"Overwrite xla_dump_as_* options in raw_options only if raw_options.xla_dump_to is set. Otherwise keep debug_options settings. This is needed to access the flags state in PjRtStreamExecutorLoadedExecutable::Execute. Specifically, I need to access dumping options in order to dump unoptimized hlo module with arguments during execution correctly.",2025-02-24T17:28:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87955
copybara-service[bot],[XLA:GPU][TMA] Add extract/insert lowering for the 0-d tensors.,[XLA:GPU][TMA] Add extract/insert lowering for the 0d tensors.,2025-02-24T17:24:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87954
copybara-service[bot],PR #22800: Change the default value of print_operand_shape_ to false and print_large_constants_ to true.,"PR CC(Variance Inflation Factor estimate for each layer as guidance for dropout): Change the default value of print_operand_shape_ to false and print_large_constants_ to true. Imported from GitHub PR https://github.com/openxla/xla/pull/22800 Operand shape in long hlo text adds redundant information, which shouldn't be required. Changing the default value to off. The large constants were also printed earlier by default print options, and it is required for parsability and reproducibility. Turning this on by default. This is still controlled by debug option and the default value of that flag disables the large constants, and that behavior is not changed. Just the default print options change here. Copybara import of the project:  e30dea20489b3fb4d03d373fec0391d69486f4aa by Shraiysh Vaishay : Change the default value of print_operand_shape_ to false and print_large_constants_ to true. Operand shape in long hlo text adds redundant information, which shouldn't be required. Changing the default value to off. The large constants were also printed earlier by default print options, and it is required for parsability and reproducibility. Turning this on by default. This is still controlled by debug option and the default value of that flag disables the large constants, and that behavior is not changed. Just the default print options change here.  7008af0dd0ce342ecbe9475f1d0e277319f1705a by Shraiysh Vaishay : Handle tests  b22d5f95cfb7e15f930a2198279a76c38593cc53 by Shraiysh Vaishay : Fix more tests  d51579cae7359c6426a87ad4a7ff1b4b0c80f74a by Shraiysh Vaishay : Fix more tests Merging this change closes CC(Variance Inflation Factor estimate for each layer as guidance for dropout) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22800 from shraiysh:change_default_print_op_shape d51579cae7359c6426a87ad4a7ff1b4b0c80f74a",2025-02-24T17:06:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87953
copybara-service[bot],Triton patches to fix compilation on Blackwell: ,"Triton patches to fix compilation on Blackwell:   Do not use ""ptxasblackwell"" when compiling for B200.  Fix MMAv5 precondition check to only allow warp sizes of 4 or 8",2025-02-24T16:58:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87952
copybara-service[bot],[XLA] Enable iota & tuple to be constant folded into ops.,[XLA] Enable iota & tuple to be constant folded into ops.,2025-02-24T16:46:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87951
copybara-service[bot],[XLA:GPU] Triton XLA: Lower 0-D tensors to scalars in Triton-XLA for Loads/Stores.,[XLA:GPU] Triton XLA: Lower 0D tensors to scalars in TritonXLA for Loads/Stores.,2025-02-24T16:23:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87950
copybara-service[bot],[XLA:MSA] Remove reference to internal names.,[XLA:MSA] Remove reference to internal names.,2025-02-24T15:48:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87949
copybara-service[bot],Integrate LLVM at llvm/llvm-project@c80b99d98ad0,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match c80b99d98ad0,2025-02-24T13:54:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87948
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T13:19:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87947
misterBart,TfLite. Fix of issue 61269,TfLite. Fix of issue https://github.com/tensorflow/tensorflow/issues/61269 and corresponding LiteRT issue https://github.com/googleaiedge/LiteRT/issues/165  Solves compile `error C2039: 'any_cast': is not a member of 'std'` with Visual Studio 2019 and 2022.,2025-02-24T13:11:42Z,awaiting review comp:lite size:XS,open,0,2,https://github.com/tensorflow/tensorflow/issues/87946,Any news? It's been two months ago and my PR is simple.,"Hi ,  Can you please review this PR? Thanks !"
copybara-service[bot],[XLA:GPU][TMA] Add an alias for TmaDescriptorAttr.,[XLA:GPU][TMA] Add an alias for TmaDescriptorAttr.,2025-02-24T12:52:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87945
copybara-service[bot],PR #22437: Added frontend attribute handling to explicit_stream_annotation_async_wrapper ,"PR CC(tensorflow1.11.0rc1 fails to build): Added frontend attribute handling to explicit_stream_annotation_async_wrapper  Imported from GitHub PR https://github.com/openxla/xla/pull/22437 This is a small change that ensures the frontend attributes are correctly passed to both the `asyncstart` and `asyncdone` created pairs. This also clears the scheduling attributes that are directly on the call operation and inner ops. The specific goal of this change is to have stable support combining the scheduling group ids with stream annotation in JAX.  ```python with set_xla_metadata(_scheduling_group_id=1):    result = compute_on(""gpu_stream:1"")(jitted_func)(...) ``` Currently, the issue stems from the `set_xla_metadata` context manager, which will apply the frontend attribute to all operations, including the ones within our `jitted_func`. When the same scheduling annotations is found in two `HloComputation`s, an error is raised in `LegalizeSchedulingAnnotations`. This is intended to avoid hitting this check, and cleaning up the annotations on the wrapped streamed computation. Copybara import of the project:  994c2eee3c946102270587681f5c17b994cbb6a9 by chaser : Added frontend attributed handling  9db58b2b988dc2288d42126271223f924aac19f9 by chaser : Added clearing of scheduling annotations  a83e32a34ba5d64a29c7f01b03536f27decd8125 by chaser : Added HloInstruction.erase_frontend_attribute Merging this change closes CC(tensorflow1.11.0rc1 fails to build) Reverts 52fc64b538c7291b8caa0de7b0bfdcf7762376e8 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22437 from chaserileyroberts:chase/frontend_forward_async_wrapper a83e32a34ba5d64a29c7f01b03536f27decd8125",2025-02-24T12:36:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87944
copybara-service[bot],Upgrade to Bazel 7.4.1 for TensorFlow,"Upgrade to Bazel 7.4.1 for TensorFlow  Disabled Bzlmod for now before we start the migration  Disabled  `modify_execution_info` due to https://github.com/bazelbuild/bazel/pull/16262  Explicitly added `Wl,undefined,dynamic_lookup` as linkopt on macOS after https://github.com/bazelbuild/bazel/pull/16414  Set `force_no_whole_archive` for host features  Addressed Windows linking issue by adding `//:__subpackages__` in `exports_filter` of `//tensorflow/python:pywrap_tensorflow_internal`   Removed `license` attribute on cc_shared_library  Fixed license checks",2025-02-24T11:23:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87943
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T11:19:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87942
copybara-service[bot],Update Ruy to latest version,Update Ruy to latest version,2025-02-24T11:08:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87941
sallyannedapoz90,I'm a spammer,Here  spams,2025-02-24T10:16:17Z,TFLiteConverter,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87940,Please don't spam
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T09:39:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87939
copybara-service[bot],Implements google_tensor Dispatch Metrics APIs,Implements google_tensor Dispatch Metrics APIs,2025-02-24T09:15:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87938
chuntl,Qualcomm AI Engine Direct - Add log utils for core module,Summary:  Implement default and android version of log utils for core module  Add test for log util  Use LogOff as default log level  Unify to use log util in core module,2025-02-24T08:51:40Z,ready to pull size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87937
copybara-service[bot],PR #22970: Fix bug in post order traversal of computation instructions,"PR CC(protobuf not seem to be recognized but it is installed): Fix bug in post order traversal of computation instructions Imported from GitHub PR https://github.com/openxla/xla/pull/22970 While creating post order traversal, an instruction may have a user outside the computation. This is the case when we are constructing new instructions to store in replacements for cloning the computation later. This user should be ignored. Added test for the same. Because of this, functions like `ToString()`, and `GetUniqueGteInstruction()` encounter errors. They rely on postorder traversal to have all the instructions. Copybara import of the project:  326469b7cab50e90616094dffe36758afef815e1 by Shraiysh Vaishay : Fix bug in post order traversal of computation instructions While creating post order traversal, an instruction may have a user outside the computation. This is the case when we are constructing new instructions to store in replacements for cloning the computation later. This user should be ignored. Added test for the same. Because of this, functions like `ToString()`, and `GetUniqueGteInstruction()` encounter errors. They rely on postorder traversal to have all the instructions. Merging this change closes CC(protobuf not seem to be recognized but it is installed) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22970 from shraiysh:fixpostordertraversal 326469b7cab50e90616094dffe36758afef815e1",2025-02-24T07:55:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87936
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T07:48:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87935
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T05:45:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87934
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T05:14:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87933
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T05:07:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87932
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T05:07:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87931
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T05:06:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87930
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T05:05:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87929
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T05:03:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87928
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T05:02:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87927
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T05:02:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87926
diddlywob,Help with reference," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 13.3.06ubuntu2~24.04) 13.3.0  CUDA/cuDNN version CUDA 12.0  GPU model and memory NVIDIA GeForce RTX 4050  Current behavior? I am expecting my model to run without giving any type of warnings. I know for certain that my data does not have any NaN values in it, so I am not sure why am I getting the warning I am receiving. My output should look something like the below: 2138/14403 ━━━━━━━━━━━━━━━━━━━━ 2:45 14ms/step  accuracy: 0.7590  loss: 1.1440E0000 00:00:1740370043.647457  Standalone code to reproduce the issue ```shell I know the issue has to do with this part of the code. I am just not sure where. import os os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async' import math import pyarrow.parquet as pq import pandas as pd import numpy as np import tensorflow as tf from tensorflow.keras.regularizers import l2 from scipy.sparse import hstack, csr_matrix from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import OneHotEncoder from itertools import combinations   Initialize Encoders & Scaler  onehot_encoder = OneHotEncoder(handle_unknown=""ignore"", sparse_output=False) scaler = StandardScaler()   Stream Train/Test Split in Chunks  def stream_split_data(parquet_files, categorical_columns, numeric_columns):     """"""Splits merged data into training and testing sets dynamically in chunks.""""""     for chunk in stream_merged_data(parquet_files, categorical_columns, numeric_columns, merge_key):         mask = np.random.rand(len(chunk)) = end_idx:                 return   Stop when we reach end index             X_batch = X_chunk[i:i + batch_size]             y_batch = y_chunk[i:i + batch_size]             yield X_batch, y_batch             current_idx += batch_size   Create `tf.data.Dataset` Using the Chunked Generator  train_dataset = tf.data.Dataset.from_generator(     lambda: preprocess_dense_batches(parquet_files, categorical_columns, numeric_columns, batch_size, end_idx=int(train_ratio * total_rows)),     output_signature=(         tf.TensorSpec(shape=(None, one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model)), dtype=tf.float32),         tf.TensorSpec(shape=(None,), dtype=tf.float32)     ) ).repeat().prefetch(buffer_size=tf.data.AUTOTUNE) test_dataset = tf.data.Dataset.from_generator(     lambda: preprocess_dense_batches(parquet_files, categorical_columns, numeric_columns, batch_size, end_idx=total_rows),     output_signature=(         tf.TensorSpec(shape=(None, one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model)), dtype=tf.float32),         tf.TensorSpec(shape=(None,), dtype=tf.float32)     ) ).prefetch(buffer_size=tf.data.AUTOTUNE) class_weight_dict = {     0: 1.0,      1: 2.0,       2: 3.0    } steps_per_epoch = math.ceil((train_ratio * total_rows) / batch_size) test_steps = math.ceil(((1  train_ratio) * total_rows) / batch_size) print(""Datasets created dynamically."") input_all_data = Input(shape=(one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model),), name='all_data_input') print(""Creating the dense layers..."") x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(input_all_data) x = BatchNormalization()(x) x = Dropout(0.3)(x) x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x) x = BatchNormalization()(x) x = Dropout(0.3)(x) output = Dense(3, activation='softmax')(x)   3 neurons, softmax for multiclass  Compile the model model = Model(inputs=input_all_data, outputs=output) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) print(""\nBegin training the model.\n"") history_all_data = model.fit(     train_dataset,                         validation_data=test_dataset,          epochs=1,                         steps_per_epoch=steps_per_epoch,     validation_steps=test_steps,     class_weight=class_weight_dict,   Pass class weights     verbose=1 ) ```  Relevant log output ```shell WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1740371136.641692   17996 service.cc:148] XLA service 0x7f9fc4004850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: I0000 00:00:1740371136.642763   17996 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9 20250223 23:25:36.704967: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. I0000 00:00:1740371136.874435   17996 cuda_dnn.cc:529] Loaded cuDNN version 90300 I0000 00:00:1740371138.351218   17996 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.  1490/14403 ━━━━━━━━━━━━━━━━━━━━ 3:33 17ms/step  accuracy: 0.5462  loss: 2.3473E0000 00:00:1740371164.392105   17996 buffer_comparator.cc:157] Difference at 0: nan, expected 1.65142 E0000 00:00:1740371164.392245   17996 buffer_comparator.cc:157] Difference at 1: nan, expected 1.48098 E0000 00:00:1740371164.392259   17996 buffer_comparator.cc:157] Difference at 2: nan, expected 1.67361 E0000 00:00:1740371164.392283   17996 buffer_comparator.cc:157] Difference at 3: nan, expected 3.16466 E0000 00:00:1740371164.392288   17996 buffer_comparator.cc:157] Difference at 4: nan, expected 2.17621 E0000 00:00:1740371164.392296   17996 buffer_comparator.cc:157] Difference at 5: nan, expected 3.38546 E0000 00:00:1740371164.392319   17996 buffer_comparator.cc:157] Difference at 6: nan, expected 1.67085 E0000 00:00:1740371164.392324   17996 buffer_comparator.cc:157] Difference at 7: nan, expected 1.38643 E0000 00:00:1740371164.392342   17996 buffer_comparator.cc:157] Difference at 8: nan, expected 1.76836 E0000 00:00:1740371164.392347   17996 buffer_comparator.cc:157] Difference at 9: nan, expected 3.48673 20250223 23:26:04.392363: E external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:987] Results do not match the reference. This is likely a bug/unexpected loss of precision.  2967/14403 ━━━━━━━━━━━━━━━━━━━━ 2:58 16ms/step  accuracy: 0.5747  loss: 2.0899E0000 00:00:1740371185.763887   17997 buffer_comparator.cc:157] Difference at 0: nan, expected 3.13696 E0000 00:00:1740371185.764129   17997 buffer_comparator.cc:157] Difference at 1: nan, expected 3.33544 E0000 00:00:1740371185.764148   17997 buffer_comparator.cc:157] Difference at 2: nan, expected 2.92697 E0000 00:00:1740371185.764155   17997 buffer_comparator.cc:157] Difference at 3: nan, expected 2.59307 E0000 00:00:1740371185.764161   17997 buffer_comparator.cc:157] Difference at 4: nan, expected 2.88114 E0000 00:00:1740371185.764169   17997 buffer_comparator.cc:157] Difference at 5: nan, expected 2.56242 E0000 00:00:1740371185.764175   17997 buffer_comparator.cc:157] Difference at 6: nan, expected 2.62881 E0000 00:00:1740371185.764181   17997 buffer_comparator.cc:157] Difference at 7: nan, expected 2.84552 E0000 00:00:1740371185.764234   17997 buffer_comparator.cc:157] Difference at 8: nan, expected 2.49929 E0000 00:00:1740371185.764246   17997 buffer_comparator.cc:157] Difference at 9: nan, expected 2.47989 20250223 23:26:25.764275: E external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:987] Results do not match the reference. This is likely a bug/unexpected loss of precision. ```",2025-02-24T05:00:27Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87925,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T04:59:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87924
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T04:56:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87923
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T04:55:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87922
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T04:54:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87921
copybara-service[bot],Internal change for visibility,Internal change for visibility,2025-02-24T04:42:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87920
diddlywob,Results Do Not Match Reference," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 13.3.06ubuntu2~24.04) 13.3.0  CUDA/cuDNN version CUDA 12.0  GPU model and memory NVIDIA GeForce RTX 4050  Current behavior? I am expecting my model to run without giving any type of warnings. I know for certain that my data does not have any NaN values in it, so I am not sure why am I getting the warning I am receiving. My output should look something like the below: 2138/14403 ━━━━━━━━━━━━━━━━━━━━ 2:45 14ms/step  accuracy: 0.7590  loss: 1.1440E0000 00:00:1740370043.647457  Standalone code to reproduce the issue ```shell import os os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async' import math import pyarrow.parquet as pq import pandas as pd import numpy as np import tensorflow as tf from tensorflow.keras.regularizers import l2 from scipy.sparse import hstack, csr_matrix from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import OneHotEncoder from itertools import combinations   Initialize Encoders & Scaler  onehot_encoder = OneHotEncoder(handle_unknown=""ignore"", sparse_output=False) scaler = StandardScaler()   Stream Train/Test Split in Chunks  def stream_split_data(parquet_files, categorical_columns, numeric_columns):     """"""Splits merged data into training and testing sets dynamically in chunks.""""""     for chunk in stream_merged_data(parquet_files, categorical_columns, numeric_columns, merge_key):         np.random.seed(42)   Ensures the split is always the same upon restarting         mask = np.random.rand(len(chunk)) = end_idx:                 return   Stop when we reach end index             X_batch = X_chunk[i:i + batch_size]             y_batch = y_chunk[i:i + batch_size]             yield X_batch, y_batch             current_idx += batch_size   Create `tf.data.Dataset` Using the Chunked Generator  train_dataset = tf.data.Dataset.from_generator(     lambda: preprocess_dense_batches(parquet_files, categorical_columns, numeric_columns, batch_size, end_idx=int(train_ratio * total_rows)),     output_signature=(         tf.TensorSpec(shape=(None, one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model)), dtype=tf.float32),         tf.TensorSpec(shape=(None,), dtype=tf.float32)     ) ).repeat().prefetch(buffer_size=tf.data.AUTOTUNE) test_dataset = tf.data.Dataset.from_generator(     lambda: preprocess_dense_batches(parquet_files, categorical_columns, numeric_columns, batch_size, end_idx=total_rows),     output_signature=(         tf.TensorSpec(shape=(None, one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model)), dtype=tf.float32),         tf.TensorSpec(shape=(None,), dtype=tf.float32)     ) ).prefetch(buffer_size=tf.data.AUTOTUNE)   Class Weights Handling  class_weight_dict = {     0: 1.0,   Pushes (0.5)     1: 2.0,   Misses (0)     2: 3.0    Hits (1) }   Calculate Steps per Epoch  steps_per_epoch = math.ceil((train_ratio * total_rows) / batch_size) test_steps = math.ceil(((1  train_ratio) * total_rows) / batch_size) print(""Datasets created dynamically."")   Define Your Neural Network (Unchanged)  input_all_data = Input(shape=(one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model),), name='all_data_input') print(""Creating the dense layers..."")  Create the dense layers x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(input_all_data) x = BatchNormalization()(x) x = Dropout(0.3)(x) x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x) x = BatchNormalization()(x) x = Dropout(0.3)(x) output = Dense(3, activation='softmax')(x)   3 neurons, softmax for multiclass print(""Compiling the model..."")  Compile the model model = Model(inputs=input_all_data, outputs=output) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])   Train Model Using Existing Training Loop  print(""\nBegin training the model.\n"") history_all_nfl_data = model.fit(     train_dataset,                     Training dataset     validation_data=test_dataset,      Testing dataset     epochs=1,                         steps_per_epoch=steps_per_epoch,     validation_steps=test_steps,     class_weight=class_weight_dict,   Pass class weights     verbose=1 ) ```  Relevant log output ```shell WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1740371136.641692   17996 service.cc:148] XLA service 0x7f9fc4004850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: I0000 00:00:1740371136.642763   17996 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9 20250223 23:25:36.704967: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. I0000 00:00:1740371136.874435   17996 cuda_dnn.cc:529] Loaded cuDNN version 90300 I0000 00:00:1740371138.351218   17996 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.  1490/14403 ━━━━━━━━━━━━━━━━━━━━ 3:33 17ms/step  accuracy: 0.5462  loss: 2.3473E0000 00:00:1740371164.392105   17996 buffer_comparator.cc:157] Difference at 0: nan, expected 1.65142 E0000 00:00:1740371164.392245   17996 buffer_comparator.cc:157] Difference at 1: nan, expected 1.48098 E0000 00:00:1740371164.392259   17996 buffer_comparator.cc:157] Difference at 2: nan, expected 1.67361 E0000 00:00:1740371164.392283   17996 buffer_comparator.cc:157] Difference at 3: nan, expected 3.16466 E0000 00:00:1740371164.392288   17996 buffer_comparator.cc:157] Difference at 4: nan, expected 2.17621 E0000 00:00:1740371164.392296   17996 buffer_comparator.cc:157] Difference at 5: nan, expected 3.38546 E0000 00:00:1740371164.392319   17996 buffer_comparator.cc:157] Difference at 6: nan, expected 1.67085 E0000 00:00:1740371164.392324   17996 buffer_comparator.cc:157] Difference at 7: nan, expected 1.38643 E0000 00:00:1740371164.392342   17996 buffer_comparator.cc:157] Difference at 8: nan, expected 1.76836 E0000 00:00:1740371164.392347   17996 buffer_comparator.cc:157] Difference at 9: nan, expected 3.48673 20250223 23:26:04.392363: E external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:987] Results do not match the reference. This is likely a bug/unexpected loss of precision.  2967/14403 ━━━━━━━━━━━━━━━━━━━━ 2:58 16ms/step  accuracy: 0.5747  loss: 2.0899E0000 00:00:1740371185.763887   17997 buffer_comparator.cc:157] Difference at 0: nan, expected 3.13696 E0000 00:00:1740371185.764129   17997 buffer_comparator.cc:157] Difference at 1: nan, expected 3.33544 E0000 00:00:1740371185.764148   17997 buffer_comparator.cc:157] Difference at 2: nan, expected 2.92697 E0000 00:00:1740371185.764155   17997 buffer_comparator.cc:157] Difference at 3: nan, expected 2.59307 E0000 00:00:1740371185.764161   17997 buffer_comparator.cc:157] Difference at 4: nan, expected 2.88114 E0000 00:00:1740371185.764169   17997 buffer_comparator.cc:157] Difference at 5: nan, expected 2.56242 E0000 00:00:1740371185.764175   17997 buffer_comparator.cc:157] Difference at 6: nan, expected 2.62881 E0000 00:00:1740371185.764181   17997 buffer_comparator.cc:157] Difference at 7: nan, expected 2.84552 E0000 00:00:1740371185.764234   17997 buffer_comparator.cc:157] Difference at 8: nan, expected 2.49929 E0000 00:00:1740371185.764246   17997 buffer_comparator.cc:157] Difference at 9: nan, expected 2.47989 20250223 23:26:25.764275: E external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:987] Results do not match the reference. This is likely a bug/unexpected loss of precision. ```",2025-02-24T04:41:34Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87919,Are you satisfied with the resolution of your issue? Yes No
diddlywob,Results do not match the reference. This is likely a bug/unexpected loss of precision.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 13.3.06ubuntu2~24.04) 13.3.0  CUDA/cuDNN version CUDA 12.0  GPU model and memory NVIDIA GeForce RTX 4050  Current behavior? I am simply expecting my model to run without giving any type of warnings. I know for certain that my data does not have any NaN values in it, so I am not sure why am I getting the warning I am receiving. My output should look something like the below:  2138/14403 ━━━━━━━━━━━━━━━━━━━━ 2:45 14ms/step  accuracy: 0.7590  loss: 1.1440E0000 00:00:1740370043.647457    Standalone code to reproduce the issue ```shell import os os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async' import math import pyarrow.parquet as pq import pandas as pd import numpy as np import tensorflow as tf from tensorflow.keras.regularizers import l2 from scipy.sparse import hstack, csr_matrix from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import OneHotEncoder from itertools import combinations   Initialize Encoders & Scaler  onehot_encoder = OneHotEncoder(handle_unknown=""ignore"", sparse_output=False) scaler = StandardScaler()   Stream Train/Test Split in Chunks  def stream_split_data(parquet_files, categorical_columns, numeric_columns):     """"""Splits merged data into training and testing sets dynamically in chunks.""""""     for chunk in stream_merged_data(parquet_files, categorical_columns, numeric_columns, merge_key):         np.random.seed(42)   Ensures the split is always the same upon restarting         mask = np.random.rand(len(chunk)) = end_idx:                 return   Stop when we reach end index             X_batch = X_chunk[i:i + batch_size]             y_batch = y_chunk[i:i + batch_size]             yield X_batch, y_batch             current_idx += batch_size   Create `tf.data.Dataset` Using the Chunked Generator  train_dataset = tf.data.Dataset.from_generator(     lambda: preprocess_dense_batches(parquet_files, categorical_columns, numeric_columns, batch_size, end_idx=int(train_ratio * total_rows)),     output_signature=(         tf.TensorSpec(shape=(None, one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model)), dtype=tf.float32),         tf.TensorSpec(shape=(None,), dtype=tf.float32)     ) ).repeat().prefetch(buffer_size=tf.data.AUTOTUNE) test_dataset = tf.data.Dataset.from_generator(     lambda: preprocess_dense_batches(parquet_files, categorical_columns, numeric_columns, batch_size, end_idx=total_rows),     output_signature=(         tf.TensorSpec(shape=(None, one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model)), dtype=tf.float32),         tf.TensorSpec(shape=(None,), dtype=tf.float32)     ) ).prefetch(buffer_size=tf.data.AUTOTUNE)   Class Weights Handling  class_weight_dict = {     0: 1.0,   Pushes (0.5)     1: 2.0,   Misses (0)     2: 3.0    Hits (1) }   Calculate Steps per Epoch  steps_per_epoch = math.ceil((train_ratio * total_rows) / batch_size) test_steps = math.ceil(((1  train_ratio) * total_rows) / batch_size) print(""Datasets created dynamically."")   Define Your Neural Network (Unchanged)  input_all_data = Input(shape=(one_hot_encoder.transform(pd.DataFrame([[""0""] * len(categorical_columns)], columns=categorical_columns)).shape[1] + len(numeric_columns_for_model),), name='all_data_input') print(""Creating the dense layers..."")  Create the dense layers x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(input_all_data) x = BatchNormalization()(x) x = Dropout(0.3)(x) x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x) x = BatchNormalization()(x) x = Dropout(0.3)(x) output = Dense(3, activation='softmax')(x)   3 neurons, softmax for multiclass classification (hit, miss, push) print(""Compiling the model..."")  Compile the model model = Model(inputs=input_all_nfl_data, outputs=output) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])   Train Model Using Existing Training Loop  print(""\nBegin training the model.\n"") history_all_nfl_data = model.fit(     train_dataset,                     Training dataset     validation_data=test_dataset,      Testing dataset     epochs=1,                         steps_per_epoch=steps_per_epoch,     validation_steps=test_steps,     class_weight=class_weight_dict,   Pass class weights     verbose=1 ) ```  Relevant log output ```shell WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1740371136.641692   17996 service.cc:148] XLA service 0x7f9fc4004850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: I0000 00:00:1740371136.642763   17996 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9 20250223 23:25:36.704967: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. I0000 00:00:1740371136.874435   17996 cuda_dnn.cc:529] Loaded cuDNN version 90300 I0000 00:00:1740371138.351218   17996 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.  1490/14403 ━━━━━━━━━━━━━━━━━━━━ 3:33 17ms/step  accuracy: 0.5462  loss: 2.3473E0000 00:00:1740371164.392105   17996 buffer_comparator.cc:157] Difference at 0: nan, expected 1.65142 E0000 00:00:1740371164.392245   17996 buffer_comparator.cc:157] Difference at 1: nan, expected 1.48098 E0000 00:00:1740371164.392259   17996 buffer_comparator.cc:157] Difference at 2: nan, expected 1.67361 E0000 00:00:1740371164.392283   17996 buffer_comparator.cc:157] Difference at 3: nan, expected 3.16466 E0000 00:00:1740371164.392288   17996 buffer_comparator.cc:157] Difference at 4: nan, expected 2.17621 E0000 00:00:1740371164.392296   17996 buffer_comparator.cc:157] Difference at 5: nan, expected 3.38546 E0000 00:00:1740371164.392319   17996 buffer_comparator.cc:157] Difference at 6: nan, expected 1.67085 E0000 00:00:1740371164.392324   17996 buffer_comparator.cc:157] Difference at 7: nan, expected 1.38643 E0000 00:00:1740371164.392342   17996 buffer_comparator.cc:157] Difference at 8: nan, expected 1.76836 E0000 00:00:1740371164.392347   17996 buffer_comparator.cc:157] Difference at 9: nan, expected 3.48673 20250223 23:26:04.392363: E external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:987] Results do not match the reference. This is likely a bug/unexpected loss of precision.  2967/14403 ━━━━━━━━━━━━━━━━━━━━ 2:58 16ms/step  accuracy: 0.5747  loss: 2.0899E0000 00:00:1740371185.763887   17997 buffer_comparator.cc:157] Difference at 0: nan, expected 3.13696 E0000 00:00:1740371185.764129   17997 buffer_comparator.cc:157] Difference at 1: nan, expected 3.33544 E0000 00:00:1740371185.764148   17997 buffer_comparator.cc:157] Difference at 2: nan, expected 2.92697 E0000 00:00:1740371185.764155   17997 buffer_comparator.cc:157] Difference at 3: nan, expected 2.59307 E0000 00:00:1740371185.764161   17997 buffer_comparator.cc:157] Difference at 4: nan, expected 2.88114 E0000 00:00:1740371185.764169   17997 buffer_comparator.cc:157] Difference at 5: nan, expected 2.56242 E0000 00:00:1740371185.764175   17997 buffer_comparator.cc:157] Difference at 6: nan, expected 2.62881 E0000 00:00:1740371185.764181   17997 buffer_comparator.cc:157] Difference at 7: nan, expected 2.84552 E0000 00:00:1740371185.764234   17997 buffer_comparator.cc:157] Difference at 8: nan, expected 2.49929 E0000 00:00:1740371185.764246   17997 buffer_comparator.cc:157] Difference at 9: nan, expected 2.47989 20250223 23:26:25.764275: E external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:987] Results do not match the reference. This is likely a bug/unexpected loss of precision. ```",2025-02-24T04:27:53Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87918,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Fix cpu/gpu benchmarks github workflows to run on steps correctly.,Fix cpu/gpu benchmarks github workflows to run on steps correctly.,2025-02-24T04:06:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87917
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T03:23:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87916
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T03:12:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87915
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-24T03:06:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87914
0gur1,A heap oob write in TensorArray.write," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf2.18.0  Custom code Yes  OS platform and distribution Ubuntu 22.04.5  Mobile device _No response_  Python version Python 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In a model, if there is a lambda layer containing the TensorArray.write operation, the model service will crash after sending data for inference.  Standalone code to reproduce the issue ```shell In a python code, we can varify the vulnerability quickly with the following poc. import tensorflow as tf .function() def foo():   ta = tf.TensorArray(tf.float32, size=10,dynamic_size=True, clear_after_read=False)   ta = ta.write(tf.cast(0xffffffff, tf.int32),1)   return ta.read(1) a=foo() print(a) ```  Relevant log output ```shell  crash Thread 1 ""python"" received signal SIGSEGV, Segmentation fault. 0x00007fffebfa5560 in tensorflow::TensorListSetItem::Compute(tensorflow::OpKernelContext*) () from /home/test/ai/kerash5/tfvenv/lib/python3.10/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 (gdb) bt CC(未找到相关数据)  0x00007fffebfa5560 in tensorflow::TensorListSetItem::Compute(tensorflow::OpKernelContext*) ()    from /home/test/tfvenv/lib/python3.10/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 CC(Add support for Python 3.x)  0x00007ffff563d0b9 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::SimplePropagatorState::TaggedNode const&, long) ()    from /home/test/tfvenv/lib/python3.10/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 CC(OSX Yosemite ""can't determine number of CPU cores: assuming 4"")  0x00007ffff566c844 in std::_Function_handler), tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector, std::allocator >, tensorflow::Tensor>, std::allocator, std::allocator >, tensorflow::Tensor> > > const&, std::vector, std::allocator >, std::allocator, std::allocator > > > const&, std::vector >*)::$_1>::_M_invoke(std::_Any_data const&, std::function&&) ()    from /home/test/tfvenv/lib/python3.10/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 CC(JVM, .NET Language Support)  0x00007ffff563bbd7 in tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(absl::lts_20230802::InlinedVector >*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*) ()    from /home/test/tfvenv/lib/python3.10/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 CC(Installation over pip fails to import with protobuf 2.6.1)  0x00007ffff563eb8e in tensorflow::(anonymous namespace)::ExecutorState::NodeDone(absl::lts_20230802::Status const&, absl::lts_20230802::InlinedVector >*, tensorflow::NodeExecStatsInterface*, tensorflow::SimplePropagatorState::TaggedNodeRType  for more, q to quit, c to continue without pagingq Quit (gdb) x/i $rip => 0x7fffebfa5560 :	lock decq 0x8(%rbx) (gdb) i r rbx rbx            0x151               337 (gdb) x/8gx 0x5555577db8700x20 0x5555577db850:	0x0000555559a09268	0x0000015555d278d8 0x5555577db860:	0x0000000000000001	【0x0000000000000151】>rbx 0x5555577db870:	0x0000000000000000	0x0001003000000003 0x5555577db880:	0x0000000000000000	0x0000000000000000  oob write It will write a Tensor before the vector address(0x5555577db870).  r12 is the start of vector, r15 is the index and r14 is the src Tensor which will be written to the index 0x7fffebfa550b :	lea    (%r12,%r15,1),%rbx 0x7fffebfa550f :	movzbl 0xd(%r14),%r13d (gdb) x/4gx $r14 0x5555599e3170:	0x00005555599ea3f8	0x0000015558132bf8 0x5555599e3180:	0x0000000000000001	0x00005555596e7370 (gdb) i r r12 r12            0x5555577db870      93825028438128 (gdb) i r r15 r15            0xffffffffffffffe0  32 before writing (gdb) x/4gx $rbx 0x5555577db850:	0x0000000000000000	0x0000000000000000 0x5555577db860:	0x00007fffaf2fb1e0	0x0000000000000151 copy contents of r14 to an oob address rbx 0x7fffebfa5514 :	mov    0x10(%r14),%rax 0x7fffebfa5518 :	mov    %rax,0x10(%r12,%r15,1) ... 0x7fffebfa552c :	vmovups (%r14),%xmm0 0x7fffebfa5531 :	vmovups %xmm0,(%rbx) after writing (gdb) x/4gx 0x5555577db8700x20 0x5555577db850:	0x00005555599ea3f8	0x0000015558132bf8 0x5555577db860:	0x0000000000000001	0x0000000000000151 ```",2025-02-24T02:27:14Z,type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/87912,"Hi **** , Thanks for reaching out! The issue is caused by the large index value, which exceeds memory bounds. Using excessively large indices can lead to crashes due to memory allocation failures. Could you try using a reasonable index value? It should work correctly. I tested it on my end, and it worked fine. For your reference, I am attaching a gist with a working example. Thank you!","Hi, Thanks for your reply. I get your point to use a  resonable index value. And I create this issue just to illustrate the potential security issues which may be exploited by attackers. We cannot expect users to always perform reasonable actions. The root cause lies in https://github.com/tensorflow/tensorflow/blob/6550e4bd80223cdb8be6c3afd1f81e86a4d433c3/tensorflow/core/kernels/list_kernels.ccL483 `resize_if_index_out_of_bounds_ comes` from `dynamic_size`. If `dynmic_size` is set to True, it will come to [1]. Since `l>tensors().size()` is an unsigned integer, index will be converted to an unsigned int and match the condition with a value of `0xffffffff`. At [2], it will resize the vector to 0. Finally it writes to index 1 at [3], which will lead to a heap oob write operation. ```     TensorList* output_list = nullptr;     OP_REQUIRES_OK(c, ForwardInputOrCreateNewList(c, 0, 0, *l, &output_list));     int32_t index = c>input(1).scalar()();     if (!resize_if_index_out_of_bounds_) {       OP_REQUIRES(c, index tensors().size(),                   errors::InvalidArgument(""Trying to modify element "", index,                                           "" in a list with "",                                           l>tensors().size(), "" elements.""));     } else if (index >= l>tensors().size()) { //>[1]       output_list>tensors().resize(index + 1, Tensor(DT_INVALID));//>[2]     }     output_list>tensors()[index] = value; //>[3] ``` So a validation for the return value of `resize` may solve the problem in my opinion."
dennys,Should I continue using TF-TRT (TensorRT in TensorFlow) after 2.18?, Issue type Others  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.17.1  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I just upgrade to TensorFlow 2.17.1 and find TensorRT support is disabled in CUDA builds since TensorFlow 2.18. And TFTRT (TensorRT in TensorFlow) is archived on 2025/2/5. I'm not sure should I continue using TFTRT in the future? Or I should use native TensorRT?  Standalone code to reproduce the issue ```shell N/A ```  Relevant log output ```shell ```,2025-02-24T00:59:50Z,stat:awaiting response type:others 2.17,closed,0,3,https://github.com/tensorflow/tensorflow/issues/87911,"Hi **** , Thanks for raising your concern here. As per the official documentation, TensorRT support has been disabled in CUDA builds starting from TensorFlow 2.18.0. Regarding whether to continue using TFTRT or switch to native TensorRT, it depends on your use case: If you are working with TensorFlow models and need TensorRT acceleration, you will need to use TensorFlow 2.17 or earlier, as these versions still support TFTRT. However, this means you would not benefit from newer TensorFlow features. If you prioritize TensorRT optimization and performance, switching to native TensorRT could be a better choice. For more details, please refer to the official documentation, which I have attached for your reference. Thank you!","Got it, I think I should migrate to native TensorRT, thanks for your feedback.",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Use `addressable_devices_` instead of `devices_` in case of the multi-host environment.,Use `addressable_devices_` instead of `devices_` in case of the multihost environment.,2025-02-24T00:51:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87910
copybara-service[bot],Clear out XLA computations from compilation caches after finalizing the TensorFlow session.,"Clear out XLA computations from compilation caches after finalizing the TensorFlow session. After compiling a TF Graph into an XLA HLO program and after compiling the HLO into an executable, we keep around a `std::shared_ptrcomputation`. When the compiled HLO contains many constants, its heap memory consumption is significant and otherwise unreferenced after initialization. This CL adds an entrypoint `DeviceCompilationCache::Finalize`, which is exposed as `DeviceCompiler::Finalize`, which is an implementation of the virtual function `ResourceBase::Finalize`. `ResourceBase::Finalize` returns `absl::AnyInvocable` so that we can defer destruction of finalized objects owned by `ResourceBase` until after we release the lock `ResourceMgr::mu_`.",2025-02-24T00:17:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87909
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T22:37:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87908
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T20:38:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87907
copybara-service[bot],[xla:cpu] Align KernelArgs to enable aligned moves on a hot path,[xla:cpu] Align KernelArgs to enable aligned moves on a hot path ``` name                                     old cpu/op   new cpu/op   delta BM_SelectAndScatterF32/128/process_time   318µs ± 2%   306µs ± 2%  3.62%  (p=0.000 n=38+38) BM_SelectAndScatterF32/256/process_time  1.28ms ± 1%  1.23ms ± 2%  4.24%  (p=0.000 n=39+35) BM_SelectAndScatterF32/512/process_time  5.75ms ± 2%  5.57ms ± 2%  3.06%  (p=0.000 n=35+36) name                                     old time/op          new time/op          delta BM_SelectAndScatterF32/128/process_time   318µs ± 2%           307µs ± 2%  3.66%  (p=0.000 n=38+40) BM_SelectAndScatterF32/256/process_time  1.28ms ± 1%          1.23ms ± 2%  4.19%  (p=0.000 n=39+37) BM_SelectAndScatterF32/512/process_time  5.39ms ± 1%          5.21ms ± 2%  3.41%  (p=0.000 n=38+38) ```,2025-02-23T19:11:04Z,ready to pull,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87906,> [xla:cpu] Align KernelArgs to enable aligned moves on a hot path >  > ``` > name                                     old cpu/op   new cpu/op   delta > BM_SelectAndScatterF32/128/process_time   318µs ± 2%   306µs ± 2%  3.62%  (p=0.000 n=38+38) > BM_SelectAndScatterF32/256/process_time  1.28ms ± 1%  1.23ms ± 2%  4.24%  (p=0.000 n=39+35) > BM_SelectAndScatterF32/512/process_time  5.75ms ± 2%  5.57ms ± 2%  3.06%  (p=0.000 n=35+36) >  > name                                     old time/op          new time/op          delta > BM_SelectAndScatterF32/128/process_time   318µs ± 2%           307µs ± 2%  3.66%  (p=0.000 n=38+40) > BM_SelectAndScatterF32/256/process_time  1.28ms ± 1%          1.23ms ± 2%  4.19%  (p=0.000 n=39+37) > BM_SelectAndScatterF32/512/process_time  5.39ms ± 1%          5.21ms ± 2%  3.41%  (p=0.000 n=38+38) > ``` > 
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T18:53:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87905
copybara-service[bot],Make xla_cc_test default to shuffling test cases.,Make xla_cc_test default to shuffling test cases. This helps catch test case order dependencies at presubmit time.,2025-02-23T17:56:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87904
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T17:04:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87903
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T15:21:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87902
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T13:33:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87901
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T11:33:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87900
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T09:46:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87899
copybara-service[bot],Enable Integration Testing for Google Tensor Compiler Plugin,Enable Integration Testing for Google Tensor Compiler Plugin,2025-02-23T09:19:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87898
copybara-service[bot],Add tfrt gpu pjrt client,Add tfrt gpu pjrt client,2025-02-23T09:19:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87897
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T08:15:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87896
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T07:41:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87895
copybara-service[bot],Integrate LLVM at llvm/llvm-project@cf50936b23ac,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match cf50936b23ac,2025-02-23T06:51:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87894
JacksonDivakar,Inconsistent Behavior When Using tf.keras.metrics.Accuracy with F1-Score and Precision," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 24.0  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? !Image !Image !Image  Standalone code to reproduce the issue ```shell 1)Compile a model using tf.keras.metrics.Accuracy() along with custom F1score and precision metrics. 2)Observe the incorrect behavior in the reported accuracy. 3)Compare with behavior when using ""accuracy"" in the metrics list. Link : https://github.com/JacksonDivakarProjects/Others/blob/main/accuracy_issue%20(2).ipynb ```  Relevant log output ```shell ```",2025-02-23T06:34:05Z,stat:awaiting response type:bug stale type:others comp:keras TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/87893,"Hi **** , Please post this issue on kerasteam/keras repo. as this issue is more related to keras Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T06:29:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87892
copybara-service[bot],[IFRT] Change `tsl::RCReference<xla::ifrt::DeviceList>` to `xla::ifrt::DeviceListRef`,"[IFRT] Change `tsl::RCReference` to `xla::ifrt::DeviceListRef` IFRT will introduce a wrapper around `tsl::RCReference` that is more concise and is also safer to use (e.g., equality and hash compares the dereferenced value, not the `tsl::RCReference`. This migration will happen in 3 steps: 1. Define `xla::ifrt::DeviceListRef` alias that is interchangeable with `tsl::RCReference`. 2. Change `tsl::RCReference` to `xla::ifrt::DeviceListRef` in IFRT API, implementations, and user code. (current step) 3. Introduce a real wrapper `xla::ifrt::DeviceListRef`, replacing the alias.",2025-02-23T06:02:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87891
copybara-service[bot],[IFRT] Change `tsl::RCReference<xla::ifrt::DeviceList>` to `xla::ifrt::DeviceListRef`,"[IFRT] Change `tsl::RCReference` to `xla::ifrt::DeviceListRef` IFRT will introduce a wrapper around `tsl::RCReference` that is more concise and is also safer to use (e.g., equality and hash compares the dereferenced value, not the `tsl::RCReference`. This migration will happen in 3 steps: 1. Define `xla::ifrt::DeviceListRef` alias that is interchangeable with `tsl::RCReference`. 2. Change `tsl::RCReference` to `xla::ifrt::DeviceListRef` in IFRT API, implementations, and user code. (current step) 3. Introduce a real wrapper `xla::ifrt::DeviceListRef`, replacing the alias.",2025-02-23T05:57:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87890
copybara-service[bot],[IFRT] Change `tsl::RCReference<xla::ifrt::DeviceList>` to `xla::ifrt::DeviceListRef`,"[IFRT] Change `tsl::RCReference` to `xla::ifrt::DeviceListRef` IFRT will introduce a wrapper around `tsl::RCReference` that is more concise and is also safer to use (e.g., equality and hash compares the dereferenced value, not the `tsl::RCReference`. This migration will happen in 3 steps: 1. Define `xla::ifrt::DeviceListRef` alias that is interchangeable with `tsl::RCReference`. 2. Change `tsl::RCReference` to `xla::ifrt::DeviceListRef` in IFRT API, implementations, and user code. (current step) 3. Introduce a real wrapper `xla::ifrt::DeviceListRef`, replacing the alias.",2025-02-23T05:54:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87889
copybara-service[bot],[IFRT] Change `tsl::RCReference<xla::ifrt::DeviceList>` to `xla::ifrt::DeviceListRef`,"[IFRT] Change `tsl::RCReference` to `xla::ifrt::DeviceListRef` IFRT will introduce a wrapper around `tsl::RCReference` that is more concise and is also safer to use (e.g., equality and hash compares the dereferenced value, not the `tsl::RCReference`. This migration will happen in 3 steps: 1. Define `xla::ifrt::DeviceListRef` alias that is interchangeable with `tsl::RCReference`. 2. Change `tsl::RCReference` to `xla::ifrt::DeviceListRef` in IFRT API, implementations, and user code. (current step) 3. Introduce a real wrapper `xla::ifrt::DeviceListRef`, replacing the alias.",2025-02-23T05:48:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87888
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T04:40:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87887
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T04:35:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87886
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T04:33:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87885
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T04:28:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87884
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T04:27:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87883
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T04:26:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87882
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-23T04:25:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87881
copybara-service[bot],Reverts 52fc64b538c7291b8caa0de7b0bfdcf7762376e8,Reverts 52fc64b538c7291b8caa0de7b0bfdcf7762376e8 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22292 from openxla:cudnn_explicit_cuda_graph c03beef9515c0198d6eb1518b10a483b6a1b9c41,2025-02-22T22:51:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87880
copybara-service[bot],Internal change of visibility,Internal change of visibility,2025-02-22T20:22:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87879
copybara-service[bot],[PJRT:GPU] Fix typo in error message.,[PJRT:GPU] Fix typo in error message.,2025-02-22T19:59:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87878
Aren1414,Create stock_price_prediction.py,"Added new examples for stock price prediction using LSTM, MNIST recognition, and CNN image classification",2025-02-22T19:43:09Z,size:S,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87877,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Add components needed by the new tfrt gpu pjrt client,Add components needed by the new tfrt gpu pjrt client,2025-02-22T17:51:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87876
copybara-service[bot],[IFRT] Change `tsl::RCReference<xla::ifrt::DeviceList>` to `xla::ifrt::DeviceListRef`,"[IFRT] Change `tsl::RCReference` to `xla::ifrt::DeviceListRef` IFRT will introduce a wrapper around `tsl::RCReference` that is more concise and is also safer to use (e.g., equality and hash compares the dereferenced value, not the `tsl::RCReference`. This migration will happen in 3 steps: 1. Define `xla::ifrt::DeviceListRef` alias that is interchangeable with `tsl::RCReference`. 2. Change `tsl::RCReference` to `xla::ifrt::DeviceListRef` in IFRT API, implementations, and user code. (current step) 3. Introduce a real wrapper `xla::ifrt::DeviceListRef`, replacing the alias.",2025-02-22T16:33:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87875
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T15:04:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87874
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T10:03:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87873
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T09:52:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87872
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T08:00:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87871
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T07:54:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87870
copybara-service[bot],[xla:cpu] Remove TraceMe annotations from individual thunks,"[xla:cpu] Remove TraceMe annotations from individual thunks Because all thunks in XLA:CPU are nonblocking, TraceMe annotations are super confusing as they only capture the time that it takes to launch async tasks into the thread pool. Instead ThunkExecutor should use TraceMe producers/consumers to correctly profile async thunks execution. Also it's very easy to forget to add TraceMe to individual thunks (judging by the fact that not all thunks have TraceMe annotations), and it's better to annotate them in a single place.",2025-02-22T06:47:07Z,kokoro:force-run ready to pull,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87869
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T05:23:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87868
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T05:02:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87867
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T04:51:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87866
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T04:47:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87865
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T04:39:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87864
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T04:34:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87863
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T04:33:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87862
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T04:32:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87861
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T04:32:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87860
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T04:31:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87859
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T04:30:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87858
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T04:28:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87857
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T04:24:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87856
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T04:21:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87855
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-22T04:10:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87854
copybara-service[bot],Integrate the compiler flags into the tooling,Integrate the compiler flags into the tooling,2025-02-22T04:00:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87853
copybara-service[bot],[IFRT] Introduce a wrapper for `tsl::RCReference<T>` and update `xla::ifrt::DeviceListRef`,"[IFRT] Introduce a wrapper for `tsl::RCReference` and update `xla::ifrt::DeviceListRef` IFRT will introduce a wrapper around `tsl::RCReference` that is more concise and is also safer to use (e.g., equality and hash compares the dereferenced value, not the `tsl::RCReference`. This migration will happen in 3 steps: 1. Define `xla::ifrt::DeviceListRef` alias that is interchangeable with `tsl::RCReference`. 2. Change `tsl::RCReference` to `xla::ifrt::DeviceListRef` in IFRT API, implementations, and user code. 3. Introduce a real wrapper `xla::ifrt::DeviceListRef`, replacing the alias. (current step) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/87759 from tensorflow:gaikwadrahul8patch1 adaffbab3e9cc85b362614065e6ae9a022256f02",2025-02-22T03:52:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87852
copybara-service[bot],"Add to/from string for compiler flags. Also move to compiler/plugin, this doesn't belong in vendor code.","Add to/from string for compiler flags. Also move to compiler/plugin, this doesn't belong in vendor code.",2025-02-22T03:52:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87851
copybara-service[bot],[IFRT] Change `tsl::RCReference<xla::ifrt::DeviceList>` to `xla::ifrt::DeviceListRef`,"[IFRT] Change `tsl::RCReference` to `xla::ifrt::DeviceListRef` IFRT will introduce a wrapper around `tsl::RCReference` that is more concise and is also safer to use (e.g., equality and hash compares the dereferenced value, not the `tsl::RCReference`. This migration will happen in 3 steps: 1. Define `xla::ifrt::DeviceListRef` alias that is interchangeable with `tsl::RCReference`. 2. Change `tsl::RCReference` to `xla::ifrt::DeviceListRef` in IFRT API, implementations, and user code. (current step) 3. Introduce a real wrapper `xla::ifrt::DeviceListRef`, replacing the alias.",2025-02-22T03:51:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87850
copybara-service[bot],[IFRT] Add `xla::ifrt::DeviceListRef` alias,"[IFRT] Add `xla::ifrt::DeviceListRef` alias IFRT will introduce a wrapper around `tsl::RCReference` that is more concise and is also safer to use (e.g., equality and hash compares the dereferenced value, not the `tsl::RCReference`. This migration will happen in 3 steps: 1. Define `xla::ifrt::DeviceListRef` alias that is interchangeable with `tsl::RCReference`. (current step) 2. Change `tsl::RCReference` to `xla::ifrt::DeviceListRef` in IFRT API, implementations, and user code. 3. Introduce a real wrapper `xla::ifrt::DeviceListRef`, replacing the alias.",2025-02-22T03:33:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87849
copybara-service[bot],Ensure that fetch window is never bigger than the full trace duration.,Ensure that fetch window is never bigger than the full trace duration.,2025-02-22T02:12:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87848
copybara-service[bot],Fix instances where HloRunnerPjRt would not wait on execution to complete.,Fix instances where HloRunnerPjRt would not wait on execution to complete.,2025-02-22T01:39:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87842
copybara-service[bot],"Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `all-gather` with an `all-to-all`.","Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `allgather` with an `alltoall`. Given the following input ``` ENTRY entry {   %param0 = f32[14,257] parameter(0), sharding={devices=[1,2]0,1}   %param1 = f32[14,116] parameter(1), sharding={devices=[1,2]0,1}   ROOT %concatenate = f32[14,373] concatenate(%param0, %param1),     dimensions={1}, sharding={devices=[1,2]0,1} } ``` Previously, we (1) replicate the input along the concat dimension, (2) apply concat, (3) partition the result with dynamicslice. With this change, we (1) use alltoall to move sharding axis from the concat dim to batch dim for operands, (2) apply concat, and then (3) use alltoall to reshard the result. Reverts 81b0a48fcf8618fdab0a03907b05a65413399585",2025-02-22T01:34:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87841
copybara-service[bot],Integrate compiler flags with the core libs,Integrate compiler flags with the core libs,2025-02-22T01:34:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87840
copybara-service[bot],Internal issue resolved; dynamic copy_test test cases can once again be enabled.,Internal issue resolved; dynamic copy_test test cases can once again be enabled. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/88314 from tensorflow:fixtypos18 1571f5e89b578bcf3c09af69808bdce49bc1bed5,2025-02-22T01:15:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87838
copybara-service[bot],Add compiler flag interface to the compiler plugin api/impls.,Add compiler flag interface to the compiler plugin api/impls.,2025-02-22T01:14:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87837
copybara-service[bot],Various MacOS QoL enhancements,Various MacOS QoL enhancements Part 1 of https://github.com/openxla/xla/pull/16696,2025-02-22T01:13:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87836
copybara-service[bot],Make tsl_cc_test shuffle tests by default. This makes it more likely to catch test order dependencies at presubmit time.,Make tsl_cc_test shuffle tests by default. This makes it more likely to catch test order dependencies at presubmit time.,2025-02-22T00:53:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87835
copybara-service[bot],"Switch xla_test, etc to static linking within Google.","Switch xla_test, etc to static linking within Google. Previously, we switched xla_test, etc to static linking to catch duplicate main() definitions at build time. We had to revert the change as it increased test binary sizes and broke Nvidia's build. In this second attempt, we make the change only for the Google internal build, so that external users aren't affected.",2025-02-22T00:47:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87834
copybara-service[bot],Add flag (only on flatbuffer export tool) to disable buffer sharing in flatbuffer.,Add flag (only on flatbuffer export tool) to disable buffer sharing in flatbuffer. Some downstream tools won't support this.,2025-02-22T00:31:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87833
copybara-service[bot],Remove `release` configs from XLA's version of the TensorFlow bazelrc except for MacOS,Remove `release` configs from XLA's version of the TensorFlow bazelrc except for MacOS,2025-02-22T00:31:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87832
copybara-service[bot],Expose ExecutableBuildOptions::CompilationEnvironments::CreateFromProto to python,Expose ExecutableBuildOptions::CompilationEnvironments::CreateFromProto to python Add a default TpuCompilationEnvironment to the wiz export,2025-02-22T00:21:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87831
copybara-service[bot],[IFRT] Log when executable names clash for ifrt.CallOp.,[IFRT] Log when executable names clash for ifrt.CallOp.,2025-02-22T00:08:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87829
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-21T23:58:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87827
copybara-service[bot],"Remove iOS, Android, and `with_xla_support` configs from XLA's copy of the TensorFlow .bazelrc","Remove iOS, Android, and `with_xla_support` configs from XLA's copy of the TensorFlow .bazelrc",2025-02-21T23:21:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87822
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-21T23:10:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87820
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-21T23:04:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87819
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-21T23:01:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87818
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes. Reverts 4966569daa20d7296d62360a2b6ebeee10286040,2025-02-21T22:56:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87817
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-21T22:55:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87816
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-21T22:53:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87815
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-21T22:46:20Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87814,"في السبت، ٢٢ فبراير ٢٠٢٥, ١٢:٥٣ ص copybaraservice[bot]  كتب: > Cleanup: Fix includes. >  > You can view, comment on, or merge this pull request online at: > >   https://github.com/tensorflow/tensorflow/pull/87814 > Commit Summary > >     30de802 >     >    Cleanup: Fix includes. > > File Changes > > (1 file ) > >     *M* tensorflow/core/grappler/optimizers/data/fusion_utils_test.cc >     >    (3) > > Patch Links: > >     https://github.com/tensorflow/tensorflow/pull/87814.patch >     https://github.com/tensorflow/tensorflow/pull/87814.diff > > — > Reply to this email directly, view it on GitHub > , or unsubscribe >  > . > You are receiving this because you are subscribed to this thread.Message > ID: ***@***.***> > https://github.com/tensorflow/tensorflow/commit/d4de90bf967a7b7785aec850036ba96ad4658325"
copybara-service[bot],Bring back expanded error message.,Bring back expanded error message.,2025-02-21T22:04:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87813
copybara-service[bot],"Revert the change that defaults xla_test, etc, to linkstatic. This change increased the test binary size and broken NVIDIA's CI.","Revert the change that defaults xla_test, etc, to linkstatic. This change increased the test binary size and broken NVIDIA's CI.",2025-02-21T21:34:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87812
copybara-service[bot],Add a step to compute device stats for GPU benchmarks and update to the latest version of actions/upload-artifact,Add a step to compute device stats for GPU benchmarks and update to the latest version of actions/uploadartifact,2025-02-21T21:10:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87811
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@5e9b356b,Integrate StableHLO at openxla/stablehlo,2025-02-21T20:37:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87810
copybara-service[bot],[XLA:GPU] Add comments to remove send/recv validation when we replace it,[XLA:GPU] Add comments to remove send/recv validation when we replace it,2025-02-21T20:23:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87809
copybara-service[bot],Add a step to compute device stats for GPU benchmarks and update to the latest version of actions/upload-artifact,Add a step to compute device stats for GPU benchmarks and update to the latest version of actions/uploadartifact,2025-02-21T20:08:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87808
copybara-service[bot],[XLA:GPU] Remove unused should_process callback in pipeliner,[XLA:GPU] Remove unused should_process callback in pipeliner,2025-02-21T19:56:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87807
copybara-service[bot],[XLA:GPU] Support postprocessing of peeled ops in the trailing while loop iteration,[XLA:GPU] Support postprocessing of peeled ops in the trailing while loop iteration Reverts 1fe54335f208c63c8b7b3df12ae745249427adcb,2025-02-21T19:51:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87806
copybara-service[bot],Fix more bugs where a test library is not `alwayslink`.,"Fix more bugs where a test library is not `alwayslink`. A library that defines test cases should be marked as `alwayslink` to ensure that the test case registration does happen. Without this attribute, the tests *might* work (e.g. if dynamic linking is used) but this is unreliable. It's safer to mark the test libraries as `alwayslink` s.t. the test cases can be found regardless of whether static or dynamic linking is used.",2025-02-21T19:34:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87805
copybara-service[bot],Integrate Triton up to [c5036b9b](https://github.com/openai/triton/commits/c5036b9ba1b60b53a7cecaf58a0c8b8cf8ac557b),Integrate Triton up to c5036b9b,2025-02-21T19:10:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87804
copybara-service[bot],Properly cleanup PJRT_Error if test fail,Properly cleanup PJRT_Error if test fail,2025-02-21T18:57:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87803
copybara-service[bot],Integrate LLVM at llvm/llvm-project@cf50936b23ac,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match cf50936b23ac,2025-02-21T18:36:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87802
copybara-service[bot],"Revert the change that defaults xla_test, etc, to linkstatic. This change increased the test binary size and broken NVIDIA's CI.","Revert the change that defaults xla_test, etc, to linkstatic. This change increased the test binary size and broken NVIDIA's CI.",2025-02-21T18:28:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87801
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-21T18:26:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87800
copybara-service[bot],[XLA:GPU] Fix `HasCycle` function,"[XLA:GPU] Fix `HasCycle` function This is needed to avoid deadlocks when running maxtext with PP and FSDP. In this case, we see collectivepermutes with multiple cycles, that were falsely categorized as acyclic. The result is a decomposed collectivepermute issuing a cyclic recv leading into a deadlock. Reverts 52fc64b538c7291b8caa0de7b0bfdcf7762376e8",2025-02-21T18:24:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87799
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-21T18:20:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87798
copybara-service[bot],PR #22292: [GPU] Support cuDNN explicit CUDA graph construction.,PR CC(Docker with python 3.6): [GPU] Support cuDNN explicit CUDA graph construction. Imported from GitHub PR https://github.com/openxla/xla/pull/22292 Some cuDNN graph engines now support explicit CUDA graph construction instead of stream capture. XLA will now switch between explicit construction and the already implemented stream capture accordingly. Copybara import of the project:  caf22d33e606a6b2ab00d14aa9082550515c404c by Ilia Sergachev : [GPU] Support cuDNN explicit CUDA graph construction. Some cuDNN graph engines now support explicit CUDA graph construction instead of stream capture. XLA will now switch between explicit construction and the already implemented stream capture accordingly.  23bb1ea89959a10b90b7892196bec41621c9b093 by Ilia Sergachev : Log graphs that don't support CUDA graph native API.  dd31aeab7edc21a39531817e96a6eecfb0d5b96f by Ilia Sergachev : Skip the added test with old cuDNN versions.  eeafdbf5f61b111fa3285fb2cfcb65efc91c6b62 by Ilia Sergachev : Address review comments.  c03beef9515c0198d6eb1518b10a483b6a1b9c41 by Ilia Sergachev : Fix build errors. Merging this change closes CC(Docker with python 3.6) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22292 from openxla:cudnn_explicit_cuda_graph c03beef9515c0198d6eb1518b10a483b6a1b9c41,2025-02-21T18:11:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87797
LaithMustafa,spam," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible). ``` (You can paste links or attach files by dragging & dropping them below)  Provide links to your updated versions of the above two colab notebooks.  Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model. ```  Option B: Paste your code here or provide a link to a custom endtoend colab ``` (You can paste links or attach files by dragging & dropping them below)  Include code to invoke the TFLite Converter Python API and the errors.  Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model. ```  3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2025-02-21T18:02:20Z,invalid TFLiteConverter,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87796,Please don't spam.
copybara-service[bot],[xla:cpu] Use f16 to f32 upcast for polynomial math approximations,[xla:cpu] Use f16 to f32 upcast for polynomial math approximations For consistency with Eigen (Tensorflow) use f16 to f32 casting to compute approximations.,2025-02-21T17:23:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87795
copybara-service[bot],Add signature-key based API for CC Compiled Model.,Add signaturekey based API for CC Compiled Model.,2025-02-21T17:21:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87794
copybara-service[bot],Internal,Internal,2025-02-21T17:09:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87793
copybara-service[bot],[XLA:GPU/Triton] Fix Triton's F16 -> F8E5M2 conversion logic on A100.,"[XLA:GPU/Triton] Fix Triton's F16 > F8E5M2 conversion logic on A100. Also add an exhaustive conversion test to make sure that conversion follows correct RTNE semantics. Note: the test needs to be disabled on H100, because Triton's conversion logic is also wrong therethe NVIDIA intrinsics that is used doesn't preserve infinities. This'll need to be handled in a future change.",2025-02-21T16:22:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87792
copybara-service[bot],Refactor CC Compiled Model buffer requirements functions.,Refactor CC Compiled Model buffer requirements functions.,2025-02-21T16:22:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87791
copybara-service[bot],Reverts 1da25ea7cfc71b9af2edcd1b7db3859ae8b7ec5a,Reverts 1da25ea7cfc71b9af2edcd1b7db3859ae8b7ec5a,2025-02-21T16:21:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87790
copybara-service[bot],"[XLA:GPU] Adding TritonXLAToTriton pass that is intended to lower the newly created triton_xla.tile, extract, and insert. Also function rewrite is done for func op.","[XLA:GPU] Adding TritonXLAToTriton pass that is intended to lower the newly created triton_xla.tile, extract, and insert. Also function rewrite is done for func op.",2025-02-21T16:13:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87789
M329855,01115002431,"**System information**  Android Device information (use `adb shell getprop ro.build.fingerprint`   if possible):  TensorFlow Lite in Play Services SDK version (found in `build.gradle`):  Google Play Services version     (`Settings` > `Apps` > `Google Play Services` > `App details`): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to or attach code demonstrating the problem. **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2025-02-21T16:09:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87788
copybara-service[bot],Refactor CC CompiledModel buffer creation functions.,Refactor CC CompiledModel buffer creation functions.,2025-02-21T16:08:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87787
copybara-service[bot],Fix invalid pointer in environment_options,Fix invalid pointer in environment_options,2025-02-21T15:51:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87786
copybara-service[bot],"[XLA:Python] Make sure we hold the GIL while writing to the ""view"" passed to a bf_getbuffer method.","[XLA:Python] Make sure we hold the GIL while writing to the ""view"" passed to a bf_getbuffer method. See https://github.com/python/cpython/issues/130409 . We need to prevent concurrent GC traversals at least under freethreading mode to avoid a data race. The same APIs that hold the GIL under nonfreethreading builds also have the effect of preventing concurrent garbage collection under freethreading mode: GC is a stoptheworld process that requires that all active Python threads be suspended. Fixes https://github.com/jaxml/jax/issues/26305",2025-02-21T15:32:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87784
copybara-service[bot],Add missing newline in `accelerator.h`,Add missing newline in `accelerator.h`,2025-02-21T15:14:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87780
copybara-service[bot],Some minor polishing of the release docs for 2.19.,"Some minor polishing of the release docs for 2.19. 1. Fix indentation.  The indentation of the first three bullet points in the markdown sources did not match the indentation of the fourth and fifth bullet points, nor of the bullet points further below. 2. Wrap some long lines in the markdown sources, in particular where there were some lines wrapped but others not wrapped in the same bullet point list. 3. Use ""Python API"" rather than ""Interpreter"" as the subheading for changes affecting the `tf.lite.Interpreter` Python class, for consistency with the earlier heading ""C++ API"" in the same bullet point list.",2025-02-21T13:53:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87774
copybara-service[bot],[xla:emitters] move VectorizeLoadsAndStores to xla/codegen/emitters,[xla:emitters] move VectorizeLoadsAndStores to xla/codegen/emitters And make it possible to set the device as CPU. This pass will soon be shared with the CPU pipeline  will be needed by the loop emitter.,2025-02-21T12:59:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87767
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22744 from openxla:hlo_convert d7100949730e14d590208336e989e5e1b9097d5f,2025-02-21T12:56:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87766
copybara-service[bot],The checkpoint table names and the corresponding layouts should be sorted in same order.,The checkpoint table names and the corresponding layouts should be sorted in same order.,2025-02-21T12:18:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87763
copybara-service[bot],Avoids Segmentation fault when dispatcher library is not found,Avoids Segmentation fault when dispatcher library is not found,2025-02-21T11:48:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87762
copybara-service[bot],Adds LITERT_FATAL to logging,Adds LITERT_FATAL to logging,2025-02-21T11:44:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87761
copybara-service[bot],PR #22744: Add a tool to convert HLO files between different formats.,PR CC([Feature request] tfcompile AOT gonna support tensorflow_transform?): Add a tool to convert HLO files between different formats. Imported from GitHub PR https://github.com/openxla/xla/pull/22744 Copybara import of the project:  89cce688af7574902b2d9a5eb8c7343d322abeb2 by Ilia Sergachev : Add a tool to convert HLO files between different formats.  d7100949730e14d590208336e989e5e1b9097d5f by Ilia Sergachev : address review comments Merging this change closes CC([Feature request] tfcompile AOT gonna support tensorflow_transform?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22744 from openxla:hlo_convert d7100949730e14d590208336e989e5e1b9097d5f,2025-02-21T11:36:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87760
gaikwadrahul8,Fix 08 broken links in development.md,"Hi, Team I found 08 broken documentation links in this file development.md so I have updated those broken links to functional link. Please review and merge this change as appropriate. Thank you for your consideration.",2025-02-21T11:33:10Z,comp:lite ready to pull size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87759
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T11:09:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87758
copybara-service[bot],Reverts 9f2c4fd4c09a2098980790d9264f4560dbf0cf54,Reverts 9f2c4fd4c09a2098980790d9264f4560dbf0cf54,2025-02-21T11:05:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87757
copybara-service[bot],Integrate LLVM at llvm/llvm-project@43d71baae36c,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 43d71baae36c,2025-02-21T09:27:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87756
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T09:19:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87755
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T09:16:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87754
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T09:14:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87753
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T09:10:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87752
copybara-service[bot],PR #87613: Explicit print options while dumping HLO,"PR CC(Explicit print options while dumping HLO): Explicit print options while dumping HLO Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/87613 We are changing the `HloModule::ToString` function to consider the debug options related to printing like, `xla_dump_hlo_as_long_text`, `xla_dump_large_constants` etc. These options are already embedded into the `HloModule` and are set via the command line and so, the `ToString` function should default to these functions.  We are also changing the default value of HloPrintOptions: `set_print_operand_shape(false)` and `set_print_large_constants(true)`. The related changes in XLA are https://github.com/openxla/xla/pull/22614 and https://github.com/openxla/xla/pull/22800. This breaks some of the tests in tensorflow because it relies on the `ToString` function and the default print options. As a temporary fix, we are making the default print options explicit here. Once the XLA change gets merged, we can remove this and fix the tests (or not, depending on what the TF community decides, but the behavior of the tool will be intentional.) Copybara import of the project:  fe5a22699723a33292c4ebdbe186a41cf47217c0 by Shraiysh Vaishay : Explicit print options while dumping HLO We are changing the `HloModule::ToString` function to consider the debug options related to printing like, `xla_dump_hlo_as_long_text`, `xla_dump_large_constants` etc. These options are already embedded into the `HloModule` and are set via the command line and so, the `ToString` function should default to these functions. We are also changing the default value of HloPrintOptions: `set_print_operand_shape(false)` and `set_print_large_constants(true)`. The related changes in XLA are https://github.com/openxla/xla/pull/22614 and https://github.com/openxla/xla/pull/22800. This breaks some of the tests in tensorflow because it relies on the `ToString` function and the default print options. As a temporary fix, we are making the default print options explicit here. Once the XLA change gets merged, we can remove this and fix the tests (or not, depending on what the TF community decides, but the behavior of the tool will be intentional.) Merging this change closes CC(Explicit print options while dumping HLO) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/87613 from shraiysh:fixxlatests fe5a22699723a33292c4ebdbe186a41cf47217c0",2025-02-21T08:45:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87751
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T08:41:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87750
copybara-service[bot],Add support for calibrating composite decompositions,Add support for calibrating composite decompositions,2025-02-21T08:01:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87749
Outlast2024,Update README.md,**OutLast**,2025-02-21T07:35:13Z,size:XS invalid,closed,0,4,https://github.com/tensorflow/tensorflow/issues/87748,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","> Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). >  > View this failed invocation of the CLA check for more information. >  > For the most up to date status, view the checks section at the bottom of the pull request.","Please don't use ""add file""/""update file""/""fix file""/etc. commit messages. These are hard to reason about when looking at the history of the file/repository. Instead, please write explanatory git commit messages. The commit message is also the title of the PR if the PR has only one commit. It is thus twice important to have commit messages that are relevant, as PRs would be easier to understand and easier to analyze in search results. For how to write good quality git commit messages, please consult https://cbea.ms/gitcommit/ ",Please don't spam as it goes against code of conduct of both TF and GitHub and can get you banned
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T07:27:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87747
copybara-service[bot],Add comments for recently extracted method which is now public (NFC).,Add comments for recently extracted method which is now public (NFC).,2025-02-21T07:26:33Z,,closed,1,0,https://github.com/tensorflow/tensorflow/issues/87746
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T07:10:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87745
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T07:01:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87744
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T06:56:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87743
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T06:55:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87742
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T06:50:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87741
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T06:48:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87740
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T06:48:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87739
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T06:44:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87738
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T06:41:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87737
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T06:38:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87736
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T06:26:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87735
copybara-service[bot],Internal change for visibility,Internal change for visibility,2025-02-21T05:49:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87734
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-21T05:43:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87733
copybara-service[bot],Integrate LLVM at llvm/llvm-project@204dcafec0ec,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 204dcafec0ec,2025-02-21T05:12:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87732
copybara-service[bot],Add a flatbuffer util (python) function for getting the builtin options as a given type.,Add a flatbuffer util (python) function for getting the builtin options as a given type.,2025-02-21T04:58:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87731
copybara-service[bot],litert: Add uint8 support,litert: Add uint8 support,2025-02-21T01:37:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87730
copybara-service[bot],litert: Discover CompilerPlugin only when the path is configured.,"litert: Discover CompilerPlugin only when the path is configured. Since FindLiteRtSharedLibsHelper() search recursively, it could visit the entire filesystem. Also prevent FindLiteRtSharedLibsHelper() from crashing by checking access permission.",2025-02-21T01:32:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87729
copybara-service[bot],Add OpMetricsDb for complete steps to OpStats.,Add OpMetricsDb for complete steps to OpStats.,2025-02-21T01:30:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87728
copybara-service[bot],[XLA:CollectivePipeliner] Assign unique annotations to the peeled-off scheduling groups (for the kForward and kBackward modes of the pipeliner).,[XLA:CollectivePipeliner] Assign unique annotations to the peeledoff scheduling groups (for the kForward and kBackward modes of the pipeliner).,2025-02-21T01:01:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87727
copybara-service[bot],XLA Tooling: Improve and Update the documentation to include new features introduced at the tool. ,"XLA Tooling: Improve and Update the documentation to include new features introduced at the tool.  1. changed Heading format to [`tool name`]  2. Added missing  how to build a binary instruction for each tool.  3. Added hloopt tool new feature documentation.  4. Updated hloopt tool old features documentation to clarify, it is deviceless compilation, corrected paths, links.",2025-02-21T00:41:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87726
copybara-service[bot],Update TFRT dependency to use revision,Update TFRT dependency to use revision http://github.com/tensorflow/runtime/commit/345ae8d48df934a5e440febea9a5e1ec856868b0.,2025-02-21T00:25:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87725
copybara-service[bot],Reverts 7012234c3646cf0ea86f2d38d183f0f2200d12e5,Reverts 7012234c3646cf0ea86f2d38d183f0f2200d12e5,2025-02-21T00:06:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87724
copybara-service[bot],Fix CreateInput/OutputBuffers in compiled model test.,Fix CreateInput/OutputBuffers in compiled model test.,2025-02-21T00:04:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87723
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-20T23:48:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87722
copybara-service[bot],Expose ExecuteThunks as a public method on GpuExecutable.,Expose ExecuteThunks as a public method on GpuExecutable.,2025-02-20T23:36:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87721
copybara-service[bot],"[xla:cpu] Replace custom ScheduleAll with ""standard"" Worker::Parallelize","[xla:cpu] Replace custom ScheduleAll with ""standard"" Worker::Parallelize",2025-02-20T23:29:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87720
copybara-service[bot],[HLO->MHLO] Don't validate mhlo.token sharding mismatches.,"[HLO>MHLO] Don't validate mhlo.token sharding mismatches. Legacy programs pass a tuple of multiple sharding values for a single mhlo.token output. When we use HLO>MHLO import with tuple flattening, this check breaks since there are more than one flattened tuple values, and only one function result.",2025-02-20T23:25:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87719
copybara-service[bot],"Adds visibility restriction to some XLA bzl files to prevent them from being used outside of XLA, as they are internal implementation details.","Adds visibility restriction to some XLA bzl files to prevent them from being used outside of XLA, as they are internal implementation details. This CL is not complete. It's the first step that establishes the mechanism. Once I get buyin on the approach, I'll follow up with more CLs to add visibility restriction to the other XLA bazl files.",2025-02-20T23:21:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87718
copybara-service[bot],Integrate LLVM at llvm/llvm-project@4a8f41456515,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 4a8f41456515,2025-02-20T23:14:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87717
copybara-service[bot],Disable copy-reshape -> reshape-copy transformation when the first copy is from host.,Disable copyreshape > reshapecopy transformation when the first copy is from host.,2025-02-20T23:09:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87716
copybara-service[bot],Nothing to see here. Move along.,Nothing to see here. Move along.,2025-02-20T23:02:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87715
copybara-service[bot],[tsl:concurrency] Micro optimizations for AsyncValue::AndThen,"[tsl:concurrency] Micro optimizations for AsyncValue::AndThen Instead of relying on absl::AnyInvocable keep Waiter directly in the linked list node, this improves performance by: 1. Avoiding one extra heap allocation for the absl::AnyInvocable 2. Remove one pointer indirection in RunWaiters BEFORE:  Benchmark                      Time             CPU   Iterations  BM_AddAndThenCallback       20.3 ns         20.3 ns     34354892 AFTER:  Benchmark                      Time             CPU   Iterations  BM_AddAndThenCallback       12.2 ns         12.2 ns     57932249",2025-02-20T22:58:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87714
copybara-service[bot],Nothing to see here. Move along.,Nothing to see here. Move along.,2025-02-20T22:58:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87713
copybara-service[bot],Fix build error in //tflite/ios:TensorFlowLiteC_framework.,Fix build error in //tflite/ios:TensorFlowLiteC_framework.,2025-02-20T22:42:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87712
copybara-service[bot],Change test_base dependency from compiler/mlir/lite to compiler/mlir/,Change test_base dependency from compiler/mlir/lite to compiler/mlir/,2025-02-20T22:39:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87711
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-20T22:37:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87710
sachinmuradi,[oneDNN][XLA:CPU] Add NonmaxsuppresionV3/V4 in slow ops,"This PR adds NonMaxSuppressionv3/v4 operation as slow ops when translating in tf2XLA. When NonMaxSuppressionv3/v4 are not clustered in XLA computation, we saw that many models like retinanet / fasterrcnn models show significant improvement in performance (retinanet shows about 88% improvement).",2025-02-20T22:35:42Z,size:XS,closed,0,2,https://github.com/tensorflow/tensorflow/issues/87709," Thanks for reviewing PR and Added unit test for this as requested. About the state of allow_slow_ops : Inside mark_for_compilation_pass.cc, the OperationFilter  object is created from XlaOpRegistry::DeviceRegistration object, which initialized cluster_slow_ops as false and since during the creation of OperationFilter  OperationFilter >allow_slow_ops =  DeviceRegistration>cluster_slow_ops , allow_slow_ops  will be false. Hope this helps.",Thanks  for approving.
Schnipper24,Direction Problem after creating model / So after sucsessfully creating my Model and saving it automatic in the folder i need i get this message," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64Bit  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source): abslpy==1.0.0 appdirs==1.4.4 astunparse==1.6.3 attrs==22.1.0 audioread==3.0.0 cachetools==5.2.0 certifi==2022.9.24 cffi==1.15.1 chardet==5.0.0 charsetnormalizer==2.1.1 colorama==0.4.5 contourpy==1.0.5 cycler==0.11.0 Cython==0.29.32 dataclasses==0.6 decorator==5.1.1 dill==0.3.5.1 dmtree==0.1.7 etils==0.8.0 fire==0.4.0 flatbuffers==1.12 fonttools==4.37.4 gast==0.4.0 ginconfig==0.5.0 googleapicore==2.8.2 googleapipythonclient==2.64.0 googleauth==2.12.0 googleauthhttplib2==0.1.0 googleauthoauthlib==0.4.6 googlecloudbigquery==3.3.3 googlecloudbigquerystorage==2.16.0 googlecloudcore==2.3.2 googlecrc32c==1.5.0 googlepasta==0.2.0 googleresumablemedia==2.4.0 googleapiscommonprotos==1.56.4 grpcio==1.48.2 grpciostatus==1.48.2 h5py==3.1.0 httplib2==0.20.4 idna==3.4 importlibmetadata==5.0.0 importlibresources==5.9.0 joblib==1.2.0 kaggle==1.5.12 keras==2.9.0 kerasnightly==2.5.0.dev2021032900 KerasPreprocessing==1.1.2 kiwisolver==1.4.4 labelImg==1.8.6 libclang==14.0.6 librosa==0.8.1 llvmlite==0.36.0 lml==0.1.0 lxml==4.9.1 Markdown==3.4.1 MarkupSafe==2.1.1 matplotlib==3.4.3 neuralstructuredlearning==1.4.0 numba==0.53.0  2. Code Provide code to help us reproduce your issues using one of the following options:  https://www.tensorflow.org/lite/tutorials/model_maker_object_detection  https://github.com/tzutalin/labelImg import sys, getopt, time, pathlib from tflite_model_maker.config import ExportFormat from tflite_model_maker.config import QuantizationConfig from tflite_model_maker import model_spec from tflite_model_maker import object_detector def main(argv):     dir = None     batch_size = 8     epochs = 50     try:         opts, _ = getopt.getopt(argv, ""hd:b:e:"", [""help"",""directory="",""batchSize="",""epochs=""])         for opt, arg in opts:             print(opt + "":"" + arg)             if opt == ""h, help"":                 raise Exception()             elif opt in (""d"", ""directory""):                 dir = str(arg)             elif opt in (""b"", ""batchSize""):                 batch_size = int(arg)             elif opt in (""e"", ""epochs""):                 epochs = int(arg)         if (dir is None):             raise Exception()     except Exception:         print(""Specify a directory that contains your dataset."")         print(""createmodel.py d "")         sys.exit(2)     start = round(time.time() * 1000)      select object recognition model architecture     spec = model_spec.get(""efficientdet_lite0"")     spec.config.var_freeze_expr = ""(efficientnetresample_p6)""     spec.config.tflite_max_detections = 25     print(spec.config)      load input data specific to an ondevice ML app     train_data, validation_data, test_data = object_detector.DataLoader.from_csv(dir + ""/dataset.csv"")      customize the TensorFlow model     model = object_detector.create(         train_data,         model_spec=spec,         batch_size=batch_size,         epochs=epochs,         train_whole_model=False     )     model.summary()      evaluate the model     print(model.evaluate(test_data))      export to Tensorflow Lite model and label file in `export_dir`     path = pathlib.PurePath(dir)     model.export(export_dir=""build/"" + path.name + ""/"")     model.export(export_dir=""build/"" + path.name + ""/"", export_format=ExportFormat.LABEL)      evaluate the tensorflow lite model     print(model.evaluate_tflite(""build/"" + path.name + ""/model.tflite"", test_data))     stop = round(time.time() * 1000)     print(""process image: {} ms"".format(stop  start)) if __name__ == ""__main__"":    main(sys.argv[1:]) **So after sucsessfully creating my Model and saving it automatic in the folder i need i get this message:** Traceback (most recent call last):   File ""C:\Users\Robert\Desktop\IT_FischerTechnik_Klotz\Robert_KI\machinelearning\objectdetection\createmodel.py"", line 75, in      main(sys.argv[1:])   File ""C:\Users\Robert\Desktop\IT_FischerTechnik_Klotz\Robert_KI\machinelearning\objectdetection\createmodel.py"", line 69, in main     print(model.evaluate_tflite(""build/"" + path.name + ""/model.tflite"", test_data))   File ""C:\Users\Robert\AppData\Local\Programs\Python\Python39\lib\sitepackages\tensorflow_examples\lite\model_maker\core\task\object_detector.py"", line 155, in evaluate_tflite     return self.model_spec.evaluate_tflite(tflite_filepath, ds, len(data),   File ""C:\Users\Robert\AppData\Local\Programs\Python\Python39\lib\sitepackages\tensorflow_examples\lite\model_maker\core\task\model_spec\object_detector_spec.py"", line 373, in evaluate_tflite     lite_runner = eval_tflite.LiteRunner(tflite_filepath, only_network=False)   File ""C:\Users\Robert\AppData\Local\Programs\Python\Python39\lib\sitepackages\tensorflow_examples\lite\model_maker\third_party\efficientdet\keras\eval_tflite.py"", line 67, in __init__     self.interpreter = tf.lite.Interpreter(tflite_model_path)   File ""C:\Users\Robert\AppData\Local\Programs\Python\Python39\lib\sitepackages\tensorflow\lite\python\interpreter.py"", line 455, in __init__     _interpreter_wrapper.CreateWrapperFromFile( ValueError: Could not open 'build/ALLE_Klötze/model.tflite'. **I can even see the saved Model but still it says there is no file. I really dont know how to make it. (ROOKIE)** !Image",2025-02-20T22:29:17Z,TFLiteConverter,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87708,I Guess i found the Answer by myself and changed the folder from Klötze to Kloetze and it was done .
copybara-service[bot],[XLA:GPU][Emitters] Restrict the inliner.,[XLA:GPU][Emitters] Restrict the inliner. Inline only if there are more than 1 call to the callee in the caller. Background: https://github.com/jaxml/jax/issues/26162 contains an example of a MoF fusion that takes forever to compile.  The indexingbased partitioner in combination with this change fixes the issue.,2025-02-20T21:56:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87707
copybara-service[bot],Add execution stream ID to `xla::ifrt::Executable`,Add execution stream ID to `xla::ifrt::Executable` Executables with the same stream id will be executed sequentially in the runtime in the program order.  While executables with different stream ids may execute concurrently.  Concurrent execution support depends on the backend implementation and is not guaranteed.,2025-02-20T21:56:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87706
copybara-service[bot],#litert Add a automatically added accelerator compilation structure.,litert Add a automatically added accelerator compilation structure. This structure allows passing metadata that is generated during the model compilation onto accelerators when they alter the underlying runtime.,2025-02-20T21:54:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87705
copybara-service[bot],Updates MinimumMemoryBudgetRequired() to work with the interval-based representation.,Updates MinimumMemoryBudgetRequired() to work with the intervalbased representation.,2025-02-20T21:42:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87704
copybara-service[bot],Fix whitespace for workflows,Fix whitespace for workflows,2025-02-20T21:37:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87703
copybara-service[bot],Cleanup: Fix includes.,Cleanup: Fix includes.,2025-02-20T21:26:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87702
Mojken,"Absolute path to /bin/bash only works on some systems, consider changing",Consider changing the absolute path to bash in this makefile to `/usr/bin/env bash` to support more systems. https://github.com/tensorflow/tensorflow/blob/3db52be7be81a87c623cdeb7f03d3767521c5246/tensorflow/lite/tools/make/MakefileL3,2025-02-20T21:23:33Z,stat:awaiting response type:support comp:lite,closed,0,3,https://github.com/tensorflow/tensorflow/issues/87701,"Hi,   I apologize for the delay in my response, It seems like you're referring to older versions of TensorFlow(like `2.6.*,2.7.* or 2.8.*`) and I see we have added WARNING : Using Makefile to build TensorFlow Lite is deprecated at the Aug 2021. Please use CMake or Bazel instead. Please refer to the Build TensorFlow Lite with CMake and Build TensorFlow Lite for ARM boards for the details. Please refer this official documentation If I have missed something here please let me know. Thank you for your cooperation and patience.","Interesting, thanks for the headsup. I'm not building it myself but running into an issue stemming from this being included in another project. I'll forward this to the downstream maintainer, sorry to have wasted your time!",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Invoke `xla_tests_package_groups()` only once in the codebase. This avoids duplicating the same package group definition.,Invoke `xla_tests_package_groups()` only once in the codebase. This avoids duplicating the same package group definition.,2025-02-20T20:58:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87700
copybara-service[bot],Enable QAT Einsum FakeQuants tests. And bug fixes in the converter lowering to handle edge cases.,Enable QAT Einsum FakeQuants tests. And bug fixes in the converter lowering to handle edge cases.,2025-02-20T20:55:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87699
copybara-service[bot],[HLO-OPT] Add `--emit-proto` flag to convert input hlo in text to proto format.,[HLOOPT] Add `emitproto` flag to convert input hlo in text to proto format.,2025-02-20T20:26:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87698
copybara-service[bot],Style improvements to `xla_tests_package_groups`.,"Style improvements to `xla_tests_package_groups`. The `xla_tests_package_groups` defines a package group named `friends`. There are a couple of issues with its style:  The macro takes a `name` parameter and the doc string says its the name of the package group. However, this is misleading as the parameter is not used at all.  The callsite syntax `xla_tests_package_groups()` doesn't make it clear that it defines a target named `friends`. This adds cognitive burden for readers who try to figure out where `friends` is defined. The fix is to make sure the `name` parameter is really used and mandatory. This style is consistent with how most bzl functions work, and makes the call sites more readable.",2025-02-20T20:10:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87697
copybara-service[bot],#litert Create the NPU accelerator.,litert Create the NPU accelerator. The accelerator is not yet automatically registered to the LiteRT environment.,2025-02-20T20:08:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87696
copybara-service[bot],Refined logic to automatically disable embedding pipelining,Refined logic to automatically disable embedding pipelining,2025-02-20T19:56:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87695
tensorflow-jenkins,"r2.19 cherry-pick: cf611dd7c6e ""Update 2.19.0 release notes that we stopped publishing libtensorflow packages""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/cf611dd7c6e1ca46a9ba8e4fb8c07ed42ffe5278,2025-02-20T19:54:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87694
copybara-service[bot],Cleanup: remove unnecessary string conversions.,Cleanup: remove unnecessary string conversions.,2025-02-20T19:38:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87693
copybara-service[bot],[XLA:SPMD] Optimize the partitioning for element-wise operations when all operands share the same sharding.,"[XLA:SPMD] Optimize the partitioning for elementwise operations when all operands share the same sharding. Let us take `C with S3 = add(A with S1, B with S2)` as an example, where A, B, C are tensors, S1, S2, S3 are their shardings. Before this change, we always have ``` A with S3 = reshard(A with S1, new_sharding=S3) B with S3 = reshard(B with S2, new_sharding=S3) C with S3 = add(A with S3, B with S3) ``` With this cl, if S1 and S2 are the same, we will have ``` C with S1 = add(A with S1, B with S1) C with S3 = reshard(C with S1) ``` The new partitioning method can reduce the number of reshards.",2025-02-20T19:10:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87692
copybara-service[bot],PR #22758: Fixes for two Blackwell tests,"PR CC(Tensorflow CPU 1.5 VS 1.6 on Windows 7  64 bits  Bug on 1.6 version): Fixes for two Blackwell tests Imported from GitHub PR https://github.com/openxla/xla/pull/22758 //xla/service/gpu/autotuning:gemm_fusion_autotuner_test and //xla/service:elemental_ir_emitter_test. Also, a cosmetic change to //xla/service/gpu:gpu_compiler_test. Copybara import of the project:  51dedc3c778b4f225b8e85d2e2284cdd3e32b19c by Dimitris Vardoulakis : Fixes for two Blackwell tests. Merging this change closes CC(Tensorflow CPU 1.5 VS 1.6 on Windows 7  64 bits  Bug on 1.6 version) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22758 from dimvar:fixesforblackwelltests 51dedc3c778b4f225b8e85d2e2284cdd3e32b19c",2025-02-20T19:07:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87691
copybara-service[bot],Modify the hash value for CseKey to also hash on result accuracy. The cse pass should also group instructions with same result accuracy. We should also check for result accuracy fields when checking if two hlo instructions are the same.,Modify the hash value for CseKey to also hash on result accuracy. The cse pass should also group instructions with same result accuracy. We should also check for result accuracy fields when checking if two hlo instructions are the same.,2025-02-20T19:01:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87690
copybara-service[bot],[XLA:GPU] only store information about symbol dimension tiling for triton nested fusion,"[XLA:GPU] only store information about symbol dimension tiling for triton nested fusion Previously we were storing tiling sizes for both contracting and noncontracting dimensions for a nested fusion. Unfortunatelly this information is both redundant (as tiling for noncontracting dimension will be set by the outer fusion) and missing information ""which of the dimension is contracting?"" to pick the tile size. Later, in tiling analysis we will have to walk the tree again and find dot for each fusion to understand what tiling parameter we want to set for the new symbol (K). If we store _only_ the contracting dimension then we should have an easier time  tiling for contracting dimension is the only element of the output_tile_sizes of the nested fusion.",2025-02-20T18:29:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87689
copybara-service[bot],[xla:cpu] Do not create empty WorkQueue partitions,[xla:cpu] Do not create empty WorkQueue partitions,2025-02-20T18:26:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87688
copybara-service[bot],Make tensorflow-io-gcs-filesystem an optional dependency.,"Make tensorflowiogcsfilesystem an optional dependency. The dependency has been causing a number of issues, see: https://github.com/tensorflow/tensorflow/pull/82771 The support for the package is currently uncertain, and has been on the low end for a while  no Windows wheels since 0.32.0, for example. It has been capped on <py3.12 for a while now also.",2025-02-20T18:25:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87687
copybara-service[bot],[XLA:CPU][fix] Verify that the kernel thunk invariant arguments in the serialized and deserialized proto match,[XLA:CPU][fix] Verify that the kernel thunk invariant arguments in the serialized and deserialized proto match,2025-02-20T18:22:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87686
copybara-service[bot],[xla:cpu] Do not enqueue new workers if we have workers in steal loop,[xla:cpu] Do not enqueue new workers if we have workers in steal loop Avoid launching redundant tasks that will not find any work to do.,2025-02-20T18:19:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87685
copybara-service[bot],Integrate LLVM at llvm/llvm-project@4a8f41456515,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 4a8f41456515,2025-02-20T18:12:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87684
copybara-service[bot],Don't run CUDA test with msan.,Don't run CUDA test with msan.,2025-02-20T17:38:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87683
copybara-service[bot],Update 2.19.0 release notes that we stopped publishing libtensorflow packages,Update 2.19.0 release notes that we stopped publishing libtensorflow packages,2025-02-20T16:25:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87682
copybara-service[bot],"[XLA] Remove DynCastOrNull, CastOrNull","[XLA] Remove DynCastOrNull, CastOrNull",2025-02-20T15:14:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87681
copybara-service[bot],[lite] Add TfLiteTensorGetDimsSignature,"[lite] Add TfLiteTensorGetDimsSignature `TfLiteTensor::dims_signature` is empty or null if all dimensions are known, which makes it inconsistent with `tf.lite.Interpreter.get_input_details()['shape_signature']`. https://www.tensorflow.org/api_docs/python/tf/lite/Interpreterget_input_details Provide a function with a consistent interface. `TfLiteTensorGetDimsSignature` returns `dims_signature` if there are unknown dimensions and `dims` otherwise.",2025-02-20T15:05:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87680
copybara-service[bot],[XLA:GPU] Consider flops in XLA:SoL model.,[XLA:GPU] Consider flops in XLA:SoL model.,2025-02-20T14:21:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87679
copybara-service[bot],[XLA:GPU][Emitters] Use indexing maps in the partitioner.,[XLA:GPU][Emitters] Use indexing maps in the partitioner.,2025-02-20T14:06:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87678
copybara-service[bot],Enable XNNPack Gelu for all cases.,"Enable XNNPack Gelu for all cases. It is faster and more accurate, what's not to like?",2025-02-20T14:00:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87677
vivkong,Fix compile error in tensorflow/python/tfcompile_wrapper.cc on s390x,This fixes the following error on s390x: ``` ImportError: Unable to import _pywrap_tfcompile; you must build TensorFlow with XLA.  You may need to build tensorflow with flag define=with_xla_support=true.  Original error: /home/test/.cache/bazel/_bazel_test/3a51eda24c560ebddf29b66b8fa0460e/execroot/org_tensorflow/bazelout/s390xoptexec50AE0418/bin/tensorflow/python/tools/saved_model_cli.runfiles/org_tensorflow/tensorflow/python/_pywrap_tfcompile.so: undefined symbol: _ZN4llvm3sys22getDefaultTargetTripleB5cxx11Ev ```,2025-02-20T14:00:04Z,ready to pull size:XS,open,0,9,https://github.com/tensorflow/tensorflow/issues/87676,"Hi, wondering if the checks can be kicked off again?   I ran the tests against master and didn't encounter errors.  Thanks for your help.","So, the error here is that copybara import into the internal system has failed. Can you try to rebase the PR on top of the current master branch? Maybe that would nudge copybara to retry",Ah I see.  Thanks for the info!  I've rebased on top of master  hope it does the trick.,Same type of error.  can you look into why copybara import fails here? I don't have access to those logs.,It doesn't like the Build file: `.../tensorflow/python/BUILD:504:2: syntax error near )` Is there an extra right parentheses there? ,"Nice catch. Yeah, there was a duplicated `)`",Thanks  and  for your help.  Fixed the change.  Please help kick off the checks.  Thanks!, Sorry it looks like there's a failure.  I don't have access to the failure.  Please let me know if I can make a change to help merge this.  Thanks.,"There is an issue with the internal version of the code, given a slightly different syntax between Bazel and the internal equivalent. Nothing for you to do here, we just need to bridge the internal gap on the internal version of this."
copybara-service[bot],Add comment to explain code that is somewhat hard to understand (NFC).,Add comment to explain code that is somewhat hard to understand (NFC).,2025-02-20T13:06:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87675
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T12:41:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87674
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T12:28:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87673
copybara-service[bot],Integrate LLVM at llvm/llvm-project@dc326d0b01f6,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match dc326d0b01f6,2025-02-20T11:49:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87672
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T11:29:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87671
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T11:25:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87670
Kakarute,"[WSL2 + Docker + TensorFlow] CUDA failed to initialize in TensorFlow container (RTX 5090, CUDA 12.8)"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.2 LTS (WSL2)  Mobile device _No response_  Python version 3.12.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 12.8, cuDNN 8.x (inside TensorFlow container)  GPU model and memory NVIDIA GeForce RTX 5090, 32GB VRAM  Current behavior? 1. System Information OS: Windows 11 Pro (WSL2) WSL2 Distro: Ubuntu 22.04 GPU: NVIDIA GeForce RTX 5090 (VRAM 32GB) NVIDIA Driver Version: 572.16 CUDA Version: 12.8 NVIDIA Container Toolkit Version: Latest Docker Version: 26.1.3 TensorFlow Container: nvcr.io/nvidia/tensorflow:25.01tf2py3 2. Steps to Reproduce & Issue Description ✅ 1) NVIDIA driver and GPU recognition check in WSL2 I first verified that the NVIDIA GPU is recognized correctly in WSL2 by running: nvidiasmi ✅ Output (GPU recognized properly) Thu Feb 20 17:07:18 2025 ±+  ±±±+ ✅ This confirms that NVIDIA drivers and CUDA work correctly in the WSL2 + Docker environment. ❌ 3) Issue: TensorFlow container fails to initialize CUDA I then launched the TensorFlow container and checked if CUDA was accessible. bash docker run rm gpus all it nvcr.io/nvidia/tensorflow:25.01tf2py3 bash ✅ Successfully entered the TensorFlow container == TensorFlow == NVIDIA Release 25.01tf2 (build 134984172) TensorFlow Version 2.17.0 Container image Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved. … ❌ However, CUDA failed to initialize with the following error: ERROR: The NVIDIA Driver is present, but CUDA failed to initialize. GPU functionality will not be available. [[ Named symbol not found (error 500) ]] Next, I checked if TensorFlow inside the container recognized the GPU by running: python3 c “import tensorflow as tf; print(tf.config.list_physical_devices(‘GPU’))” ❌ Output TensorFlow does not detect any GPUs. 3. Debugging Attempts 1️⃣ Checked for libcuda.so inside the container find /usr name “libcuda.so*” ✅ Output (Files are present) /usr/local/cuda12.8/compat/lib.real/libcuda.so /usr/local/cuda12.8/compat/lib.real/libcuda.so.1 /usr/local/cuda12.8/compat/lib.real/libcuda.so.570.86.10 /usr/lib/x86_64linuxgnu/libcuda.so /usr/lib/x86_64linuxgnu/libcuda.so.1 2️⃣ Checked environment variables export LD_LIBRARY_PATH=/usr/lib/x86_64linuxgnu:/usr/local/cuda12.8/compat/lib.real:$LD_LIBRARY_PATH Still, the same error persists. 3️⃣ Ran TensorFlow container with modified options bash docker run gpus all ipc=host ulimit memlock=1 ulimit stack=67108864 it nvcr.io/nvidia/tensorflow:25.01tf2py3 bash 🚨 Still, CUDA failed to initialize error persists. 4. Summary & Questions Findings so far: NVIDIA driver and CUDA work correctly in WSL2 and Docker. CUDA container (nvidia/cuda:12.3.0baseubuntu22.04) detects GPU properly. However, TensorFlow container (nvcr.io/nvidia/tensorflow:25.01tf2py3) fails to initialize CUDA. TensorFlow does not detect GPU (tf.config.list_physical_devices('GPU') returns []). Questions: Is the combination of NVIDIA RTX 5090 + CUDA 12.8 + TensorFlow 2.17.0 expected to work in WSL2? What could be causing CUDA failed to initialize in the TensorFlow container while it works fine in the CUDA container? Are there any additional configurations required to make TensorFlow detect the GPU inside the container? ✅ Looking for guidance on debugging and fixing this issue. Any insights or suggestions would be greatly appreciated!  Standalone code to reproduce the issue ```shell be included in the above content ```  Relevant log output ```shell ```",2025-02-20T10:28:50Z,stat:awaiting response type:bug stale comp:gpu 2.17,closed,0,8,https://github.com/tensorflow/tensorflow/issues/87669,"Hi **** , Apologies for the delay, and thanks for raising your concern here. This issue might be related to compatibility. I noticed a version mismatch between CUDA and TensorFlow. Could you please verify that you are using compatible versions to avoid errors? I am providing the official documentation for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,I have the same problem. New version for RTX50 series are needed because of CUDA 12.8. Tensorflow 2.18 only support cuda 12.5,"I have a working (buggy as hell) build, it might help you, maybe, full details in my Issue (I'm not using Docker, so it may not solve your issue): https://github.com/tensorflow/tensorflow/issues/89272",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,it worsk on my pc :D chatgpt 4.0 helps me that it work and only with chatgpt it work xD so hard xD i have windows 11 pro then wsl2 ubuntu 24.04 then linux installeiton docker.. tensorflow 2.17.0 with toolkit 2.18.1 ctreaderopenapi etc thje container: et c Kakarute opened on Feb 20 the same solve lkike ghis :D i have chatgpt give his post an schatgpt made it for me that it run xD 5090 master my KI that trade on forex stock runs python tensorflow perfectly stabile and only chatgpt know how it work xD blup greeze insta: stonypiles
chuntl,Qualcomm AI Engine Direct - Add log utils for core module,Summary:  Implement default and android version of log utils for core module  Add test for log util  Use VERBOSE as default log level,2025-02-20T10:05:59Z,size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87668
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T10:01:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87667
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T09:46:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87666
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T09:44:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87665
copybara-service[bot],[XLA:CPU] Store arguments and results separately in KernelSpec,[XLA:CPU] Store arguments and results separately in KernelSpec,2025-02-20T09:43:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87664
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T09:36:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87663
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T09:35:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87662
caofx0418,LiteRT delegate for Pixel Phones (Soc Tensor G2/G3/G4) TPUs,"Before Android 15, LiteRT nnapi delegate could accelerate NN models.  But after the release of Android 15, NNAPI will be deprecated. Compare to Qualcomm SOC, it offers QNN TFlite Delegate. Will the Pixel Phones offer TPU delegate ?",2025-02-20T09:25:29Z,type:support comp:lite,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87661,"Hi,   Thank you for reporting this issue. I apologize for the delayed response.  As this appears to be a duplicate of issue https://github.com/googleaiedge/LiteRT/issues/969 in the LiteRT repository (https://github.com/googleaiedge/LiteRT/issues/969), I will close this issue to consolidate the discussion.  Please refer to https://github.com/googleaiedge/LiteRT/issues/969 for updates and further information. Thank you for your understanding."
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T09:23:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87660
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T09:21:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87659
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T09:18:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87658
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T09:14:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87657
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T09:12:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87656
tilakrayal,Fix the broken link in index.md,,2025-02-20T08:18:14Z,comp:lite ready to pull size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87655
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T07:52:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87654
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T07:29:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87653
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T07:29:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87652
copybara-service[bot],PR #22826: [XLA:GPU] Only enable cuDNN flash attention segment mask generation on hopper and above,"PR CC(Win10 C++ TF1.9, error LNK2001, build by bazel  !): [XLA:GPU] Only enable cuDNN flash attention segment mask generation on hopper and above Imported from GitHub PR https://github.com/openxla/xla/pull/22826 Copybara import of the project:  8985672fa3a7bb543624a1e5cafbd2975e3c1214 by cjkkkk : init Merging this change closes CC(Win10 C++ TF1.9, error LNK2001, build by bazel  !) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22826 from Cjkkkk:enable_segment_packing_only_on_hopper 8985672fa3a7bb543624a1e5cafbd2975e3c1214",2025-02-20T07:29:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87651
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T07:28:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87650
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T07:22:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87649
copybara-service[bot],[XLA] Partial rollback to the previous implementation so that hlo casting utils does not use tsl::down_cast and we can insert better debug information regarding which HLO and what subclasses are involved. Performance is identical to using tsl::down_cast since both implementations only call dynamic_cast under debug build. Updated comments and added test coverage are kept.,[XLA] Partial rollback to the previous implementation so that hlo casting utils does not use tsl::down_cast and we can insert better debug information regarding which HLO and what subclasses are involved. Performance is identical to using tsl::down_cast since both implementations only call dynamic_cast under debug build. Updated comments and added test coverage are kept.,2025-02-20T07:22:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87648
copybara-service[bot],Integrate LLVM at llvm/llvm-project@57bac14f4bcc,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 57bac14f4bcc,2025-02-20T06:43:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87647
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T06:36:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87646
Venkat6871,Fix typos in documentation strings,"Hi, Team I observed few typos in the documentation strings and I have fixed those typos so please do the needful. Thank you.",2025-02-20T06:31:56Z,awaiting review ready to pull size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87645
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T06:28:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87644
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T05:59:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87643
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T05:45:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87642
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T05:22:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87641
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T05:20:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87640
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T05:20:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87639
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T05:19:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87638
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T05:19:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87637
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T05:15:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87636
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T05:14:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87635
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T05:12:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87634
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T05:08:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87633
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T05:07:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87632
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T04:44:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87631
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T04:43:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87630
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T04:41:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87629
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T04:41:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87628
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T04:40:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87627
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T04:39:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87626
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T04:39:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87625
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T04:37:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87624
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T04:36:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87623
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T04:36:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87622
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T04:31:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87621
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-20T04:27:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87620
copybara-service[bot],[xla:cpu] Do not fuse concat along the minor dimension,[xla:cpu] Do not fuse concat along the minor dimension,2025-02-20T04:21:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87619
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f178e51747b4,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f178e51747b4,2025-02-20T04:19:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87618
copybara-service[bot],Fix zero-sized tensor recognized as graphInput in QNN plugin.,Fix zerosized tensor recognized as graphInput in QNN plugin.,2025-02-20T04:02:44Z,,closed,1,0,https://github.com/tensorflow/tensorflow/issues/87617
copybara-service[bot],Make targets under lite/experimental/litert/c/ and cc/ public,Make targets under lite/experimental/litert/c/ and cc/ public,2025-02-20T03:20:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87616
copybara-service[bot],Fix typo in raw buffer void* dst -> const void* src.,Fix typo in raw buffer void* dst > const void* src.,2025-02-20T02:46:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87615
copybara-service[bot],litert: Add test case of shared inputs,litert: Add test case of shared inputs The shared_input_cpu_npu model has CPU / NPU shared inputs.,2025-02-20T00:40:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87614
shraiysh,Explicit print options while dumping HLO,"We are changing the `HloModule::ToString` function to consider the debug options related to printing like, `xla_dump_hlo_as_long_text`, `xla_dump_large_constants` etc. These options are already embedded into the `HloModule` and are set via the command line and so, the `ToString` function should default to these functions.  We are also changing the default value of HloPrintOptions: `set_print_operand_shape(false)` and `set_print_large_constants(true)`. The related changes in XLA are https://github.com/openxla/xla/pull/22614 and https://github.com/openxla/xla/pull/22800. This breaks some of the tests in tensorflow because it relies on the `ToString` function and the default print options. As a temporary fix, we are making the default print options explicit here. Once the XLA change gets merged, we can remove this and fix the tests (or not, depending on what the TF community decides, but the behavior of the tool will be intentional.)",2025-02-20T00:27:21Z,awaiting review ready to pull comp:xla size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87613
copybara-service[bot],Allowlist all existing users of //third_party/tensorflow/compiler/jit:jit.,Allowlist all existing users of //third_party/tensorflow/compiler/jit:jit. This prevents the jit library from being used in new packages.,2025-02-20T00:27:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87612
copybara-service[bot],xla::ShardingPropagation. Avoid applying sharding constraints if the operand is used by multiple sharding constraints with different shardings.,"xla::ShardingPropagation. Avoid applying sharding constraints if the operand is used by multiple sharding constraints with different shardings. With `B = customcall(A), custom_call_target=""Sharding""`, we can set the sharding for A when all the three conditions are true. 1. Unspecified dims are empty. Otherwise, the sharding is open and can be further modified. 2. A has no sharding. We cannot overwrite the existing one. 3. A does not have other sharding constraints. A can have multiple sharding constraints with the same sharding. The first two conditions are checked before this cl. This cl add the third condition.",2025-02-20T00:20:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87611
copybara-service[bot],[XLA:MSA] Allow dynamic-slice when performing post allocation transformation in MSA.,"[XLA:MSA] Allow dynamicslice when performing post allocation transformation in MSA. Also, bypass users of constants when checking for inplace users. Reverts 4966569daa20d7296d62360a2b6ebeee10286040",2025-02-20T00:12:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87610
copybara-service[bot],Add a test for the `rng-bit-generator-expander` HLO optimization pass.,Add a test for the `rngbitgeneratorexpander` HLO optimization pass. This increases `rngbitgeneratorexpander` coverage from about 4% to about 89%.,2025-02-19T23:59:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87609
copybara-service[bot],Add copybara rules to update CL header dependencies for tests,Add copybara rules to update CL header dependencies for tests,2025-02-19T23:47:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87608
copybara-service[bot],[xla:python] Relax checks in custom call batch partitioner when some inputs are not sharded.,"[xla:python] Relax checks in custom call batch partitioner when some inputs are not sharded. The current behavior is to crash sharding propagation when some inputs to a batch partitionable FFI call don't have an associated sharding, but we really only require that at least on operand is sharded, and should perform the default action otherwise.",2025-02-19T23:45:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87607
copybara-service[bot],Nothing to see here. Move along.,Nothing to see here. Move along.,2025-02-19T23:39:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87606
copybara-service[bot],Use data_table_utils.cc to support DataTable JSON conversion for HloStats tool,Use data_table_utils.,2025-02-19T23:34:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87605
copybara-service[bot],Add a field for pcie bandwidth in SchedulerConfig,Add a field for pcie bandwidth in SchedulerConfig,2025-02-19T23:33:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87604
copybara-service[bot],Simple tool to run a model with CompiledModel APIs,Simple tool to run a model with CompiledModel APIs Usage: $ run_model graph= $ run_model graph= dispatch_library_dir=,2025-02-19T23:25:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87603
copybara-service[bot],litert: Create default signature if model doesn't have it,litert: Create default signature if model doesn't have it Signature is needed for CompiledModel implementation. This fix generates a default signature if model doesn't have it.,2025-02-19T22:54:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87602
copybara-service[bot],Fix syntax errors in gpu_benchmarks.yml and add a step to compute the cost,Fix syntax errors in gpu_benchmarks.yml and add a step to compute the cost,2025-02-19T22:47:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87601
disgoraj,fix-bug : add more datatype,This PR fixes issue CC(tf.raw_ops.Round outputs zeros for any integer tensor) by adding int32 and int64 support for the Round operation.,2025-02-19T22:38:17Z,stat:awaiting response stale size:XS comp:core,closed,0,4,https://github.com/tensorflow/tensorflow/issues/87600,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi  , Can you please sign CLA , thank you",This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,This PR was closed because it has been inactive for 14 days since being marked as stale. Please reopen if you'd like to work on this further.
copybara-service[bot],Add tensorflow/profiler as a dependency for tensorflow,Add tensorflow/profiler as a dependency for tensorflow This is part of an ongoing migration to move code out of tensorflow/tensorflow and into tensorflow/profiler,2025-02-19T22:31:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87599
copybara-service[bot],Remove extraneous include as it will always be override by the mutable schema,Remove extraneous include as it will always be override by the mutable schema,2025-02-19T22:21:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87598
copybara-service[bot],Add INT4 support in TFL::TransposeOp converter + runtime.,Add INT4 support in TFL::TransposeOp converter + runtime.,2025-02-19T22:19:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87597
copybara-service[bot],Properly define header file include sequence,Properly define header file include sequence,2025-02-19T22:04:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87596
copybara-service[bot],PR #22829: Add GCC Version Detection and Enforce XNNPACK Compatibility Flags,PR CC(can tensorflow tensorRT (with multiple outputs) be used for object dectection? can ): Add GCC Version Detection and Enforce XNNPACK Compatibility Flags Imported from GitHub PR https://github.com/openxla/xla/pull/22829  Description:    Added `_get_gcc_major_version` function to `configure.py` to determine the major version of GCC.    Adjusted XNNPACK build flags dynamically based on detected Clang and GCC versions:      Disabled `mavxvnniint8` for Clang : Add GCC Version Detection and Enforce XNNPACK Compatibility Flags Merging this change closes CC(can tensorflow tensorRT (with multiple outputs) be used for object dectection? can ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22829 from apivovarov:gcc_ver_flags a44a5d6ebd5fbdbc544f97dc7cc00fbe03c6eba7,2025-02-19T22:01:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87595
copybara-service[bot],Consolidate passes that operate on imported HLO,Consolidate passes that operate on imported HLO,2025-02-19T21:49:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87594
copybara-service[bot],Rename `CompilerPluginLibraryPath` to `CompilerPluginLibraryDir` to align with DispatchLibraryDir.,Rename `CompilerPluginLibraryPath` to `CompilerPluginLibraryDir` to align with DispatchLibraryDir.,2025-02-19T21:49:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87593
copybara-service[bot],Augment `stablehlo-legalize-quant-to-math` to support NCHW layout.,Augment `stablehlolegalizequanttomath` to support NCHW layout.,2025-02-19T21:45:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87592
copybara-service[bot],Integrate LLVM at llvm/llvm-project@dca73063653c,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match dca73063653c,2025-02-19T21:15:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87591
copybara-service[bot],remove kokoro_init_linux from cpu_builds which runs apt-get openjdk-21-jdk to avoid dpkg error,remove kokoro_init_linux from cpu_builds which runs aptget openjdk21jdk to avoid dpkg error,2025-02-19T21:00:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87590
copybara-service[bot],"[hlo-opt] Produce fatal message if pass already registered. There are multiple codepath via a HLO pass can be registered to the hlo-opt pass registry. To avoid multiple registrations, produce a fatal error.","[hloopt] Produce fatal message if pass already registered. There are multiple codepath via a HLO pass can be registered to the hloopt pass registry. To avoid multiple registrations, produce a fatal error. also it will help to identify if a user adds new pass with a same name as the existing pass.",2025-02-19T20:49:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87589
copybara-service[bot],Add buffer requirements check to LiteRT compiled model and dispatch delegate tests.,Add buffer requirements check to LiteRT compiled model and dispatch delegate tests.,2025-02-19T20:38:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87588
copybara-service[bot],Test Eigen update.,Test Eigen update.,2025-02-19T20:37:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87587
tensorflow-jenkins,"r2.19 cherry-pick: 2b775610a7b ""Update libtpu installation index path""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/2b775610a7b49450f406933e7a96d06ff37e3bd3,2025-02-19T19:54:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87586
copybara-service[bot],Add dynamic range quantize pass with strict QDQ mode,Add dynamic range quantize pass with strict QDQ mode strict QDQ mode explicitly adds the annotations so run the dynamic range quantize passes in that mode.,2025-02-19T19:45:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87585
copybara-service[bot],Change size_in_bytes argument type from int to size_t.,"Change size_in_bytes argument type from int to size_t. Other uses of it are size_t, so this makes it consistent.",2025-02-19T19:43:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87584
copybara-service[bot],[XLA:LatencyHidingScheduler] Fix the double counting in `RecursivelyComputeResourceMap`.,"[XLA:LatencyHidingScheduler] Fix the double counting in `RecursivelyComputeResourceMap`. This function is only called when we want to understand whether scheduling the current instruction crosses the overlap limit, so it only focuses on the `kOccupy` type of resources.",2025-02-19T19:10:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87583
copybara-service[bot],Add Q/DQ ops after convolution.,Add Q/DQ ops after convolution.,2025-02-19T18:44:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87582
copybara-service[bot],Expose ExecutableBuildOptions::CompileOptions and the method AddEnv to python,Expose ExecutableBuildOptions::CompileOptions and the method AddEnv to python Add a default TpuCompilationEnvironment to the wiz export,2025-02-19T18:10:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87581
copybara-service[bot],Pass to replace TFL::BatchMatMulOp with TFL::FullyConnectedOp if weights tensor is INT4 quantized and is of rank-2.,Pass to replace TFL::BatchMatMulOp with TFL::FullyConnectedOp if weights tensor is INT4 quantized and is of rank2.,2025-02-19T18:00:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87580
jabidahscreationssystems,Untitled,Add a new file named `Bith`.,2025-02-19T17:57:44Z,size:XS invalid,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87579,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Patch Tensorflow memory corruption issue related to VARIANTS,Patch Tensorflow memory corruption issue related to VARIANTS This patch should mitigate a memory corruption issue in the TF codebase.,2025-02-19T17:29:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87578
copybara-service[bot],Integrate LLVM at llvm/llvm-project@760ec2c38e0c,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 760ec2c38e0c,2025-02-19T16:22:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87577
copybara-service[bot],Enable `AddOptimizationPasses` when `quant_specs.strict_qdq_mode` is true.,"Enable `AddOptimizationPasses` when `quant_specs.strict_qdq_mode` is true. * This includes ewise optimization, broadcast optimization, batchmatmul optimization and optimize pass. * Add a missing case for int4 in `lower_quant_annotations_helper`.",2025-02-19T16:18:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87576
copybara-service[bot],"Fix per-axis quantization of TFL::TransposeOp. This is currently hard-coded to I8FP32, making this generic to support any per-axis quant types.","Fix peraxis quantization of TFL::TransposeOp. This is currently hardcoded to I8FP32, making this generic to support any peraxis quant types.",2025-02-19T16:09:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87575
copybara-service[bot],Clean up LiteRT compiled model tests.,Clean up LiteRT compiled model tests.,2025-02-19T16:03:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87574
copybara-service[bot],Remove excessive info logging.,Remove excessive info logging.,2025-02-19T15:55:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87573
copybara-service[bot],PR #22800: Change the default value of print_operand_shape_ to false and print_large_constants_ to true.,"PR CC(Variance Inflation Factor estimate for each layer as guidance for dropout): Change the default value of print_operand_shape_ to false and print_large_constants_ to true. Imported from GitHub PR https://github.com/openxla/xla/pull/22800 Operand shape in long hlo text adds redundant information, which shouldn't be required. Changing the default value to off. The large constants were also printed earlier by default print options, and it is required for parsability and reproducibility. Turning this on by default. This is still controlled by debug option and the default value of that flag disables the large constants, and that behavior is not changed. Just the default print options change here. Copybara import of the project:  7d2427432c6924becc5df8276fd01991d129a5ed by Shraiysh Vaishay : Change the default value of print_operand_shape_ to false and print_large_constants_ to true. Operand shape in long hlo text adds redundant information, which shouldn't be required. Changing the default value to off. The large constants were also printed earlier by default print options, and it is required for parsability and reproducibility. Turning this on by default. This is still controlled by debug option and the default value of that flag disables the large constants, and that behavior is not changed. Just the default print options change here. Merging this change closes CC(Variance Inflation Factor estimate for each layer as guidance for dropout) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22800 from shraiysh:change_default_print_op_shape 7d2427432c6924becc5df8276fd01991d129a5ed",2025-02-19T15:48:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87572
copybara-service[bot],Create Dependabot.yml to manage GitHub Actions versions,Create Dependabot.yml to manage GitHub Actions versions,2025-02-19T15:23:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87571
copybara-service[bot],Add cache entries for reshape ops in SPMD.,"Add cache entries for reshape ops in SPMD. We may have two compatible sharding pairs when handling reshape. If we have two pairs, we use the first one. We can still use the second one to add a reshard cache. Given the following reshape, ``` p0 = bf16[8,8] parameter(0), sharding={replicated} reshape = bf16[64] reshape(p0), sharding={devices=[4] (bf16[16], bf16[64]) {   %param = bf16[8,8]{1,0} parameter(0), sharding={replicated}   %constant = s32[4]{0} constant({0, 2, 4, 6})   %partitionid = u32[] partitionid()   %dynamicslice = s32[1]{0} dynamicslice(s32[4]{0} %constant, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.1 = s32[] reshape(s32[1]{0} %dynamicslice)   %constant.1 = s32[] constant(0)   %dynamicslice.1 = bf16[2,8]{1,0} dynamicslice(bf16[8,8]{1,0} %param, s32[] %reshape.1, s32[] %constant.1), dynamic_slice_sizes={2,8}   %reshape.2 = bf16[16]{0} reshape(bf16[2,8]{1,0} %dynamicslice.1)   %allgather = bf16[64]{0} allgather(bf16[16]{0} %reshape.2), channel_id=1, replica_groups={{0,1,2,3}}, dimensions={0}, use_global_device_ids=true   %abs.1 = bf16[64]{0} abs(bf16[64]{0} %allgather)   ROOT %tuple.1 = (bf16[16]{0}, bf16[64]{0}) tuple(bf16[16]{0} %reshape.2, bf16[64]{0} %abs.1) } ``` With this change, we replace reshard (allgather) with reshape ``` ENTRY %reshape_spmd (param: bf16[8,8]) > (bf16[16], bf16[64]) {   %param = bf16[8,8]{1,0} parameter(0), sharding={replicated}   %constant = s32[4]{0} constant({0, 2, 4, 6})   %partitionid = u32[] partitionid()   %dynamicslice = s32[1]{0} dynamicslice(s32[4]{0} %constant, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.1 = s32[] reshape(s32[1]{0} %dynamicslice)   %constant.1 = s32[] constant(0)   %dynamicslice.1 = bf16[2,8]{1,0} dynamicslice(bf16[8,8]{1,0} %param, s32[] %reshape.1, s32[] %constant.1), dynamic_slice_sizes={2,8}   %reshape.2 = bf16[16]{0} reshape(bf16[2,8]{1,0} %dynamicslice.1)   %reshape.3 = bf16[64]{0} reshape(bf16[8,8]{1,0} %param)   %abs.1 = bf16[64]{0} abs(bf16[64]{0} %reshape.3)   ROOT %tuple.1 = (bf16[16]{0}, bf16[64]{0}) tuple(bf16[16]{0} %reshape.2, bf16[64]{0} %abs.1) } ```",2025-02-19T08:19:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87533
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-19T08:13:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87532
copybara-service[bot],Restrict XLA bzl files to XLA itself and legacy users.,Restrict XLA bzl files to XLA itself and legacy users.,2025-02-19T08:09:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87531
copybara-service[bot],Integrate LLVM at llvm/llvm-project@0afe2bd21b06,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 0afe2bd21b06,2025-02-19T08:05:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87530
copybara-service[bot],Integrate LLVM at llvm/llvm-project@0afe2bd21b06,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 0afe2bd21b06,2025-02-19T06:56:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87529
copybara-service[bot],PR #87409: Qualcomm AI Engine Direct - Fix copyright format,PR CC(Qualcomm AI Engine Direct  Fix copyright format): Qualcomm AI Engine Direct  Fix copyright format Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/87409 Summary:  Fix QC copyright format Copybara import of the project:  ca369708931dfdc2a5eee6e6c70ada5d652ebdda by jiunkaiy : Qualcomm AI Engine Direct  Fix copyright format Summary:  Fix QC copyright format Merging this change closes CC(Qualcomm AI Engine Direct  Fix copyright format) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/87409 from jiunkaiy:dev/jiunkaiy/qualcomm_copyright ca369708931dfdc2a5eee6e6c70ada5d652ebdda,2025-02-19T06:15:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87528
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-19T05:39:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87527
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-19T05:37:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87526
copybara-service[bot],[Gradients] Add experimental special case for `IdentityN` that avoids constructing large zero tensors.,[Gradients] Add experimental special case for `IdentityN` that avoids constructing large zero tensors.,2025-02-19T05:22:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87525
copybara-service[bot],Update libtpu installation index path,"Update libtpu installation index path Update the libtpu installation index to https://storage.googleapis.com/libtpuwheels/index.html, which includes both stable and nightly libtpu versions, as per the cloud libtpu team's guidance. Also update the libtpu version in the setup.py. It starts to differ from the TF version to support JAX, and it requires manual updates for new releases for now.",2025-02-19T04:47:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87524
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-19T04:15:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87523
copybara-service[bot],Add composite test model,Add composite test model,2025-02-19T03:16:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87522
copybara-service[bot],Add a test for the `rng-expander` HLO optimization pass.,Add a test for the `rngexpander` HLO optimization pass.,2025-02-19T01:50:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87521
copybara-service[bot],Add Op builder for RMS norm composite.,Add Op builder for RMS norm composite.,2025-02-19T01:37:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87520
copybara-service[bot],Added incarnation to `GetTaskState` RPC in coordination service.,Added incarnation to `GetTaskState` RPC in coordination service.,2025-02-19T01:16:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87519
copybara-service[bot],Update the visibility paths.,Update the visibility paths.,2025-02-19T01:14:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87518
copybara-service[bot],Adds a xla_bzl_visibility macro for declaring the visibility of a bzl file.,Adds a xla_bzl_visibility macro for declaring the visibility of a bzl file.,2025-02-19T01:12:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87517
copybara-service[bot],Simplify heartbeat options for PjRt distributed runtime.,Simplify heartbeat options for PjRt distributed runtime.,2025-02-19T01:10:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87516
copybara-service[bot],Clarify that multiple --xla_dump_hlo_as_* flags can be combined.,Clarify that multiple xla_dump_hlo_as_* flags can be combined.,2025-02-19T00:59:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87515
copybara-service[bot],"Remove `build --define=xnn_enable_avxvnniint8=false` from top-level .bazelrc, add it to relevant MacOS builds","Remove `build define=xnn_enable_avxvnniint8=false` from toplevel .bazelrc, add it to relevant MacOS builds This was originally an imprecise change from me, adding this in the configure.py generated bazelrc will be handled in https://github.com/openxla/xla/pull/22829.",2025-02-19T00:47:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87514
copybara-service[bot],Int4 support for TFL::CastOp.,Int4 support for TFL::CastOp.,2025-02-19T00:37:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87513
copybara-service[bot],PR #22830: Fix macOS build.,PR CC(can tensorflow tensorRT be used for  optimization of object detection model): Fix macOS build. Imported from GitHub PR https://github.com/openxla/xla/pull/22830 This fixes builds on macOS without Xcode (with Clang from Homebrew for example). Copybara import of the project:  4b5afd519ff62e8fe9d013030789edfc56ad6110 by Ilia Sergachev : Update apple_support to 1.18.0.  9a9c42f6836ce758add28550cf157552a426ae92 by Ilia Sergachev : Add bazel_features 1.25.0. Merging this change closes CC(can tensorflow tensorRT be used for  optimization of object detection model) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22830 from openxla:fix_osx_build 9a9c42f6836ce758add28550cf157552a426ae92,2025-02-19T00:34:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87512
copybara-service[bot],No need to include QNN headers in dispatch delegate BUILD.,No need to include QNN headers in dispatch delegate BUILD.,2025-02-18T23:52:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87511
copybara-service[bot],PR #87305: Qualcomm AI Engine Direct - Another implementation of DUS without accuracy issue.,PR CC(Qualcomm AI Engine Direct  Another implementation of DUS without accuracy issue.): Qualcomm AI Engine Direct  Another implementation of DUS without accuracy issue. Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/87305  What New implementation of DUS which resolves the previously found accuracy issue.  Tests ``` [] Global test environment teardown [==========] 97 tests from 5 test suites ran. (3264 ms total) [  PASSED  ] 97 tests. ``` Copybara import of the project:  84abc58d90e7504acc000fe16e164422e05f111d by chunhsue : Qualcomm AI Engine Direct  Another implementation of DUS without accuracy issue. Merging this change closes CC(Qualcomm AI Engine Direct  Another implementation of DUS without accuracy issue.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/87305 from jiunkaiy:dev/chunhsue/new_DUS 84abc58d90e7504acc000fe16e164422e05f111d,2025-02-18T23:32:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87510
copybara-service[bot],Refactor `PjRtStreamExecutorDeviceDescription` and `StreamExecutorGpuTopologyDescription`,Refactor `PjRtStreamExecutorDeviceDescription` and `StreamExecutorGpuTopologyDescription`,2025-02-18T23:06:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87509
copybara-service[bot],Remove `ci_continuous_only.yml` for now as we have no continuous only GitHub Actions builds,Remove `ci_continuous_only.yml` for now as we have no continuous only GitHub Actions builds,2025-02-18T22:13:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87508
copybara-service[bot],Add flop adjustment to custom call which using cublasLt matmul in xprof,Add flop adjustment to custom call which using cublasLt matmul in xprof,2025-02-18T22:13:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87507
copybara-service[bot],Fix a trivial typo,Fix a trivial typo,2025-02-18T21:44:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87506
copybara-service[bot],Move some strategy generation utilities from auto_sharding_dot_handler.cc to,Move some strategy generation utilities from auto_sharding_dot_handler..h with the intention of using the utilities more broadly throughout the codebase. Reverts 1e0f639a26b2aafd2732ae0e64817b7cdd387c81,2025-02-18T21:25:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87505
copybara-service[bot],Increase the TF GPU wheel size to 630 MB.,Increase the TF GPU wheel size to 630 MB.,2025-02-18T21:21:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87504
copybara-service[bot],litert: Handle tensors are shared with CPU kernels,"litert: Handle tensors are shared with CPU kernels Currently, it uses NPU requirements When a tensor is shared with bot CPU and NPU. This CL changes to return a CPU friendly requirements to make it runnable with CPU kernels. In NPU, it will creates own NPU TensorBuffer and copy the content. To check the shared tensors, introduced the CheckCpuTensors() method and cpu_tensors_ set. Also updated SetCustomAllocationForTensor() to compatible with kTfLiteNonCpu type.",2025-02-18T21:21:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87503
copybara-service[bot],[PJRT:CPU] Support per-execution process_index and collectives overrides,"[PJRT:CPU] Support perexecution process_index and collectives overrides This changes extends `xla::ExecuteContext` as a new subclass `xla::CpuExecuteContext` recognized by the PjRt CPU client. `xla::CpuExecuteContext` accepts optional `process_index` and `collectives`. When specified, each configuration will override `process_index` and/or `collectives` instead of using the value specified at CPU client construction time. The goal of these overrides is to allow reconfiguring collectives while endpoints can be dynamically added or removed. Since reconfiguration does not require recreating the CPU client, any user state can be preserved across collectives reconfiguration and reduce the overall system reconfiguration cost by not having to checkpoint the state outside of the CPU client.",2025-02-18T20:56:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87502
copybara-service[bot],Need to update ready of occupiers when adding a new occupier.,Need to update ready of occupiers when adding a new occupier. Also make kCopy resource shareable. Add a TargetPtr() method in HloGraphNode for null check since some tests pass in null target.,2025-02-18T20:47:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87501
copybara-service[bot],Partially roll back to old ways of initializing QNN manager.,Partially roll back to old ways of initializing QNN manager.,2025-02-18T19:53:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87500
copybara-service[bot],Remove `mac_arm.patch` now that we use an LLVM with https://github.com/llvm/llvm-project/pull/127637,Remove `mac_arm.patch` now that we use an LLVM with https://github.com/llvm/llvmproject/pull/127637,2025-02-18T19:48:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87499
copybara-service[bot],PR #22800: Change the default value of print_operand_shape_ to false and print_large_constants_ to true.,"PR CC(Variance Inflation Factor estimate for each layer as guidance for dropout): Change the default value of print_operand_shape_ to false and print_large_constants_ to true. Imported from GitHub PR https://github.com/openxla/xla/pull/22800 Operand shape in long hlo text adds redundant information, which shouldn't be required. Changing the default value to off. The large constants were also printed earlier by default print options, and it is required for parsability and reproducibility. Turning this on by default. This is still controlled by debug option and the default value of that flag disables the large constants, and that behavior is not changed. Just the default print options change here. Copybara import of the project:  e30dea20489b3fb4d03d373fec0391d69486f4aa by Shraiysh Vaishay : Change the default value of print_operand_shape_ to false and print_large_constants_ to true. Operand shape in long hlo text adds redundant information, which shouldn't be required. Changing the default value to off. The large constants were also printed earlier by default print options, and it is required for parsability and reproducibility. Turning this on by default. This is still controlled by debug option and the default value of that flag disables the large constants, and that behavior is not changed. Just the default print options change here.  7008af0dd0ce342ecbe9475f1d0e277319f1705a by Shraiysh Vaishay : Handle tests  b22d5f95cfb7e15f930a2198279a76c38593cc53 by Shraiysh Vaishay : Fix more tests  d51579cae7359c6426a87ad4a7ff1b4b0c80f74a by Shraiysh Vaishay : Fix more tests Merging this change closes CC(Variance Inflation Factor estimate for each layer as guidance for dropout) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22800 from shraiysh:change_default_print_op_shape d51579cae7359c6426a87ad4a7ff1b4b0c80f74a",2025-02-18T19:39:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87498
copybara-service[bot],[Dtensor] Remove MHLO dependency from DTensor. ,[Dtensor] Remove MHLO dependency from DTensor.  There are uses of mhlo.sharding but it is just a string as a dictionary key. We are not planning to change it for now.,2025-02-18T19:20:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87497
copybara-service[bot],#litert Fix `litert_accelerator_registration.h` include statements.,litert Fix `litert_accelerator_registration.h` include statements.,2025-02-18T19:01:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87496
copybara-service[bot],#litert Add missing include.,litert Add missing include.,2025-02-18T18:59:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87495
copybara-service[bot],#litert Pass the compilation options to accelerators' `CreateDelegate()`.,litert Pass the compilation options to accelerators' `CreateDelegate()`.,2025-02-18T18:54:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87494
copybara-service[bot],#litert Simplify `LiteRtAppendAcceleratorCompilationOptions()`'s usage.,"litert Simplify `LiteRtAppendAcceleratorCompilationOptions()`'s usage. This change allows passing the head of the list as a `nullptr` instead of having to manually set it. Before: ```cpp if (!options>accelerator_compilation_options) {   options>accelerator_compilation_options = accelerator_compilation_options; } else {   LiteRtAppendAcceleratorCompilationOptions(       options>accelerator_compilation_options,       accelerator_compilation_options); } ``` After ```cpp LiteRtAppendAcceleratorCompilationOptions(     &options>accelerator_compilation_options,     accelerator_compilation_options); ```",2025-02-18T18:53:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87493
copybara-service[bot],#litert Improve helper macros performance.,litert Improve helper macros performance. By adding move operations for the `ErrorStatusReturnHelper`. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/87645 from tensorflow:fixtypos17 eb42bd508217699c2dc507b96159f64ca2bfab0a,2025-02-18T18:49:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87492
copybara-service[bot],[XLA:GPU][TMA] Add an attribute to label func args with the TMA data.,[XLA:GPU][TMA] Add an attribute to label func args with the TMA data.,2025-02-18T18:26:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87491
copybara-service[bot],PR #22815: [PJRT C API] Ensure C Compliance for all C headers,"PR CC(Object detection api training issue): [PJRT C API] Ensure C Compliance for all C headers Imported from GitHub PR https://github.com/openxla/xla/pull/22815 `` headers are C++ headers that wrap their `` counteparts in the std namespace and reexports them as well. It is meant to be consumed by C++ compilers, not C compilers. Since this is a C API, this PR replaces usages of `` include statements by their C counterparts only for exported C api headers. This PR supersedes https://github.com/openxla/xla/pull/22082 and fixes it across the whole C API. Copybara import of the project:  d2a1096100428e1cc61787b054feb998a73bf725 by Corentin Kerisit : [PJRT C API] Ensure C Compliance for all C headers  headers are C++ headers that wrap their  counteparts in the std namespace and reexports them as well.. It is meant to be consumed by C++ compilers, not C compilers. Since this is a C API, this PR replaces usages of  include statements by their C counterparts only for exported C api headers. This PR supersedes https://github.com/openxla/xla/pull/22082 and fixes it across the whole C API.  f1f6eb6b9c05dc16f150eaf33fbe471e9f13b56c by Corentin Kerisit : Add missing typedef when refering to structs Merging this change closes CC(Object detection api training issue) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22815 from cerisier:cerisir/fixccompatibility f1f6eb6b9c05dc16f150eaf33fbe471e9f13b56c",2025-02-18T18:16:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87490
copybara-service[bot],Removed unused AsNcclUniqueIds function.,Removed unused AsNcclUniqueIds function.,2025-02-18T18:03:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87489
copybara-service[bot],Fix a typo in third_party/tensorflow/core/framework/tensor.h.,Fix a typo in third_party/tensorflow/core/framework/tensor.h.,2025-02-18T17:34:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87488
copybara-service[bot],Allow flatbuffer_export.cc to handle/serialize StableHLO ops to TFL FB. This is currently disabled.,Allow flatbuffer_export./serialize StableHLO ops to TFL FB. This is currently disabled.,2025-02-18T16:53:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87487
copybara-service[bot],Fix a typo in third_party/tensorflow/core/framework/tensor.h.,Fix a typo in third_party/tensorflow/core/framework/tensor.h.,2025-02-18T16:43:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87486
copybara-service[bot],[mlir/lite] mmap_allocation: Use sysconf(_SC_PAGE_SIZE) on Android,[mlir/lite] mmap_allocation: Use sysconf(_SC_PAGE_SIZE) on Android Remove special case ifdef.  There's no need to use getpagesize() and have a different code path.,2025-02-18T16:36:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87485
copybara-service[bot],[XLA:CPU] Add invariant arguments to KernelSpec.,[XLA:CPU] Add invariant arguments to KernelSpec.,2025-02-18T16:07:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87484
copybara-service[bot],[XLA:CPU] Store arguments and results separately in KernelSpec,[XLA:CPU] Store arguments and results separately in KernelSpec,2025-02-18T16:07:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87483
copybara-service[bot],Reduce dependencies on `MultiHostHloRunner::LoadHloModuleAndArguments`.,Reduce dependencies on `MultiHostHloRunner::LoadHloModuleAndArguments`. Most users just need to load the module from a text file.,2025-02-18T16:03:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87482
copybara-service[bot],[XLA:CPU] Improve invariant argument checking in KernelThunk execution,[XLA:CPU] Improve invariant argument checking in KernelThunk execution,2025-02-18T15:58:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87481
copybara-service[bot],Add a patch to llvm to fix jax's mac arm builds,Add a patch to llvm to fix jax's mac arm builds,2025-02-18T15:41:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87480
copybara-service[bot],Integrate LLVM at llvm/llvm-project@9d24f9437944,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 9d24f9437944,2025-02-18T15:34:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87479
copybara-service[bot],[xla:cpu] Optimize non-grouped convolution,[xla:cpu] Optimize nongrouped convolution ``` BM_Conv2D/8/5/5/1/1/1/32/process_time                     1.57µs ± 8%  1.54µs ± 8%   1.84%  (p=0.007 n=40+40) BM_Conv2D/8/5/5/4/1/1/32/process_time                     3.09µs ± 4%  3.09µs ± 4%     ~     (p=0.770 n=40+40) BM_Conv2D/8/128/128/4/1/1/8/process_time                   872µs ±10%   899µs ±14%   +3.09%  (p=0.044 n=38+40) BM_Conv2D/8/32/32/128/1/1/1024/process_time               71.4ms ±11%  70.7ms ±10%     ~     (p=0.382 n=38+35) BM_Conv2D/16/32/32/128/1/1/1024/process_time               187ms ±13%   189ms ±11%     ~     (p=0.622 n=40+40) BM_Conv2D/32/32/32/128/1/1/1024/process_time               347ms ±10%   345ms ±10%     ~     (p=0.642 n=40+40) BM_Conv2D/32/64/64/32/1/1/64/process_time                 43.2ms ± 7%  43.6ms ±13%     ~     (p=0.418 n=40+40) BM_Conv2D/32/256/256/4/1/1/16/process_time                 127ms ± 4%   128ms ± 4%     ~     (p=0.074 n=35+39) BM_Conv2D/32/64/64/4/1/1/16/process_time                  2.46ms ±11%  2.46ms ±15%     ~     (p=0.717 n=38+40) BM_Conv2D/32/32/32/96/1/1/96/process_time                 23.7ms ± 7%  23.7ms ± 6%     ~     (p=0.939 n=40+37) BM_Conv2D/8/5/5/1/3/3/32/process_time                     20.4µs ± 1%  21.3µs ± 2%   +4.27%  (p=0.000 n=39+38) BM_Conv2D/8/5/5/4/3/3/32/process_time                     51.0µs ± 1%  27.0µs ± 2%  47.13%  (p=0.000 n=37+35) BM_Conv2D/8/128/128/4/3/3/8/process_time                  72.3ms ± 3%  21.9ms ± 3%  69.71%  (p=0.000 n=40+39) BM_Conv2D/8/32/32/128/3/3/1024/process_time                761ms ± 5%   637ms ± 7%  16.30%  (p=0.000 n=38+40) BM_Conv2D/16/32/32/128/3/3/1024/process_time               1.28s ± 6%   1.10s ± 5%  14.50%  (p=0.000 n=38+39) BM_Conv2D/32/32/32/128/3/3/1024/process_time               2.55s ± 3%   2.23s ± 5%  12.41%  (p=0.000 n=40+39) BM_Conv2D/32/64/64/32/3/3/64/process_time                  301ms ± 6%   180ms ±10%  40.25%  (p=0.000 n=38+38) BM_Conv2D/32/256/256/4/3/3/16/process_time                 1.46s ± 4%   0.39s ±14%  73.50%  (p=0.000 n=38+37) BM_Conv2D/32/64/64/4/3/3/16/process_time                  80.0ms ± 4%  24.0ms ± 3%  69.97%  (p=0.000 n=40+39) BM_Conv2D/32/32/32/96/3/3/96/process_time                  259ms ± 6%   201ms ± 9%  22.40%  (p=0.000 n=39+38) BM_GroupedConv2D/1/45/45/1024/5/5/1024/1024/process_time        491ms ± 6%   509ms ± 8%   +3.55%  (p=0.001 n=40+40) BM_Conv1DStrided/1/129/process_time                            29.2ms ± 6%  25.7ms ± 9%  11.94%  (p=0.000 n=39+40) BM_Conv1DStrided/3/129/process_time                             136ms ± 4%    76ms ± 8%  44.51%  (p=0.000 n=40+40) BM_Conv1DTransposedStrided/129/1/process_time                  28.1ms ± 5%  28.2ms ± 6%     ~     (p=0.950 n=40+40) BM_Conv1DTransposedStrided/129/3/process_time                  67.8ms ± 7%  66.5ms ± 7%   2.00%  (p=0.010 n=38+40) BM_Conv1DTransposedStridedNonDefaultLayout/129/1/process_time  25.0ms ± 5%  24.8ms ± 6%     ~     (p=0.074 n=39+39) BM_Conv1DTransposedStridedNonDefaultLayout/129/3/process_time  67.1ms ± 4%  65.5ms ± 7%   2.33%  (p=0.001 n=38+40) BM_Conv2DStrided/process_time                                  31.6ms ± 6%  28.0ms ± 5%  11.50%  (p=0.000 n=40+40) BM_Conv2DTransposedStrided/process_time                        28.0ms ± 4%  27.9ms ± 4%     ~     (p=0.555 n=39+40) BM_GroupedConv2DStrided/128/128/128/process_time               64.7ms ± 4%  64.8ms ± 3%     ~     (p=0.429 n=40+40) BM_GroupedConv2DTransposedStrided/128/128/128/process_time      5.58s ± 1%   5.58s ± 1%     ~     (p=0.172 n=39+39) BM_GroupedConv2DStrided/128/128/16/process_time                37.3ms ± 3%  37.6ms ± 2%   +0.72%  (p=0.010 n=40+38) BM_GroupedConv2DTransposedStrided/128/128/16/process_time       1.02s ± 2%   1.02s ± 2%     ~     (p=0.195 n=37+38) ```,2025-02-18T15:23:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87478
copybara-service[bot],[mlir/lite] flatbuffer_export: Improve message for StableHLO ops,"[mlir/lite] flatbuffer_export: Improve message for StableHLO ops To convert StableHLO ops, Translator::Translate must be called with serialize_stablehlo_ops=true.  If using TfLiteConverter, this is done by setting target_spec.suggested_ops={tf.lite.OpsSet.EXPERIMENTAL_STABLEHLO_OPS}. However, this was not obvious from the old error message of ``` error: 'vhlo.reduce_v1' op is not part of the vhlo support yet. ``` which was also misleading, since `vhlo.reduce_v1` is, in fact, supported with the right options. The new error message is: ``` error: 'vhlo.reduce_v1' op is part of the vhlo support, but Translator::Translate was called with serialize_stablehlo_ops=false.  If using TfLiteConverter, set target_spec.suggested_ops={tf.lite.OpsSet.EXPERIMENTAL_STABLEHLO_OPS}. ```",2025-02-18T14:50:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87477
copybara-service[bot],#litert Add `LiteRtAnyTypeToString` and `litert::Get<T>(LiteRtAny&)`.,litert Add `LiteRtAnyTypeToString` and `litert::Get(LiteRtAny&)`. The new C++ extraction helper checks that the object stored in the `LiteRtAny` is of a compatible type.,2025-02-18T14:46:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87476
copybara-service[bot],[mlir/lite] ConvertTfExecutorToTFLOrFlatbuffer: Delete serialize_stablehlo_ops arg,[mlir/lite] ConvertTfExecutorToTFLOrFlatbuffer: Delete serialize_stablehlo_ops arg This is never used.  Its use was removed in cl/647742961 (Clone the module into a new context to erase the unused tensors from memory). https://github.com/tensorflow/tensorflow/commit/df25ab1dafc77af272c39f513329c5a21456245a Cleanup,2025-02-18T14:03:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87475
copybara-service[bot],Integrate LLVM at llvm/llvm-project@74656476b860,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 74656476b860,2025-02-18T14:01:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87474
copybara-service[bot],Fix neuron library loading when libneuron_adapter.so is installed system-wide.,Fix neuron library loading when libneuron_adapter.so is installed systemwide.,2025-02-18T13:20:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87473
copybara-service[bot],Fix macos build.,Fix macos build.,2025-02-18T13:14:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87472
copybara-service[bot],Integrate LLVM at llvm/llvm-project@9d24f9437944,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 9d24f9437944,2025-02-18T13:11:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87471
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T13:05:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87470
copybara-service[bot],[XLA:GPU] Fix slow compile time for https://github.com/jax-ml/jax/issues/26162,[XLA:GPU] Fix slow compile time for https://github.com/jaxml/jax/issues/26162,2025-02-18T13:04:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87469
knebojsa11,[RNN] Conversion of model containing GRU layer to quantized TFLite causes Segmentation Fault," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04  TensorFlow installation (pip package or built from source): both pip package and built from source  TensorFlow library (version, if pip package or github SHA, if built from source): 2.18  2. Code import os os.environ[""TF_USE_LEGACY_KERAS""] = ""1"" import tensorflow as tf import numpy as np import tf_keras as keras def gru_model():     """"""Factory method for gru model.""""""     gru_input = keras.layers.Input(batch_input_shape=(1, 1, 64),                                     name=""gru_input"")     gru_state_in = keras.layers.Input(batch_input_shape=(1, 128),                                        name=""gru_state_in"")     gru_output, gru_state_out = keras.layers.GRU(128,                                                  activation=""tanh"",                                                  recurrent_activation=""sigmoid"",                                                  use_bias=True,                                                  return_sequences=False,                                                  bias_initializer=""random_uniform"",                                                  return_state=True)([gru_input, gru_state_in])     keras_model = keras.Model(inputs=[gru_input, gru_state_in], outputs=[gru_output, gru_state_out])     return keras_model model = gru_model() train_inputs = [tf.random.uniform((1, 1, 64), minval=1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32),                 tf.random.uniform((1, 128), minval=1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32)] train_outputs = tf.random.uniform((1, 128), minval=1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32) model.compile(optimizer='adam', loss='mse', metrics=['mse']) model.fit([train_inputs], train_outputs, batch_size=1, epochs=1) model.summary() model.save(""ticket_gru.keras"") print(""Converting to floating point Tensorflow Lite"") converter_fp = tf.lite.TFLiteConverter.from_keras_model(model) converter_fp.optimizations = [tf.lite.Optimize.DEFAULT] converter_fp.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS] tflite_model_fp = converter_fp.convert() with open(""ticket_gru_fp.tflite"",""wb"") as f: f.write(tflite_model_fp) print(""Success converting to floating point Tensorflow Lite containing WHILE operator in \ subgraph 0 and cell implementation in subgraph 1"") print(""Converting to quantized Tensorflow Lite"") def representative_data():   for _ in range(10):     yield {model.inputs[0].name:tf.random.uniform((1, 1, 64), minval=1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32, seed=None, name=None),            model.inputs[1].name:tf.random.uniform((1, 128), minval=1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32, seed=None, name=None)} converter_q = tf.lite.TFLiteConverter.from_keras_model(model) converter_q.optimizations = [tf.lite.Optimize.DEFAULT] converter_q.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS] converter_q.inference_input_type = tf.int8 converter_q.inference_output_type = tf.int8 converter_q._experimental_disable_per_channel = True converter_q.representative_dataset = representative_data print(""Call to convert method of the converter_q will cause SEGFAULT during model calibration"") tflite_model_q = converter_q.convert() print(""This line is never reached"") with open(""ticket_gru_8_8.tflite"",""wb"") as f: f.write(tflite_model_q)  3. Failure after conversion Converter fails with throwing Segmentation fault. The fault is tracked down to the Calibration phase where the representative dataset is fead to the floating point model inferred by the interpreter instantiated by the converter. The main subgraph processing causes nested subgraph processing as a part of the WHILE operator invoke. All the operator in the nested subgraph are invoked, but the return from the model invoke ends with Segmentation Fault. The segmentation fault can be traced down to Tensorflow 2.14. Tensorflow 2.13 handles the quantized conversion properly. Conversion to floating point TFLite model works fine, and the floating point inference with same data that is used as representative dataset in quantized conversion does complete without problems  4. (optional) RNN conversion support Yes  RNN conversion support. Prefixed in the title as [RNN]  5. (optional) Any other info / logs 20250218 11:45:02.889542: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used. 20250218 11:45:04.329812: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303) 1/1 [==============================]  ETA: 0s  loss: 1.0199  gru_loss: 0.5099  gru_1_loss: 0.5099  gru_mse: 0.5099  gru_1_mse: 1/1 [==============================]  1s 1s/step  loss: 1.0199  gru_loss: 0.5099  gru_1_loss: 0.5099  gru_mse: 0.5099  gru_1_mse: 0.5099 Model: ""model"" __________________________________________________________________________________________________  Layer (type)                Output Shape                 Param    Connected to                   ==================================================================================================  gru_input (InputLayer)      [(1, 1, 64)]                 0         []                              gru_state_in (InputLayer)   [(1, 128)]                   0         []                              gru (GRU)                   [(1, 128),                   74496     ['gru_input[0][0]',                                          (1, 128)]                              'gru_state_in[0][0]']         ================================================================================================== Total params: 74496 (291.00 KB) Trainable params: 74496 (291.00 KB) Nontrainable params: 0 (0.00 Byte) __________________________________________________________________________________________________ Converting to floating point Tensorflow Lite WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1739875508.650404  896323 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format. W0000 00:00:1739875508.650452  896323 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency. 20250218 11:45:08.650993: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp96nzf981 20250218 11:45:08.656672: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve } 20250218 11:45:08.656711: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp96nzf981 I0000 00:00:1739875508.691084  896323 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled 20250218 11:45:08.698338: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle. 20250218 11:45:08.768960: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp96nzf981 20250218 11:45:08.807201: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 156213 microseconds. 20250218 11:45:09.144839: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. 20250218 11:45:09.264954: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 0.174 M  ops, equivalently 0.087 M  MACs Success converting to floating point Tensorflow Lite containing WHILE operator in subgraph 0 and cell implementation in subgraph 1 Converting to quantized Tensorflow Lite Call to convert method of the converter_q will cause SEGFAULT during model calibration /opt/samba/nxf35112/keras2_experiments/venv/lib/python3.10/sitepackages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.   warnings.warn( W0000 00:00:1739875512.496385  896323 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format. W0000 00:00:1739875512.496430  896323 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency. 20250218 11:45:12.496633: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpfx3q8bul 20250218 11:45:12.501629: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve } 20250218 11:45:12.501670: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpfx3q8bul 20250218 11:45:12.542409: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle. 20250218 11:45:12.609623: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpfx3q8bul 20250218 11:45:12.649789: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 153160 microseconds. 20250218 11:45:13.053211: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 0.174 M  ops, equivalently 0.087 M  MACs Segmentation fault",2025-02-18T11:08:09Z,stat:awaiting tensorflower type:support comp:lite TFLiteConverter TF 2.18,open,0,3,https://github.com/tensorflow/tensorflow/issues/87468,"Hi,   I apologize for the delay in my response, I have been able to replicate the similar behavior from my end for reference here is gistfile so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention. Thank you for your cooperation and patience.","Hi,  I apologize for the delay in my response, The error occurs because GRU layers have complex state management that conflicts with full integer quantization so if full integer quantization is not compulsary for your use case/ project you can go with either float16 Quantization or dynamic range quantization which is more compatible with RNN layers.  This is a limitation in the TensorFlow Lite quantization system when dealing with complex recurrent structures. GRU and LSTM layers involve internal states and complex cell implementations that the quantization process sometimes cannot properly handle resulting in segmentation faults during model calibration. I have tried from my end with float16 Quantization and dynamic range quantization it's working as expected for your reference here is gistfile Thank you for your cooperation and patience.",Dear   I really need the model to be fully quantized so I am interested if there are plans to fix the broken feature in the converter. Best Regards
copybara-service[bot],PR #22723: Fix call of overloaded Tile is ambiguous,"PR CC(bug: tf.train.Saver.save() fails if 2 tf.contrib.cudnn_rnn.cudnnLSTM instances passed to saver are built with build() method): Fix call of overloaded Tile is ambiguous Imported from GitHub PR https://github.com/openxla/xla/pull/22723  Fix GCC13 Build Error in AutoSharding Due to vector vs. absl::Span Ambiguity When building auto_sharding with GCC13, the following build error occurred: ``` xla/hlo/experimental/auto_sharding/auto_sharding.cc:895:37: error: call of overloaded 'Tile(const xla::Shape&, , , const xla::spmd::DeviceMesh&)' is ambiguous   895              ^~~~ ```  Solution: To resolve the ambiguity between `std::vector>` and `absl::Span` in `Tile()`, I introduced an overloaded Tile() function that takes `std::initializer_list mesh_dims`. Now, expressions like the following now compile successfully with GCC13: ``` Tile(shape, {0}, {0}, device_mesh); ```  Additional Changes  Removed the `Tile()` declaration from `auto_sharding.h` since it is already declared in `auto_sharding_util.h`. Copybara import of the project:  a28207f171c086130097d520079648b1548976dc by Alexander Pivovarov : Fix call of overloaded Tile is ambiguous Merging this change closes CC(bug: tf.train.Saver.save() fails if 2 tf.contrib.cudnn_rnn.cudnnLSTM instances passed to saver are built with build() method) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22723 from apivovarov:fix_Tile_init_list a28207f171c086130097d520079648b1548976dc",2025-02-18T11:04:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87467
copybara-service[bot],This is an internal clean-up change.,This is an internal cleanup change.,2025-02-18T10:58:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87466
copybara-service[bot],Teach GTest how to print `litert::Expected` values.,Teach GTest how to print `litert::Expected` values.,2025-02-18T10:53:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87465
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T10:17:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87464
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T09:35:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87463
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T09:30:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87462
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T09:18:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87461
copybara-service[bot],Use custom HLO serialization for HloUnoptimizedSnapshot.,Use custom HLO serialization for HloUnoptimizedSnapshot. This change makes it possible to dump HloUnoptimizedSnapshot protos that are over 2GiB in size (the proto binary serialization limit).,2025-02-18T09:17:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87460
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T09:13:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87459
copybara-service[bot],PR #22614: Fix hlo_opt printing of Hlo module,PR CC(what is operator name for python corresponding to kReducePrecision): Fix hlo_opt printing of Hlo module Imported from GitHub PR https://github.com/openxla/xla/pull/22614 The tool `hloopt` was not honoring the debug options within the HloModule while printing the HloModule. These options should be honored by the default printing of the HloModule as they are a part of the same HloModule. Fixed the print method to do this. This should now be reflected in all the tools using these debug options. Copybara import of the project:  a22584a819a0fc6ee8f41b4c50f4f8d68a6a2184 by Shraiysh Vaishay : Fix hlo_opt printing of Hlo module The tool `hloopt` was not honoring the debug options within the HloModule while printing the HloModule. These options should be honored by the default printing of the HloModule as they are a part of the same HloModule. Fixed the print method to do this. This should now be reflected in all the tools using these debug options.  b42178b4da3fd5f81fc2d50346cb2f9b18153ab5 by Shraiysh Vaishay : Rebase and avoid edits to testcases.  51cdfbfa355efe34936073fd68d4e19191131bb7 by Shraiysh Vaishay : Addressed failing test Merging this change closes CC(what is operator name for python corresponding to kReducePrecision) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22614 from shraiysh:fix_dumping_hlo_opt 51cdfbfa355efe34936073fd68d4e19191131bb7,2025-02-18T08:00:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87458
jwnhy,[CUDA] Check failed work_element_count >= 0," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Ubuntu 24.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuDNN 90300  GPU model and memory H100 80G  Current behavior? F0000 00:00:1739864447.179429 1679389 gpu_launch_config.h:129] Check failed: work_element_count >= 0 (1881720796 vs. 0) *** Check failure stack trace: ***     @     0x7fffec867bc4  absl::lts_20230802::log_internal::LogMessage::SendToLog()     @     0x7fffec8679c4  absl::lts_20230802::log_internal::LogMessage::Flush()     @     0x7fffec867fe9  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()     @     0x7fffe33017d4  tensorflow::functor::TransformFilter::operator()()     @     0x7fffe1896f56  tensorflow::LaunchConvOpImpl()     @     0x7fffe1a03561  tensorflow::Conv2DOp::Compute()     @     0x7ffff51474d5  tensorflow::BaseGPUDevice::Compute()     @     0x7ffff51f8f48  tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run()     @     0x7ffff51b8724  tensorflow::FunctionLibraryRuntimeImpl::RunSync()     @     0x7ffff51c5730  tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync()     @     0x7ffff51cbb7d  tensorflow::ProcessFunctionLibraryRuntime::RunSync()     @     0x7fffdcd3ddf0  tensorflow::KernelAndDeviceFunc::Run()     @     0x7fffdcce98d6  tensorflow::EagerKernelExecute()     @     0x7fffdccf33b0  tensorflow::ExecuteNode::Run()     @     0x7fffdcd39244  tensorflow::EagerExecutor::SyncExecute()     @     0x7fffdcce925b  tensorflow::(anonymous namespace)::EagerLocalExecute()     @     0x7fffdcce6929  tensorflow::DoEagerExecute()     @     0x7fffdccea660  tensorflow::EagerExecute()     @     0x7fffdc3209f7  tensorflow::EagerOperation::Execute()     @     0x7fffdcd37943  tensorflow::CustomDeviceOpHandler::Execute()     @     0x7fffd9b38cf5  TFE_Execute     @     0x7fffee231efa  TFE_Py_FastPathExecute_C()     @     0x7fffedad6893  pybind11::detail::argument_loader::call()     @     0x7fffedad67cf  pybind11::cpp_function::initialize()::{lambda() CC(Add support for Python 3.x)}::__invoke()     @     0x7fffedab08df  pybind11::cpp_function::dispatcher()     @           0x54d2d4  cfunction_call  Standalone code to reproduce the issue ```shell import tensorflow as tf from keras import layers import os os.environ[""TF_DISABLE_RZ_CHECK""] = ""1"" os.environ[""TF_GPU_ALLOCATOR""] = ""cuda_malloc_async"" tf.keras.backend.set_image_data_format('channels_first') gpus = tf.config.experimental.list_physical_devices('GPU') for gpu in gpus:     tf.config.experimental.set_memory_growth(gpu, True) tensor = tf.random.uniform([100, 100, 100]) model = layers.Conv1D(kernel_size=3, strides=98, filters=8044155, groups=1) model(tensor) ```  Relevant log output ```shell ```",2025-02-18T07:48:37Z,type:bug comp:gpu TF 2.8,open,0,3,https://github.com/tensorflow/tensorflow/issues/87457,"Hi **** , Apologies for the delay, and thanks for raising your concern here. I observed that you are using an older version TensorFlow 2.8. Could you please update to the latest version for better results? I ran your code on Colab using TensorFlow 2.18.0, and it threw the following error message: ` ResourceExhaustedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name:  ` This error occurs because the program is trying to allocate more GPU memory than is available. The issue seems to be caused by the large number of filters used in your model. To resolve this, I reduced the number of filters, and it worked fine for me. I am attaching a gist here for your reference. Thank you!","Oh, sorry, I mistyped the version number. I am using the latest tensorflow 2.18... The memory issue is fine on my machine, probably because I am using the H100 with 80GiB of VRAM.",The following is the environment I am using. ``` nvidiacublascu12        12.5.3.2                 pypi_0    pypi nvidiacudacupticu12    12.5.82                  pypi_0    pypi nvidiacudanvcccu12     12.5.82                  pypi_0    pypi nvidiacudanvrtccu12    12.5.82                  pypi_0    pypi nvidiacudaruntimecu12  12.5.82                  pypi_0    pypi nvidiacudnncu12         9.3.0.75                 pypi_0    pypi nvidiacufftcu12         11.2.3.61                pypi_0    pypi nvidiacurandcu12        10.3.6.82                pypi_0    pypi nvidiacusolvercu12      11.6.3.83                pypi_0    pypi nvidiacusparsecu12      12.5.1.3                 pypi_0    pypi nvidiancclcu12          2.21.5                   pypi_0    pypi nvidianvjitlinkcu12     12.5.82                  pypi_0    pypi openssl                   3.0.15               h5eee18b_0 opteinsum                3.4.0                    pypi_0    pypi optree                    0.14.0                   pypi_0    pypi packaging                 24.2                     pypi_0    pypi pexpect                   4.9.0                    pypi_0    pypi pip                       25.0            py312h06a4308_0 protobuf                  5.29.3                   pypi_0    pypi ptyprocess                0.7.0                    pypi_0    pypi pygments                  2.19.1                   pypi_0    pypi python                    3.12.9               h5148396_0 readline                  8.2                  h5eee18b_0 requests                  2.32.3                   pypi_0    pypi rich                      13.9.4                   pypi_0    pypi setuptools                75.8.0          py312h06a4308_0 six                       1.17.0                   pypi_0    pypi sqlite                    3.45.3               h5eee18b_0 tensorboard               2.18.0                   pypi_0    pypi tensorboarddataserver   0.7.2                    pypi_0    pypi tensorflow                2.18.0                   pypi_0    pypi t ```
copybara-service[bot],Add a field in PluginCompileOptions to specify the input program format.,Add a field in PluginCompileOptions to specify the input program format.,2025-02-18T07:02:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87456
jwnhy,[CUDA] cudaErrorInvalidConfiguration detected by compute-sanitizer," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.8.1  Custom code Yes  OS platform and distribution Ubuntu 24.04  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 90300  GPU model and memory H100 80G  Current behavior? computesanitizer reports Program hit cudaErrorInvalidConfiguration  Standalone code to reproduce the issue ```shell import tensorflow as tf from keras import layers import os os.environ[""TF_DISABLE_RZ_CHECK""] = ""1"" os.environ[""TF_GPU_ALLOCATOR""] = ""cuda_malloc_async"" tf.keras.backend.set_image_data_format('channels_first') gpus = tf.config.experimental.list_physical_devices('GPU') for gpu in gpus:     tf.config.experimental.set_memory_growth(gpu, True) tensor = tf.random.uniform([2, 388814, 2]) model = layers.Conv1D(filters=2, kernel_size=1, strides=2, groups=2) model(tensor) ```  Relevant log output ```shell ========= COMPUTESANITIZER ========= Program hit cudaErrorInvalidConfiguration (error 9) due to ""invalid configuration argument"" on CUDA API call to cudaLaunchKernelExC. =========     Saved host backtrace up to driver entry point at error =========     Host Frame: [0x4aa4a5] =========                in /lib/x86_64linuxgnu/libcuda.so.1 =========     Host Frame: [0x1d1b509] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0x15f94a1] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0x1461f45] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0x14627e9] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0xfefdb1] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0xff0704] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0x4a33ad] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0x4a3888] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame: [0x4c4a94] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9 =========     Host Frame:cudnn::backend::execute(cudnnContext*, cudnn::backend::ExecutionPlan const&, cudnn::backend::VariantPack&) [0x134c78] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9 =========     Host Frame:cudnnBackendExecute [0x134d8e] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9 =========     Host Frame: [0x8d5cf] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9 =========     Host Frame:cuda_graph_util::CudaGraphInfo::init(void*, cudnn::backend::OperationSetFinalizedMode_t) [0x868e5] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9 =========     Host Frame:cudnn::backend::cudnnBackendGetMetadataFromGraph(void*) [0x125fac] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9 =========     Host Frame: [0x153281] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_heuristic.so.9 =========     Host Frame: [0x1a4291] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_heuristic.so.9 =========     Host Frame: [0x1a593a] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_heuristic.so.9 =========     Host Frame: [0x151dea] ```",2025-02-18T05:21:21Z,type:bug comp:gpu TF 2.8,open,0,2,https://github.com/tensorflow/tensorflow/issues/87455,"Hi **** , Apologies for the delay, and thanks for raising your concern here. I noticed that you are using an older version, TensorFlow 2.8. Could you please update to the latest version for better performance and compatibility? I ran your code on Colab using TensorFlow 2.18.0 and the nightly version with GPU support, and I did not encounter any issues. Please find the gist here for your reference. Thank you!",Sorry I mistyped the version number... I found this issue on the latest tensorflow 2.18. I am using an H100 with 80GiB ram to find this issue. Can you try to reproduce this on a card with more ram?
jwnhy,[CUDA] illegal memory read in ShuffleInTensor3Simple," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuDNN 90300  GPU model and memory H100 80G  Current behavior? computesanitizer detects invalid memory read.  Standalone code to reproduce the issue ```shell import tensorflow as tf from keras import layers import os os.environ[""TF_DISABLE_RZ_CHECK""] = ""1"" os.environ[""TF_GPU_ALLOCATOR""] = ""cuda_malloc_async"" tf.keras.backend.set_image_data_format('channels_first') gpus = tf.config.experimental.list_physical_devices('GPU') for gpu in gpus:     tf.config.experimental.set_memory_growth(gpu, True) tensor = tf.random.uniform([1, 79768, 2]) model = layers.Conv1D(filters=65470, kernel_size=1, strides=32827, groups=1) model(tensor) ```  Relevant log output ```shell ========= COMPUTESANITIZER ========= Invalid __global__ read of size 4 bytes =========     at void tensorflow::functor::ShuffleInTensor3Simple(int, const T1 *, tensorflow::functor::Dimension, T1 *)+0x630 =========     by thread (64,0,0) in block (84,0,0) =========     Address 0x32f9d8d188 is out of bounds =========     and is 5,845,702,776 bytes before the nearest allocation at 0x3456472a00 of size 20,889,643,840 bytes =========     Saved host backtrace up to driver entry point at kernel launch time =========     Host Frame: [0x37f187] =========                in /lib/x86_64linuxgnu/libcuda.so.1 =========     Host Frame: [0x15a13] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cuda_runtime/lib/libcudart.so.12 =========     Host Frame:cudaLaunchKernel [0x75750] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cuda_runtime/lib/libcudart.so.12 =========     Host Frame:absl::lts_20230802::Status tensorflow::GpuLaunchKernel, float*, int, float const*, tensorflow::functor::Dimension, float*>(void (*)(int, float const*, tensorflow::functor::Dimension, float*), dim3, dim3, unsigned long, CUstream_st*, int, float const*, tensorflow::functor::Dimension, float*) [0x2b5018e4] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::functor::TransformFilter::operator()(Eigen::GpuDevice const&, tensorflow::FilterTensorFormat, Eigen::TensorMap, 16, Eigen::MakePointer>, Eigen::TensorMap, 16, Eigen::MakePointer>) [0x2b5015ff] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:void tensorflow::LaunchConvOpImpl(tensorflow::OpKernelContext*, bool, tensorflow::Tensor const&, tensorflow::Tensor const&, absl::lts_20230802::InlinedVector > const&, absl::lts_20230802::InlinedVector > const&, tensorflow::Padding const&, std::vector > const&, tensorflow::TensorFormat, tensorflow::Tensor*) [0x29a96f55] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::Conv2DOp::Compute(tensorflow::OpKernelContext*) [0x29c03560] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) [0x6d474d4] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) [0x6df8f47] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span, std::vector >*) [0x6db8723] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector, std::allocator > >*, std::function) const [0x6dc572f] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span, std::vector >*) const [0x6dcbb7c] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector, std::allocator > >*, tsl::CancellationManager*, std::optional const&, std::optional const&, tsl::CoordinationServiceAgent*) [0x24f3ddef] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector > const&, std::optional const&, tsl::core::RefCountPtr const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span, std::optional const&) [0x24ee98d5] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::ExecuteNode::Run() [0x24ef33af] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) [0x24f39243] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) [0x24ee925a] ```",2025-02-18T05:14:25Z,type:bug comp:gpu TF 2.8,open,0,3,https://github.com/tensorflow/tensorflow/issues/87454,"Hi **** , Apologies for the delay, and thanks for raising your concern here. I observed that you are using an older version TensorFlow 2.8 which might be causing the issue. Could you please try using the latest version for better results? I ran your code on Colab using TensorFlow 2.18.0 with GPU, and it produced the following proper error message: ` ResourceExhaustedError: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[1,79768,65470] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator gpu_async_0 [Op:StatelessRandomUniformV2] name:  ` This error indicates that the GPU is running out of memory (OOM) due to large tensor allocations. To resolve this, I reduced the memory requirements based on the model, and it worked fine for me. I am providing a gist here for your reference. Thank you!",Sorry I mistyped the version number... I found this issue on the latest tensorflow 2.18. I am using an H100 with 80GiB ram to find this issue. Can you try to reproduce this on a card with more ram?,The following is the environment I am using. ``` nvidiacublascu12        12.5.3.2                 pypi_0    pypi nvidiacudacupticu12    12.5.82                  pypi_0    pypi nvidiacudanvcccu12     12.5.82                  pypi_0    pypi nvidiacudanvrtccu12    12.5.82                  pypi_0    pypi nvidiacudaruntimecu12  12.5.82                  pypi_0    pypi nvidiacudnncu12         9.3.0.75                 pypi_0    pypi nvidiacufftcu12         11.2.3.61                pypi_0    pypi nvidiacurandcu12        10.3.6.82                pypi_0    pypi nvidiacusolvercu12      11.6.3.83                pypi_0    pypi nvidiacusparsecu12      12.5.1.3                 pypi_0    pypi nvidiancclcu12          2.21.5                   pypi_0    pypi nvidianvjitlinkcu12     12.5.82                  pypi_0    pypi openssl                   3.0.15               h5eee18b_0 opteinsum                3.4.0                    pypi_0    pypi optree                    0.14.0                   pypi_0    pypi packaging                 24.2                     pypi_0    pypi pexpect                   4.9.0                    pypi_0    pypi pip                       25.0            py312h06a4308_0 protobuf                  5.29.3                   pypi_0    pypi ptyprocess                0.7.0                    pypi_0    pypi pygments                  2.19.1                   pypi_0    pypi python                    3.12.9               h5148396_0 readline                  8.2                  h5eee18b_0 requests                  2.32.3                   pypi_0    pypi rich                      13.9.4                   pypi_0    pypi setuptools                75.8.0          py312h06a4308_0 six                       1.17.0                   pypi_0    pypi sqlite                    3.45.3               h5eee18b_0 tensorboard               2.18.0                   pypi_0    pypi tensorboarddataserver   0.7.2                    pypi_0    pypi tensorflow                2.18.0                   pypi_0    pypi ```
johnnkp,Add CUDA plugin registration checking,"This pull request is to fix https://github.com/openxla/xla/issues/20803. Test result is as follows. No error. ``` 20250205 12:19:54.390955: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1738729197.544741    4373 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3315 MB memory:  > device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:29:00.0, compute capability: 6.1 Model: ""functional_4"" ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ ┃ Layer (type)                    ┃ Output Shape           ┃       Param  ┃ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ │ input_layer (InputLayer)        │ (None, None, 128)      │             0 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ encoder (Encoder)               │ (None, None, 128)      │       662,528 │ ├─────────────────────────────────┼────────────────────────┼───────────────┤ │ out_pretraining (Dense)         │ (None, None, 128)      │        16,512 │ └─────────────────────────────────┴────────────────────────┴───────────────┘  Total params: 679,040 (2.59 MB)  Trainable params: 679,040 (2.59 MB)  Nontrainable params: 0 (0.00 B) ```",2025-02-18T05:13:58Z,comp:xla size:M,closed,0,3,https://github.com/tensorflow/tensorflow/issues/87453," I already created as https://github.com/openxla/xla/pull/22391, but they didn't review.",">  I already created as openxla/xla CC(Fixed broken links), but they didn't review. Sorry about your experience. The problem is that it was assigned to someone who is currently out of office. We need to find a different reviewer.",Closed as https://github.com/tensorflow/tensorflow/pull/88512 merged.
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T04:37:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87452
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T04:33:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87451
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T04:32:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87450
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T04:32:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87449
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T04:31:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87448
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T04:29:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87447
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T04:28:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87446
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T04:27:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87445
copybara-service[bot],Fix length of separators,Fix length of separators,2025-02-18T04:25:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87444
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T04:25:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87443
copybara-service[bot],Fix length of separators,Fix length of separators,2025-02-18T04:24:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87442
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T04:23:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87441
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T04:23:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87440
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T04:23:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87439
jwnhy,[CUDA] illegal memory write in ShuffleInTensor3Simple," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.8.1  Custom code Yes  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version cuDNN: 90300  GPU model and memory H100 80G  Current behavior? computesanitizer reports Illegal Memory Access. Expect: no IMA should occur.  Standalone code to reproduce the issue ```shell import tensorflow as tf from keras import layers import os os.environ[""TF_DISABLE_RZ_CHECK""] = ""1"" tf.keras.backend.set_image_data_format('channels_first') gpus = tf.config.experimental.list_physical_devices('GPU') for gpu in gpus:     tf.config.experimental.set_memory_growth(gpu, True) tensor = tf.random.uniform([2, 1, 392366]) model = layers.Conv1D(filters=2147483647, kernel_size=1, strides=392366, groups=1) model(tensor) ```  Relevant log output ```shell ========= COMPUTESANITIZER ========= Invalid __global__ write of size 4 bytes =========     at void tensorflow::functor::ShuffleInTensor3Simple(int, const T1 *, tensorflow::functor::Dimension, T1 *)+0x660 =========     by thread (128,0,0) in block (8,0,0) =========     Address 0x2e9c2ff600 is out of bounds =========     and is 517 bytes after the nearest allocation at 0x2c9c2ff400 of size 8,589,934,588 bytes =========     Saved host backtrace up to driver entry point at kernel launch time =========     Host Frame: [0x37f187] =========                in /lib/x86_64linuxgnu/libcuda.so.1 =========     Host Frame: [0x15a13] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cuda_runtime/lib/libcudart.so.12 =========     Host Frame:cudaLaunchKernel [0x75750] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../../nvidia/cuda_runtime/lib/libcudart.so.12 =========     Host Frame:absl::lts_20230802::Status tensorflow::GpuLaunchKernel, float*, int, float const*, tensorflow::functor::Dimension, float*>(void (*)(int, float const*, tensorflow::functor::Dimension, float*), dim3, dim3, unsigned long, CUstream_st*, int, float const*, tensorflow::functor::Dimension, float*) [0x2b5018e4] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::functor::TransformFilter::operator()(Eigen::GpuDevice const&, tensorflow::FilterTensorFormat, Eigen::TensorMap, 16, Eigen::MakePointer>, Eigen::TensorMap, 16, Eigen::MakePointer>) [0x2b5015ff] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:void tensorflow::LaunchConvOpImpl(tensorflow::OpKernelContext*, bool, tensorflow::Tensor const&, tensorflow::Tensor const&, absl::lts_20230802::InlinedVector > const&, absl::lts_20230802::InlinedVector > const&, tensorflow::Padding const&, std::vector > const&, tensorflow::TensorFormat, tensorflow::Tensor*) [0x29a96f55] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::Conv2DOp::Compute(tensorflow::OpKernelContext*) [0x29c03560] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) [0x6d474d4] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) [0x6df8f47] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span, std::vector >*) [0x6db8723] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector, std::allocator > >*, std::function) const [0x6dc572f] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span, std::vector >*) const [0x6dcbb7c] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_framework.so.2 =========     Host Frame:tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector, std::allocator > >*, tsl::CancellationManager*, std::optional const&, std::optional const&, tsl::CoordinationServiceAgent*) [0x24f3ddef] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector > const&, std::optional const&, tsl::core::RefCountPtr const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span, std::optional const&) [0x24ee98d5] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::ExecuteNode::Run() [0x24ef33af] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) [0x24f39243] =========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 =========     Host Frame:tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) [0x24ee925a] ```",2025-02-18T04:22:19Z,type:bug comp:gpu TF 2.8,open,0,2,https://github.com/tensorflow/tensorflow/issues/87438,"Hi **** , Apologies for the delay, and thanks for your patience. I ran your code on Colab using TensorFlow 2.18.0, and it threw the following error: ``` ResourceExhaustedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name:  ``` This error occurs because the program is trying to allocate more GPU memory than is available. The issue seems to be caused by the large number of filters used in your model. To resolve this, I reduced the number of filters, and it worked fine for me. I am attaching a gist here for your reference. Thank you!",Sorry I mistyped the version number... I found this issue on the latest tensorflow 2.18. I am using an H100 with 80GiB ram to find this issue. Can you try to reproduce this on a card with more ram?
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T04:20:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87437
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-18T04:15:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87436
copybara-service[bot],Create GitHub action to run a small GPU HLO benchmark and save XSpace data.,Create GitHub action to run a small GPU HLO benchmark and save XSpace data.,2025-02-18T01:23:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87435
copybara-service[bot],[MultiHostHloRunner] Dump multihost hlo runner XSpace in text format for better readability.,[MultiHostHloRunner] Dump multihost hlo runner XSpace in text format for better readability.,2025-02-18T01:06:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87434
copybara-service[bot],Reverts 35fb4a99534dc85e47eeedf760ab691ddda31219,Reverts 35fb4a99534dc85e47eeedf760ab691ddda31219,2025-02-17T21:41:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87433
datasciantle,NaN loss on multi-gpu training," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version v1.12.1122039g784fed5357b 2.20.0dev20250211  Custom code Yes  OS platform and distribution Amazon Linux 2  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 12.3 (Build V12.3.107)  GPU model and memory GPU Model: NVIDIA A10G Memory per GPU: 24 GB  Current behavior? I find that training a distributed model  with either MirroredStrategy or MultiWorkerMirroredStrategy  that the loss jumps to NaN on the 61st batch. The problem vanishes if I train on a single GPU (no distribute strategy), or if I use tf 2.13.1. The problem also vanishes if I set the number of steps per epoch to be less than 61. I've attached some reproducible code but note that I find the exact same finding (the role of batch 61, single GPU, v.2.13) on other datasets, other model architectures, and with different batch sizes.  Standalone code to reproduce the issue ```shell call this test.py  these pass python3 test.py steps_per_epoch 60 strategy None python3 test.py steps_per_epoch 60 strategy MirroredStrategy python3 test.py steps_per_epoch 60 strategy MultiWorkerMirroredStrategy python3 test.py steps_per_epoch 61 strategy None  these fail python3 test.py steps_per_epoch 61 strategy MirroredStrategy python3 test.py steps_per_epoch 61 strategy MultiWorkerMirroredStrategy import tensorflow as tf import numpy as np import argparse parser = argparse.ArgumentParser() parser.add_argument('batch_size', type=int, default=64) parser.add_argument('steps_per_epoch', type=int, default=128) parser.add_argument('strategy', type=str, default='None') args = parser.parse_args()  For reproducibility tf.random.set_seed(42) np.random.seed(42)  Parameters n_string = 256       used for the CLS token value n_features = 256    length of the output label vector dim_model = 64 batch_size = int(args.batch_size) steps_per_epoch = int(args.steps_per_epoch) def random_input(n_features, n_string):     """"""     Generate random input features in the integer domain.     """"""     return tf.random.uniform(shape=(n_features,), minval=0, maxval=n_string, dtype=tf.int32) def random_mask(input_features, batch_masking_rate):     """"""     Randomly mask input features in the integer domain.     Each element is retained with probability (1  batch_masking_rate) and zeroed otherwise.     """"""      Generate random values for each element in the same shape as input_features.     rand_vals = tf.random.uniform(shape=tf.shape(input_features), minval=0.0, maxval=1.0)      Create a binary mask: retain element if random value >= masking_rate, else 0.      This yields 1 with probability (1  batch_masking_rate) and 0 with probability batch_masking_rate.     mask = tf.cast(rand_vals >= batch_masking_rate, dtype=tf.int32)     return input_features * mask def process_example(output_label):     """"""     Given an output_label vector, generate a random masking rate,     apply random masking to produce input_features, and prepend a CLS token.     All operations remain in the integer domain.     """"""      Draw a random masking rate uniformly between 0 and 0.5.      (Note: masking_rate is used only for the masking decision and is not passed on.)     masking_rate = tf.random.uniform([], minval=0.0, maxval=0.5)      output_label is already an integer tensor.     masked_input = random_mask(output_label, masking_rate)      Return a features dictionary and a labels dictionary.     features = {""input_features"": masked_input}     labels = {""the_label"": output_label}     return features, labels def make_dataset(num_examples, n_features, n_string, batch_size):     """"""     Create a tf.data.Dataset of random output labels.     Each output label is a random integer tensor of shape (n_features,).     """"""      Create a dataset of num_examples random output labels.     dummy_examples = [random_input(n_features, n_string) for _ in range(num_examples)]      Build a tf.data.Dataset from the dummy output labels.     dataset = tf.data.Dataset.from_tensor_slices(dummy_examples)     dataset = dataset.map(process_example)     dataset = dataset.batch(batch_size)     return dataset.repeat() def make_the_model(n_features, n_string, dim_model):     """"""     Build a simple model that takes input_features and predicts the_label.     """"""     input_features = tf.keras.layers.Input(shape=(n_features,), dtype=tf.int32, name=""input_features"")     embedding = tf.keras.layers.Embedding(n_string, dim_model)(input_features)     mha = tf.keras.layers.MultiHeadAttention(         num_heads=8,         key_dim=dim_model//8,         dropout=0.,         name='mha_0'     )(embedding, embedding)     mha = tf.keras.layers.MultiHeadAttention(         num_heads=8,         key_dim=dim_model//8,         dropout=0.,         name='mha_1'     )(mha, mha)      Flatten the embedding and apply a dense layer to predict the_label.     the_label = tf.keras.layers.Dense(n_features, activation=None, name=""the_label"")(mha)     model = tf.keras.Model(inputs={'input_features': input_features}, outputs={'the_label': the_label})     optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)     loss_dict = {'the_label': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)}     model.compile(optimizer=optimizer, loss=loss_dict)     return model training_data = make_dataset(1024, n_features, n_string, batch_size) validation_data = make_dataset(256, n_features, n_string, batch_size)  print an example of the training data  for batch in training_data.take(1):      print(batch)  Build the model if args.strategy == 'MirroredStrategy':     print(""Using MirroredStrategy."")     strategy = tf.distribute.MirroredStrategy()     with strategy.scope():         this_model = make_the_model(n_features, n_string, dim_model)         this_model.summary()         print(f'Training with batch size {batch_size} and steps per epoch {steps_per_epoch} with {strategy.num_replicas_in_sync} replicas.') if args.strategy == 'MultiWorkerMirroredStrategy':     print(""Using MultiWorkerMirroredStrategy."")     strategy = tf.distribute.MultiWorkerMirroredStrategy()     with strategy.scope():         this_model = make_the_model(n_features, n_string, dim_model)         this_model.summary()         print(f'Training with batch size {batch_size} and steps per epoch {steps_per_epoch} with {strategy.num_replicas_in_sync} replicas.') if args.strategy == 'None':     print(""No strategy."")     this_model = make_the_model(n_features, n_string, dim_model)     this_model.summary()     print(f'Training with batch size {batch_size} and steps per epoch {steps_per_epoch} with 1 GPU.')  Train the model call_backs = tf.keras.callbacks.TerminateOnNaN() this_model.fit(training_data, validation_data=validation_data, epochs=100, steps_per_epoch=steps_per_epoch, validation_steps=4, callbacks=[call_backs])```  Relevant log output ```shell ```",2025-02-17T21:04:11Z,stat:awaiting response type:bug stale comp:gpu TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/87432,"Hi **** , Apologies for the delay, and thanks for raising your concern here. I tried running your code using TensorFlow 2.18.0 (stable) on VM instances, but I did not encounter any issues. I am attaching the output below for your reference. And I recommend using stable versions for better results. ``` [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')] ``` ``` (tf_env) maayaragpu1:~$ python3 test.py steps_per_epoch 61 strategy MultiWorkerMirroredStrategy 20250305 11:01:28.725763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1741172488.746097   33345 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1741172488.752394   33345 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250305 11:01:28.774422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. I0000 00:00:1741172491.290716   33345 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  > device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5 I0000 00:00:1741172491.293431   33345 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13764 MB memory:  > device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5 Using MultiWorkerMirroredStrategy. Model: ""functional"" ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ Layer (type)                  ┃ Output Shape              ┃         Param  ┃ Connected to               ┃ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩ │ input_features (InputLayer)   │ (None, 256)               │               0 │                           │ ├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤ │ embedding (Embedding)         │ (None, 256, 64)           │          16,384 │ input_features[0][0]       │ ├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤ │ mha_0 (MultiHeadAttention)    │ (None, 256, 64)           │          16,640 │ embedding[0][0],           │ │                               │                           │                 │ embedding[0][0]            │ ├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤ │ mha_1 (MultiHeadAttention)    │ (None, 256, 64)           │          16,640 │ mha_0[0][0], mha_0[0][0]   │ ├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤ │ the_label (Dense)             │ (None, 256, 256)          │          16,640 │ mha_1[0][0]                │ └───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘  Total params: 66,304 (259.00 KB)  Trainable params: 66,304 (259.00 KB)  Nontrainable params: 0 (0.00 B) Training with batch size 64 and steps per epoch 61 with 2 replicas. Epoch 1/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 10s 48ms/step  loss: 5.5452  val_loss: 5.5454 Epoch 2/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step  loss: 5.5448  val_loss: 5.5454 . . . . Epoch 97/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step  loss: 1.3885  val_loss: 1.3389 Epoch 98/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step  loss: 1.4202  val_loss: 1.3394 Epoch 99/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 3s 44ms/step  loss: 1.3504  val_loss: 1.3385 Epoch 100/100 61/61 ━━━━━━━━━━━━━━━━━━━━ 3s 44ms/step  loss: 1.4391  val_loss: 1.3389 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Integrate LLVM at llvm/llvm-project@34cf04b59b8d,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 34cf04b59b8d,2025-02-17T20:24:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87431
copybara-service[bot],Integrate LLVM at llvm/llvm-project@50581ef1ee45,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 50581ef1ee45,2025-02-17T19:43:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87430
MayureshMore,[Documentation] Appended fallback comment in tools.,Changes made in branch: **MayureshMore:master** [Documentation] Appended fallback comment in tools.,2025-02-17T18:05:37Z,size:XS invalid,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87429,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],PR #22782: [NFC] Move HLO-handling functions from runner to module util.,PR CC(Rel notes 1 12): [NFC] Move HLOhandling functions from runner to module util. Imported from GitHub PR https://github.com/openxla/xla/pull/22782 Copybara import of the project:  6a1fd278aedbfade2accaf18888ca0033ae51fb2 by Ilia Sergachev : [NFC] Move HLOhandling functions from runner to module util. Merging this change closes CC(Rel notes 1 12) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22782 from openxla:refactor_hlo_fn 6a1fd278aedbfade2accaf18888ca0033ae51fb2,2025-02-17T17:50:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87428
copybara-service[bot],Debug Apple Silicon CI disk space issue.,Debug Apple Silicon CI disk space issue.,2025-02-17T17:45:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87427
copybara-service[bot],Fix typos in `lite/core:model_building` (2nd edition),Fix typos in `lite/core:model_building` (2nd edition),2025-02-17T17:35:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87426
copybara-service[bot],Attaching custom options with TAC filters:,"Attaching custom options with TAC filters:  Add google.any.proto to op_filter message which can encapsulate any opaque proto that is parsed & managed using a callback.  Update filter so that it can also ""invert"" match & specify CPU v/s non CPU  Pass callback to filtering pass that annotates matched op with fingerprint of unpacked custom options.  Disable inlining during TFL export, but enable after custom IP export.  Merge compiler options with default options (can be changed).",2025-02-17T16:55:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87425
copybara-service[bot],PR #22782: [NFC] Move HLO-handling functions from runner to module util.,PR CC(Rel notes 1 12): [NFC] Move HLOhandling functions from runner to module util. Imported from GitHub PR https://github.com/openxla/xla/pull/22782 Copybara import of the project:  6a1fd278aedbfade2accaf18888ca0033ae51fb2 by Ilia Sergachev : [NFC] Move HLOhandling functions from runner to module util. Merging this change closes CC(Rel notes 1 12) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22782 from openxla:refactor_hlo_fn 6a1fd278aedbfade2accaf18888ca0033ae51fb2,2025-02-17T16:45:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87424
copybara-service[bot],Optionally support setting options on ShardyXLA pass at constrution time.,Optionally support setting options on ShardyXLA pass at constrution time.,2025-02-17T16:16:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87423
copybara-service[bot],PR #22385: [XLA:GPU] Fix test ConditionalOpTest.SwappedInputsInSequentialConditionals failed when enabling command buffer.,"PR CC(session = tf.Session()): [XLA:GPU] Fix test ConditionalOpTest.SwappedInputsInSequentialConditionals failed when enabling command buffer. Imported from GitHub PR https://github.com/openxla/xla/pull/22385 This PR fix test ConditionalOpTest.SwappedInputsInSequentialConditionals failure when enabling command buffer. The issue is that command buffer lowering process misinterprets the bool predict to int32 predict, so the predict value is corrupted.   the original errors are:  ``` xla/tests/client_library_test_base.cc:466: Failure Value of: LiteralTestUtil::Near(expected, actual, error)   Actual: false ( Mismatches in shape (f32[], f32[]) (2 elements): Array at shape index {0}, Mismatch count 1 (100.0000%) in shape f32[] (1 elements), abs bound 0.001, rel bound 0 Top relative error mismatches:   actual             5.55000019, expected             11.2399998, index {}, rel error    0.506, abs error     5.69 : Array at shape index {1}, Mismatch count 1 (100.0000%) in shape f32[] (1 elements), abs bound 0.001, rel bound 0 Top relative error mismatches:   actual             11.2399998, expected             5.55000019, index {}, rel error     1.03, abs error     5.69 Expected literal: ( f32[] 11.24, f32[] 5.55 ) Actual literal: ( f32[] 5.55, f32[] 11.24 )) ``` Copybara import of the project:  272959fb11a5306414ef924536130e4b3456fa37 by Shawn Wang : fix case command buffer unittest failure  a254286ec86013186380bb0502769bdc01580130 by Shawn Wang : restore default falgs  df91f58d5546edfd625ad3d4825c9bbf5e72971e by Shawn Wang : fix test failure  0f12478c89a85f2d0bbca6fceae4eb7779f84868 by Shawn Wang : fix  33befa603257c76897a250130582bde67049cae5 by Shawn Wang : update ptx code Merging this change closes CC(session = tf.Session()) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22385 from shawnwang18:shawnw/enable_cuda_graph_by_default 33befa603257c76897a250130582bde67049cae5",2025-02-17T15:59:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87422
cybersupersoap,"`tf.raw_ops.QuantizeAndDequantizeV3` aborts with ""Check failed: d >= 0 (0 vs. -4)"""," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tfnightly 2.19.0  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `aborted issue` in TensorFlow when I used API `tf.raw_ops.QuantizeAndDequantizeV3` . I have confirmed that below code would crash on `tfnightly 2.19.0dev20250207` (nightlybuild). Please find the gist to reproduce the issue.  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np input_data = tf.random.uniform(shape=[1, 2, 3, 2], dtype=tf.dtypes.float32) input_min = tf.constant([0.0], dtype=tf.dtypes.float32) input_max = tf.constant([1.0], dtype=tf.dtypes.float32) output = tf.raw_ops.QuantizeAndDequantizeV3(input=input_data, input_min=input_min, input_max=input_max, num_bits=8, signed_input=True, range_given=True, narrow_range=False, axis=4, name=None) ```  Relevant log output ```shell 20250217 12:20:26.671705: F tensorflow/core/framework/tensor_shape.cc:358] Check failed: d >= 0 (0 vs. 4) Aborted (core dumped) ```",2025-02-17T15:33:19Z,stat:awaiting response type:bug stale comp:ops TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/87421,", I request you to take a look at this issue where a similar issue was raised and it is still open. Also I request to follow the similar issues which has been proposed to have the updates on the similar issue. Thank you!","> [](https://github.com/cybersupersoap), I request you to take a look at this issue where a similar issue was raised and it is still open. Also I request to follow the similar issues which has been proposed to have the updates on the similar issue. Thank you! Thank you for your reply!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
cybersupersoap,"`tf.raw_ops.LookupTableExportV2` aborts with ""Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64"""," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tfnightly 2.19.0  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04 LTS  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? encountered an `aborted issue` in TensorFlow when I used API `tf.raw_ops.LookupTableExportV2` . I have confirmed that below code would crash on `tfnightly 2.19.0dev20250207` (nightlybuild). Please find the gist to reproduce the issue.  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np keys = tf.constant([0, 1, 2], dtype=tf.int64) values = tf.constant([1, 2, 3], dtype=tf.float32)   table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(keys, values), 1) export_keys, export_values = tf.raw_ops.LookupTableExportV2(table_handle=table.resource_handle, Tkeys=tf.int64, Tvalues=tf.int64) ```  Relevant log output ```shell 20250217 12:21:39.561442: F tensorflow/core/framework/tensor.cc:857] Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64 Aborted (core dumped) ```",2025-02-17T15:19:44Z,type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/87420,I tried running your code on Colab using TensorFlow v2.18.0 and the nightly version. I faced the same issue. Please find gist here for reference. Thank you!
cybersupersoap,"`tf.experimental.numpy.swapaxes` aborts with ""Check failed: d >= 0 (0 vs. -2)"""," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tfnightly 2.19.0  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04 LTS  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `aborted issue` in TensorFlow when I used API `tf.experimental.numpy.swapaxes` . I have confirmed that below code would crash on `tfnightly 2.19.0dev20250207` (nightlybuild) Please find the gist to reproduce the issue.  Standalone code to reproduce the issue ```shell import tensorflow import tensorflow as tf import numpy import numpy as np input_data = np.arange(24).reshape(2, 3, 4) output_data = tf.experimental.numpy.swapaxes(input_data, 5, 2) ```  Relevant log output ```shell 20250217 12:30:19.854628: F tensorflow/core/framework/tensor_shape.cc:358] Check failed: d >= 0 (0 vs. 2) Aborted (core dumped) ```",2025-02-17T15:12:11Z,type:bug comp:apis TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/87419,", tf.experimental.numpy.swapaxes is in an experimental stage which is equivalent to numpy.swapaxes. The numpy.swapaxes args are axis1 and 2 which are int. I tried with the int as mentioned in the official document and the output was as expected. Kindly find the gist of it here. https://www.tensorflow.org/api_docs/python/tf/experimental/numpy/swapaxes https://numpy.org/doc/stable/reference/generated/numpy.swapaxes.html Thank you!", Thank you for looking at this. I think this issue can be treated as a bug because the crash could potentially lead to a DOS attack.
copybara-service[bot],Extract SizeAndStrideExpression into its own target (NFC).,Extract SizeAndStrideExpression into its own target (NFC). We want to reuse some of the logic to compute size and stride from affine expressions. This move is a preparation for that.,2025-02-17T13:30:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87418
copybara-service[bot],Remove minimum tile size restriction in Triton autotuner and emitter,"Remove minimum tile size restriction in Triton autotuner and emitter Upstream Triton recently refactored their lowering logic, and now seems to support small tiles as well.",2025-02-17T12:46:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87417
copybara-service[bot],Extract ConstraintExpression class into its own target (NFC).,"Extract ConstraintExpression class into its own target (NFC). It is usually a good idea to put separate classes into their own targets. This makes merge conflicts less likely and can also mean that fewer tests will be triggered (e.g. only the SymbolicTile tests when changing something related to SymbolicTile, but not changing ConstraintExpression).",2025-02-17T12:34:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87416
copybara-service[bot],[XLA:GPU] Update printers/parsers for the tile/extract/insert ops.,[XLA:GPU] Update printers/parsers for the tile/extract/insert ops.,2025-02-17T12:08:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87415
uri-granta,Memory leak when compiling tfp.util.TransformedVariable since TF 2.14 (worked fine before!)," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.14+ (present in 2.14, 2.16, 2.18; not an issue in 2.11, 2.12, 2.13)  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Upgrading a GPflowbased workflow to use TF 2.14 is showing memory leaks where none previously occurred. Preliminary investigations connected this to the compilation of GPflow models: in the code snippet below the memory usage increases over time (by around 1MB per iteration for a total of ~200MB) when run with TF 2.14, 2.16 or 2.18, but not when run with TF 2.11, 2.12 or 2.13. Raising as a TF bug as code that was previously executing fine is now appearing to leak memory, though there is also a chance that this is an issue in GPflow. Any suggestions for helping identify the leaked memory would be very welcome!  Standalone code to reproduce the issue ```shell import gc import gpflow import keras.backend import numpy as np import os import psutil import tensorflow as tf  define simple GPflow model X = np.array([[0.865], [0.666], [0.804], [0.771], [0.147], [0.866], [0.007], [0.026], [0.171], [0.889], [0.243], [0.028]]) Y = np.array([[1.57], [3.48], [3.12], [3.91], [3.07], [1.35], [3.80], [3.82], [3.49], [1.30], [4.00], [3.82]]) model = gpflow.models.GPR((X, Y), kernel=gpflow.kernels.SquaredExponential())  repeatedly compile and evaluate the model's log marginal likelihood for i in range(200):      garbage collect to remove some of the noise     keras.backend.clear_session(); gc.collect()      compile and evaluate closure     tf.function(model.log_marginal_likelihood)()      track memory usage     print(f""[{i+1}] Memory usage: {psutil.Process(os.getpid()).memory_info().rss / 1024 **2} MiB"") ```  Relevant log output ```shell ```",2025-02-17T11:48:58Z,type:bug type:performance TF 2.18,open,0,5,https://github.com/tensorflow/tensorflow/issues/87414,"Hi **granta** , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab using TensorFlow 2.13.0 it is working as expected and 2.18.0 faced the same issue like you. The main cause might be version compatibility. When using GPflow, you need to install the GPflow package along with its dependencies TensorFlow and TensorFlow Probability ensuring all versions are compatible. According to the GPflow documentation, compatibility is mentioned only up to TensorFlow 2.12. Therefore, the exact TensorFlow Probability version required for the latest TensorFlow versions is unknown. I recommend first confirming the compatibility versions in the GPflow documentation. It would be best to check with GPflow for the correct compatibility versions for the latest TensorFlow releases. Thank you!","Hi , Thanks for looking into this! **I've since managed to reproduce the issue without using `gpflow` at all, just with a `TransformedVariable` from `tensorflowprobability`.** (Also FYI `gpflow` does actually support tensorflow up to 2.16, though it looks like the docs aren't up to date!) As before, the following script shows continually increasing memory usage when run with TF 2.1418 but not when run with TF 2.1113, though the memory increase is smaller than before (only around 0.1MB per iteration) and takes a few iterations to get going. ```python import gc import os os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' import psutil import keras.backend import tensorflow as tf import tensorflow_probability as tfp class Kernel:     def __init__(self):          note that using tfp.bijectors.Identity() instead doesn't leak (though tfp.bijectors.Exp() does)         self._variance = tfp.util.TransformedVariable(tf.constant(1.), tfp.bijectors.Softplus())     def variance(self):          note that just returning self.variance doesn't leak         return tf.squeeze(self._variance) kernel = Kernel() for i in range(400):      garbage collect to remove noise     keras.backend.clear_session(); gc.collect()      compile and evaluate closure     tf.function(lambda: kernel.variance())()      track memory usage     print(f""[{i+1}] Memory usage: {psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2} MiB"") ```","FYI here's an even simpler class that can be used in the previous example: ```python class Kernel:     def __init__(self):         self._variance = tfp.util.DeferredTensor(tf.Variable(1.), tfp.bijectors.Shift(0.0))     def variance(self):         return tf.convert_to_tensor(self._variance) ``` As before, setting the bijector to `tfp.bijectors.Identity()` removes the leak. However, changing this bijector's `_forward(self, x)` method to return `x + 0` rather than `x` reintroduces it. AFAICT the bijector is never evaluated in the test, so this is presumably all to do with compilation.","So it looks like the issue in this particular example is that in every compilation, a pair of weak references to the input and output `SymbolicTensor`'s aren't being garbage collected, which in turn means that their cleanup callbacks aren't being called to remove them from the global `BijectorCache` (`tfp.bijectors.bijector._cache`). By contrast, both in TF=2.14 both references *are* garbage collected and the callbacks *are* called. Note that the `BijectorCache` code in tfp hasn't changed in the last 4 years, and the `gpflow` example from the beginning continues to show issues even after manually disabling the cache, so I suspect the cache is just a symptom and the underlying cause is somehow connected to why the compiled graphs aren't being garbage collected (or at least why their cleanup callbacks aren't being called).","So I'm pretty sure the `BijectoCache` issue is due to https://github.com/tensorflow/tensorflow/commit/333ee99ecb9bd8f122c683defa74281bb3bd1664. Before this change, when initialising a `Function`, TF used to call `TracingCompiler._get_concrete_function_internal_garbage_collected`, which ended up creating and deleting a `ConcreteFunctionGarbageCollector`, dismantling the function graph at the end. After this change, when initialising a `Function`, TF now calls `tracing_compilation.trace_function`, which similarly creates a `ConcreteFunctionGarbageCollector`. However, because `tracing_options.bind_graph_to_function` is `False` (the default value, unchanged by `Function._generate_tracing_options`) it also calls `release` on the garbage collector before deleting it, preventing it from dismantling the function graph (and therefore from cleaning up the bijector cache). Note though that this probably isn't the whole story. Changing `Function._initialize` to use `dataclasses.replace(self._variable_creation_config, bind_graph_to_function=True)` instead does fix the memory leak in the two bijector examples above. However, it doesn't fix the original leaks discovered in `gpflow` (which based on profiling may now be connected to the `gradient_registry` somehow)."
copybara-service[bot],Integrate LLVM at llvm/llvm-project@912b154f3a3f,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 912b154f3a3f,2025-02-17T10:35:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87413
copybara-service[bot],[XLA:GPU] Move lit tests for triton_xla dialect into [ir|transforms]/tests.,[XLA:GPU] Move lit tests for triton_xla dialect into [ir|transforms]/tests.,2025-02-17T10:32:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87412
copybara-service[bot],PR #21746: [NVIDIA GPU] Add collective-permute combiner,"PR CC(Adding op doc fixes): [NVIDIA GPU] Add collectivepermute combiner Imported from GitHub PR https://github.com/openxla/xla/pull/21746 For collectivepermutes with small message sizes, it is beneficial to combine them into a single collective because 1. it gets rid of some kernel launch overhead, and allows NCCL to do some message fusion; 2. fewer collectives make it easier for LHS to make better decision. On top of the multioperand collectivepermute added in https://github.com/openxla/xla/pull/18838, this PR adds a combiner for collectivepermutes. Copybara import of the project:  c03a8fb5bd42cf3a365e1684537e78544a75a937 by Terry Sun : add collective permute combiner  6a3159e89444ea342c25d8d996c994accd68a30d by Terry Sun : polishing and doc string updates  241486538b9fd7c6d9ac3a3fdcf0149fc8b4bdfb by Terry Sun : handle unused variable Coauthoredby: Penporn Koanantakool  Merging this change closes CC(Adding op doc fixes) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21746 from terryysun:terryysun/combine_collective_permute 4cd1404185f31953c4158b25b131315a2e53f3ab",2025-02-17T10:03:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87411
copybara-service[bot],[pjrt] Removed `PjRtStreamExecutorLoadedExecutable::IsReturnedFutureSupported`,[pjrt] Removed `PjRtStreamExecutorLoadedExecutable::IsReturnedFutureSupported` It is true in all clients.,2025-02-17T09:18:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87410
jiunkaiy,Qualcomm AI Engine Direct - Fix copyright format,Summary:  Fix QC copyright format,2025-02-17T09:17:59Z,awaiting review ready to pull size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87409
copybara-service[bot],[pjrt] Removed unused `PjRtBuffer::CopyToRemoteDeviceScattered` and `ScatterDetails`,[pjrt] Removed unused `PjRtBuffer::CopyToRemoteDeviceScattered` and `ScatterDetails`,2025-02-17T08:50:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87408
copybara-service[bot],PR #22706: Increase the compilation thread stack size to 4MB,"PR CC(CUDA 10): Increase the compilation thread stack size to 4MB Imported from GitHub PR https://github.com/openxla/xla/pull/22706 When JIT compiling modules using a process runner, the default stack size on OS X is too low. Fx this StableHLO module will fail to compile with a stack overflow error using this command: ``` bazel run spawn_strategy=sandboxed //xla/tools:run_hlo_module  platform=CPU input_format=stablehlo stack_overflow.mlir ``` This PR will increase the thread stack size for the compilation thread pool to 4MB per thread. Copybara import of the project:  31d88dbd8cfbb99986202bd450c71fd21b5a4223 by Kasper Nielsen : Increase the compilation thread stack size to 4MB Merging this change closes CC(CUDA 10) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22706 from kasper0406:kn/increasecompilationthreadstack 31d88dbd8cfbb99986202bd450c71fd21b5a4223",2025-02-17T08:46:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87407
copybara-service[bot],[XLA:GPU] Move lit tests for triton_xla dialect into [ir|transforms]/tests.,[XLA:GPU] Move lit tests for triton_xla dialect into [ir|transforms]/tests.,2025-02-17T08:35:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87406
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T05:00:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87404
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T04:30:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87403
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T04:30:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87402
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T04:27:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87401
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T04:26:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87400
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T04:18:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87399
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T04:18:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87398
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T04:16:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87397
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T04:16:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87396
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T04:15:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87395
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T04:10:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87394
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T04:01:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87393
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T03:49:47Z,ready to pull,open,0,0,https://github.com/tensorflow/tensorflow/issues/87392
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T02:52:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87391
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T02:49:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87390
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-17T02:48:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87389
huuthien11,Khảo chăn,> 007F987436229   _Originally posted by  in 1432afe_,2025-02-16T22:25:18Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87388,Please don't spam
copybara-service[bot],[pjrt] Removed unused `PjRtClient::MakeCrossHostReceiveBufferForGather` and `GatherDetails`,[pjrt] Removed unused `PjRtClient::MakeCrossHostReceiveBufferForGather` and `GatherDetails`,2025-02-16T20:33:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87387
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T18:16:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87386
AD-lite24,Warning in XNNPACK: unable to enable JIT: not compiled with JIT enabled," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.1  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version GCC 9 aarch cross compiler  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Upon updating our tflite service to 2.17.1, XNNPACK fails to apply with this cryptic error  `Warning in XNNPACK: unable to enable JIT: not compiled with JIT enabled` I don't understand this error at all, there is really only way to build XNNPACK while building TFLite as described in CMake, so I have no idea what this is referring to. We didn't face this issue with earlier versions (2.7.0)  Standalone code to reproduce the issue ```shell Just applying XNNPACK delegate to any model ```  Relevant log output ```shell ```",2025-02-16T14:38:54Z,stat:awaiting response type:support stale comp:lite comp:lite-xnnpack 2.17,closed,0,11,https://github.com/tensorflow/tensorflow/issues/87385,"Hi, lite24  Thank you for bringing this issue to our attention, I believe you followed this official documentation Build LiteRT with CMake, if possible could you please help me with exact steps before encountering mentioned warning to replicate the same behavior from my end ? Thank you for your cooperation and understanding.","Hi   Please find the CMakeLists.txt we use to build the project (tflite is built alongside it) ```cmake cmake_minimum_required(VERSION 3.10) SET(TARGET voxltfliteserver) set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(TFLITE_ENABLE_GPU ON CACHE BOOL ""Enable GPU delegate support"") set(TFLITE_ENABLE_NNAPI ON CACHE BOOL ""Enable NNAPI delegate support"") set(XNNPACK_ENABLE_ARM_BF16 OFF CACHE BOOL) set(XNNPACK_ENABLE_ARM_I8MM OFF CACHE BOOL) set(TENSORFLOW_SOURCE_DIR ""~/tensorflow_src"" CACHE PATH ""Directory with checkout of tensorflow"") add_subdirectory(""${TENSORFLOW_SOURCE_DIR}/tensorflow/lite"" ""${CMAKE_CURRENT_BINARY_DIR}/tensorflowlite"" EXCLUDE_FROM_ALL) option(BUILD_QRB5165 ""Build the qrb5165 binary"" OFF) if(BUILD_QRB5165)     add_definitions(DBUILD_QRB5165) endif() if(CMAKE_BUILD_TYPE STREQUAL ""DEBUG"")     set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} g3 Wall Wuninitialized Wmaybeuninitialized fnoomitframepointer"") elseif(CMAKE_BUILD_TYPE STREQUAL ""RELEASE"")     set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} O3 fsee fomitframepointer fnosignedzeros fnomatherrno funrollloops"") endif()  Enable compile optimizations  Enable debug flags (use if you want to debug in gdb)  Build from all source files file(GLOB_RECURSE all_src_files *.c*) add_executable(${TARGET} 	${all_src_files} ) include_directories(     ../include/     ../include/model_helper/     /usr/include/opencv4/       apq8096 SPECIFIC      /usr/include/tensorflow/lite/tools/make/downloads/absl/      /usr/include/tensorflow/lite/tools/make/downloads/flatbuffers/include/      qrb5165 SPECIFIC     /usr/include/flatbuffers/include/     /usr/include/abseilcpp/     /usr/include/ruy/ ) set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} Wnoimplicitfallthrough"") if (BUILD_QRB5165) set(LINK_LIBS ""modal_pipe""     ""modal_json""     ""glib2.0""     ""dl""     ""stdc++""     ""pthread""     ""z""     ""cutils""     ""log""     ""sync""     ""gthread2.0""     ""pcre""     ""modal_pipe""     ""modal_json""     ""opencv_core""     ""opencv_highgui""     ""opencv_imgproc""     ""opencv_imgcodecs""     ""opencv_dnn""     ""gsl""     ""llvmqcom""     ""adreno_utils""     ""OpenCL""     ""CB""     ""EGL_adreno""     ""GLESv2_adreno""     ""pthreadpool""     ""rt"") endif() target_link_libraries(${TARGET}     Wl,rpathlink,/usr/lib64/     L/usr/lib64/     ${LINK_LIBS}     tensorflowlite )  make sure everything is installed where we want  LIB_INSTALL_DIR comes from the parent cmake file install( 	TARGETS			${TARGET} 	LIBRARY			DESTINATION ${LIB_INSTALL_DIR} 	RUNTIME			DESTINATION /usr/bin 	PUBLIC_HEADER	DESTINATION /usr/include ) ``` and our build script ```shell echo ""Applying MAI Patches"" patch uN ~/tensorflow_src/tensorflow/lite/CMakeLists.txt i ~/patches/cmake_fix.patch echo ""Done Applying Patches"" mkdir p build cd build   cmake DCMAKE_TOOLCHAIN_FILE=${TOOLCHAIN_QRB5165} DCMAKE_BUILD_TYPE=DEBUG DBUILD_QRB5165=${BUILD_QRB5165} DCMAKE_VERBOSE_MAKEFILE=ON DCMAKE_CXX_FLAGS=""${CMAKE_CXX_FLAGS} std=c++17 march=armv8a"" ${EXTRA_OPTS} ../ make j5 VERBOSE=1 cd ../ ``` with patches ```  tensorflow/tensorflow/lite/CMakeLists.txt.orig	20220209 08:54:18.370750801 0800 +++ tensorflow/tensorflow/lite/CMakeLists.txt	20220209 08:43:51.505795653 0800 @@ 59,13 +59,13 @@    ${CMAKE_PREFIX_PATH}  )  include(CMakeDependentOption) option(TFLITE_ENABLE_RUY ""Enable experimental RUY integration"" OFF) +option(TFLITE_ENABLE_RUY ""Enable experimental RUY integration"" ON)  option(TFLITE_ENABLE_RESOURCE ""Enable experimental support for resources"" ON)  option(TFLITE_ENABLE_NNAPI ""Enable NNAPI (Android only)."" ON) cmake_dependent_option(TFLITE_ENABLE_NNAPI_VERBOSE_VALIDATION ""Enable NNAPI verbose validation."" OFF +cmake_dependent_option(TFLITE_ENABLE_NNAPI_VERBOSE_VALIDATION ""Enable NNAPI verbose validation."" ON                         ""TFLITE_ENABLE_NNAPI"" ON)  option(TFLITE_ENABLE_MMAP ""Enable MMAP (unsupported on Windows)"" ON) option(TFLITE_ENABLE_GPU ""Enable GPU"" OFF) +option(TFLITE_ENABLE_GPU ""Enable GPU"" ON)  option(TFLITE_ENABLE_METAL ""Enable Metal delegate (iOS only)"" OFF)  option(TFLITE_ENABLE_XNNPACK ""Enable XNNPACK backend"" ON) @@ 86,7 +86,7 @@  endif()  set(_TFLITE_ENABLE_NNAPI ""${TFLITE_ENABLE_NNAPI}"")  if(NOT ""${CMAKE_SYSTEM_NAME}"" STREQUAL ""Android"")   set(_TFLITE_ENABLE_NNAPI OFF) +  set(_TFLITE_ENABLE_NNAPI ON)  endif()  set(_TFLITE_ENABLE_MMAP ""${TFLITE_ENABLE_MMAP}"")  if(${CMAKE_SYSTEM_NAME} MATCHES ""Windows"") ``` Once the project is built, the entry point simply invokes the interpreter in a standard manner and tries to apply the delegate. There is a an issue in the cross compilation process with respect to protobuf where the protobuf library built is built for the target but the host tries to use it. We resolved it by manually building protobuf for the host architecture and setting the appropriate system paths.", Was there any update on this? ,"Hi, lite24  If possible could you please add this `set(XNNPACK_ENABLE_JIT ON CACHE BOOL ""Enable JIT in XNNPACK"")` in CMakeLists.txt and see is it resolving your issue or not ? after updating the CMake configuration clean your build directory and recompile to apply the changes. Thank you for your cooperation and patience.","Hi   Thank you for the reply I was not aware that such a flag existed. Yes that error was resolved but a new error has popped up ``` INFO: Created TensorFlow Lite XNNPACK delegate for CPU. INFO: XNNPack weight cache not enabled. VERBOSE: Replacing 94 out of 144 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 21 partitions for the whole graph. ERROR: /home/root/tensorflow_src/tensorflow/lite/core/subgraph.cc:600 tensor>delegate == nullptr  tensor>delegate == delegate was not true. ERROR: Restored original execution plan after delegate application failure. INFO: Error in applying the default TensorFlow Lite delegate indexed at 0, and all previously applied delegates are reverted. Inference time: 115846 ms Output: 6.30584e43 ``` This error also did not used to occur earlier and has only come up once we upgraded tflite. Could you please tell us why this might be occuring","Hi, lite24 Good to hear that initial reported issue got resolved, Troubleshooting TFLite delegate application failures can be challenging.  I recommend the following approaches: Increase logging verbosity to obtain more detailed information about the delegate application process and also verify the model compatibility with the chosen delegate. You can run the model without any delegates to ensure that the model itself is functioning correctly. This helps isolate whether the issue is with the model or the delegate.  Please check if the tensors are already owned by another delegate. The error message `tensor>delegate == nullptr  tensor>delegate == delegate was not true` indicates a conflict please ensure that tensors are not being shared between delegates.You can do this by inspecting the tensor's delegate before applying a new one.  Please check for unsupported operations some operations in your model may not be supported by the delegate. For XNNPACK, you can find the list of supported operations in the XNNPACK documentation and for GPU delegates  Thank you for your cooperation and understanding."," As you said it likely fails due to tensors being owned by two delegates. The issue occurs due to a failure to apply GPU delegate (which is a separate issue), and it then falls back to XNNPACK to run on CPU. If I don't apply GPU delegates XNNPACK works fine.  This I believe is a bug that did not occur in older versions, the fallback to XNNPACK is likely not changing ownership of the tensors.","Hi, lite24  I apologize for the delayed response, if possible could you please give it try with latest stable version of `TensorFlow 2.19.0` and `tfnightly` and see is it resolving your issue or not ? Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
carlosg-m,Implemnet Stochastic Depth for ConvNeXt models, Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.16.1  Custom code Yes  OS platform and distribution Ubuntu 22.04.4 LTS  Mobile device _No response_  Python version 3.11.0  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? As in the original paper and HuggingFace implementation `tf.keras.applications.convnext` family of models should support **Stochastic Depth**.  Standalone code to reproduce the issue ```shell tf.keras.applications.convnext(drop_path_rate=...) ```,2025-02-16T14:35:21Z,stat:awaiting response type:feature stale comp:keras,closed,0,3,https://github.com/tensorflow/tensorflow/issues/87384,"Hi **m** , Apologies for the delay, and thank you for your patience. Please post this issue on kerasteam/keras repo. as this issue is more related to keras Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
khangtruong2252314,Colab TPU crash on transformer import," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.18  Custom code No  OS platform and distribution Google colab default  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Import transformer module get crash on a colab notebook with TPU   Standalone code to reproduce the issue ```shell from transformers import TFT5ForConditionalGeneration ''' This will crash the TPU session ''' ```  Relevant log output ```shell ""Crashed for unknown reason"" ```",2025-02-16T13:13:48Z,type:bug,closed,0,5,https://github.com/tensorflow/tensorflow/issues/87383,Are there any error messages during the installation process? It could help identify what is causing the issue,"No, sorry. I just open the colab and import, then crash. Perhaps you now can recreate it quickly as it is the default colab session. There are some session log that might be useful.","Is there something in the error message that points to Tensorflow? Or, in other words, should this be opened on Colab repo or on transformers repo?","Ah, sorry, I was working with tensorflow that I didn’t realize it was colab’s fault. ",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T05:39:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87382
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T05:27:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87381
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T04:33:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87380
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T04:32:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87379
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T04:29:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87378
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T04:23:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87377
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T04:22:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87376
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T04:20:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87375
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T04:19:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87374
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T04:16:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87373
copybara-service[bot],Automated Code Change,Automated Code Change Reverts 35fb4a99534dc85e47eeedf760ab691ddda31219,2025-02-16T04:15:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87372
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T04:11:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87371
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T03:47:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87370
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T03:45:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87369
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T03:39:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87368
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-16T03:36:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87367
MoritzKronberger,GPU and CPU utilization dropping to 0% during long training runs," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tf 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.1 LTS  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA 2.14  GPU model and memory _No response_  Current behavior? When running the attached test script for longer, at some point during the training GPU and CPU utilization will both fall to 0%, although neither VRAM nor RAM are exhausted. The training on my NVIDIA L40S vGPU slows from ~35ms/step with batch size 256 to minutes per step. Training speed only recovers on reboot. Larger batch sizes will make the error occurr later during training. I am unsure if this is an issue of my machine/the vGPU configuration.  Standalone code to reproduce the issue ```shell import tensorflow as tf print(""TensorFlow version:"", tf.__version__) print(""CPU devices:"", tf.config.list_physical_devices(""CPU"")) print(""GPU devices:"", tf.config.list_physical_devices(""GPU"")) print(""Using cuDNN:"", tf.test.is_built_with_cuda()) gpus = tf.config.list_physical_devices(""GPU"") for gpu in gpus:     tf.config.experimental.set_memory_growth(gpu, True) with tf.device(""GPU:0""):     num_samples = 18_000     input_length = 480     input_channels = 1     X = tf.random.normal((num_samples, input_length, input_channels), dtype=tf.float32)     Y = tf.random.normal((num_samples, input_length, input_channels), dtype=tf.float32)     input = tf.keras.layers.Input(shape=(input_length, input_channels))     conv = tf.keras.layers.Conv1D(         filters=256,         kernel_size=5,         strides=1,         padding='same',         activation=None,         input_shape=(input_length, input_channels)     )(input)     pool = tf.keras.layers.AveragePooling1D(         pool_size=2,         strides=2     )(conv)     deconv = tf.keras.layers.Conv1DTranspose(         filters=256,         kernel_size=4,         strides=2,         padding='same',         activation=None     )(pool)     dense = tf.keras.layers.Dense(2048*2, activation='tanh')(deconv)     out = tf.keras.layers.Dense(input_channels, activation='linear')(dense)     model = tf.keras.models.Model(         inputs=input,         outputs=out     )   type: ignore     model.summary()     model.compile(optimizer='adam', loss='mse', jit_compile=True)     dataset = tf.data.Dataset.from_tensor_slices((X, Y))     dataset = dataset.batch(256).prefetch(tf.data.AUTOTUNE)     model.fit(dataset, epochs=500, verbose=1) ```  Relevant log output ```shell 20250216 06:35:01.114446: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250216 06:35:01.824571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1739687702.107514    2941 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1739687702.197108    2941 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250216 06:35:02.874809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. TensorFlow version: 2.18.0 CPU devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')] GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] Using cuDNN: True I0000 00:00:1739687707.790282    2941 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5117 MB memory:  > device: 0, name: NVIDIA L40S8C, pci bus id: 0000:03:04.0, compute capability: 8.9 20250216 06:35:07.832778: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:07.838416: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. /home/mokro1/canilm/venv/lib/python3.12/sitepackages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.   super().__init__(activity_regularizer=activity_regularizer, **kwargs) 20250216 06:35:07.899661: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing Cast input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:07.903310: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.228058: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.228711: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.230774: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing StatelessRandomGetKeyCounter input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:CPU:0 but is actually on /job:localhost/replica:0/task:0/device:GPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.249462: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.260795: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing Cast input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.263365: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.264361: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.265277: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing StatelessRandomGetKeyCounter input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:CPU:0 but is actually on /job:localhost/replica:0/task:0/device:GPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.274980: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing Cast input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.276006: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.276518: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.276878: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing StatelessRandomGetKeyCounter input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:CPU:0 but is actually on /job:localhost/replica:0/task:0/device:GPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.285657: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing Cast input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.286505: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.286793: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. 20250216 06:35:09.287169: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing StatelessRandomGetKeyCounter input CC(未找到相关数据) was expected to be on /job:localhost/replica:0/task:0/device:CPU:0 but is actually on /job:localhost/replica:0/task:0/device:GPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck. ```",2025-02-15T21:12:30Z,type:bug TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/87366,"Hi **** , Thanks for raising your concern here. Could you please provide information about the cuDNN version you are using? There might be a mismatch in version compatibility. Please check all the compatible versions to avoid issues. I am providing the documentation for your reference. I also tried with reduced sizes, and it worked fine for me. Please find gist1 and gist2 here for your reference. Thank you!","I forgot to add `tf.config.experimental.set_device_policy(""warn"")` to the standalone code, which produces the log output on my local machine. Trying this in Colab did not produce the same logs, but I am usure if Colab is filtering logs. The crash happens after ~200 epochs on my machine (without reduced sizes), 10 epochs are not an issue. Regarding cuDNN: `nvidiacudnncu12=9.3.0.75` and `nvidiacudnncu11=8.5.0.96` are installed, according to TF logs 9.3.0.75 is used."
tvortsa,Comeon guys!," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version all  Custom code Yes  OS platform and distribution no  Mobile device no  Python version no  Bazel version no  GCC/compiler version no  CUDA/cuDNN version no  GPU model and memory no  Current behavior? Это конечно все интересно, нейросетки, все дела, но, блин: Вопервых: СUDA только в линукс или я недопонял чегото? Вовторых: на Deno я так и не смог поставить этот ваш tensorflow Хреновато както для мегагуглпроекта не? Я пока на brain.js пошел, а что делать!?  Standalone code to reproduce the issue ```shell meh ```  Relevant log output ```shell ```",2025-02-15T12:41:03Z,type:bug invalid,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87362,Please don't spam
Raju-07,Request for Python 3.13.2 Compatibility in TensorFlow," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.8 i dont know  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Subject: Request for Python 3.13.2 Compatibility in TensorFlow Dear TensorFlow Team, I hope this message finds you well. I am writing to express my appreciation for the incredible work you have done with TensorFlow. It has been an invaluable tool for my machine learning and data science projects. I recently upgraded to Python 3.13.2 and noticed that TensorFlow is not yet compatible with this version. As an enthusiastic user of both Python and TensorFlow, I kindly request that you consider adding support for Python 3.13.2 in an upcoming release. This would greatly benefit the community and allow us to continue using the latest Python features alongside TensorFlow. Thank you for your attention to this matter. I look forward to your response and appreciate your continuous efforts in improving TensorFlow. Best regards, Raju Yadav  Standalone code to reproduce the issue ```shell Subject: Request for Python 3.13.2 Compatibility in TensorFlow Dear TensorFlow Team, I hope this message finds you well. I am writing to express my appreciation for the incredible work you have done with TensorFlow. It has been an invaluable tool for my machine learning and data science projects. I recently upgraded to Python 3.13.2 and noticed that TensorFlow is not yet compatible with this version. As an enthusiastic user of both Python and TensorFlow, I kindly request that you consider adding support for Python 3.13.2 in an upcoming release. This would greatly benefit the community and allow us to continue using the latest Python features alongside TensorFlow. Thank you for your attention to this matter. I look forward to your response and appreciate your continuous efforts in improving TensorFlow. Best regards, Raju Yadav ```  Relevant log output ```shell ```",2025-02-15T09:54:25Z,type:support,closed,0,3,https://github.com/tensorflow/tensorflow/issues/87361,Please see https://github.com/tensorflow/tensorflow/issues/78774.,Please search for duplicates before opening new issues.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-15T09:44:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87360
copybara-service[bot],compat: Update forward compatibility horizon to 2025-02-15,compat: Update forward compatibility horizon to 20250215,2025-02-15T09:16:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87359
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-15T05:47:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87358
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-15T05:11:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87357
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-15T05:07:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87356
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-15T05:04:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87355
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-15T05:02:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87354
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-15T04:59:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87353
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-15T04:50:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87352
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-15T04:45:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87351
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-15T04:39:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87350
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-15T04:38:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87349
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-15T04:37:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87348
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-15T04:27:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87347
copybara-service[bot],Return InvalidArgument when a model does not have the correct tags.,Return InvalidArgument when a model does not have the correct tags.,2025-02-15T02:37:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87346
copybara-service[bot],Reverts 37bffc34ec432bcc331fa0cc7634c3890418c48f,Reverts 37bffc34ec432bcc331fa0cc7634c3890418c48f,2025-02-15T02:37:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87345
copybara-service[bot],Create data_table_utils preparing for the profiler convertion,Create data_table_utils preparing for the profiler convertion,2025-02-15T01:15:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87344
copybara-service[bot],Regenerate tf_generated_ops.td after adding a new attribute to the WriteTrainingPredictions custom op to support writing vector predictions to file storage.,Regenerate tf_generated_ops.td after adding a new attribute to the WriteTrainingPredictions custom op to support writing vector predictions to file storage.,2025-02-15T01:13:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87343
copybara-service[bot],PR #22757: Fix MemcpyP2pWhileLoopCorrectness test failure in collective e2e suite,"PR CC(keras.model.predict also needs two inputs (one for sample, and one for label) when used with tf.data together): Fix MemcpyP2pWhileLoopCorrectness test failure in collective e2e suite Imported from GitHub PR https://github.com/openxla/xla/pull/22757 Copybara import of the project:  906cec63655f93e2fcf23b1530d721b499cc8750 by TJ Xu : Fix MemcpyP2pWhileLoopCorrectness test failure in collective e2e suite Merging this change closes CC(keras.model.predict also needs two inputs (one for sample, and one for label) when used with tf.data together) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22757 from Tixxx:tixxx/fix_22588 906cec63655f93e2fcf23b1530d721b499cc8750",2025-02-15T00:15:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87342
copybara-service[bot],Tweak formatting of a few lines in `generate_hlo_test_checks.py`.,Tweak formatting of a few lines in `generate_hlo_test_checks.py`.,2025-02-15T00:00:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87341
copybara-service[bot],litert: Workaround fix of MTK Dispatch API,litert: Workaround fix of MTK Dispatch API,2025-02-14T23:51:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87340
copybara-service[bot],split QuantOps from register_common_dialects,split QuantOps from register_common_dialects,2025-02-14T23:27:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87339
copybara-service[bot],Install free threaded python3.13t to the ml_build arm64 docker image,Install free threaded python3.13t to the ml_build arm64 docker image python3.13nogil is a freethreaded build of python3.13.,2025-02-14T22:40:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87338
copybara-service[bot],Add components that are going to be used by tfrt_gpu_client,Add components that are going to be used by tfrt_gpu_client,2025-02-14T22:07:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87337
copybara-service[bot],Migrate tf/compiler/mlir/quantization/stablehlo:quantization_patterns to use TF's fork of QuantOps Dialect from LiteRT's fork,Migrate tf/compiler/mlir/quantization/stablehlo:quantization_patterns to use TF's fork of QuantOps Dialect from LiteRT's fork,2025-02-14T21:38:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87336
copybara-service[bot],IFRT proxy: Add custom_call_program_serdes to common_serdes,IFRT proxy: Add custom_call_program_serdes to common_serdes,2025-02-14T21:38:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87335
copybara-service[bot],[XLA:MSA] Fix an issue in MSA where `operands_in_alternate_memory_map_` was not correctly updated when there is cross program prefetch.,[XLA:MSA] Fix an issue in MSA where `operands_in_alternate_memory_map_` was not correctly updated when there is cross program prefetch.,2025-02-14T21:24:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87334
copybara-service[bot],Call out `generate_hlo_test_checks.py` in online documentation.,Call out `generate_hlo_test_checks.py` in online documentation.,2025-02-14T21:23:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87333
copybara-service[bot],remove nocheck_visibility from testing LiteRT repo,remove nocheck_visibility from testing LiteRT repo,2025-02-14T21:11:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87332
copybara-service[bot],Don't run the `--nobuild` command for MACOS_CPU_ARM64.,Don't run the `nobuild` command for MACOS_CPU_ARM64. We can probably install `parallel` via `brew` in an extra setup command but for now align with MACOS_CPU_X86.,2025-02-14T21:11:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87331
copybara-service[bot],[StableHLO] Only emit StableHLO from xla_computation_to_mlir_module,[StableHLO] Only emit StableHLO from xla_computation_to_mlir_module Update to use new HLO to StableHLO API. Currently all users of this function have this flag set to true so should be a low impact change.,2025-02-14T20:05:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87330
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T19:41:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87329
copybara-service[bot],Don't install Bazelisk.,Don't install Bazelisk. Install currently fails: ``` INFO:root:Starting process: sudo wget noverbose O /usr/local/bin/bazel https://github.com/bazelbuild/bazelisk/releases/download/v1.11.0/bazeliskdarwinarm64 /usr/local/bin/bazel: No such file or directory ```,2025-02-14T19:32:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87328
copybara-service[bot],[XLA] C++ coding style cleanup,[XLA] C++ coding style cleanup,2025-02-14T19:24:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87327
copybara-service[bot],Create build rules for building wheels,Create build rules for building wheels,2025-02-14T19:13:29Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87326,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],[RFC] [XLA][TPU][MSA] Use HW Spec instead of hardcoding bandwidth multipliers,"[RFC] [XLA][TPU][MSA] Use HW Spec instead of hardcoding bandwidth multipliers Changes:  Instead of using a constant for bandwidth multiplier for alternative memory vs default memory, use HW spec data to derive the multiplier differentiating read vs write.  While it doesn't affect performance much, this change adds support for providing different read vs write multipliers.",2025-02-14T18:07:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87325
copybara-service[bot],add wheel building rule,add wheel building rule,2025-02-14T18:05:44Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/87324,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],[XLA] Fix a bug in SchedulingAnnotationCrossesOverlapLimit.,"[XLA] Fix a bug in SchedulingAnnotationCrossesOverlapLimit. Before this code change, we checked whether scheduling each instruction in a given group individually would cross any limits. This check was not strong enough because i) async instructions in the group are supposed to overlap each other and ii) total resource usage of the group can still exceed the limit while individual usages do not. For example, if the allgather limit is 1, we should not allow a group with 2 async allgathers to be scheduled. With this code change, we compute the ""accumulated"" resource usage of the annotation group and compare that against the limit.",2025-02-14T17:38:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87323
copybara-service[bot],Move utility aspect rules to the `third_party/py/python_wheel.bzl`.,Move utility aspect rules to the `third_party/py/python_wheel.bzl`. The aspects can be used in wheel build rules across all Google ML projects. They provide capability to build the wheels for the crosscompile configuration.,2025-02-14T17:12:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87322
copybara-service[bot],Reverts 8f0f3a5af6edc9fe9471929245312df6658af888,Reverts 8f0f3a5af6edc9fe9471929245312df6658af888,2025-02-14T17:03:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87321
copybara-service[bot],Create JAX wheel build target.,"Create JAX wheel build target. This change introduces a uniform way of building the artifacts and controlling the filename version suffixes (see the changes for jaxlib, CUDA and PJRT plugins in https://github.com/jaxml/jax/pull/25126) Previously JAX wheel was built via `python3 m build` command. The resulting wheel contained the python packages files in `jax` folder (e.g. the files in the subdirs that have `__init__.py` file). You can still build the JAX wheel with `python3 m build` command.  Bazel command example for building nightly JAX wheel: ``` bazel build :jax_wheel \   config=ci_linux_x86_64 \   repo_env=HERMETIC_PYTHON_VERSION=3.10 \   repo_env=ML_WHEEL_TYPE=custom \   repo_env=ML_WHEEL_BUILD_DATE=20250211 \   repo_env=ML_WHEEL_GIT_HASH=$(git revparse HEAD) ``` Resulting wheel: ``` bazelbin/dist/jax0.5.1.dev20250211+d4f1f2278py3noneany.whl ```",2025-02-14T16:45:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87320
copybara-service[bot],[XLA:GPU] Support cases in legacy matmul emitter where the broadcast multiplier is not equal to the block_k size.,"[XLA:GPU] Support cases in legacy matmul emitter where the broadcast multiplier is not equal to the block_k size. The fix allows to use block_k less than broadcast size (256) for the cases like the one below: [B, 8, 256, M] broadcast([B, 8, M]) then [B, 2048, M] bitcast([B, 8, 256, M]) then [B,M,N] dot([B, 2048, M], ...) We check that the block_k is not equal to the broadcast size and if yes then calculate the argument for advance from the block_k size, broadcast size and the current index value of the for loop.",2025-02-14T16:32:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87319
copybara-service[bot],Fix typos in `lite/core:model_building`,Fix typos in `lite/core:model_building`,2025-02-14T15:49:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87318
copybara-service[bot],PR #22640: [NFC] Deduplicate functions between HLO runners.,PR CC(Fix warning for format specifier): [NFC] Deduplicate functions between HLO runners. Imported from GitHub PR https://github.com/openxla/xla/pull/22640 Copybara import of the project:  e0e1de485c9120c1d52c8f10d843c13097802e07 by Ilia Sergachev : [NFC] Deduplicate functions between HLO runners.  aeff13d83e159fd8e2cb12082a2549ccdbe0b2b6 by Ilia Sergachev : [NFC] Delete unused functions.  c073163cd1ce91ddb12c7b52d760194f33de9fe2 by Ilia Sergachev : Update xla/service/hlo_runner_interface.: Allan Renucci   b043017e52b444fbfff1c49655952899f6c4fd6d by Ilia Sergachev : Update xla/service/hlo_runner_interface.h Coauthoredby: Allan Renucci   48383f2873e8614afc678c3d7842ca1815d53d40 by Ilia Sergachev : Update xla/service/hlo_runner_interface.h Coauthoredby: Allan Renucci   d5bca65f50c2e1b05ce36f1d485c107806bfbc94 by Ilia Sergachev : Rename HloProtoToModule to CreateModuleFromProto for consistency.  be158dda3fd2450e1df9d91e833a563eb5a70119 by Ilia Sergachev : Update xla/service/hlo_runner_interface.h Coauthoredby: Allan Renucci   e34efb499fa492929d9a3116a9dfbc112667e1b3 by Ilia Sergachev : Update xla/service/hlo_runner_interface.h Coauthoredby: Allan Renucci  Merging this change closes CC(Fix warning for format specifier) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22640 from openxla:cleanup_hlo_functions e34efb499fa492929d9a3116a9dfbc112667e1b3,2025-02-14T12:26:03Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87317,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],[XLA:GPU] Remove rng get and update state from unsupported ops,[XLA:GPU] Remove rng get and update state from unsupported ops Missed in https://github.com/openxla/xla/pull/22462,2025-02-14T12:00:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87316
copybara-service[bot],[XLA:GPU] Add merge functionality to collective perf table gen.,[XLA:GPU] Add merge functionality to collective perf table gen. Convenience function to merge multiple perf tables as one. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/87213 from fujunwei:enable_opencl_delegate 03d141bf56629baa7a40ee5234246b9f4c306eb6,2025-02-14T11:50:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87315
copybara-service[bot],Integrate Triton up to [c5036b9b](https://github.com/openai/triton/commits/c5036b9ba1b60b53a7cecaf58a0c8b8cf8ac557b),Integrate Triton up to c5036b9b,2025-02-14T11:44:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87314
Venkat6871,Fix typos in documentation strings,"Hi, Team I observed few typos in the documentation strings and I have fixed those typos so please do the needful. Thank you.",2025-02-14T11:37:53Z,ready to pull size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87313
copybara-service[bot],PR #22680: Changing the default value of the flag xla_dump_hlo_as_long_text,"PR CC(1.12rc0 cherrypick request: Pin wheel=0.31.1 to work around issue https://github.com/pypa/auditwheel/issues/102): Changing the default value of the flag xla_dump_hlo_as_long_text Imported from GitHub PR https://github.com/openxla/xla/pull/22680 This stems from the difference in HLO dumps from tools like hloopt, multihost_hlo_runner, and hlo_runner_main. Although the options related to printing HLO are tied to the HLO via DebugOptions, these tools have different behaviors because the tool hloopt uses `ToString()` function from `HloModule` while `functional_hlo_runner` uses a separate dumping utility from `xla/service/dump.cc`. To standardize this behavior, we first are making the default behavior uniform by setting the default value of long text HLO dumps to be true. This ensures that the HLO dumps will be functional by default (for example, backend_config will be printed). Copybara import of the project:  0818c7871688a8e18cefdaea7bd17fd7aba293d9 by Shraiysh Vaishay : Changing the default value of the flag xla_dump_hlo_as_long_text This stems from the difference in HLO dumps from tools like hloopt, multihost_hlo_runner, and hlo_runner_main. Although the options related to printing HLO are tied to the HLO via DebugOptions, these tools have different behaviors because the tool hloopt uses `ToString()` function from `HloModule` while `functional_hlo_runner` uses a separate dumping utility from `xla/service/dump.cc`. To standardize this behavior, we first are making the default behavior uniform by setting the default value of long text HLO dumps to be true. This ensures that the HLO dumps will be functional by default (for example, backend_config will be printed). Merging this change closes CC(1.12rc0 cherrypick request: Pin wheel=0.31.1 to work around issue https://github.com/pypa/auditwheel/issues/102) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22680 from shraiysh:xla_dump_hlo_as_long_text_as_true 0818c7871688a8e18cefdaea7bd17fd7aba293d9",2025-02-14T10:55:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87312
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T10:05:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87311
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T09:47:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87310
copybara-service[bot],"PR #22645: Fix error in the gpu_specs README. The spec is the TargetConfig, which includes the device description.","PR CC(Problem with CUDA 10 and Tensorflow 1.11): Fix error in the gpu_specs README. The spec is the TargetConfig, which includes the device description. Imported from GitHub PR https://github.com/openxla/xla/pull/22645 Copybara import of the project:  5fefad4bdb1b947844c7c2d8ff1029f5c99aace5 by Dimitris Vardoulakis : Fix error in the specs README. The spec is the TargetConfig, which includes the device description.  39f8e9f343de45b4678a6760b9a9720d48163299 by Dimitris Vardoulakis : Update xla/tools/hlo_opt/gpu_specs/README.md Coauthoredby: Allan Renucci  Merging this change closes CC(Problem with CUDA 10 and Tensorflow 1.11) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22645 from dimvar:addreadmetogpu_specs 39f8e9f343de45b4678a6760b9a9720d48163299",2025-02-14T09:30:47Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/87309,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],PR #22575: [XLA:GPU] Fix triton sparse dot lowering on Blackwell,"PR CC(Preferred way of installing TensorRT with Tensorflow 1.11 on Ubuntu?): [XLA:GPU] Fix triton sparse dot lowering on Blackwell Imported from GitHub PR https://github.com/openxla/xla/pull/22575 Sparse dot is supported for MMA v2 and v3 only, and sm100/sm120 should use MMA v2 (v3 is Hopperonly). Copybara import of the project:  bd4c827db0e4adbff629bf0b02d09ff2860e4fb2 by Sergey Kozub : [XLA:GPU] Fix triton sparse dot lowering on Blackwell Merging this change closes CC(Preferred way of installing TensorRT with Tensorflow 1.11 on Ubuntu?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22575 from openxla:skozub/sm100_sparse bd4c827db0e4adbff629bf0b02d09ff2860e4fb2",2025-02-14T09:05:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87308
copybara-service[bot],PR #22680: Changing the default value of the flag xla_dump_hlo_as_long_text,"PR CC(1.12rc0 cherrypick request: Pin wheel=0.31.1 to work around issue https://github.com/pypa/auditwheel/issues/102): Changing the default value of the flag xla_dump_hlo_as_long_text Imported from GitHub PR https://github.com/openxla/xla/pull/22680 This stems from the difference in HLO dumps from tools like hloopt, multihost_hlo_runner, and hlo_runner_main. Although the options related to printing HLO are tied to the HLO via DebugOptions, these tools have different behaviors because the tool hloopt uses `ToString()` function from `HloModule` while `functional_hlo_runner` uses a separate dumping utility from `xla/service/dump.cc`. To standardize this behavior, we first are making the default behavior uniform by setting the default value of long text HLO dumps to be true. This ensures that the HLO dumps will be functional by default (for example, backend_config will be printed). Copybara import of the project:  0818c7871688a8e18cefdaea7bd17fd7aba293d9 by Shraiysh Vaishay : Changing the default value of the flag xla_dump_hlo_as_long_text This stems from the difference in HLO dumps from tools like hloopt, multihost_hlo_runner, and hlo_runner_main. Although the options related to printing HLO are tied to the HLO via DebugOptions, these tools have different behaviors because the tool hloopt uses `ToString()` function from `HloModule` while `functional_hlo_runner` uses a separate dumping utility from `xla/service/dump.cc`. To standardize this behavior, we first are making the default behavior uniform by setting the default value of long text HLO dumps to be true. This ensures that the HLO dumps will be functional by default (for example, backend_config will be printed). Merging this change closes CC(1.12rc0 cherrypick request: Pin wheel=0.31.1 to work around issue https://github.com/pypa/auditwheel/issues/102) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22680 from shraiysh:xla_dump_hlo_as_long_text_as_true 0818c7871688a8e18cefdaea7bd17fd7aba293d9",2025-02-14T09:00:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87307
copybara-service[bot],"Improve standalone tool in ""compile"" mode, now able to write byte code to multiple output files.","Improve standalone tool in ""compile"" mode, now able to write byte code to multiple output files. e.g. ./apply_plugin_main compile libs  model  o  o  soc_man  soc_model ",2025-02-14T08:23:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87306
chunhsue,Qualcomm AI Engine Direct - Another implementation of DUS without accuracy issue., What New implementation of DUS which resolves the previously found accuracy issue.  Tests ``` [] Global test environment teardown [==========] 97 tests from 5 test suites ran. (3264 ms total) [  PASSED  ] 97 tests. ```,2025-02-14T08:13:44Z,awaiting review ready to pull size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87305
copybara-service[bot],Integrate LLVM at llvm/llvm-project@5586541d220e,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 5586541d220e,2025-02-14T06:54:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87304
copybara-service[bot],[IFRT] Emit errors after compilations have been dispatched to avoid capturing them in scoped diagnostic handlers.,[IFRT] Emit errors after compilations have been dispatched to avoid capturing them in scoped diagnostic handlers.,2025-02-14T05:45:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87303
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T05:13:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87302
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T05:13:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87301
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T05:11:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87300
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T05:09:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87299
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T05:07:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87298
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T05:07:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87297
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T05:05:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87296
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T05:03:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87295
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T04:58:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87294
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T04:57:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87293
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T04:57:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87292
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T04:56:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87291
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T04:50:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87290
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T04:49:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87289
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T04:47:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87288
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T04:43:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87287
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-14T04:42:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87286
copybara-service[bot],Disallow get default platform on PJRT migrated tests.,Disallow get default platform on PJRT migrated tests.,2025-02-14T04:34:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87285
copybara-service[bot],Update TrackedDeviceBuffer to contain a list of refcounted raw device buffers.,"Update TrackedDeviceBuffer to contain a list of refcounted raw device buffers. Note that this still has a problem when donating foreign buffers, but allows this to be fixed as a followup.",2025-02-14T03:19:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87284
copybara-service[bot],Adds support for string and binary data processing in Colocated Python with Pathways backend.,Adds support for string and binary data processing in Colocated Python with Pathways backend.,2025-02-14T02:29:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87283
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@45989756,Integrate StableHLO at openxla/stablehlo,2025-02-14T02:23:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87282
copybara-service[bot],Adds support for string and binary data processing in Colocated Python.,Adds support for string and binary data processing in Colocated Python.,2025-02-14T02:11:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87281
copybara-service[bot],Support per-axis quantization of TFL::ReshapeOp,Support peraxis quantization of TFL::ReshapeOp,2025-02-14T01:20:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87280
copybara-service[bot],Make `generate_hlo_test_checks.py` backwards-compatible with Python 3.9.,Make `generate_hlo_test_checks.py` backwardscompatible with Python 3.9.,2025-02-14T01:11:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87279
copybara-service[bot],Removed fingerprint compute in TfOpStats write time.,Removed fingerprint compute in TfOpStats write time.,2025-02-14T01:10:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87278
copybara-service[bot],Cleanup obselete logic for Kokoro builds now that all builds modulo MacOS are on GitHub Actions,"Cleanup obselete logic for Kokoro builds now that all builds modulo MacOS are on GitHub Actions `generate_index_html.sh` is unneeded for the MacOS build due to the build script calling it anyway: https://github.com/openxla/xla/blob/main/.kokoro/macos/build.shL23, and otherwise this change is an NFC",2025-02-14T00:44:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87277
copybara-service[bot],Fix TFL::ReshapeOp to allow correct per-axis quantization types.,Fix TFL::ReshapeOp to allow correct peraxis quantization types.,2025-02-14T00:32:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87276
copybara-service[bot],Add conversion from quantization composites to stablehlo native quantized ops.,Add conversion from quantization composites to stablehlo native quantized ops.,2025-02-14T00:20:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87275
copybara-service[bot],Add runtime-specific initialization data to IFRT proxy connection creation,Add runtimespecific initialization data to IFRT proxy connection creation,2025-02-14T00:13:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87274
wonjeon,[mlir][tosa] Update Tensorflow to match TOSA v1.0 specification (part 3),"We've been pushing TOSA v1.0 LLVM patches to upstream. This PR includes commits to keep the following operators to be aligned with LLVM: [mlir][tosa] Change 'shape' of RESHAPE from attribute to input shape https://github.com/llvm/llvmproject/pull/125789 [mlir][tosa] Make Convolution Zero Points Inputs https://github.com/llvm/llvmproject/pull/122939 [mlir][tosa] Remove Quantization Attribute https://github.com/llvm/llvmproject/pull/125479 [mlir][tosa] Change ClampOp's min/max attributes https://github.com/llvm/llvmproject/pull/125197 [mlir][tosa] Make TOSA RESIZE's scale, offset, border as Input https://github.com/llvm/llvmproject/pull/124956 This patch including: [mlir][tosa] Change 'shape' attribute of RESHAPE operator to become an input Signedoffby: Won Jeon  ChangeId: I938503349f38b64db5e77a01c3a7b2bb33e8f041 [mlir][tosa] Switch zero point of convolutions to input variable type ChangeId: Ifd3f24779886377735e6694f716f23c5137f9138 [Tosa] Refactor QuantizationAttr changes due to removal of quantization attr in TOSA dialect and due to name changes in while_loop region names Signedoffby: Tai Ly  ChangeId: I09533bffcd8e2179505c7e11e1320b673266585d [Tosa] ClampOp attributes changes This patch implements changes required by Tosa ClampOp's new min_val/max_val attributes Signedoffby: Tai Ly  ChangeId: I25ba0d077fa44d4c384ab094a6070a4743383414 [TOSA] Calculate unknown reshape dimension when input is static This commit updates the reshape legalization to calculate static shape and output type when a static input shape is provided and only one dimension is unknown. ChangeId: I0843549b47131b0852fbf375f00846b1fcbe8bc6 Signedoffby: Luke Hutton  [TOSA] Numerical mismatch on tfl.transpose_conv layer * Legalization now handles cases where the layer has a bias ChangeId: Ie3ba38644d1cf8e5d6f71271e8bb6f1b5636f406 [mlir][tosa] Change resize attrs to inputs This patch implements changes required by Tosa resize op's scale/offset/border changing from attributes to inputs. Signedoffby: Tai Ly  ChangeId: I9a4319ac53298c25568fc651e249528b9a9477fc",2025-02-13T23:59:44Z,size:XL comp:lite-tosa,closed,0,6,https://github.com/tensorflow/tensorflow/issues/87273,"Local testing successfully done: INFO: Build completed successfully, 27 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test        PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test         PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test             PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test      PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test                PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test   PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test           PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test  PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test      PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test         PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 2.9s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test     PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test        PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test   PASSED in 0.3s Executed 17 out of 17 tests: 17 tests pass.",Internal CI with bazel test passing.  Assigning to  . ,Tested and confirmed that this patch works with the following LLVM integration: https://github.com/tensorflow/tensorflow/commit/01197637cb4eee7da2f1645f7367268ee44efa87,"Tested and confirmed that this patch works with the LLVM integration, https://github.com/tensorflow/tensorflow/commit/4d8657f0c13540570c65bb8fc9a17b9d66755f6f."," , thanks for your comments. Changed the PR so that it contains multiple individual patches plus the test file changes. Hopefully this helps for your review.","Local testing successfully done: INFO: Analyzed 33 targets (0 packages loaded, 33384 targets configured). INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 2.061s, Critical Path: 0.25s INFO: 2 processes: 2 local. INFO: Build completed successfully, 2 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test    (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test       (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test  (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test (cached) PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test (cached) PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 0.1s Executed 1 out of 17 tests: 17 tests pass."
copybara-service[bot],Use seprate collective resource when scheduling p2p communication,Use seprate collective resource when scheduling p2p communication This is in preparation of removing all the 4 existing p2p resources. We are simplifying the pipeline parallelism implementation here.,2025-02-13T23:09:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87272
copybara-service[bot],[DirectSession] Add experimental option for making `Finalize` clear the function library.,[DirectSession] Add experimental option for making `Finalize` clear the function library. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22588 from Tixxx:tixxx/event_barrier 3d78e81091eb0b6cb1dc22f1ff27c3a8de4cff4f,2025-02-13T22:55:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87271
copybara-service[bot],Remove docker logic from `build.py`.,"Remove docker logic from `build.py`. Docker images are fetched by GitHub Actions, and the only build that should remain on Kokoro is MacOS which doesn't use docker anyway.",2025-02-13T22:32:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87270
copybara-service[bot],Disable weight sharing for legacy chips.,Disable weight sharing for legacy chips.,2025-02-13T22:18:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87269
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T22:06:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87268
copybara-service[bot],[MHLO] Use generated constructors for MHLO->StableHLO pass.,"[MHLO] Use generated constructors for MHLO>StableHLO pass. This pass uses an option declared in tablegen, and needs the generated ctors to be able to create a pass that has an option set.",2025-02-13T22:05:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87267
copybara-service[bot],PR #86630: Qualcomm AI Engine Direct - Support DUS and Pack Op in LiteRT,"PR CC(Qualcomm AI Engine Direct  Support DUS and Pack Op in LiteRT): Qualcomm AI Engine Direct  Support DUS and Pack Op in LiteRT Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/86630  WHAT  Pack is supported using QNN Concat to address type support issue.  DUS are supported partially for specific use cases.  Reuse input static tensor.  Use absl::Span instead of std::span.  TESTS ```  build bazel build c opt cxxopt=std=c++17 //tensorflow/lite/experimental/litert/vendors/qualcomm/compiler:qnn_compiler_plugin_test  run ./bazelbin/tensorflow/lite/experimental/litert/vendors/qualcomm/compiler/qnn_compiler_plugin_test ``` ``` [] Global test environment teardown [==========] 100 tests from 4 test suites ran. (4292 ms total) [  PASSED  ] 100 tests. ``` Copybara import of the project:  5fca94baacb2d669dd1d32c26d615b79274b3644 by chunhsue : pack op builder  e5994feec2100044b7994baaecadee69982e0de7 by chunhsue : Support DUS op  33980cbee14c6b55b53f52c29f64537acc8d59d4 by chunhsue : reuse static input tensor  988465882f4e4c2d90e46dd1cf7762c8760b1daf by chunhsue : use absl span, stick to c++17  f6bedf4292218118a9b45c0d18b8b1bf46f68238 by chunhsue : refactor pack op and correct the return value of builder  2a54437cf8f703c1e380125c2aa8a48d636abb70 by chunhsue : comment out failed tests due to QNN regression Merging this change closes CC(Qualcomm AI Engine Direct  Support DUS and Pack Op in LiteRT) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/86630 from jiunkaiy:dev/chunhsue/add_DUS_and_Pack 2a54437cf8f703c1e380125c2aa8a48d636abb70",2025-02-13T21:58:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87266
copybara-service[bot],Refactor `SpmdPartitioningVisitor::HandleReshape`. No behavior change.,"Refactor `SpmdPartitioningVisitor::HandleReshape`. No behavior change. This change recovers cl/717991433 with modification. The previous one is not a pure refactoring since it assumes that the inference `in_sharding_1 > out_sharding > in_sharding_2` will have `in_sharding_1 == in_sharding_2`. This assumption may be false. In the added test target, we reshape 24 > 6x4, and have the following inferred shardings. ``` in_sharding_1: [4] out_sharding: [2,1,2] last_tile_dim_replicate in_sharding_2: [2,2] last_tile_dim_replicate ``` This change should a pure refactoring without behavior change.",2025-02-13T21:23:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87265
copybara-service[bot],Integrate LLVM at llvm/llvm-project@5586541d220e,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 5586541d220e,2025-02-13T21:07:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87264
copybara-service[bot],PR #22588: Use cuda event and Rendezvous instead of nccl allreduce as a barrier,PR CC(Update the relative file paths in the comments of tf.data kernel files): Use cuda event and Rendezvous instead of nccl allreduce as a barrier Imported from GitHub PR https://github.com/openxla/xla/pull/22588 Copybara import of the project:  5acbea5c6f5a1ed2deaae4bdbf472b9e72c6bf5e by TJ Xu : Use cuda event and Rendezvous instead of nccl allreduce as a barrier  3d78e81091eb0b6cb1dc22f1ff27c3a8de4cff4f by TJ Xu : Improve comment for the motivation Merging this change closes CC(Update the relative file paths in the comments of tf.data kernel files) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22588 from Tixxx:tixxx/event_barrier 3d78e81091eb0b6cb1dc22f1ff27c3a8de4cff4f,2025-02-13T20:58:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87263
copybara-service[bot],Fix duplicate error in LiteRT by replacing tensorflow.lite with tflite.python.lite,Fix duplicate error in LiteRT by replacing tensorflow.lite with tflite.python.lite,2025-02-13T20:24:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87262
copybara-service[bot],PR #22588: Use cuda event and Rendezvous instead of nccl allreduce as a barrier,PR CC(Update the relative file paths in the comments of tf.data kernel files): Use cuda event and Rendezvous instead of nccl allreduce as a barrier Imported from GitHub PR https://github.com/openxla/xla/pull/22588 Copybara import of the project:  5acbea5c6f5a1ed2deaae4bdbf472b9e72c6bf5e by TJ Xu : Use cuda event and Rendezvous instead of nccl allreduce as a barrier  3d78e81091eb0b6cb1dc22f1ff27c3a8de4cff4f by TJ Xu : Improve comment for the motivation Merging this change closes CC(Update the relative file paths in the comments of tf.data kernel files) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22588 from Tixxx:tixxx/event_barrier 3d78e81091eb0b6cb1dc22f1ff27c3a8de4cff4f,2025-02-13T20:22:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87261
copybara-service[bot],"Remove `gpu_build`, `parallel_gpu_execute` now lives in `build_tools/ci`","Remove `gpu_build`, `parallel_gpu_execute` now lives in `build_tools/ci`",2025-02-13T20:13:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87260
copybara-service[bot],[MHLO->StableHLO] Allow MHLO with XLA features to be partially imported to StableHLO+CHLO,[MHLO>StableHLO] Allow MHLO with XLA features to be partially imported to StableHLO+CHLO,2025-02-13T20:10:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87259
copybara-service[bot],Remake tool for inserting FileCheck directives in HLO optimization-pass tests.,"Remake tool for inserting FileCheck directives in HLO optimizationpass tests. The tool previously required the user to perform most of the steps manually, only automating the replacement of hardcoded symbols with regex captures. It now automatically runs an optimizer on the test file, writes FileCheck directives based on the optimized HLO, replaces symbols with regex captures, and inserts the FileCheck directives above their respective test cases. The step of replacing explicit symbols with regex captures has also been improved to support capturing function names and to only add disambiguation suffixes when necessary.",2025-02-13T19:42:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87258
copybara-service[bot],Make `hlo-opt` return error status when given an invalid `--passes` argument.,Make `hloopt` return error status when given an invalid `passes` argument. `hloopt` previously logged an error in this case but did not indicate an error in its exit status. Note that the program now exits immediately upon encountering an invalid `passes` argument; it previously logged an error and continued executing. Fixing this bug also revealed that the `algebraic_simplifier.hlo` test file wasn't doing anything because the `AlgebraicSimplifier` pass wasn't registered in `hloopt`. This CL therefore also registers that pass and updates its test accordingly.,2025-02-13T19:22:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87257
copybara-service[bot],[xla:python] Remove unused _single_device_array_to_np_array on ArrayImpl.,[xla:python] Remove unused _single_device_array_to_np_array on ArrayImpl.,2025-02-13T19:18:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87256
copybara-service[bot],Factor out CheckpointIterator test from disabled test file.,Factor out CheckpointIterator test from disabled test file.,2025-02-13T18:55:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87255
copybara-service[bot],Refactor `hlo_sharding_util::ReshapeSharding` by reducing the if-else branches.,"Refactor `hlo_sharding_util::ReshapeSharding` by reducing the ifelse branches. We also highlight a TODO in this cl, which will be revisited later. No behavior change.",2025-02-13T18:38:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87254
copybara-service[bot],HloRunnerPjRt should respect static device layout if present.,HloRunnerPjRt should respect static device layout if present. `ExecuteReplicated` was overwriting the device layout even if one was present in the module config.,2025-02-13T18:25:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87253
copybara-service[bot],Add model_name as field in v1 compat graph conversion count streamz metrics.,Add model_name as field in v1 compat graph conversion count streamz metrics.,2025-02-13T18:01:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87252
copybara-service[bot],"PR #22645: Fix error in the gpu_specs README. The spec is the TargetConfig, which includes the device description.","PR CC(Problem with CUDA 10 and Tensorflow 1.11): Fix error in the gpu_specs README. The spec is the TargetConfig, which includes the device description. Imported from GitHub PR https://github.com/openxla/xla/pull/22645 Copybara import of the project:  5fefad4bdb1b947844c7c2d8ff1029f5c99aace5 by Dimitris Vardoulakis : Fix error in the specs README. The spec is the TargetConfig, which includes the device description.  39f8e9f343de45b4678a6760b9a9720d48163299 by Dimitris Vardoulakis : Update xla/tools/hlo_opt/gpu_specs/README.md Coauthoredby: Allan Renucci  Merging this change closes CC(Problem with CUDA 10 and Tensorflow 1.11) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22645 from dimvar:addreadmetogpu_specs 39f8e9f343de45b4678a6760b9a9720d48163299",2025-02-13T17:17:26Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87251,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],[hlo-translate] Accept VHLO in hlo-translate tool,[hlotranslate] Accept VHLO in hlotranslate tool,2025-02-13T16:45:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87250
copybara-service[bot],Split `BasicDeviceList` into its own BUILD target and make it visible only to IFRT implementations,"Split `BasicDeviceList` into its own BUILD target and make it visible only to IFRT implementations After this CL, IFRT users will no longer have visibility to `BasicDeviceList`. This ensures that IFRT users use `Client::MakeDeviceList()` to create a device list instead of directly calling `BasicDeviceList::Create()`.",2025-02-13T15:50:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87249
copybara-service[bot],Add TensorBufferConvertTo function.,Add TensorBufferConvertTo function.,2025-02-13T15:48:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87248
copybara-service[bot],[XLA:GPU] Add python bindings to collective perf table generator.,[XLA:GPU] Add python bindings to collective perf table generator. Also add logical defaults for search space and pass in replica groups as string instead of plumbing through IotaReplicaGroupList to python bindings.,2025-02-13T15:12:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87247
gaikwadrahul8,Fix a broken link in java.md,"Hi, Team I found a broken documentation link in this java.md file so I have updated that link to functional link. Please review and merge this change as appropriate. Thank you for your consideration.",2025-02-13T12:52:09Z,comp:lite ready to pull size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87246
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T12:27:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87245
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T12:26:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87244
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T12:25:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87243
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T12:23:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87242
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T12:22:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87241
copybara-service[bot],PR #22593: [XLA:GPU] Fix triton dot op on sm120 (RTX50xx),"PR CC([XLA/tfcompile] fix type of enum for build with MSVC): [XLA:GPU] Fix triton dot op on sm120 (RTX50xx) Imported from GitHub PR https://github.com/openxla/xla/pull/22593 Triton doesn't currently support sm120 GPUs  adding a patch to fix that, the upstream support should be available soon. Converting ""12.0"" arch into ""10.0"" is not correct, as they're not compatible  removing this. Copybara import of the project:  5b5752dc3b3ae3611ffeca0f55a87f130ff1e8bb by Sergey Kozub : [XLA:GPU] Fix triton dot op on sm120 (RTX50xx) Merging this change closes CC([XLA/tfcompile] fix type of enum for build with MSVC) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22593 from openxla:skozub/sm120_dot 5b5752dc3b3ae3611ffeca0f55a87f130ff1e8bb",2025-02-13T12:21:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87240
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T12:19:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87239
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T12:17:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87238
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T12:17:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87237
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T12:17:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87236
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T12:17:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87235
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T12:13:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87234
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T12:08:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87233
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T12:08:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87232
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T12:08:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87231
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T11:53:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87230
copybara-service[bot],Fix typo in a comment (NFC).,Fix typo in a comment (NFC).,2025-02-13T11:44:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87229
copybara-service[bot],PR #22640: [NFC] Deduplicate functions between HLO runners.,PR CC(Fix warning for format specifier): [NFC] Deduplicate functions between HLO runners. Imported from GitHub PR https://github.com/openxla/xla/pull/22640 Copybara import of the project:  e0e1de485c9120c1d52c8f10d843c13097802e07 by Ilia Sergachev : [NFC] Deduplicate functions between HLO runners.  aeff13d83e159fd8e2cb12082a2549ccdbe0b2b6 by Ilia Sergachev : [NFC] Delete unused functions.  c073163cd1ce91ddb12c7b52d760194f33de9fe2 by Ilia Sergachev : Update xla/service/hlo_runner_interface.: Allan Renucci   b043017e52b444fbfff1c49655952899f6c4fd6d by Ilia Sergachev : Update xla/service/hlo_runner_interface.h Coauthoredby: Allan Renucci   48383f2873e8614afc678c3d7842ca1815d53d40 by Ilia Sergachev : Update xla/service/hlo_runner_interface.h Coauthoredby: Allan Renucci   d5bca65f50c2e1b05ce36f1d485c107806bfbc94 by Ilia Sergachev : Rename HloProtoToModule to CreateModuleFromProto for consistency.  be158dda3fd2450e1df9d91e833a563eb5a70119 by Ilia Sergachev : Update xla/service/hlo_runner_interface.h Coauthoredby: Allan Renucci   e34efb499fa492929d9a3116a9dfbc112667e1b3 by Ilia Sergachev : Update xla/service/hlo_runner_interface.h Coauthoredby: Allan Renucci  Merging this change closes CC(Fix warning for format specifier) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22640 from openxla:cleanup_hlo_functions e34efb499fa492929d9a3116a9dfbc112667e1b3,2025-02-13T11:20:56Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/87228,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T10:53:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87227
KeskoPreeti,Tensorflow in pycharm setup issue," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.12.0  Custom code Yes  OS platform and distribution windows 11  Mobile device windows11  Python version 3.9.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? error while running the file predeiction.py                                                                                                                                                      (c) Microsoft Corporation. All rights reserved. (venv_new) C:\analytics_driven_enginemaster\analytics_driven_enginemaster>python m analysis.prediction [20250213 15:38:26,485] INFO  Starting prediction pipeline... ['Number' 'Opened' 'State' 'Symptom category' 'Short description' 'State'] Traceback (most recent call last):   File ""C:\analytics_driven_enginemaster\analytics_driven_enginemaster\venv_new\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 62, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The paging file is too small for this operation to complete. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File """", line 1, in    File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\multiprocessing\spawn.py"", line 116, in spawn_main     exitcode = _main(fd, parent_sentinel)   File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\multiprocessing\spawn.py"", line 125, in _main     prepare(preparation_data)   File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\multiprocessing\spawn.py"", line 234, in prepare     _fixup_main_from_name(data['init_main_from_name'])   File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\multiprocessing\spawn.py"", line 258, in _fixup_main_from_name     main_content = runpy.run_module(mod_name,   File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 225, in run_module     return _run_module_code(code, init_globals, run_name, mod_spec)   File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 97, in _run_module_code     _run_code(code, mod_globals, init_globals,   File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""C:\analytics_driven_enginemaster\analytics_driven_enginemaster\analysis\prediction.py"", line 3, in      import tensorflow as tf   File ""C:\analytics_driven_enginemaster\analytics_driven_enginemaster\venv_new\lib\sitepackages\tensorflow\__init__.py"", line 37, in      from tensorflow.python.tools import module_util as _module_util   File ""C:\analytics_driven_enginemaster\analytics_driven_enginemaster\venv_new\lib\sitepackages\tensorflow\python\__init__.py"", line 36, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   File ""C:\analytics_driven_enginemaster\analytics_driven_enginemaster\venv_new\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 77, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\analytics_driven_enginemaster\analytics_driven_enginemaster\venv_new\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 62, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The paging file is too small for this operation to complete. expected  to process ticket analytics output files based on input  Standalone code to reproduce the issue ```shell import datetime import logging import tensorflow as tf import numpy as np from analysis.data_helpers import add_analyser, find_keywords, add_translated_data from analysis.save_and_get_models import get_model, load_tokeniser, load_vectorizer, decode_data as decode, get_encoder from config.config import output_col, input_path, unique_col, columns_for_parsing, output_path, output_col_apps, data  from TA_semantic_approach.main import predict_semantic_cti import gc import os import torch torch.set_num_threads(1)   Limits PyTorch parallelism to avoid excessive memory usage torch.cuda.empty_cache()   Clears unused GPU memory (if using CUDA)  Reduce TensorFlow memory usage os.environ[""TF_FORCE_GPU_ALLOW_GROWTH""] = ""true"" os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""   Forces CPU processing tf.config.threading.set_intra_op_parallelism_threads(1) tf.config.threading.set_inter_op_parallelism_threads(1) def predict_output_gartner_cti_infra(df, col=[], name='', path='', flag=''):     models = get_model(flag)      tokenize = load_tokeniser()     vectorizer = load_vectorizer(flag)     df = add_analyser(df, path=path, columns_for_parsing=col)      data = tokenize.texts_to_matrix(df['Analysis'])     value = vectorizer.transform(df['Analysis'].values).toarray()     for key, model in models.items():         input = {}         for col in output_col[key]['input']:             if col != 'Analysis':                 input[col.replace('/ ', '').replace(' ', '_').replace('/', '')] = tf.keras.utils.to_categorical(                     get_encoder(list(df[col]), col.replace('/ ', '').replace(' ', '_').replace('/', '')))             else:                 input[col] = value         data = model.predict([val for x, val in input.items()])         for i, col in enumerate(output_col[key]['output']):             if len(output_col[key]['output']) > 1:                 df[col] = decode([tf.argmax(x) for x in data[i]],                                  col.replace('/ ', '').replace(' ', '_').replace('/', ''))             else:                 df[col] = decode([tf.argmax(x) for x in data],                                  col.replace('/ ', '').replace(' ', '_').replace('/', ''))     print(df)     df[['Category', 'Type', 'Item']] = df['Intent'].str.split(',', 3, expand=True)      cti_df = pd.read_excel('resources\CTI Model.xlsx', sheet_name='Sheet2')           df.fillna('', inplace=True)      cti_df.fillna('', inplace=True)           df = find_keywords(df, [i.lower() for i in list(cti_df['Type'])], mapp=list(cti_df['Category']),                         columns=['Type', 'Category'])      df = find_keywords(df, [i.lower() for i in list(cti_df['Item']) if i != ''], mapp=list(cti_df['Item_Replace']),                         columns=['Item'])     return df def predict_output_gartner_apps(df, col=[], name='', path='', flag=''):     models = get_model(flag)      tokenize = load_tokeniser()     vectorizer = load_vectorizer(flag)     df = add_translated_data(df, path=path, columns_for_parsing=col)     df = add_analyser(df, path=path, columns_for_parsing=col)      data = tokenize.texts_to_matrix(df['Analysis'])     df = df[df['Analysis'].notna()]     value = vectorizer.transform(df['Analysis'].values).toarray()     for key, model in models.items():         input = {}         for col in output_col_apps[key]['input']:             if col != 'Analysis':                 input[col.replace('/ ', '').replace(' ', '_').replace('/', '')] = tf.keras.utils.to_categorical(                     get_encoder(list(df[col]), col.replace('/ ', '').replace(' ', '_').replace('/', '')))             else:                 input[col] = value         data = model.predict([val for x, val in input.items()])         for i, col in enumerate(output_col_apps[key]['output']):             if len(output_col_apps[key]['output']) > 1:                 df[col] = decode([tf.argmax(x) for x in data[i]],                                  col.replace('/ ', '').replace(' ', '_').replace('/', ''))             else:                 df[col] = decode([tf.argmax(x) for x in data],                                  col.replace('/ ', '').replace(' ', '_').replace('/', ''))      df[['Category', 'Type','Item']] = df['Intent'].str.split(',', 3, expand=True)      cti_df = pd.read_excel('resources\CTI Model.xlsx', sheet_name='Sheet2')           df.fillna('', inplace=True)      cti_df.fillna('', inplace=True)           df = find_keywords(df, [i.lower() for i in list(cti_df['Type'])], mapp=list(cti_df['Category']),                         columns=['Type', 'Category'])      df = find_keywords(df, [i.lower() for i in list(cti_df['Item']) if i != ''], mapp=list(cti_df['Item_Replace']),                         columns=['Item'])     return df if __name__ == '__main__':     logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s  %(message)s')     logging.info(""Starting prediction pipeline..."")     gc.garbage.clear()     import pandas as pd     import os      df=pd.read_excel('C:\\Users\\sabatank\\Desktop\\test_new_data\\Training_input_20210715T03_21_12.xlsx')      df.fillna('', inplace=True)      df['Intent'] = df.apply(lambda x: '' if x['Analysis'] == '' else x['Analysis'], axis=1)      df.to_excel('C:\\Users\\sabatank\\Desktop\\test_new_data\\Training_input_20210715_12.xlsx')     for file in os.listdir(input_path):         gc.garbage.clear()         try:             if data == 'both':                 df = pd.read_excel(input_path + file)                 df = df[df[unique_col].notna()]                 df.fillna('', inplace=True)                 grouped_data = df.groupby(df.Area)                 df_apps = grouped_data.get_group(""Apps"")                 df_infra = grouped_data.get_group(""Infra"")                 df_out_infra = predict_output_gartner_cti_infra(df_infra, columns_for_parsing, file.split('.')[0],                                                                 input_path + file, 'infra')                 df_out_apps = predict_output_gartner_apps(df_apps, columns_for_parsing, file.split('.')[0],                                                           input_path + file, 'apps')                  df_out_apps_cti=predict_semantic_cti(df_out_apps,['Analysis'],file.split('.')[0], input_path+file)                  df_out_apps_cti[['Category', 'Type','Item']] = df_out_apps_cti['Intent'].str.split(',', 3, expand=True)                 final_df = pd.concat([df_out_infra, df_out_apps])             if data == 'apps':                 df = pd.read_excel((input_path + file), sheet_name='Page 1')                 df = df[['Number', 'Opened', 'State', 'Symptom category', 'Short description', 'State']]                 df = df[df[unique_col].notna()]                 df_apps = df[df[columns_for_parsing[0]].notna()]                  df_apps = df_apps.head(10)                 tickets_per_request = 1000                 final_df = pd.DataFrame()                 count = 0                  final_df=predict_output_gartner_apps(df_apps,columns_for_parsing, file.split('.')[0], input_path+file,'apps')                 for index, sub_df in df_apps.groupby(                         np.arange(len(df_apps)) // tickets_per_request):   post request with 100 records                     try:                         output = predict_output_gartner_apps(sub_df, columns_for_parsing, file.split('.')[0],                                                              input_path + file, 'apps')                         if output is None:                             continue                         nowTime = str(datetime.datetime.now().replace(microsecond=0).isoformat()).replace(':', '_')                         output = output.applymap(lambda x: x.encode('unicode_escape').                                                  decode('utf8') if isinstance(x, str) else x)                         output.to_excel(output_path + file.split('.')[0].replace(' ', '') + '.xlsx', index=False)                          output.to_excel(output_path + file.split('.')[0].replace(' ', '') + '_' + nowTime + '.xlsx',                                            index=False)                         final_df = pd.DataFrame(output)                         count = count + 5000                     except Exception as e:                         logging.error(f""Exception occurred: {e}"")                         logging.info(""Remaining files are stored in output path."")                         nowTime = str(datetime.datetime.now().replace(microsecond=0).isoformat()).replace(':', '_')                         df_success = final_df.drop(                             ['Debt Classification', 'Avoidable', 'Intent', 'Category', 'Type', 'Item', 'Translated_data',                              'Analysis'], axis=1, inplace=True, errors='ignore')                         df_remaining = pd.concat([df_apps, df_success]).drop_duplicates(keep=False)                         df_remaining.to_excel(                             output_path + 'remaining\\' + file.split('.')[0].replace(' ', '') + '_' + nowTime + '.xlsx',                             index=False)                         logging.error(""Exception encountered. Stopping process."")                         print(str(e))                     print(count)                 nowTime = str(datetime.datetime.now().replace(microsecond=0).isoformat()).replace(':', '_')                 final_df.to_excel(output_path + file.split('.')[0].replace(' ', '') + '_final' + nowTime + '.xlsx',                                   index=False)                  final_df=predict_semantic_cti(df_out_apps, 'Analysis', file.split('.')[0], input_path + file)                  final_df[['Category', 'Type','Item']] = final_df['Intent'].str.split(',', 3, expand=True)             if data == 'infra':                 df = pd.read_excel(input_path + file)                 df = df[df[unique_col].notna()]                 df_infra = df[df[columns_for_parsing[0]].notna()]                 final_df = predict_output_gartner_cti_infra(df_infra, columns_for_parsing, file.split('.')[0],                                                             input_path + file, 'infra')             nowTime = str(datetime.datetime.now().replace(microsecond=0).isoformat()).replace(':', '_')              final_df.to_excel(output_path + file.split('.')[0].replace(' ','') + '_' + nowTime + '.xlsx', index=False)             logging.info(""Prediction process completed successfully."")         except Exception as e:             print(e)             break ```  Relevant log output ```shell ```",2025-02-13T10:24:36Z,stat:awaiting response type:build/install subtype:windows TF 2.12,closed,0,3,https://github.com/tensorflow/tensorflow/issues/87226,", Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: ```  You need to install the MSVC 2019 redistributable  Your CPU does not support AVX2 instructions  Your CPU/Python is on 32 bits  There is a library that is in a different location/not installed on your system that cannot be loaded. ``` Also kindly provide the environment details and the steps followed to install the tensorflow. CC(Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.) Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Please always search for similar issues before opening duplicates,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],PR #22572: Fix build break in convolution_thunk_internal,"PR CC([nGraph] Fixed the broken unit test build): Fix build break in convolution_thunk_internal Imported from GitHub PR https://github.com/openxla/xla/pull/22572 After this change https://github.com/openxla/xla/commit/8e6b84bd729a1d079ed5035f1ba8afc9d205587e, following build error occurred: ``` In file included from xla/backends/cpu/runtime/convolution_thunk_f16.cc:16: ./xla/backends/cpu/runtime/convolution_thunk_internal.h: In lambda function: ./xla/backends/cpu/runtime/convolution_thunk_internal.h:345:71: error: no matching function for call to ‘tsl::CountDownAsyncValueRef::CountDown() const’   345         ^~~~~~~~~ ./xla/tsl/concurrency/async_value_ref.h:919:8: note:   passing ‘const tsl::CountDownAsyncValueRef*’ as ‘this’ argument discards qualifiers ``` Copybara import of the project:  69268a699eb07efb65be4e42087f75d504f750a6 by Milica Makevic : Allow modification of captured variable in nested lambda Merging this change closes CC([nGraph] Fixed the broken unit test build) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22572 from ROCm:ci_build_fix_250211 69268a699eb07efb65be4e42087f75d504f750a6",2025-02-13T09:14:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87225
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T09:14:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87224
copybara-service[bot],[XLA:GPU][NFC] Clean-up directory structure. Make it like in emitters/.,[XLA:GPU][NFC] Cleanup directory structure. Make it like in emitters/. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22572 from ROCm:ci_build_fix_250211 69268a699eb07efb65be4e42087f75d504f750a6,2025-02-13T09:04:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87223
copybara-service[bot],Support partition with index in LiteRt Core. ,"Support partition with index in LiteRt Core.  CompilerPlugin::Partition now should return List>, where Index is the vendor specified partition index(color) for each Op supported. LiteRt Core unionfind algorithm now runs on each partitions, and combines all buckets into one as result. The behavior of plugin remains unchanged if all indices are the same.",2025-02-13T08:48:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87222
copybara-service[bot],Skip custom fusions in ParallelTaskAssigner.,Skip custom fusions in ParallelTaskAssigner.,2025-02-13T08:20:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87221
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T07:43:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87220
copybara-service[bot],[Function runtime] Avoid copying reachable function definitions when graph collection is disabled.,"[Function runtime] Avoid copying reachable function definitions when graph collection is disabled. Additionally, avoid copying each function during the `UpdateTPUEmbeddingModePass` for the common case where the function does not include any TPUEmbedding layer ops.",2025-02-13T07:32:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87219
copybara-service[bot],Integrate LLVM at llvm/llvm-project@165a3d6a9b16,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 165a3d6a9b16,2025-02-13T06:42:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87218
copybara-service[bot],internal BUILD rule visibility,internal BUILD rule visibility,2025-02-13T06:28:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87217
copybara-service[bot],Add list of supported mtk socs.,Add list of supported mtk socs.,2025-02-13T06:21:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87216
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T05:44:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87215
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-13T05:21:05Z,ready to pull,open,0,0,https://github.com/tensorflow/tensorflow/issues/87214
fujunwei,Fix build errors to enable OpenCL GPU delegate in Chromium,"The Chromium CL6246337 builds OpenCL GPU delegate with Chromium, but there are some errors need to be fixed with this PR.",2025-02-13T04:36:08Z,comp:lite size:S,closed,0,1,https://github.com/tensorflow/tensorflow/issues/87213, Could you please help to review this PR? thanks.
copybara-service[bot],Unify interpreter.py.oss into interpreter.py,Unify interpreter.py.oss into interpreter.py,2025-02-13T04:05:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87212
copybara-service[bot],Install uv in CI Dockerfiles,Install uv in CI Dockerfiles uv is much faster than pip for installing Python packages.,2025-02-13T03:00:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87211
copybara-service[bot],[XLA:CPU] Remove unneeded MHLO dependencies from XLA CPU compiler,[XLA:CPU] Remove unneeded MHLO dependencies from XLA CPU compiler,2025-02-13T02:47:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87210
copybara-service[bot],[PJRT:CPU] Derive ExecutableRunOptions::run_id from ExecuteOptions::launch_id,"[PJRT:CPU] Derive ExecutableRunOptions::run_id from ExecuteOptions::launch_id This change uses `ExecuteOptions::launch_id` (> 0) as `ExecutableRunOptions::run_id`. This allows using a consistent `run_id` to be used for a set of executions dispatched using perdevice calls of `PjRtLoadedExecutable::ExecuteSharded()`, matching the capability of the sharded executions on TPU. If `launch_id` is unset (= 0), it falls back to the previous behavior of using an intenral counter that is incremented for any execution. JAX executables now uses an increasing sequence of `launch_id` for each execution. Its initial value is set to the fingerprint of the executables to preserve the existing capability of choosing lowcollision `launch_id` between different executables.",2025-02-13T01:47:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87209
copybara-service[bot],Allow user to pass model file descriptor through Dispatch API,Allow user to pass model file descriptor through Dispatch API,2025-02-13T01:44:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87208
copybara-service[bot],[XLA:GPU] Remove unneeded MHLO dependencies from `gpu_compiler`.,[XLA:GPU] Remove unneeded MHLO dependencies from `gpu_compiler`.,2025-02-13T01:42:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87207
copybara-service[bot],"In cudnn_fused_conv_rewriter.h, allow clamp to omitted when converting f32 to s8.","In cudnn_fused_conv_rewriter.h, allow clamp to omitted when converting f32 to s8. As an implementation detail, XLA already clamps when converting float to int, so it's ok to patternmatch a fused_conv_outputting_f32>convert_to_s8 into a fused_conv_outputting_s8, even without a clamp in between the fused conv and convert. Even so, I would still recommend users have a clamp in their code, since the implicit clamping behavior is unspecified.",2025-02-13T01:26:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87206
copybara-service[bot],fix one precondition of quantization pass,fix one precondition of quantization pass,2025-02-13T01:18:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87205
copybara-service[bot],Delete Kokoro GPU builds now that GitHub Actions GPU builds block,Delete Kokoro GPU builds now that GitHub Actions GPU builds block,2025-02-13T01:01:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87204
copybara-service[bot],"Make xla_test, etc default to linkstatic to catch duplicate symbols at build time.","Make xla_test, etc default to linkstatic to catch duplicate symbols at build time.",2025-02-13T00:38:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87203
copybara-service[bot],[XLA] Googly changes,"[XLA] Googly changes Don't mixup host {in,out}feed with other types of {in,out}feed.",2025-02-13T00:31:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87202
copybara-service[bot],Make tsl_cc_test default to linkstatic to catch duplicate symbols at build time.,Make tsl_cc_test default to linkstatic to catch duplicate symbols at build time.,2025-02-13T00:26:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87201
copybara-service[bot],Enable `bazel build --nobuild` to prevent network flakes for TensorFlow builds,Enable `bazel build nobuild` to prevent network flakes for TensorFlow builds Removes the usage of their `py_cpp_test_filters` config which is incompatible with `bazel build nobuild` and instead replicate the effect of the config by specifying bazel options explicitly.,2025-02-13T00:19:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87200
copybara-service[bot],Add accuracy field to unary ops,"Add accuracy field to unary ops   * Cbrt   * Cos   * Exp, Exp2   * Expm1   * Log   * Logistic   * Log1p   * Rsqrt   * Sin   * Sqrt   * Tan   * Tanh which allows users to select implementation that will satisfy the requested accuracy.",2025-02-13T00:19:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87199
copybara-service[bot],Use device duck typing for empty device lists,"Use device duck typing for empty device lists We are soon going to require all IFRT users to use `Client::MakeDeviceList()` to create a device list instead of calling `BasicDeviceList::Create()`. `PyDeviceList(nb::tuple py_device_assignment)` with an empty device assignment cannot infer the IFRT client to use, so it seems the simplest to fall back to device duck typing like we already do for device lists with devices from multiple clients.",2025-02-13T00:18:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87198
copybara-service[bot],Flag to exclude tensorflow.lite from tf_python_api_gen_v2 to fix duplicate registration under LiteRT repo,Flag to exclude tensorflow.lite from tf_python_api_gen_v2 to fix duplicate registration under LiteRT repo,2025-02-13T00:02:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87197
copybara-service[bot],Split `CompileOnlyIfRtClient` into its own directory,Split `CompileOnlyIfRtClient` into its own directory,2025-02-12T23:29:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87196
rtg0795,"Update setup.py, requirements.in and generate lock files",,2025-02-12T23:12:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87195
copybara-service[bot],[XLA] Clean up the implementation for broadcast sinking past elementwise ops and add a test.,[XLA] Clean up the implementation for broadcast sinking past elementwise ops and add a test. This is a pure refactoring  no functional changes.,2025-02-12T23:09:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87194
copybara-service[bot],[XLA] Add debug option for detecting cycles in fixed-point loops.,"[XLA] Add debug option for detecting cycles in fixedpoint loops. Due to the way the ""changed"" signal is reported by passes within a fixedpoint loop today, there are various scenarios in which a fixedpoint loop that is ""converged"" may continue to run forever: *  A composite pipeline is being run to fixedpoint, and one pass exactly undoes the effect of another. *  An individual pass falsely reports that it changed a module (perhaps because it undoes its own change). *  The fixedpoint loop sees the module go through a cycle of states. While this check is too expensive to enable by default, it presents as a useful debug option. If we have reason to suspect one of the above scenarios is occurring, this option will allow us to identify the passes involved and address the root cause on an individual basis.",2025-02-12T23:04:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87193
copybara-service[bot],Fix length of separators,Fix length of separators,2025-02-12T23:01:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87192
copybara-service[bot],Fully pipeline recv and recv-done ops and do not pipeline send ops,"Fully pipeline recv and recvdone ops and do not pipeline send ops This allows start receiving data in an earlier loop iteration while the data to be sent may not be availavle yet. In the context of pipeline parallelism, this enables overlap of the stages' compute and communication.",2025-02-12T22:57:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87191
copybara-service[bot],Add more vlogs to p2p pipeliner to aid debugging,Add more vlogs to p2p pipeliner to aid debugging,2025-02-12T22:53:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87190
copybara-service[bot],Move TensorFlow CPU/GPU builds to presubmit,Move TensorFlow CPU/GPU builds to presubmit,2025-02-12T22:18:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87189
copybara-service[bot],Plumb layout through the creation of PjRtArrays.,"Plumb layout through the creation of PjRtArrays. This is in preparation to support arrays with no local shards, so that layout may not be accessible from a buffer.",2025-02-12T21:03:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87188
copybara-service[bot],[NCCL] Fix ras build issue in OSS.,[NCCL] Fix ras build issue in OSS.,2025-02-12T20:58:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87187
copybara-service[bot],Add stale check for debugging segfault.,Add stale check for debugging segfault.,2025-02-12T20:37:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87186
copybara-service[bot],Move ARM CI back to `c4a`,Move ARM CI back to `c4a` Moving to the old machine type didn't fix the failures unfortunately... So move back for now to get the speedup from the newer machine type.,2025-02-12T20:26:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87185
copybara-service[bot],[IFRT] Add log message to CHECK in compile_atom_program_pass to aid with debugging,[IFRT] Add log message to CHECK in compile_atom_program_pass to aid with debugging,2025-02-12T20:08:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87184
copybara-service[bot],Make simpler to reason about expansion logic from Iota to legacy replica groups.,Make simpler to reason about expansion logic from Iota to legacy replica groups.,2025-02-12T19:50:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87183
copybara-service[bot],Do not add control dependency from send to recv-done in decomposed collective-permute,Do not add control dependency from send to recvdone in decomposed collectivepermute This control dependency is not needed in GPU.,2025-02-12T19:47:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87182
copybara-service[bot],Merge pjrt_stream_executor_client.h and gpu/se_gpu_pjrt_client.h into,Merge pjrt_stream_executor_client.h and gpu/se_gpu_pjrt_client.h into one target as there are no direct users of pjrt_stream_executor_client.h.,2025-02-12T19:40:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87181
copybara-service[bot],Ensure InterpreterFactoryImpl's constructor is kept,"Ensure InterpreterFactoryImpl's constructor is kept Add  annotation to the constructor, ensuring it's kept even if `keep class X` rule semantics change as it relates to keeping the default (empty) constructor.",2025-02-12T19:36:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87180
copybara-service[bot],Add AllReduceInfo to step data for TPUs.,Add AllReduceInfo to step data for TPUs.,2025-02-12T19:32:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87179
copybara-service[bot],Integrate LLVM at llvm/llvm-project@165a3d6a9b16,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 165a3d6a9b16,2025-02-12T19:29:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87178
copybara-service[bot],Internal visibility changes,Internal visibility changes,2025-02-12T19:17:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87177
copybara-service[bot],"Disable the verification step in the `FlatbufferWrapper::CreateFromBuffer` method when model is > 2GiB, since verification triggers >2GB error when loading large compiled models.","Disable the verification step in the `FlatbufferWrapper::CreateFromBuffer` method when model is > 2GiB, since verification triggers >2GB error when loading large compiled models.",2025-02-12T19:16:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87176
copybara-service[bot],"Add subgraph_input_names(), subgraph_output_names() to the SignatureRunner","Add subgraph_input_names(), subgraph_output_names() to the SignatureRunner In LiteRT, it assumes that the order of names are aligned with Subgraph. But the existing input_names(), output_names() API doesn't follow it, also there are customers who rely on the legacy order. This cl creates these new methods which returns the names which reflects the underlying Subgraph I/O order.",2025-02-12T19:06:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87175
copybara-service[bot],Use IsOkAndHolds in all-reduce combiner test,Use IsOkAndHolds in allreduce combiner test,2025-02-12T18:48:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87174
copybara-service[bot],Documents when to use xla_test/xla_cc_test/tsl_cc_test.,Documents when to use xla_test/xla_cc_test/tsl_cc_test.,2025-02-12T18:32:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87173
raphael10-collab,"CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""pthreadpool"" that is not in any export set."," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.20.0  Custom code No  OS platform and distribution Linux Ubuntu 24.04  Mobile device _No response_  Python version 3.12.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following the indications found here: https://ai.google.dev/edge/litert/build/cmakebuild_installable_package I'm trying to build tensorflowlite as installable package:     (base) raphy:~/Grasp/tflite_build$ cmake ../../tensorflow_src/tensorflow/lite/ \     > DTFLITE_ENABLE_INSTALL=ON \     > DCMAKE_FIND_PACKAGE_PREFER_CONFIG=ON \     > DSYSTEM_FARMHASH=ON \     > DSYSTEM_PTHREADPOOL=ON \     > DEigen3_DIR=/home/raphy/Grasp/eigen/share/eigen3/cmake \     > DFlatBuffers_DIR=/home/raphy/Grasp/flatbuffers/lib/cmake/flatbuffers \     > Dcpuinfo_DIR=/home/raphy/Grasp/cpuinfo/share/cpuinfo \     > Druy_DIR=/home/raphy/Grasp/ruy/lib/cmake/ruy \     > DNEON_2_SSE_DIR=/home/raphy/Grasp/NEON_2_SSE/lib/cmake/NEON_2_SSE \     > Dabsl_DIR=/home/raphy/Grasp/abseilcpp/lib/cmake/absl \     > Dgemmlowp_DIR=/usr/lib/x86_64linuxgnu/cmake/gemmlowp \     > Wnodev      Setting build type to Release, for debug builds use'DCMAKE_BUILD_TYPE=Debug'.      The C compiler identification is GNU 13.3.0      The CXX compiler identification is GNU 14.2.0      Detecting C compiler ABI info      Detecting C compiler ABI info  done      Check for working C compiler: /usr/bin/: /usr/bin/c++  skipped      Detecting CXX compile features      Detecting CXX compile features  done      Performing Test CMAKE_HAVE_LIBC_PTHREAD      Performing Test CMAKE_HAVE_LIBC_PTHREAD  Success      Found Threads: TRUE      Found farmhash: /usr/lib/x86_64linuxgnu/libfarmhash.so      Downloading FP16 to /home/raphy/Grasp/tflite_build/FP16source (define FP16_SOURCE_DIR to avoid it)     CMake Deprecation Warning at CMakeLists.txt:16 (CMAKE_MINIMUM_REQUIRED):       Compatibility with CMake  value.  Or, use the ... syntax       to tell CMake that the project requires at least  but has been updated       to work with policies introduced by  or earlier.      Configuring done (0.0s)      Generating done (0.0s)      Build files have been written to: /home/raphy/Grasp/tflite_build/FP16download     [ 11%] Creating directories for 'fp16'     [ 22%] Performing download step (download, verify and extract) for 'fp16'      Downloading...        dst='/home/raphy/Grasp/tflite_build/FP16download/fp16prefix/src/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'        timeout='none'        inactivity timeout='none'      Using src='https://github.com/Maratyszcza/FP16/archive/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'      [download 7% complete]      [download 14% complete]      [download 32% complete]      [download 44% complete]      [download 62% complete]      [download 81% complete]      [download 99% complete]      [download 100% complete]      verifying file...            file='/home/raphy/Grasp/tflite_build/FP16download/fp16prefix/src/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'      Downloading... done      extracting...          src='/home/raphy/Grasp/tflite_build/FP16download/fp16prefix/src/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'          dst='/home/raphy/Grasp/tflite_build/FP16source'      extracting... [tar xfz]      extracting... [analysis]      extracting... [rename]      extracting... [clean up]      extracting... done     [ 33%] No update step for 'fp16'     [ 44%] No patch step for 'fp16'     [ 55%] No configure step for 'fp16'     [ 66%] No build step for 'fp16'     [ 77%] No install step for 'fp16'     [ 88%] No test step for 'fp16'     [100%] Completed 'fp16'     [100%] Built target fp16      The ASM compiler identification is GNU      Found assembler: /usr/bin/: /usr/bin/ccache      Building for XNNPACK_TARGET_PROCESSOR: x86_64      Downloading cpuinfo to /home/raphy/Grasp/tflite_build/cpuinfosource (define CPUINFO_SOURCE_DIR to avoid it)     CMake Deprecation Warning at CMakeLists.txt:9 (CMAKE_MINIMUM_REQUIRED):       Compatibility with CMake  value.  Or, use the ... syntax       to tell CMake that the project requires at least  but has been updated       to work with policies introduced by  or earlier.      Configuring done (0.0s)      Generating done (0.0s)      Build files have been written to: /home/raphy/Grasp/tflite_build/cpuinfodownload     [ 11%] Creating directories for 'cpuinfo'     [ 22%] Performing download step (download, verify and extract) for 'cpuinfo'      Downloading...        dst='/home/raphy/Grasp/tflite_build/cpuinfodownload/cpuinfoprefix/src/8a1772a0c5c447df2d18edf33ec4603a8c9c04a6.zip'        timeout='none'        inactivity timeout='none'      Using src='https://github.com/pytorch/cpuinfo/archive/8a1772a0c5c447df2d18edf33ec4603a8c9c04a6.zip'      [download 0% complete]      [download 1% complete]      [download 2% complete]      [download 3% complete]      [download 4% complete]      [download 5% complete]      [download 6% complete]      [download 7% complete]      [download 8% complete]      [download 9% complete]      [download 10% complete]      [download 11% complete]      [download 12% complete]      [download 13% complete]      [download 14% complete]      [download 15% complete]      [download 16% complete]      [download 17% complete]      [download 18% complete]      [download 19% complete]      [download 20% complete]      [download 21% complete]      [download 22% complete]      [download 23% complete]      [download 24% complete]      [download 25% complete]      [download 26% complete]      [download 27% complete]      [download 28% complete]      [download 29% complete]      [download 30% complete]      [download 31% complete]      [download 32% complete]      [download 33% complete]      [download 34% complete]      [download 35% complete]      [download 36% complete]      [download 37% complete]      [download 38% complete]      [download 39% complete]      [download 40% complete]      [download 41% complete]      [download 42% complete]      [download 43% complete]      [download 44% complete]      [download 45% complete]      [download 46% complete]      [download 47% complete]      [download 48% complete]      [download 49% complete]      [download 50% complete]      [download 51% complete]      [download 52% complete]      [download 53% complete]      [download 54% complete]      [download 55% complete]      [download 56% complete]      [download 57% complete]      [download 58% complete]      [download 59% complete]      [download 60% complete]      [download 61% complete]      [download 62% complete]      [download 63% complete]      [download 64% complete]      [download 65% complete]      [download 66% complete]      [download 67% complete]      [download 68% complete]      [download 69% complete]      [download 70% complete]      [download 71% complete]      [download 72% complete]      [download 73% complete]      [download 74% complete]      [download 75% complete]      [download 76% complete]      [download 77% complete]      [download 78% complete]      [download 79% complete]      [download 80% complete]      [download 81% complete]      [download 82% complete]      [download 83% complete]      [download 84% complete]      [download 85% complete]      [download 86% complete]      [download 87% complete]      [download 88% complete]      [download 89% complete]      [download 90% complete]      [download 91% complete]      [download 92% complete]      [download 93% complete]      [download 94% complete]      [download 95% complete]      [download 96% complete]      [download 97% complete]      [download 98% complete]      [download 99% complete]      [download 100% complete]      verifying file...            file='/home/raphy/Grasp/tflite_build/cpuinfodownload/cpuinfoprefix/src/8a1772a0c5c447df2d18edf33ec4603a8c9c04a6.zip'      Downloading... done      extracting...          src='/home/raphy/Grasp/tflite_build/cpuinfodownload/cpuinfoprefix/src/8a1772a0c5c447df2d18edf33ec4603a8c9c04a6.zip'          dst='/home/raphy/Grasp/tflite_build/cpuinfosource'      extracting... [tar xfz]      extracting... [analysis]      extracting... [rename]      extracting... [clean up]      extracting... done     [ 33%] No update step for 'cpuinfo'     [ 44%] No patch step for 'cpuinfo'     [ 55%] No configure step for 'cpuinfo'     [ 66%] No build step for 'cpuinfo'     [ 77%] No install step for 'cpuinfo'     [ 88%] No test step for 'cpuinfo'     [100%] Completed 'cpuinfo'     [100%] Built target cpuinfo      Downloading FXdiv to /home/raphy/Grasp/tflite_build/FXdivsource (define FXDIV_SOURCE_DIR to avoid it)     CMake Deprecation Warning at CMakeLists.txt:9 (CMAKE_MINIMUM_REQUIRED):       Compatibility with CMake  value.  Or, use the ... syntax       to tell CMake that the project requires at least  but has been updated       to work with policies introduced by  or earlier.      Configuring done (0.0s)      Generating done (0.0s)      Build files have been written to: /home/raphy/Grasp/tflite_build/FXdivdownload     [ 11%] Creating directories for 'fxdiv'     [ 22%] Performing download step (download, verify and extract) for 'fxdiv'      Downloading...        dst='/home/raphy/Grasp/tflite_build/FXdivdownload/fxdivprefix/src/b408327ac2a15ec3e43352421954f5b1967701d1.zip'        timeout='none'        inactivity timeout='none'      Using src='https://github.com/Maratyszcza/FXdiv/archive/b408327ac2a15ec3e43352421954f5b1967701d1.zip'      [download 78% complete]      [download 100% complete]      verifying file...            file='/home/raphy/Grasp/tflite_build/FXdivdownload/fxdivprefix/src/b408327ac2a15ec3e43352421954f5b1967701d1.zip'      Downloading... done      extracting...          src='/home/raphy/Grasp/tflite_build/FXdivdownload/fxdivprefix/src/b408327ac2a15ec3e43352421954f5b1967701d1.zip'          dst='/home/raphy/Grasp/tflite_build/FXdivsource'      extracting... [tar xfz]      extracting... [analysis]      extracting... [rename]      extracting... [clean up]      extracting... done     [ 33%] No update step for 'fxdiv'     [ 44%] No patch step for 'fxdiv'     [ 55%] No configure step for 'fxdiv'     [ 66%] No build step for 'fxdiv'     [ 77%] No install step for 'fxdiv'     [ 88%] No test step for 'fxdiv'     [100%] Completed 'fxdiv'     [100%] Built target fxdiv      Downloading pthreadpool to /home/raphy/Grasp/tflite_build/pthreadpoolsource (define PTHREADPOOL_SOURCE_DIR to avoid it)     CMake Deprecation Warning at CMakeLists.txt:9 (CMAKE_MINIMUM_REQUIRED):       Compatibility with CMake  value.  Or, use the ... syntax       to tell CMake that the project requires at least  but has been updated       to work with policies introduced by  or earlier.      Configuring done (0.0s)      Generating done (0.0s)      Build files have been written to: /home/raphy/Grasp/tflite_build/pthreadpooldownload     [ 11%] Creating directories for 'pthreadpool'     [ 22%] Performing download step (download, verify and extract) for 'pthreadpool'      Downloading...        dst='/home/raphy/Grasp/tflite_build/pthreadpooldownload/pthreadpoolprefix/src/4e80ca24521aa0fb3a746f9ea9c3eaa20e9afbb0.zip'        timeout='none'        inactivity timeout='none'      Using src='https://github.com/google/pthreadpool/archive/4e80ca24521aa0fb3a746f9ea9c3eaa20e9afbb0.zip'      verifying file...            file='/home/raphy/Grasp/tflite_build/pthreadpooldownload/pthreadpoolprefix/src/4e80ca24521aa0fb3a746f9ea9c3eaa20e9afbb0.zip'      Downloading... done      extracting...          src='/home/raphy/Grasp/tflite_build/pthreadpooldownload/pthreadpoolprefix/src/4e80ca24521aa0fb3a746f9ea9c3eaa20e9afbb0.zip'          dst='/home/raphy/Grasp/tflite_build/pthreadpoolsource'      extracting... [tar xfz]      extracting... [analysis]      extracting... [rename]      extracting... [clean up]      extracting... done     [ 33%] No update step for 'pthreadpool'     [ 44%] No patch step for 'pthreadpool'     [ 55%] No configure step for 'pthreadpool'     [ 66%] No build step for 'pthreadpool'     [ 77%] No install step for 'pthreadpool'     [ 88%] No test step for 'pthreadpool'     [100%] Completed 'pthreadpool'     [100%] Built target pthreadpool      Found Python: /home/raphy/miniconda3/bin/python3.12 (found version ""3.12.8"") found components: Interpreter      Generating microkernels.cmake     No microkernel found in src/reference/unaryelementwise./reference/binaryelementwise./reference/packing.: /usr/bin/ccache           3.21.9.0      Performing Test protobuf_HAVE_LD_VERSION_SCRIPT      Performing Test protobuf_HAVE_LD_VERSION_SCRIPT  Success      Performing Test protobuf_HAVE_BUILTIN_ATOMICS      Performing Test protobuf_HAVE_BUILTIN_ATOMICS  Success      Configuring done (45.5s)     CMake Error: install(EXPORT ""tensorflowliteTargets"" ...) includes target ""tensorflowlite"" which requires target ""pthreadpool"" that is not in any export set.     CMake Error: install(EXPORT ""tensorflowliteTargets"" ...) includes target ""tensorflowlite"" which requires target ""xnnpackdelegate"" that is not in any export set.     CMake Error: install(EXPORT ""tensorflowliteTargets"" ...) includes target ""tensorflowlite"" which requires target ""XNNPACK"" that is not in any export set.      Generating done (0.7s)     CMake Generate step failed.  Build files cannot be regenerated correctly.  Standalone code to reproduce the issue ```shell (base) raphy:~/Grasp/tflite_build$ cmake ../../tensorflow_src/tensorflow/lite/ \     > DTFLITE_ENABLE_INSTALL=ON \     > DCMAKE_FIND_PACKAGE_PREFER_CONFIG=ON \     > DSYSTEM_FARMHASH=ON \     > DSYSTEM_PTHREADPOOL=ON \     > DEigen3_DIR=/home/raphy/Grasp/eigen/share/eigen3/cmake \     > DFlatBuffers_DIR=/home/raphy/Grasp/flatbuffers/lib/cmake/flatbuffers \     > Dcpuinfo_DIR=/home/raphy/Grasp/cpuinfo/share/cpuinfo \     > Druy_DIR=/home/raphy/Grasp/ruy/lib/cmake/ruy \     > DNEON_2_SSE_DIR=/home/raphy/Grasp/NEON_2_SSE/lib/cmake/NEON_2_SSE \     > Dabsl_DIR=/home/raphy/Grasp/abseilcpp/lib/cmake/absl \     > Dgemmlowp_DIR=/usr/lib/x86_64linuxgnu/cmake/gemmlowp \     > Wnodev  The C compiler identification is GNU 13.3.0  The CXX compiler identification is GNU 14.2.0  Detecting C compiler ABI info  Detecting C compiler ABI info  done  Check for working C compiler: /usr/bin/: /usr/bin/c++  skipped  Detecting CXX compile features  Detecting CXX compile features  done  Performing Test CMAKE_HAVE_LIBC_PTHREAD  Performing Test CMAKE_HAVE_LIBC_PTHREAD  Success  Found Threads: TRUE  Found farmhash: /usr/lib/x86_64linuxgnu/libfarmhash.so ```  Relevant log output ```shell ```",2025-02-12T18:05:05Z,stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux,closed,0,7,https://github.com/tensorflow/tensorflow/issues/87172,"Hi, collab I apologize for the delayed response, I see a similar issue please refer this workaround mentioned in this comment and see is it resolving your issue or not ? If issue still persists please let us know with updated error log for further investigation from our end.  Thank you for your cooperation and patience.","Hi   I did everything from scratch      (base) raphy:~$ mkdir NEW     (base) raphy:~$ cd NEW     (base) raphy:~/NEW$ git clone recursesubmodules https://github.com/tensorflow/tensorflow.git     Cloning into 'tensorflow'...     remote: Enumerating objects: 1977738, done.     remote: Counting objects: 100% (1682/1682), done.     remote: Compressing objects: 100% (758/758), done.     remote: Total 1977738 (delta 1298), reused 936 (delta 924), packreused 1976056 (from 3)     Receiving objects: 100% (1977738/1977738), 1.08 GiB | 31.86 MiB/s, done.     Resolving deltas: 100% (1627307/1627307), done.     Updating files: 100% (34863/34863), done.     (base) raphy:~/NEW$ mkdir tflite_build According to the comment https://github.com/tensorflow/tensorflow/issues/57658issuecomment1534245153 I should modify the XNNPACK's CMakeLists.txt and the ptreadpool's CMakeLists.txt files :       (base) raphy:~/NEW/tensorflow$ find name  ""CMakeLists.txt""     ./tensorflow/lite/kernels/CMakeLists.txt     ./tensorflow/lite/CMakeLists.txt     ./tensorflow/lite/examples/minimal/CMakeLists.txt     ./tensorflow/lite/examples/label_image/CMakeLists.txt     ./tensorflow/lite/profiling/proto/CMakeLists.txt     ./tensorflow/lite/tools/benchmark/CMakeLists.txt     ./tensorflow/lite/tools/cmake/modules/ml_dtypes/CMakeLists.txt     ./tensorflow/lite/tools/cmake/modules/farmhash/CMakeLists.txt     ./tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt     ./tensorflow/lite/tools/cmake/modules/xnnpack/CMakeLists.txt     ./tensorflow/lite/tools/cmake/native_tools/flatbuffers/CMakeLists.txt     ./tensorflow/lite/c/CMakeLists.txt     ./tensorflow/tools/pip_package/xla_build/CMakeLists.txt     ./tensorflow/tools/pip_package/xla_build/pip_test/CMakeLists.txt     ./tensorflow/core/example/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/cmake/modules/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/utils/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/IR/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/analysis/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/utils/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/tools/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/tools/mlirhloopt/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/transforms/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/deallocation/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/deallocation/utils/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/deallocation/transforms/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/stablehlo_ext/IR/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/stablehlo_ext/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/bindings/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/bindings/c/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/bindings/python/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/tests/CMakeLists.txt     ./third_party/xla/xla/mlir_hlo/tests/python/CMakeLists.txt But the XNNPACK's CMakeLists.txt file contains just these lines :      (base) raphy:~/NEW/tensorflow$ cat ./tensorflow/lite/tools/cmake/modules/xnnpack/CMakeLists.txt           Copyright 2022 The TensorFlow Authors. All Rights Reserved.           Licensed under the Apache License, Version 2.0 (the ""License"");      you may not use this file except in compliance with the License.      You may obtain a copy of the License at                https://www.apache.org/licenses/LICENSE2.0           Unless required by applicable law or agreed to in writing, software      distributed under the License is distributed on an ""AS IS"" BASIS,      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.      See the License for the specific language governing permissions and      limitations under the License.      The ""mcpu="" switch might be introduced externaly into CMake: either in _FLAGS or      as part of CC, CXX, ASM environmental variables (to be stored in CMAKE__COMPILER_ARG1).      This switch is not compatible with XNNPACK build mechanism and causes the XNNPACK compilation      break due to ""unsupported instructions"". This switch needs to be removed for XNNPACK      In order to isolate the changes only for XNNPACK and its depencencies, a subfolder is      introduced.     foreach(FLAG IN ITEMS CMAKE_ASM_FLAGS CMAKE_ASM_COMPILER_ARG1 CMAKE_C_FLAGS CMAKE_C_COMPILER_ARG1 CMAKE_CXX_FLAGS CMAKE_CXX_COMPILER_ARG1)       if(${FLAG})         string(REGEX REPLACE ""mcpu=[azAZ09_.^$*+?]*"" """" _tmp ${${FLAG}})         set(${FLAG} ${_tmp})       endif()     endforeach()     add_subdirectory(       ""${xnnpack_SOURCE_DIR}""       ""${xnnpack_BINARY_DIR}"" and not the lines that, according to the comment, should be modified And, as you can see from the list of CMakeLists.txt files, the CMakeLists.txt file for pthreadpool is not present What am I missing and/or doing wrong? How to make it work?",after i finished cross compiling it seems like there is a symbol missing ``` Error relocating libtensorflowlite.so: TfLiteXNNPackDelegateOptionsDefault: symbol not found Error relocating libtensorflowlite.so: TfLiteXNNPackDelegateCreateWithThreadpool: symbol not found Error relocating libtensorflowlite.so: TfLiteXNNPackDelegateDelete: symbol not found ```,"Hi, collab  Apologize for delay in my response, if possible could you please try by adding this flag `DTFLITE_ENABLE_XNNPACK:BOOL=OFF` in your cmake command and see is it working or not as expected ? ``` cmake ../../tensorflow_src/tensorflow/lite/ \ > DTFLITE_ENABLE_XNNPACK:BOOL=OFF > DTFLITE_ENABLE_INSTALL=ON \ > DCMAKE_FIND_PACKAGE_PREFER_CONFIG=ON \ > DSYSTEM_FARMHASH=ON \ > DSYSTEM_PTHREADPOOL=ON \ > DEigen3_DIR=/home/raphy/Grasp/eigen/share/eigen3/cmake \ > DFlatBuffers_DIR=/home/raphy/Grasp/flatbuffers/lib/cmake/flatbuffers \ > Dcpuinfo_DIR=/home/raphy/Grasp/cpuinfo/share/cpuinfo \ > Druy_DIR=/home/raphy/Grasp/ruy/lib/cmake/ruy \ > DNEON_2_SSE_DIR=/home/raphy/Grasp/NEON_2_SSE/lib/cmake/NEON_2_SSE \ > Dabsl_DIR=/home/raphy/Grasp/abseilcpp/lib/cmake/absl \ > Dgemmlowp_DIR=/usr/lib/x86_64linuxgnu/cmake/gemmlowp \ > Wnodev ``` Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Reverts 0f1b37b73df27b33e6256ab38b96f948eec084cc,Reverts 0f1b37b73df27b33e6256ab38b96f948eec084cc,2025-02-12T17:53:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87171
copybara-service[bot],Add a REPO.bazel file to indicate this is a Bazel repo,Add a REPO.bazel file to indicate this is a Bazel repo This will keep Bazel happy when tsl is used as a vendored repo,2025-02-12T17:10:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87170
copybara-service[bot],Modify the hash value for CseKey to also hash on result accuracy. The cse pass should also group instructions with same result accuracy. We should also check for result accuracy fields when checking if two hlo instructions are the same.,Modify the hash value for CseKey to also hash on result accuracy. The cse pass should also group instructions with same result accuracy. We should also check for result accuracy fields when checking if two hlo instructions are the same.,2025-02-12T17:09:29Z,kokoro:force-run ready to pull,closed,0,2,https://github.com/tensorflow/tensorflow/issues/87169,❤️❤️❤️,> Modify the hash value for CseKey to also hash on result accuracy. The cse pass should also group instructions with same result accuracy. We should also check for result accuracy fields when checking if two hlo instructions are the same. >    [x] ❤️❤️[]()[]()_ _
copybara-service[bot],Add bazel_features as a dependency,Add bazel_features as a dependency It's used by modern Bazel rules that are required by Bazel 7.x,2025-02-12T16:49:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87168
copybara-service[bot],Do not combine all-reduces with control dependencies.,"Do not combine allreduces with control dependencies. These control dependencies would otherwise be transferred to some gettupleelement op, which triggers an assertion when replacing the op.",2025-02-12T16:35:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87167
copybara-service[bot],Add macOS ARM64 Kokoro config.,Add macOS ARM64 Kokoro config.,2025-02-12T16:14:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87166
copybara-service[bot],Reenable pipeline parallelism test after fix,Reenable pipeline parallelism test after fix,2025-02-12T15:56:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87165
copybara-service[bot],Only pipeline send/recv if this does not violate control dependencies,Only pipeline send/recv if this does not violate control dependencies Only pipeline send/recv if they have no control predecessors that are not pipelined with them. This resolves a deadlock we observed in the pipeline parallelism tests.,2025-02-12T15:55:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87164
Cloudberrydotdev,Install Tensorflow with pip code error," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf2.18  Custom code No  OS platform and distribution Ubuntu 22  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? on page https://www.tensorflow.org/install/pip Section Create a symbolic link to ptxas: Code: ln sf $(find $(dirname $(dirname $(python c ""import nvidia.cuda_nvcc;          print(nvidia.cuda_nvcc.__file__)""))/*/bin/) name ptxas print quit) $VIRTUAL_ENV/bin/ptxas Creates error ln sf $(find $(dirname $(dirname $(python c ""import nvidia.cuda_nvcc;          print(nvidia.cuda_nvcc.__file__)""))/*/bin/) name ptxas print quit) $VIRTUAL_ENV/bin/ptxas bash: command substitution: line 27: unexpected EOF while looking for matching `""' bash: command substitution: line 28: syntax error: unexpected end of file bash: unexpected EOF while looking for matching `) Cause  There is a return after ""import nvidia.cuda_nvcc;      Code should be ln sf $(find $(dirname $(dirname $(python c ""import nvidia.cuda_nvcc; print(nvidia.cuda_nvcc.__file__)""))/*/bin/) name ptxas print quit) $VIRTUAL_ENV/bin/ptxas  Standalone code to reproduce the issue ```shell ln sf $(find $(dirname $(dirname $(python c ""import nvidia.cuda_nvcc;          print(nvidia.cuda_nvcc.__file__)""))/*/bin/) name ptxas print quit) $VIRTUAL_ENV/bin/ptxas ```  Relevant log output ```shell ```",2025-02-12T15:41:07Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/87163,", Could you please provide the steps you followed to install the tensorflow and also I suspect that you are trying to import the nvidiacuda which is not related to tensorflow. https://github.com/tensorflow/tensorflow/issues/43236 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],This is an automatic update to a device compatibility allowlist.,This is an automatic update to a device compatibility allowlist.,2025-02-12T15:24:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87162
copybara-service[bot],Support creating HloModule from proto with compilation environments.,Support creating HloModule from proto with compilation environments. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22757 from Tixxx:tixxx/fix_22588 906cec63655f93e2fcf23b1530d721b499cc8750,2025-02-12T15:10:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87161
copybara-service[bot],[XLA:GPU][TRITON:XLA] Add ops and types to support TMA.,"[XLA:GPU][TRITON:XLA] Add ops and types to support TMA. Tiled_tensor type and 3 ops: tile, insert, and extract to Triton_XLA dialect. These are going to be used to form common abstractions that would eventually lower to normal loads/stores or TMA variants.",2025-02-12T15:08:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87160
copybara-service[bot],Upgrade to Bazel 7.4.1,"Upgrade to Bazel 7.4.1  Disabled Bzlmod for now before we start the migration  Disabled  `modify_execution_info` due to https://github.com/bazelbuild/bazel/pull/16262  Explicitly added `Wl,undefined,dynamic_lookup` as linkopt on macOS after https://github.com/bazelbuild/bazel/pull/16414  Set `force_no_whole_archive` for host features  Addressed Windows linking issue by adding `//:__subpackages__` in `exports_filter` of `//tensorflow/python:pywrap_tensorflow_internal`   Removed `license` attribute on cc_shared_library  Fixed license checks",2025-02-12T15:02:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87159
copybara-service[bot],PR #22541: [ROCm] Cleanup atomics support,"PR CC(Java process crashes during model loading): [ROCm] Cleanup atomics support Imported from GitHub PR https://github.com/openxla/xla/pull/22541 Weaken the ordering barriers to match what atomicAdd does on rocm. Emulate fp16 atomic on top of packed fp16 atomic where possible. Also for bfloat16 atomics, albeit those don't get matched right now due to FloatNormalization. Left in support for fp16 and bfloat16 vector atomics. We might enable the vectorization for them in the future if we can prove the access satisfies 4byte aligment. Copybara import of the project:  b4ac2bc984b40bb33f287a4ed351b6c1560e6895 by Dragan Mladjenovic : [ROCm] Cleanup atomics support Merging this change closes CC(Java process crashes during model loading) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22541 from ROCm:atomics_cleanup b4ac2bc984b40bb33f287a4ed351b6c1560e6895",2025-02-12T14:48:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87158
copybara-service[bot],Reverts b0f125564cc0ecd1cc6e3fe753feda2ecc16e08c,Reverts b0f125564cc0ecd1cc6e3fe753feda2ecc16e08c,2025-02-12T14:46:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87157
copybara-service[bot],[pjrt] Removed PjRtDevice overloads of `PjRtClient::CreateBuffersForAsyncHostToDevice`,"[pjrt] Removed PjRtDevice overloads of `PjRtClient::CreateBuffersForAsyncHostToDevice` I also pulled in the `Shape`>`ShapeSpec` conversion code into the default implementation, since it was duplicated in a few clients.",2025-02-12T14:36:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87156
copybara-service[bot],[XLA:GPU] Rewrite if else chains into switch statements.,[XLA:GPU] Rewrite if else chains into switch statements.,2025-02-12T14:04:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87155
copybara-service[bot],Integrate LLVM at llvm/llvm-project@0e779ad4998e,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 0e779ad4998e,2025-02-12T13:44:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87154
copybara-service[bot],#sdy Add JAX backwards compatibility test.,"sdy Add JAX backwards compatibility test. This tests saving a module with one set of axis names, but loading it with another set of axis names. This does also test the custom calls:  ``  `.sdy.GlobalToLocalShape`  `.sdy.LocalToGlobalShape` But note that there are a bunch of other custom calls that will be tested in the Shardy and XLA codebases. The way the testing utils is tested here doesn't allow me to set `out_shardings` for example. So JAX can rely on the existence of those tests as stability guarantees just like for StableHLO. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22532 from ROCm:ci_fix_collective_alloc_20250210 5be78abc2fc50b65b5749f433f5766e2c77a438d",2025-02-12T12:56:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87153
copybara-service[bot],Enable test and make it pass.,"Enable test and make it pass. According to the test name, we want to ensure that DynamicUpdateSlice is not fused into the reduce fusion. There is also a test expectation that there are only 2 fusions, but my guess is that this is from before where we actually did fuse DynamicUpdateSlice into the reduce fusion.",2025-02-12T12:14:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87152
henghamao,Fail to get tf serving model metada," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution CentOS 7.9  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? We used the following code to get tf serving meta data, especially the version name of serving model. The code worked fine for tf 2.15 models. However, when we used the model exported by tf 2.18 and moved to tf 2.18, there is exception. m.zip  Standalone code to reproduce the issue ```shell def get_tf_model_path(self, model_name, port=8501):         channel = grpc.insecure_channel(f'localhost:{port}')         stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)         request = get_model_metadata_pb2.GetModelMetadataRequest()         request.model_spec.name = model_name         request.metadata_field.append(""signature_def"")         try:             response = stub.GetModelMetadata(request, timeout=10)             result = json.loads(MessageToJson(response))             model_path = self.tf_path + '/' + model_name + '/' + result['modelSpec']['version']         except grpc.RpcError as e:             print(""gRPC error: "", e)             return None         return model_path ``` The code worked fine for tf 2.15 models. However, when we used the model exported by tf 2.18, there is exception: ```  ``` The model exported by tf 2.18 is attached for your reference.  Relevant log output ```shell ```",2025-02-12T11:32:14Z,type:bug,closed,0,3,https://github.com/tensorflow/tensorflow/issues/87151,"Sorry, the issue was caused by wrong port number configurations.","Sorry, the issue was caused by wrong port number configurations.",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T11:16:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87150
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T10:39:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87149
copybara-service[bot],[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use,[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use in tests. This is to work around issues of test parametrization while `xla_gpu_enable_triton_gemm_any` needs to be worked around in the main compiler path for A100.,2025-02-12T09:32:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87148
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:29:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87147
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:27:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87146
copybara-service[bot],Change `DeviceList::FromProto()` to use `Client::MakeDeviceList()` to create device lists,"Change `DeviceList::FromProto()` to use `Client::MakeDeviceList()` to create device lists This lets the IFRT implementation control how device lists are deserialized, effectively addressing the TODO for device list SerDes in a different way. This requires passing `Client*` to `DeviceList::FromProto()` and SerDes implementations that internally call `DeviceList::FromProto()`.",2025-02-12T09:26:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87145
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:24:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87144
copybara-service[bot],[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use,[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use in tests. This is to work around issues of test parametrization while `xla_gpu_enable_triton_gemm_any` needs to be worked around in the main compiler path for A100.,2025-02-12T09:23:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87143
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:23:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87142
copybara-service[bot],[XLA:GPU] Move ConvertRangeVariablesToDimensions from scatter.cc to indexing_map.h.,[XLA:GPU] Move ConvertRangeVariablesToDimensions from scatter..h. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22532 from ROCm:ci_fix_collective_alloc_20250210 5be78abc2fc50b65b5749f433f5766e2c77a438d,2025-02-12T09:22:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87141
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:22:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87140
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:21:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87139
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:21:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87138
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:21:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87137
copybara-service[bot],PR #22532: [ROCm] Use device allocate for collective memory allocation,PR CC(Tensorflow quantization on Windows): [ROCm] Use device allocate for collective memory allocation Imported from GitHub PR https://github.com/openxla/xla/pull/22532 This extends https://github.com/openxla/xla/pull/22102 and fixes //xla/stream_executor/gpu:gpu_executor_test_gpu_amd_any Copybara import of the project:  5be78abc2fc50b65b5749f433f5766e2c77a438d by Harsha HS : [ROCm] Use device allocate for collective memory allocation This extends https://github.com/openxla/xla/pull/22102 and fixes //xla/stream_executor/gpu:gpu_executor_test_gpu_amd_any Merging this change closes CC(Tensorflow quantization on Windows) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22532 from ROCm:ci_fix_collective_alloc_20250210 5be78abc2fc50b65b5749f433f5766e2c77a438d,2025-02-12T09:21:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87136
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:20:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87135
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22512 from openxla:skozub/block_scaling_nvfp4 32e76a88b2107c079e26826417d22664cbf809a3,2025-02-12T09:20:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87134
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22532 from ROCm:ci_fix_collective_alloc_20250210 5be78abc2fc50b65b5749f433f5766e2c77a438d,2025-02-12T09:19:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87133
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:19:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87132
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:18:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87131
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:18:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87130
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:17:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87129
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:17:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87128
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:17:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87127
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:16:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87126
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:15:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87125
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22532 from ROCm:ci_fix_collective_alloc_20250210 5be78abc2fc50b65b5749f433f5766e2c77a438d,2025-02-12T09:15:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87124
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:15:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87123
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:15:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87122
copybara-service[bot],PR #22512: [XLA:GPU] Enable cuDNN kernel for NVFP4 block scaled dot,"PR CC(Installation issue with Tensorflowcpu, no module named '_pwyrap_tensorflow_internal'): [XLA:GPU] Enable cuDNN kernel for NVFP4 block scaled dot Imported from GitHub PR https://github.com/openxla/xla/pull/22512 Support NVFP4 in addition to MXFP8 hardware acceleration for the ""__op$block_scaled_dot"" custom call. This PR also addresses some nits from the internal review (like renaming a generic `CompositeType` to a more specific `CudnnMxType`). Copybara import of the project:  32e76a88b2107c079e26826417d22664cbf809a3 by Sergey Kozub : [XLA:GPU] Enable cuDNN kernel for NVFP4 block scaled dot Merging this change closes CC(Installation issue with Tensorflowcpu, no module named '_pwyrap_tensorflow_internal') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22512 from openxla:skozub/block_scaling_nvfp4 32e76a88b2107c079e26826417d22664cbf809a3",2025-02-12T09:15:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87121
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:14:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87120
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:14:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87119
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:14:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87118
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22512 from openxla:skozub/block_scaling_nvfp4 32e76a88b2107c079e26826417d22664cbf809a3,2025-02-12T09:14:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87117
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22509 from ROCm:automatic_include f4e7d6d91fa349eab54478a9f03875159378f237,2025-02-12T09:14:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87116
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22529 from ROCm:ci_fix_link_rccl_20240210 ac9f1c5a398b06752daabdd5af7c19fdf3a809a4,2025-02-12T09:14:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87115
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22512 from openxla:skozub/block_scaling_nvfp4 32e76a88b2107c079e26826417d22664cbf809a3,2025-02-12T09:13:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87114
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:12:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87113
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:11:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87112
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T09:11:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87111
copybara-service[bot],Integrate LLVM at llvm/llvm-project@ab93bd6959d6,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match ab93bd6959d6,2025-02-12T08:26:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87110
copybara-service[bot],[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use,[XLA:GPU] Add a debug option `xla_gpu_unsupported_force_triton_gemm` for use in tests. This is to work around issues of test parametrization while `xla_gpu_enable_triton_gemm_any` needs to be worked around in the main compiler path for A100.,2025-02-12T07:51:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87109
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-12T07:17:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87108
copybara-service[bot],Support split op options in LiteRt C Api. Recover split Op builder in QNN compiler plugin.,Support split op options in LiteRt C Api. Recover split Op builder in QNN compiler plugin.,2025-02-12T06:07:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87107
copybara-service[bot],make,"make `bool HloProtoMap::AddHloProto(uint64_t program_id,const xla::HloProto* hlo_proto)` private as calling it directly from outside will potentially create the memory use after free issue.",2025-02-12T05:50:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87106
copybara-service[bot],Replace the remaining use of `BasicDeviceList` in the base IFRT,"Replace the remaining use of `BasicDeviceList` in the base IFRT After this CL, we will be able to separate `BasicDeviceList` into a separate build target. This will let us control its visibility so that IFRT users (not IFRT implementations) stop calling `BasicDeviceList`.",2025-02-12T05:18:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87105
copybara-service[bot],[ODML] Move StableHLO -> MHLO conversion inside `AddMhloOptimizationPasses`. There are two paths calling `AddMhloOptimizationPasses`. It is easier to move new stablehlo passes before StableHLO -> MHLO conversion inside AddMhloOptimizationPasses.,[ODML] Move StableHLO > MHLO conversion inside `AddMhloOptimizationPasses`. There are two paths calling `AddMhloOptimizationPasses`. It is easier to move new stablehlo passes before StableHLO > MHLO conversion inside AddMhloOptimizationPasses.,2025-02-12T02:51:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87104
copybara-service[bot],[XLA]Further increase timeout to 6hr for CPU benchmarks,[XLA]Further increase timeout to 6hr for CPU benchmarks,2025-02-12T02:43:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87103
copybara-service[bot],[XLA] Add a pass for deferring constants close to users.,[XLA] Add a pass for deferring constants close to users. 1. Add constant deferring pass. 2. Add common container functionalities to HloInstructionSequence class.,2025-02-12T02:21:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87102
copybara-service[bot],HloModule is converted to proto in MlirToXlaComputation so need to make sure result accuracy is kept.,HloModule is converted to proto in MlirToXlaComputation so need to make sure result accuracy is kept.,2025-02-12T02:12:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87101
copybara-service[bot],Add temporary API for dev,Add temporary API for dev,2025-02-12T01:59:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87100
copybara-service[bot],Add targets to use libLiteRtDispatch.so,Add targets to use libLiteRtDispatch.so,2025-02-12T01:52:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87099
copybara-service[bot],Update tensorflow to point directly to se_gpu_pjrt_client.h to allow merging in the future.,Update tensorflow to point directly to se_gpu_pjrt_client.h to allow merging in the future.,2025-02-12T01:48:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87098
copybara-service[bot],1. Replaced the BaseCosts API with the OpCostManager API in MSA's CostAnalysis.,"1. Replaced the BaseCosts API with the OpCostManager API in MSA's CostAnalysis. 2. MSA's CostAnalysis directly exposes OperandBytesAccess() and OutputBytesAccess() methods, rather than through base_costs(), as it previously did.",2025-02-12T01:30:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87097
copybara-service[bot],Add utility functions to load models with multiple NPU ops,Add utility functions to load models with multiple NPU ops,2025-02-12T01:29:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87096
copybara-service[bot],Comment out the qnn compiler tests that aren't supported by new qnn implementation.,Comment out the qnn compiler tests that aren't supported by new qnn implementation.,2025-02-12T01:14:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87095
copybara-service[bot],Use public CreateViewOfDeviceBuffer for creating aliases,Use public CreateViewOfDeviceBuffer for creating aliases instead of internal APIs.,2025-02-12T00:59:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87094
copybara-service[bot],[XLA:GPU] Make command_buffer_thunk_test:DynamicSliceFusionCmd more tolerant,[XLA:GPU] Make command_buffer_thunk_test:DynamicSliceFusionCmd more tolerant,2025-02-12T00:44:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87093
copybara-service[bot],Integrate LLVM at llvm/llvm-project@ab93bd6959d6,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match ab93bd6959d6,2025-02-12T00:13:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87092
copybara-service[bot],[XLA] Add tests for hlo_pass_fix.,[XLA] Add tests for hlo_pass_fix.,2025-02-11T23:48:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87091
copybara-service[bot],Internal change only.,Internal change only.,2025-02-11T23:41:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87090
copybara-service[bot],Disable memory stats when running on macOS for the single_machine_test. This allows us to re-enable the test for mac.,Disable memory stats when running on macOS for the single_machine_test. This allows us to reenable the test for mac.,2025-02-11T23:41:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87089
copybara-service[bot],internal BUILD rule visibility,internal BUILD rule visibility,2025-02-11T23:29:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87088
tensorflow-jenkins,Update version numbers for TensorFlow 2.18.1,"Before merging this PR, please double check that it has correctly updated `core/public/version.h`, `tools/pip_package/setup.py`, and `tensorflow/tensorflow.bzl`. Also review the execution notes below: ``` Major: 2 > 2 Minor: 18 > 18 Patch: 0 > 1 WARNING: Below are potentially instances of lingering old version string  ""2.18.0"" in source directory ""tensorflow/"" that are not updated by this script.  Please check them manually! Binary file  tensorflow/lite/delegates/xnnpack/odml_sdpa_composite_gqa.tflite.bin matches Binary file  tensorflow/lite/delegates/xnnpack/odml_sdpa_composite_mha.tflite.bin matches Binary file  tensorflow/lite/delegates/xnnpack/odml_sdpa_composite_mqa.tflite.bin matches tensorflow/lite/tools/versioning/runtime_version.cc:121:2.18.0 tensorflow/lite/tools/versioning/runtime_version.cc:137:2.18.0 tensorflow/lite/tools/versioning/runtime_version.cc:321:2.18.0 tensorflow/python/compiler/tensorrt/README.md:3:2.18.0 WARNING: Below are potentially instances of lingering old version string  ""2.18.0"" in source directory ""tensorflow/"" that are not updated by this script.  Please check them manually! Binary file  tensorflow/lite/delegates/xnnpack/odml_sdpa_composite_gqa.tflite.bin matches Binary file  tensorflow/lite/delegates/xnnpack/odml_sdpa_composite_mha.tflite.bin matches Binary file  tensorflow/lite/delegates/xnnpack/odml_sdpa_composite_mqa.tflite.bin matches tensorflow/lite/tools/versioning/runtime_version.cc:121:2.18.0 tensorflow/lite/tools/versioning/runtime_version.cc:137:2.18.0 tensorflow/lite/tools/versioning/runtime_version.cc:321:2.18.0 tensorflow/python/compiler/tensorrt/README.md:3:2.18.0 ```",2025-02-11T23:13:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87087
copybara-service[bot],Reenable //xla/tsl/distributed_runtime/coordination:client_server_test on ARM,Reenable //xla/tsl/distributed_runtime/coordination:client_server_test on ARM Attempting to reenable now that GitHub Actions ARM build uses old machine type Reverts bf32902cf6d008469c59f9674c80485f5aa89ac4,2025-02-11T22:49:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87086
copybara-service[bot],Reverts bd21ff597117a11c8f70c05d57bef9ef105a4d12,Reverts bd21ff597117a11c8f70c05d57bef9ef105a4d12,2025-02-11T21:25:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87085
copybara-service[bot],Support optimization_level and memory_fitting_level XLA compilation options.,Support optimization_level and memory_fitting_level XLA compilation options.,2025-02-11T21:02:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87084
copybara-service[bot],[PjRt] GPU DMA support in PjRt layer,[PjRt] GPU DMA support in PjRt layer Enable PjRt to register host memory for use by CUDA. PjRt Caller is responsible to ensure all transfers are complete before calling DmaUnmap. PjRt Client remembers all current DMA mapped region to determine whether to use staging buffer or DMA to do data transfer.,2025-02-11T19:46:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87083
copybara-service[bot],[xla:ffi] Add context decoding for some FFI internals.,[xla:ffi] Add context decoding for some FFI internals.,2025-02-11T19:43:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87082
copybara-service[bot],Remove the Windows 2019 configs.,Remove the Windows 2019 configs. CI now runs completely on Windows 2022.,2025-02-11T19:42:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87081
copybara-service[bot],[XLA] Add `linux-x86-n2-128` runner and increase the timeout for the cpu_benchmarks workflow.,[XLA] Add `linuxx86n2128` runner and increase the timeout for the cpu_benchmarks workflow.,2025-02-11T19:38:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87080
copybara-service[bot],"[XLA] {in,out}feed SPMD spring cleaning","[XLA] {in,out}feed SPMD spring cleaning A pletora of changes: 1. Correct sharding generated for the infeed token in XlaBuilder. 2. Make SpmdPartitioner not pad infeed with manual sharding. 3. Make {in,out}feed rewrite passes work with sharding.",2025-02-11T19:37:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87079
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f290fc3df0eb,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f290fc3df0eb,2025-02-11T19:34:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87078
copybara-service[bot],[XLA:GPU] Disable `--xla_gpu_triton_gemm_any` on Ampere.,"[XLA:GPU] Disable `xla_gpu_triton_gemm_any` on Ampere. Triton's conversion logic from `f16` to `f8e5m2` is wrong preHopper. Disabling this wholesale is a bit overkill, but easiestsince this flag flip is what surfaced the issue in the first place.",2025-02-11T19:30:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87077
copybara-service[bot],Delete TensorFlow GPU Kokoro build now that GitHub Actions build runs continuously,Delete TensorFlow GPU Kokoro build now that GitHub Actions build runs continuously Also don't `bazel build nobuild` for the new TensorFlow GHA GPU build.,2025-02-11T19:26:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87076
copybara-service[bot],Add helper to get custom op code from a litert op,Add helper to get custom op code from a litert op,2025-02-11T19:22:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87075
copybara-service[bot],Move users of `BasicDeviceList::Create()` to `Client::MakeDeviceList()`,"Move users of `BasicDeviceList::Create()` to `Client::MakeDeviceList()` IFRT is moving towards runtimecontrolled device list creation. This CL moves most of explicit device list creation from `BasicDeviceList::Create()` to `Client::MakeDeviceList()`. Once the migration is done, `BasicDeviceList::Create()` will be reserved only for IFRT implementations and all IFRT users will be expected to use `Client::MakeDeviceList::Create()` to create device lists.",2025-02-11T19:06:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87074
tensorflow-jenkins,Update release notes for TensorFlow 2.18.1,"This PR is intentionally incomplete. One of the Release Owners for 2.18.1 needs to fill in the internal release notes for this version before the PR gets submitted. Click on the :pencil2: icon in the header for `RELEASE.md` under ""Files Changed"" above.",2025-02-11T19:06:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87073
copybara-service[bot],In progress. Adds support for string processing in Colocated Python.,In progress. Adds support for string processing in Colocated Python.,2025-02-11T18:48:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87072
copybara-service[bot],Create GitHub Actions build for TensorFlow GPU,Create GitHub Actions build for TensorFlow GPU,2025-02-11T18:19:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87071
copybara-service[bot],Properly denote parameter types in litert.py per litert_test.py,Properly denote parameter types in litert.py per litert_test.py,2025-02-11T18:14:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87070
copybara-service[bot],Create copybara configuration to move LiteRT implementation,Create copybara configuration to move LiteRT implementation,2025-02-11T18:13:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87069
copybara-service[bot],This is an internal change,This is an internal change,2025-02-11T17:59:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87068
copybara-service[bot],"Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`","Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`",2025-02-11T17:54:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87067
copybara-service[bot],[XLA:GPU] Disable autotuning for tests to prevent flakiness.,[XLA:GPU] Disable autotuning for tests to prevent flakiness.,2025-02-11T17:54:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87066
copybara-service[bot],[XLA:GPU] Move ConvertRangeVariablesToDimensions from scatter.cc to indexing_map.h.,[XLA:GPU] Move ConvertRangeVariablesToDimensions from scatter..h.,2025-02-11T17:52:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87065
copybara-service[bot],Migrate fuzz tests to always use PjRt.,Migrate fuzz tests to always use PjRt.,2025-02-11T16:49:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87064
cybersupersoap,`gradient_checker.compute_gradient` can cause a crash," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tfnightly 2.19.0dev20241025  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04.3 LTS  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `Floating point exception` issue in TensorFlow when I used API `gradient_checker.compute_gradient`. I have confirmed that below code would crash on tfnightly 2.19.0dev20241025 (nightlybuild). Please find the gist to reproduce the issue.  Standalone code to reproduce the issue ```shell import math from absl.testing import parameterized import numpy as np import tensorflow as tf from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import errors_impl from tensorflow.python.framework import tensor_shape from tensorflow.python.framework import test_util from tensorflow.python.ops import gen_nn_ops from tensorflow.python.ops import gradient_checker from tensorflow.python.ops import gradients_impl from tensorflow.python.ops import nn_ops import tensorflow.python.ops.nn_grad   pylint: disable=unusedimport from tensorflow.python.platform import test from tensorflow.python.platform import tf_logging from tensorflow.python.util.compat import collections_abc from tensorflow.python.eager import context def DtypesToTest(use_gpu):    double datatype is currently not supported for convolution ops    on the ROCm platform   optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]   if use_gpu:     if not test_util.GpuSupportsHalfMatMulAndConv():       return optional_float64 + [dtypes.float32]     else:        It is important that float32 comes before float16 here,        as we will be using its gradients as reference for fp16 gradients.       return optional_float64 + [dtypes.float32, dtypes.float16]   else:     return optional_float64 + [dtypes.float32, dtypes.float16, dtypes.bfloat16] def _ConstructAndTestGradientForConfig(     batch, input_shape, filter_shape, in_depth, out_depth, stride,     padding, test_input, data_format, use_gpu):   input_planes, input_rows, input_cols = input_shape   filter_planes, filter_rows, filter_cols = filter_shape   input_shape = [batch, input_planes, input_rows, input_cols, in_depth]   filter_shape = [       filter_planes, filter_rows, filter_cols, in_depth, out_depth   ]   if isinstance(stride, collections_abc.Iterable):     strides = [1] + list(stride) + [1]   else:     strides = [1, stride, stride, stride, 1]   if padding == ""VALID"":     output_planes = int(         math.ceil((input_planes  filter_planes + 1.0) / strides[1]))     output_rows = int(         math.ceil((input_rows  filter_rows + 1.0) / strides[2]))     output_cols = int(         math.ceil((input_cols  filter_cols + 1.0) / strides[3]))   else:     output_planes = int(math.ceil(float(input_planes) / strides[1]))     output_rows = int(math.ceil(float(input_rows) / strides[2]))     output_cols = int(math.ceil(float(input_cols) / strides[3]))   output_shape = [batch, output_planes, output_rows, output_cols, out_depth]   input_size = 1   for x in input_shape:     input_size *= x   filter_size = 1   for x in filter_shape:     filter_size *= x   input_data = [x * 1.0 / input_size for x in range(0, input_size)]   filter_data = [x * 1.0 / filter_size for x in range(0, filter_size)]   for data_type in DtypesToTest(use_gpu=use_gpu):      TODO(mjanusz): Modify gradient_checker to also provide max relative      error and synchronize the tolerance levels between the tests for forward      and backward computations.     if data_type == dtypes.float64:       tolerance = 1e8     elif data_type == dtypes.float32:       tolerance = 5e3     elif data_type == dtypes.float16:       tolerance = 5e3 if test.is_built_with_rocm() else 1e3     elif data_type == dtypes.bfloat16:       tolerance = 1e2     sess = tf.compat.v1.Session()     with sess.as_default():       orig_input_tensor = constant_op.constant(           input_data, shape=input_shape, dtype=data_type, name=""input"")       filter_tensor = constant_op.constant(           filter_data, shape=filter_shape, dtype=data_type, name=""filter"")       if data_format == ""NCDHW"":         input_tensor = test_util.NHWCToNCHW(orig_input_tensor)         new_strides = test_util.NHWCToNCHW(strides)       else:         input_tensor = orig_input_tensor         new_strides = strides       conv = nn_ops.conv3d(           input_tensor,           filter_tensor,           new_strides,           padding,           data_format=data_format,           name=""conv"")       jacob_t, jacob_n = gradient_checker.compute_gradient(           orig_input_tensor, input_shape, conv, output_shape) with context.graph_mode():   _ConstructAndTestGradientForConfig(data_format=""NDHWC"",use_gpu=False,batch=2, input_shape=(3, 7, 6), filter_shape=(3, 3, 3), in_depth=2, out_depth=0, stride=3, padding='VALID', test_input=True) ```  Relevant log output ```shell Fatal Python error: Floating point exception ```",2025-02-11T16:38:56Z,type:support WIP comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/87063,", I tried to execute the mentioned code on tensorflow v2.17 & tfnightly and observed that the crash is happened in both the versions. Please allow to deep dive the issue and provide the root cause for the same. Kindly find the gist of it here. Thank you!"
copybara-service[bot],Only cache jax.Array._npy_value when a copy is required.,"Only cache jax.Array._npy_value when a copy is required. As discovered in https://github.com/jaxml/jax/issues/26216, for nonstandard dtypes, calling `np.array` on a JAX array will unnecessarily cache the constructed `_npy_value` even when a copy isn't required. This change updates the logic to only save the cached value when it is a copy. This fixes https://github.com/jaxml/jax/issues/26216 by making the behavior consistent across dtypes, but we probably also want to expose a mechanism for clearing this cached value regardless.",2025-02-11T16:26:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87062
copybara-service[bot],Rollback as this change is affecting internal model builds,Rollback as this change is affecting internal model builds Reverts 58b6d752be7670004178dab34f62926cdcd29ae5,2025-02-11T16:00:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87061
copybara-service[bot],PR #22554: [GPU] Fix cuDNN fusion compiler support of control predecessors.,PR CC(Adding custom op instructions lacking the GPU building instructions): [GPU] Fix cuDNN fusion compiler support of control predecessors. Imported from GitHub PR https://github.com/openxla/xla/pull/22554 Control predecessors have to be handled like in https://github.com/openxla/xla/blob/28887817aa29aef860211b131e4f6901ef590d4c/xla/service/gpu/transforms/fusion_wrapper.ccL133L137 to make the removal of the original instruction safe. Copybara import of the project:  7a98896fbf411a3bf6112d61dfb0fe59e55f712e by Ilia Sergachev : [GPU] Fix cuDNN fusion compiler support of control predecessors. Merging this change closes CC(Adding custom op instructions lacking the GPU building instructions) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22554 from openxla:fix_cudnn_fusion_compilation 7a98896fbf411a3bf6112d61dfb0fe59e55f712e,2025-02-11T15:49:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87060
copybara-service[bot],[xla:cpu:onednn] Enable oneDNN thread pool targets,[xla:cpu:onednn] Enable oneDNN thread pool targets Add build rules that have empty srcs/hdrs and deps when building without Graph API. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22532 from ROCm:ci_fix_collective_alloc_20250210 5be78abc2fc50b65b5749f433f5766e2c77a438d,2025-02-11T14:48:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87059
copybara-service[bot],Prepare support for multiple fusion roots.,"Prepare support for multiple fusion roots. Change the data structures to hold more than one tiling, in case we have more than one fusion root. No real support for multiple fusion roots yet.",2025-02-11T14:18:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87058
copybara-service[bot],PR #22529: [ROCm] Fix link error with rccl by adding libnuma link dependency,PR CC(Feature request: Make contrib.receptive_field extensible): [ROCm] Fix link error with rccl by adding libnuma link dependency Imported from GitHub PR https://github.com/openxla/xla/pull/22529 Copybara import of the project:  ac9f1c5a398b06752daabdd5af7c19fdf3a809a4 by Harsha HS : [ROCm] Fix link error with rccl by adding libnuma link dependency Merging this change closes CC(Feature request: Make contrib.receptive_field extensible) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22529 from ROCm:ci_fix_link_rccl_20240210 ac9f1c5a398b06752daabdd5af7c19fdf3a809a4,2025-02-11T14:12:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87057
copybara-service[bot],PR #22537: [GPU] Add cuDNN 9.7.1.,PR CC(122): [GPU] Add cuDNN 9.7.1. Imported from GitHub PR https://github.com/openxla/xla/pull/22537 Copybara import of the project:  cdecd418a3159a798c7f2c6ac1e17a8f8ce6d862 by Ilia Sergachev : [GPU] Add cuDNN 9.7.1. Merging this change closes CC(122) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22537 from openxla:cudnn_971 cdecd418a3159a798c7f2c6ac1e17a8f8ce6d862,2025-02-11T14:11:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87056
copybara-service[bot],PR #22509: [ROCm] Avoid hardcoding hipcc compiler includes,PR CC(Make sure broken tests are filtered out in XLA tests suites.): [ROCm] Avoid hardcoding hipcc compiler includes Imported from GitHub PR https://github.com/openxla/xla/pull/22509 Copybara import of the project:  f4e7d6d91fa349eab54478a9f03875159378f237 by Dragan Mladjenovic : [ROCm] Avoid hardcoding hipcc compiler includes Merging this change closes CC(Make sure broken tests are filtered out in XLA tests suites.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22509 from ROCm:automatic_include f4e7d6d91fa349eab54478a9f03875159378f237,2025-02-11T14:02:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87055
copybara-service[bot],PR #84975: build(aarch64): Update to oneDNN-3.7 + ACL-24.12,"PR CC(build(aarch64): Update to oneDNN3.7 + ACL24.12): build(aarch64): Update to oneDNN3.7 + ACL24.12 Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/84975 Bumps the aarch64compatible oneDNN version to 3.7 and the ACL version to 24.12. This brings better performance, improved memory management, and numerous bug fixes over the previous, long outofdate versions. cc:   Copybara import of the project:  01c30f3277c2b943ebc61a0b23ab402d48b3e1d2 by Siddhartha Menon : build(aarch64): Update to oneDNN3.7 + ACL24.12 Bumps the aarch64compatible oneDNN version to 3.7 and the ACL version to 24.12. This brings better performance, improved memory management, and numerous bug fixes over the previous, long outofdate versions. Signedoffby: Siddhartha Menon  Merging this change closes CC(build(aarch64): Update to oneDNN3.7 + ACL24.12) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/84975 from Sqvid:tfbump3.7 01c30f3277c2b943ebc61a0b23ab402d48b3e1d2",2025-02-11T13:01:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87054
copybara-service[bot],[XLA:GPU] Fix broken `//third_party/tensorflow/compiler/xla/service/gpu:determinism_test` test.,[XLA:GPU] Fix broken `//third_party/tensorflow/compiler/xla/service/gpu:determinism_test` test.,2025-02-11T12:57:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87053
copybara-service[bot],PR #20332: [ds-fusion] Add runtime support for host calculation of offsets in ds fusion,"PR CC(An attempt at Tensorflow and Bazel CPU/GPU build (v1.8.0)): [dsfusion] Add runtime support for host calculation of offsets in ds fusion Imported from GitHub PR https://github.com/openxla/xla/pull/20332 This patch adds the support for calculating offset on the host at runtime when the offset depends on the loop induction variable. This is done by extracting the offset computation, the induction variable initialization and the induction variable update as independent computations and they are evaluated on the host at runtime. This avoids devicetohost copy for this fusion in these cases. Copybara import of the project:  5c85fe7140d462c3282e422c2d9090028afd6eb4 by Shraiysh Vaishay : Add runtime support for host calculation of offsets in ds fusion This patch adds the support for calculating offset on the host at runtime when the offset depends on the loop induction variable. This is done by extracting the offset computation, the induction variable initialization and the induction variable update as independent computations and they are evaluated on the host at runtime. This avoids devicetohost copy for this fusion in these cases.  b5573b0ceb00d302634c0d95afb9d1ff7d697e64 by Shraiysh Vaishay : Addressed comments  decde73b33ba03a74ce896918c9cf9aaff13ddca by Shraiysh Vaishay : Rebase  f98d9dc15b6d0b9b7a2368f55de607a65da1c56e by Shraiysh Vaishay : Rebase Merging this change closes CC(An attempt at Tensorflow and Bazel CPU/GPU build (v1.8.0)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20332 from shraiysh:ds_fusion_3 f98d9dc15b6d0b9b7a2368f55de607a65da1c56e",2025-02-11T12:42:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87052
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T12:37:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87051
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T12:33:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87050
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22376 from shraiysh:fix_collective_backend_config 1151e05ed74f57722d2599a837520773f46a7b77,2025-02-11T11:56:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87049
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T11:48:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87048
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T11:05:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87047
copybara-service[bot],[XLA:GPU] Check that tensor/vector should be flattened first.,"[XLA:GPU] Check that tensor/vector should be flattened first. An upsteam change [0] adds a constant folder for `vector.insert`. `mv::getAsValues` will create `arith.constant` ops for attributes. If the tensor/vector already flattened, the constant is dead, so rewriter will delete the constant, but run the `RewriteVectorInsert` again, causing an infinite loop. [0]  https://github.com/llvm/llvmproject/commit/7ae78a6cdb6ce9ad1534ed10519649fb3d47aca9",2025-02-11T11:05:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87046
copybara-service[bot],PR #22452: [ROCm] Fix missing header in hipblaslt_wrapper.h,PR CC(Documentation for tf.train.init_from_checkpoint doesn't say what it does when): [ROCm] Fix missing header in hipblaslt_wrapper.h Imported from GitHub PR https://github.com/openxla/xla/pull/22452 Copybara import of the project:  225a63f3d9596e8827bf04904d9b8691de2b0bc7 by Dragan Mladjenovic : [ROCm] Fix missing header in hipblaslt_wrapper.h Merging this change closes CC(Documentation for tf.train.init_from_checkpoint doesn't say what it does when) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22452 from ROCm:hipblaslt_wrapper 225a63f3d9596e8827bf04904d9b8691de2b0bc7,2025-02-11T11:05:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87045
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T10:11:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87044
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:32:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87043
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:31:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87042
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:28:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87041
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:28:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87040
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:28:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87039
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:25:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87038
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:24:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87037
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:23:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87036
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:21:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87035
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:20:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87034
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:18:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87033
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:17:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87032
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:17:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87031
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:17:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87030
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:17:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87029
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:16:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87028
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:16:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87027
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:15:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87026
copybara-service[bot],[NCCL] Upgrade TF NCCL version to 2.25.1,[NCCL] Upgrade TF NCCL version to 2.25.1,2025-02-11T09:15:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87025
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:15:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87024
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22376 from shraiysh:fix_collective_backend_config 1151e05ed74f57722d2599a837520773f46a7b77,2025-02-11T09:15:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87023
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:15:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87022
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:14:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87021
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:14:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87020
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:14:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87019
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:14:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87018
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:13:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87017
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:13:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87016
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T09:12:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87015
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f290fc3df0eb,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f290fc3df0eb,2025-02-11T09:08:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87014
copybara-service[bot],Update `pthreadpool` version used by `cmake`.,Update `pthreadpool` version used by `cmake`.,2025-02-11T08:48:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87013
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T08:39:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87012
copybara-service[bot],Move users of `BasicDeviceList::Create()` to `Client::MakeDeviceList()`,"Move users of `BasicDeviceList::Create()` to `Client::MakeDeviceList()` IFRT is moving towards runtimecontrolled device list creation. This CL moves most of explicit device list creation from `BasicDeviceList::Create()` to `Client::MakeDeviceList()`. Once the migration is done, `BasicDeviceList::Create()` will be reserved only for IFRT implementations and all IFRT users will be expected to use `Client::MakeDeviceList::Create()` to create device lists.",2025-02-11T07:01:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87011
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T06:56:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87010
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-11T06:32:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87008
copybara-service[bot],Handling incompatible operand type during HLO -> Mhlo conversion.,Handling incompatible operand type during HLO > Mhlo conversion.,2025-02-11T05:42:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87007
copybara-service[bot],[IFRT] Fully support the dtype arg to AssembleArrayFromSingleDeviceArrays in IFRT Proxy.,[IFRT] Fully support the dtype arg to AssembleArrayFromSingleDeviceArrays in IFRT Proxy.,2025-02-11T05:26:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87006
copybara-service[bot],[IFRT] Add new Client::AssembleArrayFromSingleDeviceArrays API that takes a `dtype` argument.,"[IFRT] Add new Client::AssembleArrayFromSingleDeviceArrays API that takes a `dtype` argument. This is necessary to support IFRT/PJRT arrays with no buffers, as a step toward MPMD/pipeline parallelism.",2025-02-11T04:45:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87005
copybara-service[bot],Fix build issues and re-enable running benchmarks in cpu_benchmarks.yml,Fix build issues and reenable running benchmarks in cpu_benchmarks.yml,2025-02-11T04:30:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87004
copybara-service[bot],In progress. Testing. Change the existing mapping of kString->Object to kString->StringDType in IFRT.,In progress. Testing. Change the existing mapping of kString>Object to kString>StringDType in IFRT.,2025-02-11T02:28:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87003
copybara-service[bot],Use SourceTargetPairs in HloCollectivePermuteInstruction.,Use SourceTargetPairs in HloCollectivePermuteInstruction.,2025-02-11T02:28:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/87002
copybara-service[bot],Move users of `BasicDeviceList::Create()` to `Client::MakeDeviceList()`,"Move users of `BasicDeviceList::Create()` to `Client::MakeDeviceList()` IFRT is moving towards runtimecontrolled device list creation. This CL moves most of explicit device list creation from `BasicDeviceList::Create()` to `Client::MakeDeviceList()`. Once the migration is done, `BasicDeviceList::Create()` will be reserved only for IFRT implementations and all IFRT users will be expected to use `Client::MakeDeviceList::Create()` to create device lists.",2025-02-11T02:06:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87001
copybara-service[bot],Update compiler plugin to make sure all new IR uses the same buffer manager. This will substantially decrease the number of copies during partition step.,Update compiler plugin to make sure all new IR uses the same buffer manager. This will substantially decrease the number of copies during partition step.,2025-02-11T01:29:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/87000
copybara-service[bot],[XLA] Don't check recursively for thread mismatches,[XLA] Don't check recursively for thread mismatches,2025-02-11T01:12:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86999
copybara-service[bot],Add memcpy implementation of all-gather.,Add memcpy implementation of allgather.,2025-02-11T01:06:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86998
copybara-service[bot],"Compile multi-subgraph model as multiple QNN graph in a single context binary, if weights can be shared. Enabling weight sharing in compiled QNN context binary.","Compile multisubgraph model as multiple QNN graph in a single context binary, if weights can be shared. Enabling weight sharing in compiled QNN context binary.",2025-02-11T00:56:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86997
copybara-service[bot],[ODML] Use `mlir::stablehlo::createStablehloAggressiveSimplificationPass()` instead of `mlir::createCanonicalizerPass()`. mlir::createCanonicalizerPass() is no-op for StableHLO ops as canonicalization is default to disabled for stablehlo ops.,[ODML] Use `mlir::stablehlo::createStablehloAggressiveSimplificationPass()` instead of `mlir::createCanonicalizerPass()`. mlir::createCanonicalizerPass() is noop for StableHLO ops as canonicalization is default to disabled for stablehlo ops.,2025-02-11T00:35:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86996
copybara-service[bot],Use machines with 4 GPUs for TensorFlow CI,Use machines with 4 GPUs for TensorFlow CI,2025-02-11T00:32:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86995
copybara-service[bot],[ROCm] Suppress offsetof-extensions warning for rocm build,[ROCm] Suppress offsetofextensions warning for rocm build This closes https://github.com/openxla/xla/pull/22347,2025-02-11T00:25:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86994
copybara-service[bot],[XLA:SchedulingAnnotations] Uniquify annotation ids for unrolled scheduling groups.,[XLA:SchedulingAnnotations] Uniquify annotation ids for unrolled scheduling groups.,2025-02-11T00:21:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86993
copybara-service[bot],Support split op options in LiteRt C Api. Recover split Op builder in QNN compiler plugin.,Support split op options in LiteRt C Api. Recover split Op builder in QNN compiler plugin.,2025-02-11T00:18:03Z,ready to pull,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86992
copybara-service[bot],Support Mean op options in LiteRt C Api. Recover Mean Op builder in QNN compiler plugin.,Support Mean op options in LiteRt C Api. Recover Mean Op builder in QNN compiler plugin.,2025-02-11T00:10:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86991
copybara-service[bot],Increase tolerance of MatmulReplicated test again.,Increase tolerance of MatmulReplicated test again. It started failing with https://github.com/openxla/xla/commit/b39e89e68847a610e7d0f1eb25bc0ef998b7a36b.,2025-02-10T23:45:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86990
copybara-service[bot],Move ARM CI to old `t2a` machine type.,Move ARM CI to old `t2a` machine type. This is an attempted fix for some of the strange errors seen on the Actions ARM build.,2025-02-10T23:35:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86989
copybara-service[bot],Create JAX GPU GitHub Actions build,Create JAX GPU GitHub Actions build,2025-02-10T23:32:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86988
copybara-service[bot],Change tensor clone to share buffer when source and dest have the same manager.,Change tensor clone to share buffer when source and dest have the same manager.,2025-02-10T23:32:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86987
copybara-service[bot],Update model to share its buffer manager with any IR created under it.,Update model to share its buffer manager with any IR created under it.,2025-02-10T23:14:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86986
copybara-service[bot],Support gather op options in LiteRt C Api. Recover Gather Op builder in QNN compiler plugin.,Support gather op options in LiteRt C Api. Recover Gather Op builder in QNN compiler plugin.,2025-02-10T23:11:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86985
copybara-service[bot],Rename the unrelated scratch buffer mechanism to avoid confusion with weights buffer stuff.,Rename the unrelated scratch buffer mechanism to avoid confusion with weights buffer stuff.,2025-02-10T23:09:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86984
copybara-service[bot],Move users of `BasicDeviceList::Create()` to `Client::MakeDeviceList()`,"Move users of `BasicDeviceList::Create()` to `Client::MakeDeviceList()` IFRT is moving towards runtimecontrolled device list creation. This CL moves most of explicit device list creation from `BasicDeviceLIst::Create()` to `Client::MakeDeviceList()`. Once the migration is done, `BasicDeviceList::Create()` will be reserved only for IFRT implementations and all IFRT users will be expected to use `Client::MakeDeviceList::Create()` to create device lists.",2025-02-10T23:04:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86983
copybara-service[bot],Move users of `BasicDeviceList::Create()` to `Client::MakeDeviceList()`,"Move users of `BasicDeviceList::Create()` to `Client::MakeDeviceList()` IFRT is moving towards runtimecontrolled device list creation. This CL moves most of explicit device list creation from `BasicDeviceLIst::Create()` to `Client::MakeDeviceList()`. Once the migration is done, `BasicDeviceList::Create()` will be reserved only for IFRT implementations and all IFRT users will be expected to use `Client::MakeDeviceList::Create()` to create device lists.",2025-02-10T23:01:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86982
copybara-service[bot],[jax-transfer-lib]: Add timeouts to the event loop.,[jaxtransferlib]: Add timeouts to the event loop.,2025-02-10T22:39:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86981
copybara-service[bot],Fix litert vendor shared lib names to be distinct.,Fix litert vendor shared lib names to be distinct.,2025-02-10T22:27:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86980
copybara-service[bot],Move users of `BasicDeviceList::Create()` to `Client::MakeDeviceList()`,"Move users of `BasicDeviceList::Create()` to `Client::MakeDeviceList()` IFRT is moving towards runtimecontrolled device list creation. This CL moves most of explicit device list creation from `BasicDeviceLIst::Create()` to `Client::MakeDeviceList()`. Once the migration is done, `BasicDeviceList::Create()` will be reserved only for IFRT implementations and all IFRT users will be expected to use `Client::MakeDeviceList::Create()` to create device lists.",2025-02-10T22:24:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86979
copybara-service[bot],Integrate LLVM at llvm/llvm-project@1c583c19bb79,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 1c583c19bb79,2025-02-10T22:12:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86978
Jerry-Ge,[mlir][tosa] Add missing dependencies from tflite kernels,For fixing patch:  [TOSA] Add GELU approximate attr and transform to Tosa CC([TOSA ] Add GELU approximate attr and transform to Tosa),2025-02-10T21:54:20Z,size:XS,closed,0,2,https://github.com/tensorflow/tensorflow/issues/86977,"locally verified ``` INFO: Build option test_env has changed, discarding analysis cache. INFO: Analyzed 33 targets (0 packages loaded, 33377 targets configured). INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 2.421s, Critical Path: 0.81s INFO: 2 processes: 2 local. INFO: Build completed successfully, 2 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test (cached) PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test (cached) PASSED in 0.6s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test    (cached) PASSED in 0.6s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test       (cached) PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test (cached) PASSED in 0.6s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test  (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test (cached) PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test (cached) PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test (cached) PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test (cached) PASSED in 0.7s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test (cached) PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test (cached) PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test PASSED in 0.3s Executed 1 out of 17 tests: 17 tests pass. There were tests whose specified size is too big. Use the test_verbose_timeout_warnings command line option to see which ones these are. + set e + '[' 0 gt 0 ']' + '[' 0 gt 0 ']' ```",Assigning to  
tensorflow-jenkins,Update version numbers for TensorFlow 2.19.0-rc0,"Before merging this PR, please double check that it has correctly updated `core/public/version.h`, `tools/pip_package/setup.py`, and `tensorflow/tensorflow.bzl`. Also review the execution notes below: ``` Major: 2 > 2 Minor: 19 > 19 Patch: 0 > 0 WARNING: Below are potentially instances of lingering old version string  ""2.19.0"" in source directory ""tensorflow/"" that are not updated by this script.  Please check them manually! tensorflow/tensorflow.bzl:98:2.19.0 tensorflow/tensorflow.bzl:99:2.19.0 tensorflow/lite/experimental/litert/test/testdata/rms_norm.mlir:1:2.19.0 tensorflow/lite/tools/versioning/runtime_version.cc:433:2.19.0 tensorflow/tools/pip_package/setup.py:51:2.19.0 tensorflow/tools/pip_package/setup.py:112:2.19.0 WARNING: Below are potentially instances of lingering old version string  ""2.19.0"" in source directory ""tensorflow/"" that are not updated by this script.  Please check them manually! tensorflow/tensorflow.bzl:98:2.19.0 tensorflow/tensorflow.bzl:99:2.19.0 tensorflow/lite/experimental/litert/test/testdata/rms_norm.mlir:1:2.19.0 tensorflow/lite/tools/versioning/runtime_version.cc:433:2.19.0 tensorflow/tools/pip_package/setup.py:51:2.19.0 tensorflow/tools/pip_package/setup.py:112:2.19.0 ```",2025-02-10T21:45:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86976
rtg0795,Disable flaky test flag for TF 2.19 release,,2025-02-10T21:25:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86975
copybara-service[bot],Add debug option to control whether GetDefaultPlatform() is allowed (defaults to true).,Add debug option to control whether GetDefaultPlatform() is allowed (defaults to true).,2025-02-10T20:19:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86974
copybara-service[bot],"Add pywrap dependencies for ""common_deps"" and ""clib"" targets.","Add pywrap dependencies for ""common_deps"" and ""clib"" targets.",2025-02-10T19:57:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86973
copybara-service[bot],Reverts bd21ff597117a11c8f70c05d57bef9ef105a4d12,Reverts bd21ff597117a11c8f70c05d57bef9ef105a4d12,2025-02-10T19:55:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86972
copybara-service[bot],PR #85477: Integrate Op Builder with LiteRT Compile Part,"PR CC(Integrate Op Builder with LiteRT Compile Part): Integrate Op Builder with LiteRT Compile Part Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/85477  WHAT We replace the compiler part with Qualcomm implementations. This PR include commits in 3 PRs below. Follow these PRs or commit should help you review. You can get more details in the PR descriptions. 1. https://github.com/jiunkaiy/tensorflow/pull/1 2. https://github.com/jiunkaiy/tensorflow/pull/3 3. https://github.com/jiunkaiy/tensorflow/pull/5  TEST You can checkout this branch and run this test: ``` bazel build  c opt cxxopt=std=c++20 //tensorflow/lite/experimental/litert/vendors/qualcomm/compiler:qnn_compiler_plugin_test ./bazelbin/tensorflow/lite/experimental/litert/vendors/qualcomm/compiler/qnn_compiler_plugin_test ``` I disable these models because I don't have them.  kFeedForwardModel,  kKeyEinsumModel,  kQueryEinsumModel,  kValueEinsumModel,  kAttnVecEinsumModel,  kROPEModel,  kLookUpROPEModel,  kRMSNormModel,  kSDPAModel,  kAttentionModel,  kTransformerBlockModel,  kQSimpleMul16x16Model,  kQMulAdd16x16Model,  kQQueryEinsum16x8Model,  kQKeyEinsum16x8Model,  kQVauleEinsum16x8Model,  kQAttnVecEinsum16x8Model And you will see ``` [] Global test environment teardown [==========] 55 tests from 3 test suites ran. (338 ms total) [  PASSED  ] 54 tests. [  FAILED  ] 1 test, listed below: [  FAILED  ] SupportedOpsTest/QnnPluginOpValidationTest.SupportedOpsTest/4, where GetParam() = ""simple_slice_op.tflite"" ``` There are some bugs in simple_slice_op.mlir so the op validation will fail. Copybara import of the project:  b89672b2c477564b7dbcf0a327d860810461c151 by weilhuanquic : 1. Replace compiler part with op builders in core module 2. Support Op validation 3. Add unit tests for partition and compile Merging this change closes CC(Integrate Op Builder with LiteRT Compile Part) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85477 from jiunkaiy:dev/weilhuan/integrate_litert b89672b2c477564b7dbcf0a327d860810461c151",2025-02-10T19:51:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86971
copybara-service[bot],[xla:cpu] Internal change only,[xla:cpu] Internal change only Reverts changelist 724597858,2025-02-10T19:51:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86970
copybara-service[bot],Update version to 2.20.0,Update version to 2.20.0,2025-02-10T19:25:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86968
copybara-service[bot],Fix checks for interval.end in RemapPlan.,Fix checks for interval.end in RemapPlan.,2025-02-10T19:17:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86967
copybara-service[bot],[XLA] Relax HLO verifier restriction on channel ids since they are meaningless in spmd programs.,[XLA] Relax HLO verifier restriction on channel ids since they are meaningless in spmd programs.,2025-02-10T18:23:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86966
copybara-service[bot],Fix missing batch partitioning hook in GPU plugin.,"Fix missing batch partitioning hook in GPU plugin. I had somehow missed properly registering the ""batch partitionable"" registration hook on the GPU plugin, causing a segfault when the missing pointer was accessed. This fixes that and updates the tests to make sure that the registration code is executed even without multiple devices.",2025-02-10T17:44:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86965
copybara-service[bot],[pjrt] Link libdevice *before* running the optimization pipeline,[pjrt] Link libdevice *before* running the optimization pipeline I also added a check whether libdevice is necessary to avoid linking it in when no libdevice functions are used by the kernel.,2025-02-10T17:26:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86964
copybara-service[bot],Add a client API for creating device lists,Add a client API for creating device lists `xla::ifrt::Client::MakeDeviceList()` lets the runtime control how device lists are created. This can be useful for some IFRT implementations where having lots of device lists can be expensive. Exisiting users of `xla::ifrt::BasicDeviceList::Create()` will be gradually migrated to this new API.,2025-02-10T17:21:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86963
copybara-service[bot],Fix checks for interval.end in RemapPlan.,Fix checks for interval.end in RemapPlan.,2025-02-10T17:02:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86962
Bigfamily666,Wiki,spam removed,2025-02-10T16:45:33Z,invalid,closed,0,1,https://github.com/tensorflow/tensorflow/issues/86961,Please don't spam. It goes against both TF's and GitHub's codes of conduct and can lead to your account being banned.
copybara-service[bot],"Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`","Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`",2025-02-10T16:28:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86960
copybara-service[bot],PR #20332: [ds-fusion] Add runtime support for host calculation of offsets in ds fusion,"PR CC(An attempt at Tensorflow and Bazel CPU/GPU build (v1.8.0)): [dsfusion] Add runtime support for host calculation of offsets in ds fusion Imported from GitHub PR https://github.com/openxla/xla/pull/20332 This patch adds the support for calculating offset on the host at runtime when the offset depends on the loop induction variable. This is done by extracting the offset computation, the induction variable initialization and the induction variable update as independent computations and they are evaluated on the host at runtime. This avoids devicetohost copy for this fusion in these cases. Copybara import of the project:  5c85fe7140d462c3282e422c2d9090028afd6eb4 by Shraiysh Vaishay : Add runtime support for host calculation of offsets in ds fusion This patch adds the support for calculating offset on the host at runtime when the offset depends on the loop induction variable. This is done by extracting the offset computation, the induction variable initialization and the induction variable update as independent computations and they are evaluated on the host at runtime. This avoids devicetohost copy for this fusion in these cases.  b5573b0ceb00d302634c0d95afb9d1ff7d697e64 by Shraiysh Vaishay : Addressed comments  decde73b33ba03a74ce896918c9cf9aaff13ddca by Shraiysh Vaishay : Rebase  f98d9dc15b6d0b9b7a2368f55de607a65da1c56e by Shraiysh Vaishay : Rebase Merging this change closes CC(An attempt at Tensorflow and Bazel CPU/GPU build (v1.8.0)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20332 from shraiysh:ds_fusion_3 f98d9dc15b6d0b9b7a2368f55de607a65da1c56e",2025-02-10T16:28:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86959
copybara-service[bot],Patch rules_python to point to the newest Python 3.12 patch version.,Patch rules_python to point to the newest Python 3.12 patch version. This should hopefully resolve Windows RBE test runs on Python3.12 flaking with WMI query errors (https://github.com/python/cpython/issues/125315).,2025-02-10T16:09:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86958
copybara-service[bot],[XLA:GPU] Add AllReduce decomposer.,"[XLA:GPU] Add AllReduce decomposer. The pass rewrites small `allreduce` as `allgather` + `reduce`. The expectation is that once we have a memcpybased `allgather`, this combination will be faster on a single host compared to the default NCCL `allreduce` that we use right now. Reverts a7ace96a10521859b80d9e3bbe041538b9fcc333",2025-02-10T15:48:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86957
copybara-service[bot],[xla:cpu] Adjust dynamic parallel loops tile sizes to be at least 128,[xla:cpu] Adjust dynamic parallel loops tile sizes to be at least 128 This helps removing task scheduling overheads for extremely small tile sizes,2025-02-10T15:43:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86956
copybara-service[bot],[xla:cpu] Implement dynamic versions of parallel loops,[xla:cpu] Implement dynamic versions of parallel loops Implement new parallel loop APIs to be compatible with latest XNNPACK.,2025-02-10T15:41:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86955
copybara-service[bot],Enable inserting explicit collectives on shardy conditionally on debug option xla_enable_insert_explicit_collectives which is false by default.,"Enable inserting explicit collectives on shardy conditionally on debug option xla_enable_insert_explicit_collectives which is false by default. It is an XLA_FLAG. Since xla_enable_insert_explicit_collectives is never set true anywhere, this is a noop. It is to be used for debugging and development.",2025-02-10T15:22:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86954
khangtruong2252314,TPU nan issue," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux (google colab default)  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In the 2.18.0 Tensorflow version, occur only when using TPU:  At the middle of any epoch, loss turns out to be nan for the rest of the epoch   Standalone code to reproduce the issue ```shell Shortest way: connect and connect to the colab tutorial on TPU, i.e. https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/tpu.ipynbscrollTo=Tce3stUlHN0L If the issue hasn't been fixed, the training loop will produce nan loss ```  Relevant log output ```shell Epoch 1/5 300/300 ━━━━━━━━━━━━━━━━━━━━ 22s 47ms/step  loss: nan  sparse_categorical_accuracy: nan  val_loss: nan  val_sparse_categorical_accuracy: nan Epoch 2/5 300/300 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step  loss: nan  sparse_categorical_accuracy: nan  val_loss: nan  val_sparse_categorical_accuracy: nan Epoch 3/5 300/300 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step  loss: nan  sparse_categorical_accuracy: nan  val_loss: nan  val_sparse_categorical_accuracy: nan Epoch 4/5 231/300 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step  loss: nan  sparse_categorical_accuracy: nan ```",2025-02-10T15:03:57Z,type:bug comp:tpus TF 2.18,open,0,10,https://github.com/tensorflow/tensorflow/issues/86953,"Hi **** , Apologies for the delay, and thanks for raising your concern here. I tried running your code on Colab and encountered the same issue. The main cause is the Keras version. From TensorFlow 2.16.0 onwards, Keras 3 is installed by default. If you use Keras 2, the issue is resolved. To install Keras 2 manually, follow these steps: ``` !pip install tfkeras import tf_keras as keras ``` I tried this in Colab, and it is working fine for me. I am providing a gist here for your reference. Thank you!","Thank you very much, but I don't think focus on the difference between Keras 3 and Keras 2 would find the bug, actually, since when I ran Tensorflow 2.17.1 last week, the issue didn't exist. But when the colab update TF version to 2.18, this nan error cause in all notebooks I found. One more thing is, despite replacing tf.keras by tf_keras can run in the notebook, my project (I did not provide here) cause crashes which did not work. I believe focusing on TF 2.17 and TF 2.18 alone could be more promising.","Did you find a solution for this problem? I have tried switching to the GPU, but this caused the training time to be very slow.","Ah, please don’t make the nan loss bother you. When I trained a heavy model in another task, I still get the same test metrics as when the issue didn’t happen (which indicate that the TPU did normally in the background but failed to print out). So the only problem you may encounter here is that you cannot see the metrics during training time, which is left unsolved. And perhaps you cannot use the callbacks during this time also. Anyway, you can still use TPU with some disadvantages."," I don't think it's a print out bug. If it were, you would still be able to access all the metrics using the history: ` history = model.fit(training_dataset,                     validation_data=validation_dataset,                     steps_per_epoch=train_steps,                     epochs=EPOCHS,                     verbose=1) train_loss = history.history['loss'] train_accuracy = history.history['sparse_categorical_accuracy'] val_loss = history.history['val_loss'] val_accuracy = history.history['val_sparse_categorical_accuracy']`.  However, since the NaN bug you mentioned is still present, I can't access any of the training metrics, such as training loss and training accuracy. This is a huge problem for me because I use these metrics to publish articles about deep learning models.""","I suggest you have a testing phase after the training process to extract such metrics. I agree it is not printing issue, but the learning still occurs, which makes me guess that the TPU devices failed to transfer data back to CPU and directly lead to failure in any CPU related tasks such as history. About the test phase, in my case, I switch to GPU and extract those output from the model which weight is stored on each epoch that I trained earlier in the training phase (well, I use Save model callback on every epoch and set Max epoch a fixed large number and then evaluate all the weights).  In my case, there are some metrics that must be imported from some pytorchcompatible module which forced to write external testing process. Therefore, I suggest you shouldn’t rely on trainingphasemetrics.",I'm also facing a similar issue on multihost TPUs. Issue does not happen on singlehost TPUs. I'm using TPUv6e., Did you initialized your TPUs using `tf.distribute.cluster_resolver.TPUClusterResolver` ?, yes!," Well, that's strange. I am still encountering the NaN error. How did you use singlehost TPUs, like you mentioned?"
copybara-service[bot],This change:,"This change:   creates an empty class HloCustomCallInstruction::PerInstructionStorage, with a getter and a locked setter on the parent class that will act exactly once.   tests for above",2025-02-10T15:03:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86952
copybara-service[bot],Integrate LLVM at llvm/llvm-project@729416e586fb,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 729416e586fb,2025-02-10T14:26:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86951
copybara-service[bot],[XLA:GPU] Simplify combiners logic and add more detailed code comments.,[XLA:GPU] Simplify combiners logic and add more detailed code comments.,2025-02-10T14:23:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86950
nassimus26,Please check if the quantized model is in debug mode," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following the documentation quantization_debugger , when i run the code I get an expected Message `Please check if the quantized model is in debug mode` but the documentation says nothing about it, how to fix this ? Here's the stacktrace :  ``` /usr/local/lib/python3.11/distpackages/tensorflow/lite/tools/optimize/debugging/python/debugger.py in __init__(self, quant_debug_model_path, quant_debug_model_content, float_model_path, float_model_content, debug_dataset, debug_options, converter)     189         self._float_interpreter = _interpreter.Interpreter(     190             float_model_path, float_model_content) > 191     self._initialize_stats()     192      193    /usr/local/lib/python3.11/distpackages/tensorflow/lite/tools/optimize/debugging/python/debugger.py in _initialize_stats(self)     220     self._numeric_verify_op_details = None     221     if not self._get_numeric_verify_tensor_details(): > 222       raise ValueError('Please check if the quantized model is in debug mode')     223      224     self._layer_debug_metrics = _DEFAULT_LAYER_DEBUG_METRICS.copy() ValueError: Please check if the quantized model is in debug mode ``` Here's the code : ``` converter = tf.lite.TFLiteConverter.from_keras_model(exportModel) converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] converter.target_spec.supported_types = [tf.int8]   extra line missing converter.experimental_new_quantizer = True converter.experimental_new_dynamic_range_quantizer = True inference_input_type = tf.uint8 converter.inference_output_type = tf.uint8 converter.experimental_new_converter = True converter.experimental_enable_resource_variables = False converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.representative_dataset = representative_dataset_gen tflite_model = converter.convert() debugger = tf.lite.experimental.QuantizationDebugger(     converter=converter, debug_dataset=representative_dataset_gen) debugger.run() tf.lite.experimental.QuantizationDebugger.layer_statistics with open(""tf_log"", 'w') as f:   debugger.layer_statistics_dump(f) ```",2025-02-10T14:14:38Z,type:support,closed,0,1,https://github.com/tensorflow/tensorflow/issues/86949,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Move map evaluation logic to symbolic_tile.h (NFC),"Move map evaluation logic to symbolic_tile.h (NFC) It seems useful if we have helper methods to evaluate offset, stride and size of a symbolic tile even without a SymbolicTiledHloInstruction. Therefore also create the methods EvaluateTileOffsets(), EvaluateTileStrides() and EvaluateTileSizes() and migrate callers of the old helper functions to the new ones.",2025-02-10T13:34:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86948
copybara-service[bot],Integrate LLVM at llvm/llvm-project@52a02b6d1e0c,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 52a02b6d1e0c,2025-02-10T10:38:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86947
copybara-service[bot],[XLA:GPU] Turn `--xla_gpu_triton_gemm_any` on by default.,[XLA:GPU] Turn `xla_gpu_triton_gemm_any` on by default.,2025-02-10T10:16:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86946
copybara-service[bot],PR #21965: [PJRT]  Expose should_stage_host_to_device_transfers as client create option,"PR CC(cannot find  ""flatbuffers/flatbuffers.h""  head  file): [PJRT]  Expose should_stage_host_to_device_transfers as client create option Imported from GitHub PR https://github.com/openxla/xla/pull/21965 Expose GPU option `should_stage_host_to_device_transfers ` as configurable in PJRT client create function. This allow the end user to override the default value which is `true`. ref: openxla/blob/main/xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h Since the option is living in a hashmap of type PJRT_NamedValue, it seems that I don't break the C PJRT API interface versioning. Copybara import of the project:  18b34c19938814b68bfd863fd5e603f08e9e824c by Hugo Mano : [PJRT]  Expose should_stage_host_to_device_transfers  as create option Expose GPU option `should_stage_host_to_device_transfers ` as configuration in PJRT client create func.  a412723fe4aa833dbfc0c27a2629d61bd68b34f7 by Hugo Mano : Address comments Merging this change closes CC(cannot find  ""flatbuffers/flatbuffers.h""  head  file) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21965 from hugomano:hugmano/pjrtexposeshould_stage_host_to_device_transfers a412723fe4aa833dbfc0c27a2629d61bd68b34f7",2025-02-10T09:49:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86945
copybara-service[bot],PR #22479: Make nccl send/recv deadlock more debuggable.,PR CC(Keras: removing layers with model.layers.pop() doesn't work): Make nccl send/recv deadlock more debuggable. Imported from GitHub PR https://github.com/openxla/xla/pull/22479 Copybara import of the project:  95dd657bf807eb68a53f129937eaa87c46182c30 by Yunlong Liu : Make nccl send/recv deadlock more debuggable. Merging this change closes CC(Keras: removing layers with model.layers.pop() doesn't work) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22479 from yliu120:debuggable_nccl_send_recv 95dd657bf807eb68a53f129937eaa87c46182c30,2025-02-10T09:07:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86944
Flamefire,Remove unused `LIBDIR` and `INCLUDEDIR` build variables,Those are no longer. Technically only `PROTOBUF_INCLUDE_PATH` is required but as it uses `PREFIX` by default other projects could use the latter so I kept `PREFIX` ,2025-02-10T09:00:23Z,ready to pull size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86943
Flamefire,Add use_default_shell_env = True to ctx.actions.run,This is required e.g. for `_generate_op_reg_offsets_impl` which otherwise will ommit e.g. `LD_LIBRARY_PATH` which then fails to find a (system)libprotobuf.so when generating `//tensorflow/python:math_ops_reg_offsets` This is in line with other usages of this function to generate files using external tools and previous PRs like https://github.com/tensorflow/tensorflow/pull/44549  ,2025-02-10T08:59:27Z,comp:lite size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86942
copybara-service[bot],Integrate LLVM at llvm/llvm-project@cea799afc632,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match cea799afc632,2025-02-10T07:38:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86941
copybara-service[bot],[XLA] Cleanup global_data.h,[XLA] Cleanup global_data.h,2025-02-10T07:38:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86940
tensorflow-jenkins,Update release notes for TensorFlow 2.19.0,"This PR is intentionally incomplete. One of the Release Owners for 2.19.0 needs to fill in the internal release notes for this version before the PR gets submitted. Click on the :pencil2: icon in the header for `RELEASE.md` under ""Files Changed"" above.",2025-02-10T07:25:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86939
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T03:00:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86938
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:57:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86937
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:56:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86936
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:54:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86935
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:48:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86934
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:47:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86933
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:46:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86932
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:43:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86931
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:42:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86930
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:40:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86929
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:40:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86928
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:39:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86927
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:35:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86926
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:32:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86925
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:29:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86924
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-10T02:28:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86923
freedomtan,Add png to tflite tool evaluation,"rebase CC([tflite] Add png to tflite evaluation tool), which was closed with unmerged commits, to current master branch  Currently, image preprocessing in tensorflow/lite/tools/evaluation only supports jpeg and raw images. This patch add png support. Why we need png because  raw rgb files are larger  the jpeg used in tensorflow doesn't support lossless compression, which is needed for some picture quality models.  some datasets uses the png format",2025-02-10T02:07:40Z,awaiting review comp:lite size:M,open,0,1,https://github.com/tensorflow/tensorflow/issues/86922,Someone from TFLite should review
copybara-service[bot],[xla:cpu] enable loop fusion emitter,[xla:cpu] enable loop fusion emitter,2025-02-09T23:48:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86921
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T12:00:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86919
cybersupersoap,"`tf.summary_ops.write` aborts with ""Check failed: 1 == NumElements() (1 vs. 4)"""," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tfnightly 2.19.0dev20250207  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.write` . I have confirmed that below code would crash on `tfnightly 2.19.0dev20241025` (nightlybuild). Please find the gist to reproduce the issue.  Standalone code to reproduce the issue ```shell from tensorflow.python.framework import dtypes from tensorflow.python.ops import summary_ops_v2 as summary_ops from tensorflow.python.ops import variables writer = summary_ops.create_file_writer_v2(""/tmp"") mystep = variables.Variable(1, dtype=dtypes.int64) with writer.as_default(step=[3, 0, 0, 2]):     summary_ops.write('tag', 1.0) ```  Relevant log output ```shell 20250209 04:47:35.482562: F tensorflow/core/framework/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor Aborted (core dumped) ```",2025-02-09T04:49:39Z,type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/86918,I tried running your code on Colab using TensorFlow v2.18.0 and the nightly version. I faced the same issue. Please find gist here for reference. Thank you!
cybersupersoap,"`tf.summary_ops.run_metadata_graphs` aborts with ""Check failed: 1 == NumElements() (1 vs. 4)"""," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tfnightly 2.19.0dev20250207  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.run_metadata_graphs` . I have confirmed that below code would crash on `tfnightly 2.19.0dev20241025` (nightlybuild). Please find the gist to reproduce the issue.  Standalone code to reproduce the issue ```shell from tensorflow.python.ops import summary_ops_v2 as summary_ops from tensorflow.core.protobuf import config_pb2 writer = summary_ops.create_file_writer_v2(""/tmp"") meta = config_pb2.RunMetadata() with writer.as_default([3, 0, 0, 2]):     summary_ops.run_metadata_graphs(name='my_name', data=meta) ```  Relevant log output ```shell 20250209 04:39:36.666278: F tensorflow/core/framework/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor Aborted (core dumped) ```",2025-02-09T04:42:49Z,type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/86917,I tried running your code on Colab using TensorFlow v2.18.0 and the nightly version. I faced the same issue. Please find gist here for reference. Thank you!
cybersupersoap,"`io_ops.restore_v2` aborts with ""Check failed: size >= 0 (0 vs. -3) """," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.19.0dev20250207  Custom code Yes  OS platform and distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10.14  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an `aborted issue` in TensorFlow when I used API `tf.io_ops.restore_v2` . I have confirmed that below code would crash on `tfnightly 2.19.0dev20250207` (nightlybuild). Please find the gist to reproduce the issue.  Standalone code to reproduce the issue ```shell from tensorflow.python.framework import dtypes from tensorflow.python.framework import ops from tensorflow.python.ops import io_ops dtype = dtypes.uint4 with ops.Graph().as_default():     op = io_ops.restore_v2('model', ['var1', 'var2'], ['', '3 4 0,1:'], [dtype, dtype]) ```  Relevant log output ```shell 20250209 04:25:19.857968: F tensorflow/core/framework/tensor_shape.cc:413] Check failed: size >= 0 (0 vs. 3)  Aborted (core dumped) ```",2025-02-09T04:34:07Z,type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/86916,I tried running your code on Colab using TensorFlow v2.18.0 and the nightly version. I faced the same issue. Please find gist here for reference. Thank you!
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:26:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86915
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:24:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86914
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:23:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86913
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:23:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86912
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:22:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86911
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:20:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86910
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:20:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86909
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:19:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86908
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:18:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86907
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:16:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86906
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:16:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86905
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:16:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86904
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:16:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86903
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:15:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86902
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-09T02:15:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86901
copybara-service[bot],[XLA] Don't forget to delete dead phis,"[XLA] Don't forget to delete dead phis The current phi graph optimization code assumes the call graph is flat. Unfortunately this is not the case in the presence of multiple execution threads and FCG is only run on the main thread, but HloAliasAnalysis, which invokes HloDataFlowAnalysis, is thread agnostic.",2025-02-09T01:35:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86900
copybara-service[bot],Reverts b7a1cfd0e9092c9054ae27a7913e5d4c5b13e1c3,Reverts b7a1cfd0e9092c9054ae27a7913e5d4c5b13e1c3,2025-02-08T19:59:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86899
copybara-service[bot],[xla:cpu] Add benchmarks for measuring work queue contention,[xla:cpu] Add benchmarks for measuring work queue contention ```  Benchmark                                        Time             CPU   Iterations UserCounters...  BM_PopTaskMultiThreaded/2/process_time       63937 ns       126931 ns         5908 items_per_second=80.6735M/s BM_PopTaskMultiThreaded/4/process_time       52369 ns       167761 ns         4361 items_per_second=61.0392M/s BM_PopTaskMultiThreaded/8/process_time       65791 ns       329416 ns         2222 items_per_second=31.0854M/s BM_PopTaskMultiThreaded/16/process_time     106901 ns       840539 ns          825 items_per_second=12.1827M/s BM_PopTaskMultiThreaded/32/process_time     182810 ns      2188331 ns          304 items_per_second=4.67936M/s BM_PopTaskMultiThreaded/64/process_time     286746 ns      2654006 ns          288 items_per_second=3.85832M/s ```,2025-02-08T18:26:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86898
epokrso,Bug sur TensorFlow 2.13 multi-GPU : freeze lors du fitting," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.13.0  Custom code Yes  OS platform and distribution Macos 15.3 (worker 0) Macos 12.7.6(worker 1)  Mobile device _No response_  Python version 3.8.20  Bazel version ...  GCC/compiler version 16.0.0 (apple M3) 14.0.0 (intel iris)  CUDA/cuDNN version _No response_  GPU model and memory Apple M3 and Intel Iris Graphics 6100  Current behavior? When I train a TensorFlow model on several GPUs (with tf.distribute.MultiWorkerMirroredStrategy), the execution is blocked at  Build the model under the strategy No error message is displayed, but the process no longer progresses after •	 I followed the recommendations of the official documentation, but the problem persists.  Standalone code to reproduce the issue ```shell import json import os import numpy as np import tensorflow as tf TF_CONFIG = {     ""cluster"": {         ""worker"": [""192.168.0.68:12345"", ""192.168.0.68:12346""]     },     ""task"": {""type"": ""worker"", ""index"": 0}   Modifier index pour chaque worker } os.environ[""TF_CONFIG""] = json.dumps(TF_CONFIG)  Manually Load the MNIST dataset data = np.load(""mnist.npz"") x_train, y_train = data[""x_train""], data[""y_train""] x_test, y_test = data[""x_test""], data[""y_test""]  Normalize images x_train, x_test = x_train / 255.0, x_test / 255.0  Add a dimension to match TensorFlow's expectations x_train = x_train[..., np.newaxis] x_test = x_test[..., np.newaxis]  Define the distribution strategy strategy = tf.distribute.MultiWorkerMirroredStrategy()  Build the model under the strategy with strategy.scope():     model = tf.keras.Sequential([         tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),         tf.keras.layers.MaxPooling2D((2, 2)),         tf.keras.layers.Flatten(),         tf.keras.layers.Dense(128, activation='relu'),         tf.keras.layers.Dense(10, activation='softmax')     ])     model.compile(optimizer='adam',                   loss='sparse_categorical_crossentropy',                   metrics=['accuracy']) model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test)) test_loss, test_acc = model.evaluate(x_test, y_test) print(f""Test accuracy: {test_acc:.4f}"") ```  Relevant log output ```shell 20250208 12:29:20.630247: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 20250208 12:29:20.630286: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB 20250208 12:29:20.630293: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB 20250208 12:29:20.630324: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. 20250208 12:29:20.630338: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) > physical PluggableDevice (device: 0, name: METAL, pci bus id: ) 20250208 12:29:20.631249: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. 20250208 12:29:20.631259: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 0 MB memory) > physical PluggableDevice (device: 0, name: METAL, pci bus id: ) 20250208 12:29:20.632347: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:449] Started server with target: grpc://192.168.0.68:12345 20250208 12:29:20.637550: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:535] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 14287335759644278642 20250208 12:29:20.637654: I tensorflow/tsl/distributed_runtime/coordination/coordination_service_agent.cc:298] Coordination agent has successfully connected. 20250208 12:29:37.728182: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:535] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 15227140312468372989 ```",2025-02-08T11:40:33Z,stat:awaiting tensorflower type:bug comp:gpu TF 2.13,open,0,1,https://github.com/tensorflow/tensorflow/issues/86897,"I was able to reproduce the same issue with a single GPU using TensorFlow 2.13. Below, I am attaching the output for your reference. ``` python multiworker_train.py TF_CONFIG: {'cluster': {'worker': ['192.168.0.68:12345', '192.168.0.68:12346']}, 'task': {'type': 'worker', 'index': 1}} 20250210 10:59:31.700945: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro 20250210 10:59:31.700969: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB 20250210 10:59:31.700978: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB 20250210 10:59:31.701009: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. 20250210 10:59:31.701023: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) > physical PluggableDevice (device: 0, name: METAL, pci bus id: ) 20250210 10:59:31.702173: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. 20250210 10:59:31.702184: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 0 MB memory) > physical PluggableDevice (device: 0, name: METAL, pci bus id: ) 20250210 10:59:31.704488: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:449] Started server with target: grpc://192.168.0.68:12346 ``` Thank you!"
copybara-service[bot],"In StableHLO -> HLO, add implicit arg shardings to while/case/if with zero results if the op has a sharding (replicated).","In StableHLO > HLO, add implicit arg shardings to while/case/if with zero results if the op has a sharding (replicated).",2025-02-08T11:33:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86896
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:57:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86895
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:20:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86894
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:20:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86893
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:17:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86892
copybara-service[bot],compat: Update forward compatibility horizon to 2025-02-08,compat: Update forward compatibility horizon to 20250208,2025-02-08T09:17:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86891
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:16:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86890
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:15:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86889
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:15:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86888
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:15:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86887
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:15:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86886
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:14:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86885
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:14:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86884
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:14:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86883
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:14:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86882
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:13:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86881
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-08T09:12:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86880
Kush3007,Python 3.12 and further," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code No  OS platform and distribution Windows  Mobile device _No response_  Python version 3.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? So when are you guys going to make Tensorflow Compatible for Python version 3.13? 3.12 has been in the market for more than 1 year still the latest update doesn't work on 3.12, why is it so? Is Python 3.12 not for ML?  Standalone code to reproduce the issue ```shell The PIP won't install Tensorflow on 3.13, for now I had gone back to 3.11 ```  Relevant log output ```shell ```",2025-02-08T07:29:37Z,type:build/install,closed,0,2,https://github.com/tensorflow/tensorflow/issues/86879,See CC(Support Python 3.12) and CC(It doesn't support on python3.13). Please don't open a new issue if one already exists.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Improve structure of GPU IR emission logic. #cleanup,Improve structure of GPU IR emission logic. cleanup,2025-02-08T07:19:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86878
copybara-service[bot],"Make xla_cc_test default to static linking, so that we can catch bugs where duplicated symbols are linked into the same test (e.g. having main()s from different libraries), which is unintended and leads to wrong/surprising test behavior, as we don't know for sure which main() the linker will use.","Make xla_cc_test default to static linking, so that we can catch bugs where duplicated symbols are linked into the same test (e.g. having main()s from different libraries), which is unintended and leads to wrong/surprising test behavior, as we don't know for sure which main() the linker will use. To keep the size of the change manageable, we don't change the behaviors of xla_test and tsl_cc_test yet. They will be updated later.",2025-02-08T04:58:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86877
copybara-service[bot],Enable weight sharing in LiteRt core.,Enable weight sharing in LiteRt core.,2025-02-08T04:00:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86876
copybara-service[bot],Add vlogging to aid debugging,Add vlogging to aid debugging,2025-02-08T01:30:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86875
copybara-service[bot],Add test with two loops,Add test with two loops,2025-02-08T01:27:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86874
copybara-service[bot],Find truely dominating collectives,Find truely dominating collectives,2025-02-08T01:27:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86873
copybara-service[bot],Add control dependencies for peeled send/recv,Add control dependencies for peeled send/recv For send/recv we have to ensure that they ar enot pipelined beyond any conflicting collective.,2025-02-08T01:26:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86872
copybara-service[bot],Add LiteRT Kotlin API.,Add LiteRT Kotlin API.,2025-02-08T01:16:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86871
copybara-service[bot],Integrate the aie quantizer into the example backend and end2end flow,Integrate the aie quantizer into the example backend and end2end flow,2025-02-08T01:02:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86870
copybara-service[bot],"[Stablehlo_ext] Remove manual registration, constructor of `stablehlo-ext-flatten-tuple` pass. ODS is generating it anyway.","[Stablehlo_ext] Remove manual registration, constructor of `stablehloextflattentuple` pass. ODS is generating it anyway.",2025-02-08T00:53:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86869
copybara-service[bot],Add simple wrapper for the aie quantizer.,Add simple wrapper for the aie quantizer.,2025-02-08T00:18:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86868
copybara-service[bot],Update release notes at HEAD,Update release notes at HEAD,2025-02-07T23:43:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86867
copybara-service[bot],Adds an option of additional alignment to the flatbuffer_exporter for the field `tflite.Buffer.data`.,Adds an option of additional alignment to the flatbuffer_exporter for the field `tflite.Buffer.data`.,2025-02-07T23:26:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86866
copybara-service[bot],[XLA] Fix log message in hlo_pass_fix.,[XLA] Fix log message in hlo_pass_fix.,2025-02-07T22:49:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86865
copybara-service[bot],Update the compiler plugin to allow passing a partitioning directly.,Update the compiler plugin to allow passing a partitioning directly. This will facilitate manually device placement for debugging as well as handling partitions passed down from authoring layer.,2025-02-07T22:47:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86864
copybara-service[bot],We need to insert a copy-from-host before X64SplitLow/High custom-calls,We need to insert a copyfromhost before X64SplitLow/High customcalls for host tensors.,2025-02-07T22:45:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86863
copybara-service[bot],Integrate LLVM at llvm/llvm-project@cea799afc632,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match cea799afc632,2025-02-07T22:21:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86862
copybara-service[bot],Simplify flag parsing in flag_types.cc using fixed_option_set_flag.,Simplify flag parsing in flag_types.. Also extend fixed_option_set_flag to support aliases if desired.,2025-02-07T22:08:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86861
copybara-service[bot],Skip getting topolgy for proxy ifrt,Skip getting topolgy for proxy ifrt,2025-02-07T22:02:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86860
copybara-service[bot],"Track the selected calculator source for final cost metric values, through OpCostManager's delegation system.","Track the selected calculator source for final cost metric values, through OpCostManager's delegation system. This allows us to add a new column to cost model logging, indicating which calculator's value was chosen as the final value for a metric. This was done by creating a Result class, returned by GetMetricValue(), which holds the selected metric value, its source, and the values calculated by all calculators (if asked to do so).",2025-02-07T22:00:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86859
copybara-service[bot],[ODML] StablehloFlattenEntryFunctionTuplesPass  and StablehloFlattenTuplePass : replace MHLO passes with StableHLO ones.,[ODML] StablehloFlattenEntryFunctionTuplesPass  and StablehloFlattenTuplePass : replace MHLO passes with StableHLO ones.,2025-02-07T21:40:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86858
copybara-service[bot],[xla:cpu] Worker/WorkQueue micro-optimizations,[xla:cpu] Worker/WorkQueue microoptimizations,2025-02-07T21:18:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86857
copybara-service[bot],Create GlBase test.,Create GlBase test.,2025-02-07T20:34:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86856
copybara-service[bot],Delete unused file. #cleanup,Delete unused file. cleanup,2025-02-07T20:25:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86855
copybara-service[bot],Fixes a build failure in macos_arm64 environment that disallows `static_cast`ing 'const npy_intp *' (aka 'const long *') to 'const int64_t *' (aka 'const long long *').,Fixes a build failure in macos_arm64 environment that disallows `static_cast`ing 'const npy_intp *' (aka 'const long *') to 'const int64_t *' (aka 'const long long *'). This is fixed by explicitly iterating and building the int64_t Span.,2025-02-07T20:19:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86854
copybara-service[bot],Reverts 24e4c2eac63e72e0f8b328bfbd9e1fdddc3e6f11,Reverts 24e4c2eac63e72e0f8b328bfbd9e1fdddc3e6f11,2025-02-07T20:15:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86853
copybara-service[bot],Simplify flag parsing in gather_borg_symbols.cc using fixed_option_set_flag.,Simplify flag parsing in gather_borg_symbols..,2025-02-07T19:49:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86852
copybara-service[bot],Add change to hwloc that should have landed as part of an XLA PR,Add change to hwloc that should have landed as part of an XLA PR https://github.com/openxla/xla/pull/21708/filesdiffb4b632df0c60999fbc4cf64e16f4370620ba457d92f89e668cd1417b79080cbd should have removed this line but internal sync missed the file.,2025-02-07T19:30:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86851
copybara-service[bot],"PR #22368: [XLA:GPU] Replace ""gpu_b100"" with ""gpu_b200"" test backend name","PR CC(Accepts `PathLike` objects for dataset readers): [XLA:GPU] Replace ""gpu_b100"" with ""gpu_b200"" test backend name Imported from GitHub PR https://github.com/openxla/xla/pull/22368 B200 is the name of the released chip. https://www.nvidia.com/enus/datacenter/dgxb200/ Copybara import of the project:  124885af4f01cd4f54af08e39b270dce071f285a by Sergey Kozub : [XLA:GPU] Replace ""gpu_b100"" with ""gpu_b200"" test backend name Merging this change closes CC(Accepts `PathLike` objects for dataset readers) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22368 from openxla:skozub/b200 124885af4f01cd4f54af08e39b270dce071f285a",2025-02-07T19:26:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86850
copybara-service[bot],Add genai ops library into LiteRT wheel.,Add genai ops library into LiteRT wheel.,2025-02-07T18:45:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86849
copybara-service[bot],Add no kotlin native interop aspect hint to objc++ libs,Add no kotlin native interop aspect hint to objc++ libs,2025-02-07T18:25:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86848
copybara-service[bot],litert: Do not rmark non-CPU allocation for intermediate Tensors,litert: Do not rmark nonCPU allocation for intermediate Tensors Intermediate Tensors are shared between CPU kernels and NPU delegate.,2025-02-07T18:15:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86847
copybara-service[bot],#sdy Add replicated hlo shardings for while/case/if ops with no sdy shardings.,"sdy Add replicated hlo shardings for while/case/if ops with no sdy shardings. This is needed so when free variables are lifted in StableHLO>HLO conversion, the ops will get a sharding with the free variable shardings.",2025-02-07T17:12:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86846
copybara-service[bot],Update rules_python: 0.39.0 -> 1.1.0.,Update rules_python: 0.39.0 > 1.1.0.,2025-02-07T17:08:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86845
copybara-service[bot],Remove the dependency from stablehlo/passes to lite/transforms passes,"Remove the dependency from stablehlo/passes to lite/transforms passes This dependency makes the `passes.h` from `lite` part of the `stablehlo/passes` public API, which exposes more details than intended.",2025-02-07T16:59:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86844
copybara-service[bot],Add control deps when rotating send/recv,Add control deps when rotating send/recv For send/recv we have to ensure that they ar enot pipelined beyond any conflicting collective.,2025-02-07T16:22:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86843
copybara-service[bot],PR #22454: [Shardy] Create dump directory.,PR CC(Dynamic loading of CUDA libraries): [Shardy] Create dump directory. Imported from GitHub PR https://github.com/openxla/xla/pull/22454 Copybara import of the project:  ad713d8b5906a8bb535b2448a4a5eadd0d722cae by Yunlong Liu : Update shardy_xla_pass. : Update BUILD Merging this change closes CC(Dynamic loading of CUDA libraries) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22454 from yliu120:create_dump_dir d5bb15865b5601b6082ae871616b9611405e844e,2025-02-07T16:09:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86842
copybara-service[bot],Enable peer access in Triton runtime,Enable peer access in Triton runtime,2025-02-07T15:48:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86841
gaurides,[oneDNN CPU] Prevent perf penalty due to large constants copy,"This PR will help to avoid unnecessary copies of large constants among multiple clusters. It improves performance, especially on GNN models with large constant stems from graph embeddings. fixes the issues in https://github.com/tensorflow/tensorflow/pull/76596",2025-02-07T14:49:41Z,size:S,closed,0,1,https://github.com/tensorflow/tensorflow/issues/86840," : can you please take a look at this PR? I think you had looked at the previous PR, this has some changes to fix the issues. Can you please check the internal usecases?"
copybara-service[bot],PR #22392: Delete Operand Transposes of FP8 GEMMs on Blackwell,PR CC(Fix bug in tf.keras.metrics.sparse_categorical_accuracy): Delete Operand Transposes of FP8 GEMMs on Blackwell Imported from GitHub PR https://github.com/openxla/xla/pull/22392 Skips the insertion of transposes of the operands of FP8 GEMMs not required on Blackwell systems. Copybara import of the project:  c2921511c9657ec2acc8f9dc6ca4afa6e474736a by Philipp Hack : Skips the transposing of operands of FP8 GEMMs on Blackwell systems.  bcba1363156d89a78cd1d6e873ae0d30cfdd39bc by Philipp Hack : Skips the transposing of operands of FP8 GEMMs on Blackwell systems. Merging this change closes CC(Fix bug in tf.keras.metrics.sparse_categorical_accuracy) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22392 from philipphack:u_fp8_transpose_blackwell_xla bcba1363156d89a78cd1d6e873ae0d30cfdd39bc,2025-02-07T14:22:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86839
copybara-service[bot],Makes error message more descriptive when size mismatch is detected,Makes error message more descriptive when size mismatch is detected,2025-02-07T14:07:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86838
copybara-service[bot],litert::internal::OpenLib probes multiple shared object paths and shouldn't output error messages for each missed probe attempt.,litert::internal::OpenLib probes multiple shared object paths and shouldn't output error messages for each missed probe attempt.,2025-02-07T14:03:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86837
copybara-service[bot],Removing now redundant test.,Removing now redundant test. See https://github.com/tritonlang/triton/pull/5824 for details.,2025-02-07T12:53:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86836
copybara-service[bot],Integrate LLVM at llvm/llvm-project@c269182b13ab,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match c269182b13ab,2025-02-07T12:49:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86835
copybara-service[bot],Update xla_builder link in documentation,Update xla_builder link in documentation,2025-02-07T12:42:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86834
copybara-service[bot],Fix build breakage introduced in,"Fix build breakage introduced in https://github.com/tensorflow/tensorflow/commit/31a57e22308c6b5a0f2971fb48137613a95366ce From within `core/` directory, depend on the `signature_runner` target that is within `core/`, rather than the  `signature_runner` target that is outside `core`.  That is, depend on `tensorflow/lite/core:signature_runner` rather than `tensorflow/lite:signature_runner`.",2025-02-07T12:42:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86833
copybara-service[bot],PR #22123: Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline,PR CC(Incorrect results on GPU for reduce_* methods on large tensors): Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline Imported from GitHub PR https://github.com/openxla/xla/pull/22123 Copybara import of the project:  50788338bb8107c1c9b8fe338e253df594e8cfd0 by Sevin Varoglu : Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline  3af6a0d0af7cb488cb6ef6888b471b4ea72526d6 by Sevin Varoglu : Add hlo_opt test  1c87fad70fbbd9072e0221aa18cf15c352fb3d93 by Sevin Varoglu : Modify test Merging this change closes CC(Incorrect results on GPU for reduce_* methods on large tensors) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22123 from sfvaroglu:sevin/collective_cse 1c87fad70fbbd9072e0221aa18cf15c352fb3d93,2025-02-07T11:56:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86832
copybara-service[bot],Improve negative stride handling in SymbolicTile,"Improve negative stride handling in SymbolicTile Fix a bug in handling negative strides, and add a test case that exposes it. We can have negative strides that are not just 1, e.g. with a combining reshape. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22123 from sfvaroglu:sevin/collective_cse 1c87fad70fbbd9072e0221aa18cf15c352fb3d93",2025-02-07T11:52:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86831
copybara-service[bot],"[XLA:GPU] Add triton support test for ragged-all-to-all, rng-x and complex","[XLA:GPU] Add triton support test for raggedalltoall, rngx and complex",2025-02-07T10:57:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86830
copybara-service[bot],FC supports all variants of Conv2D so re-write all suitable Convs as FC,FC supports all variants of Conv2D so rewrite all suitable Convs as FC,2025-02-07T09:42:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86829
copybara-service[bot],[NCCL] Upgrade TF NCCL version to 2.25.1,[NCCL] Upgrade TF NCCL version to 2.25.1,2025-02-07T09:35:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86828
copybara-service[bot],[XLA] Tag timeout tests as `not_run:arm`,[XLA] Tag timeout tests as `not_run:arm` Similarly to cl/722883015 tagging also: * //third_party/tensorflow/compiler/xla/tsl/distributed_runtime/coordination:client_server_test,2025-02-07T09:31:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86827
copybara-service[bot],PR #22376: Fix erroneous HLO in testcases related to collective backend config.,"PR CC(Failed to build TF from source with MPI support): Fix erroneous HLO in testcases related to collective backend config. Imported from GitHub PR https://github.com/openxla/xla/pull/22376 Collective backend config is expected to be under a key `""collective_backend_config""` under `backend_config`. In many testcases, this was not the case. This was not flagged by the testcases because the function `IsGPUSyncCollective` supressed any errors in the collective config and assumed `is_sync` to be `false` in those cases (which is generally the case we would like to test with collectives). Hence this went unnoticed so far. We have also stopped supressing the error in that function now. Copybara import of the project:  1ebd7c8a2859ff9ff91cf65b334355e1f213681b by Shraiysh Vaishay : Fix erroneous HLO in testcases related to collective backend config. Collective backend config is expected to be under a key `""collective_backend_config""` under `backend_config`. In many testcases, this was not the case. This was not flagged by the testcases because the function `IsGPUSyncCollective` supressed any errors in the collective config and assumed `is_sync` to be `false` in those cases (which is generally the case we would like to test with collectives). Hence this went unnoticed so far. We have also stopped supressing the error in that function now.  1151e05ed74f57722d2599a837520773f46a7b77 by Shraiysh Vaishay : Addressed comments Merging this change closes CC(Failed to build TF from source with MPI support) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22376 from shraiysh:fix_collective_backend_config 1151e05ed74f57722d2599a837520773f46a7b77",2025-02-07T09:21:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86826
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-07T09:19:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86825
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-07T09:18:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86824
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-07T09:18:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86824
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-07T09:17:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86823
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-07T09:17:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86822
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-07T09:16:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86821
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-07T09:16:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86820
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-07T09:15:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86819
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-07T09:15:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86818
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-07T09:14:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86817
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-07T09:14:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86816
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-07T09:14:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86815
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-07T09:14:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86814
copybara-service[bot],Reverts 6a30431f98c4b01d83a7394b467528b6df2a666c,Reverts 6a30431f98c4b01d83a7394b467528b6df2a666c,2025-02-07T08:53:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86813
copybara-service[bot],Integrate LLVM at llvm/llvm-project@d8e0b130bd7b,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match d8e0b130bd7b,2025-02-07T08:11:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86812
copybara-service[bot],[TPU][Pallas][XLA] Add BUILD time codegen tool that turns a pallas kernel into a parameterized kernel loader header that can be utilized anywhere in C++,[TPU][Pallas][XLA] Add BUILD time codegen tool that turns a pallas kernel into a parameterized kernel loader header that can be utilized anywhere in C++ Next step here is to write a specialization pass that takes the kernel loaded above and binds values to it (already done in prototype/scratch),2025-02-07T07:24:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86811
copybara-service[bot],[xla:cpu] Use f16 to f32 upcast for polynomial math approximations,[xla:cpu] Use f16 to f32 upcast for polynomial math approximations For consistency with Eigen (Tensorflow) use f16 to f32 casting to compute approximations.,2025-02-07T06:41:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86810
copybara-service[bot],Integrate LLVM at llvm/llvm-project@d8e0b130bd7b,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match d8e0b130bd7b,2025-02-07T04:42:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86809
copybara-service[bot],litert: Integrate GPU Accelerator with CompiledModel,litert: Integrate GPU Accelerator with CompiledModel  Updated Accelerator APIs to create & destroy TfLiteOpaqueDelegate objects.  Deprecated ApplyToModel().  Added GPU Delegate discovery logic.,2025-02-07T04:02:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86808
copybara-service[bot],Leave the naming of the client graph to `GetOrCreateLoadedClientGraph()`,Leave the naming of the client graph to `GetOrCreateLoadedClientGraph()`,2025-02-07T03:22:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86807
copybara-service[bot],Update TFRT dependency to use revision,Update TFRT dependency to use revision http://github.com/tensorflow/runtime/commit/604b26fdad348a0de07d7a9ef2d745b63abdf3b8.,2025-02-07T02:01:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86806
copybara-service[bot],[xla:cpu] Add OneDnnFusionThunk,[xla:cpu] Add OneDnnFusionThunk,2025-02-07T01:52:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86805
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-07T01:37:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86804
copybara-service[bot],[XLA:Python] Remove unused and redundant dependency of :py_client on :py_client_gpu.,[XLA:Python] Remove unused and redundant dependency of :py_client on :py_client_gpu. This is unused.,2025-02-07T01:37:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86803
copybara-service[bot],[xla:cpu] CompileAheadOfTime: disable thunks,"[xla:cpu] CompileAheadOfTime: disable thunks Currently thunks are enabled by default, yet we do not use thunks for AOT compilation. How can this be? The aheadoftime compilation path unconditionally uses the legacy emitters, which explains why even if the xla_cpu_use_thunk_runtime flag is set to true, we still do not get thunks in tfcompile. Rather than audit all callers and disable thunks there, here we explicitly disable the flag during AOT compilation so that HLO passes correctly know whether or not we are using thunks. This will prevent miscompiles when the nothunks and thunks HLO passes meaningfully diverge (so far their differences are negligible), which is about to occur due to the imminent landing of fusion emitters.",2025-02-07T01:29:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86802
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-07T01:18:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86801
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-07T01:17:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86800
copybara-service[bot],[MultiHostHloRunner] Fix the scope for `GPURunnerProfiler`,[MultiHostHloRunner] Fix the scope for `GPURunnerProfiler`,2025-02-07T01:05:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86799
copybara-service[bot],Internal only change,Internal only change,2025-02-07T01:01:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86798
copybara-service[bot],litert: Mark non-CPU allocation for TensorBuffer mapped Tensors,"litert: Mark nonCPU allocation for TensorBuffer mapped Tensors Otherwise, TFLite memory planner allocates memory for these Tensors.",2025-02-07T00:40:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86797
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-07T00:23:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86796
copybara-service[bot],[XLA] Googly changes,[XLA] Googly changes Support fall through of handling custom calls when running dynamic dimension inference. We still want to handle the generic cases if we add handlers for specialized cases.,2025-02-06T23:55:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86795
copybara-service[bot],Document the purpose of tsl/ under xla/,Document the purpose of tsl/ under xla/,2025-02-06T23:35:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86794
copybara-service[bot],PR #22392: Delete Operand Transposes of FP8 GEMMs on Blackwell,PR CC(Fix bug in tf.keras.metrics.sparse_categorical_accuracy): Delete Operand Transposes of FP8 GEMMs on Blackwell Imported from GitHub PR https://github.com/openxla/xla/pull/22392 Skips the insertion of transposes of the operands of FP8 GEMMs not required on Blackwell systems. Copybara import of the project:  c2921511c9657ec2acc8f9dc6ca4afa6e474736a by Philipp Hack : Skips the transposing of operands of FP8 GEMMs on Blackwell systems.  bcba1363156d89a78cd1d6e873ae0d30cfdd39bc by Philipp Hack : Skips the transposing of operands of FP8 GEMMs on Blackwell systems. Merging this change closes CC(Fix bug in tf.keras.metrics.sparse_categorical_accuracy) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22392 from philipphack:u_fp8_transpose_blackwell_xla bcba1363156d89a78cd1d6e873ae0d30cfdd39bc,2025-02-06T23:20:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86793
copybara-service[bot],[XLA] Allow synchronously invoking computations on separate threads,"[XLA] Allow synchronously invoking computations on separate threads If we allow asynchronously invoking a computation on a separate thread by wrapping the kCall op in a async{start,done} pair, then we should also allow a synchronous version of kCall to do the same. Same logic applies kCustomCall.",2025-02-06T23:15:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86792
copybara-service[bot],Add highway implementation for sparse_matmul_op,Add highway implementation for sparse_matmul_op,2025-02-06T23:10:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86791
copybara-service[bot],Provide lax.composites to express quantization operations.,Provide lax.composites to express quantization operations.,2025-02-06T23:08:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86790
copybara-service[bot],add an option to remove call-start/done to sparse core computations.,add an option to remove callstart/done to sparse core computations. used for hybridsim until it support sparsecore.,2025-02-06T23:04:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86789
copybara-service[bot],PjRt clients do not support ToLiteralSync with empty tuple shapes.,PjRt clients do not support ToLiteralSync with empty tuple shapes. Instead of calling ToLiteralSync we just construct a new empty tuple literal instead.,2025-02-06T22:39:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86788
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@04c5a341,Integrate StableHLO at openxla/stablehlo,2025-02-06T22:33:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86787
copybara-service[bot],[MultiHostHloRunner] Fix profiler lifetime in multihost hlo runner.,[MultiHostHloRunner] Fix profiler lifetime in multihost hlo runner.,2025-02-06T21:53:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86786
copybara-service[bot],[xla:cpu] Add run and setup scripts for e2e gemma2 pyTorch,[xla:cpu] Add run and setup scripts for e2e gemma2 pyTorch,2025-02-06T21:27:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86785
copybara-service[bot],[XLA:Python] Remove unused reference to CUDA/ROCM headers.,[XLA:Python] Remove unused reference to CUDA/ROCM headers.,2025-02-06T21:27:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86784
copybara-service[bot],Remove Thunk::Cleanup method.,"Remove Thunk::Cleanup method. The method was effectively unused, since it wasn't overridden by SequentialThunk, and so SequentialThunk wouldn't call Cleanup on its subthunks. NcclRaggedAllToAllStartThunk overrode Cleanup to free some device buffers, but these were never freed since Cleanup was not called. The memory is now stored in DeviceMemoryHandles, which automatically free the buffers in the destructor.",2025-02-06T21:14:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86783
copybara-service[bot],Internal change only.,Internal change only.,2025-02-06T21:08:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86782
copybara-service[bot],Rolling back due to broken tests.,"Rolling back due to broken tests. Original change: Build IFRT shardings with both addressable and nonaddressable devices, instead of only addressable devices (rollforward). Reverts e3b0866fb48abd1d20a074a8361608a72c20ad3f",2025-02-06T21:05:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86781
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-06T20:47:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86780
copybara-service[bot],Update and/or wrap `Executable` when calling into `HloRunnerInterface`.,Update and/or wrap `Executable` when calling into `HloRunnerInterface`.,2025-02-06T20:44:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86779
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-06T20:42:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86778
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-06T20:39:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86777
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-06T20:30:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86776
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-06T20:29:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86775
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-06T20:27:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86774
gaikwadrahul8,Fix 03 broken links in play_services.md,"Hi, Team I found 03 broken documentation links in this play_services.md file so I have updated those links to functional links. Please review and merge this change as appropriate. Thank you for your consideration.",2025-02-06T20:15:28Z,comp:lite size:XS,closed,0,1,https://github.com/tensorflow/tensorflow/issues/86773,"Copybara mentioned the same PR as closed in the prior comments, thereby closing this one."
copybara-service[bot],Extract conflicting collective analysis,Extract conflicting collective analysis,2025-02-06T20:11:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86772
copybara-service[bot],Make a continuous only Github Actions based TensorFlow CPU build,Make a continuous only Github Actions based TensorFlow CPU build In the future this will be promoted to presubmit Passing run here: https://github.com/openxla/xla/actions/runs/13185352502/job/36806036630?pr=22264,2025-02-06T20:06:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86771
copybara-service[bot],Remove TensorFlow CPU Kokoro build now that we have equivalent GitHub Actions build,Remove TensorFlow CPU Kokoro build now that we have equivalent GitHub Actions build,2025-02-06T19:56:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86770
copybara-service[bot],Fix and add some log,Fix and add some log,2025-02-06T19:38:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86769
copybara-service[bot],Define TFL_GatherNd as a DRQ-able Op.,Define TFL_GatherNd as a DRQable Op.,2025-02-06T19:30:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86768
copybara-service[bot],PR #22382: [ROCm] Fix test scripts under `build_tools/rocm`,PR CC(Creating pip package for TensorFlow with GPU support results in 0 byte simple_console_for_windows.zip): [ROCm] Fix test scripts under `build_tools/rocm` Imported from GitHub PR https://github.com/openxla/xla/pull/22382 Copybara import of the project:  58d80b5b44e7dc1bdfd9567debdfb7b588e06e70 by Harsha HS : [ROCm] Fix test scripts under build_tools/rocm Merging this change closes CC(Creating pip package for TensorFlow with GPU support results in 0 byte simple_console_for_windows.zip) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22382 from ROCm:ci_fix_rocm_test_scripts_20250205 58d80b5b44e7dc1bdfd9567debdfb7b588e06e70,2025-02-06T19:26:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86767
copybara-service[bot],Add TfFunctionDb generation in OSS.,Add TfFunctionDb generation in OSS.,2025-02-06T19:20:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86766
copybara-service[bot],"[XLA:GPU] Make sharded autotuning use ""more unique"" keys, and allow it to reload results already present in the key-value store.","[XLA:GPU] Make sharded autotuning use ""more unique"" keys, and allow it to reload results already present in the keyvalue store. Previously, reautotuning the same set of fusions on the same host at different points during the lifetime of a given keyvalue store would cause sharded autotuning to crash. This was surfaced more prominently after a recent change started canonicalizing fusion strings before using their hash as a key in the keyvalue store. Autotuning two different modules containing the same set of fusions could therefore result in a collision during the second compilation, and a subsequent crash. Simply allowing to read from the cache did not seem like a good solution, given that different modules may be compiled with different options, and the cache can therefore hold stale results that shouldn't be accessed. There didn't seem to be a very good way to resolve this issue given the current design of (sharded) autotuning. The proper solution would be to remove autotuning from the compilation pipeline. To fix the issue faster, we elected here to also hash the module as part of the cache key, and to allow cache hits. Fetching more runtime information to uniquify the cache key was deemed undesirable: for one thing, the cache key derivation needs to be deterministic across hosts; for a second, passing enough information down from the runtime to truly make results unique would force us to further intertwine levels of abstraction that should be independent. There is still an issue whereby autotuning the same set of fusions within the same module twice within the lifetime of the same PjRt client could result in fetching stale cache dataif the user changes compilation options between both runs. Nevertheless, this issue was already there, and this change makes it more unlikely to occur in common scenarios. The hope is that switching to a better compilation model (single host compilation) and hoisting autotuning out of the compilation pipeline will allow us to definitely resolve this issue in the future.",2025-02-06T18:48:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86765
copybara-service[bot],PR #22123: Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline,PR CC(Incorrect results on GPU for reduce_* methods on large tensors): Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline Imported from GitHub PR https://github.com/openxla/xla/pull/22123 Copybara import of the project:  50788338bb8107c1c9b8fe338e253df594e8cfd0 by Sevin Varoglu : Add ScheduleAwareCollectiveOpsCSE to the GPU pipeline  3af6a0d0af7cb488cb6ef6888b471b4ea72526d6 by Sevin Varoglu : Add hlo_opt test  1c87fad70fbbd9072e0221aa18cf15c352fb3d93 by Sevin Varoglu : Modify test Merging this change closes CC(Incorrect results on GPU for reduce_* methods on large tensors) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22123 from sfvaroglu:sevin/collective_cse 1c87fad70fbbd9072e0221aa18cf15c352fb3d93,2025-02-06T17:39:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86764
copybara-service[bot],[XLA:GPU] add a feature flag for generic triton gemm emitter,[XLA:GPU] add a feature flag for generic triton gemm emitter,2025-02-06T17:11:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86763
copybara-service[bot],[XLA:GPU] Do not regard the 'fusion' op as part of the `HloComputationFusion`.,"[XLA:GPU] Do not regard the 'fusion' op as part of the `HloComputationFusion`. Instead, check for this case in `ResolveUsers` and `ResolveOperand`, by querying whether the `fused_expression_root` is part of the `HloFusionAdaptor`. This prevents us from stepping into nested fusions.",2025-02-06T16:46:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86762
copybara-service[bot],Add transitional header for mlir_roundtrip_flags,Add transitional header for mlir_roundtrip_flags,2025-02-06T16:36:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86761
copybara-service[bot],Fix race condition in FenceInsertionPass,"Fix race condition in FenceInsertionPass It looks like the idea here was to add some basic memoization so we don't end up with a ton of recursive calls, and potentially deadlock. However, doing that through a static variable is problematic both because it's not threadsafe, and because it's a silent memory leak, since we never free up the set (so a longrunning program would just continue adding stuff to it as we compile new kernels indefinitely). I'm still trying to get a good upstreamable reproducer, but this should fix the issue for now so we don't crash in production.",2025-02-06T16:15:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86760
copybara-service[bot],PR #21965: [PJRT]  Expose should_stage_host_to_device_transfers as client create option,"PR CC(cannot find  ""flatbuffers/flatbuffers.h""  head  file): [PJRT]  Expose should_stage_host_to_device_transfers as client create option Imported from GitHub PR https://github.com/openxla/xla/pull/21965 Expose GPU option `should_stage_host_to_device_transfers ` as configurable in PJRT client create function. This allow the end user to override the default value which is `true`. ref: openxla/blob/main/xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h Since the option is living in a hashmap of type PJRT_NamedValue, it seems that I don't break the C PJRT API interface versioning. Copybara import of the project:  18b34c19938814b68bfd863fd5e603f08e9e824c by Hugo Mano : [PJRT]  Expose should_stage_host_to_device_transfers  as create option Expose GPU option `should_stage_host_to_device_transfers ` as configuration in PJRT client create func.  a412723fe4aa833dbfc0c27a2629d61bd68b34f7 by Hugo Mano : Address comments Merging this change closes CC(cannot find  ""flatbuffers/flatbuffers.h""  head  file) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21965 from hugomano:hugmano/pjrtexposeshould_stage_host_to_device_transfers a412723fe4aa833dbfc0c27a2629d61bd68b34f7",2025-02-06T16:11:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86759
copybara-service[bot],Move p2p pipeliner into its own pass,"Move p2p pipeliner into its own pass This is in preparation of global post processing of control dependencies, which is not possible in the local post processing callbacks.",2025-02-06T16:09:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86758
copybara-service[bot],[xla:emitters] fix type mismatch for several passes,[xla:emitters] fix type mismatch for several passes I've seen Windows failures because of these after applying some upcoming changes. Fix them.,2025-02-06T15:36:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86757
copybara-service[bot],[xla:cpu] CompileAheadOfTime: add nothunks check,[xla:cpu] CompileAheadOfTime: add nothunks check,2025-02-06T15:17:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86756
copybara-service[bot],[XLA:GPU] Add collective perf table tool.,[XLA:GPU] Add collective perf table tool. This produces a derating curve of network throughput at an HLO op level.,2025-02-06T15:16:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86755
copybara-service[bot],[XLA:GPU] Model bytes accessed.,[XLA:GPU] Model bytes accessed.,2025-02-06T14:44:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86754
copybara-service[bot],[tf:xla] tfcompile: explicitly disable thunk runtime for AOT,"[tf:xla] tfcompile: explicitly disable thunk runtime for AOT Currently thunks are enabled by default, yet we do not use thunks for AOT compilation. How can this be? The aheadoftime compilation path unconditionally uses the legacy emitters, which explains why even if the xla_cpu_use_thunk_runtime flag is set to true, we still do not get thunks in tfcompile. Explicitly disable the flag so that HLO passes correctly know whether or not we are using thunks. This will prevent miscompiles when the nothunks and thunks HLO passes meaningfully diverge (so far their differences are negligible), which is about to occur due to the imminent landing of fusion emitters. tl;dr: we should have disabled thunks explicitly for tfcompile from the beginning. Do it now.",2025-02-06T14:17:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86753
AlanBogarin,unexpected import during stub creation from mypy-protobuf," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version master  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.12.8  Bazel version 6.5.0  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I'm doing a stub distribution for tensorflow, and when I used mypyprotobuf to create stubs for .proto files, it resulted in an import that I didn't expect The format it should create is: ```python import tensorflow.tsl.protobuf.histogram_pb2 ``` but it does ```python import xla.tsl.protobuf.histogram_pb2 ``` I don't really understand how they structure the configurations to compile from Bazel Is there a setting I'm missing, or should I make the change manually? Is there a way for tensorflow to automatically create the stubs files from the compiled proto files? Here I leave you the code to speed up your analysis. third_party\xla\xla\tsl\protobuf\histogram.proto ```proto syntax = ""proto3""; package tensorflow; option cc_enable_arenas = true; option java_multiple_files = true; option java_package = ""org.tensorflow.framework""; option go_package = ""github.com/google/tsl/tsl/go/core/protobuf/summary_go_proto""; // Serialization format for histogram module in // tsl/lib/histogram/histogram.h message HistogramProto {   double min = 1;   double max = 2;   double num = 3;   double sum = 4;   double sum_squares = 5;   // Parallel arrays encoding the bucket boundaries and the bucket values.   // bucket(i) is the count for the bucket i.  The range for   // a bucket is:   //   i == 0:  DBL_MAX .. bucket_limit(0)   //   i != 0:  bucket_limit(i1) .. bucket_limit(i)   repeated double bucket_limit = 6 [packed = true];   repeated double bucket = 7 [packed = true]; } ``` tensorflow\core\framework\summary.proto ```proto syntax = ""proto3""; package tensorflow; import public ""xla/tsl/protobuf/histogram.proto""; import ""tensorflow/core/framework/tensor.proto""; option cc_enable_arenas = true; option java_outer_classname = ""SummaryProtos""; option java_multiple_files = true; option java_package = ""org.tensorflow.framework""; option go_package = ""github.com/tensorflow/tensorflow/tensorflow/go/core/framework/summary_go_proto""; // Metadata associated with a series of Summary data message SummaryDescription {   // Hint on how plugins should process the data in this series.   // Supported values include ""scalar"", ""histogram"", ""image"", ""audio""   string type_hint = 1; } // A SummaryMetadata encapsulates information on which plugins are able to make // use of a certain summary value. message SummaryMetadata {   message PluginData {     // The name of the plugin this data pertains to.     string plugin_name = 1;     // The content to store for the plugin. The best practice is for this to be     // a binary serialized protocol buffer.     bytes content = 2;   }   // Data that associates a summary with a certain plugin.   PluginData plugin_data = 1;   // Display name for viewing in TensorBoard.   string display_name = 2;   // Longform readable description of the summary sequence. Markdown supported.   string summary_description = 3;   // Class of data stored in this time series. Required for compatibility with   // TensorBoard's generic data facilities (`DataProvider`, et al.). This value   // imposes constraints on the dtype and shape of the corresponding tensor   // values. See `DataClass` docs for details.   DataClass data_class = 4; } enum DataClass {   // Unknown data class, used (implicitly) for legacy data. Will not be   // processed by data ingestion pipelines.   DATA_CLASS_UNKNOWN = 0;   // Scalar time series. Each `Value` for the corresponding tag must have   // `tensor` set to a rank0 tensor of type `DT_FLOAT` (float32).   DATA_CLASS_SCALAR = 1;   // Tensor time series. Each `Value` for the corresponding tag must have   // `tensor` set. The tensor value is arbitrary, but should be small to   // accommodate direct storage in database backends: an upper bound of a few   // kilobytes is a reasonable rule of thumb.   DATA_CLASS_TENSOR = 2;   // Blob sequence time series. Each `Value` for the corresponding tag must   // have `tensor` set to a rank1 tensor of bytestring dtype.   DATA_CLASS_BLOB_SEQUENCE = 3; } // A Summary is a set of named values to be displayed by the // visualizer. // // Summaries are produced regularly during training, as controlled by // the ""summary_interval_secs"" attribute of the training operation. // Summaries are also produced at the end of an evaluation. message Summary {   message Image {     // Dimensions of the image.     int32 height = 1;     int32 width = 2;     // Valid colorspace values are     //   1  grayscale     //   2  grayscale + alpha     //   3  RGB     //   4  RGBA     //   5  DIGITAL_YUV     //   6  BGRA     int32 colorspace = 3;     // Image data in encoded format.  All image formats supported by     // image_codec::CoderUtil can be stored here.     bytes encoded_image_string = 4;   }   message Audio {     // Sample rate of the audio in Hz.     float sample_rate = 1;     // Number of channels of audio.     int64 num_channels = 2;     // Length of the audio in frames (samples per channel).     int64 length_frames = 3;     // Encoded audio data and its associated RFC 2045 content type (e.g.     // ""audio/wav"").     bytes encoded_audio_string = 4;     string content_type = 5;   }   message Value {     // This field is deprecated and will not be set.     string node_name = 7;     // Tag name for the data. Used by TensorBoard plugins to organize data. Tags     // are often organized by scope (which contains slashes to convey     // hierarchy). For example: foo/bar/0     string tag = 1;     // Contains metadata on the summary value such as which plugins may use it.     // Take note that many summary values may lack a metadata field. This is     // because the FileWriter only keeps a metadata object on the first summary     // value with a certain tag for each tag. TensorBoard then remembers which     // tags are associated with which plugins. This saves space.     SummaryMetadata metadata = 9;     // Value associated with the tag.     oneof value {       float simple_value = 2;       bytes obsolete_old_style_histogram = 3;       Image image = 4;       HistogramProto histo = 5;       Audio audio = 6;       TensorProto tensor = 8;     }   }   // Set of values for the summary.   repeated Value value = 1; } ```  Standalone code to reproduce the issue ```shell Commands to recreate >>> git clone https://github.com/tensorflow/tensorflow.git >>> cd tensorflow >>> mkdir out >>> protoc tensorflow/core/framework/summary.proto mypy_out=out/ I=""."" I=""third_party/xla/"" ```  Relevant log output ```shell ```",2025-02-06T14:03:17Z,type:build/install type:support,open,0,4,https://github.com/tensorflow/tensorflow/issues/86752,", Could you please provide the error log which you are facing the issue. It helps to analyse the issue in an effective way. Thank you!","> [](https://github.com/AlanBogarin), Could you please provide the error log which you are facing the issue. It helps to analyse the issue in an effective way. Thank you! ``` PS D:\Alan\github> git clone https://github.com/tensorflow/tensorflow.git Cloning into 'tensorflow'... remote: Enumerating objects: 1972623, done. remote: Counting objects: 100% (428/428), done. remote: Compressing objects: 100% (280/280), done. remote: Total 1972623 (delta 289), reused 169 (delta 148), packreused 1972195 (from 2) Receiving objects: 100% (1972623/1972623), 1.07 GiB  None = ...,     ) > None: ...     def ClearField(self, field_name: typing.Literal[""value"", b""value""]) > None: ... global___Summary = Summary ```","When I install tensorflow with pip, tsl is inside tensorflow ``` tensorflow/     tsl/         profiler/             ...         protobuf/             histogram_pb2.py             ...         ...     ... ```", Is there any progress?
copybara-service[bot],NFC: Polish HLO text in `hlo_traversal_test.cc`.,NFC: Polish HLO text in `hlo_traversal_test.cc`. * Improve readability of HLO IR and make op names consistent. * Use `TF_ASSERT_OK_AND_ASSIGN` to prevent the test from crashing with a fatal error when `ParseAndReturnVerifiedModule` fails * Define a formatter for `HloInstructionAdaptor` for more informative error messages This CL does not change any test logic.,2025-02-06T13:25:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86751
copybara-service[bot],Upgrade Abseil to lts_2024_07_22.,Upgrade Abseil to lts_2024_07_22.,2025-02-06T13:25:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86750
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f308af757d72,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f308af757d72,2025-02-06T12:26:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86749
copybara-service[bot],FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22091 from ROCm:ci_fix_rocm_gemm_fusion a161c86d14b5c3e5a61d47474142646f141814ae,FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22091 from ROCm:ci_fix_rocm_gemm_fusion a161c86d14b5c3e5a61d47474142646f141814ae,2025-02-06T11:15:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86748
copybara-service[bot],Remove normalization for making stride always positive.,"Remove normalization for making stride always positive. This causes issues down the line when we compute tile_offset_indexing. If we want to emit a reverse op, we would actually need to normalize (but then also adjust the tile offsets to make it work). We can bring normalization back once we want to support emitting reverse, and have a fix for adjusting the tile offsets. Without normalization, reverse can potentially be supported by propagating it up to the load (by using negative stride). Only remaining problem here would be to handle padding correctly.",2025-02-06T10:54:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86747
copybara-service[bot],Integrate LLVM at llvm/llvm-project@e151b1d1f678,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match e151b1d1f678,2025-02-06T10:50:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86746
copybara-service[bot],Allow sharing of subgraph inputs for dynamic update slice,Allow sharing of subgraph inputs for dynamic update slice,2025-02-06T10:35:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86745
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T10:08:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86744
copybara-service[bot],[XLA:GPU] NFC: move HloFusionInstructionAdaptor to implementation file.,[XLA:GPU] NFC: move HloFusionInstructionAdaptor to implementation file.,2025-02-06T10:07:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86743
copybara-service[bot],"Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`","Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`",2025-02-06T10:03:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86742
copybara-service[bot],PR #22256: [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell,"PR CC(Option: Use fused_batch_norm instead of batch_norm for layer normalization): [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell Imported from GitHub PR https://github.com/openxla/xla/pull/22256 Update the `BlockScalingRewriter` pass to allow lowering the ""__op$block_scaled_dot"" to a cuDNN graph (instead of HLO graph)  this lowers to a block scaled dot kernel via cuDNN frontend (since v1.10). The only format supported currently is MXFP8 (both dot inputs are quantized to E4M3FN/E5M2 tensors with E8M0 scales using block size 32). MXFP8 docs: https://www.opencompute.org/documents/ocpmicroscalingformatsmxv10specfinalpdf The cuDNN kernel has some requirements for the input shapes, so some padding may be needed. Also, the scale tensor must be swizzled (transposed in a specific manner). The tests are added that verify both the pass transformations and the runtime correctness. The pass is also enabled in the same PR. Copybara import of the project:  145554dea5fec31a44b968f344bcd325e894e930 by Sergey Kozub : [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell Merging this change closes CC(Option: Use fused_batch_norm instead of batch_norm for layer normalization) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22256 from openxla:skozub/block_scaling_cudnn 145554dea5fec31a44b968f344bcd325e894e930",2025-02-06T09:32:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86741
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:30:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86740
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:27:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86739
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:26:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86738
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:23:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86737
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:21:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86736
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22091 from ROCm:ci_fix_rocm_gemm_fusion a161c86d14b5c3e5a61d47474142646f141814ae,2025-02-06T09:20:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86735
Venkat6871,Fix typos in documentation strings,"Hi, Team I observed few typos in the documentation strings and I have fixed those typos so please do the needful. Thank you.",2025-02-06T09:19:26Z,size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86734
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:18:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86733
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:18:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86732
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:17:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86731
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:16:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86730
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22091 from ROCm:ci_fix_rocm_gemm_fusion a161c86d14b5c3e5a61d47474142646f141814ae,2025-02-06T09:15:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86729
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:15:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86728
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:15:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86727
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:15:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86726
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:15:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86725
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:14:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86724
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:14:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86723
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22256 from openxla:skozub/block_scaling_cudnn 145554dea5fec31a44b968f344bcd325e894e930,2025-02-06T09:14:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86722
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T09:13:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86721
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T08:52:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86720
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T08:52:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86719
copybara-service[bot],Integrate LLVM at llvm/llvm-project@8b448842c476,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 8b448842c476,2025-02-06T08:37:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86718
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-06T07:53:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86717
copybara-service[bot],[ODML] StablehloOptimizePass : MHLO -> StableHLO,[ODML] StablehloOptimizePass : MHLO > StableHLO,2025-02-06T06:39:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86716
chuntl,Qualcomm AI Engine Direct - Validate and update soc table,Summary:  Validate exisiting soc infos  Add some new soc infos,2025-02-06T06:19:43Z,comp:lite size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86715
copybara-service[bot],Simplify flag parsing in hlo_runner.cc using fixed_options_flag.,Simplify flag parsing in hlo_runner..,2025-02-06T06:18:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86714
chuntl,Qualcomm AI Engine Direct - Validate and update soc table,Summary:  Validate exisiting soc infos  Add some new soc infos,2025-02-06T06:15:51Z,size:S,closed,0,1,https://github.com/tensorflow/tensorflow/issues/86713,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],"Optimize pattern for mul(a, a) -> pow(a, 2)","Optimize pattern for mul(a, a) > pow(a, 2) Also,  mul(pow(a, 2), a)) > pow(a, 3)  mul(a, square(a)) > pow(a, 3)",2025-02-06T05:56:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86712
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-06T04:19:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86711
copybara-service[bot],Simplify flag parsing in functional_hlo_runner.cc using the fixed_options_flag library.,Simplify flag parsing in functional_hlo_runner.. The new implementation is more declarative and less errorprone.,2025-02-06T04:13:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86710
copybara-service[bot],Update nanobind to head.,"Update nanobind to head. This includes https://github.com/wjakob/nanobind/commit/b9af9d448cbcbda2fd88f67804aafd9ae2ccb303, which fixes a crash bug in JAX under Python free threading on aarch64. nanobind v2.5.0 is only a few commits behind this, so this is not far from a released version.",2025-02-06T03:12:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86709
copybara-service[bot],Add vanilla support of multi subgraph in QNN compiler plugin.,Add vanilla support of multi subgraph in QNN compiler plugin.,2025-02-06T02:39:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86708
copybara-service[bot],[ODML] StablehloFuseConvolutionPass : migrated from MHLO -> StableHLO,[ODML] StablehloFuseConvolutionPass : migrated from MHLO > StableHLO,2025-02-06T01:53:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86707
copybara-service[bot],"lite: Keep signature input, output names order sync with SignatureDef.","lite: Keep signature input, output names order sync with SignatureDef. In LiteRT, it assumes that input_names(), output_names() follows the SignatureDef in the given model.",2025-02-06T01:50:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86706
copybara-service[bot],Fix heuristic for determining if SparseCore is used.,Fix heuristic for determining if SparseCore is used.,2025-02-06T01:26:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86705
draganmladjenovic,[ROCm] Enable unsafe fp atomics and cleanup gpu_device_functions.h,,2025-02-06T01:07:03Z,awaiting review size:M comp:core,open,0,2,https://github.com/tensorflow/tensorflow/issues/86704,A gentle ping.,  A gentles of the gentle pings.
copybara-service[bot],Delete JAX CPU Kokoro now that the JAX CPU GitHub Actions build blocks,Delete JAX CPU Kokoro now that the JAX CPU GitHub Actions build blocks,2025-02-06T00:38:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86703
copybara-service[bot],[XLA] Fix log message in hlo_pass_fix.,[XLA] Fix log message in hlo_pass_fix.,2025-02-06T00:34:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86702
copybara-service[bot],Add python directory for litert,Add python directory for litert,2025-02-06T00:24:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86701
copybara-service[bot],Fix incorrect unknown flags error message.,"Fix incorrect unknown flags error message. Before, if both unknown and known flags were in XLA_FLAGS, the error would list the unknown flags and also incorrectly some known flags as being unknown. Now, only the unknown flags are shown. The issue was tsl::Flags::Parse would set its argc and argv parameters to only have the unknown flags. The caller in parse_flags_from_env. for the argv parameter. But then the caller would use the size of the vector when printing unknown flags, which was not mutated by tsl::Flags::Parse, instead of argc, which was mutated.",2025-02-06T00:21:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86700
copybara-service[bot],Update Eigen to commit:4c38131a16803130b66266a912029504f2cf23cd,Update Eigen to commit:4c38131a16803130b66266a912029504f2cf23cd,2025-02-06T00:13:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86699
copybara-service[bot],Integrate LLVM at llvm/llvm-project@8b448842c476,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 8b448842c476,2025-02-06T00:08:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86698
copybara-service[bot],Fix GatherClientLibraryTest under PjRt.,"Fix GatherClientLibraryTest under PjRt. It was unclear to me what this test was actually trying to achieve because it is not clearly documented. If it is trying to exercise a specific behavior with the old nonPjRt `Client`, then that should probably live elsewhere. I've converted it to something that can just run on top of `HloPjRtTestBase` directly. I added some boilerplate to allow the `XlaBuilder` code to remain, though it might be better to just use a HLO string directly.",2025-02-05T23:49:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86697
copybara-service[bot],Add TfFunctionDb generation in OSS.,Add TfFunctionDb generation in OSS.,2025-02-05T23:38:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86696
copybara-service[bot],Expose InferenceStats method to be used internally.,Expose InferenceStats method to be used internally.,2025-02-05T23:34:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86695
copybara-service[bot],Update TFRT dependency to use revision,Update TFRT dependency to use revision http://github.com/tensorflow/runtime/commit/5d18d4ac03fc02dd40d5a8b06c65a3ca77a0596a.,2025-02-05T23:33:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86694
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-05T23:20:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86693
copybara-service[bot],Add quantization dimension verification of stablehlo.uniform_(de)quantize ops.,Add quantization dimension verification of stablehlo.uniform_(de)quantize ops.,2025-02-05T23:14:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86692
copybara-service[bot],"Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`","Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`",2025-02-05T22:55:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86691
copybara-service[bot],Change the compile function type to take a model rather than a list of subgraphs,Change the compile function type to take a model rather than a list of subgraphs,2025-02-05T22:31:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86690
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f8287f6c373f,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f8287f6c373f,2025-02-05T22:22:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86689
copybara-service[bot],Enables LiteRtTensorBufferRequirements lookup by tensor name.,Enables LiteRtTensorBufferRequirements lookup by tensor name.,2025-02-05T22:21:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86688
copybara-service[bot],"Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`","Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`",2025-02-05T22:19:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86687
copybara-service[bot],Update and/or wrap `Executable` when calling into `HloRunnerInterface`.,Update and/or wrap `Executable` when calling into `HloRunnerInterface`.,2025-02-05T22:15:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86686
copybara-service[bot],AdvancedMatchShapeCoveringDynamicIndexInstruction now simulates index updates.,AdvancedMatchShapeCoveringDynamicIndexInstruction now simulates index updates.,2025-02-05T22:13:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86685
copybara-service[bot],Update TFRT dependency to use revision,Update TFRT dependency to use revision http://github.com/tensorflow/runtime/commit/5d18d4ac03fc02dd40d5a8b06c65a3ca77a0596a.,2025-02-05T22:10:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86684
copybara-service[bot],"Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`","Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`",2025-02-05T22:04:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86683
copybara-service[bot],[xla:ifrt] Update ifrt::Future comment,[xla:ifrt] Update ifrt::Future comment (ifrt|pjrt)::Future already have correct move semantics and act as a moveonly value if payload type is moveonly.,2025-02-05T22:01:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86682
copybara-service[bot],"Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`","Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`",2025-02-05T21:45:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86681
copybara-service[bot],Reverts changelist 723307829,Reverts changelist 723307829,2025-02-05T21:39:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86680
copybara-service[bot],"Include ""GenAIOpsRegisterer"" function into LiteRT pypi.","Include ""GenAIOpsRegisterer"" function into LiteRT pypi.",2025-02-05T21:38:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86679
copybara-service[bot],[easy] [XLA] s/CHECK/Status for a part of MSA,[easy] [XLA] s/CHECK/Status for a part of MSA,2025-02-05T20:50:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86678
copybara-service[bot],Update Eigen to commit:4c2611d27cb9a5fe12e18c8953097871db1d3abf,Update Eigen to commit:4c2611d27cb9a5fe12e18c8953097871db1d3abf,2025-02-05T20:26:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86677
copybara-service[bot],Improve the format of the PJRT C `CHANGELOG.md`.,Improve the format of the PJRT C `CHANGELOG.md`.,2025-02-05T20:25:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86676
zahiqbal,Gfx950 platform arch support,Added support for AMD GPU gfx950 platform,2025-02-05T20:22:48Z,size:XL,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86675
copybara-service[bot],Add `HloEvaluator` customization to PjRt `InterpreterClient`.,Add `HloEvaluator` customization to PjRt `InterpreterClient`. `InterpreterClient` hardcoded the `HloEvaluator` implementation that it uses internally.,2025-02-05T20:18:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86674
copybara-service[bot],Add more logging for debug,Add more logging for debug,2025-02-05T20:01:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86673
copybara-service[bot],First bits for experimental XnnGraphFusion pass.,First bits for experimental XnnGraphFusion pass.,2025-02-05T19:50:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86672
copybara-service[bot],"Add RawBuffer pjrt API. Right now, only supports: creation, h2d and d2h.","Add RawBuffer pjrt API. Right now, only supports: creation, h2d and d2h. The capi support will be added in: https://github.com/openxla/xla/pull/22272 .",2025-02-05T19:33:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86671
github-actions[bot],Issue created for Rollback of PR #82210: Align num_threads parameter validation with documentation,Merged PR CC(Align num_threads parameter validation with documentation) is rolled back in d9039b966f5b2468b37a2e7d04208fe3713d2e0d.     Please follow up with the reviewer and close this issue once its resolved.,2025-02-05T19:25:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86670
copybara-service[bot],Adds a library to make fixed options flags easier and safer to define.,Adds a library to make fixed options flags easier and safer to define. xla defines custom commandline flag parsing logic in several places. These definitions follow a common pattern: there is a fixed set of options for the flag type (e.g. the flag type is an enum). Such definitions are repetitious and errorprone (one has to make sure that the name => value and value => name mappings are consistent and have no duplicate names or values). This library abstracts away the tedious details and will be used in subsequent changes to make the definitions simpler and safer.,2025-02-05T19:06:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86669
copybara-service[bot],Remove TSL's `requirements_lock_3_11.txt`,Remove TSL's `requirements_lock_3_11.txt` This file has no meaning now that TSL is no longer independently buildable,2025-02-05T19:04:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86668
copybara-service[bot],"Rollback TF PR #82210 to fix LiteRT pypi error ""'Interpreter' object has no attribute '_interpreter'"".","Rollback TF PR CC(Align num_threads parameter validation with documentation) to fix LiteRT pypi error ""'Interpreter' object has no attribute '_interpreter'"". Reverts 0f26341d36164bab69cf162e3eed0ec1292987e7",2025-02-05T18:55:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86667
copybara-service[bot],Exit checkpoints_iterator promptly when no more checkpoints are expected.,Exit checkpoints_iterator promptly when no more checkpoints are expected.,2025-02-05T18:39:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86666
copybara-service[bot],[pjrt] Removed unused overloads of `PjRtClient::CreateBuffersForAsyncHostToDevice`,[pjrt] Removed unused overloads of `PjRtClient::CreateBuffersForAsyncHostToDevice`,2025-02-05T18:16:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86665
copybara-service[bot],"Remove top level `WORKSPACE`, `.bazelrc`, `.bazelversion` and `tools/` from TSL","Remove top level `WORKSPACE`, `.bazelrc`, `.bazelversion` and `tools/` from TSL TSL is no longer independently buildable, so these files have no meaning",2025-02-05T18:11:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86664
copybara-service[bot],Update Eigen to commit:c079ee5e44f770c67f135e340dcbd5b59012e4a6,Update Eigen to commit:c079ee5e44f770c67f135e340dcbd5b59012e4a6,2025-02-05T18:08:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86663
copybara-service[bot],Skip TSAN tests as they are also not supported.,Skip TSAN tests as they are also not supported.,2025-02-05T18:05:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86662
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-05T18:00:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86661
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-05T18:00:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86660
copybara-service[bot],"Build IFRT shardings with both addressable and non-addressable devices, instead of only addressable devices (roll-forward).","Build IFRT shardings with both addressable and nonaddressable devices, instead of only addressable devices (rollforward). Reverts 814b7dcd21487da4be271cbcf58a5d463f5e1a1a",2025-02-05T17:45:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86659
copybara-service[bot],[XLA:CPU] Make loop unrolling on by default in IrCompiler.,[XLA:CPU] Make loop unrolling on by default in IrCompiler. Reverts 6ae9c29c1fcf8905812599fec4c692b5bf47fd0e,2025-02-05T17:39:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86658
copybara-service[bot],[pjrt] Removed the deprecated overload of `BufferFromHostLiteral`,[pjrt] Removed the deprecated overload of `BufferFromHostLiteral`,2025-02-05T17:34:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86657
copybara-service[bot],[XLA:GPU] Remove the restrictions that prevent us from fusing the subchannel dequantisation sequence from Triton tiling propagation. ,"[XLA:GPU] Remove the restrictions that prevent us from fusing the subchannel dequantisation sequence from Triton tiling propagation.  There are two cases for the broadcast: 1) Channel quantisation case:       a) we have 2d weights + 1d scales + 2d activations.      b) In triton prolog we prepare the corresponding block pointers      c) inside the for loop along the kdim every time we load the same 1d tile for scalers, expand it to 2d [block_m,1], and broadcast to block_k elements along newly added dim to [block_m, block_k].      d) then do the multiply and dot 2) Subchannel quantisation case:      a) we have 2d weights [M,K] + 2d scales [M,K/q] + 2d activations where q is the subchannel size.      b) In triton prolog we prepare the corresponding block pointers      c) inside the for loop along the kdim every time we load the 2d [M,1] tile for scalers and broadcast to block_k elements along the k dim to [block_m, block_k].      d) then do the rest I.e. the difference is that the scalers matrix is the 2d matrix from the very beginning but it is smaller along the k dim and we need to advance it along k dim only by one column instead of block_k columns. It is already 2d, so, we don't need to add the dimension. We could emit the right code if we know that there was the subchannel broadcast and and we know the size of the subchannel. We do this analysis in the triton_tiling_propagation by detecting the broadcast with the follow up bitcast combination like [B,c,M] > [B,c,q,M] > [B,c*q,M]. I.e. we do the broadcast but the follow up bitcast merges the broadcasted dim with another nonempty dim. This schema works for the cases when block_k == subchannel_size and for the case when we have split_k == 1. These two restrictions could be addressed in the follow up cls. Reverts b7a1cfd0e9092c9054ae27a7913e5d4c5b13e1c3",2025-02-05T17:17:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86656
copybara-service[bot],[XLA] Fix scheduling annotations to avoid creating invalid overlap of instructions,[XLA] Fix scheduling annotations to avoid creating invalid overlap of instructions,2025-02-05T16:49:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86655
copybara-service[bot],[XLA:GPU] Maximally combine sync collectives.,[XLA:GPU] Maximally combine sync collectives. We dryrun the scheduler to collect the set of collectives that don't overlap with compute. We then combine these maximally (i.e. up to the device memory limit).,2025-02-05T16:28:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86654
jpienaar,Use nested namespace,,2025-02-05T16:26:05Z,size:XS,closed,0,3,https://github.com/tensorflow/tensorflow/issues/86653,"Shouldn't merge this directly, this is breaking Copybara","Hi  , we are testing to ensure copybara will actually overwrite as part of some partner onboarding ","Ohh, sorry, did not know. It would be awesome if that works"
copybara-service[bot],[XLA:GPU] Mark copy op as supported by triton,[XLA:GPU] Mark copy op as supported by triton * codegen succeeds for copy op with all triton supported datatypes,2025-02-05T16:13:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86652
copybara-service[bot],[XLA] Rollback 'Support nested fusions in HloFusionAdaptor',"[XLA] Rollback 'Support nested fusions in HloFusionAdaptor' We don't want the HloFusionAdaptor to handle nested fusions transparently. Instead, *inner* fusion instructions should be treated like any other instruction. Handling this is not part of this change. Reverts 8c446ecc5741f5daacb43e20410905cf51c7e05b",2025-02-05T15:47:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86651
copybara-service[bot],"Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""`","Remove `srcs_version` and `python_version` attributes, as they already default to `""PY3""` FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22041 from sfvaroglu:sevin/channel_id_flag 335de387e78559e37a9a018aa1b2c367a208a97e",2025-02-05T15:28:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86650
copybara-service[bot],[XLA] Preserve AUTO layout when converting from HLO to StableHLO,"[XLA] Preserve AUTO layout when converting from HLO to StableHLO In HLO, AUTO layout is encoded as missing layout in `entry_computation_layout`. In StableHLO, it's marked using `mhlo.layout_mode = ""auto""` attribute of the main@ function argument or return value.",2025-02-05T15:15:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86649
copybara-service[bot],[XLA:CPU] Don't unroll dimensions except for the trailing one.,[XLA:CPU] Don't unroll dimensions except for the trailing one.,2025-02-05T15:06:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86648
copybara-service[bot],PR #22041: Enable xla_ignore_channel_id flag by default,PR CC(Can't Import Keras from Tensorflow library): Enable xla_ignore_channel_id flag by default Imported from GitHub PR https://github.com/openxla/xla/pull/22041 Rename `xla_experimental_ignore_channel_id` flag as `xla_ignore_channel_id flag` and enable it by default. :  94baa4a5f32ff9421709471fc880fe59a0e164c6 by Sevin Varoglu : Enable xla_ignore_channel_id flag by default  d63c6c022aa7b4c81c1bd38a07419da08fc6e3b9 by Sevin Varoglu : Fix error  335de387e78559e37a9a018aa1b2c367a208a97e by Sevin Varoglu : Fix format Merging this change closes CC(Can't Import Keras from Tensorflow library) Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22041 from sfvaroglu:sevin/channel_id_flag 335de387e78559e37a9a018aa1b2c367a208a97e,2025-02-05T14:26:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86647
copybara-service[bot],[XLA:GPU] Add AllGather support to collective generation tool.,[XLA:GPU] Add AllGather support to collective generation tool.,2025-02-05T14:02:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86646
copybara-service[bot],Rollback [XLA:CPU] Make loop unrolling on by default in IrCompiler.,Rollback [XLA:CPU] Make loop unrolling on by default in IrCompiler. Reverts 1e7976e5c5869d34eac64352d411dfb2c8ca4966 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22334 from ROCm:rocm_fix_flaky_gpu_compiler_test c5f600f03aa87d155bb624bedb0584e635af190e,2025-02-05T14:01:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86645
copybara-service[bot],[XLA:GPU] Add ReduceScatter support to collective generation tool.,[XLA:GPU] Add ReduceScatter support to collective generation tool.,2025-02-05T14:00:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86644
copybara-service[bot],[XLA] Tag timeout tests as `not_run:arm`,[XLA] Tag timeout tests as `not_run:arm` Similarly to cl/722883015 tagging also: * //third_party/tensorflow/compiler/xla/python/transfer:socket_bulk_transport_test * //third_party/tensorflow/compiler/xla/python/transfer:socketserver_test * //third_party/tensorflow/compiler/xla/python/transfer:event_loop_test Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981,2025-02-05T13:37:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86643
copybara-service[bot],Removing some patches by:,Removing some patches by:  Upstreaming internal testto remove file entirely. Also removing patch with redundant (already upstream) tests.  Verifying an issue is already fixed.  Attempting to upstream 2 changes. Added comments to remove 2 more patches in a followup if they land successfully.,2025-02-05T13:29:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86642
copybara-service[bot],PR #22334: [ROCm] Fix flaky gpu compiler test when building with rocm,PR CC([tflite] fix calculating of output pixels): [ROCm] Fix flaky gpu compiler test when building with rocm Imported from GitHub PR https://github.com/openxla/xla/pull/22334 This change fixes the flaky gpu compiler test used to run on rocm CI pipeline gate. Triton pipeline was wrongly using the TritonGPUAccelerateMatmul pass which supports cuda only. In rocm there is a different pass which is now used in the rocm pipeline. https://github.com/tritonlang/triton/blob/main/third_party/amd/lib/TritonAMDGPUTransforms/AccelerateAMDMatmul.cpp Copybara import of the project:  c5f600f03aa87d155bb624bedb0584e635af190e by Alexandros Theodoridis : Fix flaky gpu compiler test when building with rocm Merging this change closes CC([tflite] fix calculating of output pixels) Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22334 from ROCm:rocm_fix_flaky_gpu_compiler_test c5f600f03aa87d155bb624bedb0584e635af190e,2025-02-05T12:45:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86641
copybara-service[bot],Use custom HLO deserialization for HloUnoptimizedSnapshot.,Use custom HLO deserialization for HloUnoptimizedSnapshot. This change makes it possible to read HloUnoptimizedSnapshot protos that are over 2GiB in size and were dumped using custom proto serialization.,2025-02-05T12:20:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86640
copybara-service[bot],[xla:gpu:triton] Create tma_utils with functions & tests that help with emitting TMA through triton. (see child cl to see how most of these get used).,[xla:gpu:triton] Create tma_utils with functions & tests that help with emitting TMA through triton. (see child cl to see how most of these get used). This also helps to isolate TMA that can be used in other places.,2025-02-05T12:17:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86639
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981,2025-02-05T11:03:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86638
copybara-service[bot],PR #22091: [ROCM] Try targeting cuBLAS if it's not profitable to fuse dot,PR CC(Failed to load the native TensorFlow runtime.): [ROCM] Try targeting cuBLAS if it's not profitable to fuse dot Imported from GitHub PR https://github.com/openxla/xla/pull/22091 TritonTest.NonstandardLayoutWithManyNonContractingDims TritonTest.NonstandardLayoutWithManyNonContractingDimsReversedLayout If it's not profitable to fuse dot and cuBLAS is not requiring extra padding then targeting custom call would be optimal. Align this check for both ROCm and CUDA here. rotation would you please have a look? Copybara import of the project:  a161c86d14b5c3e5a61d47474142646f141814ae by Jian Li : [ROCM] Try targeting cuBLAS if it's not profitable to fuse dot Merging this change closes CC(Failed to load the native TensorFlow runtime.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22091 from ROCm:ci_fix_rocm_gemm_fusion a161c86d14b5c3e5a61d47474142646f141814ae,2025-02-05T10:18:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86637
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T10:18:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86636
copybara-service[bot],PR #22256: [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell,"PR CC(Option: Use fused_batch_norm instead of batch_norm for layer normalization): [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell Imported from GitHub PR https://github.com/openxla/xla/pull/22256 Update the `BlockScalingRewriter` pass to allow lowering the ""__op$block_scaled_dot"" to a cuDNN graph (instead of HLO graph)  this lowers to a block scaled dot kernel via cuDNN frontend (since v1.10). The only format supported currently is MXFP8 (both dot inputs are quantized to E4M3FN/E5M2 tensors with E8M0 scales using block size 32). MXFP8 docs: https://www.opencompute.org/documents/ocpmicroscalingformatsmxv10specfinalpdf The cuDNN kernel has some requirements for the input shapes, so some padding may be needed. Also, the scale tensor must be swizzled (transposed in a specific manner). The tests are added that verify both the pass transformations and the runtime correctness. The pass is also enabled in the same PR. Copybara import of the project:  145554dea5fec31a44b968f344bcd325e894e930 by Sergey Kozub : [XLA:GPU] Enable cuDNN kernel for block scaled dot on Blackwell Merging this change closes CC(Option: Use fused_batch_norm instead of batch_norm for layer normalization) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22256 from openxla:skozub/block_scaling_cudnn 145554dea5fec31a44b968f344bcd325e894e930",2025-02-05T10:18:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86635
copybara-service[bot],test XLA build.,test XLA build. Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T10:16:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86634
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-05T10:10:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86633
copybara-service[bot],Remove dead code (NFC),Remove dead code (NFC) We compute the total number of tiles in a variable `num_tiles` but then never use it. So remove it. Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T09:54:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86632
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T09:41:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86631
chunhsue,Qualcomm AI Engine Direct - Support DUS and Pack Op in LiteRT, WHAT  Pack is supported using QNN Concat to address type support issue.   DUS are supported partially for specific use cases.   Reuse input static tensor.   Use absl::Span instead of std::span.   TESTS ```  build bazel build c opt cxxopt=std=c++17 //tensorflow/lite/experimental/litert/vendors/qualcomm/compiler:qnn_compiler_plugin_test  run ./bazelbin/tensorflow/lite/experimental/litert/vendors/qualcomm/compiler/qnn_compiler_plugin_test ``` ``` [] Global test environment teardown [==========] 100 tests from 4 test suites ran. (4292 ms total) [  PASSED  ] 100 tests. ```,2025-02-05T09:32:58Z,comp:lite size:XL,closed,0,5,https://github.com/tensorflow/tensorflow/issues/86630,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , Can you please resolve the conflicts? Thank you!","Thanks for the reminder, done! > Hi , Can you please resolve the conflicts? Thank you!","Hi , Rebased again, please check.  Thanks!",> please uncomment the dus and pack tests in qnn_compiler_plugin_test and check they work Thanks! Didn't notice https://github.com/tensorflow/tensorflow/commit/ac02f89a538615c506ac15d51fba9abfe90483b5 not being in my base. Fixed in 6d5b5ab8653fd27cf202025b8e912e95e14f9eda. Tested with QNN 2.29.0.
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T09:21:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86629
chuntl,partition sample,,2025-02-05T09:21:38Z,size:M,closed,0,1,https://github.com/tensorflow/tensorflow/issues/86628,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T09:21:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86627
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T09:18:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86626
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-05T09:17:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86625
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-05T09:16:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86624
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981,2025-02-05T09:16:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86623
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T09:16:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86622
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981,2025-02-05T09:15:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86621
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981,2025-02-05T09:15:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86620
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T09:15:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86619
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T09:15:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86618
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-05T09:14:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86617
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981,2025-02-05T09:14:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86616
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981,2025-02-05T09:14:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86615
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T09:14:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86614
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-05T09:12:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86613
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-05T09:12:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86612
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-05T08:10:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86611
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T07:29:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86610
copybara-service[bot],internal change for visibilily,internal change for visibilily,2025-02-05T07:06:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86609
copybara-service[bot],Use matchers_oss in vendor code,Use matchers_oss in vendor code Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T06:40:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86608
usmonali4,inconsistent result of ```tf.raw_ops.Rsqrt``` on CPU and GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04 (google colab)  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5/9  GPU model and memory Tesla T4  Current behavior? result of ``tf.raw_ops.Rsqrt``` is inconsistent between CPU and GPU  Standalone code to reproduce the issue ```shell import tensorflow as tf x_0 = tf.constant([[[1.2031, 0.2168], [0.1177, 0.6758]],                      [[1.8203, 0.9688], [0.8828, 0.0767]]], dtype=tf.bfloat16) with tf.device('CPU:0'):   result_cpu = tf.raw_ops.Rsqrt(x=x_0)   print(""Output on CPU:"", result_cpu) with tf.device('GPU:0'):   result_gpu = tf.raw_ops.Rsqrt(x=x_0)   print(""Output on GPU:"", result_gpu) max_abs_diff = tf.reduce_max(tf.abs(result_cpu  result_gpu)).numpy() is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e3,  atol=1e2) print(""Max absolute difference:"", max_abs_diff) print(""Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3:"", is_consistent.numpy()) ```  Relevant log output ```shell Output on CPU: tf.Tensor( [[[0.910156 2.14062]   [2.92188 1.21875]]  [[0.742188 1.01562]   [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16) Output on GPU: tf.Tensor( [[[0.914062 2.15625]   [2.90625 1.21875]]  [[0.738281 1.01562]   [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16) Max absolute difference: 0.015625 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: False ```",2025-02-05T06:12:29Z,stat:awaiting tensorflower type:bug comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/86607,"  weird, no issue here: ```bash TensorFlow Version: 2.16.2 Python Version: 3.10.16  (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] Python Implementation: CPython CUDA Version (TensorFlow): 12.0 cuDNN Version (TensorFlow): 8 GPU Name (PyTorch): NVIDIA GeForce RTX 4070 SUPER CUDA Version (PyTorch): 12.0 cuDNN Version (PyTorch): 8907 Driver Version: 550.144.03 Ubuntu Version: PRETTY_NAME=""Ubuntu 24.04.1 LTS"" Model name:                           AMD Ryzen 5 7600 6Core Processor Output on CPU: tf.Tensor( [[[0.914062 2.15625]   [2.90625 1.21875]]  [[0.738281 1.01562]   [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16) Output on GPU: tf.Tensor( [[[0.914062 2.15625]   [2.90625 1.21875]]  [[0.738281 1.01562]   [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16) Max absolute difference: 0.0000000000 Consistency check (CPU vs GPU) with atol=1e6 and rtol=1e7: True ```"
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets. Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T06:02:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86606
copybara-service[bot],"Fix inference request analysis aggregated on batch size, by aggregating only the requests included in a single batch, as large request split into multiple batches will introduce confusing results (eg. the device time will be the sum of the 2 batch processing).","Fix inference request analysis aggregated on batch size, by aggregating only the requests included in a single batch, as large request split into multiple batches will introduce confusing results (eg. the device time will be the sum of the 2 batch processing).",2025-02-05T05:31:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86605
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T05:16:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86604
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T04:58:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86603
copybara-service[bot],Update TFRT dependency to use revision,Update TFRT dependency to use revision http://github.com/tensorflow/runtime/commit/5d18d4ac03fc02dd40d5a8b06c65a3ca77a0596a.,2025-02-05T04:39:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86602
copybara-service[bot],Fix race when accessing `runners_`.,Fix race when accessing `runners_`. Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-05T04:09:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86601
copybara-service[bot],[XLA] Set OS-scope thread names for debuggability,[XLA] Set OSscope thread names for debuggability This makes it easier for outside tooling to understand a crash or do other postmortem debugging.,2025-02-05T03:44:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86600
copybara-service[bot],"In literal_util.cc, use absl::uniform_int_distribution.","In literal_util.cc, use absl::uniform_int_distribution. absl::uniform_int_distribution is faster than std::uniform_int_distribution. This makes initializing literals in run_hlo_module faster. In particular, I tested the following HLO:     ENTRY f {       arg = s8[2000000000] parameter(0)       ROOT add_result = s8[2000000000] add(arg, arg)     } It takes 7.8 seconds to initialize the input literal with the absl function, and 18.2 with the std function. I had to change several tests, which were sensitive to the exact values randomlyinitialized Literals were initialized to. Literals are initialized to a fixed seed, but this change causes such literals to be initialized to different values than before. Unfortunately the absl version of uniform_real_distribution is not faster. It takes 25.5 seconds with absl and 8.3 with std on the HLO when s8 is replaced with f16. The function does become faster if we use an absl::BitGen instead of an std::minstd_rand0, but this is considerable more work to implement as it will affect many places in the codebase which currently use an std::minstd_rand0. Also, changing the floatingpoint RNG algorithm will likely cause many more tests to fail which are inadvertently reliant on the current specific values literals are initialized to. Maybe I'll do this in the future, maybe not.",2025-02-05T03:37:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86599
copybara-service[bot],Vectorize group_sizes by including more lhs dimensions.,Vectorize group_sizes by including more lhs dimensions.,2025-02-05T03:35:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86598
copybara-service[bot],PR #22258: [GPU][NFC] Avoid always printing complete PGLE profiles.,PR CC(Fix Windows GPU Build failure caused by nccl): [GPU][NFC] Avoid always printing complete PGLE profiles. Imported from GitHub PR https://github.com/openxla/xla/pull/22258 Copybara import of the project:  025352635a155e447559d83c471369559aad5981 by Ilia Sergachev : [GPU][NFC] Avoid always printing complete PGLE profiles. Merging this change closes CC(Fix Windows GPU Build failure caused by nccl) Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981,2025-02-05T02:59:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86597
copybara-service[bot],Add new allocation type kTfLiteNonCpu,Add new allocation type kTfLiteNonCpu This allocation type indicates the Tensor is associated with HW buffer using litert::TensorBuffer. Also added an experimental API of TfLiteOpaqueTensorSetNonCpuAllocation(),2025-02-05T02:35:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86596
copybara-service[bot],add an option to remove call-start/done to sparse core computations.,add an option to remove callstart/done to sparse core computations. used for hybridsim until it support sparsecore.,2025-02-05T01:32:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86595
copybara-service[bot],Introduce a `function_runs_at_most_once` parameter to LocalExecutorParams for cases where we know that no one will be using the executor again.,"Introduce a `function_runs_at_most_once` parameter to LocalExecutorParams for cases where we know that no one will be using the executor again. This allows us to clear the executor state when we no longer need it, and thus reduce peak memory (and HBM) usage.",2025-02-05T00:11:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86594
copybara-service[bot],Add the list of Qualcomm SoCs supporting NPU.,Add the list of Qualcomm SoCs supporting NPU.,2025-02-04T23:33:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86593
WadieBishoy25,Update on the README and the CONTRIBUTING files,"Hi tensorflow Team, I hope this message finds you well. My name is Wadie Botros, and I am currently working on an assignment for a course at Potsdam University, Germany. As part of the assignment, I reviewed your repository to improve its readability and maintainability. Changes Made:  Update the README file with 'Common Issues and Troubleshooting' Section.  Update the CONTRIBUTING file with 'QUICK START GUIDE' Section. I believe these changes will make the code easier to maintain and understand for future contributors. If you have any questions or need further modifications, please feel free to discuss them with me. I am happy to make any adjustments as needed. Thank you for considering my contribution. I look forward to your feedback. Best regards, Wadie Botros",2025-02-04T23:27:08Z,size:S invalid,closed,0,1,https://github.com/tensorflow/tensorflow/issues/86592,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Create GitHub Actions based T4 GPU CI job,Create GitHub Actions based T4 GPU CI job,2025-02-04T23:15:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86591
copybara-service[bot],Install free threaded python3.13t to the ml_build x86 docker image,Install free threaded python3.13t to the ml_build x86 docker image python3.13nogil is a freethreaded build of python3.13.,2025-02-04T22:31:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86590
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-04T22:30:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86589
copybara-service[bot],Add a heuristic to avoid constant folding operations that may result in large constants,Add a heuristic to avoid constant folding operations that may result in large constants,2025-02-04T22:25:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86588
copybara-service[bot],[XLA:CPU] Undo make loop unrolling on by default in IrCompiler.,[XLA:CPU] Undo make loop unrolling on by default in IrCompiler. Reverts 1e7976e5c5869d34eac64352d411dfb2c8ca4966,2025-02-04T22:23:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86587
copybara-service[bot],PR #22041: Enable xla_ignore_channel_id flag by default,PR CC(Can't Import Keras from Tensorflow library): Enable xla_ignore_channel_id flag by default Imported from GitHub PR https://github.com/openxla/xla/pull/22041 Rename `xla_experimental_ignore_channel_id` flag as `xla_ignore_channel_id flag` and enable it by default. :  94baa4a5f32ff9421709471fc880fe59a0e164c6 by Sevin Varoglu : Enable xla_ignore_channel_id flag by default  d63c6c022aa7b4c81c1bd38a07419da08fc6e3b9 by Sevin Varoglu : Fix error  335de387e78559e37a9a018aa1b2c367a208a97e by Sevin Varoglu : Fix format Merging this change closes CC(Can't Import Keras from Tensorflow library) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22041 from sfvaroglu:sevin/channel_id_flag 335de387e78559e37a9a018aa1b2c367a208a97e,2025-02-04T22:21:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86586
copybara-service[bot],Create LiteRT pre-submit CI to GitHub actions.,Create LiteRT presubmit CI to GitHub actions.,2025-02-04T22:21:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86585
copybara-service[bot],[XLA] Make the compiler respond to the new optimization_level and memory_fitting_level options when available.,[XLA] Make the compiler respond to the new optimization_level and memory_fitting_level options when available. We temporarily continue to respect the legacy exec_time_optimization_effort option if the new one is not specified. This will be phased out once all users have been migrated away from the old option.,2025-02-04T22:06:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86584
copybara-service[bot],Return arrays from `ArrayImpl._check_and_rearrange`.,"Return arrays from `ArrayImpl._check_and_rearrange`. This is in preparation for a larger change, so that input buffers can be checked before Array creation in XLA and the user gets more helpful JAX error messages instead of XLA errors. Reverts 135a67d02fc6282a323fc4ad42ef7d8a687995e6",2025-02-04T21:54:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86583
copybara-service[bot],Add HloRunnerPjRt support for output tuples w/ mixed array and non-array shapes.,"Add HloRunnerPjRt support for output tuples w/ mixed array and nonarray shapes. Prior to this change, `HloRunnerPjRt` would fail during execution of `FfiCustomCallTest.Tokens` (on CPU) because the `GenerateExecutionOptions` function called `ComputationLayout::FlattenedResultLayouts`. This function returns an error if any of the subshapes of the tuple are not arrays. While it is possible to adapt the implementation of this particular function to return only array shapes and skip over the nonarray shapes, it would also require changes in how the output buffers are allocated in the CPU compiler.",2025-02-04T21:46:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86582
copybara-service[bot],Mark tf_dialect_to_executor as deprecated.,Mark tf_dialect_to_executor as deprecated.,2025-02-04T21:14:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86581
copybara-service[bot],[XLA] Introduce discrete scheme for execution time optimization and memory fitting effort.,"[XLA] Introduce discrete scheme for execution time optimization and memory fitting effort. This will replace the previously introduced floatingpoint effort levels, which will soon be removed.",2025-02-04T20:55:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86580
copybara-service[bot],Update Eigen to commit:b73bb766a57f872b4fe376c4818cd9b17dc1a761,Update Eigen to commit:b73bb766a57f872b4fe376c4818cd9b17dc1a761,2025-02-04T20:36:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86579
copybara-service[bot],Integrate Triton up to [b39c1e14](https://github.com/openai/triton/commits/b39c1e14b8f2029bc6a8798e4914d2692edf97d8),Integrate Triton up to b39c1e14,2025-02-04T20:31:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86578
copybara-service[bot],Stop modifying the TraceEventsContainer in DoStoreAsLevelDbTable. This behavior,Stop modifying the TraceEventsContainer in DoStoreAsLevelDbTable. This behavior is not intuitive (modifying a const value that was passed in) and unnecessary. Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-04T19:55:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86577
copybara-service[bot],[xla:copy_insertion] Avoid adding a redundant control dependence from a,"[xla:copy_insertion] Avoid adding a redundant control dependence from a pipelined RecvDone to its previous Recv in a whileloop. When we add a copy of the RecvDone, we also add a control dependence from the copy to the Recv. If the copy is later on remove, the control dependence from the RecvDone to the Recv becomes the only side effect of the pass, which is not intended.",2025-02-04T19:13:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86576
copybara-service[bot],PR #22248: Pass CC 10.0 to Triton for sm_120.,PR CC(Calculating custom metrics with tf.estimator.DNNRegressor in TensorFlow 1.10): Pass CC 10.0 to Triton for sm_120. Imported from GitHub PR https://github.com/openxla/xla/pull/22248 Workaround to avoid spurious failures in Triton when compiling for sm_120. Copybara import of the project:  a5be158483e775ba461f81b888558d15fdb70b36 by Dimitris Vardoulakis : Pass CC 10.0 to Triton for sm_120.  725c24702021fb04428060b8143cae65da24ce2d by Dimitris Vardoulakis : Address review comments. Merging this change closes CC(Calculating custom metrics with tf.estimator.DNNRegressor in TensorFlow 1.10) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22248 from dimvar:tritonworkaroundforsm120 725c24702021fb04428060b8143cae65da24ce2d,2025-02-04T19:11:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86575
copybara-service[bot],"litert: Strip symbols for compiler plugin, dispatch API","litert: Strip symbols for compiler plugin, dispatch API",2025-02-04T19:00:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86574
copybara-service[bot],[XLA:GPU] Add support for multiple updates per replica in RaggedAllToAll.,[XLA:GPU] Add support for multiple updates per replica in RaggedAllToAll. A recent proposal suggested to extend the API of ra2a to allow to send multiple updates in one op. Before we would need to emit multiple chained ra2a to achieve the same effect.,2025-02-04T18:57:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86573
copybara-service[bot],Add `--xnnpack_runtime_flags` option to pass raw XNNPACK flags to XNNPACK,Add `xnnpack_runtime_flags` option to pass raw XNNPACK flags to XNNPACK,2025-02-04T18:51:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86572
copybara-service[bot],Reverting to investigate test issue.,Reverting to investigate test issue. Reverts 6b431d3ca515c8afc08b1bdfaf4253adf4234b19,2025-02-04T17:50:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86571
copybara-service[bot],[xla:gpu:triton] Extract more functions into IterableInput,"[xla:gpu:triton] Extract more functions into IterableInput This is the last of the refactors that extracts the logic necessary to emit TMA versus tensor pointer loads.  In the next CL, I override some of these functions for TMA for a simpler & more seamless transition, allowing the tma logic to be in one place). This allows brings in functionality to allow one input to be associated with more than one inter_arg in the ForLoop. This is important for the implementation of TMA, but for now just sets the count to 1. Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981",2025-02-04T17:15:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86570
copybara-service[bot],[XLA] tool to print indexing map of operands,"[XLA] tool to print indexing map of operands example output ``` Output 0 operand 0: (d0) > (d0), domain: d0 in [0, 1023] Output 0 operand 1: (d0) > (), domain: d0 in [0, 1023] Output 1 operand 0: (d0) > (d0), domain: d0 in [0, 1023] Output 1 operand 1: (d0) > (), domain: d0 in [0, 1023] Output 1 operand 2: (d0) > (d0), domain: d0 in [0, 1023] ```",2025-02-04T16:56:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86569
copybara-service[bot],[pjrt] Removed deprecated `PjRtBuffer::CopyToDevice`,[pjrt] Removed deprecated `PjRtBuffer::CopyToDevice` Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981,2025-02-04T16:48:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86568
copybara-service[bot],Fixes crash during model loading for tensors with empty shape.,Fixes crash during model loading for tensors with empty shape.,2025-02-04T16:29:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86567
copybara-service[bot],[XLA] fail if the change bit reported by a pass does not agree with HLO hash comparison,"[XLA] fail if the change bit reported by a pass does not agree with HLO hash comparison Usually we trust that pass truthfully reports if it did update the HLO. Shockingly there are cases, e.g. algsimp, that wrongly report ""unchanged"" pass result. Having a flag makes it easier to detect existing and new cases of such bugs.",2025-02-04T16:01:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86566
copybara-service[bot],[XLA] Add const reference versions of `ForEachInstructionWithPred` and `ForEachInstructionWithOpcode`.,[XLA] Add const reference versions of `ForEachInstructionWithPred` and `ForEachInstructionWithOpcode`. These are more permissive and semantically equivalent.,2025-02-04T15:54:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86565
copybara-service[bot],[XLA:GPU] Move collectives related code under `transforms/collectives/`.,[XLA:GPU] Move collectives related code under `transforms/collectives/`.,2025-02-04T15:22:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86564
copybara-service[bot],PR #22061: [GPU][NFC] Cleanup.,PR CC(Add //tensorflow:install_headers target): [GPU][NFC] Cleanup. Imported from GitHub PR https://github.com/openxla/xla/pull/22061 Copybara import of the project:  c492d7adb8422c6c23a5d1b4dbcd816f9739a079 by Ilia Sergachev : [GPU][NFC] Cleanup. Merging this change closes CC(Add //tensorflow:install_headers target) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22061 from openxla:cleanup_predicate c492d7adb8422c6c23a5d1b4dbcd816f9739a079,2025-02-04T14:40:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86563
copybara-service[bot],PR #22238: [GPU] Dump PTX before compiling it.,PR CC(TENSORFLOW quantize_graph.py throws error : Graph_def is invalid at node error): [GPU] Dump PTX before compiling it. Imported from GitHub PR https://github.com/openxla/xla/pull/22238 This helps debugging uncompilable modules. Copybara import of the project:  62074b787bff9ad14fc1576d60e53ed0f85e7d64 by Ilia Sergachev : [GPU] Dump PTX before compiling it. This helps debugging uncompilable modules. Merging this change closes CC(TENSORFLOW quantize_graph.py throws error : Graph_def is invalid at node error) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22238 from openxla:dump_ptx 62074b787bff9ad14fc1576d60e53ed0f85e7d64,2025-02-04T14:37:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86562
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f8287f6c373f,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f8287f6c373f,2025-02-04T14:26:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86561
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f6578c3d809b,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f6578c3d809b,2025-02-04T14:22:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86560
copybara-service[bot],[XLA] Support different operand and result types in AlgebraicSimplifierVisitor::HandlePad.,[XLA] Support different operand and result types in AlgebraicSimplifierVisitor::HandlePad. I checked that none of the other cases in HandlePad require any adjustments.,2025-02-04T14:21:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86559
copybara-service[bot],[XLA:GPU] Add helper function to dry-run the scheduler and collect sync collectives.,[XLA:GPU] Add helper function to dryrun the scheduler and collect sync collectives. We will use the function to maximally combine sync collectives.,2025-02-04T11:56:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86558
copybara-service[bot],[NFC] Add a VLOG for tiled HLO computations,[NFC] Add a VLOG for tiled HLO computations Useful for debugging the kind of tiling each op in the fusion ends up using.,2025-02-04T11:54:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86557
copybara-service[bot],[xla] test_utils: delete IsMlirLoweringEnabled helper,[xla] test_utils: delete IsMlirLoweringEnabled helper Refers to an implementation that was removed some time ago. Delete it.,2025-02-04T10:32:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86556
copybara-service[bot],PR #19834: [ds-fusion]Add support for async dynamic slice fusion,"PR CC(Attention Wrapper state error.): [dsfusion]Add support for async dynamic slice fusion Imported from GitHub PR https://github.com/openxla/xla/pull/19834 This patch adds async handling to dynamic slice fusion when the hero operation is a collective operation. Currently, only reducescatter is supported as a hero operation in dynamic slice thunk, so this patch also follows the same. Added a test with compute, to ensure that communication and compute overlap in the thunks emitted. Copybara import of the project:  13553a14592ed9e2c3bbabdff0051f18a0f27d2e by Shraiysh Vaishay : Add support for async dynamic slice fusion This patch adds async handling to dynamic slice fusion when the hero operation is a collective operation. Currently, only reducescatter is supported as a hero operation in dynamic slice thunk, so this patch also follows the same. Added a test with compute, to ensure that communication and compute overlap in the thunks emitted.  e2c598694b035e94c0b698b0835caf3f8d47a81c by Shraiysh Vaishay : Addressed comments.  ae73bb0e26227870d74f4aa0dcbc3532e4b012e1 by Shraiysh Vaishay : Rebase and fix build errors  c68b096c87115908013889ee99754cb7c3e0776b by Shraiysh Vaishay : Rebase  7609238f6757e789e1d3d910d0687c60e0a4e36b by Shraiysh Vaishay : Addressed comments  6b47e685e7f333cd49b137497f90c2ea30a9e15a by Shraiysh Vaishay : Address comments  12890a56c29c8383a8a9d5ebefa54db52b64a099 by Shraiysh Vaishay : Address comments  a585534c08c6cfabc259319312025386494abe17 by Shraiysh Vaishay : Fix Executable > OpaueExecutable Merging this change closes CC(Attention Wrapper state error.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19834 from shraiysh:asyncdynamicslicefusion a585534c08c6cfabc259319312025386494abe17",2025-02-04T10:26:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86555
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T10:15:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86554
weilhuan-quic,Support Qualcomm Op Builders for LiteRT," WHAT  Support Op implementations based on the wrappers.  Use `TensorPool` to manage the intermediate tensors, `TensorPool` can be abstract or change to allocator if need.  All builders are functions which receive `TensorPool`, input output `TensorWrapper`, and the opspecific parameters.  All builders are functions which return `std::vector`  The interface of all builders is open to change based on the requirement.  Make these builders independent to LiteRT/tflite, use c++ primitive types as parameters.  Only dependent on QNN and `Wrappers`.  Supported Op  ",2025-02-04T10:09:09Z,size:XL,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86553
copybara-service[bot],[pjrt] Deprecated `PjRtBuffer::CopyToDevice`,[pjrt] Deprecated `PjRtBuffer::CopyToDevice` It now has a default implementation which delegates to `CopyToMemorySpace`. I will update all usages and remove `CopyToDevice` in a follow up.,2025-02-04T09:52:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86552
copybara-service[bot],[XLA:GPU] Inline `is_nop` into `GpuConvertAsyncCollectivesToSync`.,[XLA:GPU] Inline `is_nop` into `GpuConvertAsyncCollectivesToSync`. I intend to reuse `GpuConvertAsyncCollectivesToSync` and would like to avoid having to keep the `is_nop` predicates in sync.,2025-02-04T09:51:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86551
copybara-service[bot],[XLA:CPU] Enable more flexible layout assignment in DotThunk.,[XLA:CPU] Enable more flexible layout assignment in DotThunk.,2025-02-04T09:49:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86550
copybara-service[bot],PR #21708: NUMA-pin host memory buffers for D2H/H2D transfers,"PR CC(tf.contrib.estimator.InMemoryEvaluatorHook runs every step): NUMApin host memory buffers for D2H/H2D transfers Imported from GitHub PR https://github.com/openxla/xla/pull/21708 This ensures that the pinned host buffers used for transfers between host and device are pinned to the NUMA node closest to the device. It had a previous life as https://github.com/openxla/xla/pull/15216. In a benchmark that triggers large, concurrent, copies from all devices to the host then achieved D2H throughput is around 33 GiB/s with NUMA pinning on a DGX H100 node (2xCPU, 8xH100). Without pinning, the achieved throughput is around 13.5 GiB/s from the same benchmark. While it is already possible to achieve the correct NUMA pinning in processperGPU and processperNUMAnode configurations using `numactl` or similar, achieving correct pinning in processpernode configuration requires logic inside XLA. Copybara import of the project:  0eab66c25a49d2c360f5fc5251f08ac8cce4c3ac by Olli Lupton : NUMApin host memory buffers for D2H/H2D transfers  57a46645d6716a55dc3617ff0ad613d31c267804 by Olli Lupton : 256 byte alignment for host allocations when NUMA is not enabled  ad2895a795d8f60512f164d85607510eb45c1b6a by Olli Lupton : Address review comments  629777ec4b2230909a2f98d48c9e726006ec09e8 by Olli Lupton : std::string_view > absl::string_view  21587a5fcf1f57743388256ef28c214ac5b50a23 by Olli Lupton : Apply 's suggested Bazel changes  175c5f6a5ae9c30cca3ac23131744ef1cabfd228 by Olli Lupton : add missing dependency Merging this change closes CC(tf.contrib.estimator.InMemoryEvaluatorHook runs every step) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21708 from olupton:numa 175c5f6a5ae9c30cca3ac23131744ef1cabfd228",2025-02-04T09:45:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86549
copybara-service[bot],PR #22259: [NFC] Fix VLOG statements in cuDNN FMHA graph builder,"PR CC(`tf.nn.softmax` gives an error at execution time for certain empty inputs): [NFC] Fix VLOG statements in cuDNN FMHA graph builder Imported from GitHub PR https://github.com/openxla/xla/pull/22259 Remove ""if (VLOG_IS_ON(x))"" wrappers around VLOG, as they're useless. This PR also fixes useaftermove of the ""graph"" local variable, which was previously not discovered, as it only happened when VLOG was enabled. (so not quite NFC) Copybara import of the project:  defd81ed2ba416b2013d0000283c5925cf3d9137 by Sergey Kozub : [NFC] Fix VLOG statements in cuDNN FMHA graph builder Merging this change closes CC(`tf.nn.softmax` gives an error at execution time for certain empty inputs) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22259 from openxla:skozub/fmha_vlog defd81ed2ba416b2013d0000283c5925cf3d9137",2025-02-04T09:34:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86548
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:18:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86547
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:17:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86546
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:17:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86545
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981,2025-02-04T09:17:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86544
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:17:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86543
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:17:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86542
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:16:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86541
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:16:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86540
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-04T09:16:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86539
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-04T09:16:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86538
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:15:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86537
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-04T09:15:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86536
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:15:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86535
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:15:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86534
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:15:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86533
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:15:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86532
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:15:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86531
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-04T09:15:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86530
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:15:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86529
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:14:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86528
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22259 from openxla:skozub/fmha_vlog defd81ed2ba416b2013d0000283c5925cf3d9137,2025-02-04T09:14:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86527
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:14:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86526
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:14:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86525
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:13:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86524
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:13:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86523
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:13:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86522
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:13:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86521
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:13:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86520
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:13:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86519
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:12:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86518
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-02-04T09:12:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86517
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:12:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86516
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:12:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86515
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T09:11:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86514
copybara-service[bot],Remove unused dependencies.,"Remove unused dependencies. The dependencies may have been needed before, but some passes have been moved.",2025-02-04T08:54:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86513
copybara-service[bot],Integrate LLVM at llvm/llvm-project@8f025f2a93ee,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 8f025f2a93ee,2025-02-04T08:43:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86512
copybara-service[bot],Add TensorAdapter for dynamic loading of Google Tensor Compiler API,"Add TensorAdapter for dynamic loading of Google Tensor Compiler API This commit introduces the `Adapter` class, which provides an abstraction for dynamically loading and interfacing with the Google Tensor Compiler API. The adapter uses `dlopen` and `dlsym` to load the shared library at runtime and expose the `CompileFlatbuffer` function for compiling TFLite buffers. 1. Supports dynamic loading to decouple client code from the compiler implementation. 2. Provides a C++ interface to interact with the compiler API. 3. Utilizes Cstyle linkage for compatibility and to avoid name mangling. 4. Includes error handling for library loading and symbol resolution. This change ensures modularity, allowing flexibility in loading different versions of the compiler without recompiling the client code.",2025-02-04T05:27:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86511
copybara-service[bot],Add support for lowering mhlo.fft with ranks > 3 to tfl.rfft2d.,Add support for lowering mhlo.fft with ranks > 3 to tfl.rfft2d.,2025-02-04T05:16:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86510
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-04T05:08:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86509
copybara-service[bot],Log status of timeout_fn in checkpoints_iterator.,"Log status of timeout_fn in checkpoints_iterator. Previously, nothing was logged if a timeout_fn was provided and it indicated timeout.",2025-02-04T05:01:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86508
copybara-service[bot],Add memcpy implementation of ragged-all-to-all.,Add memcpy implementation of raggedalltoall. The implementation is similar to the memcpy implementation of alltoall.,2025-02-04T03:48:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86507
usmonali4,inconsistent result of ```tf.raw_ops.Tan``` on CPU and GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5/9  GPU model and memory Tesla T4  Current behavior? ```tf.raw_ops.Tan``` produces inconsistent results between CPU and GPU  Standalone code to reproduce the issue ```shell import tensorflow as tf real = tf.constant([1.5634333], dtype=tf.float32) imag = tf.constant([0.020735], dtype=tf.float32) complex_tensor = tf.complex(real, imag) with tf.device('/CPU:0'):     result_cpu = tf.raw_ops.Tan(x=complex_tensor)     print(result_cpu) with tf.device('/GPU:0'):     result_gpu = tf.raw_ops.Tan(x=complex_tensor)     print(result_gpu) Comparing whole complex numbers max_abs_diff = tf.reduce_max(tf.abs(result_cpu  result_gpu)).numpy() is_cons = tf.experimental.numpy.allclose(result_cpu, result_gpu, rtol=1e6,  atol=1e5) print(""Max absolute difference:"", max_abs_diff) print(""Consistency check (CPU vs GPU) with atol=1e5 and rtol=1e6:"", is_cons.numpy()) Comparing by parts real_part_cpu = tf.math.real(result_cpu) real_part_gpu = tf.math.real(result_gpu) real_part_diff = tf.reduce_max(tf.abs(real_part_cpu  real_part_gpu)).numpy() real_part_cons = tf.experimental.numpy.allclose(real_part_cpu, real_part_gpu, rtol=1e6,  atol=1e5) imag_part_cpu = tf.math.imag(result_cpu) imag_part_gpu = tf.math.imag(result_gpu) imag_part_diff = tf.reduce_max(tf.abs(imag_part_cpu  imag_part_gpu)).numpy() imag_part_cons = tf.experimental.numpy.allclose(imag_part_cpu, imag_part_gpu, rtol=1e6,  atol=1e5) print(""Real parts absolute difference:"", real_part_diff) print(""Real parts Consistency check with atol=1e5 and rtol=1e6:"", real_part_cons.numpy()) print(""Imag parts absolute difference:"", imag_part_diff) print(""Imag parts Consistency check with atol=1e5 and rtol=1e6:"", imag_part_cons.numpy()) ```  Relevant log output ```shell tf.Tensor([15.205576+42.834145j], shape=(1,), dtype=complex64) tf.Tensor([15.205605+42.834225j], shape=(1,), dtype=complex64) Max absolute difference: 8.5064334e05 Consistency check (CPU vs GPU) with atol=1e5 and rtol=1e6: False Real parts absolute difference: 2.861023e05 Real parts Consistency check with atol=1e5 and rtol=1e6: False Imag parts absolute difference: 8.010864e05 Imag parts Consistency check with atol=1e5 and rtol=1e6: False ```",2025-02-04T03:18:15Z,type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/86506,I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TFnightly on both CPU and GPU. Please find gist1 and gist2 here for your reference. Thank you!,"reproduced on different configuration: ```bash TensorFlow Version: 2.16.2 Python Version: 3.10.16  (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] Python Implementation: CPython CUDA Version (TensorFlow): 12.0 cuDNN Version (TensorFlow): 8 GPU Name (PyTorch): NVIDIA GeForce RTX 4070 SUPER CUDA Version (PyTorch): 12.0 cuDNN Version (PyTorch): 8907 Driver Version: 550.144.03 Ubuntu Version: PRETTY_NAME=""Ubuntu 24.04.1 LTS"" Model name:                           AMD Ryzen 5 7600 6Core Processor tf.Tensor([15.205576+42.834145j], shape=(1,), dtype=complex64) tf.Tensor([15.205498+42.83405j], shape=(1,), dtype=complex64) Max absolute difference: 0.0001233304 Consistency check (CPU vs GPU) with atol=1e5 and rtol=1e6: False Real parts absolute difference: 7.8201294e05 Real parts Consistency check with atol=1e5 and rtol=1e6: False Imag parts absolute difference: 9.536743e05 Imag parts Consistency check with atol=1e5 and rtol=1e6: False ```"
copybara-service[bot],"Add a c-api extension for raw buffers. Right now, only supports: creation, h2d and d2h.","Add a capi extension for raw buffers. Right now, only supports: creation, h2d and d2h.",2025-02-04T03:11:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86505
copybara-service[bot],Fix collective-permute host memory not being unregistered.,"Fix collectivepermute host memory not being unregistered. CUDA host memory was registered in Initialize() and unregistered in Cleanup() but Cleanup() is not called. Now instead store host memory as a steam_executor::MemoryAllocation object, which automatically unregisters it in the destructor.",2025-02-04T03:03:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86504
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f8287f6c373f,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f8287f6c373f,2025-02-04T02:00:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86503
copybara-service[bot],It is difficult to parse allocation results in the MSA logs because we print them twice and at neither point are the allocations finalized.,"It is difficult to parse allocation results in the MSA logs because we print them twice and at neither point are the allocations finalized. So, the following changes were made: 1) Allocations are logged when they are finalized, and the logs indicate that they are finalized. 2) The ""Allocations List Begin"" logging has been removed, being replaced by 1. 3) The logging of allocations (along with bytes accessed values), in inefficient allocation site processing, has been labeled and indented to indicate that it is part of inefficient allocation site processing.",2025-02-04T01:53:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86502
copybara-service[bot],Tag `//xla/pjrt/distributed:client_server_test` and `//xla/tests:dynamic_ops_test_cpu` as `not_run:arm`,Tag `//xla/pjrt/distributed:client_server_test` and `//xla/tests:dynamic_ops_test_cpu` as `not_run:arm` These are failing and it's not totally clear what the culprit is.,2025-02-04T01:52:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86501
copybara-service[bot],Fix two bugs in NcclAllToAllStartThunk.,"Fix two bugs in NcclAllToAllStartThunk. The first bug is that alltoall ops with multiple replica groups did not work, because the thunk stored a map from local_id to some temporary memory used by the a2a implementation, where local_id was relative to the start of the replica_group. But this means devices of different groups would use the same temporary memory, overwriting each other's results. The fix is to change the map's key from local_id to StreamExecutor*. The second bug is that the temporary memory mentioned above is registered as host memory but never deregistered. It is deregistered in NcclAllToAllStartThunk::Cleanup(), but Cleanup() is never called. If Cleanup() were to be called, it would fix the bug, but cause the memory to registered and deregistered every run of the executable, which is unacceptably slow. The fix is to deregister the memory in the thunk destructor instead, which is implicitly done by storing a se::MemoryAllocation instead of a int64_t* in the map. Since the two fixes affect the exact same code, I'm putting them in a single change instead of two separate changes.",2025-02-04T01:17:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86500
copybara-service[bot],"Today there is no sign in the logs when we start processing a new buffer (tied to an HloValue). Our best log that prints the buffer is ""Creating AllocationValues for""; however, we don't always print that line.","Today there is no sign in the logs when we start processing a new buffer (tied to an HloValue). Our best log that prints the buffer is ""Creating AllocationValues for""; however, we don't always print that line. So, 2 changes were made. 1) We indicate with VLOG(3) when we start processing a new buffer, and we print that buffer. 2) Since, we're print the buffer in 1, we no longer need to print it when printing ""Creating AllocationValues for""",2025-02-04T01:14:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86499
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-02-04T00:47:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86498
copybara-service[bot],[XLA] Create documentation page for XLA terminology.,[XLA] Create documentation page for XLA terminology.,2025-02-04T00:30:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86497
copybara-service[bot],Install free threaded python3.13t to the ml_build x86 and arm64 cpu docker images,Install free threaded python3.13t to the ml_build x86 and arm64 cpu docker images python3.13nogil is a freethreaded build of python3.13.,2025-02-04T00:20:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86496
copybara-service[bot],Make a continuous only Github Actions based TensorFlow CPU build,Make a continuous only Github Actions based TensorFlow CPU build In the future this will be promoted to presubmit Passing run here: https://github.com/openxla/xla/actions/runs/13185352502/job/36806036630?pr=22264,2025-02-03T23:49:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86495
copybara-service[bot],Fix segfault when loading metadata buffers with offset.,Fix segfault when loading metadata buffers with offset.,2025-02-03T23:44:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86494
copybara-service[bot],[xla:cpu] fusion emitters: do not fuse inside computations called from scatter,[xla:cpu] fusion emitters: do not fuse inside computations called from scatter The scatter fusion emitter won't be able to handle fusions inside computations called from the scatter instruction. Pave the way for the scatter fusion emitter by forbidding those fusions.,2025-02-03T23:43:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86493
copybara-service[bot],Refactor `FindRotateRightPattern` and `FindPadWithWrapPattern` for concat operations in SPMD partitioner.,Refactor `FindRotateRightPattern` and `FindPadWithWrapPattern` for concat operations in SPMD partitioner.,2025-02-03T23:32:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86492
copybara-service[bot],Rename neuron adapter to neuron adapter api in mediatek code.,Rename neuron adapter to neuron adapter api in mediatek code.,2025-02-03T23:04:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86491
copybara-service[bot],Internal change for mtk code.,Internal change for mtk code.,2025-02-03T23:03:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86490
copybara-service[bot],build defs to manage mediatek object files within google3 (for linux). Add test to check api can be initialized.,build defs to manage mediatek object files within google3 (for linux). Add test to check api can be initialized.,2025-02-03T23:01:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86489
copybara-service[bot],Use the actual neuron adapter header in mediatek code,Use the actual neuron adapter header in mediatek code,2025-02-03T22:58:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86488
copybara-service[bot],Extend Dispatch API to report HW-specific metrics.,"Extend Dispatch API to report HWspecific metrics. The following pseudocode shows how metrics can be collected for a given inference run:    LiteRtDispatchStartMetricsCollection(invocation_context, detail_level);    run inference    LiteRtDispatchMetrics metrics;    LiteRtDispatchStopMetricsCollection(invocation_context, &metrics);    int num_metrics;    LiteRtDispatchGetNumMetrics(metrics, &num_metrics);    for (int i = 0; i < num_metrics; ++i) {       Metric metric;       LiteRtDispatchGetMetric(metrics, i, &metric)       process metric    }    LiteRtDispatchDestroyMetrics(metrics); Note this changelist doesn't yet connect the introduced API to the CompiledModel API; that would be object of a future changelist.",2025-02-03T22:40:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86487
copybara-service[bot],Switch tsl::OkStatus usage to absl::OkStatus.,Switch tsl::OkStatus usage to absl::OkStatus.,2025-02-03T22:32:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86486
copybara-service[bot],Make compiled model async execution explicit,Make compiled model async execution explicit This is accomplished by introducing new functions.,2025-02-03T22:14:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86485
copybara-service[bot],Propagate source ranges in location.,"Propagate source ranges in location. Previously only the line info was propagated. Given the new source range location support, propagate source range.",2025-02-03T21:33:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86484
copybara-service[bot],"[xla:python] Add a mechanism for ""batch partitioning"" of FFI calls.","[xla:python] Add a mechanism for ""batch partitioning"" of FFI calls. This is the first in a series of changes to add a simple API for supporting a set of common sharding and partitioning patterns for FFI calls. The high level motivation is that custom calls (including FFI calls) are opaque to the SPMD partitioner, and the only ways to customize the partitioning behavior is to (a) explicitly register an `xla::CustomCallPartitoner` with XLA, or (b) use the `jax.experimental.custom_partitioning` APIs. Option (a) isn't generally practical for most use cases where the FFI handler lives in an external binary. Option (b) is flexible, and supports all common use cases, but it requires embedding Python callbacks in to the HLO, which can lead to issues including cache misses. Furthermore, `custom_partitioning` is overpowered for many use cases, where only (what I will call) ""batch partitioning"" is supported. In this case, ""batch partitioning"" refers to the behavior of many FFI calls where they can be trivially partitioned on some number of (leading) dimensions, with the same call being executed independently on each shard of data. If the data are sharded on nonbatch dimensions, partitioning will still reshard the data to be replicated on the nonbatch dimensions. This kind of partitioning logic applies to all the LAPACK/cuSOLVER/etc.backed linear algebra functions in jaxlib, as well as some external users of `custom_partitioning`. The approach I'm taking here is to add a new registration function to the XLA client, which let's a user label their FFI call as batch partitionable. Then, when lowering the custom call, the user passes the number of batch dimensions as a frontend attribute, which is then interpreted by the SPMD partitioner. In parallel with this change, shardy has added support for sharding propagation across custom calls using a string representation that is similar in spirit to this approach, but somewhat more general. However, the shardy implementation still requires a Python callback for the partitioning step, so it doesn't (yet!) solve all of the relevant problems with the `custom_partitioning` approach. Ultimately, it should be possible to have the partitioner parse the shardy sharding rule representation, but I wanted to start with the minimal implementation. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22454 from yliu120:create_dump_dir d5bb15865b5601b6082ae871616b9611405e844e",2025-02-03T21:29:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86483
copybara-service[bot],Remove unused deprecated constructor of HloCollectiveInstruction,Remove unused deprecated constructor of HloCollectiveInstruction,2025-02-03T20:42:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86482
copybara-service[bot],[SHARDY] Remove outdated dependency on mlir_hlo,[SHARDY] Remove outdated dependency on mlir_hlo,2025-02-03T20:42:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86481
copybara-service[bot],Add host offloading rewriter to despecializer,Add host offloading rewriter to despecializer,2025-02-03T20:06:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86480
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T20:00:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86479
copybara-service[bot],Replace LITERT_ASSERT_STATUS_OK with LITERT_ASSERT_OK to fix the broken TAP.,Replace LITERT_ASSERT_STATUS_OK with LITERT_ASSERT_OK to fix the broken TAP.,2025-02-03T19:58:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86478
copybara-service[bot],[XLA:GPU] Re-enable CubSortPairs test on Hopper.,"[XLA:GPU] Reenable CubSortPairs test on Hopper. This test has been disabled for key value pairs of types U16, F64. The test passes at head and can thus be reenabled.",2025-02-03T19:55:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86477
copybara-service[bot],Update and/or wrap `Executable` when calling into `HloRunnerInterface`.,Update and/or wrap `Executable` when calling into `HloRunnerInterface`.,2025-02-03T19:52:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86476
copybara-service[bot],Add fake `OpaqueExecutable` and update HLO runner interface to use it.,Add fake `OpaqueExecutable` and update HLO runner interface to use it. We want to migrate all uses of `xla::Executable` that interact with the HLO runners to `xla::OpaqueExecutable`. This will be a new class that is not a member of the `xla::Executable` class hierarchy. The plan is for this class to have no public fields or accessors and for it to solely be used for wrapping runnerspecific executables within. This is step 1/3.,2025-02-03T19:51:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86475
copybara-service[bot],Fix includes in `HloRunnerInterface` implementations.,Fix includes in `HloRunnerInterface` implementations.,2025-02-03T19:37:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86474
copybara-service[bot],[HLO-OPT] Tool : register HWI passes from hlo/transforms/ directory,[HLOOPT] Tool : register HWI passes from hlo/transforms/ directory,2025-02-03T19:25:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86473
copybara-service[bot],"Move the many options for printing of instructions into separate file,","Move the many options for printing of instructions into separate file, as this header file is too long already.",2025-02-03T18:59:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86472
copybara-service[bot],Stop passing absl::string_view objects as const references.,Stop passing absl::string_view objects as const references. clangtidy warns it's more efficient to pass them by value.,2025-02-03T18:46:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86471
copybara-service[bot],PR #21375: [ds-fusion] Get While loop analysis with copy fusion,"PR CC(Raspberry Pi install command not properly formatted.): [dsfusion] Get While loop analysis with copy fusion Imported from GitHub PR https://github.com/openxla/xla/pull/21375 In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index. Copybara import of the project:  3147ec926aa1c6fdfa2f4376668434c9a2fbeb87 by Shraiysh Vaishay : [dsfusion] Get While loop analysis with copy fusion In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index.  a435fbd2eadc17269d7bccbe141dcf7a21cc20e8 by Shraiysh Vaishay : Relay control dependencies while converting fusion to call (extractor) Merging this change closes CC(Raspberry Pi install command not properly formatted.) Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21375 from shraiysh:while_loop_analysis a435fbd2eadc17269d7bccbe141dcf7a21cc20e8",2025-02-03T18:39:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86470
copybara-service[bot],Add GL buffer type to LiteRT.,Add GL buffer type to LiteRT.,2025-02-03T18:27:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86469
copybara-service[bot],PR #21708: NUMA-pin host memory buffers for D2H/H2D transfers,"PR CC(tf.contrib.estimator.InMemoryEvaluatorHook runs every step): NUMApin host memory buffers for D2H/H2D transfers Imported from GitHub PR https://github.com/openxla/xla/pull/21708 This ensures that the pinned host buffers used for transfers between host and device are pinned to the NUMA node closest to the device. It had a previous life as https://github.com/openxla/xla/pull/15216. In a benchmark that triggers large, concurrent, copies from all devices to the host then achieved D2H throughput is around 33 GiB/s with NUMA pinning on a DGX H100 node (2xCPU, 8xH100). Without pinning, the achieved throughput is around 13.5 GiB/s from the same benchmark. While it is already possible to achieve the correct NUMA pinning in processperGPU and processperNUMAnode configurations using `numactl` or similar, achieving correct pinning in processpernode configuration requires logic inside XLA. Copybara import of the project:  0eab66c25a49d2c360f5fc5251f08ac8cce4c3ac by Olli Lupton : NUMApin host memory buffers for D2H/H2D transfers  57a46645d6716a55dc3617ff0ad613d31c267804 by Olli Lupton : 256 byte alignment for host allocations when NUMA is not enabled  ad2895a795d8f60512f164d85607510eb45c1b6a by Olli Lupton : Address review comments  629777ec4b2230909a2f98d48c9e726006ec09e8 by Olli Lupton : std::string_view > absl::string_view  21587a5fcf1f57743388256ef28c214ac5b50a23 by Olli Lupton : Apply 's suggested Bazel changes  175c5f6a5ae9c30cca3ac23131744ef1cabfd228 by Olli Lupton : add missing dependency Merging this change closes CC(tf.contrib.estimator.InMemoryEvaluatorHook runs every step) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21708 from olupton:numa 175c5f6a5ae9c30cca3ac23131744ef1cabfd228",2025-02-03T18:19:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86468
copybara-service[bot],Use a smart pointer to store profiler and use `DumpProtoToDirectory` to store XSpace,Use a smart pointer to store profiler and use `DumpProtoToDirectory` to store XSpace to create directories automatically for when `xla_gpu_dump_xspace_to` doesnt exist,2025-02-03T18:16:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86467
copybara-service[bot],Stop using some tsl aliases to absl types in XLA.,Stop using some tsl aliases to absl types in XLA.,2025-02-03T18:09:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86466
copybara-service[bot],Integrate LLVM at llvm/llvm-project@386af4a5c64a,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 386af4a5c64a,2025-02-03T18:07:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86465
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@7775e3e2,Integrate StableHLO at openxla/stablehlo,2025-02-03T17:58:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86464
copybara-service[bot],"Parse XLA_FLAGS environment variable every time, conditionally on xla_flags_reset flag.","Parse XLA_FLAGS environment variable every time, conditionally on xla_flags_reset flag. Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805",2025-02-03T16:58:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86463
copybara-service[bot],1. Renamed google_tensor_compiler_plugin to compiler_plugin as it is already inside the google_tensor/compiler,1. Renamed google_tensor_compiler_plugin to compiler_plugin as it is already inside the google_tensor/compiler 2. Updated the CompileSinglePartition Method in compiler_plugin 3. Reactivated the CompileMulSubgraph Test,2025-02-03T15:44:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86462
copybara-service[bot],PR #22215: make host callback error really an error,PR CC(How to get max and min x and y of bounding boxes from Object_detection_image.py?): make host callback error really an error Imported from GitHub PR https://github.com/openxla/xla/pull/22215 Copybara import of the project:  99693747b52b6f13683f9b120e4670c824ed748f by Yunlong Liu : make host callback error really an error Merging this change closes CC(How to get max and min x and y of bounding boxes from Object_detection_image.py?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22215 from yliu120:cudastream 99693747b52b6f13683f9b120e4670c824ed748f,2025-02-03T15:39:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86461
copybara-service[bot],Fix copy-and-paste error in copybara transformation for hwloc.h,Fix copyandpaste error in copybara transformation for hwloc.h The transformation deals with `third_party/hwloc/hwlocmaster/include/hwloc.h` as if it was a prefix for a whole subdirectory of includes. This changes fixes that.,2025-02-03T15:33:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86460
copybara-service[bot],[Cleanup] Use CHECK_NE nullptr,[Cleanup] Use CHECK_NE nullptr,2025-02-03T15:30:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86459
copybara-service[bot],[xla:cpu] add scatter fusion emitter,[xla:cpu] add scatter fusion emitter Note that the emitter is disabled for now. We will enable it in a followup CL.,2025-02-03T14:37:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86458
copybara-service[bot],[xla:emitters] add FusionWrapperBase,[xla:emitters] add FusionWrapperBase And have XLA:GPU use it. This base class will soon be shared with XLA:CPU. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/23551 from openxla:skozub/warn_tmem 45f3580601098a8a1af489dccb54803acd315143,2025-02-03T13:12:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86457
copybara-service[bot],PR #21998: Add kCall to GetFusibleComputations,"PR CC(TensorRT INT8 assertion failed when performing calibration): Add kCall to GetFusibleComputations Imported from GitHub PR https://github.com/openxla/xla/pull/21998 Previously, we had issues with the stream annotation being applied to nonGemm operations, and having the compilation fail at IR emitter stage. Operations that normally should've been fused weren't getting fused inside the async computation. This PR is intended to fix this issue by including the call computations when searching for fusible computations. This was suggested by   Copybara import of the project:  939f105c3f053eaa14f119e67b5c92ab9e0b3aca by chaser : Add kCall to GetFusibleComputations  1ef97c83e26cb61dc222bd9982f58a140c5d97a9 by chaser : Add test to priority_fusion, edits based on comments Merging this change closes CC(TensorRT INT8 assertion failed when performing calibration) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21998 from chaserileyroberts:chase/fusions_in_async_computation 1ef97c83e26cb61dc222bd9982f58a140c5d97a9",2025-02-03T13:10:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86456
copybara-service[bot],Integrate LLVM at llvm/llvm-project@386af4a5c64a,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 386af4a5c64a,2025-02-03T13:02:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86455
copybara-service[bot],[pjrt] Removed the deprecated overloads of `BufferFromHostBuffer`,[pjrt] Removed the deprecated overloads of `BufferFromHostBuffer`,2025-02-03T12:28:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86454
copybara-service[bot],"[pjrt] Removed unused ""interpreter"" PjRt client","[pjrt] Removed unused ""interpreter"" PjRt client",2025-02-03T12:26:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86453
copybara-service[bot],[XLA:GPU] Rename `IsSyncCollective` and move to a GPU specific file.,[XLA:GPU] Rename `IsSyncCollective` and move to a GPU specific file. The implementation is specific to the GPU backend. Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981,2025-02-03T11:36:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86451
copybara-service[bot],[XLA:GPU] Delete passes and rewriters related to cuDNN FMHA.,"[XLA:GPU] Delete passes and rewriters related to cuDNN FMHA. The custom call remains registered in the frontend, and remains explicitly callable from input HLO. JAX users can use the dedicated attention API to target cuDNN explicitly. The related flag is also deprecated, but remains part of the commandline options for the time being. A future change will delete that as well.",2025-02-03T11:36:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86450
copybara-service[bot],PR #21962: [NFC] Clarify HLO runner status message when execution is disabled.,PR CC(how to do speech test?): [NFC] Clarify HLO runner status message when execution is disabled. Imported from GitHub PR https://github.com/openxla/xla/pull/21962 Copybara import of the project:  847e797822a494d6b95039fd756cf0963028ce38 by Ilia Sergachev : [NFC] Clarify HLO runner status message when execution is disabled. Merging this change closes CC(how to do speech test?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21962 from openxla:fix_runner_message 847e797822a494d6b95039fd756cf0963028ce38,2025-02-03T11:29:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86449
copybara-service[bot],Integrate Triton up to [199da94e](https://github.com/openai/triton/commits/47c730b33a4dfe6d38f9a80b98801917199da94e),Integrate Triton up to 199da94e,2025-02-03T11:18:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86448
copybara-service[bot],PR #22117: [XLA:GPU] Fix FMHA unit tests to not use FMHA rewriter,PR CC(Bad output for sqrt?): [XLA:GPU] Fix FMHA unit tests to not use FMHA rewriter Imported from GitHub PR https://github.com/openxla/xla/pull/22117 * use FMHA custom call directly instead of calling rewriter. Copybara import of the project:  f0a8e4242edaf16df2d42ba490cdad6573aefc95 by cjkkkk : fix  9edd2c7f6047a648e4f83d851dd279042e60c41a by cjkkkk : fix format Merging this change closes CC(Bad output for sqrt?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22117 from Cjkkkk:fix_fmha_rewriter 9edd2c7f6047a648e4f83d851dd279042e60c41a,2025-02-03T11:15:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86447
copybara-service[bot],Update KleidiAI version to `v1.3.0`.,Update KleidiAI version to `v1.3.0`.,2025-02-03T10:34:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86446
copybara-service[bot],PR #22164: [GPU] Fix sharded autotuning compatibility with result caching.,PR CC(Fix broken link in rnn_colorbot): [GPU] Fix sharded autotuning compatibility with result caching. Imported from GitHub PR https://github.com/openxla/xla/pull/22164 The algorithm before this change was:   collect unique noncached fusions   shard them   autotune the shard   publish the shard   read the other shards   merge them into the local cache This did not work when the ranks observed the cache(s) in different states  sharding requires the set of autotuned fusions to be exactly the same. The new algorithm is:   collect unique fusions ignoring the caches   shard them   skip fusions present in the local caches of the rank   autotune the remainder   publish cached + new autotuned results comprising the shard   read the other shards   merge them into the local cache overwriting on conflicts Copybara import of the project:  80e2579255e43fa693c481d3a5775251cffb6335 by Ilia Sergachev : [GPU] Fix sharded autotuning compatibility with result caching. The algorithm before this change was:   collect unique noncached fusions   shard them   autotune the shard   publish the shard   read the other shards   merge them into the local cache This did not work when the ranks observed the cache(s) in different states  sharding requires the set of autotuned fusions to be exactly the same. The new algorithm is:   collect unique fusions ignoring the caches   shard them   skip fusions present in the local caches of the rank   autotune the remainder   publish cached + new autotuned results comprising the shard   read the other shards   merge them into the local cache overwriting on conflicts Merging this change closes CC(Fix broken link in rnn_colorbot) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22164 from openxla:fix_sharded_autotuning_with_caching 80e2579255e43fa693c481d3a5775251cffb6335,2025-02-03T10:01:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86445
copybara-service[bot],Use custom HLO serialization for HloUnoptimizedSnapshot.,Use custom HLO serialization for HloUnoptimizedSnapshot. This change makes it possible to dump HloUnoptimizedSnapshot protos that are over 2GiB in size (the proto binary serialization limit).,2025-02-03T09:21:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86444
copybara-service[bot],[XLA:GPU] move DotDecompose out of simplification pipeline,"[XLA:GPU] move DotDecompose out of simplification pipeline That seems to be a better approach then moving TransposeFold to simplification2 in 961e5c25fbd4082a1ac4f2e0865ad28163d12f7d: 1. There is a report that previous change has resulted in perf degradation https://github.com/openxla/xla/pull/22081 2. I have found another case when DotDecompose is competing with algsimp. Added a test for that. Overall, having an pass that expands operation together with passes that are trying to do the simplification asks for such infinite loops.  For archeologists:   passes DotDimensionSorter and DotDecomposer were added along with GpuAlgebraicSimplifier as it previously could have added multiple contracting dimensions to dot. But cudnn does not support dots with 2+ dimensions, forcing us to use a less efficient loop emitter.  That what ""// AlgebraicSimplifier may add contracting dimensions to a dot."" comment was about. After a while simplifier started to use supports_non_canonical_dots to guard against this case. So it should be safe to remove dot decomposer and friends. Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22258 from openxla:schedule_vlog 025352635a155e447559d83c471369559aad5981",2025-02-03T09:16:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86443
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f0d05b099daf,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f0d05b099daf,2025-02-03T09:01:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86442
copybara-service[bot],PR #22164: [GPU] Fix sharded autotuning compatibility with result caching.,PR CC(Fix broken link in rnn_colorbot): [GPU] Fix sharded autotuning compatibility with result caching. Imported from GitHub PR https://github.com/openxla/xla/pull/22164 The algorithm before this change was:   collect unique noncached fusions   shard them   autotune the shard   publish the shard   read the other shards   merge them into the local cache This did not work when the ranks observed the cache(s) in different states  sharding requires the set of autotuned fusions to be exactly the same. The new algorithm is:   collect unique fusions ignoring the caches   shard them   skip fusions present in the local caches of the rank   autotune the remainder   publish cached + new autotuned results comprising the shard   read the other shards   merge them into the local cache overwriting on conflicts Copybara import of the project:  80e2579255e43fa693c481d3a5775251cffb6335 by Ilia Sergachev : [GPU] Fix sharded autotuning compatibility with result caching. The algorithm before this change was:   collect unique noncached fusions   shard them   autotune the shard   publish the shard   read the other shards   merge them into the local cache This did not work when the ranks observed the cache(s) in different states  sharding requires the set of autotuned fusions to be exactly the same. The new algorithm is:   collect unique fusions ignoring the caches   shard them   skip fusions present in the local caches of the rank   autotune the remainder   publish cached + new autotuned results comprising the shard   read the other shards   merge them into the local cache overwriting on conflicts Merging this change closes CC(Fix broken link in rnn_colorbot) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22164 from openxla:fix_sharded_autotuning_with_caching 80e2579255e43fa693c481d3a5775251cffb6335,2025-02-03T08:32:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86441
copybara-service[bot],Separate collective_permute_cycle for cycle management from SourceTargetPairs container.,Separate collective_permute_cycle for cycle management from SourceTargetPairs container.,2025-02-03T08:03:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86440
tilakrayal,Fixed broken link in create.md,"Hello Team, I found 01 broken documentation link for TensorFlow Lite Model Maker for text Classfication  hyperlinks in create.md file.  So I have updated those links to functional links. Please review and merge this change as appropriate. Thank you!",2025-02-03T07:16:05Z,awaiting review comp:lite ready to pull size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86439
copybara-service[bot],Performance of all casts is now close to raw static_cast and order of magnitude faster than dynamic_cast.,Performance of all casts is now close to raw static_cast and order of magnitude faster than dynamic_cast.,2025-02-03T06:57:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86438
copybara-service[bot],Introduce (de)serialisation of `HloUnoptimizedSnapshot`. The snapshot is serialized in the following format:,Introduce (de)serialisation of `HloUnoptimizedSnapshot`. The snapshot is serialized in the following format: * Metadata * Raw arguments The metadata is `HloUnoptimizedSnapshot` with `HloModuleProto` and a descriptor for each argument. The descriptor specifies the size of the argument in bytes. The raw arguments are serialized in the same format as `Literal::Serialize`. I Updated the `HloUnoptimizedSnapshot` proto structure to store the metadata; introduced `CodedStreamInput(/Output)Iterator`  interface for iterators over protobuf CodedStreams. This allows to use already existing `Literal::[De]Serialize` functions.,2025-02-03T06:36:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86437
amitorko,Update execute.cc for #58676,,2025-02-03T03:58:26Z,size:S comp:core invalid,closed,0,3,https://github.com/tensorflow/tensorflow/issues/86436,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Update execute.(Higher Memory Usage with model.predict in Recent TF Versions (TF 2.10, 2.11 etc))","Hi  , Can you please sign the CLA, Thank you!"
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T03:51:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86435
usmonali4,inconsistent result of ```tf.raw_ops.LogSoftmax``` on CPU and GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04 (google colab)  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5/9  GPU model and memory Tesla T4  Current behavior? result of ``tf.raw_ops.LogSoftmax``` is inconsistent between CPU and GPU  Standalone code to reproduce the issue ```shell import tensorflow as tf logits = tf.constant([[0.0664, 2.3906]], dtype=tf.bfloat16) with tf.device('CPU:0'):   result_cpu = tf.raw_ops.LogSoftmax(logits=logits)   print(""Output on CPU:"", result_cpu) with tf.device('GPU:0'):   result_gpu = tf.raw_ops.LogSoftmax(logits=logits)   print(""Output on GPU:"", result_gpu) max_abs_diff = tf.reduce_max(tf.abs(result_cpu  result_gpu)).numpy() is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e3,  atol=1e2) print(""Max absolute difference:"", max_abs_diff) print(""Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3:"", is_consistent.numpy()) ```  Relevant log output ```shell Output on CPU: tf.Tensor([[0.0825195 2.53125]], shape=(1, 2), dtype=bfloat16) Output on GPU: tf.Tensor([[0.0825195 2.54688]], shape=(1, 2), dtype=bfloat16) Max absolute difference: 0.015625 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: False ```",2025-02-03T03:30:58Z,type:bug WIP comp:ops TF 2.18,open,0,1,https://github.com/tensorflow/tensorflow/issues/86434,"reproduced on different configuration: ```bash TensorFlow Version: 2.16.2 Python Version: 3.10.16  (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] Python Implementation: CPython CUDA Version (TensorFlow): 12.0 cuDNN Version (TensorFlow): 8 GPU Name (PyTorch): NVIDIA GeForce RTX 4070 SUPER CUDA Version (PyTorch): 12.0 cuDNN Version (PyTorch): 8907 Driver Version: 550.144.03 Ubuntu Version: PRETTY_NAME=""Ubuntu 24.04.1 LTS"" Model name:                           AMD Ryzen 5 7600 6Core Processor Output on CPU: tf.Tensor([[0.0825195 2.53125]], shape=(1, 2), dtype=bfloat16) Output on GPU: tf.Tensor([[0.0825195 2.54688]], shape=(1, 2), dtype=bfloat16) Max absolute difference: 0.015625 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: False ```"
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T03:04:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86433
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T03:03:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86432
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T03:03:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86431
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T03:02:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86430
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T03:00:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86429
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:55:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86428
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:54:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86427
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:53:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86426
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:53:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86425
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:52:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86424
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:51:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86423
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:51:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86422
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:50:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86421
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:49:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86420
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:49:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86419
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:48:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86418
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:45:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86417
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:44:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86416
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:39:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86415
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-03T02:38:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86414
hmaarrfk,nvrtc-builtins are private don't link to them,This is a small patch to help with cuda compatibility. I do'nt think this is necessary and links to `lib/libnvrtcbuiltins.so.12.6.85` which is not compatible accross different versions of cuda. xref: https://github.com/condaforge/tensorflowfeedstock/pull/414issuecomment2629135833,2025-02-03T02:08:34Z,awaiting review comp:gpu size:XS,open,0,26,https://github.com/tensorflow/tensorflow/issues/86413,I guess this is why I get a dependency hell while trying to blend in PyTorch and TensorFlow in pip.,"Please don't submit this change. `libnvrtcbuiltins.so.12.5` is dynamically loaded by `libnvrtc.so.12`: ``` file=libnvrtcbuiltins.so.12.5 [0];  dynamically loaded by /b/f/w/bazelout/k8opt/bin/tensorflow/core/kernels/matmul_op_test_gpu.runfiles/org_tensorflow/tensorflow/core/kernels/../../../_solib_local/_U/libnvrtc.so.12 [0] ``` You can check the test logs after running the command `bazel test repo_env=HERMETIC_PYTHON_VERSION=3.10 config release_gpu_linux //cuda:override_include_cuda_libs=true //tensorflow/core/kernels:matmul_op_test_gpu repo_env=USE_PYWRAP_RULES=True action_env=LD_DEBUG=files,libs`. If we don't have `libnvrtcbuiltins.so.12.5` in NEEDED section of TensorFlow ELF files, and we don't load it into Bazel cache, then the linker will search for the library outside of Bazel cache  this makes CUDA nonhermetic. I've read the discussion in https://github.com/condaforge/tensorflowfeedstock/pull/414issuecomment2629135833  as far as I understand, you are having issues with forward compatibility mode for CUDA 12.6. Let me try to reproduce the error."," Just to understand the situation a bit more clear: do you really need to build Tensorflow with hermetic CUDA? This configuration is mostly used for internal testing. If you want to have the same configuration as the release TF wheel (with CUDA stubs), then you need to provide `config=cuda_wheel` for the `bazel build` command.","I couldn't reproduce the problem with `libnvrtcbuiltins.so` with CUDA 12.6.3. I've tried with`//:enable_forward_compatibility=true` and without it  still no issues. `nvidiasmi` shows the following information: `Driver Version: 555.42.06`. To troubleshoot the issue further, I need more details about the environment where Bazel command is executed, and Bazel command options.","Thank you for the time spent reviewing! >  If you want to have the same configuration as the release TF wheel (with CUDA stubs), then you need to provide config=cuda_wheel for the bazel build command. I can try this too. I honestly didn't know which to choose. let me try.","I tried ```     echo ""build config=cuda_wheel"" >> .bazelrc ``` in our build scripts https://github.com/condaforge/tensorflowfeedstock/pull/421 and unfortunately it just seems to ""freeze"" my build locally.","Hi  may I ask you to provide the logs? Not sure what you mean by ""frozen build"".",It ran for 12 hours and got to this point: !image tfnonhermeticjoblogs.txt The build script can be found in https://github.com/condaforge/tensorflowfeedstock/pull/421,It looks like a GitHub action problem. `config=cuda_wheel` replaces real CUDA libraries dependencies with stubs defined here. Tensorflow and JAX nightly builds use the same configuration.,"Hi , when executing `configure`, it does need to set environment variables for hermetic CUDA like `HERMETIC_CUDA_COMPUTE_CAPABILITIES`. Otherwise, the script will wait for the standard input. https://github.com/tensorflow/tensorflow/blob/6550e4bd80223cdb8be6c3afd1f81e86a4d433c3/configure.pyL994L999 I don't think this problem exists in XLA or JAX. What do you suggest for the `configure` script?","Hi  , you can just provide ""need CUDA"" in the configuration, and skip answering all other questions, in this case hermetic CUDA environment variables take default values. I've just tried it on my machine: ``` $ ./configure  You have bazel 6.5.0 installed. Please specify the location of python. [Default is /usr/bin/python3]:  Found possible Python library paths:   /usr/lib/python3/distpackages   /usr/local/lib/python3.9/distpackages Please input the desired Python library path to use.  Default is [/usr/lib/python3/distpackages] Do you wish to build TensorFlow with ROCm support? [y/N]:  No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: y CUDA support will be enabled for TensorFlow. Please specify the hermetic CUDA version you want to use or leave empty to use the default version.  Please specify the hermetic cuDNN version you want to use or leave empty to use the default version.  Please specify a list of commaseparated CUDA compute capabilities you want to build with. You can find the compute capability of your device at: https://developer.nvidia.com/cudagpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code. Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:  Please specify the local CUDA path you want to use or leave empty to use the default version.  Please specify the local CUDNN path you want to use or leave empty to use the default version.  Please specify the local NCCL path you want to use or leave empty to use the default version.  Do you want to use clang as CUDA compiler? [Y/n]:  Clang will be used as CUDA compiler. Please specify clang path that to be used as host compiler. [Default is /usr/lib/llvm18/bin/clang]:  You have Clang 18.1.8 installed. Please specify optimization flags to use during compilation when bazel option ""config=opt"" is specified [Default is Wnosigncompare]:  Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:  Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""config="" to your build command. See .bazelrc for more details.         config=mkl             Build with MKL support.         config=mkl_aarch64     Build with oneDNN and Compute Library for the Arm Architecture (ACL).         config=monolithic      Config for mostly static monolithic build.         config=numa            Build with NUMA support.         config=dynamic_kernels         (Experimental) Build kernels into separate shared objects.         config=v1              Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:         config=nogcp           Disable GCP support.         config=nonccl          Disable NVIDIA NCCL support. Configuration finished ``` This is the resulting config: ``` $ cat .tf_configure.bazelrc  build action_env PYTHON_BIN_PATH=""/usr/bin/python3"" build action_env PYTHON_LIB_PATH=""/usr/lib/python3/distpackages"" build python_path=""/usr/bin/python3"" build:cuda repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=""3.5,7.0"" build config=cuda_clang build action_env CLANG_CUDA_COMPILER_PATH=""/usr/lib/llvm18/bin/clang"" build config=cuda_clang build:opt copt=Wnosigncompare build:opt host_copt=Wnosigncompare test test_size_filters=small,medium test test_env=LD_LIBRARY_PATH test:v1 test_tag_filters=benchmarktest,no_oss,oss_excluded,no_gpu,oss_serial test:v1 build_tag_filters=benchmarktest,no_oss,oss_excluded,no_gpu test:v2 test_tag_filters=benchmarktest,no_oss,oss_excluded,no_gpu,oss_serial,v1only test:v2 build_tag_filters=benchmarktest,no_oss,oss_excluded,no_gpu,v1only ``` `HERMETIC_CUDA_VERSION` and `HERMETIC_CUDNN_VERSION` are defined in `.bazelrc`: https://github.com/tensorflow/tensorflow/blob/master/.bazelrcL228L229","Hi  , Can you please resolve the conflicts? Many thanks !","> Hi  , Can you please resolve the conflicts? Many thanks ! Is there renewed intent to merge?","Can you remind me please, what exactly is not working with `libnvrtcbuiltins.so`? Are you trying to use different CUDA versions when blending PyTorch and Tensorflow together? As mentioned in https://github.com/tensorflow/tensorflow/pull/86413issuecomment2648726318, this change makes CUDA nonhermetic.","Truthfully, I don't fully understand the ramifications of  hermetic cuda. The way we use it is: 1. We specify hermetic cuda to point to our installation of cuda, located outside the ""system installation"". 2. We compile with version 12.X. 3. We wish to run with version 12.Y>=12.X. 4. System cuda can be 12.Z (any unrelated version). As I understand it, cuda compatibility allows this kind of behavior.  I understand that this might trigger some exceptional bugs in tensorflow since tensorflow is a heavy user of the CUDA API. I think  and  might be in a better source if you have followup questions. My understanding is that you aren't using any symbols located in the `libnvrtcbuiltins` (which is required to match at the Minor version of cuda, but rather you only use `libnvrtc.so.12` which has shared symbols accros versions of CUDA 12."," thank you for the explanation. I think the case you've described applies to local CUDA installation only. In this situation I can suggest commenting out `cc_import` for `libnvrtcbuiltins.so` when `LOCAL_CUDA_PATH` is set. It can be done somewhere at these lines  https://github.com/openxla/xla/blob/main/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzlL188L195. You can add special type of comments around that `cc_import`, and replace them by real comment symbols when `LOCAL_CUDA_PATH` is set (for `cuda_nvrtc` repo only). This will preserve hermetic CUDA state when nonlocal installation is used.",Ok i opened https://github.com/condaforge/tensorflowfeedstock/issues/427 to track and we can get back to you.,"We will followup soon™ (soon™ is meant to be a joke, but I do appreciate your response and marked this as draft to signal that the ball is a bit in our hand to respond technically to your suggestion!)",Would you check if this change solves the issue please? https://github.com/tensorflow/tensorflow/pull/90073/files,"Not sure if i have too much time to personally spend on github + condaforge and coding this next week. It seems that this code will effectively be removed for ""local_cuda"". How do we ensure that we trigger the `local_cuda` codepath? is it enough to specify the environment variable `LOCAL_CUDA_PATH` like we do https://github.com/condaforge/tensorflowfeedstock/blob/main/recipe/build.shL134 or is there more we have to do?","Yes, providing `repo_env=LOCAL_CUDA_PATH=` will be sufficient to disable `nvrtcbuiltins` dependency."," the patch didn't apply cleanly to 2.18 but I think I managed to adapt it. there is one more patch that we carry around for a similar purpose that helps us get around the following error: ``` [18,829 / 28,256] 8 actions running ERROR: /home/conda/feedstock_root/build_artifacts/debug_1743300640382/work/tensorflow/cc/BUILD:655:22: Executing genrule //tensorflow/cc:array_ops_genrule failed: (Exit 127): bash failed: error executing command (from target //tensorflow/cc:array_ops_genrule)   (cd /home/conda/feedstock_root/build_artifacts/debug_1743300640382/_build_env/share/bazel/bfd45444dcb773346f0373b4b3db4f62/execroot/org_tensorflow && \   exec env  \     GCC_HOST_COMPILER_PATH=/home/conda/feedstock_root/build_artifacts/debug_1743300640382/_build_env/bin/x86_64condalinuxgnugcc \     PATH=/home/conda/feedstock_root/build_artifacts/debug_1743300640382/work:/home/conda/feedstock_root/build_artifacts/debug_1743300640382/_build_env/bin:/home/conda/feedstock_root/build_artifacts/debug_1743300640382/_build_env/bin:/home/conda/feedstock_root/build_artifacts/debug_1743300640382/_h_env/bin:/home/conda/feedstock_root/build_artifacts/debug_1743300640382/_h_env/bin:/opt/conda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/conda/bin \     PYTHON_BIN_PATH=/home/conda/feedstock_root/build_artifacts/debug_1743300640382/_h_env/bin/python \     PYTHON_LIB_PATH=/home/conda/feedstock_root/build_artifacts/debug_1743300640382/_h_env/lib/python3.12/sitepackages \     TF2_BEHAVIOR=1 \     TF_SYSTEM_LIBS=astor_archive,astunparse_archive,boringssl,com_github_googlecloudplatform_google_cloud_cpp,com_github_grpc_grpc,com_google_absl,com_google_protobuf,curl,cython,dill_archive,flatbuffers,gast_archive,gif,icu,libjpeg_turbo,org_sqlite,png,pybind11,snappy,zlib \   /bin/bash bazelout/k8opt/bin/tensorflow/cc/array_ops_genrule.genrule_script.sh)  Configuration: 8cd406efead458e7533b2bcf0e7401dead9590a5410f20c300a5880a32f35463  Execution platform: //:platform bazelout/k8optexec50AE0418/bin/tensorflow/cc/ops/array_ops_gen_cc: error while loading shared libraries: libcuda.so.1: cannot open shared object file: No such file or directory ERROR: /home/conda/feedstock_root/build_artifacts/debug_1743300640382/work/tensorflow/tools/pip_package/BUILD:266:9 Action tensorflow/tools/pip_package/wheel_house failed: (Exit 127): bash failed: error executing command (from target //tensorflow/cc:array_ops_genrule)   (cd /home/conda/feedstock_root/build_artifacts/debug_1743300640382/_build_env/share/bazel/bfd45444dcb773346f0373b4b3db4f62/execroot/org_tensorflow && \   exec env  \     GCC_HOST_COMPILER_PATH=/home/conda/feedstock_root/build_artifacts/debug_1743300640382/_build_env/bin/x86_64condalinuxgnugcc \     PATH=/home/conda/feedstock_root/build_artifacts/debug_1743300640382/work:/home/conda/feedstock_root/build_artifacts/debug_1743300640382/_build_env/bin:/home/conda/feedstock_root/build_artifacts/debug_1743300640382/_build_env/bin:/home/conda/feedstock_root/build_artifacts/debug_1743300640382/_h_env/bin:/home/conda/feedstock_root/build_artifacts/debug_1743300640382/_h_env/bin:/opt/conda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/conda/bin \     PYTHON_BIN_PATH=/home/conda/feedstock_root/build_artifacts/debug_1743300640382/_h_env/bin/python \     PYTHON_LIB_PATH=/home/conda/feedstock_root/build_artifacts/debug_1743300640382/_h_env/lib/python3.12/sitepackages \     TF2_BEHAVIOR=1 \     TF_SYSTEM_LIBS=astor_archive,astunparse_archive,boringssl,com_github_googlecloudplatform_google_cloud_cpp,com_github_grpc_grpc,com_google_absl,com_google_protobuf,curl,cython,dill_archive,flatbuffers,gast_archive,gif,icu,libjpeg_turbo,org_sqlite,png,pybind11,snappy,zlib \   /bin/bash bazelout/k8opt/bin/tensorflow/cc/array_ops_genrule.genrule_script.sh)  Configuration: 8cd406efead458e7533b2bcf0e7401dead9590a5410f20c300a5880a32f35463  Execution platform: //:platform INFO: Elapsed time: 7807.112s, Critical Path: 538.40s INFO: 19532 processes: 5067 internal, 14465 local. FAILED: Build did NOT complete successfully ``` Might you have (in your mind!) an other patch for the following workaround we have https://github.com/condaforge/tensorflowfeedstock/blob/main/recipe/patches/0030removedependenciestolibcuda.patch","Hi  , it looks like you don't have `libcuda.so.1` (usermode driver) in `PATH` or `LD_LIBRARY_PATH`. You commented out the stub `libcuda.so.1` from the dependencies, and now the linker fails to find it.","The reason why we remove the `libcuda.so.1` stub is that we pull it from the ""system's installation"" which might not even exist for CPU only systems. (Advanced) users may have: 1. A shared CPU/GPU conda environment. 2. Force install the GPU packages 3. CPU only machine (without system cuda) tries to run `import tensorflow` 4. I expect the step above to work. When we patched things out we managed to get things to compile, with GPU support when the users have a GPU, and keeping tensorflow importable when they don't have the CUDA drivers on their CPUonly machine.","There are several ways to build Tensorflow, so I'm trying to guess which one you use here. 1) CPU machine (no NVIDIA driver installed, CUDA libs shouldn't be present in the ELF files `NEEDED` section) `bazel build  config=cuda_wheel //tensorflow/tools/pip_package:wheel` 2) CPU machine (no NVIDIA driver installed, CUDA libs should be present in the ELF files `NEEDED` section) `bazel build  //cuda:override_include_cuda_libs=true //:enable_forward_compatibility=true //tensorflow/tools/pip_package:wheel` The build will use `libcuda.so.1` provided in the downloaded NVIDIA UMD redistribution. 3) GPU machine (NVIDIA driver installed, CUDA libs shouldn't be present in the ELF files `NEEDED` section) `bazel build  config=cuda_wheel //tensorflow/tools/pip_package:wheel` 4) GPU machine (NVIDIA driver installed, CUDA libs should be present in the ELF files `NEEDED` section) `bazel build  //cuda:override_include_cuda_libs=true //tensorflow/tools/pip_package:wheel` > The reason why we remove the libcuda.so.1 stub is that we pull it from the ""system's installation"" which might not even exist for CPU only systems. In this case you might need either to use 1) or to use 2) (without removing cuda stub in the patch)."," i might have limited bandwidth in the near term, i'm hoping that others in the condaforge team can jump in before me if they have time to think through the (thorough) information you've provided. I'll try to personally get to this next week."
copybara-service[bot],always use fusion emitters,always use fusion emitters,2025-02-03T01:46:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86412
copybara-service[bot],[ODML] StablehloUnfuseBatchNormPass: Migrate from MHLO -> StableHLO.,[ODML] StablehloUnfuseBatchNormPass: Migrate from MHLO > StableHLO.,2025-02-03T00:33:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86411
dicotom,ERROR: An error occurred during the fetch of repository 'local_config_def_file_filter': Auto-Configuration Error: Visual C++ build tools not found on your machine," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution windows 11 pro  Mobile device N/A  Python version 3.12  Bazel version 6.5.0  GCC/compiler version 	CLANG 17.0.6  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? that it would be compiled, I followed the instructions for windows https://www.tensorflow.org/install/source_windows  Standalone code to reproduce the issue ```shell N/A ```  Relevant log output ```shell DEBUG: C:/users/user/_bazel_user/yopxtjri/external/local_xla/third_party/py/python_repo.bzl:107:10: Using hermetic Python 3.12 INFO: Repository local_config_def_file_filter instantiated at:   C:/users/user/tensorflow/WORKSPACE:64:14: in    C:/users/user/tensorflow/tensorflow/workspace2.bzl:936:19: in workspace   C:/users/user/tensorflow/tensorflow/workspace2.bzl:117:30: in _tf_toolchains Repository rule def_file_filter_configure defined at:   C:/users/user/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl:54:44: in  ERROR: An error occurred during the fetch of repository 'local_config_def_file_filter':    Traceback (most recent call last):         File ""C:/users/user/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl"", line 32, column 28, in _def_file_filter_configure_impl                 auto_configure_fail(""Visual C++ build tools not found on your machine"")         File ""C:/users/user/_bazel_user/yopxtjri/external/bazel_tools/tools/cpp/lib_cc_configure.bzl"", line 112, column 9, in auto_configure_fail                 fail(""\n%sAutoConfiguration Error:%s %s\n"" % (red, no_color, msg)) Error in fail: AutoConfiguration Error: Visual C++ build tools not found on your machine ERROR: C:/users/user/tensorflow/WORKSPACE:64:14: fetching def_file_filter_configure rule //external:local_config_def_file_filter: Traceback (most recent call last):         File ""C:/users/user/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl"", line 32, column 28, in _def_file_filter_configure_impl                 auto_configure_fail(""Visual C++ build tools not found on your machine"")         File ""C:/users/user/_bazel_user/yopxtjri/external/bazel_tools/tools/cpp/lib_cc_configure.bzl"", line 112, column 9, in auto_configure_fail                 fail(""\n%sAutoConfiguration Error:%s %s\n"" % (red, no_color, msg)) Error in fail: AutoConfiguration Error: Visual C++ build tools not found on your machine ERROR: C:/users/user/tensorflow/tensorflow/python/BUILD:978:8: //tensorflow/python:pywrap_tensorflow_filtered_def_file depends on //:def_file_filter in repository  which failed to fetch. no such package '//': AutoConfiguration Error: Visual C++ build tools not found on your machine ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted: INFO: Elapsed time: 584.412s INFO: 0 processes. FAILED: Build did NOT complete successfully (453 packages loaded, 9221 targets configured)     currently loading: // ... (6 packages)     Fetching repository ; starting 6s     Fetching ...r/yopxtjri/external/eigen_archive; Extracting eigen33d0937c6bdf5ec999939fb17f2a553183d14a74.tar.gz 6s     Fetching repository ; starting ```",2025-02-02T19:31:16Z,type:build/install,closed,0,2,https://github.com/tensorflow/tensorflow/issues/86410,Are you satisfied with the resolution of your issue? Yes No,my fault.
copybara-service[bot],[XLA:FFI] Add an FFI compatible implementation of tsl::CountDownAsyncValueRef.,[XLA:FFI] Add an FFI compatible implementation of tsl::CountDownAsyncValueRef. This supports the common pattern of enqueuing a specific number of async tasks within an FFI handler.,2025-02-02T19:09:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86409
copybara-service[bot],Update XNNPACK version.,Update XNNPACK version.,2025-02-02T14:02:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86408
Cloudberrydotdev,User Guide: Deprecated Nvidia Docker Link," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version any  Custom code No  OS platform and distribution Linux GPU  Mobile device _No response_  Python version any  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The link to the Nvidia Docker github https://github.com/NVIDIA/nvidiadocker?tab=readmeovfile Reports: This repository has been archived by the owner on Jan 22, 2024. It is now readonly.  and provides a link to: https://github.com/NVIDIA/nvidiacontainertoolkit Titled: Build and run containers leveraging NVIDIA GPUs   Standalone code to reproduce the issue ```shell This is a documentation bug and no code errors are involved. ```  Relevant log output ```shell See above ```",2025-02-02T09:16:37Z,type:docs-bug stat:awaiting response type:build/install stale,closed,0,9,https://github.com/tensorflow/tensorflow/issues/86407,https://www.tensorflow.org/install/docker Sorry I meant to include this url for the documentation page that holds the error link.,The relevant text is: Docker is the easiest way to enable TensorFlow GPU support on Linux since only the NVIDIA® GPU driver is required on the host machine (the NVIDIA® CUDA® Toolkit does not need to be installed). The link is in the text: NVIDIA® GPU driver,  您好，邮件已经收到，我会尽快处理的。谢谢,"Sam, Thanks for picking this up. I'm not sure what happens now. Do I wait for a reply from the Tensorflow documentation team before closing this issue? Regards Ian Berry On Sun, 2 Feb 2025, 16:35 Sam Fletcher, ***@***.***> wrote: > The issue is that the TensorFlow documentation still links to the > nowarchived NVIDIA Docker repository. Instead, it should link to the new, > active repository: > > Solution: > >    1. The TensorFlow documentation team needs to update the incorrect >    link. >    2. The old link: > >    https://github.com/NVIDIA/nvidiadocker?tab=readmeovfile > >    Should be replaced with: > >    https://github.com/NVIDIA/nvidiacontainertoolkit > >    3. If you're relying on the old NVIDIA Docker setup, transition to >    using the *NVIDIA Container Toolkit* as per the new repository. You >    can follow their official setup guide: > > For now, you can manually install TensorFlow with GPU support using the > NVIDIA Container Toolkit instead of the deprecated NVIDIA Docker. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",", Thank you for reporting the issue. I have raised the internal request for the mentioned change and will be updated once it gets submitted. Thank you!",", The raised request was submitted and also the changes are reflected in the official document. https://www.tensorflow.org/install/dockertensorflow_docker_requirements reflecting to https://github.com/NVIDIA/nvidiacontainertoolkit !Image Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
usmonali4,inconsistent result of ```tf.raw_ops.L2Loss``` on CPU and GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04 (google colab)  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5/9  GPU model and memory Tesla T4  Current behavior? result of ```tw.raw_ops.L2Loss``` is inconsistent between CPU and GPU  Standalone code to reproduce the issue ```shell import tensorflow as tf t = tf.constant([     [[0.9922, 1.4922],       [0.0376,  0.1504],       [0.6172,  1.2266]],     [[0.1387,  1.3047],       [0.3535, 0.0471],       [0.0437,  0.2637]] ], dtype=tf.bfloat16) with tf.device('CPU:0'):   result_cpu = tf.raw_ops.L2Loss(t=t)   print(""Output on CPU:"", result_cpu) with tf.device('GPU:0'):   result_gpu = tf.raw_ops.L2Loss(t=t)   print(""Output on GPU:"", result_gpu) max_abs_diff = tf.reduce_max(tf.abs(result_cpu  result_gpu)).numpy() is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e3,  atol=1e2) print(""Max absolute difference:"", max_abs_diff) print(""Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3:"", is_consistent.numpy()) ```  Relevant log output ```shell Output on CPU: tf.Tensor(3.51562, shape=(), dtype=bfloat16) Output on GPU: tf.Tensor(3.53125, shape=(), dtype=bfloat16) Max absolute difference: 0.015625 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: False ```",2025-02-02T07:05:17Z,type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/86406,I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TFnightly on both CPU and GPU. Please find gist1 and gist2 here for your reference. Thank you!,"no issue here: ```bash TensorFlow Version: 2.16.2 Python Version: 3.10.16  (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] Python Implementation: CPython CUDA Version (TensorFlow): 12.0 cuDNN Version (TensorFlow): 8 GPU Name (PyTorch): NVIDIA GeForce RTX 4070 SUPER CUDA Version (PyTorch): 12.0 cuDNN Version (PyTorch): 8907 Driver Version: 550.144.03 Ubuntu Version: PRETTY_NAME=""Ubuntu 24.04.1 LTS"" Model name:                           AMD Ryzen 5 7600 6Core Processor Output on CPU: tf.Tensor(3.53125, shape=(), dtype=bfloat16) Output on GPU: tf.Tensor(3.53125, shape=(), dtype=bfloat16) Max absolute difference: 0 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: True ```"
regularRandom,TF wheel shouldn't be built with CUDA dependencies," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution CentOS Stream 9  Mobile device _No response_  Python version 3.9  Bazel version 6.5.0  GCC/compiler version 11.5.0  CUDA/cuDNN version 12.6.1/8.9.7.29  GPU model and memory 2080 Ti 11GB  Current behavior? Build fails with following error: Error in fail: TF wheel shouldn't be built with CUDA dependencies. Please provide `config=cuda_wheel` for bazel build command. If you absolutely need to add CUDA dependencies, provide `//cuda:override_include_cuda_libs=true`. Current driver: ` NVIDIASMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8 ` Build command: ` bazel build config=cuda local_cpu_resources=HOST_CPUS*.8 //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow ` What I missed?  Standalone code to reproduce the issue ```shell You have bazel 6.5.0 installed. Please specify the location of python. [Default is /usr/bin/python3]: Found possible Python library paths:   /usr/lib/python3.9/sitepackages   /usr/lib64/python3.9/sitepackages   /usr/local/lib/python3.9/sitepackages   /usr/local/lib64/python3.9/sitepackages Please input the desired Python library path to use.  Default is [/usr/lib/python3.9/sitepackages] Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: y CUDA support will be enabled for TensorFlow. Please specify the hermetic CUDA version you want to use or leave empty to use the default version. 12.6.1 Please specify the hermetic cuDNN version you want to use or leave empty to use the default version. Please specify a list of commaseparated CUDA compute capabilities you want to build with. You can find the compute capability of your device at: https://developer.nvidia.com/cudagpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code. Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.5 Please specify the local CUDA path you want to use or leave empty to use the default version. Please specify the local CUDNN path you want to use or leave empty to use the default version. Please specify the local NCCL path you want to use or leave empty to use the default version. Do you want to use clang as CUDA compiler? [Y/n]: n nvcc will be used as CUDA compiler. Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/lib64/ccache/gcc]: Please specify optimization flags to use during compilation when bazel option ""config=opt"" is specified [Default is Wnosigncompare]: Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""config="" to your build command. See .bazelrc for more details.         config=mkl             Build with MKL support.         config=mkl_aarch64     Build with oneDNN and Compute Library for the Arm Architecture (ACL).         config=monolithic      Config for mostly static monolithic build.         config=numa            Build with NUMA support.         config=dynamic_kernels         (Experimental) Build kernels into separate shared objects.         config=v1              Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:         config=nogcp           Disable GCP support.         config=nonccl          Disable NVIDIA NCCL support. Configuration finished ```  Relevant log output ```shell ERROR: /usr/src/tensorflow/tensorflow/tools/pip_package/BUILD:278:9: in tf_wheel rule //tensorflow/tools/pip_package:wheel: Traceback (most recent call last):         File ""/usr/src/tensorflow/tensorflow/tools/pip_package/utils/tf_wheel.bzl"", line 72, column 13, in _tf_wheel_impl                 fail(""TF wheel shouldn't be built with CUDA dependencies."" + Error in fail: TF wheel shouldn't be built with CUDA dependencies. Please provide `config=cuda_wheel` for bazel build command. If you absolutely need to add CUDA dependencies, provide `//cuda:override_include_cuda_libs=true`. ERROR: /usr/src/tensorflow/tensorflow/tools/pip_package/BUILD:278:9: Analysis of target '//tensorflow/tools/pip_package:wheel' failed ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted: INFO: Elapsed time: 189.679s INFO: 0 processes. FAILED: Build did NOT complete successfully (790 packages loaded, 56723 targets configured) ```",2025-02-02T04:59:19Z,type:build/install subtype: ubuntu/linux TF 2.18,open,0,4,https://github.com/tensorflow/tensorflow/issues/86405,", Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.In this case, Tensorflow v2.18 is compatible with python 3.93.12, compiler(Clang) 17.0.6, Bazel  6.5.0,  cudNN9.3, CUDA12.5 https://www.tensorflow.org/install/sourcegpu Also To build tensorflow GPU package: `bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow config=cuda config=cuda_wheel` https://www.tensorflow.org/install/sourcebuild_the_package Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.," where can I find a list of exact requirements for 2.18 & 2.19? Currently I have CUDA 12.6, libcudnn88.9.7.29, Bazel 6.5.0, clang19.1.7, ccache 4.5.1. Downgraded to CUDA 12.5  it fails with the following message: `ERROR: /root/.cache/bazel/_bazel_md/cbe8b06b94787a6b39e59564d90f2497/external/boringssl/BUILD:133:11: Compiling winaarch64/crypto/test/trampolinearmv8win.S failed: (Exit 1): clang failed: error executing command (from target //:crypto) /usr/lib64/ccache/clang MD MF bazelout/k8opt/bin/external/boringssl/_objs/crypto/trampolinearmv8win.pic.d ... (remaining 59 arguments skipped) clang: error: argument unused during compilation: 'cudapath=external/cuda_nvcc' [Werror,Wunusedcommandlineargument] Target //tensorflow/tools/pip_package:wheel failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 2.910s, Critical Path: 0.11s INFO: 41 processes: 41 internal. FAILED: Build did NOT complete successfully ` What I missed?","I don't understand what is going on. Each try ends with the new error message. I removed bazel & ccache caches (rm rf) & tensorflow sources. Then I recloned it from github and now I am getting following error in origin/r2.18 branch: ` ERROR: /root/.cache/bazel/_bazel_md/cbe8b06b94787a6b39e59564d90f2497/external/local_xla/xla/service/gpu/BUILD:2261:13: Compiling xla/service/gpu/stream_executor_util_kernel.cu.: (Exit 1): clang failed: error executing command (from target //xla/service/gpu:stream_executor_util_kernel) /usr/lib64/ccache/clang MD MF bazelout/k8opt/bin/external/local_xla/xla/service/gpu/_objs/stream_executor_util_kernel/stream_executor_util_kernel.cu.pic.d ... (remaining 168 arguments skipped) clang: error: unknown argument: 'Xcudafatbinary=compressall' clang: error: unknown argument: 'nvcc_options=exptrelaxedconstexpr' clang: warning: CUDA version is newer than the latest partially supported version 12.5 [Wunknowncudaversion] clang: error: GPU arch sm_35 is supported by CUDA versions between 7.0 and 11.8 (inclusive), but installation at external/cuda_nvcc is ; use 'cudapath' to specify a different CUDA install, pass a different GPU arch with 'cudagpuarch', or pass 'nocudaversioncheck' Target //tensorflow/tools/pip_package:wheel failed to build Use verbose_failures to see the command lines of failed build steps. ERROR: /usr/src/tensorflow/tensorflow/tools/pip_package/BUILD:266:9 Action tensorflow/tools/pip_package/wheel_house failed: (Exit 1): clang failed: error executing command (from target //xla/service/gpu:stream_executor_util_kernel) /usr/lib64/ccache/clang MD MF bazelout/k8opt/bin/external/local_xla/xla/service/gpu/_objs/stream_executor_util_kernel/stream_executor_util_kernel.cu.pic.d ... (remaining 168 arguments skipped) ` I just removed CUDA 12.6 and installed 12.5  what is wrong with that? `cudatoolkit12512.5.11.x86_64`"
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T04:28:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86404
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T03:18:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86403
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:48:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86402
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:47:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86401
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:47:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86400
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:45:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86399
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:43:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86398
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:43:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86397
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:43:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86396
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:43:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86395
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:40:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86394
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:40:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86393
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:36:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86392
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:36:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86391
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:35:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86390
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:29:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86389
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:28:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86388
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:28:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86387
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:26:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86386
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:25:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86385
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-02T02:24:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86384
copybara-service[bot],"Remove unused/rarely used functions from HloOpCode. Parametarize test for HloOpCode. Change `HloOpcodeArity` return type to `int8_t`, since `kHloOpcodeMaxArity` is 31.","Remove unused/rarely used functions from HloOpCode. Parametarize test for HloOpCode. Change `HloOpcodeArity` return type to `int8_t`, since `kHloOpcodeMaxArity` is 31.",2025-02-02T01:11:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86383
copybara-service[bot],Remove unnecessary pointers in CollectiveDeviceList.,Remove unnecessary pointers in CollectiveDeviceList.,2025-02-02T01:08:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86382
copybara-service[bot],[XLA:GPU] pass builder explicitly for better Emitter Loc annotations of the generated triton.,[XLA:GPU] pass builder explicitly for better Emitter Loc annotations of the generated triton. This fix helps us understand from which line and by which function a specific triton instruction was submitted.,2025-02-01T15:23:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86381
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T12:32:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86380
copybara-service[bot],[xla:cpu] introduce FusionWrapper pass,"[xla:cpu] introduce FusionWrapper pass This pass wraps scatter ops with a fusion, so that the fusion emitter will be able to do its thing.",2025-02-01T10:55:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86379
usmonali4,inconsistent result of ```tf.raw_ops.BiasAddGrad``` on CPU and GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5/9  GPU model and memory Tesla T4  Current behavior? ```tf.raw_ops.BiasAddGrad``` produces inconsistent results between CPU and GPU  Standalone code to reproduce the issue ```shell import tensorflow as tf out_backprop = tf.constant([     [         [             [[ 0.2207,  2.1094], [0.3730, 1.0625], [ 1.7031,  0.7148]],              [[ 1.5078, 0.6719], [0.6367,  0.5039], [2.3281,  0.5078]]         ],         [             [[0.3574,  0.0461], [ 2.3750, 2.9688], [0.5703, 2.0156]],             [[ 0.8125,  1.7656], [0.9570,  0.6250], [0.6914, 0.4746]]         ],         [             [[0.3750, 0.7383], [ 0.3691,  0.4570], [ 1.1641,  0.2715]],             [[1.2969, 0.9844], [0.4863,  1.0938], [1.4297,  0.8086]]         ]     ],     [         [             [[ 0.3730,  0.8477], [0.3887,  1.2266], [ 0.0859, 0.5742]],             [[0.7383, 0.2432], [0.7578, 0.8281], [0.1660, 0.9336]]         ],         [             [[ 1.4297,  0.6797], [1.6172,  0.4941], [0.3047, 0.3711]],             [[0.6250, 0.7617], [ 0.9453,  0.1064], [ 1.4062, 2.9531]]         ],         [             [[1.4297, 0.1387], [ 0.0625,  1.0469], [0.1953,  1.6406]],             [[0.3047,  0.5117], [ 1.8125,  1.1797], [0.8789, 0.4688]]         ]     ] ], dtype=tf.bfloat16) with tf.device('CPU:0'):   result_cpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=""NCHW"")   print(""BiasAddGrad Output on CPU:"", result_cpu) with tf.device('GPU:0'):   result_gpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=""NCHW"")   print(""BiasAddGrad Output on GPU:"", result_gpu) max_abs_diff = tf.reduce_max(tf.abs(result_cpu  result_gpu)).numpy() is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e3,  atol=1e2) print(""Max absolute difference:"", max_abs_diff) print(""Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3:"", is_consistent.numpy()) ```  Relevant log output ```shell BiasAddGrad Output on CPU: tf.Tensor([0.09375 3.96875 1.70312], shape=(3,), dtype=bfloat16) BiasAddGrad Output on GPU: tf.Tensor([0.078125 4 1.70312], shape=(3,), dtype=bfloat16) Max absolute difference: 0.03125 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: False ```",2025-02-01T10:43:06Z,stat:awaiting tensorflower type:bug comp:ops TF 2.18,open,0,5,https://github.com/tensorflow/tensorflow/issues/86378,", I think the issue is specific to GPU, I was able to reproduce the issue on colab with GPU runtime. But, when the inputs are changed to float64 precision, the results are as expected. The reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors. The same behavior is not observed on Apple M1, using numpy or in CPU. https://github.com/tensorflow/tensorflow/issues/58479 Thank you!",", similar reasoning as in this comment  CC(inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU)/comment, but the acceptable tolerance thresholds have been set to atol=1e2 and rtol=1e3 considering that the dtypes of results are bfloat16 (16‑bit floating‐point types). Glad to hear your thoughts on this; thanks for your time.",", Looks like the other issues which were created are similar to CC(inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU) . Could you please close the other issues which were raised which helps to track the issue in a better way. Thank you!",", if you take for example this report and CC(inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU), I believe that the core issue is different since they are targeting different api's with different types of tensors","Reproduced here: ```bash TensorFlow Version: 2.16.2 Python Version: 3.10.16  (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] Python Implementation: CPython CUDA Version (TensorFlow): 12.0 cuDNN Version (TensorFlow): 8 GPU Name (PyTorch): NVIDIA GeForce RTX 4070 SUPER CUDA Version (PyTorch): 12.0 cuDNN Version (PyTorch): 8907 Driver Version: 550.144.03 Ubuntu Version: PRETTY_NAME=""Ubuntu 24.04.1 LTS"" Model name:                           AMD Ryzen 5 7600 6Core Processor BiasAddGrad Output on CPU: tf.Tensor([0.0625 3.96875 1.70312], shape=(3,), dtype=bfloat16) BiasAddGrad Output on GPU: tf.Tensor([0.078125 4 1.70312], shape=(3,), dtype=bfloat16) Max absolute difference: 0.03125 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: False ```"
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T10:27:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86377
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T10:08:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86376
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:43:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86375
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:36:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86374
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:31:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86373
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:20:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86372
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:19:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86371
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:18:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86370
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:16:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86369
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:16:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86368
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:16:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86367
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:16:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86366
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:15:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86365
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:14:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86364
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:14:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86363
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:13:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86362
copybara-service[bot],compat: Update forward compatibility horizon to 2025-02-01,compat: Update forward compatibility horizon to 20250201,2025-02-01T09:13:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86361
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:13:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86360
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:12:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86359
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:12:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86358
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:12:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86357
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:07:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86356
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T09:04:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86355
dependabot[bot],Bump the github-actions group with 8 updates,"Bumps the githubactions group with 8 updates:  Updates `google/osvscanneraction` from 1.9.0 to 1.9.2  Release notes Sourced from google/osvscanneraction's releases.  v1.9.2 What's Changed  Update to v1.9.2 by @​hogo6002 in google/osvscanneraction CC(Can't install on ubuntu 12.04.5 LTS)  Full Changelog: https://github.com/google/osvscanneraction/compare/v1.9.1...v1.9.2 v1.9.1 What's Changed  Update to use osvscanner v1.9.1 chore(deps): update workflows by @​renovatebot in google/osvscanneraction CC(Integration with blaze ecosystem numba python to llvm compiler?) Update to v1.9.1 by @​anotherrex in google/osvscanneraction CC(error __init__() got an unexpected keyword argument 'syntax') chore(deps): update workflows by @​renovatebot in google/osvscanneraction CC(Object Detection)  Full Changelog: https://github.com/google/osvscanneraction/compare/v1.9.0...v1.9.1    Commits  764c918 Merge pull request  CC(Can't install on ubuntu 12.04.5 LTS) from google/updatetov1.9.2 af3118a Update unified workflow example to point to v1.9.2 reusable workflows e994fd8 Update reusable workflows to point to v1.9.2 actions f8115f2 Update actions to use v1.9.2 osvscanner image daa2c68 Merge pull request  CC(Object Detection) from renovatebot/renovate/workflows af00d40 chore(deps): update workflows c411404 Merge pull request  CC(error __init__() got an unexpected keyword argument 'syntax') from google/updatetov1.9.1 1ab2a61 Update unified workflow example to point to v1.9.1 reusable workflows 8bd1ce1 Update reusable workflows to point to v1.9.1 actions cbb0295 Update actions to use v1.9.1 osvscanner image Additional commits viewable in compare view    Updates `actions/setuppython` from 5.3.0 to 5.4.0  Release notes Sourced from actions/setuppython's releases.  v5.4.0 What's Changed Enhancements:  Update cache error message by @​aparnajyothiy in actions/setuppython CC(tensorflow.bzl.bzl doesn't exist, unsurprisingly) Enhance Workflows: Add Ubuntu24, Remove Python 3.8  by @​priyakinthali in actions/setuppython CC(Wrong file dimension when calling maybe_download function) Configure Dependabot settings by @​HarithaVattikuti in actions/setuppython CC(Minor comment spelling fix.)  Documentation changes:  Readme update  recommended permissions by @​benwells in actions/setuppython CC(Example code for SummaryIterator using `tf.train.summary_iterator`.) Improve Advanced Usage examples by @​lrq3000 in actions/setuppython CC(Casting int64 to int8 is not supported)  Dependency updates:  Upgrade undici from 5.28.4 to 5.28.5 by @​dependabot in actions/setuppython CC(pip installation failing on fedora) Upgrade urllib3 from 1.25.9 to 1.26.19 in /tests/data by @​dependabot in actions/setuppython CC(Udacity example 1: import urllib for download) Upgrade actions/publishimmutableaction from 0.0.3 to 0.0.4 by @​dependabot in actions/setuppython CC(Fix merge gone wrong) Upgrade /httpclient from 2.2.1 to 2.2.3 by @​dependabot in actions/setuppython CC(Fixed tensorboard installation for pip) Upgrade requests from 2.24.0 to 2.32.2 in /tests/data by @​dependabot in actions/setuppython CC(Tensorflow Android not building for nonarm architectures.) Upgrade /cache to ^4.0.0 by @​priyagupta108 in actions/setuppython CC(TensorFlow: fix word2vec timeouts on GPU by pinning model on CPU)  New Contributors  @​benwells made their first contribution in actions/setuppython CC(Example code for SummaryIterator using `tf.train.summary_iterator`.) @​HarithaVattikuti made their first contribution in actions/setuppython CC(Minor comment spelling fix.) @​lrq3000 made their first contribution in actions/setuppython CC(Casting int64 to int8 is not supported)  Full Changelog: https://github.com/actions/setuppython/compare/v5...v5.4.0    Commits  4237552 Improve Advanced Usage examples ( CC(Casting int64 to int8 is not supported)) 709bfa5 Bump requests from 2.24.0 to 2.32.2 in /tests/data ( CC(Tensorflow Android not building for nonarm architectures.)) ceb20b2 Bump @​actions/httpclient from 2.2.1 to 2.2.3 ( CC(Fixed tensorboard installation for pip)) 0dc2d2c Bump actions/publishimmutableaction from 0.0.3 to 0.0.4 ( CC(Fix merge gone wrong)) feb9c6e Bump urllib3 from 1.25.9 to 1.26.19 in /tests/data ( CC(Udacity example 1: import urllib for download)) d0b4fc4 Bump undici from 5.28.4 to 5.28.5 ( CC(pip installation failing on fedora)) e3dfaac Configure Dependabot settings ( CC(Minor comment spelling fix.)) b8cf3eb Use the new cache service: upgrade /cache to ^4.0.0 ( CC(TensorFlow: fix word2vec timeouts on GPU by pinning model on CPU)) 1928ae6 Update README.md ( CC(Example code for SummaryIterator using `tf.train.summary_iterator`.)) 3fddbee Enhance Workflows: Add Ubuntu24, Remove Python 3.8  ( CC(Wrong file dimension when calling maybe_download function)) Additional commits viewable in compare view    Updates `peterevans/createpullrequest` from 7.0.5 to 7.0.6  Release notes Sourced from peterevans/createpullrequest's releases.  Create Pull Request v7.0.6 ⚙️ Fixes an issue with commit signing where unicode characters in file paths were not preserved. What's Changed  build(depsdev): bump @​vercel/ncc from 0.38.1 to 0.38.2 by @​dependabot in peterevans/createpullrequest CC(Remove read_analogies() from word2vec class initialization) Update distribution by @​actionsbot in peterevans/createpullrequest CC(Tests for examples/learn) build(deps): bump @​octokit/pluginrestendpointmethods from 13.2.4 to 13.2.5 by @​dependabot in peterevans/createpullrequest CC(wrong use of sequence_loss_by_example in ptb_word_lm.py?) build(depsdev): bump @​types/node from 18.19.50 to 18.19.54 by @​dependabot in peterevans/createpullrequest CC(Elementwise tf.cond (like theano switch)) build(deps): bump @​octokit/pluginpaginaterest from 11.3.3 to 11.3.5 by @​dependabot in peterevans/createpullrequest CC(Moving data from CPU to GPU is slow ) Update distribution by @​actionsbot in peterevans/createpullrequest CC(Bug: Exception ignored in BaseSession.__del__) build(depsdev): bump @​types/node from 18.19.54 to 18.19.55 by @​dependabot in peterevans/createpullrequest CC(new stacks) build(deps): bump @​actions/core from 1.10.1 to 1.11.1 by @​dependabot in peterevans/createpullrequest CC(Run a TensorFlow demo model : IOError: CRC check failed) build(deps): bump @​octokit/pluginrestendpointmethods from 13.2.5 to 13.2.6 by @​dependabot in peterevans/createpullrequest CC(A link in tensorflow's website is ineffective) build(depsdev): bump eslintpluginimport from 2.30.0 to 2.31.0 by @​dependabot in peterevans/createpullrequest CC(Given one input tensor, why tf.nn.max_pool generate two tensors?) build(deps): bump @​octokit/pluginthrottling from 9.3.1 to 9.3.2 by @​dependabot in peterevans/createpullrequest CC(Can't create placeholder with partially defined shape e.g. (1, 10)) Update distribution by @​actionsbot in peterevans/createpullrequest CC(Fix few warnings that I see on macosxclang compilation) build(depsdev): bump typescript from 5.6.2 to 5.6.3 by @​dependabot in peterevans/createpullrequest CC(Documentation for 'Adding a new op') build(deps): bump undici from 6.19.8 to 6.20.1 by @​dependabot in peterevans/createpullrequest CC(Error malloc(): memory corruption) Update distribution by @​actionsbot in peterevans/createpullrequest CC(retrain.py validation and testing evaluation seems incorrect) build(depsdev): bump @​types/node from 18.19.55 to 18.19.58 by @​dependabot in peterevans/createpullrequest CC(Problem in distributed tensorflow demo?) build(depsdev): bump @​types/jest from 29.5.13 to 29.5.14 by @​dependabot in peterevans/createpullrequest CC(Correction in Dequantize comments) build(depsdev): bump @​types/node from 18.19.58 to 18.19.60 by @​dependabot in peterevans/createpullrequest CC(Dev request  LSTM RNN) chore: don't bundle undici by @​benmccann in peterevans/createpullrequest CC(Device placement error while using multi gpus on single machine by distributed version) Update distribution by @​actionsbot in peterevans/createpullrequest CC(How to restore a distributed model and continue to train it with more workers?) chore: use nodefetchnative support for proxy env vars by @​peterevans in peterevans/createpullrequest CC(Slicing error: Using a `tf.Tensor` as a Python `bool` is not allowed) build(depsdev): bump @​types/node from 18.19.60 to 18.19.64 by @​dependabot in peterevans/createpullrequest CC(minor typo fix) build(depsdev): bump undici from 6.20.1 to 6.21.0 by @​dependabot in peterevans/createpullrequest CC(Placeholder names are inconsistent when reentering a scope) build(depsdev): bump @​vercel/ncc from 0.38.2 to 0.38.3 by @​dependabot in peterevans/createpullrequest CC(Replaced _logger.warn with _logger.warning) docs: note pushtorepo classic PAT workflow scope requirement by @​scop in peterevans/createpullrequest CC(Branch 128485842) docs: spelling fixes by @​scop in peterevans/createpullrequest CC(Branch 128492931) build(depsdev): bump typescript from 5.6.3 to 5.7.2 by @​dependabot in peterevans/createpullrequest CC(Fix an error message.) build(depsdev): bump prettier from 3.3.3 to 3.4.0 by @​dependabot in peterevans/createpullrequest CC(Error when running distributed MNIST example) build(depsdev): bump @​types/node from 18.19.64 to 18.19.66 by @​dependabot in peterevans/createpullrequest CC(Update version string to 0.10.0rc0) docs(README): clarify that an existing open PR is managed by @​caugner in peterevans/createpullrequest CC('tensorflow.contrib.learn' has no attribute 'infer_real_valued_columns_from_input') Update distribution by @​actionsbot in peterevans/createpullrequest CC(Fixed typo:) build(deps): bump @​octokit/pluginpaginaterest from 11.3.5 to 11.3.6 by @​dependabot in peterevans/createpullrequest CC(Fix go build errors) build(depsdev): bump @​types/node from 18.19.66 to 18.19.67 by @​dependabot in peterevans/createpullrequest CC(boolean_mask failed in iOS: Running model failed:Invalid argument: No OpKernel was registered to support Op 'Gather' with these attrs) build(depsdev): bump prettier from 3.4.0 to 3.4.1 by @​dependabot in peterevans/createpullrequest CC(Should *.pb.h files be generated in crosscompile cases?) build(depsdev): bump eslintimportresolvertypescript from 3.6.3 to 3.7.0 by @​dependabot in peterevans/createpullrequest CC(Can I add a py_func to a queue?) build(depsdev): bump prettier from 3.4.1 to 3.4.2 by @​dependabot in peterevans/createpullrequest CC(Inception retraining / transfer learning fails when running with GPU) build(depsdev): bump @​types/node from 18.19.67 to 18.19.68 by @​dependabot in peterevans/createpullrequest CC(word2vec_basic.py : to prevent possible issue with libpng) build(deps): bump plimit from 6.1.0 to 6.2.0 by @​dependabot in peterevans/createpullrequest CC(Branch 128859117) Update distribution by @​actionsbot in peterevans/createpullrequest CC(Dying Threads?) fix: preserve unicode in filepaths when commit signing by @​peterevans in peterevans/createpullrequest CC(Branch 128894163)  New Contributors  @​benmccann made their first contribution in peterevans/createpullrequest CC(Device placement error while using multi gpus on single machine by distributed version) @​scop made their first contribution in peterevans/createpullrequest CC(Branch 128485842) @​caugner made their first contribution in peterevans/createpullrequest CC('tensorflow.contrib.learn' has no attribute 'infer_real_valued_columns_from_input')    ... (truncated)   Commits  67ccf78 fix: preserve unicode in filepaths when commit signing ( CC(Branch 128894163)) bb88e27 build: update distribution ( CC(Dying Threads?)) b378ed5 build(deps): bump plimit from 6.1.0 to 6.2.0 ( CC(Branch 128859117)) fa9200e build(depsdev): bump @​types/node from 18.19.67 to 18.19.68 ( CC(word2vec_basic.py : to prevent possible issue with libpng)) 16e0059 build(depsdev): bump prettier from 3.4.1 to 3.4.2 ( CC(Inception retraining / transfer learning fails when running with GPU)) 5bffd5a build(depsdev): bump eslintimportresolvertypescript ( CC(Can I add a py_func to a queue?)) a22a0dd build(depsdev): bump prettier from 3.4.0 to 3.4.1 ( CC(Should *.pb.h files be generated in crosscompile cases?)) b27ce37 build(depsdev): bump @​types/node from 18.19.66 to 18.19.67 ( CC(boolean_mask failed in iOS: Running model failed:Invalid argument: No OpKernel was registered to support Op 'Gather' with these attrs)) 4e0cc19 build(deps): bump @​octokit/pluginpaginaterest from 11.3.5 to 11.3.6 ( CC(Fix go build errors)) 25b6871 docs: update scopes for pushtofork Additional commits viewable in compare view    Updates `actions/uploadartifact` from 4.4.3 to 4.6.0  Release notes Sourced from actions/uploadartifact's releases.  v4.6.0 What's Changed  Expose env vars to control concurrency and timeout by @​yacaovsnc in actions/uploadartifact CC(optimize slice_input_producer)  Full Changelog: https://github.com/actions/uploadartifact/compare/v4...v4.6.0 v4.5.0 What's Changed  fix: deprecated Node.js version in action by @​hamirmahal in actions/uploadartifact CC(Fixed random_contrast link in the Deep CNN tutorial) Add new artifactdigest output by @​bdehamer in actions/uploadartifact CC(Tiny fix to API docs)  New Contributors  @​hamirmahal made their first contribution in actions/uploadartifact CC(Fixed random_contrast link in the Deep CNN tutorial) @​bdehamer made their first contribution in actions/uploadartifact CC(Tiny fix to API docs)  Full Changelog: https://github.com/actions/uploadartifact/compare/v4.4.3...v4.5.0    Commits  65c4c4a Merge pull request  CC(optimize slice_input_producer) from actions/yacaovsnc/add_variable_for_concurrency_a... 0207619 move files back to satisfy licensed ci 1ecca81 licensed cache updates 9742269 Expose env vars to controll concurrency and timeout 6f51ac0 Merge pull request  CC(Tiny fix to API docs) from bdehamer/bdehamer/artifactdigest c40c16d add new artifactdigest output 735efb4 bump @​actions/artifact from 2.1.11 to 2.2.0 184d73b Merge pull request  CC(Fixed random_contrast link in the Deep CNN tutorial) from hamirmahal/fix/deprecatednodejsusageinaction b4a0a98 Merge branch 'main' into fix/deprecatednodejsusageinaction See full diff in compare view    Updates `github/codeqlaction` from 3.27.5 to 3.28.8  Release notes Sourced from github/codeqlaction's releases.  v3.28.8 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.8  29 Jan 2025  Enable support for Kotlin 2.1.10 when running with CodeQL CLI v2.20.3.  CC(Fix for build issue 2742;)  See the full CHANGELOG.md for more information. v3.28.7 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.7  29 Jan 2025 No user facing changes. See the full CHANGELOG.md for more information. v3.28.6 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.6  27 Jan 2025  Reenable debug artifact upload for CLI versions 2.20.3 or greater.  CC(Modifying MNIST example to distributed version: could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR)  See the full CHANGELOG.md for more information. v3.28.5 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.5  24 Jan 2025  Update default CodeQL bundle version to 2.20.3.  CC(Branch 124290852)  See the full CHANGELOG.md for more information. v3.28.4 CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. 3.28.4  23 Jan 2025   ... (truncated)   Changelog Sourced from github/codeqlaction's changelog.  CodeQL Action Changelog See the releases page for the relevant changes to the CodeQL CLI and language packs. [UNRELEASED] No user facing changes. 3.28.8  29 Jan 2025  Enable support for Kotlin 2.1.10 when running with CodeQL CLI v2.20.3.  CC(Fix for build issue 2742;)  3.28.7  29 Jan 2025 No user facing changes. 3.28.6  27 Jan 2025  Reenable debug artifact upload for CLI versions 2.20.3 or greater.  CC(Modifying MNIST example to distributed version: could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR)  3.28.5  24 Jan 2025  Update default CodeQL bundle version to 2.20.3.  CC(Branch 124290852)  3.28.4  23 Jan 2025 No user facing changes. 3.28.3  22 Jan 2025  Update default CodeQL bundle version to 2.20.2.  CC(Update roadmap.md) Fix an issue downloading the CodeQL Bundle from a GitHub Enterprise Server instance which occurred when the CodeQL Bundle had been synced to the instance using the CodeQL Action sync tool and the Actions runner did not have Zstandard installed.  CC(Branch 124251558) Uploading debug artifacts for CodeQL analysis is temporarily disabled.  CC(Tensorflow with Pyinstaller)  3.28.2  21 Jan 2025 No user facing changes. 3.28.1  10 Jan 2025  CodeQL Action v2 is now deprecated, and is no longer updated or supported. For better performance, improved security, and new features, upgrade to v3. For more information, see this changelog post.  CC(Import error) Update default CodeQL bundle version to 2.20.1.  CC(Fix broken link to Anaconda installation)  3.28.0  20 Dec 2024  Bump the minimum CodeQL bundle version to 2.15.5.  CC(Feature Request: Support for YARN cluster manager for Distributed TensorFlow) Don't fail in the unusual case that a file is on the search path.  CC(Max Pooling NCHW).  3.27.9  12 Dec 2024   ... (truncated)   Commits  dd74661 Merge pull request  CC(Missing pywrap_tensorflow) from github/updatev3.28.8a91a3f767 3210a3c Fix Kotlin version in changelog 72f9d02 Update changelog for v3.28.8 a91a3f7 Merge pull request  CC(Fix for build issue 2742;) from github/igfoo/kot2.1.10 c520fb5 Merge pull request  CC(running digits.py) from github/mergeback/v3.28.7tomain6e545590 3879c57 Add changelog entry 0c21937 Run &quot;npm run build&quot; 5a61bf0 Kotlin: The 2.20.3 release supports Kotlin 2.1.10. 163d119 Update checkedin dependencies bcf5cec Update changelog and version after v3.28.7 Additional commits viewable in compare view    Updates `docker/setupbuildxaction` from 3.7.1 to 3.8.0  Release notes Sourced from docker/setupbuildxaction's releases.  v3.8.0  Make cloud prefix optional to download buildx if driver is cloud by @​crazymax in docker/setupbuildxaction CC(parse_example can be _much_ faster than parse_single_example) Bump @​actions/core from 1.10.1 to 1.11.1 in docker/setupbuildxaction CC(convert_to_records.py don't write all values into .tfrecords file) Bump @​docker/actionstoolkit from 0.39.0 to 0.48.0 in docker/setupbuildxaction CC(Cifar10 eval script verbose output) Bump crossspawn from 7.0.3 to 7.0.6 in docker/setupbuildxaction CC(Failed to bazel build when executing label_image example )  Full Changelog: https://github.com/docker/setupbuildxaction/compare/v3.7.1...v3.8.0    Commits  6524bf6 Merge pull request  CC(parse_example can be _much_ faster than parse_single_example) from crazymax/buildxcloudlatest 8d5e074 chore: update generated content 7199e57 make cloud prefix optional to download buildx if driver is cloud db63cee Merge pull request  CC(Failed to bazel build when executing label_image example ) from docker/dependabot/github_actions/codecov/codecov... 043ebe1 Merge pull request  CC(Cifar10 eval script verbose output) from docker/dependabot/npm_and_yarn/docker/actionsto... 686da90 chore: update generated content a3d7487 Merge pull request  CC(Failed to bazel build when executing label_image example ) from docker/dependabot/npm_and_yarn/crossspawn7.0.6 4dcdbce build(deps): bump @​docker/actionstoolkit from 0.39.0 to 0.48.0 1a8ac74 ci: fix deprecated input for codecovaction e827ebe build(deps): bump crossspawn from 7.0.3 to 7.0.6 Additional commits viewable in compare view    Updates `docker/buildpushaction` from 6.10.0 to 6.13.0  Release notes Sourced from docker/buildpushaction's releases.  v6.13.0  Bump @​docker/actionstoolkit from 0.51.0 to 0.53.0 in docker/buildpushaction CC(ImportError: cannot import name server)  Full Changelog: https://github.com/docker/buildpushaction/compare/v6.12.0...v6.13.0 v6.12.0  Bump @​docker/actionstoolkit from 0.49.0 to 0.51.0 in docker/buildpushaction CC(Support for halffloats (float16/fp16))  Full Changelog: https://github.com/docker/buildpushaction/compare/v6.11.0...v6.12.0 v6.11.0  Handlebar defaultContext support for buildcontexts input by @​crazymax in docker/buildpushaction CC(DEFINE_bool alias not working on Mac Python testoninstall) Bump @​docker/actionstoolkit from 0.46.0 to 0.49.0 in docker/buildpushaction CC(ImportError: cannot import name tensorboard_server )  Full Changelog: https://github.com/docker/buildpushaction/compare/v6.10.0...v6.11.0    Commits  ca877d9 Merge pull request  CC(ImportError: cannot import name server) from docker/dependabot/npm_and_yarn/docker/actionst... d2fe919 chore: update generated content f0fc9ec chore(deps): Bump @​docker/actionstoolkit from 0.51.0 to 0.53.0 67a2d40 Merge pull request  CC(Support for halffloats (float16/fp16)) from docker/dependabot/npm_and_yarn/docker/actionst... 0b1b1c9 chore: update generated content b6a7c2c chore(deps): Bump @​docker/actionstoolkit from 0.49.0 to 0.51.0 31ca4e5 Merge pull request  CC(did grammatical clean up) from crazymax/bakev6 e613db9 update bakeaction to v6 b32b51a Merge pull request  CC(ImportError: cannot import name tensorboard_server ) from docker/dependabot/npm_and_yarn/docker/actionst... 594bf46 Merge pull request  CC(Automated Docker image build and test) from crazymax/fixe2e Additional commits viewable in compare view    Updates `actions/stale` from 9.0.0 to 9.1.0  Release notes Sourced from actions/stale's releases.  v9.1.0 What's Changed  Documentation update by @​Marukome0743 in actions/stale CC(keep numpy version in pip.sh) Add workflow file for publishing releases to immutable action package by @​Jcambass in actions/stale CC(libcuda suffix issue) Update undici from 5.28.2 to 5.28.4 by @​dependabot in actions/stale CC(tensorflow 0.7.0 gpuenabled version crashes bad on import) Update actions/checkout from 3 to 4 by @​dependabot in actions/stale CC(Adding summaries changes random number generation) Update actions/publishaction from 0.2.2 to 0.3.0 by @​dependabot in actions/stale CC(Python 3 test failure: //tensorflow/tensorboard/backend:server_test) Update tsjest from 29.1.1 to 29.2.5 by @​dependabot in actions/stale CC(fix broken links in docs) Update @​actions/core from 1.10.1 to 1.11.1 by @​dependabot in actions/stale CC(Build of pip package with current HEAD of bazel fails) Update @​types/jest from 29.5.11 to 29.5.14 by @​dependabot in actions/stale CC('utf8' codec can't decode byte (in tutorial)) Update @​actions/cache from 3.2.2 to 4.0.0 by @​dependabot in actions/stale CC(Fixed spelling)  New Contributors  @​Marukome0743 made their first contribution in actions/stale CC(keep numpy version in pip.sh) @​Jcambass made their first contribution in actions/stale CC(libcuda suffix issue)  Full Changelog: https://github.com/actions/stale/compare/v9...v9.1.0    Commits  5bef64f build(deps): bump @​actions/cache from 3.2.2 to 4.0.0 ( CC(Fixed spelling)) fa77dfd build(depsdev): bump @​types/jest from 29.5.11 to 29.5.14 ( CC('utf8' codec can't decode byte (in tutorial))) f04443d build(deps): bump @​actions/core from 1.10.1 to 1.11.1 ( CC(Build of pip package with current HEAD of bazel fails)) 5c715b0 build(depsdev): bump tsjest from 29.1.1 to 29.2.5 ( CC(fix broken links in docs)) f691222 build(deps): bump actions/publishaction from 0.2.2 to 0.3.0 ( CC(Python 3 test failure: //tensorflow/tensorboard/backend:server_test)) df990c2 build(deps): bump actions/checkout from 3 to 4 ( CC(Adding summaries changes random number generation)) 6e472ce Merge pull request  CC(libcuda suffix issue) from actions/Jcambasspatch1 d10ba64 Merge pull request  CC(tensorflow 0.7.0 gpuenabled version crashes bad on import) from actions/dependabot/npm_and_yarn/undici5.28.4 bbf3da5 resolve check failures 6a2e61d Add workflow file for publishing releases to immutable action package Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore  major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)  ` ignore  minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)  ` ignore ` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)  ` unignore ` will remove all of the ignore conditions of the specified dependency  ` unignore  ` will remove the ignore condition of the specified dependency and ignore conditions ",2025-02-01T08:42:19Z,ready to pull size:S dependencies github_actions,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86354
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T08:37:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86353
copybara-service[bot],Remove the is_last_transfer parameter,Remove the is_last_transfer parameter,2025-02-01T07:28:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86352
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T07:18:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86351
usmonali4,inconsistent result of ```tf.raw_ops.BatchMatMulV2``` on CPU and GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5/9  GPU model and memory Tesla T4  Current behavior? getting inconsistent results of ```tf.raw_ops.BatchMatMulV2``` between CPU and GPU  Standalone code to reproduce the issue ```shell import tensorflow as tf x_0 = tf.constant([     [[[[ 1.3594, 0.3027], [1.4141,  0.2969]],       [[ 0.9141,  1.7812], [ 1.2266,  0.8594]]],      [[[  0.8359, 0.9414], [1.7969, 0.7461]],       [[  0.3164,  0.3691], [ 0.7656,  0.2354]]]],     [[[[ 0.5898,  1.3516], [ 0.4902, 0.1045]],       [[ 0.1099,  1.5078], [ 0.2852, 0.0957]]],      [[[0.9883,  1.3203], [0.2715, 1.7578]],       [[ 0.1602, 0.4336], [0.6875, 0.4492]]]] ], dtype=tf.bfloat16) y = tf.constant([     [[[  0.6836, 0.6562], [0.5508, 0.8438]],       [[  1.6094, 0.9883], [0.1318,  1.1094]]],     [[[  0.4062, 1.1094], [0.7188, 1.7578]],       [[ 1.0391, 0.6602], [ 0.8359, 0.6562]]] ], dtype=tf.bfloat16)  with tf.device('CPU:0'):     result_cpu = tf.raw_ops.BatchMatMulV2(         x=x_0,          y=y,     )     print(result_cpu) with tf.device('GPU:0'):     result_gpu = tf.raw_ops.BatchMatMulV2(         x=x_0,          y=y,     )     print(result_gpu) max_abs_diff = tf.reduce_max(tf.abs(result_cpu  result_gpu)).numpy() is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e3,  atol=1e2) print(""Max absolute difference:"", max_abs_diff) print(""Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3:"", is_consistent.numpy()) ```  Relevant log output ```shell tf.Tensor( [[[[[0.761719 1.14062]     [1.125 0.675781]]    [[1.70312 2.875]     [1.85938 0.257812]]]   [[[1.01562 0.726562]     [0.193359 3.29688]]    [[0.0201416 0.449219]     [0.597656 0.65625]]]]  [[[[1.14062 0.75]     [0.392578 0.233398]]    [[0.375 1.78125]     [0.470703 0.386719]]]   [[[1.34375 1.21875]     [1.14844 3.39062]]    [[0.195312 0.388672]     [0.337891 0.746094]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16) tf.Tensor( [[[[[0.761719 1.14844]     [1.13281 0.675781]]    [[1.70312 2.875]     [1.85938 0.259766]]]   [[[1.01562 0.726562]     [0.193359 3.3125]]    [[0.0201416 0.451172]     [0.597656 0.660156]]]]  [[[[1.14844 0.753906]     [0.392578 0.233398]]    [[0.375 1.78125]     [0.470703 0.388672]]]   [[[1.35156 1.22656]     [1.15625 3.39062]]    [[0.196289 0.390625]     [0.337891 0.75]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16) Max absolute difference: 0.015625 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: False ```",2025-02-01T07:13:36Z,type:bug comp:ops TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/86350,I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TFnightly on both CPU and GPU. Please find gist1 and gist2 here for your reference. Thank you!,"not reproduced here: ```bash TensorFlow Version: 2.16.2 Python Version: 3.10.16  (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] Python Implementation: CPython CUDA Version (TensorFlow): 12.0 cuDNN Version (TensorFlow): 8 GPU Name (PyTorch): NVIDIA GeForce RTX 4070 SUPER CUDA Version (PyTorch): 12.0 cuDNN Version (PyTorch): 8907 Driver Version: 550.144.03 Ubuntu Version: PRETTY_NAME=""Ubuntu 24.04.1 LTS"" Model name:                           AMD Ryzen 5 7600 6Core Processor tf.Tensor( [[[[[0.761719 1.14844]     [1.13281 0.675781]]    [[1.70312 2.875]     [1.85938 0.259766]]]   [[[1.01562 0.726562]     [0.193359 3.3125]]    [[0.0201416 0.451172]     [0.597656 0.660156]]]]  [[[[1.14844 0.753906]     [0.392578 0.233398]]    [[0.375 1.78125]     [0.470703 0.388672]]]   [[[1.35156 1.22656]     [1.15625 3.39062]]    [[0.196289 0.390625]     [0.337891 0.75]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16) tf.Tensor( [[[[[0.761719 1.14844]     [1.13281 0.675781]]    [[1.70312 2.875]     [1.85938 0.259766]]]   [[[1.01562 0.726562]     [0.193359 3.3125]]    [[0.0201416 0.451172]     [0.597656 0.660156]]]]  [[[[1.14844 0.753906]     [0.392578 0.233398]]    [[0.375 1.78125]     [0.470703 0.388672]]]   [[[1.35156 1.22656]     [1.15625 3.39062]]    [[0.196289 0.390625]     [0.337891 0.75]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16) Max absolute difference: 0 Consistency check (CPU vs GPU) with atol=1e2 and rtol=1e3: True ```"
harshaljanjani,fix(kernels): Handle empty values with non-empty row splits in RaggedTensorToTensor,This commit addresses a segmentation fault in the `RaggedTensorToTensor` op when processing empty values with nonempty row splits:  Checking for empty values before processing.  Ensuring consistent handling of dimension sizes.  Providing clear error messages for invalid input configurations. Might fix CC(Segmentation fault (core dumped) in `RaggedTensorToTensor`).,2025-02-01T06:51:34Z,ready to pull size:M comp:core,open,0,2,https://github.com/tensorflow/tensorflow/issues/86349,"> Please make sure to not include irrelevant spacing changes. Understood, it's my first time contributing here; thanks for the information! Will take care of the linting next time around.","> Can you please make sure to run all tests and make sure they pass? Hello , thanks for the reply. Actually, I'm not quite able to figure out why my local setup's failing with these `bash r not found` errors. Besides, I tried converting the CRLF endings to LF endings (given that I'm running the bazel tests in WSL). I wished to ask if there's a better way to set up and run tests locally that you'd recommend with WSL, as I've read the CONTRIBUTING.md file and set it up to the tee but am still facing these issues that impede my progress; thanks!"
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T05:16:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86348
copybara-service[bot],Automated Code Change,Automated Code Change,2025-02-01T05:08:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86347
copybara-service[bot],Internal only change,Internal only change,2025-02-01T02:16:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86346
copybara-service[bot],"[xla:tsl] concurrency: tag xla_cpu_runtime_{srcs,hdrs} as non-prod-compatible","[xla:tsl] concurrency: tag xla_cpu_runtime_{srcs,hdrs} as nonprodcompatible",2025-02-01T01:33:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86345
copybara-service[bot],[xla:cpu] xla.proto: introduce xla_cpu_use_fusion_emitters flag,[xla:cpu] xla.proto: introduce xla_cpu_use_fusion_emitters flag The implementation is coming up next.,2025-02-01T01:32:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86344
copybara-service[bot],[xla:cpu] kernel_api_ir_builder: expose SetKernelFunctionAttributes,[xla:cpu] kernel_api_ir_builder: expose SetKernelFunctionAttributes So that fusion emitters will be able to set these same attributes. The fusion emitters are landing soon.,2025-02-01T01:27:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86343
copybara-service[bot],[xla:cpu] kernel_api_ir_builder: expose helpers to get KernelParams,[xla:cpu] kernel_api_ir_builder: expose helpers to get KernelParams This paves the way for upcoming work.,2025-02-01T01:24:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86342
copybara-service[bot],"[xla:emitters] tag XLA, XLA:CPU and XLA:GPU dialects as non-prod-compatible","[xla:emitters] tag XLA, XLA:CPU and XLA:GPU dialects as nonprodcompatible This paves the way for XLA:CPU fusion emitters. Note that XLA:CPU is nonprodcompatible, whereas XLA:GPU is not. The CPU fusion emitters will depend on the XLA, XLA:CPU and XLA:GPU dialects, and given that the emitters' dependents in XLA:CPU are nonprodcompatible, the three dialects have to be as well. XLA:CPU passes also have to be tagged. Crucially, thanks to the parent CLs, XLA:GPU passes are not used anymore by any of the above dialects nor by XLA:CPU passes, so XLA:GPU remains essentially untouched; we just tag the XLA:GPU dialect. Some common libraries in xla/codegen/emitters are also tagged.",2025-02-01T01:20:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86341
copybara-service[bot],[xla:emitters] move LowerXlaToScfPass and LowerXlaLoopsToScfPass to xla/codegen/emitters,"[xla:emitters] move LowerXlaToScfPass and LowerXlaLoopsToScfPass to xla/codegen/emitters Will be shared with the CPU pipeline. Note that some GPU ops are also converted, but that's OK because what we don't want is to depend on GPU passes from common bits; depending on GPU ops is OK. Reverts 6b431d3ca515c8afc08b1bdfaf4253adf4234b19",2025-02-01T01:16:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86340
copybara-service[bot],[xla:emitters] move MergePointersToSameSlicePass to xla/codegen/emitters,[xla:emitters] move MergePointersToSameSlicePass to xla/codegen/emitters Will be shared with the CPU pipeline.,2025-02-01T01:15:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86339
copybara-service[bot],Delete ARM64 Kokoro build now that Github Actions based build is blocking,Delete ARM64 Kokoro build now that Github Actions based build is blocking,2025-02-01T00:53:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86338
copybara-service[bot],Integrate LLVM at llvm/llvm-project@6deee0d5b36c,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 6deee0d5b36c,2025-02-01T00:28:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86337
copybara-service[bot],Update sha values for JAX cuda images,Update sha values for JAX cuda images,2025-02-01T00:28:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86336
copybara-service[bot],Add Q/DQ annotation interfaces in ai-edge-jax,Add Q/DQ annotation interfaces in aiedgejax,2025-02-01T00:03:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86335
copybara-service[bot],"Return arrays from `ArrayImpl._check_and_rearrange`. Build IFRT shardings with both addressable and non-addressable devices, instead of only addressable devices.","Return arrays from `ArrayImpl._check_and_rearrange`. Build IFRT shardings with both addressable and nonaddressable devices, instead of only addressable devices. This is a rollforward of two previous rollbacks after fixing breakages.",2025-01-31T23:55:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86334
copybara-service[bot],Add Q/DQ annotation lowering support.,Add Q/DQ annotation lowering support. LowerQuantAnnotationsPass now supports quant.quantize and quant.dequantize composite lowering. These patterns make adjustments to the function signatures if necessary.,2025-01-31T23:45:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86333
copybara-service[bot],Update RaggedAllToAll API to clarify supported shapes for offsets/sizes operands.,Update RaggedAllToAll API to clarify supported shapes for offsets/sizes operands.,2025-01-31T23:38:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86332
copybara-service[bot],Reverts e05f2be59cd34b3f0b2291d91ba1353e90fb1c25,Reverts e05f2be59cd34b3f0b2291d91ba1353e90fb1c25,2025-01-31T23:23:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86331
copybara-service[bot],Add control dependencies to conflicting collectives when decomposing collective-permute into send/recv,"Add control dependencies to conflicting collectives when decomposing collectivepermute into send/recv This is to ensure that conflicting collectives are not scheduled in between the decomposed send/recv ops, which would cause deadlocks.",2025-01-31T23:07:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86330
copybara-service[bot],Annotate decomposed send/recv and conflicting collectives to run them in parallel,"Annotate decomposed send/recv and conflicting collectives to run them in parallel Find all collectives conflicting with the collective permutes that we want to decompose. Annotate them to run them in parallel with nonconflicting collectives, e.g. those used on inner sharding strategies. The annotation allows us to later execute them on a separate stream.",2025-01-31T23:07:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86329
copybara-service[bot],[xla:cpu] Delete unused timeslice parameter from parallel loop runner,[xla:cpu] Delete unused timeslice parameter from parallel loop runner,2025-01-31T22:37:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86328
copybara-service[bot],PR #22073: Add a README file in the gpu_specs directory.,PR CC(Fix ReLU layer serialization bug): Add a README file in the gpu_specs directory. Imported from GitHub PR https://github.com/openxla/xla/pull/22073 Copybara import of the project:  659afd527c8cbe9e67dca813cb182c13c41889ef by Dimitris Vardoulakis : Add a README file in the gpu_specs directory.  9a8bf3a49eff2882c8fbbea60644ca1c8ad45333 by Dimitris Vardoulakis : Improved the README. Merging this change closes CC(Fix ReLU layer serialization bug) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22073 from dimvar:addreadmetogpu_specs 9a8bf3a49eff2882c8fbbea60644ca1c8ad45333,2025-01-31T22:32:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86327
copybara-service[bot],Consider send/recv in IsNonFusionCollective,Consider send/recv in IsNonFusionCollective,2025-01-31T22:20:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86326
copybara-service[bot],[DNS] Add StableHLO simplification passes to PJRT,[DNS] Add StableHLO simplification passes to PJRT,2025-01-31T21:16:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86325
copybara-service[bot],Reverts 46f98fe577a1591a800d17cbb0c0b4000ff13f74,Reverts 46f98fe577a1591a800d17cbb0c0b4000ff13f74,2025-01-31T21:12:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86324
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22107 from dimvar:fixconfigurefileforblackwell a718233bcf95d047a11b1d18366133f08d3b1eee,2025-01-31T20:56:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86323
copybara-service[bot],Fix GPU memcpy a2a crash with multiple replica groups.,"Fix GPU memcpy a2a crash with multiple replica groups. AllToAll with the flag set_xla_gpu_use_memcpy_local_p2p=1 would previously crash or give incorrect results when there were multiple replica groups on GPUs. The issue is NcclAllToAllStartThunk had maps from the device ID to various buffers used for the alltoall. The device ID was previously modded by the size of the current replica_group, which meant devices in different replica groups could share device IDs, causing them to use share buffers, causing incorrect results or crashes. The device ID is no longer modded by the size of the replica_group.",2025-01-31T20:36:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86322
copybara-service[bot],"nb::cast cannot actually cast to span for some reason, so just always","nb::cast cannot actually cast to span for some reason, so just always leave as a nb::object.",2025-01-31T20:21:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86321
copybara-service[bot],Integrate LLVM at llvm/llvm-project@956c0707d909,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 956c0707d909,2025-01-31T19:44:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86320
copybara-service[bot],[HLO] Use llvm::StringRef when building MHLO string attributes instead of relying on implicit casting,[HLO] Use llvm::StringRef when building MHLO string attributes instead of relying on implicit casting,2025-01-31T19:44:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86319
copybara-service[bot],"#litert Remove `test_macros.h`, superseded by `litert_macros.h`","litert Remove `test_macros.h`, superseded by `litert_macros.h`",2025-01-31T19:22:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86318
copybara-service[bot],PR #22107: Change cuda_configure.bzl to handle blackwell ptx variants with accelerated features.,PR CC(Add float16 support for CTCLoss): Change cuda_configure.bzl to handle blackwell ptx variants with accelerated features. Imported from GitHub PR https://github.com/openxla/xla/pull/22107 Copybara import of the project:  a718233bcf95d047a11b1d18366133f08d3b1eee by Dimitris Vardoulakis : Change cuda_configure.bzl to handle blackwell ptx variants with accelerated features. Merging this change closes CC(Add float16 support for CTCLoss) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22107 from dimvar:fixconfigurefileforblackwell a718233bcf95d047a11b1d18366133f08d3b1eee,2025-01-31T19:20:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86317
copybara-service[bot],Reverts b77fc611f78c5511c3881060b5c695df9c72d3a8,Reverts b77fc611f78c5511c3881060b5c695df9c72d3a8,2025-01-31T18:36:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86316
copybara-service[bot],[xla:cpu] Do not put moved-from XnnRuntime back into the pool,[xla:cpu] Do not put movedfrom XnnRuntime back into the pool,2025-01-31T18:33:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86315
copybara-service[bot],In Progress.,In Progress. Adds a minimal but viable implementation of string arrays (with `numpy.dtypes.StringDType`) in JAX. Currently this only supports making of a string array by means of either `jax.numpy.asarray` or `jax.device_put` and reading it back with `jax.device_get`.,2025-01-31T18:17:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86314
copybara-service[bot],Reverts 0e749daf4bcf5423c48534611927dd28e985dccc,Reverts 0e749daf4bcf5423c48534611927dd28e985dccc,2025-01-31T18:14:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86313
copybara-service[bot],[XLA] Clean up and refactor HeapSimulator.,[XLA] Clean up and refactor HeapSimulator.,2025-01-31T18:10:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86312
copybara-service[bot],Display the flax_2b E2E benchmark results to show TTFT and E2E latency,Display the flax_2b E2E benchmark results to show TTFT and E2E latency,2025-01-31T18:08:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86311
ceschi,Stateful LSTM bug with batch size," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.18  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When running a stateful LSTM one needs to fix a batch size. Previously this was achieved with the `batch_input_shape=(batch_size, sequence_length, num_features)` argument, but with the most recent version of TF batch_input_shape is not recognised as valid.  Basically the error is the same as this https://github.com/tensorflow/tensorflow/issues/64061 Below is some LLM generated code to reproduce the behaviour. Also, the `model.reset_states()` bit appears broken.  Standalone code to reproduce the issue ```shell import numpy as np import tensorflow as tf  Set a fixed batch size batch_size = 32  Create some random training data  We'll have sequences of length 5, with 1 feature per time step sequence_length = 5 num_features = 1 num_samples = 100   Total number of samples (must be divisible by batch_size)  Ensure num_samples is a multiple of batch_size num_samples = (num_samples // batch_size) * batch_size X_train = np.random.rand(num_samples, sequence_length, num_features) y_train = np.random.rand(num_samples, 1)   Example target values  Reshape y_train to match expected output shape if needed y_train = y_train.reshape(1,1)  Create the stateful LSTM model model = tf.keras.models.Sequential() model.add(tf.keras.layers.LSTM(units=64,   Number of LSTM units                                batch_input_shape=(batch_size, sequence_length, num_features),                                stateful=True,                                return_sequences=False)) often false for a final prediction model.add(tf.keras.layers.Dense(units=1))  Output layer with 1 unit  Compile the model model.compile(optimizer='adam', loss='mse')  Train the model epochs = 10 for epoch in range(epochs):      Shuffle data indices for each epoch (important for stateful LSTMs)     indices = np.arange(num_samples)     np.random.shuffle(indices)     X_train = X_train[indices]     y_train = y_train[indices]     model.fit(X_train, y_train, batch_size=batch_size, epochs=1, shuffle=False)  Shuffle must be false      Reset states after each epoch (essential for stateful LSTMs)     model.reset_states() ```  Relevant log output ```shell ```",2025-01-31T18:07:07Z,stat:awaiting response type:bug stale comp:keras TF 2.18,closed,0,7,https://github.com/tensorflow/tensorflow/issues/86310,"Hello Jordan, thanks for the reply. I am indeed using TF 2.18 (and Python 3.11.0 on Win11), though if I run your code I get precisely the error I referred to in the first place: `ValueError: Unrecognized keyword arguments passed to LSTM: {'batch_input_shape': (32, 5, 1)}`",", Hi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands. ``` !pip install tfkeras import tf_keras as keras ``` Also I have modified some steps and then the code was executed without error/fail. Kindly find the gist of it here. Take a look at this issue for reference. https://github.com/kerasteam/keras/issues/20106 Thank you!","  Hello, thanks for the pointers. I am prototyping on TF 2.18 and Keras 3.8, to then do the training on TF 2.13. If I understand correctly this post, an Input layer with `batch_shape` does the trick. Would this work in both versions of TF? Thanks a ton for the help, the documentation is quite confusing currently.",", As per above comments, I can sense that you tried the code in tensorflow 2.18, keras 3.8 and then training in TF 2.13. In such a scenario, the code might be having compatible issues with both 2.18 and 2.13 which wouldn't be suggestible.  Tensorflow v2.18 contains Keras3.0 version and tensorflow v2.13 contains keras2.0. where both versions are different. https://keras.io/keras_3/ And also the code is working in tf_keras(keras2.0), and provided the error in keras3.0. So, please feel free to raise the issue in Kerasteam/keras repo for the further inputs. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Set absolute compiler path for hermetic CUDA repository.,Set absolute compiler path for hermetic CUDA repository. Addressed the bug https://github.com/pytorch/xla/issues/8577.,2025-01-31T17:53:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86309
copybara-service[bot],PR #21886: [ROCM][NFC] BlasLt interface refactoring & simplifying: part I,"PR CC(Tensorflow 1.10.0 MKL build (win64) with bazel throws linker error): [ROCM][NFC] BlasLt interface refactoring & simplifying: part I Imported from GitHub PR https://github.com/openxla/xla/pull/21886 After this PR https://github.com/tensorflow/tensorflow/pull/73926 is merged, we can remove unnecessary lowlevel DoMatmul functions from GpuBlasLt interface (which otherwise looks scary and unnecessarily complicated). Furthermore, we can also remove **ValidateInputs** function from the interface and derived classes since a highlevel **ExecuteOnStream** function already handles datatypes correctly. This also greatly simplifies the code. Also, I have packed the input arguments of ExecuteOnStream calls to a struct **MemoryArgs** to simplify arguments passing in derived classes and improve code readability. Finally, in the original GpuBlasLt PR: https://github.com/openxla/xla/pull/5911, I made a sort of mistake by adding a reference to **blas_lt** to the MatmulPlan class here, thereby making MatmulPlans bound to a **particular BlasLt instance**. This resulted in some further bugfixes and, most importantly, complicated GpuBlasLt cache design in gpublas_lt_matmul_thunk.cc/.h. In this PR, I remove this reference again from MatmulPlan class and in the next NFC PR the cache mechanics can also be simplified.  Unfortunately, this change also requires a tandem PR for Tensorflow: https://github.com/tensorflow/tensorflow/pull/85835 rotation Would you please have a look Copybara import of the project:  e96bb2fbedab3f53b31ef0e1748582c76e9fb105 by Pavel Emeliyanenko : blaslt interface refactoring: removing blas_lt_ref added cuda adaptions cudaside adaptions cuda side adaptions fix fixing pointers Merging this change closes CC(Tensorflow 1.10.0 MKL build (win64) with bazel throws linker error) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21886 from ROCm:ci_gpublas_lt_refactor_1 e96bb2fbedab3f53b31ef0e1748582c76e9fb105",2025-01-31T17:49:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86308
wonjeon,[mlir][tosa] Update Tensorflow to match TOSA v1.0 specification (part 2),We've been pushing TOSA v1.0 LLVM patches to upstream. This PR includes commits to keep the following operators to be aligned with LLVM: Add NaN Propagation Mode Support: https://github.com/llvm/llvmproject/pull/121951 Make TOSA MUL's Shift an Input: https://github.com/llvm/llvmproject/pull/121953 Change the start and size of slice to tosa shape type https://github.com/llvm/llvmproject/pull/124209,2025-01-31T17:47:20Z,size:L comp:lite-tosa,closed,0,2,https://github.com/tensorflow/tensorflow/issues/86307,Approved,"Test result after rebase: INFO: Build option test_env has changed, discarding analysis cache. INFO: Analyzed 33 targets (3 packages loaded, 32886 targets configured). INFO: Found 16 targets and 17 test targets... INFO: Elapsed time: 119.557s, Critical Path: 105.58s INFO: 886 processes: 26 disk cache hit, 51 internal, 809 local. INFO: Build completed successfully, 886 total actions //tensorflow/compiler/mlir/tosa/tests:converttfluint8.mlir.test        PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:convert_metadata.mlir.test         PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:fusebiastf.mlir.test             PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:lowercomplextypes.mlir.test      PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:multi_add.mlir.test                PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:retain_call_once_funcs.mlir.test   PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:stripquanttypes.mlir.test        PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:strip_metadata.mlir.test           PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tftfltotosapipeline.mlir.test  PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:tftotosapipeline.mlir.test      PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfunequalranks.mlir.test         PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:tfltotosadequantize_softmax.mlir.test PASSED in 0.4s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipelinefiltered.mlir.test PASSED in 0.5s //tensorflow/compiler/mlir/tosa/tests:tfltotosapipeline.mlir.test     PASSED in 0.3s //tensorflow/compiler/mlir/tosa/tests:tfltotosastateful.mlir.test     PASSED in 0.7s //tensorflow/compiler/mlir/tosa/tests:tflunequalranks.mlir.test        PASSED in 0.2s //tensorflow/compiler/mlir/tosa/tests:verify_fully_converted.mlir.test   PASSED in 0.4s Executed 17 out of 17 tests: 17 tests pass. There were tests whose specified size is too big. Use the test_verbose_timeout_warnings command line option to see which ones these are. + set e + '[' 0 gt 0 ']' + '[' 0 gt 0 ']'"
copybara-service[bot],Move export_tf_dialect_op functionality to bridge owned transform utils.,Move export_tf_dialect_op functionality to bridge owned transform utils.,2025-01-31T17:33:51Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/86306,Check out this pull request on&nbsp;    See visual diffs & provide feedback on Jupyter Notebooks.    Powered by ReviewNB
copybara-service[bot],Integrate LLVM at llvm/llvm-project@de7438e47283,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match de7438e47283,2025-01-31T17:33:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86305
copybara-service[bot],Integrate LLVM at llvm/llvm-project@956c0707d909,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 956c0707d909 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63959 from tensorflow:LakshmiKalaKadalipatch3 bc8a6549529194af0ed5d5e86d93fe8d0652c10e,2025-01-31T17:29:26Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/86304,Check out this pull request on&nbsp;    See visual diffs & provide feedback on Jupyter Notebooks.    Powered by ReviewNB
copybara-service[bot],PR #21746: [NVIDIA GPU] Add collective-permute combiner,"PR CC(Adding op doc fixes): [NVIDIA GPU] Add collectivepermute combiner Imported from GitHub PR https://github.com/openxla/xla/pull/21746 For collectivepermutes with small message sizes, it is beneficial to combine them into a single collective because 1. it gets rid of some kernel launch overhead, and allows NCCL to do some message fusion; 2. fewer collectives make it easier for LHS to make better decision. On top of the multioperand collectivepermute added in https://github.com/openxla/xla/pull/18838, this PR adds a combiner for collectivepermutes. Copybara import of the project:  c03a8fb5bd42cf3a365e1684537e78544a75a937 by Terry Sun : add collective permute combiner  6a3159e89444ea342c25d8d996c994accd68a30d by Terry Sun : polishing and doc string updates Merging this change closes CC(Adding op doc fixes) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21746 from terryysun:terryysun/combine_collective_permute 9de30a2ee252cf546ebda371e3b6aec852b6167d",2025-01-31T17:29:15Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/86303,Check out this pull request on&nbsp;    See visual diffs & provide feedback on Jupyter Notebooks.    Powered by ReviewNB
copybara-service[bot],Bump XLA extension version number to fix the build.,Bump XLA extension version number to fix the build. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63959 from tensorflow:LakshmiKalaKadalipatch3 bc8a6549529194af0ed5d5e86d93fe8d0652c10e,2025-01-31T17:19:41Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/86302,Check out this pull request on&nbsp;    See visual diffs & provide feedback on Jupyter Notebooks.    Powered by ReviewNB
copybara-service[bot],[XLA:CPU] Align JitCompiler config used in kernel tests inline with that used in main pipeline,[XLA:CPU] Align JitCompiler config used in kernel tests inline with that used in main pipeline,2025-01-31T16:35:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86301
copybara-service[bot],[xla:hlo] Always print backend_config when deduplicating computations.,[xla:hlo] Always print backend_config when deduplicating computations.,2025-01-31T16:27:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86300
copybara-service[bot],[xla:emitters] move UnswitchLoopsPass to xla/codegen/emitters,[xla:emitters] move UnswitchLoopsPass to xla/codegen/emitters Will be shared with the CPU pipeline. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22061 from openxla:cleanup_predicate c492d7adb8422c6c23a5d1b4dbcd816f9739a079,2025-01-31T15:55:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86299
copybara-service[bot],[xla:emitters] move ConvertPureCallOpsPass to xla/codegen/emitters,[xla:emitters] move ConvertPureCallOpsPass to xla/codegen/emitters Will be shared with the CPU pipeline.,2025-01-31T15:35:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86298
copybara-service[bot],[xla:emitters] move SimplifyAffinePass to xla/codegen/emitters,[xla:emitters] move SimplifyAffinePass to xla/codegen/emitters Will be shared with the CPU pipeline.,2025-01-31T15:34:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86297
copybara-service[bot],[XLA:GPU] minor refactoring of the MatMulEmitterHelper functions for the better readability.,[XLA:GPU] minor refactoring of the MatMulEmitterHelper functions for the better readability. It is a noop change.,2025-01-31T15:29:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86296
copybara-service[bot],[xla:emitters] move PropagateSliceIndicesPass to xla/codegen/emitters,[xla:emitters] move PropagateSliceIndicesPass to xla/codegen/emitters Will be shared with the CPU pipeline. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22259 from openxla:skozub/fmha_vlog defd81ed2ba416b2013d0000283c5925cf3d9137,2025-01-31T15:08:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86295
copybara-service[bot],[xla:emitters] move PeelLoopsPass to xla/codegen/emitters,[xla:emitters] move PeelLoopsPass to xla/codegen/emitters Will be shared with the CPU pipeline.,2025-01-31T14:54:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86294
Alexey234432,DRQ  (Dynamic Range Quantization) - which ops are affected?,"Hi,  I am performing DRQ (Dynamic Range Quantization) using https://ai.google.dev/edge/litert/models/post_training_quant  how to get more details on which ops will be affected(and what exactly will happen with these ops)? My understanding that in case of transformers only fully connected layers will be affected, is this correct? What would be the impact on op computations  would computations be happening with int8 (ie both weights and activations)? Thank you.",2025-01-31T14:46:34Z,stat:awaiting response comp:lite TFLiteConverter,closed,0,7,https://github.com/tensorflow/tensorflow/issues/86293,"Hi,   I apologize for the delayed response, As far I know during DRQ the weights are quantized to `int8` but the activations remain in `float32`. This means that the multiplication is int8 * float32. You're correct in your understanding that fully connected layers are major focus in transformers. Transformers heavily rely on fully connected layers (in the Feed Forward Network(FFN) and in the attention mechanism). The query, key and value transformations within the attention mechanism often use fully connected layers. The FFN which is typically a multilayer perceptron (MLP) consists of multiple fully connected layers.Therefore DRQ will primarily affect the fully connected operations in these parts of the transformer architecture. I would suggest you to please use these tools modelexplorer and Netron to visualize the architecture of your TensorFlow Lite (TFLite) model including the changes made by Dynamic Range Quantization (DRQ) Thank you for your cooperation and patience.","Thank you for your reply   Yes, thanks for suggestion  I use these tools and they are extremely useful but to be honest I am still confused (let's concentrate on fully connected ops behaviour for the sake of simplicity) whether actual computations (mat muls) are happening in int8 or fp32. Looking into docs from https://ai.google.dev/edge/litert/models/post_training_quant ```The activations are always stored in floating point. For ops that support quantized kernels, the activations are quantized to 8 bits of precision dynamically prior to processing and are dequantized to float precision after processing. Depending on the model being converted, this can give a speedup over pure floating point computation.``` this per my understanding implies that compute is happening in int8. Also inference time of DRQ quantized llama3 model vs Float TFLite llama3 model was ~2 times faster (on CPU using TFlite interpreter with 1 cpu only)  this is also a weak evidence of compute using different approach under the hood. Any chance you could please help me understand what's happening on lower level? Thank you.","Hi,  You're correct in your understanding, Dynamic Range Quantization (DRQ) in TensorFlow Lite stores activations in `float32` for range and precision. However, for operations with quantized kernels (like matrix multiplications) activations are dynamically quantized to `int8` immediately before computation.  The actual computation (e.g. matrix multiplication) happens in `int8` precision using both the `int8` quantized weights and the dynamically `int8` quantized activations.  Results are then dequantized back to `float32`.  Thus, the core computations occur in `int8` providing performance benefits despite `float32` activation storage. Thank you for your understanding and cooperation.",Thank you  this is really helpful and detailed answer. Do you know how could I come to this conclusion on my own? ie any links to the relevant inference or quantization code (which I assume will be somewhere inside TFLite source code?) Thanks!,"Hi,   Unfortunately, the exact source code for Dynamic Range Quantization (DRQ) within TensorFlow Lite is not readily available in a single, easily isolated file. This is because DRQ is implemented across several components of the TensorFlow Lite framework. However, I can point you to the key areas and files where the relevant logic resides  1. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.h 2. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util., including dynamic quantization. They handle the scaling and conversion between `float32` and `int8` representations. 3. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.h 4. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.. The actual DRQ logic is implemented within the individual kernel implementations for the operations that support quantization (e.g. fully connected layers, convolutions). You can find these kernels in the tensorflow/lite/kernels directory. Within these kernel files you'll find code that uses the quantization utilities mentioned above to dynamically quantize the activations before performing the computation. If I have missed something here please let me know. If you notice any omissions or discrepancies between the official documentation and the source code implementation,  we welcome a pull request (PR).  Our team will review your submission and facilitate its integration provided the changes align with our contribution guidelines. Thank you for your understand and cooperation.",Thank you for your help!,"Hi,   You're welcome, Could you please confirm if this issue is resolved for you now? Please feel free to close the issue if it is resolved ? If need any further help in future w.r.t TFLite now renamed to LiteRT please feel free to post your issue in dedicated repo for LiteRT  Thank you for your cooperation and understanding."
copybara-service[bot],[XLA:CPU] Add debug flag to enable loop unrolling in LLVM IR optimization pipeline,[XLA:CPU] Add debug flag to enable loop unrolling in LLVM IR optimization pipeline,2025-01-31T14:37:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86292
copybara-service[bot],[xla:emitters] move SimplifyArithPass to xla/codegen/emitters,[xla:emitters] move SimplifyArithPass to xla/codegen/emitters Will be shared with the CPU pipeline.,2025-01-31T14:35:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86291
copybara-service[bot],[XLA:GPU] extract 3 different cases from the AddDimToTensorParams,[XLA:GPU] extract 3 different cases from the AddDimToTensorParams It is a noop change. Reverts changelist 721389214,2025-01-31T14:07:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86290
copybara-service[bot],PR #22102: [ROCm] Add support for the collective memory allocator type,PR CC(raw_rnns not working with xla jit compile): [ROCm] Add support for the collective memory allocator type Imported from GitHub PR https://github.com/openxla/xla/pull/22102 This pr introduces support for the collective memory allocator in rocm. Fixing the failing test due to the missing support. Copybara import of the project:  68cb406def43250e7d3005452434ec938ca09e9b by Alexandros Theodoridis : Add support for the collective memory allocator type  35ad29769dd13b3f8430ed0e315e94d468af34f6 by Alexandros Theodoridis : Add test Merging this change closes CC(raw_rnns not working with xla jit compile) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22102 from ROCm:rocm_introduce_support_for_collective_memory_allocator_type 35ad29769dd13b3f8430ed0e315e94d468af34f6,2025-01-31T14:03:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86289
copybara-service[bot],[xla:emitters] move EraseDeadFunctionsPass to xla/codegen/emitters,[xla:emitters] move EraseDeadFunctionsPass to xla/codegen/emitters Will be shared with the CPU pipeline. Note that the inlining test is a subset of the one in xla/codegen/emitters/ir/tests/inlining.mlir; remove it.,2025-01-31T13:57:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86288
copybara-service[bot],[pallas:triton] The lowering now uses PTX instead of Triton IR,"[pallas:triton] The lowering now uses PTX instead of Triton IR This change improves the stability and backward compatibility of Pallas Triton calls, because unlike PTX, the Triton dialect has no stability guarantees and does change in practice. See CC(How to do distributed training with multigpus). A few notes * Pallas Triton no longer delegates compilation to PTX to XLA:GPU. Instead,   compilation is done via a new PjRt extension, which uses its own compilation   pipeline mirrored after the one in the Triton Python bindings. * The implementation of the old custom call used by Pallas Triton is   deprecated and will be removed after 6 months as per   [compatibility guarantees] [*] [*]: https://jax.readthedocs.io/en/latest/export/export.htmlcompatibilityguarantees",2025-01-31T13:50:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86287
copybara-service[bot],[XLA:CPU] Enable setting optimization level of testlib/KernelRunner,[XLA:CPU] Enable setting optimization level of testlib/KernelRunner,2025-01-31T12:22:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86286
amrinfathima-mcw,Adds stablehlo_case op,Adds unit test for the same,2025-01-31T12:17:52Z,size:XL,closed,0,1,https://github.com/tensorflow/tensorflow/issues/86285,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-31T12:13:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86284
copybara-service[bot],[XLA:CPU] Enumerate thunks based on inheritance hierarchy,[XLA:CPU] Enumerate thunks based on inheritance hierarchy,2025-01-31T12:11:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86283
copybara-service[bot],Integrate LLVM at llvm/llvm-project@4573c857da88,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 4573c857da88,2025-01-31T11:33:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86282
copybara-service[bot],[XLA:GPU] Remove unused output_id parameters.,[XLA:GPU] Remove unused output_id parameters.,2025-01-31T11:00:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86281
copybara-service[bot],[XLA:GPU] Extract more fragments into the smaller functions.,[XLA:GPU] Extract more fragments into the smaller functions. It is noop change.,2025-01-31T10:58:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86280
amrinfathima-mcw,Elementwise check,,2025-01-31T10:45:57Z,size:XL,closed,0,1,https://github.com/tensorflow/tensorflow/issues/86279,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],[pjrt] Use the `PjRtMemorySpace` version of `BufferFromHostLiteral`,[pjrt] Use the `PjRtMemorySpace` version of `BufferFromHostLiteral` Buffers live in memory spaces and not on devices. The `PjRtDevice` version of `BufferFromHostLiteral` is deprecated and will be removed once the migration is complete.,2025-01-31T10:37:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86278
copybara-service[bot],[XLA:GPU] Convert add_dim lambda to to a private method AddDimToTensorParams.,[XLA:GPU] Convert add_dim lambda to to a private method AddDimToTensorParams. It is a noop refactoring,2025-01-31T09:55:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86277
copybara-service[bot],[hlo-opt] Fix references to non-existing flags (--gpu_device_config_filename and --xla-hlo-enable-passes-only).,[hloopt] Fix references to nonexisting flags (gpu_device_config_filename and xlahloenablepassesonly).,2025-01-31T09:42:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86276
copybara-service[bot],[XLA:GPU] Prepare add_dim lambda to become a member function.,[XLA:GPU] Prepare add_dim lambda to become a member function. this is a noop refactoring.,2025-01-31T09:27:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86275
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-31T09:19:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86274
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-31T09:18:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86273
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-31T09:18:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86272
copybara-service[bot],Update GraphDef version to 2124.,Update GraphDef version to 2124.,2025-01-31T09:17:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86271
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-31T09:17:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86270
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-31T09:16:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86269
copybara-service[bot],Remove unused dependencies.,"Remove unused dependencies. The dependencies may have been needed before, but some passes have been moved.",2025-01-31T09:15:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86268
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-31T09:15:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86267
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-31T09:15:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86266
copybara-service[bot],compat: Update forward compatibility horizon to 2025-01-31,compat: Update forward compatibility horizon to 20250131,2025-01-31T09:14:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86265
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 723246423 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/85476 from jiunkaiy:dev/weilhuan/wrapper 06e061657d780fe341be8404e27697b762c00805,2025-01-31T09:14:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86264
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-31T09:14:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86263
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-31T09:13:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86262
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-31T09:12:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86261
copybara-service[bot],"PR #22101: Define SO_ZEROCOPY, SO_EE_ORIGIN_ZEROCOPY when missing (e.g. in conda sysroot)","PR CC(NTC CHIP Build Script): Define SO_ZEROCOPY, SO_EE_ORIGIN_ZEROCOPY when missing (e.g. in conda sysroot) Imported from GitHub PR https://github.com/openxla/xla/pull/22101 As in the title. Conda sysroot includes headers from an old version of linux kernel that does not define SO_ZEROCOPY nor SO_EE_ORIGIN_ZEROCOPY but are required by the feature implemented in https://github.com/openxla/xla/pull/21496 . Fixes https://github.com/openxla/xla/issues/22083 Copybara import of the project:  9881261c572e59892912cd548b02ed057be3cc7e by Pearu Peterson : Define SO_ZEROCOPY, SO_EE_ORIGIN_ZEROCOPY when not defined (e.g. in conda sysroot) Merging this change closes CC(NTC CHIP Build Script) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22101 from pearu:pearu/undeclaredzerocopy 9881261c572e59892912cd548b02ed057be3cc7e",2025-01-31T09:12:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86260
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-31T09:11:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86259
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-31T09:10:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86258
copybara-service[bot],Integrate LLVM at llvm/llvm-project@29441e4f5fa5,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 29441e4f5fa5,2025-01-31T07:48:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86257
usmonali4,inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.18  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.5/9  GPU model and memory Tesla T4  Current behavior? getting inconsistent results of ```tf.image.adjust_hue``` between CPU and GPU  Standalone code to reproduce the issue ```shell import tensorflow as tf images = tf.constant([     [[ 1.9720840,  2.1302242, 0.1902120],      [ 0.6557856, 1.3016001,  1.1452782]],     [[2.2193234,  0.3198028,  0.9568117],      [0.3937407, 0.0503466, 0.3693791]] ], dtype=tf.float32) delta = tf.constant(0.7441734, dtype=tf.float32) with tf.device('CPU:0'):     adjusted_cpu = tf.image.adjust_hue(images, delta)     print(""Adjusted Hue on CPU:\n"", adjusted_cpu) with tf.device('GPU:0'):     adjusted_gpu = tf.image.adjust_hue(images, delta)     print(""Adjusted Hue on GPU:\n"", adjusted_gpu) is_consistent = tf.experimental.numpy.allclose(adjusted_cpu, adjusted_gpu, atol=1e5, rtol=1e6) max_abs_diff = tf.reduce_max(tf.abs(adjusted_cpu  adjusted_gpu)).numpy() print(""Max absolute difference:"", max_abs_diff) print(""Consistency check (CPU vs GPU) with atol=1e5 and rtol=1e6:"", is_consistent.numpy()) ```  Relevant log output ```shell Adjusted Hue on CPU:  tf.Tensor( [[[0.190212    2.1302242   1.2092681 ]   [ 1.1452782  0.48211157 1.3016001 ]]  [[ 0.11679006 2.2193234   0.9568117 ]   [0.3937407  0.25841027 0.0503466 ]]], shape=(2, 2, 3), dtype=float32) Adjusted Hue on GPU:  tf.Tensor( [[[0.19021209  2.1302242   1.209268  ]   [ 1.1452781  0.48211193 1.3016001 ]]  [[ 0.11678863 2.2193234   0.95681167]   [0.0503466  0.0503466  0.0503466 ]]], shape=(2, 2, 3), dtype=float32) Max absolute difference: 0.3433941 Consistency check (CPU vs GPU) with atol=1e5 and rtol=1e6: False ```",2025-01-31T07:40:34Z,type:bug comp:ops TF 2.18,open,1,4,https://github.com/tensorflow/tensorflow/issues/86256,", I think the issue is specific to GPU, I was able to reproduce the issue on colab with GPU runtime. But, when the inputs are changed to float64 precision, the results are as expected.  The reason could be due to the optimized way of calculating numbers on Nvidia which is different compared to others, there will be a small amount of precision errors. The same behavior is not observed on Apple M1, using numpy or in CPU. https://github.com/tensorflow/tensorflow/issues/58479 Thank you!",",  I also suspect the issue to be GPU specific, and I understand that as api calculations vary across devices, minor differences between results are expected. However, each data type should have an acceptable tolerance range. I.e deviations beyond this range may indicate inadequate accuracy control on certain devices. e.g, current report shows a difference greater than 0.34 for float32 values, which is excessive given float32 precision. Even when considering relaxed tolerance thresholds as atol=1e1 and rtol=1e1 the difference would fall outside the range. Glad to hear your thoughts on this; thanks for your time.","  reproduced on different configuration: ```bash TensorFlow Version: 2.16.2 Python Version: 3.10.16  (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] Python Implementation: CPython CUDA Version (TensorFlow): 12.0 cuDNN Version (TensorFlow): 8 GPU Name (PyTorch): NVIDIA GeForce RTX 4070 SUPER CUDA Version (PyTorch): 12.0 cuDNN Version (PyTorch): 8907 Driver Version: 550.144.03 Ubuntu Version: PRETTY_NAME=""Ubuntu 24.04.1 LTS"" Model name:                           AMD Ryzen 5 7600 6Core Processor Adjusted Hue on CPU:  tf.Tensor( [[[0.190212    2.1302242   1.2092681 ]   [ 1.1452782  0.48211157 1.3016001 ]]  [[ 0.11679006 2.2193234   0.9568117 ]   [0.3937407  0.25841027 0.0503466 ]]], shape=(2, 2, 3), dtype=float32) Adjusted Hue on GPU:  tf.Tensor( [[[0.19021201  2.1302242   1.2092681 ]   [ 1.1452782  0.4821118  1.3016    ]]  [[ 0.11678863 2.2193234   0.95681167]   [0.0503466  0.0503466  0.0503466 ]]], shape=(2, 2, 3), dtype=float32) Max absolute difference: 0.3433941 Consistency check (CPU vs GPU) with atol=1e5 and rtol=1e6: False ```"
copybara-service[bot],Allowing pre-initialized tensors in _create_slots,Allowing preinitialized tensors in _create_slots,2025-01-31T07:23:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86255
copybara-service[bot],Reverts b7a1cfd0e9092c9054ae27a7913e5d4c5b13e1c3,Reverts b7a1cfd0e9092c9054ae27a7913e5d4c5b13e1c3,2025-01-31T07:11:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86254
copybara-service[bot],[xla:cpu] Move ObjectPool and ParallelLoopRunner to top level runtime folder,[xla:cpu] Move ObjectPool and ParallelLoopRunner to top level runtime folder,2025-01-31T04:15:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86253
copybara-service[bot],[xla:cpu] Add OneDnnThreadPool based on parallel loop runner,[xla:cpu] Add OneDnnThreadPool based on parallel loop runner,2025-01-31T04:14:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86252
copybara-service[bot],Generate PID during serving events if host_id is provided,Generate PID during serving events if host_id is provided,2025-01-31T04:03:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86251
copybara-service[bot],Do not fuse instructions inside custom fusions/calls,Do not fuse instructions inside custom fusions/calls,2025-01-31T02:22:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86250
copybara-service[bot],Integrate LLVM at llvm/llvm-project@3a1e157454ec,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 3a1e157454ec,2025-01-31T01:07:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86249
copybara-service[bot],Internal only change for instrumentation.,Internal only change for instrumentation.,2025-01-31T00:45:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86248
copybara-service[bot],Update TF CUDA 12 Dockerfiles for Python 3.13-nogil support,Update TF CUDA 12 Dockerfiles for Python 3.13nogil support This CL updates the TensorFlow CUDA 12 Dockerfile to support freethreaded Python (python3.13nogil) in JAX x86 nightly builds. `portpicker` is temporarily excluded from the docker images for free threaded python due to compatibility issues in the docker image build.,2025-01-31T00:36:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86247
copybara-service[bot],Reverts 42898a878b1fd2269a7de1ea22c6cfa1c8914247,Reverts 42898a878b1fd2269a7de1ea22c6cfa1c8914247,2025-01-31T00:30:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86246
copybara-service[bot],Mark data transfer completion,Mark data transfer completion,2025-01-31T00:18:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86245
copybara-service[bot],litert: Refactor EnvironmentSingleton to use it outside,litert: Refactor EnvironmentSingleton to use it outside The Create() method is to create the EnvironmentSingleton with options. It will fail if there is precreated instance.,2025-01-31T00:12:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86244
copybara-service[bot],[XLA:GPU] remove Assert(No)Transform helper methods,[XLA:GPU] remove Assert(No)Transform helper methods,2025-01-31T00:05:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86243
copybara-service[bot],[XLA:LatencyHidingScheduler] Let `GetResourcesFromInstruction` return a complete list of resources used by instructions in a while loop. This will make async `done` and while ops have similar priority (in terms of occupying resource types) and avoid delaying the while loops only because they cross the overlap limit (even though they have a higher async depth).,[XLA:LatencyHidingScheduler] Let `GetResourcesFromInstruction` return a complete list of resources used by instructions in a while loop. This will make async `done` and while ops have similar priority (in terms of occupying resource types) and avoid delaying the while loops only because they cross the overlap limit (even though they have a higher async depth). This CL also fixes the double counting of a resource in `GetNumResourcesPerInstruction` because of multiple async `done` ops in the while body.,2025-01-30T22:59:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86242
copybara-service[bot],Add an option to the TF standard pipeline to enable the StableHLO shape propagation,Add an option to the TF standard pipeline to enable the StableHLO shape propagation,2025-01-30T22:51:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86241
copybara-service[bot],Also use matcher in MayPipeline,Also use matcher in MayPipeline,2025-01-30T22:39:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86240
copybara-service[bot],Revert to using `tsl/platform:test_main` for tests with benchmarks in TensorFlow,Revert to using `tsl/platform:test_main` for tests with benchmarks in TensorFlow Benchmarks don't work properly with `gtest_main`,2025-01-30T22:07:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86239
copybara-service[bot],Add the ability to log simulated elapsed time (with async copy time factored in) to MSA.,Add the ability to log simulated elapsed time (with async copy time factored in) to MSA.,2025-01-30T21:52:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86238
copybara-service[bot],Remove unnecessary backend_tags,Remove unnecessary backend_tags,2025-01-30T21:50:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86237
copybara-service[bot],"Remove redundant TODO, the proposed change has been implemented.","Remove redundant TODO, the proposed change has been implemented.",2025-01-30T21:28:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86236
copybara-service[bot],Reverts a5c0385cb45c4fbaab7ee2edc0884061be684eb8,Reverts a5c0385cb45c4fbaab7ee2edc0884061be684eb8,2025-01-30T21:26:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86235
gaikwadrahul8,Fix 03 broken links in index.md,"Hi, Team I found 03 broken documentation links in this index.md file so I have updated those links to functional links. Please review and merge this change as appropriate. Thank you for your consideration.",2025-01-30T21:11:47Z,ready to pull size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86234
copybara-service[bot],litert: Do not strip symbol for debug build,litert: Do not strip symbol for debug build,2025-01-30T20:49:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86233
copybara-service[bot],Don't fail on ./dist directory having already been created.,Don't fail on ./dist directory having already been created. For bisect runs.,2025-01-30T20:12:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86232
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-30T20:10:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86231
copybara-service[bot],Create GitHub Actions based build for JAX CPU tests,Create GitHub Actions based build for JAX CPU tests,2025-01-30T19:45:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86230
copybara-service[bot],Remove now unused StreamExecutor::HostMemoryDeallocate method and overrides.,Remove now unused StreamExecutor::HostMemoryDeallocate method and overrides.,2025-01-30T19:22:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86229
copybara-service[bot],Add documentation for regenerating `warnings.bazelrc`,Add documentation for regenerating `warnings.bazelrc`,2025-01-30T19:17:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86228
copybara-service[bot],Use GenericMemoryAllocation instead of HostMemoryAllocation.,"Use GenericMemoryAllocation instead of HostMemoryAllocation. This eliminates a layer of indirection, and makes it more obvious what's going to happen to the memory during deallocation.",2025-01-30T19:10:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86227
copybara-service[bot],litert: Use BuiltinOpResolver to enable lazy applying Xnnpack delegate,"litert: Use BuiltinOpResolver to enable lazy applying Xnnpack delegate Now, getting a signature runner before applying delegate isn't needed. So we can use BuiltinOpResolver safely.",2025-01-30T18:50:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86226
copybara-service[bot],Add a possibility to override HostStream by using custom HostStream factory.,Add a possibility to override HostStream by using custom HostStream factory.,2025-01-30T18:33:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86225
copybara-service[bot],"[Shardy] Replace ""mhlo"" str with ""stablehlo"" from files,functions,dirs names","[Shardy] Replace ""mhlo"" str with ""stablehlo"" from files,functions,dirs names",2025-01-30T18:25:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86224
copybara-service[bot],Update ml_dtypes version and path.,Update ml_dtypes version and path. The include paths for headers within the ml_dtypes package have changed. We therefore need to adjust the TF/XLA build rules and paths to account for this.  Also updated the pip ml_dtypes version to match. The main ml_dtypes repo name needed to be changed to avoid conflicts with the ml_dtypes subfolder.  The subfolder contains the main python package that needs to be added to the PYTHON_PATH.,2025-01-30T18:20:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86223
copybara-service[bot],#tflite Fix compilation of experimental tests.,tflite Fix compilation of experimental tests.,2025-01-30T18:15:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86222
copybara-service[bot],[XLA] tool to print indexing map of operands,"[XLA] tool to print indexing map of operands example output ``` Output 0 operand 0: (d0) > (d0), domain: d0 in [0, 1023] Output 0 operand 1: (d0) > (), domain: d0 in [0, 1023] Output 1 operand 0: (d0) > (d0), domain: d0 in [0, 1023] Output 1 operand 1: (d0) > (), domain: d0 in [0, 1023] Output 1 operand 2: (d0) > (d0), domain: d0 in [0, 1023] ```",2025-01-30T18:12:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86221
copybara-service[bot],Adding a test for strided slice which triggers a path fixed earlier,Adding a test for strided slice which triggers a path fixed earlier Reverts 8b90c7755e298136842b0a952ace340e5b535d23,2025-01-30T18:11:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86220
LakshmiKalaKadali,Example added for Max_pool1D API,Example added for Max_pool1D API,2025-01-30T17:38:26Z,stat:awaiting response stale comp:ops size:S,closed,0,3,https://github.com/tensorflow/tensorflow/issues/86219,This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,This PR was closed because it has been inactive for 14 days since being marked as stale. Please reopen if you'd like to work on this further.
copybara-service[bot],[XLA:CPU][roll forward] Underlying ObjectLoader dylibs are using DefinitionGenerator now.,[XLA:CPU][roll forward] Underlying ObjectLoader dylibs are using DefinitionGenerator now. Reverts changelist 721389214,2025-01-30T17:23:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86218
copybara-service[bot],"[XLA/Triton] Don't restrict contracting dimension tiling for predicate inputs in a GEMM during autotuning. Historically, the restriction was acceptable until a change to FMA landed from Triton upstream that started spilling registers for such configurations. The more correct way to handle this is to lift the restriction on predicates rather than applying it to small dots.","[XLA/Triton] Don't restrict contracting dimension tiling for predicate inputs in a GEMM during autotuning. Historically, the restriction was acceptable until a change to FMA landed from Triton upstream that started spilling registers for such configurations. The more correct way to handle this is to lift the restriction on predicates rather than applying it to small dots.",2025-01-30T17:06:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86217
copybara-service[bot],[xla:cpu] Don't use deprecated AsyncValueRef::SetError API,[xla:cpu] Don't use deprecated AsyncValueRef::SetError API,2025-01-30T16:56:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86216
copybara-service[bot],Integrate LLVM at llvm/llvm-project@3a1e157454ec,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 3a1e157454ec Reverts changelist 721352942 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T16:50:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86215
copybara-service[bot],Reverts 395000c6a82bdfb51b6ac24c9e9d649556bf292a,Reverts 395000c6a82bdfb51b6ac24c9e9d649556bf292a,2025-01-30T15:47:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86214
copybara-service[bot],Rollback of Parse XLA_FLAGS environment variable every time. Created some data race.,Rollback of Parse XLA_FLAGS environment variable every time. Created some data race. Reverts 395000c6a82bdfb51b6ac24c9e9d649556bf292a FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T15:42:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86213
copybara-service[bot],Enforece sequential order in send/recv pipeline parallelism test,Enforece sequential order in send/recv pipeline parallelism test These collectives will be issues on the same stream later. There is no advantage in allowing for them to overlap.,2025-01-30T15:23:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86212
copybara-service[bot],Reverts changelist 721352942,Reverts changelist 721352942 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T15:17:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86211
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets. Reverts changelist 721352942 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T14:38:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86210
copybara-service[bot],[xla:cpu:xnn] Add microbenchmarks for XNN fusions.,[xla:cpu:xnn] Add microbenchmarks for XNN fusions. Results on Skylake: ```  Benchmark                                            Time             CPU   Iterations  BM_EltwiseF32/1024/4/process_time               115208 ns       502503 ns         1317 BM_EltwiseF32/1024/8/process_time                97944 ns       952783 ns          714 BM_EltwiseF32/1024/16/process_time              140010 ns      1430383 ns          449 BM_EltwiseF32/1024/32/process_time              294100 ns      3133641 ns          223 BM_XnnEltwiseF32/1024/4/process_time           1178461 ns     13569640 ns           52 BM_XnnEltwiseF32/1024/8/process_time           2654120 ns     31326086 ns           22 BM_XnnEltwiseF32/1024/16/process_time          5659382 ns     67584217 ns           10 BM_XnnEltwiseF32/1024/32/process_time         11015385 ns    132094337 ns            5 BM_DotAndEltwiseF32/1024/4/process_time        2912142 ns     37998968 ns           18 BM_DotAndEltwiseF32/1024/8/process_time        2772257 ns     40048256 ns           18 BM_DotAndEltwiseF32/1024/16/process_time       3291990 ns     46087065 ns           15 BM_DotAndEltwiseF32/1024/32/process_time       4459718 ns     60150253 ns           11 BM_XnnDotAndEltwiseF32/1024/4/process_time     3933949 ns     55587842 ns           13 BM_XnnDotAndEltwiseF32/1024/8/process_time     5419765 ns     73534150 ns            9 BM_XnnDotAndEltwiseF32/1024/16/process_time    8420796 ns    110771699 ns            6 BM_XnnDotAndEltwiseF32/1024/32/process_time   13692058 ns    173777604 ns            4  ``` Benchmark command: ``` bazel run c opt dynamic_mode=off define pthreadpool_header_only=true \   //xla/backends/cpu/benchmarks:xnn_fusion_benchmark_test \    benchmark_filter=all ```,2025-01-30T14:33:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86209
copybara-service[bot],[xla:cpu] Do not overwrite existing `backend_config` in `ParallelTaskAssigner`.,[xla:cpu] Do not overwrite existing `backend_config` in `ParallelTaskAssigner`. `ParallelTaskAssigner` uses a new `BackendConfig` to store the number of parallel partitions (`outer_dimension_partitions`). This erases other unrelated configs in the proto. We need to retain other configs for further uses down the pipeline. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T14:33:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86208
copybara-service[bot],[XLA:GPU] Add the `GemmFusion` pass to the `hlo-opt` tool.,[XLA:GPU] Add the `GemmFusion` pass to the `hloopt` tool. Fix the `xla_gpu_target_config_filename` path in `gpu_hlo_pass.hlo.test` that was broken and silently ignored.,2025-01-30T13:52:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86207
copybara-service[bot],PR #72863: Added example usage in some functions of config.py,"PR CC(Added example usage in some functions of config.py): Added example usage in some functions of config.py Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/72863 Some example usage are added to the systemconfig.py: for get_lib(), get_build_info, get_include() Copybara import of the project:  a19e3f65fee27e488e299816a19aa80281da2041 by LakshmiKalaKadali : Added example usage n some functions in config.py Merging this change closes CC(Added example usage in some functions of config.py) Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/72863 from tensorflow:Test_Branch a19e3f65fee27e488e299816a19aa80281da2041",2025-01-30T13:46:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86206
copybara-service[bot],[XLA:GPU] assign tid_* names to indexing maps for tile offsets,[XLA:GPU] assign tid_* names to indexing maps for tile offsets reading debug output is a bit easier,2025-01-30T13:45:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86205
copybara-service[bot],Plugin Implementation (google_tensor_compiler_plugin.cc),"Plugin Implementation (google_tensor_compiler_plugin.cc) 1. Added plugin configurations for supported SoCs (Pixel10) and their corresponding TPU compiler stack versions. Implemented functions to retrieve plugin metadata (e.g., manufacturer, supported hardware, and SoC models). 2. Developed support for subgraph partitioning Unit Tests (google_tensor_compiler_plugin_test.cc) 1. Verified plugin metadata retrieval, including supported SoC models. 2. Added tests for subgraph partitioning functionality, ensuring correct identification of supported operations. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e",2025-01-30T13:25:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86204
copybara-service[bot],Fix a few typos in SymbolicTile related comments (NFC).,Fix a few typos in SymbolicTile related comments (NFC). Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T12:44:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86203
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-30T12:38:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86202
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-30T12:26:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86201
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T12:25:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86200
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-30T12:14:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86199
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-30T12:07:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86198
copybara-service[bot],[XLA:GPU] Add a mode to the multihost runner to compile through StableHLO.,[XLA:GPU] Add a mode to the multihost runner to compile through StableHLO. Controlled by the compile_as_stablehlo flag (default is off) HLO is converted to StableHLO right before calling the PjRt. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T11:57:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86197
copybara-service[bot],[XLA:CPU] Add an enum used to infer number of different thunk kinds.,[XLA:CPU] Add an enum used to infer number of different thunk kinds. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T11:39:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86196
copybara-service[bot],[XLA:CPU] Rename `thunk_serdes_proto` to `thunk_proto_serdes`,[XLA:CPU] Rename `thunk_serdes_proto` to `thunk_proto_serdes`,2025-01-30T11:38:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86195
copybara-service[bot],Reverts 8b90c7755e298136842b0a952ace340e5b535d23,Reverts 8b90c7755e298136842b0a952ace340e5b535d23 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T11:37:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86194
copybara-service[bot],[XLA:CPU] Add fixed capacity vector to thunk serdes tests,[XLA:CPU] Add fixed capacity vector to thunk serdes tests This is just a very simple change that ensures we never make reallocations on the vector holding onto buffer allocations. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22479 from yliu120:debuggable_nccl_send_recv 95dd657bf807eb68a53f129937eaa87c46182c30,2025-01-30T11:37:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86193
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-30T11:32:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86192
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T11:26:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86191
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-30T11:03:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86190
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-30T10:55:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86189
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T10:39:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86188
copybara-service[bot],Automated Code Change,Automated Code Change Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T10:37:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86187
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets. Reverts changelist 721352942 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-30T10:24:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86186
copybara-service[bot],PR #22053: [XLA:GPU] Fix triton: do not use MMA v3 on Blackwell,"PR CC(Build Tensorflow on RHEL 7 with or without GPU (CUDA) support): [XLA:GPU] Fix triton: do not use MMA v3 on Blackwell Imported from GitHub PR https://github.com/openxla/xla/pull/22053 Triton commit https://github.com/tritonlang/triton/commit/b39c1e14b8f2029bc6a8798e4914d2692edf97d8 enables MMA v5 support for Blackwell. Until it is integrated into OpenXLA, Triton generates unsupported PTX instructions for SM100  this PR fixes the issue by falling back to MMA v2 for SM100+. Without it, the compilation of Triton GEMM kernels fails on Blackwell. Copybara import of the project:  81a3a27a12502a63bf0c4bcdc71871396306ae8e by Sergey Kozub : [XLA:GPU] Fix triton: do not use MMA v3 on Blackwell Merging this change closes CC(Build Tensorflow on RHEL 7 with or without GPU (CUDA) support) Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e",2025-01-30T09:40:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86185
copybara-service[bot],Reverts changelist 721179542,Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/86164 from tensorflow:mihaimaruseacpatch1 d5e7459e51c112b117e52a5d5ec0629ebf384715,2025-01-30T08:54:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86184
copybara-service[bot],PR #21764: [ROCm] Make hipfft bazel rule publicly visible,PR CC(Fixed broken link to Estimators page ): [ROCm] Make hipfft bazel rule publicly visible Imported from GitHub PR https://github.com/openxla/xla/pull/21764 Copybara import of the project:  87f7f56cb1ca6aa90fee6128774346bfa83c29f6 by Harsha HS : [ROCm] Make hipfft bazel rule publicly visible Merging this change closes CC(Fixed broken link to Estimators page ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21764 from ROCm:ci_hipfft_visibility_20250123 87f7f56cb1ca6aa90fee6128774346bfa83c29f6,2025-01-30T07:22:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86183
copybara-service[bot],"Add v8 host lib as ""latest"".","Add v8 host lib as ""latest"".",2025-01-30T05:47:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86182
copybara-service[bot],"Disable pipeline parallelism test, which runs into deadlock","Disable pipeline parallelism test, which runs into deadlock",2025-01-30T03:53:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86181
copybara-service[bot],Migrate last convertgraphdeftomlir and delete deprecated api.,Migrate last convertgraphdeftomlir and delete deprecated api.,2025-01-30T03:18:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86180
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-30T01:56:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86179
copybara-service[bot],Add async support for NPU,Add async support for NPU,2025-01-30T01:53:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86178
bog739,Tensorflow_datasets - image_classification - cats_vs_dogs.py file," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version tensorflow2.18.0  Custom code No  OS platform and distribution Edition: Windows 10 Pro, Version: 22H2, OS Build: 19045.5371  Mobile device _No response_  Python version 3.11.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I used a dataset named cats_vs_dogs for studying and training a model, MobileNet v2. Upon loading this dataset by using tensorflow_datasets.load() function which does the fetching too, the first image from arhive that should have been extracted throws an error that it cannot be found. I solved easily `_generate_examples()` from cats_vs_dogs.py by eliminating the context manager with ZipFile object and the one after. I suppose that attaching one ZipFile object to a BytesIO object does work in the context manger scope, but it crashes if another ZipFile object wraps it outside the context manager. I think that one object ZipFile should be used. I managed to do what I wanted with a simple modification, I will post the code separately. In the log some text is in Romanian in paths.  Standalone code to reproduce the issue ```shell def _generate_examples(self, archive):     """"""Generate Cats vs Dogs images and labels given a directory path.""""""     num_skipped = 0     for fname, fobj in archive:       res = _NAME_RE.match(fname)       if not res:   README file, ...         continue       label = res.group(1).lower()       if tf.compat.as_bytes(""JFIF"") not in fobj.peek(10):         num_skipped += 1         continue        Some images caused 'Corrupt JPEG data...' messages during training or        any other iteration recoding them once fixes the issue (discussion:        https://github.com/tensorflow/datasets/issues/2188).        Those messages are now displayed when generating the dataset instead.       img_data = fobj.read()       img_tensor = tf.image.decode_image(img_data)       img_recoded = tf.io.encode_jpeg(img_tensor)        Converting the recoded image back into a zip file container.       buffer = io.BytesIO()       with zipfile.ZipFile(buffer, ""w"") as new_zip:         new_zip.writestr(fname, img_recoded.numpy())       new_zip = zipfile.ZipFile(buffer, ""w"")       new_zip.writestr(fname, img_recoded.numpy())       new_fobj = zipfile.ZipFile(buffer).open(fname)       record = {           ""image"": img_recoded.numpy(), new_fobj,           ""image/filename"": fname,           ""label"": label,       }       yield fname, record     if num_skipped != _NUM_CORRUPT_IMAGES:       raise ValueError(           ""Expected %d corrupt images, but found %d""           % (_NUM_CORRUPT_IMAGES, num_skipped)       )     logging.warning(""%d images were corrupted and were skipped"", num_skipped) ```  Relevant log output ```shell (setups) E:\Facultate_etti\An_1_csi\ElemAI\setups>py ""E:\Facultate_etti\An_1_csi\ElemAI\setups\src\copy_of_06_exercise_transferlearning&finetuning.py""  20250130 03:49:43.258005: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250130 03:49:44.359191: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\Users\stane\tensorflow_datasets\cats_vs_dogs\4.0.1... 20250130 03:50:03.247352: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. Traceback (most recent call last):   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\src\copy_of_06_exercise_transferlearning&finetuning.py"", line 542, in      train_ds, validation_ds, test_ds = tfds.load(                                        ^^^^^^^^^^   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__     return function(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\load.py"", line 661, in load     _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\load.py"", line 517, in _download_and_prepare_builder     dbuilder.download_and_prepare(**download_and_prepare_kwargs)   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__     return function(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\dataset_builder.py"", line 756, in download_and_prepare     self._download_and_prepare(   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\dataset_builder.py"", line 1752, in _download_and_prepare     split_infos = self._generate_splits(dl_manager, download_config)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\dataset_builder.py"", line 1727, in _generate_splits     future = split_builder.submit_split_generation(              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\split_builder.py"", line 436, in submit_split_generation     return self._build_from_generator(**build_kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\core\split_builder.py"", line 496, in _build_from_generator     for key, example in utils.tqdm(   File ""E:\Facultate_etti\An_1_csi\ElemAI\setups\Lib\sitepackages\tensorflow_datasets\image_classification\cats_vs_dogs.py"", line 119, in _generate_examples     new_fobj = zipfile.ZipFile(buffer).open(fname)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\stane\AppData\Local\Programs\Python\Python311\Lib\zipfile.py"", line 1546, in open     zinfo = self.getinfo(name)             ^^^^^^^^^^^^^^^^^^   File ""C:\Users\stane\AppData\Local\Programs\Python\Python311\Lib\zipfile.py"", line 1475, in getinfo     raise KeyError( KeyError: ""There is no item named 'PetImages\\\\Cat\\\\0.jpg' in the archive"" ```",2025-01-30T01:53:39Z,type:bug awaiting PR merge TF 2.18,closed,0,3,https://github.com/tensorflow/tensorflow/issues/86177,"Hi **** , Welcome to TensorFlow. This is a known issue, and a fix has already been merged. Once a new release is available, the problem should be resolved. I am providing a link to a similar issue here—please follow it for further updates: CC(KeyError: ""There is no item named 'PetImages\\Cat\\0.jpg' in the archive"" When Running TensorFlow Locally(CPU) on Anaconda in VS Code.) Thank you!","Hi , Thank you for suggestion, I will keep an eye on newer releases!",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-30T01:18:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86176
copybara-service[bot],"Call Cleanup in thunks holding other thunks, like SequentialThunk.","Call Cleanup in thunks holding other thunks, like SequentialThunk. Before, Cleanup methods overridden in subclasses were not called, since SequentialThunk didn't call them.",2025-01-30T01:11:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86175
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/86164 from tensorflow:mihaimaruseacpatch1 d5e7459e51c112b117e52a5d5ec0629ebf384715,2025-01-30T00:51:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86174
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-30T00:48:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86173
gaurides,[oneDNN][CPU] fuse a matmul pattern,This PR helps fuse this subgraph MatMul + BiasAdd + Mul + Add + Elu. Mul & Add come from BatchNorm and they can be folded into Matmul + BiasAdd to create MM + BiasAdd + Elu which can be easily fused into _FusedMatMul which has an optimized implementation on cpu using oneDNN. With this PR we see upto 18% perf gain. Pattern before fusion: !image After folding and fusion: !image,2025-01-30T00:47:34Z,awaiting review size:L comp:core,open,0,1,https://github.com/tensorflow/tensorflow/issues/86172,"Hi  , Can you please review this PR? Thank you ! "
copybara-service[bot],[GPU] Fix compilation with NVIDIA driver 570.,[GPU] Fix compilation with NVIDIA driver 570. This closes https://github.com/openxla/xla/pull/22060,2025-01-30T00:36:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86171
copybara-service[bot],"Copybara change, may affect some comments in `third_party` BUILD files.","Copybara change, may affect some comments in `third_party` BUILD files.",2025-01-30T00:22:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86170
copybara-service[bot],Enable latency hiding scheduler in pipeline parallelism tests,Enable latency hiding scheduler in pipeline parallelism tests Also enable collective permute cycle decomposition,2025-01-30T00:12:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86169
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-29T23:54:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86168
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-29T23:44:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86167
copybara-service[bot],Introduce `OpaqueExecutable` and conversion functions.,Introduce `OpaqueExecutable` and conversion functions. This patch also updates the remaining tests to use `OpaqueExecutable` correctly.,2025-01-29T23:42:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86166
copybara-service[bot],Advanced version of MatchShapeCoveringDynamicIndexInstruction to handle,Advanced version of MatchShapeCoveringDynamicIndexInstruction to handle   nonunit slice sizes.,2025-01-29T23:37:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86165
mihaimaruseac,Disable creation of issues that don't follow the template,"This is an attempt to eliminate the spam created by bot accounts. If it works here, we can then try on the other repos too.",2025-01-29T23:31:50Z,ready to pull size:XS,closed,0,3,https://github.com/tensorflow/tensorflow/issues/86164,Worth a try given the spam,Added the license part to prevent rollback.,Reopning since the squash also had the rollback and now this also has the copyright header :)
copybara-service[bot],Integrate LLVM at llvm/llvm-project@ab1ee912be95,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match ab1ee912be95,2025-01-29T23:00:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86163
copybara-service[bot],Add some vlogging and code simplifications.,Add some vlogging and code simplifications.,2025-01-29T22:55:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86162
copybara-service[bot],litert: Tune link flags of LiteRT Runtime,litert: Tune link flags of LiteRT Runtime Trim unnecessary symbols of the shared library.,2025-01-29T22:38:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86161
copybara-service[bot],Bump up arm64 Linux and Mac wheel size limits: 240M -> 245M.,Bump up arm64 Linux and Mac wheel size limits: 240M > 245M.,2025-01-29T22:09:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86160
copybara-service[bot],Add an ICYU pragma to silence linters.,Add an ICYU pragma to silence linters.,2025-01-29T22:07:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86159
copybara-service[bot],Move dlinfo wrapper to dynamic loading to simplify compilations dependencies.,Move dlinfo wrapper to dynamic loading to simplify compilations dependencies.,2025-01-29T21:53:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86158
copybara-service[bot],Add some optional verbosity to the dynamic loading lib,Add some optional verbosity to the dynamic loading lib,2025-01-29T21:36:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86157
copybara-service[bot],Mark uses of `-std=c++17` in BUILD files as being a workaround for older Bazel versions.,Mark uses of `std=c++17` in BUILD files as being a workaround for older Bazel versions.,2025-01-29T21:18:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86156
copybara-service[bot],Remove helper methods from tests to make them easier to read.,Remove helper methods from tests to make them easier to read. These helper methods are no longer needed as RunAndCheckHloRewrite can be used directly.,2025-01-29T21:01:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86155
copybara-service[bot],Add debug logging,Add debug logging,2025-01-29T20:23:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86154
copybara-service[bot],Reverts 42898a878b1fd2269a7de1ea22c6cfa1c8914247,Reverts 42898a878b1fd2269a7de1ea22c6cfa1c8914247,2025-01-29T20:06:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86153
copybara-service[bot],PR #22029: [XLA:GPU] Add support for SM101a and SM120a architectures (Blackwell),"PR CC(tf1.10.1 freeze_graph 'list index out of range'): [XLA:GPU] Add support for SM101a and SM120a architectures (Blackwell) Imported from GitHub PR https://github.com/openxla/xla/pull/22029 In addition to SM120a, also add SM101a mentioned in the PTX 8.7 spec (https://docs.nvidia.com/cuda/parallelthreadexecution/releasenotes), which is a slight variation of SM100a. Bumping the max supported PTX version to 8.7, as the LLVM PR (https://github.com/llvm/llvmproject/pull/124155) adding the support is now integrated to OpenXLA. Copybara import of the project:  be59b7a51721637d880207e7adb69a18c3a92bea by Sergey Kozub : [XLA:GPU] Add support for SM101a and SM120a architectures (Blackwell) Merging this change closes CC(tf1.10.1 freeze_graph 'list index out of range') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22029 from openxla:devel/sm120a be59b7a51721637d880207e7adb69a18c3a92bea",2025-01-29T19:44:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86134
copybara-service[bot],[XLA:CPU] Add CPU client support for layout modes.,"[XLA:CPU] Add CPU client support for layout modes. The main motivation for this change is to support userspecified input and output layouts for JAX interoperability with other libraries. For example, https://github.com/jaxml/jax/issues/25066. The logic is moreorless a direct copy of the implementation in `PjRtStreamExecutorClient`.",2025-01-29T19:36:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86124
copybara-service[bot],Use StreamExecutorAllocator instead of DeviceHostAllocator.,Use StreamExecutorAllocator instead of DeviceHostAllocator.,2025-01-29T19:36:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86123
copybara-service[bot],[xla:cpu] Add benchmarks for transpose + copy + dot,[xla:cpu] Add benchmarks for transpose + copy + dot,2025-01-29T19:32:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86113
copybara-service[bot],Reverts 576474666afd9fd7efa409d56edd48027db4b169,Reverts 576474666afd9fd7efa409d56edd48027db4b169,2025-01-29T19:30:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86112
zainZayam,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.," Issue type Others  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version  2.18.0  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version Python version: 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I encountered an error when trying to import TensorFlow. The error message is: ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Multiple times reinstalled it Downgraded my Python, still no sucess.  Standalone code to reproduce the issue ```shell Traceback (most recent call last):   File ""C:\Users\SAM\anaconda3\envs\tf_env\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File """", line 1, in    File ""C:\Users\SAM\anaconda3\envs\tf_env\lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport   File ""C:\Users\SAM\anaconda3\envs\tf_env\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 85, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\SAM\anaconda3\envs\tf_env\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```  Relevant log output ```shell ```",2025-01-29T19:28:01Z,stat:awaiting response type:build/install type:others TF 2.18,closed,0,2,https://github.com/tensorflow/tensorflow/issues/86111,", Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: ```  You need to install the MSVC 2019 redistributable  Your CPU does not support AVX2 instructions  Your CPU/Python is on 32 bits  There is a library that is in a different location/not installed on your system that cannot be loaded. ``` Also kindly provide the environment details and the steps followed to install the tensorflow. CC(Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.) Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584 Thank you!",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Fix tsan/asan error,Fix tsan/asan error,2025-01-29T19:00:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86082
copybara-service[bot],Reverts 8b90c7755e298136842b0a952ace340e5b535d23,Reverts 8b90c7755e298136842b0a952ace340e5b535d23,2025-01-29T18:57:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86081
copybara-service[bot],Clean-up LiteRT Compiler error message.,Cleanup LiteRT Compiler error message. This message is no longer relevant and is misleading in most cases. More than just `variable constant folding` is run in this stage of the converter.,2025-01-29T18:46:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86080
copybara-service[bot],- Run E2E CPU benchmarks and microbenchmarks on Linux x86 CPU and Linux ARM64 CPU, Run E2E CPU benchmarks and microbenchmarks on Linux x86 CPU and Linux ARM64 CPU  Fix relative paths to save results  Disable building and running individual hlos until the build is fixed,2025-01-29T18:42:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86079
copybara-service[bot],[xla:hlo] Add `backend_config` to comparison in `HloComputationDeduplicator` pass.,"[xla:hlo] Add `backend_config` to comparison in `HloComputationDeduplicator` pass. Code deduplicator is ignoring attributes on XLA FFI calls i.e., if two FFI calls are the same aside from their attributes, one will be purged. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/86164 from tensorflow:mihaimaruseacpatch1 d5e7459e51c112b117e52a5d5ec0629ebf384715",2025-01-29T18:40:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86078
copybara-service[bot],Integrate LLVM at llvm/llvm-project@9534d27e3321,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 9534d27e3321,2025-01-29T18:30:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86077
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@48a1e14e,Integrate StableHLO at openxla/stablehlo,2025-01-29T18:21:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86076
rsemihkoca,"Mac Air M2, TfLiteGpuDelegate, Cpp, C++ Building from source"," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.19.0  Custom code No  OS platform and distribution Macbook air m2 Macos Sequia   Mobile device _No response_  Python version 3.12  Bazel version 8.0.1  GCC/compiler version Apple clang version 16.0.0 (clang1600.0.26.6)  CUDA/cuDNN version _No response_  GPU model and memory Appledesigned integrated GPU 8GB  Current behavior? I want to be able to inference using tflite gpu delegate on m2. First of all, I wonder if this is possible. https://www.tensorflow.org/install/source?hl=trinstall_gpu_support_optional_linux_only says tf do not have officail gpu support and apple give us metal plugin. My ultimate goal is to first debug on macbook and then use tflite gpu delegate for inference on iphone 6s. I am writing an sdk for this. But I could not build it for macbook. here is the build script i use for. ```sh bazel clean expunge bazel build config=macos_arm64 \     c opt \     config=nogcp  config=nonccl \     repo_env=HERMETIC_PYTHON_VERSION=3.12 \     define tflite_with_gpu=true \     define tflite_with_metal=true \     cxxopt=std=c++17 \     cxxopt=stdlib=libc++ \     host_cxxopt=std=c++17 \     linkopt=stdlib=libc++ \     copt=std=c++17 \     copt=stdlib=libc++ \     //tensorflow/lite/delegates/gpu:metal_delegate \     //tensorflow/lite:libtensorflowlite.dylib \     //tensorflow/lite/delegates/gpu:metal_delegate.h ``` i have added logs this happend because of  copt=std=c++17 \ but when i remove it then i get ``` ERROR: /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/tensorflow/lite/delegates/gpu/metal/BUILD:68:13: Compiling tensorflow/lite/delegates/gpu/metal/common.mm failed: (Exit 1): wrapped_clang_pp failed: error executing command (from target //tensorflow/lite/delegates/gpu/metal:common) external/local_config_cc/wrapped_clang_pp target arm64applemacosx11.0 'stdlib=libc++' 'std=gnu++11' 'D_FORTIFY_SOURCE=1' fstackprotector fcolordiagnostics Wall Wthreadsafety Wselfassign ... (remaining 50 arguments skipped) In file included from tensorflow/lite/delegates/gpu/metal/common.mm:16: In file included from ./tensorflow/lite/delegates/gpu/metal/common.h:25: In file included from ./tensorflow/lite/delegates/gpu/common/status.h:19: In file included from external/com_google_absl/absl/status/status.h:58: In file included from external/com_google_absl/absl/functional/function_ref.h:53: In file included from external/com_google_absl/absl/base/attributes.h:37: In file included from external/com_google_absl/absl/base/config.h:86: external/com_google_absl/absl/base/policy_checks.h:79:2: error: ""C++ versions less than C++14 are not supported.""    79  185.00 KiB/s, done. remote: Total 9 (delta 7), reused 9 (delta 7), packreused 0 (from 0) From https://github.com/tensorflow/tensorflow  * [new branch]              exported_pr_714978844 > origin/exported_pr_714978844 Already up to date. Initializing and updating TensorFlow submodules... Building TensorFlow Lite for macOS (ARM64)... INFO: Reading 'startup' options from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=88 INFO: Reading rc options for 'clean' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'clean' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:   Inherited 'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Found applicable config definition build:short_logs in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:macos in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: apple_platform_type=macos copt=DGRPC_BAZEL_BUILD features=archive_param_file copt=w define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=no_tfrt INFO: Found applicable config definition build:no_tfrt in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils INFO: Starting clean (this may take a while). Consider using async if the clean takes more than several minutes. Starting local Bazel server and connecting to it... INFO: Reading 'startup' options from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=88 INFO: Reading rc options for 'build' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:   'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Found applicable config definition build:short_logs in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:macos_arm64 in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: cpu=darwin_arm64 macos_minimum_os=11.0 INFO: Found applicable config definition build:nogcp in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: define=no_gcp_support=true INFO: Found applicable config definition build:nonccl in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: define=no_nccl_support=true INFO: Found applicable config definition build:macos in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: apple_platform_type=macos copt=DGRPC_BAZEL_BUILD features=archive_param_file copt=w define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=no_tfrt INFO: Found applicable config definition build:no_tfrt in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils DEBUG: /private/var/tmp/_bazel_rsemihkoca/2f937d813e0974bf03f44421dae0324c/external/local_tsl/third_party/py/python_repo.bzl:87:10:  ============================= Hermetic Python configuration: Version: ""3.12"" Kind: """" Interpreter: ""default"" (provided by rules_python) Requirements_lock label: ""//:requirements_lock_3_12.txt"" ===================================== INFO: Analyzed 3 targets (157 packages loaded, 6973 targets configured). INFO: Found 3 targets... ERROR: /private/var/tmp/_bazel_rsemihkoca/2f937d813e0974bf03f44421dae0324c/external/fft2d/BUILD.bazel:27:11: Compiling fftsg2d.c failed: (Exit 1): wrapped_clang failed: error executing command (from target //:fft2d) external/local_config_cc/wrapped_clang 'D_FORTIFY_SOURCE=1' fstackprotector fcolordiagnostics Wall Wthreadsafety Wselfassign fnoomitframepointer g0 O2 DNDEBUG 'DNS_BLOCK_ASSERTIONS=1' ... (remaining 31 arguments skipped) error: invalid argument 'std=c++17' not allowed with 'C' Error in child process '/usr/bin/xcrun'. 1 INFO: Elapsed time: 69.199s, Critical Path: 2.88s INFO: 316 processes: 296 internal, 20 local. FAILED: Build did NOT complete successfully Locating macOS build output... INFO: Reading 'startup' options from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'info' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'info' from /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc:   Inherited 'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Found applicable config definition build:short_logs in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:macos in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: apple_platform_type=macos copt=DGRPC_BAZEL_BUILD features=archive_param_file copt=w define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=no_tfrt INFO: Found applicable config definition build:no_tfrt in file /Users/rsemihkoca/Projects/FaceBlurCameraSDK/tensorflow/.bazelrc: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils DEBUG: /private/var/tmp/_bazel_rsemihkoca/2f937d813e0974bf03f44421dae0324c/external/local_tsl/third_party/py/python_repo.bzl:87:10:  ============================= Hermetic Python configuration: Version: ""3.12"" Kind: """" Interpreter: ""default"" (provided by rules_python) Requirements_lock label: ""//:requirements_lock_3_12.txt"" ===================================== Could not find macOS build output. Searching for alternatives... /private/var/tmp/_bazel_rsemihkoca/2f937d813e0974bf03f44421dae0324c/execroot/org_tensorflow/bazelout/darwin_arm64opt/bin/tensorflow/lite/libtensorflowlite.dylib.runfiles/org_tensorflow/tensorflow/lite/libtensorflowlite.dylib Error: Failed to locate macOS build output. ```",2025-01-29T18:21:23Z,stat:awaiting response type:support stale comp:lite TF 2.18,closed,0,8,https://github.com/tensorflow/tensorflow/issues/86075, ,"Hi,   I apologize for the delayed response, As far I know it is possible to use the TFLite GPU delegate on macOS with M2 chip.  You are correct that TensorFlow itself doesn't have official GPU support in the same way it does on Linux with CUDA.  However, Apple provides the Metal framework and the `tensorflowmetal` plugin (if needed building from source) allows TensorFlow Lite to leverage Metal for GPU acceleration please refer this official documentation of Get started with tensorflowmetal so I believe you're using `tensorflowmetal `plugin Could you please give it try with below command and see is it working as expected or not ? if not please help me with complete steps which you followed before encountering the mentioned errors to investigate this issue further from our end ? ``` bazel clean expunge bazel build config=macos_arm64 \     c opt \     config=nogcp config=nonccl \     repo_env=HERMETIC_PYTHON_VERSION=3.12 \   Or your preferred Python version     cxxopt=""std=c++17"" \     host_cxxopt=""std=c++17"" \     linkopt=""stdlib=libc++"" \  Only if you get linker errors related to stdlib     //tensorflow/lite:libtensorflowlite.dylib ``` Thank you for your cooperation and patience.","According to the file below: ``` //tensorflow/lite/delegates/gpu:tensorflow_lite_gpu_framework \ ``` needs to be added. However, when I add it this way, it is not sufficient. I get undefined symbols error but I understand that I also need to install `tensorflowmetal`. Is that correct? [TensorFlow GitHub  BUILD file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/BUILD) ```bazel  bazel build c opt cpu ios_arm64 copt Os copt DTFLITE_GPU_BINARY_RELEASE copt fvisibility=hidden linkopt s strip always cxxopt=std=c++14 :libtensorflowlite_gpu_metal apple_platform_type=ios ios_static_framework(     name = ""tensorflow_lite_gpu_framework"",     hdrs = [         ""metal_delegate.h"",         ""metal_delegate_internal.h"",     ],     minimum_os_version = ""12.0"",     deps = ["":metal_delegate""], ) ```",also we want to use with cpp but tensorflowmetal is only available for python. in python package there is   inflating: tensorflowplugins/libmetal_plugin.dylib   can we use that or will that be enough in order to use gpu on macos ?,"Hi,   I apologize for the delay in my response, Just to confirm by using below bazel command your build completed successfully or not ? No, you do not need to install the `tensorflowmetal` Python package for your C++ TensorFlow Lite build. Your understanding that it's only for Python is correct in this context. As far I know `tensorflow_lite_gpu_framework` target for` iOS` not for `macos_arm64` ``` 1. Clean previous attempts bazel clean expunge 2. Build TFLite core and Metal delegate for macOS ARM64 bazel build config=macos_arm64 \     c opt \     config=nogcp config=nonccl \     repo_env=HERMETIC_PYTHON_VERSION=3.12 \     define tflite_with_metal=true \     cxxopt=std=c++17 \     host_cxxopt=std=c++17 \     linkopt=stdlib=libc++ \     //tensorflow/lite:libtensorflowlite.dylib \     //tensorflow/lite/delegates/gpu:metal_delegate ``` If above command completes successfully you should find the necessary library files inside your **bazelbin** directory (the exact path will be shown in the build output often under **bazelbin/tensorflow/lite/** and **bazelbin/tensorflow/lite/delegates/gpu**). Look for `libtensorflowlite.dylib` (The core TFLite library) a library for the Metal delegate. Its name might be `metal_delegate.dylib`, `libmetal_delegate.dylib` or similar. You need to explicitly tell the linker to include both `libtensorflowlite.dylib` and the Metal delegate library when building your final `libFaceBlurCameraSDK.dylib` depends on how you are building your SDK If I have missed something here please let me know. Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Handle the cases when mask is not presented.,"Handle the cases when mask is not presented. When tflite is running on GPU, mask might cause some expensive operations like reshape. When mask is not used, allzeros mask have to be passed because xnnpack expects mask is always presented. This CL is for xnnpack delegate to handle the case when mask is not presented, and/so tflite can omit mask when it's not used both for xnnpack delegate and other gpu delegates like ML drift.",2025-01-29T18:20:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86074
copybara-service[bot],[JAX][StableHLO] Migrate JAX to use StableHLO custom call with dictionary,[JAX][StableHLO] Migrate JAX to use StableHLO custom call with dictionary,2025-01-29T18:20:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86073
copybara-service[bot],PR #21746: [NVIDIA GPU] Add collective-permute combiner,"PR CC(Adding op doc fixes): [NVIDIA GPU] Add collectivepermute combiner Imported from GitHub PR https://github.com/openxla/xla/pull/21746 For collectivepermutes with small message sizes, it is beneficial to combine them into a single collective because 1. it gets rid of some kernel launch overhead, and allows NCCL to do some message fusion; 2. fewer collectives make it easier for LHS to make better decision. On top of the multioperand collectivepermute added in https://github.com/openxla/xla/pull/18838, this PR adds a combiner for collectivepermutes. Copybara import of the project:  c03a8fb5bd42cf3a365e1684537e78544a75a937 by Terry Sun : add collective permute combiner  6a3159e89444ea342c25d8d996c994accd68a30d by Terry Sun : polishing and doc string updates Merging this change closes CC(Adding op doc fixes) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21746 from terryysun:terryysun/combine_collective_permute 9de30a2ee252cf546ebda371e3b6aec852b6167d",2025-01-29T18:10:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86072
copybara-service[bot],Avoid operating on aborted NCCL communicator.,Avoid operating on aborted NCCL communicator.,2025-01-29T18:09:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86071
copybara-service[bot],PR #21825: Exclude the usage of CPU memory from the GPU memory scheduler,"PR CC(TensorFlow lite android example simply does not sync or build.): Exclude the usage of CPU memory from the GPU memory scheduler Imported from GitHub PR https://github.com/openxla/xla/pull/21825 In the MaxText optimizer state offloading, we observed no memory savings when switching from f16 to f32. The root cause is that the GPU memory scheduler does not distinguish between CPU memory and GPU memory. This commit modifies the scheduler to exclude CPU memory. Copybara import of the project:  c77eefa1b4e31724dbfa40f4ab2aa7aff16e0840 by Jane Liu : Exclude the usage of CPU memory from the GPU memory scheduler  1c5720711c6a7d8173e132922b67eee6e2e8b9dd by Jane Liu : Add the explicit return type for the closure Merging this change closes CC(TensorFlow lite android example simply does not sync or build.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21825 from zhenyingliu:scheduler 1c5720711c6a7d8173e132922b67eee6e2e8b9dd",2025-01-29T18:09:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86070
copybara-service[bot],[xla:cpu] Add XnnConvolutionThunk,[xla:cpu] Add XnnConvolutionThunk,2025-01-29T17:55:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86069
copybara-service[bot],Add an ICYU pragma to silence linters.,Add an ICYU pragma to silence linters.,2025-01-29T17:52:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86068
copybara-service[bot],[xla:cpu] Move convolution shape verification and canonical dims computation to the shared library,[xla:cpu] Move convolution shape verification and canonical dims computation to the shared library,2025-01-29T17:50:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86067
copybara-service[bot],[XLA:CPU] Make loop unrolling on by default in IrCompiler.,[XLA:CPU] Make loop unrolling on by default in IrCompiler.,2025-01-29T17:35:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86066
copybara-service[bot],Rolling integrate Triton up to [515467a9](https://github.com/openai/triton/commits/515467a9b42b92f24af988b83b7de6c8910a0a69) forward,Rolling integrate Triton up to 515467a9 forward Reverts eeaa53430b85adf40faa9086683fc420bc79c7ca,2025-01-29T17:28:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86065
copybara-service[bot],[Checkpoint] Add units (microseconds) to duration log message.,[Checkpoint] Add units (microseconds) to duration log message.,2025-01-29T17:12:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86064
copybara-service[bot],[XLA:CPU] Add execution engine and remove AddObjFile from JITCompiler,[XLA:CPU] Add execution engine and remove AddObjFile from JITCompiler Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-29T16:53:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86063
copybara-service[bot],PR #21901: Ci add rocm6.1 deps for ubuntu 20.04,PR CC(Add gradient for tf.broadcast_to): Ci add rocm6.1 deps for ubuntu 20.04 Imported from GitHub PR https://github.com/openxla/xla/pull/21901 Add rocm 6.1.0 dependency for ubuntu 20.04 Copybara import of the project:  0acf028eeca5923c7f2aa5762297686836eda310 by Alexandros Theodoridis : Add rocm6.1 deps for ubuntu 20.04  fc88c83061d6efff2482599489d622ab3114b9a7 by Alexandros Theodoridis : Fix hermetic build for 6.0  73ace5591f4731e1b95b6d3e6a349b528977c580 by Alexandros Theodoridis : Add ci config for hermetic build  bbc048bcffd9d35bfad76ff816ed22f3e3f761f8 by Alexandros Theodoridis : Introduce rocm 6.1.0 dependency for 22.04  9776f398c2711ba37333d29b934d6ba67c55dbef by Alexandros Theodoridis : Add missing 24.04 redist  acf275d57cc185b9c2122d5930d8cf54e473ad95 by Alexandros Theodoridis : Fix test  3e49285b0f55597ab5f44c1d0a422bf931d72cda by Alexandros Theodoridis : Add comment explaining the reason for a new target  35838bf8d6e678717e9b1c551f840918b00a91f8 by Alexandros Theodoridis : Rever force verbose in the compiler wrapper  2952e115b044e1a8ac8aadc7eac7802e8d79cf91 by Alexandros Theodoridis : Add explanation comment for the new target Merging this change closes CC(Add gradient for tf.broadcast_to) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21901 from ROCm:ci_add_rocm6.1_deps_for_ubuntu_20.04 2952e115b044e1a8ac8aadc7eac7802e8d79cf91,2025-01-29T16:43:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86062
copybara-service[bot],Integrate LLVM at llvm/llvm-project@9534d27e3321,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 9534d27e3321,2025-01-29T15:03:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86061
copybara-service[bot],litert builddefs updates.,litert builddefs updates.,2025-01-29T15:02:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86060
copybara-service[bot],[xla:gpu] simplify sparsity patches,"[xla:gpu] simplify sparsity patches As discussed before, we can move the sparsity patches to exist in the openxla/triton commit rather than in patches. This simplifies where all the code lives :).",2025-01-29T15:02:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86059
copybara-service[bot],[XLA:GPU] Allow AUTO layout in the multihost runner.,[XLA:GPU] Allow AUTO layout in the multihost runner.,2025-01-29T14:33:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86058
copybara-service[bot],[pjrt] `PjRtStreamExecutorDevice` now has a default memory space,[pjrt] `PjRtStreamExecutorDevice` now has a default memory space The space is set via `AttachMemorySpace` with `is_default` set to true.,2025-01-29T14:29:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86057
copybara-service[bot],[pjrt] `PjRtStreamExecutorClient` now accepts owned memory spaces,[pjrt] `PjRtStreamExecutorClient` now accepts owned memory spaces,2025-01-29T14:25:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86056
copybara-service[bot],Reverts 9dcbe84fd12c1a2c48925dfd5be954fad6d9c864,Reverts 9dcbe84fd12c1a2c48925dfd5be954fad6d9c864,2025-01-29T14:04:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86055
copybara-service[bot],Update XNNPack to the lastest version (and dependencies).,Update XNNPack to the lastest version (and dependencies).,2025-01-29T14:02:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86054
copybara-service[bot],[xla:gpu] [cleanup] Pull out some logic into IterableInput,"[xla:gpu] [cleanup] Pull out some logic into IterableInput This both simplifies the giant EmitMatmul function & makes it more generic, simplifying the TMA change (see CL chain).",2025-01-29T13:47:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86053
copybara-service[bot],[XLA:GPU] Add dependency and build configurations for NVHSMEM.,[XLA:GPU] Add dependency and build configurations for NVHSMEM.,2025-01-29T13:13:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86052
copybara-service[bot],Integrate LLVM at llvm/llvm-project@a06c89387621,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match a06c89387621,2025-01-29T12:27:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86051
copybara-service[bot],Integrate LLVM at llvm/llvm-project@d0052ebbe2e2,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match d0052ebbe2e2,2025-01-29T12:16:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86050
copybara-service[bot],PR #21948: [GPU] Upgrade cuDNN frontend to 1.10.0.,PR CC('): [GPU] Upgrade cuDNN frontend to 1.10.0. Imported from GitHub PR https://github.com/openxla/xla/pull/21948 Copybara import of the project:  affa734c3c6e2af934dd12eafe7e8771ab0ee8db by Ilia Sergachev : [GPU] Upgrade cuDNN frontend to 1.10.0. Merging this change closes CC(') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21948 from openxla:cudnn_fe_1100 affa734c3c6e2af934dd12eafe7e8771ab0ee8db,2025-01-29T12:12:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86049
space2,Buffer allocation error in Tensorflow Lite with OpenCL backend on certain platforms," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.13  Custom code No  OS platform and distribution macOS  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I noticed a memory allocation error in clCreateBuffer. The issue seems to be caused by this: 1) TFlite tries to alloca 0xa2000000 bytes of memory (value stored as size_t) 2) The call ends up in this function (tensorflow/lite/experimental/litert/runtime/opencl/buffer.cc): ``` absl::Status CreateClBuffer(cl_context context, int size_in_bytes,                             bool read_only, void* data, cl_mem* result)  ``` where the size is now int (i.e. 32 bit signed integer... so 0xa2000000 is interpreted as a negative value). 3) This function then calls clCreateBuffer, which takes the size argument as size_t again, and thus receives 0xffffffffa2000000, i.e. the signed 32 bit integer first sign extended to 64bit and then interpreted as unsigned, and thus resulting in a huge size. The issue doesn't seem to appear with the same model on Android, probably because: The max buffer allocation size on macOS (M1) seems to be 9GB (according to clinfo), but on that android device it's only 1GB (so on android tflite never tries to allocate such a huge chunk of memory).  Standalone code to reproduce the issue ```shell Unfortunately I'm not allowed to share the code/model, but looking at the function signatures one can see the issue. ```  Relevant log output ```shell ERROR: Failed to allocate device memory (clCreateBuffer): Invalid buffer size ```",2025-01-29T12:11:28Z,stat:awaiting response type:bug comp:lite TF 2.13,closed,0,8,https://github.com/tensorflow/tensorflow/issues/86048,"Hi,   Please take a look into this issue. Thank you.","Hi , have you tried upgrading/updating to LiteRT? 2.13 is quite old at this point and there is a high chance your issue is already resolved by updating to the latest version."," I haven't tried upgrading, because unfortunately in our current project that's a bit complicated. But looking at the master branch on github, in file tensorflow/lite/experimental/litert/runtime/opencl/buffer.cc:31 in function CreateClBuffer the size_in_bytes argument is int, so there will be issues with >2GB allocations. In tag v2.18.0 the code seems to be moved to tensorflow/lite/delegates/gpu/cl/util.cc:180, but the argument type is still int.",I believe this PR will fix your issue: https://github.com/tensorflow/tensorflow/pull/87584. Thanks for your help.,"Hi , the change is now merged, please feel free to test and let us know if it works.","Thank you! Will take a while to test it (we need to rebase tflite, but due to local changes that is complicated). But the fix looks good, so I think the issue can be closed."," thanks for the confirmation, please feel free to close if there is no open items left as we generally cannot close the issues ourselves. Thanks.",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Only look when looking up or inserting into the file stat cache,Only look when looking up or inserting into the file stat cache This allows better performance for latency sensitive workload that require multithreading. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-29T12:05:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86047
copybara-service[bot],Reverts 3bf73c73e338b45a5e849808e53c2494508d415a,Reverts 3bf73c73e338b45a5e849808e53c2494508d415a,2025-01-29T12:01:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86046
enthusiast666,Building static c library for xtensa DSP processors, Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.8  Custom code No  OS platform and distribution ubuntu 20.04  Mobile device ubuntu 20.04  Python version 3.10.10  Bazel version 3.1.0  GCC/compiler version 9.4.0  CUDA/cuDNN version   GPU model and memory   Current behavior? could someone suggest on how to build c libraries(.so or .a) for xtensa based processors or DSPs for linking with the applications ??  Standalone code to reproduce the issue ```shell compiler xtensacore=xyz Wall pedantic Werror Wextra Wnounusedparameter *.c L xtensaspecificlibs I /tensorflow/third_party/xla/ I /tensorflow/ I /tensorflow/third_party/xla/third_party/tsl/ nostdlib I /tensorflow/lite/ I /tensorflow/lite/core/api/ I /tensorflow/ lc lxmem ```  Relevant log output ```shell : dangerous relocation: cannot decode instruction opcode when building library.  undefined reference to `TF_NewGraph'... when try running the application without tf library ```,2025-01-29T11:46:06Z,stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/86045,"Hi,   I apologize for the delayed response, I understand you're looking for a concrete example, tutorial or Github Repo.  Unfortunately, complete publicly available Github repo of highly optimized DSP kernels for Xtensa HiFi processors integrated with TFLM are rare. This type of code is often considered proprietary and a key competitive advantage. However, you can refer this blog : Accelerating TensorFlow Lite Micro on Cadence Audio Digital Signal Processors and this example. As you've already created a corresponding issue in the tensorflow/tflitemicro repository (https://github.com/tensorflow/tflitemicro/issues/3045) which is the most appropriate repo for this type of query, we recommend closing this issue here and continuing the discussion there. This will help consolidate the discussion and ensure your query reaches the appropriate experts within the tensorflow/tflitemicro repository, where you are most likely to receive further assistance. Please refer this https://github.com/tensorflow/tflitemicro/issues/3045issuecomment2699709459 if you've any further questions please feel free to post in this issue thread https://github.com/tensorflow/tflitemicro/issues/3045 Thank you for your understanding and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
AD-lite24,Tensorflow lite cross compilation to aarch64 failing," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.1  Custom code No  OS platform and distribution Host: x86 Ubuntu 20.04  Mobile device target: aarch64  Python version N/A  Bazel version N/A  GCC/compiler version gcc 9  CUDA/cuDNN version N/A  GPU model and memory N/A  Current behavior? Build fails due to an exec format error with `protoc` ``` [ 97%] Built target protoc /usr/bin/make  f profiling/proto/CMakeFiles/profiling_info_proto.dir/build.make profiling/proto/CMakeFiles/profiling_info_proto.dir/depend make[2]: Entering directory '/home/root/build64' [ 98%] Generating profiling_info.pb.cc, profiling_info.pb.h cd /home/root/build64/profiling/proto && protoc cpp_out=/home/root/build64/profiling/proto proto_path=/home/root/tensorflow/tensorflow/lite/profiling/proto /home/root/tensorflow/tensorflow/lite/profiling/proto/profiling_info.proto /bin/sh: 1: protoc: Exec format error make[2]: *** [profiling/proto/CMakeFiles/profiling_info_proto.dir/build.make:75: profiling/proto/profiling_info.pb.cc] Error 2 make[2]: Leaving directory '/home/root/build64' make[1]: *** [CMakeFiles/Makefile2:7385: profiling/proto/CMakeFiles/profiling_info_proto.dir/all] Error 2 make[1]: Leaving directory '/home/root/build64' make: *** [Makefile:136: all] Error 2 ``` The only possible explanation for this seems to be that that the cmake build process is attempting to use `protoc` for this step  ``` [ 98%] Generating profiling_info.pb.cc, profiling_info.pb.h cd /home/root/build64/profiling/proto && protoc cpp_out=/home/root/build64/profiling/proto proto_path=/home/root/tensorflow/tensorflow/lite/profiling/proto /home/root/tensorflow/tensorflow/lite/profiling/proto/profiling_info.proto ``` But since `protoc` was built using the cross compiler tool chain it is meant for aarch64 while my host machine is trying to run it. This is likely a bug with the cross compilation process and in that case, please suggest a fix. I am not sure to what extent `protoc` is used in the build so any fix I would make cannot be completely correct.  Edit: I installed the x86 version for `protoc` separately, specifically the version 3.21.x which is the exact version that the tflite build process creates (3.21.9) and I am still facing version incompatibility issues  ``` /home/root/build64/profiling/proto/profiling_info.pb.h:17:2: error: error This file was generated by an older version of protoc which is    17   ^~~~~ ```  Standalone code to reproduce the issue ```shell `cmake DCMAKE_TOOLCHAIN_FILE=/opt/cross_toolchain/aarch64gnu9.toolchain.cmake DTFLITE_ENABLE_GPU=ON DTFLITE_ENABLE_NNAPI=ON DXNNPACK_ENABLE_ARM_BF16=OFF DXNNPACK_ENABLE_ARM_I8MM=OFF DCMAKE_CXX_FL<CXX_FLAGS=""${CMAKE_CXX_FLAGS} std=c++11"" ../tensorflow/tensorflow/lite` Using the default Cmake build process. ```  Relevant log output ```shell ```",2025-01-29T11:42:26Z,type:build/install comp:lite subtype: ubuntu/linux 2.17,closed,0,2,https://github.com/tensorflow/tensorflow/issues/86044,"Figured it out. Apparently protobuf 3.21.12 is significantly different from 3.21.9 and there is no release for 3.21.9 so need to build it from source. Still there is a bug with with the cross compilation process that should be resolved. I will try to create a PR if I end up writing a seamless solution, but for now the workaround of manually building protobuf works. Closing the issue for now but the issue has not been resolved.",Are you satisfied with the resolution of your issue? Yes No
DamarXCV,Input pipeline with RaggedTensor no longer working in +2.16 - No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.1 LTS  Mobile device _No response_  Python version 3.11 & 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? In version 2.15 and earlier i was able to have a batched `tf.data.Dataset` with `tf.RaggedTensor` as input for `model.fit` and `model.predict`, with `tf.keras.layers.Resizing` as the first layer of the model. This is no longer works in +2.16 and Keras 3 (Edit: not Keras 2). The error log contains `RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible`, which implies a missing implementation.  Standalone code to reproduce the issue ```shell import tensorflow as tf ds = tf.data.Dataset.from_tensor_slices(range(10)) \     .map(lambda x: (         tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),         0,     )) \     .batch(batch_size=4) \     .prefetch(tf.data.AUTOTUNE) model = tf.keras.Sequential([     tf.keras.layers.Resizing(height=10, width=10),     tf.keras.layers.GlobalMaxPool2D(),     tf.keras.layers.Softmax(), ]) model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy()) model.fit(ds) ```  Relevant log output ```shell I0000 00:00:1738149672.657535   10590 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7070 MB memory:  > device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:0b:00.0, compute capability: 6.1 [...]/.venv/lib/python3.12/sitepackages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis 1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?   warnings.warn( [...]/.venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py:82: UserWarning: The model does not have any trainable weights.   warnings.warn(""The model does not have any trainable weights."") WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1738149672.931281   10665 service.cc:148] XLA service 0x79b50c0038a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: I0000 00:00:1738149672.931302   10665 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1 20250129 12:21:12.940706: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at if_op.cc:291 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[] on XLA_GPU_JIT: RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible with node {{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}){{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}} The op is created at:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 	tf2xla conversion failed while converting sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. Stack trace for op definition:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 20250129 12:21:12.940972: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[] on XLA_GPU_JIT: RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible with node {{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}){{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}} The op is created at:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 	tf2xla conversion failed while converting sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. Stack trace for op definition:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 	 [[sequential_1/resizing_1/RaggedResizeImages/cond]] 	tf2xla conversion failed while converting __inference_one_step_on_data_290[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. 20250129 12:21:12.941007: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[] on XLA_GPU_JIT: RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible with node {{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}){{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}} The op is created at:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 	tf2xla conversion failed while converting sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. Stack trace for op definition:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 	 [[sequential_1/resizing_1/RaggedResizeImages/cond]] 	tf2xla conversion failed while converting __inference_one_step_on_data_290[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. 	 [[StatefulPartitionedCall]] Traceback (most recent call last):   File ""[...]/test.py"", line 43, in      model.fit(ds)   File ""[...]/.venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""[...]/.venv/lib/python3.12/sitepackages/tensorflow/python/eager/execute.py"", line 53, in quick_execute     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error: Detected at node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant defined at (most recent call last):  Detected at node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant defined at (most recent call last):  Detected unsupported operations when trying to compile graph sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[] on XLA_GPU_JIT: RaggedTensorToVariant (No registered 'RaggedTensorToVariant' OpKernel for XLA_GPU_JIT devices compatible with node {{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}}){{node sequential_1/resizing_1/RaggedResizeImages/cond/map/RaggedToVariant/RaggedTensorToVariant}} The op is created at:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 	tf2xla conversion failed while converting sequential_1_resizing_1_RaggedResizeImages_cond_false_141_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. Stack trace for op definition:  File ""test.py"", line 43, in  File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 219, in function File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/trainer.py"", line 57, in train_step File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/models/sequential.py"", line 213, in call File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 182, in call File "".venv/lib/python3.12/sitepackages/keras/src/ops/function.py"", line 171, in _run_through_graph File "".venv/lib/python3.12/sitepackages/keras/src/models/functional.py"", line 637, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/tf_data_layer.py"", line 43, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/layer.py"", line 908, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 117, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/ops/operation.py"", line 46, in __call__ File "".venv/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 156, in error_handler File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/base_image_preprocessing_layer.py"", line 208, in call File "".venv/lib/python3.12/sitepackages/keras/src/layers/preprocessing/image_preprocessing/resizing.py"", line 103, in transform_images File "".venv/lib/python3.12/sitepackages/keras/src/backend/tensorflow/image.py"", line 293, in resize 	 [[sequential_1/resizing_1/RaggedResizeImages/cond]] 	tf2xla conversion failed while converting __inference_one_step_on_data_290[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and vmodule=xla_compiler=2 to obtain a dump of the compiled functions. 	 [[StatefulPartitionedCall]] [Op:__inference_multi_step_on_iterator_298] ```",2025-01-29T11:40:28Z,type:bug comp:keras TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/86043,"Hi **** , Apologies for the delay, and thank you for raising your concern here. The main cause of your issue is the Keras installation. Starting from TensorFlow version 2.16.0, it defaults to Keras 3. If you want to use Keras 2, you need to install it manually. This is also mentioned in the documentation. I installed everything as required, and it is working fine for me. Here, I am providing a gist for your reference. Thank you!"," Thank you for your reply. Sorry, i meant Keras 3 not Keras 2, i corrected it in my original post. If i console log the version it prints `3.8.0` for Keras with the following code ``` import tensorflow as tf print(tf.__version__) print(tf.keras.__version__) ds = tf.data.Dataset.from_tensor_slices(range(10)) \     .map(lambda x: (         tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),         0,     )) \     .batch(batch_size=4) \     .prefetch(tf.data.AUTOTUNE) model = tf.keras.Sequential([     tf.keras.layers.Resizing(height=10, width=10),     tf.keras.layers.GlobalMaxPool2D(),     tf.keras.layers.Softmax(), ]) model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy()) model.fit(ds) ``` I know i should use the standalone keras import, as documented in the migration guide, but even i do so it still crashes with the error from my initial post ``` import keras import tensorflow as tf print(tf.__version__) print(keras.__version__) ds = tf.data.Dataset.from_tensor_slices(range(10)) \     .map(lambda x: (         tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),         0,     )) \     .batch(batch_size=4) \     .prefetch(tf.data.AUTOTUNE) model = keras.Sequential([     keras.layers.Resizing(height=10, width=10),     keras.layers.GlobalMaxPool2D(),     keras.layers.Softmax(), ]) model.compile(loss=keras.losses.SparseCategoricalCrossentropy()) model.fit(ds) ``` The printed versions are in both code snipets `2.18.0` and `3.8.0`. If i install tfkeras (aka Keras 2) with `pip install tfkeras` the following code works ``` import tf_keras import tensorflow as tf print(tf.__version__) print(tf_keras.__version__) ds = tf.data.Dataset.from_tensor_slices(range(10)) \     .map(lambda x: (         tf.RaggedTensor.from_tensor(tf.zeros((x + 1, x + 1, 1))),         0,     )) \     .batch(batch_size=4) \     .prefetch(tf.data.AUTOTUNE) model = tf_keras.Sequential([     tf_keras.layers.Resizing(height=10, width=10),     tf_keras.layers.GlobalMaxPool2D(),     tf_keras.layers.Softmax(), ]) model.compile(loss=tf_keras.losses.SparseCategoricalCrossentropy()) model.fit(ds) ``` But i would like to use Keras 3 and not the outdated Keras 2, which seems to not support `tf.RaggedTensor` as input for `keras.layers.Resizing`",I found kerasteam/keras CC(Add missing semicolon) which mentions  > No RaggedTensor support. We may add it back later. I guess that means that my input pipeline is not supported in Keras 3 for now.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[XLA:GPU] Do not reset AUTO layouts to defaults when parsing text HLO.,[XLA:GPU] Do not reset AUTO layouts to defaults when parsing text HLO. Enabling this by default.,2025-01-29T11:37:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86042
oscar066,Title: ValueError when adding TensorFlow Hub KerasLayer to Sequential model," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.1  Custom code No  OS platform and distribution Google colab  Mobile device _No response_  Python version Python 3.11.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When attempting to add hub.KerasLayer to a tf.keras.Sequential model, TensorFlow raises a ValueError, stating that only instances of keras.Layer can be added. However, hub.KerasLayer is a subclass of keras.Layer, so this behavior seems unexpected. I expected hub.KerasLayer to be accepted as a valid layer in the tf.keras.Sequential model, as per the TensorFlow documentation.  Standalone code to reproduce the issue ```shell import tensorflow as tf import tensorflow_hub as hub mobilenet_v2 = ""https://tfhub.dev/google/tf2preview/mobilenet_v2/classification/4"" inception_v3 = ""https://tfhub.dev/google/imagenet/inception_v3/classification/5"" classifier_model = mobilenet_v2    [""mobilenet_v2"", ""inception_v3""] {type:""raw""} IMAGE_SHAPE = (224, 224) classifier = tf.keras.Sequential([     hub.KerasLayer(classifier_model, input_shape=IMAGE_SHAPE + (3,)) ]) link to notebook: ""https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning_with_hub.ipynb"" ```  Relevant log output ```shell  ValueError                                Traceback (most recent call last)  in ()       1 IMAGE_SHAPE = (224, 224)       2  > 3 classifier = tf.keras.Sequential([       4     hub.KerasLayer(classifier_model, input_shape=IMAGE_SHAPE+(3,))       5 ]) 1 frames /usr/local/lib/python3.11/distpackages/keras/src/models/sequential.py in add(self, layer, rebuild)      94                 layer = origin_layer      95         if not isinstance(layer, Layer): > 96             raise ValueError(      97                 ""Only instances of `keras.Layer` can be ""      98                 f""added to a Sequential model. Received: {layer} "" ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received:  (of type ) ```",2025-01-29T11:01:56Z,type:bug comp:keras 2.17,closed,0,7,https://github.com/tensorflow/tensorflow/issues/86041,", Hi, By default the colab notebook is using tensorflow v2.17 which contains keras3.0 which was causing the error. Could you please try to import keras2.0 with the below commands. ```python !pip install tfkeras import tf_keras as keras ``` Also I have changed some steps like  modifying **tf_keras/keras.Sequential** instead of **tf.keras.Sequential** and the code was executed without error/fail. Kindly find the gist of it here. Thank you!","Thank you. On Thu, 30 Jan 2025, 3:26 pm Tilak, ***@***.***> wrote: >  , > 48 , > Hi, By default the colab notebook is using tensorflow v2.17 which contains > keras3.0 which was causing the error. Could you please try to import > keras2.0 with the below commands. > > !pip install tfkeras > import tf_keras as keras > > Also I have changed some steps like modifying *tf_keras/keras.Sequential* > instead of *tf.keras.Sequential* and the code was executed without > error/fail. Kindly find the gist of it here >  > . > > Thank you! > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","The solution where I installed tf_keras worked for that section, but I’m encountering a similar error in the ""Attach a classification head"" section of the same notebook. However, the previous solution does not seem to work in this case. Here is the code block : feature_extractor_layer = hub.KerasLayer(     feature_extractor_model,     input_shape=(224, 224, 3),     trainable=False) num_classes = len(class_names) model = tf.keras.Sequential([   feature_extractor_layer,   tf.keras.layers.Dense(num_classes) ]) model.summary() the error :  ValueError                                Traceback (most recent call last) [](https://localhost:8080/) in ()       1 num_classes = len(class_names)       2  > 3 model = tf.keras.Sequential(       4   feature_extractor_layer,       5   tf.keras.layers.Dense(num_classes) 1 frames [/usr/local/lib/python3.11/distpackages/keras/src/models/sequential.py in add(self, layer, rebuild)      95                 layer = origin_layer      96         if not isinstance(layer, Layer): > 97             raise ValueError(      98                 ""Only instances of `keras.Layer` can be ""      99                 f""added to a Sequential model. Received: {layer} "" ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received:  (of type ) Error after trying using tf_keras:  TypeError                                 Traceback (most recent call last) [](https://localhost:8080/) in ()       1 num_classes = len(class_names)       2  > 3 model = keras.Sequential(       4   feature_extractor_layer,       5   tf.keras.layers.Dense(num_classes) 2 frames [/usr/local/lib/python3.11/distpackages/tf_keras/src/engine/sequential.py in add(self, layer)     175                 layer = functional.ModuleWrapper(layer)     176         else: > 177             raise TypeError(     178                 ""The added layer must be an instance of class Layer. ""     179                 f""Received: layer={layer} of type {type(layer)}."" TypeError: The added layer must be an instance of class Layer. Received: layer= of type . Link to notebook: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning_with_hub.ipynb",", Could you try to modify the tf.keras to keras and execute the code.  I have changed some steps like modifying tf_keras/keras.Sequential instead of tf.keras.Sequential and the code was executed without error/fail. Kindly find the gist of it here. Thank you!","the gist notebook executed successfully however am still getting the error on this machine : here is the code : (raw_train, raw_validation, raw_test) , metadata = tfds.load(     'cats_vs_dogs',     split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],     with_info=True,     as_supervised=True, ) num_examples = metadata.splits['train'].num_examples num_classes = metadata.features['label'].num_classes print(num_examples) print(num_classes) BATCH_SIZE = 32 train_batches = raw_train.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE) validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1) test_batches = raw_test.map(format_image).batch(1) for  image_batch, label_batch in train_batches.take(1):     pass image_batch.shape module_selection = (""mobilenet_v2_100_224"", 224, 1280)   Updated module name handle_base, pixels, FV_SIZE = module_selection  Use the correct nonpreview URL MODULE_HANDLE = ""https://tfhub.dev/google/tf2preview/mobilenet_v2/feature_vector/4"".format(handle_base) IMAGE_SIZE = (pixels, pixels) feature_extractor = hub.KerasLayer(     MODULE_HANDLE,     input_shape=(224, 224, 3),     trainable=False ) model = keras.Sequential([     feature_extractor,     tf.keras.layers.Dense(num_classes, activation='softmax') ]) and here is the error:  TypeError                                 Traceback (most recent call last) Cell In[5], line 1 > 1 model = keras.Sequential(       [2     feature_extractor,       3     tf.keras.layers.Dense(num_classes, activation='softmax')       4 ])       6 model.summary() File /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/sitepackages/tensorflow/python/trackable/base.py:204, in no_automatic_dependency_tracking.._method_wrapper(self, *args, **kwargs)     202 self._self_setattr_tracking = False   pylint: disable=protectedaccess     203 try: > 204   result = method(self, *args, **kwargs)     205 finally:     206   self._self_setattr_tracking = previous_value   pylint: disable=protectedaccess File /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/sitepackages/tf_keras/src/utils/traceback_utils.py:70, in filter_traceback..error_handler(*args, **kwargs)      67     filtered_tb = _process_traceback_frames(e.__traceback__)      68      To get the full stack trace, call:      69      `tf.debugging.disable_traceback_filtering()` > 70     raise e.with_traceback(filtered_tb) from None      71 finally:      72     del filtered_tb File /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/sitepackages/tf_keras/src/engine/sequential.py:177, in Sequential.add(self, layer) ...     180     )     182 tf_utils.assert_no_legacy_layers([layer])     183 if not self._is_layer_name_unique(layer): TypeError: The added layer must be an instance of class Layer. Received: layer= of type .",The issue has been resolved. Installing tfkeras and using keras from it instead of tf.keras fixed the problem. Thank you!,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[XLA:GPU] Introduce a separate option which would control falling back to the default layout in HLO parser just for entry_computation_layout.,[XLA:GPU] Introduce a separate option which would control falling back to the default layout in HLO parser just for entry_computation_layout.,2025-01-29T10:58:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86040
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-29T10:41:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86039
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-29T10:34:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86038
Venkat6871,Fix typos in documentation strings,"Hi, Team I observed few typos in the documentation strings and I have fixed those typos so please do the needful. Thank you.",2025-01-29T09:48:37Z,size:S,closed,0,1,https://github.com/tensorflow/tensorflow/issues/86037,>  Import didn't affect any internal file This is already fixed internally. Please make sure to sync your repo to head.
copybara-service[bot],Add a reproducer for issue with reduce->bitcast->broadcast->slice pattern,"Add a reproducer for issue with reduce>bitcast>broadcast>slice pattern This caused a crash after a Triton integrate that was only caught postsubmit, so we should have a regression test to ensure it does not happen again.",2025-01-29T09:34:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86036
copybara-service[bot],PR #22008: [XLA:GPU] Upgrade cuDNN frontend library to v1.10,"PR CC(After install 1.10.1, it doesn't work now): [XLA:GPU] Upgrade cuDNN frontend library to v1.10 Imported from GitHub PR https://github.com/openxla/xla/pull/22008 The new version of cuDNN will enable using block scaled matmul in cuDNN graphs. The support of accelerated block scaled matmul (available on Blackwell GPU) in XLA requires this upgrade. Copybara import of the project:  c0fda6c2633dedfc79d037b9f0db7d0012476f06 by Sergey Kozub : Upgrade cuDNN frontend library to v1.10 Merging this change closes CC(After install 1.10.1, it doesn't work now) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22008 from openxla:devel/cudnn_frontend_110 c0fda6c2633dedfc79d037b9f0db7d0012476f06",2025-01-29T09:26:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86035
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:22:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86034
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:18:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86033
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:17:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86032
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:17:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86031
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:16:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86030
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:16:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86029
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:15:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86028
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:15:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86027
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:15:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86026
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:15:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86025
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:15:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86024
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:14:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86023
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:14:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86022
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:14:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86021
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:14:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86020
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:14:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86019
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:13:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86018
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:13:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86017
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:12:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86016
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:12:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86015
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:12:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86014
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-29T09:11:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86013
copybara-service[bot],Fix bug in AlgebraicSimplifier,Fix bug in AlgebraicSimplifier HandleMaximum and HandleMinimum did not consider ops with mixed floating point precision. Add handling for that.,2025-01-29T08:58:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86012
copybara-service[bot],PR #21800: [XLA:GPU] Add block scaling rewriter pass,"PR CC(Custom tensorflow op with output shape determined by the input tensor): [XLA:GPU] Add block scaling rewriter pass Imported from GitHub PR https://github.com/openxla/xla/pull/21800 This PR adds a transformation pass that supports custom calls to block quantize/dequantize/dot ops. Such calls are replaced by an equivalent sequence of HLO operations. This pass is supposed to support MX scaling formats, such as MXFP8, but is not limited to those and can be used with any data types and block sizes. The quantization op sequence matches the one described in the section 6.3 of the MX spec: https://www.opencompute.org/documents/ocpmicroscalingformatsmxv10specfinalpdf Once cuDNN frontend 1.10 is released, a lowering to a cuDNN graph will be enabled for the hardware that supports block scaled dot natively (i.e. Blackwell). This pass will stay disabled until then. I also plan on introducing a new HLO op, ""blockscaleddot"", which will be more generic than a custom call  for example, will have configurable dimensions numbers akin to the general dot op. This will follow in a separate PR, once that is approved, I'll replace the custom call ""__op$block_scaled_dot"" with it. Copybara import of the project:  5dcc610e804e7aaad9b79369f714a63f9f096ad8 by Sergey Kozub : Add block scaling rewriter pass Merging this change closes CC(Custom tensorflow op with output shape determined by the input tensor) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21800 from openxla:skozub/block_scaling 5dcc610e804e7aaad9b79369f714a63f9f096ad8",2025-01-29T08:27:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86011
copybara-service[bot],Reverts 14967296f7faae46550bf5d3e240194e74ebc552,Reverts 14967296f7faae46550bf5d3e240194e74ebc552,2025-01-29T08:01:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86010
gowthamSKs,Update xla_ops.cc,,2025-01-29T07:32:42Z,size:XS invalid,closed,0,3,https://github.com/tensorflow/tensorflow/issues/86009,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",Added comment,"Please don't use ""add file""/""update file""/""fix file""/etc. commit messages. These are hard to reason about when looking at the history of the file/repository. Instead, please write explanatory git commit messages. The commit message is also the title of the PR if the PR has only one commit. It is thus twice important to have commit messages that are relevant, as PRs would be easier to understand and easier to analyze in search results. For how to write good quality git commit messages, please consult https://cbea.ms/gitcommit/ "
copybara-service[bot],Fix collective memory allocation in cuda executor,Fix collective memory allocation in cuda executor `CudaExecutor::Allocate` used to always return a nullptr when the user requested an allocation the collective memory space. This was caused by a mistake in one of my refactorings a while ago.,2025-01-29T07:11:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86008
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets. Reverts 8b90c7755e298136842b0a952ace340e5b535d23,2025-01-29T05:53:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86007
copybara-service[bot],litert: Fix LiteRT Runtime C library build,litert: Fix LiteRT Runtime C library build Update the rule to include all LiteRtXXXX symbols.,2025-01-29T05:28:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86006
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21822 from openxla:devel/sm100a 267cf74a084c933e532a622da2485befdc47f8ce,2025-01-29T04:47:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86005
copybara-service[bot],litert: Refine c/litert_dispatch_delegate.h,litert: Refine c/litert_dispatch_delegate.h  c/litert_dispatch_delegate.h shouldn't rely on external C++ header files  Created separate cc/litert_dispatch_delegate.h for C++ APIs,2025-01-29T04:19:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86004
copybara-service[bot],[xla:ffi] Add support for passing RunId to FFI handlers,[xla:ffi] Add support for passing RunId to FFI handlers,2025-01-29T03:59:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86003
copybara-service[bot],[pjrt] Use the `PjRtMemorySpace` version of `BufferFromHostBuffer`,[pjrt] Use the `PjRtMemorySpace` version of `BufferFromHostBuffer` Buffers live in memory spaces and not on devices. The `PjRtDevice` version of `BufferFromHostBuffer` is deprecated and will be removed once the migration is complete.,2025-01-29T03:14:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86002
copybara-service[bot],Reverts 8b90c7755e298136842b0a952ace340e5b535d23,Reverts 8b90c7755e298136842b0a952ace340e5b535d23,2025-01-29T03:12:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/86001
copybara-service[bot],Increase tolerance of MatmulReplicated test.,Increase tolerance of MatmulReplicated test. It was previously failing on H100s. We didn't notice since our CI doesn't run tests requiring more than 2 GPUs.,2025-01-29T03:11:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/86000
copybara-service[bot],Fix rare crash in memcpy a2a.,"Fix rare crash in memcpy a2a. NcclAllToAllStartThunk's memcpy implementation would register pointers to values of an absl::flat_hash_map with cuMemHostRegister. But flat_hash_map doesn't guarantee pointer stability, so if a rehash occurred, the pointers would change, making them potentially no longer registered. Now instead of flat_hash_map, a pointer to an array is used. A node_hash_map could have alternatively been used but requires more calls to HostMemoryRegister and has worse performance. I added an 8GPU alltoall test since I didn't see the issue occur with only 2 GPUs. This issue caused occasional crashes with various errors like CUDA_ERROR_ILLEGAL_ADDRESS. I'm not sure why it didn't consistently crash. Maybe cuMemHostRegister registers a larger range of memory than what is passed in as the 'size' argument.",2025-01-29T02:39:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85999
copybara-service[bot],Add transformation to allow transposed input to BatchMatMul,Add transformation to allow transposed input to BatchMatMul,2025-01-29T01:43:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85998
copybara-service[bot],Update Bazel/CMake to use a version of XNNPACK,Update Bazel/CMake to use a version of XNNPACK,2025-01-29T01:16:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85997
copybara-service[bot],Refactor XLA's common.bara.sky to make copying of top level files and dirs more terse,"Refactor XLA's common.bara.sky to make copying of top level files and dirs more terse This is in preparation for introducing the concept of a ""moveonly"" file explicitly",2025-01-29T00:58:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85996
copybara-service[bot],Add async/c/types.h to core/c:headers_filegroup,Add async/c/types.h to core/c:headers_filegroup async/c/types.h is needed by tensorflow/lite/core/c/c_api.h,2025-01-29T00:36:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85995
copybara-service[bot],Make GPU PJRT use CreateMemoryAllocator for Host memory.,Make GPU PJRT use CreateMemoryAllocator for Host memory.,2025-01-29T00:23:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85994
copybara-service[bot],Disable async dispatch within the body of a host callback.,"Disable async dispatch within the body of a host callback. This is a follow up to https://github.com/jaxml/jax/pull/26160 and https://github.com/openxla/xla/pull/21980. See those PRs for more discussion of the motivation for this change. In this PR, we disable CPU asynchronous execution when running within the body of a host callback, because this can cause deadlocks.",2025-01-29T00:14:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85993
copybara-service[bot],Set CPU as the default acceleration option for a Compiled Model,Set CPU as the default acceleration option for a Compiled Model,2025-01-28T23:59:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85992
copybara-service[bot],Use efficient packed flatbuffer api to handle underlying tfl models.,Use efficient packed flatbuffer api to handle underlying tfl models.,2025-01-28T23:43:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85991
copybara-service[bot],Allow `GetModelBufWithByteCode` to work with models that contain more then one op.,Allow `GetModelBufWithByteCode` to work with models that contain more then one op.,2025-01-28T23:07:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85990
copybara-service[bot],Add GitHub Actions based XLA ARM64 CPU build,Add GitHub Actions based XLA ARM64 CPU build,2025-01-28T23:01:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85989
copybara-service[bot],Add support for MemoryType::kHost to CreateStreamExecutor on the relevant Executor types.,Add support for MemoryType::kHost to CreateStreamExecutor on the relevant Executor types.,2025-01-28T22:43:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85988
copybara-service[bot],[xla:pjrt] Pass host callback pointers to FFI handlers via FFI's ExecutionContext.,[xla:pjrt] Pass host callback pointers to FFI handlers via FFI's ExecutionContext.,2025-01-28T22:38:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85987
copybara-service[bot],"Create ""internal"" visibility for LiteRT-internal targets","Create ""internal"" visibility for LiteRTinternal targets",2025-01-28T22:34:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85986
copybara-service[bot],Return arrays from `ArrayImpl._check_and_rearrange`.,"Return arrays from `ArrayImpl._check_and_rearrange`. This is in preparation for a larger change, so that `_check_arrays` can be called before Array creation in XLA and the user gets more helpful JAX error messages instead of XLA errors. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e",2025-01-28T22:27:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85985
copybara-service[bot],Refactor Jax FFI lowering to prepare for implementing CPU/GPU callbacks using XLA's FFI.,"Refactor Jax FFI lowering to prepare for implementing CPU/GPU callbacks using XLA's FFI.  This refactor should have no impact on tests or publicfacing APIs.  `mlir.emit_python_callback` would eventually depend on `ffi.ffi_lowering`, which in turn depends on definitions in `mlir.py`. We break this circular dependency.",2025-01-28T22:26:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85984
copybara-service[bot],Implement Jax CPU/GPU callbacks with XLA's FFI.,"Implement Jax CPU/GPU callbacks with XLA's FFI.  Change 4 of 4 addressing CC(JVM, .NET Language Support) in https://github.com/jaxml/jax/issues/25842.",2025-01-28T22:23:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85983
copybara-service[bot],Actually copy the extra wheels into the expected directory afterwards.,Actually copy the extra wheels into the expected directory afterwards.,2025-01-28T22:21:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85982
copybara-service[bot],Make TensorBuffer owner of the associated Event,Make TensorBuffer owner of the associated Event,2025-01-28T22:11:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85981
copybara-service[bot],Add IFRT wrappers to public API,Add IFRT wrappers to public API,2025-01-28T22:08:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85980
copybara-service[bot],Create two new ml-build images. One with libcudnn9.1 and cuda 12.1 and the other one with libcudnn 9.1 and cuda 12.3.,Create two new mlbuild images. One with libcudnn9.1 and cuda 12.1 and the other one with libcudnn 9.1 and cuda 12.3.,2025-01-28T21:40:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85979
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-28T21:40:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85978
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets. Reverts a47a28e840cf97148669ba3483cd72e87f0efa5b,2025-01-28T21:31:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85977
copybara-service[bot],Integrate LLVM at llvm/llvm-project@b108fbe6ea42,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match b108fbe6ea42,2025-01-28T21:12:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85976
copybara-service[bot],Always dispatch CPU executables synchronously when they include callbacks.,"Always dispatch CPU executables synchronously when they include callbacks. As discussed in https://github.com/jaxml/jax/issues/25861 and https://github.com/jaxml/jax/issues/24255, using host callbacks within an asynchronouslydispatched CPU executable can deadlock when the body of the callback itself asynchronously dispatches JAX CPU code. My rough understanding of the problem is that the XLA intra op thread pool gets filled up with callbacks waiting for their body to execute, but there aren't enough resources to schedule the inner computations. There's probably a better way to fix this within XLA:CPU, but the temporary fix that I've come up with is to disable asynchronous dispatch on CPU when either: 1. Executing a program that includes any host callbacks, or 2. when running within the body of a callback. It seems like both of these conditions are needed in general because I was able to find test cases that failed with just one or the other implemented. This PR includes just the first change, and the second will be implemented in a followup.",2025-01-28T20:46:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85975
copybara-service[bot],Don't run thread sans on apply plugin test. In general we can't use sanitizers for x86 code that dlopens.,Don't run thread sans on apply plugin test. In general we can't use sanitizers for x86 code that dlopens.,2025-01-28T20:20:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85974
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-28T20:20:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85973
copybara-service[bot],Add some additional args for select_and_scatter_test for certain backends.,Add some additional args for select_and_scatter_test for certain backends.,2025-01-28T20:19:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85972
copybara-service[bot],[pjrt] Use the `PjRtMemorySpace` version of `BufferFromHostBuffer`,[pjrt] Use the `PjRtMemorySpace` version of `BufferFromHostBuffer` Buffers live in memory spaces and not on devices. The `PjRtDevice` version of `BufferFromHostBuffer` is deprecated and will be removed once the migration is complete. Reverts a47a28e840cf97148669ba3483cd72e87f0efa5b FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21375 from shraiysh:while_loop_analysis a435fbd2eadc17269d7bccbe141dcf7a21cc20e8,2025-01-28T19:56:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85971
copybara-service[bot],PR #21822: [XLA:GPU] Add support for SM100a architecture (Blackwell),"PR CC(Add support of CTC float64): [XLA:GPU] Add support for SM100a architecture (Blackwell) Imported from GitHub PR https://github.com/openxla/xla/pull/21822 Created `ShouldUsePtxExtension` helper for the extension suffix (this will also be used for sm120, etc). CUDA 12.8 was recently released, which supports PTX 8.7, but that is not supported by the integrated LLVM (support added in https://github.com/llvm/llvmproject/pull/124155), so leaving the association with PTX 8.6  this doesn't raise warnings during compilation. Copybara import of the project:  267cf74a084c933e532a622da2485befdc47f8ce by Sergey Kozub : Add support for SM100a architecture (Blackwell) Merging this change closes CC(Add support of CTC float64) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21822 from openxla:devel/sm100a 267cf74a084c933e532a622da2485befdc47f8ce",2025-01-28T19:56:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85970
copybara-service[bot],Remove `UpdateEntryComputationLayout` from `HloRunnerAgnosticTestBase`,Remove `UpdateEntryComputationLayout` from `HloRunnerAgnosticTestBase` Reverts a47a28e840cf97148669ba3483cd72e87f0efa5b,2025-01-28T19:55:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85969
copybara-service[bot],[pjrt] Use the `PjRtMemorySpace` version of `BufferFromHostBuffer`,[pjrt] Use the `PjRtMemorySpace` version of `BufferFromHostBuffer` Buffers live in memory spaces and not on devices. The `PjRtDevice` version of `BufferFromHostBuffer` is deprecated and will be removed once the migration is complete.,2025-01-28T19:43:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85968
copybara-service[bot],[StableHLO] Enable XLA to properly translate StableHLO custom calls with typed FFI.,[StableHLO] Enable XLA to properly translate StableHLO custom calls with typed FFI.,2025-01-28T19:40:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85967
copybara-service[bot],#litert Remove unused include in `litert_expected.h`,litert Remove unused include in `litert_expected.h`,2025-01-28T19:17:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85966
copybara-service[bot],Move TSL's `workspace*.bzl` files to XLA,Move TSL's `workspace*.bzl` files to XLA Also steal `def_file_filter` from TensorFlow as was previously done by TSL. In a followup these can be compressed further,2025-01-28T19:16:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85965
copybara-service[bot],Migrate convert_test to always use PjRt for its test backend.,Migrate convert_test to always use PjRt for its test backend.,2025-01-28T19:12:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85964
copybara-service[bot],[XLA:LAYOUT_CONSTRAINTS] Move custom call constraints before backend constraints but after other,[XLA:LAYOUT_CONSTRAINTS] Move custom call constraints before backend constraints but after other mandatory constriants.,2025-01-28T18:56:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85963
copybara-service[bot],Add `--ppc64le-compliance-tag` mandatory argument to manylinux_compliance_test.,Add `ppc64lecompliancetag` mandatory argument to manylinux_compliance_test.,2025-01-28T18:54:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85962
copybara-service[bot],Add NCCL dict entry for CUDA 12.8.0.,Add NCCL dict entry for CUDA 12.8.0.,2025-01-28T18:30:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85961
copybara-service[bot],Fix `TfrtCpuClient` allocating oversized buffers.,"Fix `TfrtCpuClient` allocating oversized buffers. `TfrtCpuClient` allocates buffers using `AbstractTfrtCpuBuffer::AllocateTrackedDeviceBuffer`, which takes an 'on device shape'. This shape informs the number of bytes that are allocated for the buffer. Since the buffer packs subbyte (nonpredicate) arrays, the buffer is oversized if using the host shape and layout for this purpose, because the host layout is not packed. This patch fixes the buffer allocation and adds a host to device roundtrip test to ensure that it is populated correctly.",2025-01-28T18:15:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85960
copybara-service[bot],Parse XLA_FLAGS environment variable every time.,Parse XLA_FLAGS environment variable every time.,2025-01-28T17:14:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85959
matteosal,Status of C/C++ APIs?,"Do they fully support running operators and models, gradients and optimizers? What's the relationship between them? Are there uptodate resources, tutorials etc on how to use them?",2025-01-28T17:07:43Z,stat:awaiting response type:support,closed,3,9,https://github.com/tensorflow/tensorflow/issues/85958,"Hi **** , Welcome to TensorFlow! Yes, TensorFlow fully supports running models, computing gradients, and using optimizers. These are fundamental building blocks for building and training machine learning models in TensorFlow. Here, I am providing the Doc1, Doc2 for your reference. If you have any further queries, please fill out all the required templates. Thank you!"," you have linked a page about Javascript, I asked for C/C++. I am asking this because it's hard to find clear information about them.  This document states at the bottom that the C API does not support gradients, but maybe that's for TF 1 so maybe it doesn't apply anymore. Then I found this C++ API reference page which just lists a bunch of symbols with no contextual explanation or introduction. Looking around there only seem to be some isolated gradient primitives but I couldn't find anything analogous to a generic method of running backpropagation through an arbitrary set of operators.","The C API is very limited, it's used to create bindings to other languages. The C++ API is used mostly for inference, I am not 100% it 100% covers training needs.","  > The C API is very limited, it's used to create bindings to other languages. Then it seems natural to expect it supports gradients, right? It's a core feature anyone wanting to create a binding for another language would need.",This is the entire C API. In particular it supports gradients.,"  Yes I noticed that function in the header. It has a note which states that not all gradients are supported. The link is broken but it's clear it's meant to lead here. This document states that implemented gradients should be declared as `Grad(...)`, and looking for this template in the entire directory produces these matches: ``` [Abs, Acos, Acosh, Add, AddN, Angle, Asin, Asinh, Atan, Atan2, Atanh, BaseFusedBatchNorm, BatchMatMul, BatchMatMulV2, BatchToSpace, BatchToSpaceND, BroadcastTo, Cast, CheckNumerics, ClipByValue, Complex, ConcatV2, Conj, Conv2D, Conv2DBackpropInput, Cos, Cosh, Cumsum, DepthToSpace, DepthwiseConv2dNative, Diag, DiagPart, Div, DivNoNan, DynamicPartition, DynamicStitch, Einsum, Erf, Erfinv, Exp, ExpandDims, Expm1, Fill, FusedBatchNormV3, GatherNd, GatherV2, Identity, Imag, Inv, L2Loss, Lgamma, Log, Log1p, LogSoftmax, MatMul, MatrixBandPart, MatrixDiag, Maximum, Mean, Minimum, MinOrMax, MirrorPad, MirrorPadGrad, Mul, Ndtri, Neg, Pack, Pad, PartitionedCall, Pow, Prod, QuantizeAndDequantize, QuantizeAndDequantizeV3, ReadVariableOp, Real, RealDiv, RefIdentity, Reshape, Reverse, ReverseSequence, Roll, Rsqrt, ScatterNd, ScatterNdNonAliasingAdd, SegmentSum, Select, SelectV2, Sigmoid, Sign, Sin, Sinh, Slice, Softmax, SoftmaxCrossEntropyWithLogits, SpaceToBatch, SpaceToBatchND, SpaceToDepth, Split, SplitV, Sqrt, Square, SquaredDifference, Squeeze, Sub, Sum, Tan, Tanh, Tile, Transpose, Unpack, UnsortedSegmentMinOrMax, UnsortedSegmentSum] ``` Several things are missing here, e.g. 1D and 3D Convolutions, ConvTranspose, Pooling, spatial resampling, RNNs. Can anyone confirm this is correct and the above gradients are the only ones available?","That is correct. PRs welcome to add more support if needed, though I think JAX is a better replacement.",Thank you,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Add support for CUDA 12.8.0 and CUDNN 9.7.0.,Add support for CUDA 12.8.0 and CUDNN 9.7.0.,2025-01-28T16:42:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85957
copybara-service[bot],Replace LITERT_ASSERT_STATUS_OK with LITERT_ASSERT_OK.,Replace LITERT_ASSERT_STATUS_OK with LITERT_ASSERT_OK.,2025-01-28T16:40:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85956
copybara-service[bot],#litert Use `LiteRtGetStatusString()` to make error messages clearer.,litert Use `LiteRtGetStatusString()` to make error messages clearer.,2025-01-28T16:33:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85955
copybara-service[bot],PR #21800: [XLA:GPU] Add block scaling rewriter pass,"PR CC(Custom tensorflow op with output shape determined by the input tensor): [XLA:GPU] Add block scaling rewriter pass Imported from GitHub PR https://github.com/openxla/xla/pull/21800 This PR adds a transformation pass that supports custom calls to block quantize/dequantize/dot ops. Such calls are replaced by an equivalent sequence of HLO operations. This pass is supposed to support MX scaling formats, such as MXFP8, but is not limited to those and can be used with any data types and block sizes. The quantization op sequence matches the one described in the section 6.3 of the MX spec: https://www.opencompute.org/documents/ocpmicroscalingformatsmxv10specfinalpdf Once cuDNN frontend 1.10 is released, a lowering to a cuDNN graph will be enabled for the hardware that supports block scaled dot natively (i.e. Blackwell). This pass will stay disabled until then. I also plan on introducing a new HLO op, ""blockscaleddot"", which will be more generic than a custom call  for example, will have configurable dimensions numbers akin to the general dot op. This will follow in a separate PR, once that is approved, I'll replace the custom call ""__op$block_scaled_dot"" with it. Copybara import of the project:  5dcc610e804e7aaad9b79369f714a63f9f096ad8 by Sergey Kozub : Add block scaling rewriter pass Merging this change closes CC(Custom tensorflow op with output shape determined by the input tensor) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21800 from openxla:skozub/block_scaling bba9d3f711bf3b18ecdd45a4de4e96d422a2122a",2025-01-28T15:55:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85954
copybara-service[bot],"Create ""internal"" visibility for LiteRT-internal targets","Create ""internal"" visibility for LiteRTinternal targets",2025-01-28T15:49:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85953
copybara-service[bot],PR #21960: Updated nanobind commit,PR CC(ppc64le: //tensorflow/python/kernel_tests:matrix_exponential_op_test fails): Updated nanobind commit Imported from GitHub PR https://github.com/openxla/xla/pull/21960 Point nanobind to the commit fixing python/c++ object concurrent accessing: https://github.com/wjakob/nanobind/issues/867 :  77e693fb39e0b737016770585c3f8786eb141474 by vfdev5 : Updated nanobind commit Merging this change closes CC(ppc64le: //tensorflow/python/kernel_tests:matrix_exponential_op_test fails) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21960 from vfdev5:updatenanobind 77e693fb39e0b737016770585c3f8786eb141474,2025-01-28T15:36:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85952
copybara-service[bot],Actually copy the extra wheels into the expected directory afterwards.,Actually copy the extra wheels into the expected directory afterwards. Reverts a47a28e840cf97148669ba3483cd72e87f0efa5b,2025-01-28T15:13:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85951
copybara-service[bot],Integrate LLVM at llvm/llvm-project@aa65f93b71de,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match aa65f93b71de,2025-01-28T14:59:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85950
copybara-service[bot],Integrate LLVM at llvm/llvm-project@e0c7f081f158,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match e0c7f081f158,2025-01-28T14:46:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85949
copybara-service[bot],PR #21886: [ROCM][NFC] BlasLt interface refactoring & simplifying: part I,"PR CC(Tensorflow 1.10.0 MKL build (win64) with bazel throws linker error): [ROCM][NFC] BlasLt interface refactoring & simplifying: part I Imported from GitHub PR https://github.com/openxla/xla/pull/21886 After this PR https://github.com/tensorflow/tensorflow/pull/73926 is merged, we can remove unnecessary lowlevel DoMatmul functions from GpuBlasLt interface (which otherwise looks scary and unnecessarily complicated). Furthermore, we can also remove **ValidateInputs** function from the interface and derived classes since a highlevel **ExecuteOnStream** function already handles datatypes correctly. This also greatly simplifies the code. Also, I have packed the input arguments of ExecuteOnStream calls to a struct **MemoryArgs** to simplify arguments passing in derived classes and improve code readability. Finally, in the original GpuBlasLt PR: https://github.com/openxla/xla/pull/5911, I made a sort of mistake by adding a reference to **blas_lt** to the MatmulPlan class here, thereby making MatmulPlans bound to a **particular BlasLt instance**. This resulted in some further bugfixes and, most importantly, complicated GpuBlasLt cache design in gpublas_lt_matmul_thunk.cc/.h. In this PR, I remove this reference again from MatmulPlan class and in the next NFC PR the cache mechanics can also be simplified.  Unfortunately, this change also requires a tandem PR for Tensorflow: https://github.com/tensorflow/tensorflow/pull/85835 rotation Would you please have a look Copybara import of the project:  e96bb2fbedab3f53b31ef0e1748582c76e9fb105 by Pavel Emeliyanenko : blaslt interface refactoring: removing blas_lt_ref added cuda adaptions cudaside adaptions cuda side adaptions fix fixing pointers Merging this change closes CC(Tensorflow 1.10.0 MKL build (win64) with bazel throws linker error) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21886 from ROCm:ci_gpublas_lt_refactor_1 e96bb2fbedab3f53b31ef0e1748582c76e9fb105",2025-01-28T14:45:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85948
copybara-service[bot],Rename enums in CudaComputeCapability according to the style guide,Rename enums in CudaComputeCapability according to the style guide,2025-01-28T14:35:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85947
copybara-service[bot],Triton: Fix missing include header,Triton: Fix missing include header,2025-01-28T13:22:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85946
copybara-service[bot],[XLA:GPU] Triton support test - minor fixes,"[XLA:GPU] Triton support test  minor fixes * stop calling IsTritonSupportedInstruction explicitly, RunSupportTest calls it and verify that the output is in sync with actual behavior,  * call RunSupportTest for all ops when multiple are tested in a single test. Reverts a47a28e840cf97148669ba3483cd72e87f0efa5b",2025-01-28T13:14:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85945
copybara-service[bot],PR #21948: [GPU] Upgrade cuDNN frontend to 1.10.0.,PR CC('): [GPU] Upgrade cuDNN frontend to 1.10.0. Imported from GitHub PR https://github.com/openxla/xla/pull/21948 Copybara import of the project:  affa734c3c6e2af934dd12eafe7e8771ab0ee8db by Ilia Sergachev : [GPU] Upgrade cuDNN frontend to 1.10.0. Merging this change closes CC(') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21948 from openxla:cudnn_fe_1100 affa734c3c6e2af934dd12eafe7e8771ab0ee8db,2025-01-28T12:16:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85944
copybara-service[bot],Reverts 1af1de23aaf9efd5e68522a4f9b9e8b27ef0c58a,Reverts 1af1de23aaf9efd5e68522a4f9b9e8b27ef0c58a,2025-01-28T11:10:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85943
copybara-service[bot],Integrate LLVM at llvm/llvm-project@2e5a5237daf8,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 2e5a5237daf8,2025-01-28T11:05:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85942
copybara-service[bot],[xla:cpu] Add more sort keywords to correctly order XLA:CPU debug options.,[xla:cpu] Add more sort keywords to correctly order XLA:CPU debug options.,2025-01-28T11:02:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85941
copybara-service[bot],Integrate LLVM at llvm/llvm-project@c4891089125d,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match c4891089125d,2025-01-28T10:05:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85940
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T10:01:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85939
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:51:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85938
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:43:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85937
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:35:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85936
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:35:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85935
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:31:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85934
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:28:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85933
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:25:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85932
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:21:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85931
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:20:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85930
copybara-service[bot],Preserve backend_config when writing the HLO for the ScriptChecker.,"Preserve backend_config when writing the HLO for the ScriptChecker. backend_config can contain important information for the lowering pipeline. If we want to run a bisect without running HLO passes, this information is not generated, so must be taken from the original HLO. Reverts 1af1de23aaf9efd5e68522a4f9b9e8b27ef0c58a",2025-01-28T09:18:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85929
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:18:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85928
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:17:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85927
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:15:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85926
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:14:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85925
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:13:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85924
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T09:12:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85923
copybara-service[bot],[XLA] Add TraceMe for ThunksEmitter::EmitEntryComputation.,[XLA] Add TraceMe for ThunksEmitter::EmitEntryComputation. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/86164 from tensorflow:mihaimaruseacpatch1 d5e7459e51c112b117e52a5d5ec0629ebf384715,2025-01-28T08:54:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85922
copybara-service[bot],Convert from `std::string_view` to `absl::string_view` explicitly,Convert from `std::string_view` to `absl::string_view` explicitly,2025-01-28T08:00:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85921
copybara-service[bot],Skip TSAN tests as they are also not supported for GPU.,Skip TSAN tests as they are also not supported for GPU.,2025-01-28T07:03:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85920
copybara-service[bot],"Fix `mxfloat` build dependencies and remove test BUILD file, to remove duplication.","Fix `mxfloat` build dependencies and remove test BUILD file, to remove duplication.",2025-01-28T06:17:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85919
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T05:36:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85918
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-28T05:29:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85917
copybara-service[bot],Support CreateBuffersForAsyncHostToDevice with shape_spec + layout in TfrtCpuClient.,Support CreateBuffersForAsyncHostToDevice with shape_spec + layout in TfrtCpuClient.,2025-01-28T04:43:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85916
copybara-service[bot],"Create ""internal"" visibility for LiteRT-internal targets","Create ""internal"" visibility for LiteRTinternal targets",2025-01-28T04:32:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85915
copybara-service[bot],Add `--xnnpack_slinky_disable_schedule` flag,Add `xnnpack_slinky_disable_schedule` flag,2025-01-28T03:05:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85914
copybara-service[bot],Set expected entry point name to empty string in model buffer append.,Set expected entry point name to empty string in model buffer append.,2025-01-28T02:09:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85913
copybara-service[bot],Support offset tensors in model load. This finishes offset tensor support.,Support offset tensors in model load. This finishes offset tensor support.,2025-01-28T02:05:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85912
copybara-service[bot],Reverts a47a28e840cf97148669ba3483cd72e87f0efa5b,Reverts a47a28e840cf97148669ba3483cd72e87f0efa5b,2025-01-28T01:44:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85911
copybara-service[bot],Handle offset tensor buffers with unified buffer management in model serialize.,Handle offset tensor buffers with unified buffer management in model serialize.,2025-01-28T01:25:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85910
copybara-service[bot],Add flags to tf_tfl_translate for pytorch saved model conversion,Add flags to tf_tfl_translate for pytorch saved model conversion,2025-01-28T01:22:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85909
copybara-service[bot],Add todo to add additional control dependencies,Add todo to add additional control dependencies,2025-01-28T01:18:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85908
copybara-service[bot],Disable sync collectives for pipeline parallelism,Disable sync collectives for pipeline parallelism,2025-01-28T01:14:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85907
copybara-service[bot],Disable cp cycle decomposition for pipeline parallelism implementations that do this at the user level,Disable cp cycle decomposition for pipeline parallelism implementations that do this at the user level Reverts changelist 721352942 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-28T01:14:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85906
copybara-service[bot],Update the run environment for E2E XLA CPU benchmarks,Update the run environment for E2E XLA CPU benchmarks,2025-01-28T00:58:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85905
copybara-service[bot],Delete XLA CPU Kokoro build now that the GitHub Actions CPU build is working,Delete XLA CPU Kokoro build now that the GitHub Actions CPU build is working,2025-01-28T00:41:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85904
copybara-service[bot],[XLA] Correct a few typos in the comments of memory_space_assignment files and clean up a couple of includes.,[XLA] Correct a few typos in the comments of memory_space_assignment files and clean up a couple of includes.,2025-01-28T00:37:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85903
copybara-service[bot],Remove remaining HloRunnerPjRt calls to UpdateEntryComputationLayout.,Remove remaining HloRunnerPjRt calls to UpdateEntryComputationLayout.,2025-01-28T00:29:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85902
copybara-service[bot],"Replace ""external_buffer"" nomenclature with ""op_asset"" and leverage the unified buffer management approach.","Replace ""external_buffer"" nomenclature with ""op_asset"" and leverage the unified buffer management approach.",2025-01-28T00:13:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85901
copybara-service[bot],Use NMS output to reduce output tensor sizes so invalid entries are never accessed,Use NMS output to reduce output tensor sizes so invalid entries are never accessed,2025-01-28T00:08:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85900
copybara-service[bot],Avoid a potential extra string allocation when constructing litert::Error,"Avoid a potential extra string allocation when constructing litert::Error This is done by always accepting a string as input as opposed to a string_view, so that if an Error object is instantiated from a temporary std::string, such as in  return litert::Error(kLiteRtErrorRuntimeFailure, absl::StrCat(""string 1"", ""string 2"")) then we can capture that temporary string and avoid building a copy of it in the litert::Error constructor",2025-01-27T23:53:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85899
copybara-service[bot],Change default pending shape func to a fixed code location instead of creating a lambda ad-hoc.,Change default pending shape func to a fixed code location instead of creating a lambda adhoc.,2025-01-27T23:48:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85898
copybara-service[bot],Install Python 3.13t in JAX arm64 base docker image,Install Python 3.13t in JAX arm64 base docker image A freethreaded Python 3.13nogil is now available in aptget. Install it following the instructions at: https://pyfreethreading.github.io/installing_cpython/__tabbed_2_3.,2025-01-27T23:28:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85897
copybara-service[bot],Add support for GCS File System in Pywrap Profiler.,Add support for GCS File System in Pywrap Profiler.,2025-01-27T23:16:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85896
copybara-service[bot],Only decompose collective permute ops that may be pipelined,Only decompose collective permute ops that may be pipelined,2025-01-27T22:38:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85895
copybara-service[bot],Wrap HLO strings in collective permute decomposer,Wrap HLO strings in collective permute decomposer,2025-01-27T22:37:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85894
copybara-service[bot],Silence msan warning by switching transferring socket addresses to use text,Silence msan warning by switching transferring socket addresses to use text format rather than just transferring it as raw bits. This is needed because ipv4 support has padding bits and sending these over the socket was causing msan failures. (Not actually a failure because they're ignored on the other side though).,2025-01-27T22:16:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85893
copybara-service[bot],Remove broken and unused support for allocating MemoryType::kHost memory in DeviceMemAllocator.,"Remove broken and unused support for allocating MemoryType::kHost memory in DeviceMemAllocator. If someone tried using it, they'd never be able to allocate memory  Allocate would always return nullptr.",2025-01-27T21:58:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85892
copybara-service[bot],Fix line wraps in gpu_latency_hiding_scheduler_test,Fix line wraps in gpu_latency_hiding_scheduler_test,2025-01-27T21:43:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85891
copybara-service[bot],Introduce pipeline parallelism optimization level,Introduce pipeline parallelism optimization level Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e,2025-01-27T21:42:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85890
copybara-service[bot],Only apply collective permute decomposer to collective-permutes at the beginning of loop bodies,Only apply collective permute decomposer to collectivepermutes at the beginning of loop bodies,2025-01-27T21:40:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85889
copybara-service[bot],"[XLA:GPU] Add triton support test for replica-id, partition-id, collective-permute-x","[XLA:GPU] Add triton support test for replicaid, partitionid, collectivepermutex",2025-01-27T21:26:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85888
copybara-service[bot],Integrate LLVM at llvm/llvm-project@2e5a5237daf8,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 2e5a5237daf8,2025-01-27T21:17:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85887
copybara-service[bot],Unify metadata storage with unified buffer management.,Unify metadata storage with unified buffer management.,2025-01-27T21:11:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85886
copybara-service[bot],"Build IFRT shardings with both addressable and non-addressable devices, instead of only addressable devices.","Build IFRT shardings with both addressable and nonaddressable devices, instead of only addressable devices.",2025-01-27T21:08:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85885
copybara-service[bot],Switch CollectiveMemoryAllocation to use CreateMemoryAllocator infrastructure.,Switch CollectiveMemoryAllocation to use CreateMemoryAllocator infrastructure.,2025-01-27T21:03:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85884
copybara-service[bot],Re-order the methods in litertmodelT. Current way is hard to read.,Reorder the methods in litertmodelT. Current way is hard to read.,2025-01-27T21:01:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85883
copybara-service[bot],Update Build files to allow visibility to for internal use_case,Update Build files to allow visibility to for internal use_case,2025-01-27T20:09:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85882
copybara-service[bot],Generalize cycle related functions and move to SourceTargetPair class. ,Generalize cycle related functions and move to SourceTargetPair class.  Replace source/target usage in CollectivePermuteValidIterationAnnotator.,2025-01-27T20:06:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85881
copybara-service[bot],"Update HLO Translate to return errors using MLIR diagnostics, update test files to use hlo-translate","Update HLO Translate to return errors using MLIR diagnostics, update test files to use hlotranslate This CL also allows us to test negative cases using lit since the errors will now be thrown with the proper file context.",2025-01-27T19:45:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85880
copybara-service[bot],Only register RegisterTransferServerTypes on linux.,"Only register RegisterTransferServerTypes on linux. Oops, this symbol is only defined on linux (where the header is included).",2025-01-27T19:36:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85879
copybara-service[bot],Only load opencl once to mitigate potential un-thread-safety in the init process.,Only load opencl once to mitigate potential unthreadsafety in the init process.,2025-01-27T19:35:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85878
copybara-service[bot],Fix compilation of `litert::Expected` for some compilers.,Fix compilation of `litert::Expected` for some compilers.,2025-01-27T19:23:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85877
copybara-service[bot],#litert Add `LiteRtGetStatusString` function.,litert Add `LiteRtGetStatusString` function.,2025-01-27T19:21:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85876
copybara-service[bot],Replace `kLiteRtAccelator*` with `kLiteRtAccelerator*`.,Replace `kLiteRtAccelator*` with `kLiteRtAccelerator*`.,2025-01-27T18:53:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85875
copybara-service[bot],Print layout info in verifier in some more places,Print layout info in verifier in some more places,2025-01-27T18:50:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85874
copybara-service[bot],Replace output_path with output_file in xla_compile_lib.,Replace output_path with output_file in xla_compile_lib. This way the error messages align with the flag the user passes to xla_compile.,2025-01-27T18:35:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85873
copybara-service[bot],Change noisy (and useless) XLA log line to VLOG,Change noisy (and useless) XLA log line to VLOG,2025-01-27T18:34:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85872
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-27T18:27:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85871
copybara-service[bot],[xla:cpu] Extract convolution_lib from a convolution_thunk,[xla:cpu] Extract convolution_lib from a convolution_thunk + Modernize convolution thunk implementation to use CountDownAsyncValueRef directly,2025-01-27T18:24:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85870
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-27T18:17:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85869
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-27T18:16:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85868
copybara-service[bot],PR #21845: [ROCM] Add missing triton MLIR int4 -> int8 rewrite pass for ROCM,PR CC(未找到相关数据): [ROCM] Add missing triton MLIR int4 > int8 rewrite pass for ROCM Imported from GitHub PR https://github.com/openxla/xla/pull/21845 ``` TritonTest.DotWithInt4WeightsOnLhsFusedWithMultiplyByChannelScales TritonTest.NonstandardLayoutInt4 TritonTest.DotWithI4WeightsOnLhsWithBitcastTo3dTensor TritonTest.DotWithI4WeightsOnLhsWithNonStandardLayoutAndMultplyInEpilogue TritonTest.LHSWithMinorDimEqualTo1 TritonTest.RHSWithMinorDimEqualTo1 TritonTest.LHSNonMinorContractingDim TritonTest.LHSNonMinorContractingDimWithBatchDim0 TritonTest.LHSMinorContractingDim TritonTest.ConvertPlusNegate TritonTest.LHSMinorContractingDimWithBatchDim0 TritonTest.RHSTestWithNotMinorContractingDim TritonTest.RHSTestWithMinorContractingDim TritonTest.RHSTestWithMinorContractingDimWithBatchDim TritonTest.RHSTestWithNotMinorContractingDimWithBatchDim0 ParametrizedTritonTest.Int4WeightsOnTheLhs ParametrizedTritonTest.Int4WeightsOnTheLhsWithBatchDim ParametrizedTritonTest.Int4WeightsOnTheRhs ``` Tests above are failing on ROCm side after int4 rewriting was moved from legacy matmul emitter to MLIR pass. This MLIR pass is now missing in ROCm triton pipeline and I'm adding it in the place. rotation: would you please take a look?  Copybara import of the project:  75e78ad365a9d55f6e299c7b64400447ceebb26d by Jian Li : [ROCM] Add missing triton MLIR int4 > int8 rewrite pass for ROCM Merging this change closes CC(未找到相关数据) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21845 from ROCm:ci_fix_rocm_triton_test 75e78ad365a9d55f6e299c7b64400447ceebb26d,2025-01-27T18:14:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85867
copybara-service[bot],Add expanded message for pass transformation consistency check.,Add expanded message for pass transformation consistency check. Pointer overload for RunAndCheckHloRewrite.,2025-01-27T17:57:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85866
copybara-service[bot],Allow building identical wheels under additional names.,"Allow building identical wheels under additional names. Currently only needed for Windows, since the same wheel is uploaded to both tensorflow_cpu, and tensorflow PyPi repos, but different names/metadata are needed.",2025-01-27T17:52:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85865
copybara-service[bot],Run E2E XLA CPU benchmarks,Run E2E XLA CPU benchmarks,2025-01-27T17:51:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85864
copybara-service[bot],Move file translates to tf lite directory for correct ownership.,Move file translates to tf lite directory for correct ownership.,2025-01-27T17:42:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85863
copybara-service[bot],Integrate LLVM at llvm/llvm-project@3684ec425904,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 3684ec425904,2025-01-27T17:42:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85862
copybara-service[bot],Migrate graphdef_to_tfl_flatbuffer to ConvertGraphToTfExecutor.,Migrate graphdef_to_tfl_flatbuffer to ConvertGraphToTfExecutor.,2025-01-27T17:41:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85861
copybara-service[bot],Open source TPU-specific input pipeline analysis.,Open source TPUspecific input pipeline analysis.,2025-01-27T17:36:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85860
copybara-service[bot],[XLA:CPU] Migrate concatenate kernel generation to ConcatenateKernelEmitter,[XLA:CPU] Migrate concatenate kernel generation to ConcatenateKernelEmitter,2025-01-27T17:31:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85859
copybara-service[bot],[XLA:Python] Fix build failure on non-linux platforms.,[XLA:Python] Fix build failure on nonlinux platforms. Don't call the transfer manager registration code if we haven't included the header.,2025-01-27T17:16:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85858
copybara-service[bot],PR #21903: [XLA:CPU] Add __truncsfhf2 and __extendhfsf2 runtime symbols.,PR CC(R1.10): [XLA:CPU] Add __truncsfhf2 and __extendhfsf2 runtime symbols. Imported from GitHub PR https://github.com/openxla/xla/pull/21903 These appear to be used by LLVM on x86 Macs. Copybara import of the project:  f80212d25799ab089ba8c3cec6fd63bad9114d9b by Peter Hawkins : [XLA:CPU] Add __truncsfhf2 and __extendhfsf2 runtime symbols. These appear to be used by LLVM on x86 Macs. Merging this change closes CC(R1.10) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21903 from hawkinsp:syms f80212d25799ab089ba8c3cec6fd63bad9114d9b,2025-01-27T17:11:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85857
copybara-service[bot],[xla:cpu] Add Lifo ready queue for completeness,[xla:cpu] Add Lifo ready queue for completeness,2025-01-27T17:08:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85856
copybara-service[bot],[xla:cpu] Add support for running multiple executions inside each HLO benchmark iteration,[xla:cpu] Add support for running multiple executions inside each HLO benchmark iteration,2025-01-27T17:03:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85855
copybara-service[bot],Change std::string_view to absl::string_view.,Change std::string_view to absl::string_view.,2025-01-27T16:51:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85854
copybara-service[bot],Update BUILD rules to allow visibility for internal packages,Update BUILD rules to allow visibility for internal packages,2025-01-27T16:47:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85853
copybara-service[bot],Rename mlir-cpu-runner to mlir-runner,Rename mlircpurunner to mlirrunner Matches https://github.com/llvm/llvmproject/commit/eb206e9ea84eff0a0596fed2de8316d924f946d1,2025-01-27T16:22:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85852
Jayprajapati19,Fix: Resolved issue #85303,,2025-01-27T16:05:27Z,size:M invalid,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85851,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Please title the issue, commit messages more descriptively. These are hard to reason about when looking at the history of the file/repository. Instead, please write explanatory git commit messages. The commit message is also the title of the PR if the PR has only one commit. It is thus twice important to have commit messages that are relevant, as PRs would be easier to understand and easier to analyze in search results. For how to write good quality git commit messages, please consult https://cbea.ms/gitcommit/ "
copybara-service[bot],[XLA:CPU] Add multithread support to FFT thunk.,"[XLA:CPU] Add multithread support to FFT thunk. As reported in https://github.com/jaxml/jax/issues/25808, the performance of XLA's CPU FFT is dramatically reduced with the thunks runtime. This is because the intraop thread pool wasn't properly passed through. This change adds multithreading support, by passing through the thread pool provided by xla::ExecuteParams, but I don't know if this is the right approach.",2025-01-27T16:02:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85850
copybara-service[bot],Add the missing wheel name specification for Windows.,Add the missing wheel name specification for Windows.,2025-01-27T15:42:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85849
copybara-service[bot],[xla:gpu] [cleanup] Enclose iterable inputs into their own class,"[xla:gpu] [cleanup] Enclose iterable inputs into their own class This brings down the ""cognitive complexity"" of the function from 78 to 60. I am working on adding TMA support to EmitMatmul, but this function is cannot support it the way it is currently written. EmitMatmul assumes that each input can be easily translated to an iterable argument, which is not the case for TMA where we are iterating over 2 indices rather than a tensor pointer. Previously, the function relied on multiple iter_arg_to* functions. This change refactors the code to enclose each input that needs to be emitted on each iteration of the ForOp into its own class. This can further be abstracted to fully emit the parameter value & calculate indices next. Previously, there were many different index calculations happening here & I tried to abstract them to be named in order for it to be more legible.",2025-01-27T15:28:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85848
copybara-service[bot],PR #21375: [ds-fusion] Get While loop analysis with copy fusion,"PR CC(Raspberry Pi install command not properly formatted.): [dsfusion] Get While loop analysis with copy fusion Imported from GitHub PR https://github.com/openxla/xla/pull/21375 In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index. Copybara import of the project:  3147ec926aa1c6fdfa2f4376668434c9a2fbeb87 by Shraiysh Vaishay : [dsfusion] Get While loop analysis with copy fusion In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index.  a435fbd2eadc17269d7bccbe141dcf7a21cc20e8 by Shraiysh Vaishay : Relay control dependencies while converting fusion to call (extractor) Merging this change closes CC(Raspberry Pi install command not properly formatted.) Reverts a47a28e840cf97148669ba3483cd72e87f0efa5b FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21375 from shraiysh:while_loop_analysis a435fbd2eadc17269d7bccbe141dcf7a21cc20e8",2025-01-27T15:22:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85847
copybara-service[bot],Update a missed value for windows_x86_cpu_2022 config.,Update a missed value for windows_x86_cpu_2022 config.,2025-01-27T15:16:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85846
copybara-service[bot],[JAX] Update AutoPGLE documentation.,[JAX] Update AutoPGLE documentation.,2025-01-27T15:15:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85845
copybara-service[bot],[XLA:GPU] Turn LHS on when PGO profile is present.,[XLA:GPU] Turn LHS on when PGO profile is present.,2025-01-27T15:14:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85844
copybara-service[bot],[XLA/Triton] Allowing 8x8-bit ints -> s32 matmuls to go through Triton.,[XLA/Triton] Allowing 8x8bit ints > s32 matmuls to go through Triton.,2025-01-27T15:06:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85843
copybara-service[bot],Integrate LLVM at llvm/llvm-project@3c79a04cc231,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 3c79a04cc231,2025-01-27T15:00:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85842
copybara-service[bot],[XLA:CPU] Add more comprehensive tests for dot emitter,[XLA:CPU] Add more comprehensive tests for dot emitter,2025-01-27T14:41:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85841
copybara-service[bot],[pjrt] Added a PjRtMemorySpace* overload for CreateUninitializedBuffer,[pjrt] Added a PjRtMemorySpace* overload for CreateUninitializedBuffer,2025-01-27T14:24:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85840
copybara-service[bot],Integrate LLVM at llvm/llvm-project@77c780d64b95,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 77c780d64b95,2025-01-27T14:22:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85839
copybara-service[bot],[xla:cpu] Move elemental kernel emitter into separate folder under codegen,[xla:cpu] Move elemental kernel emitter into separate folder under codegen FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21845 from ROCm:ci_fix_rocm_triton_test 75e78ad365a9d55f6e299c7b64400447ceebb26d,2025-01-27T13:58:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85838
copybara-service[bot],[XLA:GPU] Add sweep over batch sizes.,[XLA:GPU] Add sweep over batch sizes.,2025-01-27T13:57:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85837
copybara-service[bot],[XLA:GPU] Add operands and flops to matmul table gen.,[XLA:GPU] Add operands and flops to matmul table gen.,2025-01-27T13:17:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85836
pemeliya,[ROCM][NFC] TF-side adaptions required for BlasLt interface refactoring in XLA,This PR is to be merged together with: https://github.com/openxla/xla/pull/21886,2025-01-27T12:30:59Z,comp:gpu size:XS,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85835,"Hi  , Can you please review this PR? Thank you ! ","Closing this PR as it's already merged through https://github.com/tensorflow/tensorflow/commit/62e5733eb8b65530b2321d0d72bc9741e9c67716 Thank you again for the changes, !"
copybara-service[bot],#litert Add accelerator options API.,litert Add accelerator options API.,2025-01-27T12:23:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85834
copybara-service[bot],[XLA:GPU][Emitters] Add TransposeSpec.,"[XLA:GPU][Emitters] Add TransposeSpec. TransposeSpec keeps the input, output shapes and permutation of the transpose + the canonical shapes and permutations, i.e. transpose from     into . Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/22053 from openxla:devel/sm100_mmav2 81a3a27a12502a63bf0c4bcdc71871396306ae8e",2025-01-27T12:08:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85833
copybara-service[bot],Integrate LLVM at llvm/llvm-project@9fecb4f90717,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 9fecb4f90717,2025-01-27T11:25:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85832
copybara-service[bot],Log fused computation from Triton emitter as well,"Log fused computation from Triton emitter as well Makes it easier to get a reproducer, as just logging the fusion does not give us the actual contents of computations within the fusion.",2025-01-27T10:20:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85831
copybara-service[bot],Integrate LLVM at llvm/llvm-project@24f177df61f6,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 24f177df61f6,2025-01-27T09:33:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85830
copybara-service[bot],Allow simple multi-output fusion in the generic Triton emitter.,"Allow simple multioutput fusion in the generic Triton emitter. By simple we mean that we only allow one fusion root without users. We also only allow cases where we can just reuse an existing tiling for the extra fusion roots that is derived from the tiling of the fusion root without users. This already covers quite a few cases, see the new test cases in fusion_emitter_device_test.cc",2025-01-27T09:28:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85829
copybara-service[bot],"fix JAX mac build caused by latest XLA intergrate, by adding py_socket_transfer header dependency for mac also (originally was added only for linux).","fix JAX mac build caused by latest XLA intergrate, by adding py_socket_transfer header dependency for mac also (originally was added only for linux).",2025-01-27T09:24:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85828
copybara-service[bot],[XLA:CPU] Store ThreadSafeModule in LlvmIrKernelSource,[XLA:CPU] Store ThreadSafeModule in LlvmIrKernelSource,2025-01-27T09:17:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85827
copybara-service[bot],[xla:cpu] Rollback convolution_lib extraction because of build breakage.,[xla:cpu] Rollback convolution_lib extraction because of build breakage. Rollback of PR https://github.com/openxla/xla/pull/21854 Reverts e02b43e8c8c42ab46722c1a1523f65be3db4f36a,2025-01-27T09:08:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85826
copybara-service[bot],PR #20155: Add support for emitting a kCall from an async instruction,"PR CC(Find NCCL2 debians in configure.py): Add support for emitting a kCall from an async instruction Imported from GitHub PR https://github.com/openxla/xla/pull/20155 This is the last PR needed to enable the explicit stream annotation feature. This PR enables the IR emitter for kCall subroutines that are wrapped in async{start,done} pairs. For start, the instructions in the kCall are converted to a `SequentialThunk`. This thunk and the inner thunks are then annotated with their respective `execution_stream_ids`. (These ids come from CC([Intel MKL] Finished support for bad usernames in the CI build scripts.))  Copybara import of the project:  41bb90e33eb751953712f0188ef931310a4c58cf by chaserileyroberts : Added support for emitting a kCall from an async instruction  bd8da7b1f3a644544d0050950fa3f08431f3d3ad by chaser : Add compiler test and a WaitFor thunk on main stream.  2d730d148da474fc7f2ddcb977a13ec582a304ec by chaser : Updates based on comments  9b7a8c161e13c5357115b8b6ecdad69033112fc8 by chaser : Updates based on jreiffers comments Merging this change closes CC(Find NCCL2 debians in configure.py) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20155 from chaserileyroberts:chase/async_kcall_emitter 9b7a8c161e13c5357115b8b6ecdad69033112fc8",2025-01-27T08:22:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85825
copybara-service[bot],Update function name and comment.,Update function name and comment. This is to match the changed logic. Also add missing header includes.,2025-01-27T08:05:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85824
copybara-service[bot],PR #21805: Fix typo in cli flag help message,PR CC(Unable to convert frozen graph model to required fromat): Fix typo in cli flag help message Imported from GitHub PR https://github.com/openxla/xla/pull/21805 Copybara import of the project:  ec366967d45ac5e781feb35b4d550341bfb1a194 by Zentrik : Fix typo in cli flag help message Merging this change closes CC(Unable to convert frozen graph model to required fromat) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21805 from Zentrik:patch1 ec366967d45ac5e781feb35b4d550341bfb1a194,2025-01-27T07:14:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85823
copybara-service[bot],"PR #21806: Fix typo in cli flag help message, `destimation` -> `destination`","PR CC(Add whl file to .gitignore.): Fix typo in cli flag help message, `destimation` > `destination` Imported from GitHub PR https://github.com/openxla/xla/pull/21806 Copybara import of the project:  bb52c238d43149f71b5932b84a3bc274de2ba9ec by Zentrik : Fix typo in cli flag help message, `destimation` > `destination` Merging this change closes CC(Add whl file to .gitignore.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21806 from Zentrik:patch2 bb52c238d43149f71b5932b84a3bc274de2ba9ec",2025-01-27T07:13:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85822
copybara-service[bot],PR #21687: Fix executable creation in HloRunnerPjRt,"PR CC(ERROR: /bin/bash: line 1: 12830 Segmentation fault  ): Fix executable creation in HloRunnerPjRt Imported from GitHub PR https://github.com/openxla/xla/pull/21687 The order of evaluating function arguments is unspecified (ref), so pjrt_executable would get released first, leading to seg faults. This caused the following test failures:  all_reduce_test  broadcast_test  gather_operation_test  copy_test  replicated_io_feed_test Copybara import of the project:  1f194436bcbf558475d9ed88ce75013f2d41e686 by Milica Makevic : Fix executable creation in HloRunnerPjRt  3e61f36fe76bdaea9b50c8f103b969390d020137 by Milica Makevic : Pass pjrt executable by value in PjRtWrappedExecutable Merging this change closes CC(ERROR: /bin/bash: line 1: 12830 Segmentation fault  ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21687 from ROCm:ci_fix_hlo_runner_pjrt 3e61f36fe76bdaea9b50c8f103b969390d020137",2025-01-27T07:12:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85821
copybara-service[bot],Update links to emitter code.,Update links to emitter code. The ir and the transforms directory were moved into the emitters directory.,2025-01-27T07:12:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85820
copybara-service[bot],Add missing include,Add missing include,2025-01-27T06:37:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85819
copybara-service[bot],"Add single value get_*, add_* and set_front_end_attribute functions to HloInstruction.","Add single value get_*, add_* and set_front_end_attribute functions to HloInstruction. Added tests for these methods. Eliminates the need to create temporary attribute object when you simply want to add or set one or few attributes. Replace some of the usage.",2025-01-27T05:31:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85818
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T03:44:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85811
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T03:44:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85810
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T03:44:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85809
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T03:43:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85808
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T03:28:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85807
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T03:27:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85806
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T03:25:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85805
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T03:24:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85804
copybara-service[bot],Automated Code Change,Automated Code Change Reverts e02b43e8c8c42ab46722c1a1523f65be3db4f36a,2025-01-27T03:23:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85803
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T03:20:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85802
copybara-service[bot],[MultiHostHloRunner] Introduce GPURunnerProfiler to enable GPU profiling in OSS benchmarking,[MultiHostHloRunner] Introduce GPURunnerProfiler to enable GPU profiling in OSS benchmarking,2025-01-27T03:18:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85801
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20155 from chaserileyroberts:chase/async_kcall_emitter 9b7a8c161e13c5357115b8b6ecdad69033112fc8,2025-01-27T03:18:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85800
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T03:15:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85799
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T03:14:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85798
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21822 from openxla:devel/sm100a 267cf74a084c933e532a622da2485befdc47f8ce,2025-01-27T03:12:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85797
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:42:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85796
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:41:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85795
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:40:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85794
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:39:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85793
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:35:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85792
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:35:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85791
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:31:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85790
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:30:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85789
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:29:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85788
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:27:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85787
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:26:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85786
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:26:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85785
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:24:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85784
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:23:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85783
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:23:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85782
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:23:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85781
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:23:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85780
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-27T02:23:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85779
copybara-service[bot],Add an option to disable predict request output_filter.,Add an option to disable predict request output_filter.,2025-01-27T01:08:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85778
copybara-service[bot],Add an option to forcing skipping output_filter,Add an option to forcing skipping output_filter,2025-01-27T00:01:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85777
copybara-service[bot],[xla:cpu] Extract convolution_lib from a convolution_thunk,[xla:cpu] Extract convolution_lib from a convolution_thunk + Modernize convolution thunk implementation to use CountDownAsyncValueRef directly,2025-01-26T21:48:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85776
copybara-service[bot],[xla:cpu] Remove ACL support from convolution thunk,"[xla:cpu] Remove ACL support from convolution thunk Similar to how xnnpack support is added to XLA, ACL should have its own set of thunks for running convolutions.",2025-01-26T20:38:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85775
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-26T19:17:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85774
rexiliano89,Create cmake-multi-platform.yml,,2025-01-26T18:34:16Z,size:M invalid,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85773
copybara-service[bot],[xla:cpu] Remove code for computing optimal number of workers at run time,"[xla:cpu] Remove code for computing optimal number of workers at run time Instead of trying to figure out optimal number of workers at run time, we'd better have a cost model that can make this decision at compile time based on the XNNPACK fusion.",2025-01-26T18:21:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85772
copybara-service[bot],[xla:cpu:xnn] Take into account operand sizes when deciding if xnn fusion needs a thread pool,[xla:cpu:xnn] Take into account operand sizes when deciding if xnn fusion needs a thread pool,2025-01-26T18:20:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85771
copybara-service[bot],[xla:cpu:cnn] Add ParallelTask structs to improve performance debugging experience,[xla:cpu:cnn] Add ParallelTask structs to improve performance debugging experience Named structs instead of lambdas give a much better debugging experience.,2025-01-26T18:17:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85770
copybara-service[bot],Reverts 3e39e070f9b4dff04da40bdfb870a34e6915d6b8,Reverts 3e39e070f9b4dff04da40bdfb870a34e6915d6b8,2025-01-26T09:11:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85769
copybara-service[bot],Reduce code duplication in collective_permute_valid_iteration_annotator_test,Reduce code duplication in collective_permute_valid_iteration_annotator_test Pointer overload for RunAndCheckHloRewrite.,2025-01-26T07:27:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85768
copybara-service[bot],"Poison ""quantized int"" specializations of std::is_signed, as a first step towards removing them.","Poison ""quantized int"" specializations of std::is_signed, as a first step towards removing them. This specialization is not permitted per the C++ standard, and thus:  MSVC silently ignores specializations of `std::is_signed`, since VS2019 16.4.  Clang emits `error: 'is_signed' cannot be specialized: Users are not allowed to specialize this standard library entity [Winvalidspecialization]` by default, as of Clang 20. Also, modify all relevant callers of `std::is_signed` for which the change triggered a compilation failure to instead call `std::numeric_limits::is_signed`.  Note that `std::numeric_limits` _is_ permitted to be specialized for user types.",2025-01-26T04:36:47Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/85767,"> Poison ""quantized int"" specializations of std::is_signed, as a first step towards removing them. >  > This specialization is not permitted per the C++ standard, and thus: >  > * MSVC silently ignores specializations of `std::is_signed`, since VS2019 16.4. > * Clang emits `error: 'is_signed' cannot be specialized: Users are not allowed to specialize this standard library entity [Winvalidspecialization]` by default, as of Clang 20. >  > Also, modify all relevant callers of `std::is_signed` for which the change triggered a compilation failure to instead call `std::numeric_limits::is_signed`. >  > Note that `std::numeric_limits` _is_ permitted to be specialized for user types."
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T04:14:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85766
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T03:51:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85765
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T03:32:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85764
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T03:30:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85763
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T03:29:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85762
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T03:29:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85761
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T03:27:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85760
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T03:24:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85759
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T03:22:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85758
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T03:21:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85757
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T03:20:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85756
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T03:20:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85755
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T03:19:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85754
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T03:06:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85753
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T02:45:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85752
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T02:43:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85751
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T02:42:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85750
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T02:39:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85749
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T02:37:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85748
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T02:31:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85747
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T02:30:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85746
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T02:27:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85745
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T02:24:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85744
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-26T02:22:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85743
copybara-service[bot],Use both acquire and release semantics on atomic 'pending' counts in SimplePropagatorState.,"Use both acquire and release semantics on atomic 'pending' counts in SimplePropagatorState. We not only have to publish the memory writes of the current thread, but we have to make sure any writes from other threads are visible (if some other thread was executing at the same time).",2025-01-25T22:23:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85742
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T19:38:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85741
copybara-service[bot],Use inspect.getfile instead of inspect.getsourcefile,"Use inspect.getfile instead of inspect.getsourcefile That one is less likely to produce a `None` return value, which is not well supported in this code.",2025-01-25T13:48:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85740
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T10:03:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85739
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T10:00:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85738
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:22:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85737
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:21:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85736
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:20:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85735
copybara-service[bot],Update GraphDef version to 2118.,Update GraphDef version to 2118.,2025-01-25T09:20:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85734
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:20:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85733
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:19:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85732
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:19:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85731
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:19:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85730
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:18:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85729
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:18:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85728
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:18:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85727
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:16:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85726
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:16:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85725
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:16:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85724
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:16:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85723
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:15:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85722
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:15:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85721
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:15:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85720
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:14:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85719
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:14:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85718
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:14:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85717
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:13:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85716
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:13:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85715
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:13:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85714
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-25T09:11:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85713
copybara-service[bot],Temporarily disable mac test due to failure,Temporarily disable mac test due to failure,2025-01-25T07:11:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85712
c8ef,[NFC] Fix some minor typos.,,2025-01-25T03:46:44Z,awaiting review ready to pull size:S,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85711,I'm not sure why the internal checks failed. Can you please clarify?  😳,"I'll have to check tomorrow when I'm at work, since it's on the internal system. See diagrams at https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md","Hi, could you please review again as the internal CI still failed...",This just needs an internal approval.,"Got it, thanks! It seems to have passed now!"
copybara-service[bot],Make buffer manager context getter return a mutable reference.,Make buffer manager context getter return a mutable reference.,2025-01-25T03:44:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85710
copybara-service[bot],Update internal model API to use new internal representation of buffers. Buffer ownership is now managed at the model level.,Update internal model API to use new internal representation of buffers. Buffer ownership is now managed at the model level.,2025-01-25T03:42:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85709
copybara-service[bot],Update buffer manager so that 0 is always an empty buffer.,Update buffer manager so that 0 is always an empty buffer.,2025-01-25T03:42:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85708
copybara-service[bot],PR #21813: Support e8m0fnu for NCCL collectives,PR CC(Develop upstream pre r1.10 v2): Support e8m0fnu for NCCL collectives Imported from GitHub PR https://github.com/openxla/xla/pull/21813 Support e8m0fnu date type for NCCL collectives. Copybara import of the project:  37c5d5baf563b78c7ed0f343b6f1d74c1d9c271b by wenscarl : Support e8m0fnu for NCCL collectives  cd4f37e1019f053dbe6039953a462de637289ddc by Shu Wang : Add missing data placeholder. Merging this change closes CC(Develop upstream pre r1.10 v2) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21813 from wenscarl:e8m0_nccl cd4f37e1019f053dbe6039953a462de637289ddc,2025-01-25T01:57:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85707
copybara-service[bot],Encapsulate edge splitting validation attribute in SourceTargetPairs. ,Encapsulate edge splitting validation attribute in SourceTargetPairs.  Smaller functions in CollectivePermuteCycleDecomposer.,2025-01-25T01:48:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85706
copybara-service[bot],[xla:cpu] Disable concurrency-optimized schedule.,[xla:cpu] Disable concurrencyoptimized schedule. Reverts f16f97e1f86c5b3ce04c644339b4aa17775f8d52,2025-01-25T01:21:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85705
copybara-service[bot],Fix edge case overflow in `tf.raw_ops.RandomIndexShuffle`.,Fix edge case overflow in `tf.raw_ops.RandomIndexShuffle`.,2025-01-25T01:13:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85704
copybara-service[bot],Fix a bug when we have 2 valid live AllocationValues for an HloValue. This comes up with asychronous operations.,Fix a bug when we have 2 valid live AllocationValues for an HloValue. This comes up with asychronous operations.,2025-01-25T01:13:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85703
copybara-service[bot],[xla:cpu] rename LowerTrivialPass to LowerToLLVMPass,"[xla:cpu] rename LowerTrivialPass to LowerToLLVMPass Trivial or not, LowerToLLVM is more descriptive.",2025-01-25T00:39:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85702
copybara-service[bot],Integrate LLVM at llvm/llvm-project@e2005d146194,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match e2005d146194,2025-01-25T00:36:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85701
copybara-service[bot],[xla:cpu] complete lowering of xla_cpu dialect,"[xla:cpu] complete lowering of xla_cpu dialect This completes the lowerings of the existing ops, plus adds the following ops and their lowerings:  xla_cpu.success to set a ""success"" !xla_cpu.error. We'll   probably add a xla_cpu.set_error op in the future.  xla_cpu.thread_id to read the program's thread id. Note that we switch to the greedy pattern rewriter. I also tried the walk pattern rewriter, but it doesn't work well in this case because we rewrite the function signatures, and we end up polluting the IR by casting back and forth between !xla_cpu.call_frame and !llvm.ptr.",2025-01-25T00:02:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85700
copybara-service[bot],Add helper class for better management of tensor and bytecode buffers within the internal model api.,Add helper class for better management of tensor and bytecode buffers within the internal model api.,2025-01-24T23:54:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85699
copybara-service[bot],Reject invalid None in jax.NamedSharding(spec=None).,Reject invalid None in jax.NamedSharding(spec=None).,2025-01-24T23:25:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85698
copybara-service[bot],Reverts afa9dfee28ea8261ad6ec33fe916db0f96102c83,Reverts afa9dfee28ea8261ad6ec33fe916db0f96102c83,2025-01-24T23:23:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85697
copybara-service[bot],Extra debug logging,Extra debug logging,2025-01-24T23:12:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85696
copybara-service[bot],Update TFRT dependency to use revision,Update TFRT dependency to use revision http://github.com/tensorflow/runtime/commit/a9d01348d51b84f597c16e2b0fdbd8602ea02310.,2025-01-24T22:48:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85695
copybara-service[bot],[XLA:GPU] Remove IsForwardCycle and IsBackwardCycle as they are not used.,[XLA:GPU] Remove IsForwardCycle and IsBackwardCycle as they are not used.,2025-01-24T22:05:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85694
copybara-service[bot],Check signature has a valid subgraph ind before getting subgraph.,Check signature has a valid subgraph ind before getting subgraph.,2025-01-24T21:47:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85693
copybara-service[bot],Support `mhlo.sharding` attr inside `backend_config` of ragged_all_to_all,Support `mhlo.sharding` attr inside `backend_config` of ragged_all_to_all,2025-01-24T21:11:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85692
copybara-service[bot],Refactor IsXSpaceGrouped to verify that all device and host planes are grouped.,Refactor IsXSpaceGrouped to verify that all device and host planes are grouped.,2025-01-24T21:09:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85691
copybara-service[bot],Make Entry::state atomic so that it can be safely used to guard access to result tensors on non x86 machines.,Make Entry::state atomic so that it can be safely used to guard access to result tensors on non x86 machines.,2025-01-24T20:47:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85690
LinuxPersonEC,"TF 2.18 with GPU does not detect GPU, Cannot dlopen some GPU libraries, in a container"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code Yes  OS platform and distribution Linux Centos 7.9, RHEL 8, RHEL 9  Mobile device _No response_  Python version 3.11.0rc1  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 550.90.07  GPU model and memory _No response_  Current behavior? After discussing this on the Apptainer Git we determined the latest TFGPU running 2.18.0 does not register any GPUs. Older versions like 2.7.1gpu work just fine. `apptainer run nv  /apps/Miniforge/lib/python3.12/sitepackages/containers/tensorflow/tensorflow/latestgpu/tensorflowtensorflowlatestgpusha256\:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif python` ``` import tensorflow as tf 20250124 15:03:27.629215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1737749008.639844   35316 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1737749008.847756   35316 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250124 15:03:31.499335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. ``` ``` print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) W0000 00:00:1737749068.599039   35316 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... Num GPUs Available:  0 ``` ``` >>> print(tf.__version__) 2.18.0 ```  Standalone code to reproduce the issue ```shell shpc install tensorflow/tensorflow:latestgpu or apptainer pull docker://tensorflow/tensorflow:latestgpu apptainer run nv  /apps/Miniforge/lib/python3.12/sitepackages/containers/tensorflow/tensorflow/latestgpu/tensorflowtensorflowlatestgpusha256\:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif python python import tensorflow as tf print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) ```  Relevant log output ```shell apptainer debug exec nv  /apps/Miniforge/lib/python3.12/sitepackages/containers/tensorflow/tensorflow/latestgpu/tensorflowtensorflowlatestgpusha256:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif python DEBUG   [U=0,P=1355208]    persistentPreRun()            Apptainer version: 1.3.61 DEBUG   [U=0,P=1355208]    persistentPreRun()            Parsing configuration file /etc/apptainer/apptainer.conf DEBUG   [U=0,P=1355208]    SetBinaryPath()               Setting binary path to /usr/libexec/apptainer/bin:/usr/share/Modules/bin:/usr/local/sbin:/sbin:/bin:/usr/sbin:/usr/bin:/opt/TurboVNC/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin DEBUG   [U=0,P=1355208]    SetBinaryPath()               Using that path for all binaries DEBUG   [U=0,P=1355208]    handleConfDir()               /root/.apptainer already exists. Not creating. DEBUG   [U=0,P=1355208]    handleRemoteConf()            Ensuring file permission of 0600 on /root/.apptainer/remote.yaml DEBUG   [U=0,P=1355208]    setUmask()                    Saving umask 0002 for propagation into container DEBUG   [U=0,P=1355208]    checkEncryptionKey()          Checking for encrypted system partition DEBUG   [U=0,P=1355208]    Init()                        Image format detection DEBUG   [U=0,P=1355208]    Init()                        Check for sandbox image format DEBUG   [U=0,P=1355208]    Init()                        sandbox format initializer returned: not a directory image DEBUG   [U=0,P=1355208]    Init()                        Check for sif image format DEBUG   [U=0,P=1355208]    Init()                        sif image format detected VERBOSE [U=0,P=1355208]    SetGPUConfig()                'always use nv = yes' found in apptainer.conf DEBUG   [U=0,P=1355208]    setNVLegacyConfig()           Using legacy binds for nv GPU setup VERBOSE [U=0,P=1355208]    NvidiaIpcsPath()              persistenced socket /var/run/nvidiapersistenced/socket not found DEBUG   [U=0,P=1355208]    findOnPath()                  Found ""ldconfig"" at ""/sbin/ldconfig"" DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SHELL environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_GID environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HISTCONTROL environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding no_proxy environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HOSTNAME environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HISTSIZE environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SBATCH_PARTITION environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_OUTPUT environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SLURM_PARTITION environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_COMMAND environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_USER environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_DIR environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding PWD environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LOGNAME environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULESHOME environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MANPATH environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_RESTORE environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding __MODULES_SHARE_MANPATH environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SSH_ASKPASS environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LANG environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LS_COLORS environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_SETTARG_FULL_SUPPORT environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_PS1 environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding https_proxy environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_VERSION environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULEPATH_ROOT environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_PKG environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding TERM environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LESSOPEN environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding USER environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding NO_PROXY environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULES_RUN_QUARANTINE environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LOADEDMODULES environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SHLVL environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_ENV environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_sys environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HTTPS_PROXY environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_INIT environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HTTP_PROXY environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding http_proxy environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding S_COLORS environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding __MODULES_LMINIT environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding which_declare environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding XDG_DATA_DIRS environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULEPATH environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_UID environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_CMD environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MAIL environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULES_CMD environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_ml%% environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_which%% environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_module%% environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_scl%% environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC__module_raw%% environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding _ environment variable VERBOSE [U=0,P=1355208]    SetContainerEnv()             Not forwarding APPTAINER_DEBUG environment variable DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding USER_PATH environment variable VERBOSE [U=0,P=1355208]    SetContainerEnv()             Setting HOME=/root VERBOSE [U=0,P=1355208]    SetContainerEnv()             Setting PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin DEBUG   [U=0,P=1355208]    InitImageDrivers()            Skipping installing fuseapps image driver because running as root DEBUG   [U=0,P=1355208]    SetuidMountAllowed()          Kernel squashfs mount allowed because running as root DEBUG   [U=0,P=1355208]    init()                        Use starter binary /usr/libexec/apptainer/bin/starter VERBOSE [U=0,P=1355208]    print()                       Set messagelevel to: 5 VERBOSE [U=0,P=1355208]    init()                        Starter initialization VERBOSE [U=0,P=1355208]    is_suid()                     Check if we are running as setuid: 0 DEBUG   [U=0,P=1355208]    read_engine_config()          Read engine configuration DEBUG   [U=0,P=1355208]    init()                        Wait completion of stage1 DEBUG   [U=0,P=1355224]    set_parent_death_signal()     Set parent death signal to 9 VERBOSE [U=0,P=1355224]    init()                        Spawn stage 1 DEBUG   [U=0,P=1355224]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter DEBUG   [U=0,P=1355224]    func1()                       starter was not relocated from /usr/libexec DEBUG   [U=0,P=1355224]    func1()                       Install prefix is /usr DEBUG   [U=0,P=1355224]    startup()                     apptainer runtime engine selected VERBOSE [U=0,P=1355224]    startup()                     Execute stage 1 DEBUG   [U=0,P=1355224]    StageOne()                    Entering stage 1 DEBUG   [U=0,P=1355224]    InitImageDrivers()            Skipping installing fuseapps image driver because running as root DEBUG   [U=0,P=1355224]    prepareRootCaps()             Root full capabilities DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/proc/sys/fs/binfmt_misc"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/home"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/share"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/misc"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/net"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/mnt/smb/locker"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/mnt/smb/labshare"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/mnt/smb/staging"" as autofs mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for bind path /etc/localtime: no mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for bind path /etc/hosts: no mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for home directory /root: no mount point DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for current working directory /root: no mount point DEBUG   [U=0,P=1355224]    Init()                        Image format detection DEBUG   [U=0,P=1355224]    Init()                        Check for sandbox image format DEBUG   [U=0,P=1355224]    Init()                        sandbox format initializer returned: not a directory image DEBUG   [U=0,P=1355224]    Init()                        Check for sif image format DEBUG   [U=0,P=1355224]    Init()                        sif image format detected DEBUG   [U=0,P=1355224]    setSessionLayer()             Using overlay because it is not disabled DEBUG   [U=0,P=1355224]    PrepareConfig()               image driver is  VERBOSE [U=0,P=1355208]    wait_child()                  stage 1 exited with status 0 DEBUG   [U=0,P=1355208]    cleanup_fd()                  Close file descriptor 4 DEBUG   [U=0,P=1355208]    cleanup_fd()                  Close file descriptor 5 DEBUG   [U=0,P=1355208]    cleanup_fd()                  Close file descriptor 6 DEBUG   [U=0,P=1355208]    init()                        Set child signal mask DEBUG   [U=0,P=1355208]    init()                        Create socketpair for master communication channel DEBUG   [U=0,P=1355208]    init()                        Create RPC socketpair for communication between stage 2 and RPC server VERBOSE [U=0,P=1355208]    init()                        Spawn master process DEBUG   [U=0,P=1355230]    set_parent_death_signal()     Set parent death signal to 9 VERBOSE [U=0,P=1355230]    create_namespace()            Create mount namespace VERBOSE [U=0,P=1355208]    enter_namespace()             Entering in mount namespace DEBUG   [U=0,P=1355208]    enter_namespace()             Opening namespace file ns/mnt VERBOSE [U=0,P=1355230]    create_namespace()            Create mount namespace VERBOSE [U=0,P=1355231]    init()                        Spawn RPC server DEBUG   [U=0,P=1355208]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter DEBUG   [U=0,P=1355208]    func1()                       starter was not relocated from /usr/libexec DEBUG   [U=0,P=1355208]    func1()                       Install prefix is /usr DEBUG   [U=0,P=1355231]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter DEBUG   [U=0,P=1355231]    func1()                       starter was not relocated from /usr/libexec DEBUG   [U=0,P=1355231]    func1()                       Install prefix is /usr DEBUG   [U=0,P=1355208]    startup()                     apptainer runtime engine selected VERBOSE [U=0,P=1355208]    startup()                     Execute master process DEBUG   [U=0,P=1355231]    startup()                     apptainer runtime engine selected VERBOSE [U=0,P=1355231]    startup()                     Serve RPC requests DEBUG   [U=0,P=1355208]    InitImageDrivers()            Skipping installing fuseapps image driver because running as root DEBUG   [U=0,P=1355208]    setupSessionLayout()          Using Layer system: overlay DEBUG   [U=0,P=1355208]    setupOverlayLayout()          Creating overlay SESSIONDIR layout DEBUG   [U=0,P=1355208]    addRootfsMount()              Mount rootfs in readonly mode DEBUG   [U=0,P=1355208]    addRootfsMount()              Image type is 4096 DEBUG   [U=0,P=1355208]    addRootfsMount()              Mounting block [squashfs] image: /share/apps/Miniforge/lib/python3.12/sitepackages/containers/tensorflow/tensorflow/latestgpu/tensorflowtensorflowlatestgpusha256:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif DEBUG   [U=0,P=1355208]    addKernelMount()              Checking configuration file for 'mount proc' DEBUG   [U=0,P=1355208]    addKernelMount()              Adding proc to mount list VERBOSE [U=0,P=1355208]    addKernelMount()              Default mount: /proc:/proc DEBUG   [U=0,P=1355208]    addKernelMount()              Checking configuration file for 'mount sys' DEBUG   [U=0,P=1355208]    addKernelMount()              Adding sysfs to mount list VERBOSE [U=0,P=1355208]    addKernelMount()              Default mount: /sys:/sys DEBUG   [U=0,P=1355208]    addDevMount()                 Checking configuration file for 'mount dev' DEBUG   [U=0,P=1355208]    addDevMount()                 Adding dev to mount list VERBOSE [U=0,P=1355208]    addDevMount()                 Default mount: /dev:/dev DEBUG   [U=0,P=1355208]    addHostMount()                Not mounting host file systems per configuration VERBOSE [U=0,P=1355208]    addBindsMount()               Found 'bind path' = /etc/localtime, /etc/localtime VERBOSE [U=0,P=1355208]    addBindsMount()               Found 'bind path' = /etc/hosts, /etc/hosts DEBUG   [U=0,P=1355208]    addHomeStagingDir()           Staging home directory (/root) at /var/lib/apptainer/mnt/session/root DEBUG   [U=0,P=1355208]    addHomeMount()                Adding home directory mount [/var/lib/apptainer/mnt/session/root:/root] to list using layer: overlay DEBUG   [U=0,P=1355208]    addTmpMount()                 Checking for 'mount tmp' in configuration file DEBUG   [U=0,P=1355208]    addScratchMount()             Not mounting scratch directory: Not requested DEBUG   [U=0,P=1355208]    addLibsMount()                Checking for 'user bind control' in configuration file DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenCL.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenGL.so.0 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiacfg.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libEGL.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaeglcore.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaml.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvcuvid.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiagtk3.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaglvkspirv.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libcudadebugger.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv2.so.2 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaptxjitcompiler.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv2.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGL.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLX_nvidia.so.0 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv1_CM.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiatls.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLdispatch.so.0 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaencode.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenCL.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaptxjitcompiler.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaencode.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidianvvm.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaglsi.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaopticalflow.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaopencl.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaeglwayland.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLX.so.0 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvoptix.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiagpucomp.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGL.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv2_nvidia.so.2 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv1_CM_nvidia.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidianvvm.so.4 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libEGL.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiagtk2.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaml.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiafbc.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiafbc.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvcuvid.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaopticalflow.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLX.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv1_CM.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiacfg.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libcuda.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libvdpau_nvidia.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiartcore.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libcuda.so.1 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenGL.so to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libEGL_nvidia.so.0 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidiaglcore.so.550.120 to mount list DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLdispatch.so to mount list DEBUG   [U=0,P=1355208]    addFilesMount()               Checking for 'user bind control' in configuration file DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidiapersistenced:/usr/bin/nvidiapersistenced to mount list DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidiacudampscontrol:/usr/bin/nvidiacudampscontrol to mount list DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidiacudampsserver:/usr/bin/nvidiacudampsserver to mount list DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidiasmi:/usr/bin/nvidiasmi to mount list DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidiadebugdump:/usr/bin/nvidiadebugdump to mount list DEBUG   [U=0,P=1355208]    addResolvConfMount()          Adding /etc/resolv.conf to mount list VERBOSE [U=0,P=1355208]    addResolvConfMount()          Default mount: /etc/resolv.conf:/etc/resolv.conf DEBUG   [U=0,P=1355208]    addHostnameMount()            Skipping hostname mount, not virtualizing UTS namespace on user request DEBUG   [U=0,P=1355208]    create()                      Mount all DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting tmpfs to /var/lib/apptainer/mnt/session DEBUG   [U=0,P=1355208]    mountImage()                  Mounting loop device /dev/loop0 to /var/lib/apptainer/mnt/session/rootfs of type squashfs DEBUG   [U=0,P=1355208]    createCwdDir()                Using /root as current working directory DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting overlay to /var/lib/apptainer/mnt/session/final DEBUG   [U=0,P=1355208]    mountGeneric()                Unmounting and remounting overlay DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final DEBUG   [U=0,P=1355208]    setPropagationMount()         Set RPC mount propagation flag to SLAVE VERBOSE [U=0,P=1355208]    Passwd()                      Checking for template passwd file: /var/lib/apptainer/mnt/session/rootfs/etc/passwd VERBOSE [U=0,P=1355208]    Passwd()                      Creating passwd content VERBOSE [U=0,P=1355208]    Passwd()                      Creating template passwd file and injecting user data: /var/lib/apptainer/mnt/session/rootfs/etc/passwd DEBUG   [U=0,P=1355208]    addIdentityMount()            Adding /etc/passwd to mount list VERBOSE [U=0,P=1355208]    addIdentityMount()            Default mount: /etc/passwd:/etc/passwd VERBOSE [U=0,P=1355208]    Group()                       Checking for template group file: /var/lib/apptainer/mnt/session/rootfs/etc/group VERBOSE [U=0,P=1355208]    Group()                       Creating group content DEBUG   [U=0,P=1355208]    addIdentityMount()            Adding /etc/group to mount list VERBOSE [U=0,P=1355208]    addIdentityMount()            Default mount: /etc/group:/etc/group DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /dev to /var/lib/apptainer/mnt/session/final/dev DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /etc/localtime to /var/lib/apptainer/mnt/session/final/etc/localtime DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/etc/localtime DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /etc/hosts to /var/lib/apptainer/mnt/session/final/etc/hosts DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/etc/hosts DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /proc to /var/lib/apptainer/mnt/session/final/proc DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/proc DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting sysfs to /var/lib/apptainer/mnt/session/final/sys DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /root to /var/lib/apptainer/mnt/session/root DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/root DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/root to /var/lib/apptainer/mnt/session/final/root DEBUG   [U=0,P=1355208]    func1()                       Container /tmp resolves to ""/tmp"" DEBUG   [U=0,P=1355208]    func1()                       Container /var/tmp resolves to ""/var/tmp"" VERBOSE [U=0,P=1355208]    func1()                       Default mount: /tmp:/tmp VERBOSE [U=0,P=1355208]    func1()                       Default mount: /var/tmp:/var/tmp DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /tmp to /var/lib/apptainer/mnt/session/final/tmp DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/tmp DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/tmp to /var/lib/apptainer/mnt/session/final/var/tmp DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/var/tmp DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenCL.so to /var/lib/apptainer/mnt/session/libs/libOpenCL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenCL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenGL.so.0 to /var/lib/apptainer/mnt/session/libs/libOpenGL.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenGL.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiacfg.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiacfg.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiacfg.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libEGL.so.1 to /var/lib/apptainer/mnt/session/libs/libEGL.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libEGL.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaeglcore.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiaeglcore.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaeglcore.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaml.so to /var/lib/apptainer/mnt/session/libs/libnvidiaml.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaml.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvcuvid.so.1 to /var/lib/apptainer/mnt/session/libs/libnvcuvid.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvcuvid.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiagtk3.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiagtk3.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiagtk3.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaglvkspirv.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiaglvkspirv.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaglvkspirv.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libcudadebugger.so.1 to /var/lib/apptainer/mnt/session/libs/libcudadebugger.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libcudadebugger.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv2.so.2 to /var/lib/apptainer/mnt/session/libs/libGLESv2.so.2 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv2.so.2 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaptxjitcompiler.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiaptxjitcompiler.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaptxjitcompiler.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv2.so to /var/lib/apptainer/mnt/session/libs/libGLESv2.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv2.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGL.so to /var/lib/apptainer/mnt/session/libs/libGL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLX_nvidia.so.0 to /var/lib/apptainer/mnt/session/libs/libGLX_nvidia.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLX_nvidia.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv1_CM.so.1 to /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiatls.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiatls.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiatls.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLdispatch.so.0 to /var/lib/apptainer/mnt/session/libs/libGLdispatch.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLdispatch.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaencode.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiaencode.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaencode.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenCL.so.1 to /var/lib/apptainer/mnt/session/libs/libOpenCL.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenCL.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaptxjitcompiler.so to /var/lib/apptainer/mnt/session/libs/libnvidiaptxjitcompiler.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaptxjitcompiler.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaencode.so to /var/lib/apptainer/mnt/session/libs/libnvidiaencode.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaencode.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidianvvm.so to /var/lib/apptainer/mnt/session/libs/libnvidianvvm.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidianvvm.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaglsi.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiaglsi.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaglsi.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaopticalflow.so to /var/lib/apptainer/mnt/session/libs/libnvidiaopticalflow.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaopticalflow.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaopencl.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiaopencl.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaopencl.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaeglwayland.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiaeglwayland.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaeglwayland.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLX.so.0 to /var/lib/apptainer/mnt/session/libs/libGLX.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLX.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvoptix.so.1 to /var/lib/apptainer/mnt/session/libs/libnvoptix.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvoptix.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiagpucomp.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiagpucomp.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiagpucomp.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGL.so.1 to /var/lib/apptainer/mnt/session/libs/libGL.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGL.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv2_nvidia.so.2 to /var/lib/apptainer/mnt/session/libs/libGLESv2_nvidia.so.2 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv2_nvidia.so.2 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv1_CM_nvidia.so.1 to /var/lib/apptainer/mnt/session/libs/libGLESv1_CM_nvidia.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv1_CM_nvidia.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidianvvm.so.4 to /var/lib/apptainer/mnt/session/libs/libnvidianvvm.so.4 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidianvvm.so.4 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libEGL.so to /var/lib/apptainer/mnt/session/libs/libEGL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libEGL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiagtk2.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiagtk2.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiagtk2.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaml.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiaml.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaml.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiafbc.so to /var/lib/apptainer/mnt/session/libs/libnvidiafbc.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiafbc.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiafbc.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiafbc.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiafbc.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvcuvid.so to /var/lib/apptainer/mnt/session/libs/libnvcuvid.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvcuvid.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaopticalflow.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidiaopticalflow.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaopticalflow.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLX.so to /var/lib/apptainer/mnt/session/libs/libGLX.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLX.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv1_CM.so to /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiacfg.so to /var/lib/apptainer/mnt/session/libs/libnvidiacfg.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiacfg.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libcuda.so to /var/lib/apptainer/mnt/session/libs/libcuda.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libcuda.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libvdpau_nvidia.so to /var/lib/apptainer/mnt/session/libs/libvdpau_nvidia.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libvdpau_nvidia.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiartcore.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiartcore.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiartcore.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libcuda.so.1 to /var/lib/apptainer/mnt/session/libs/libcuda.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libcuda.so.1 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenGL.so to /var/lib/apptainer/mnt/session/libs/libOpenGL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenGL.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libEGL_nvidia.so.0 to /var/lib/apptainer/mnt/session/libs/libEGL_nvidia.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libEGL_nvidia.so.0 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidiaglcore.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidiaglcore.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidiaglcore.so.550.120 DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLdispatch.so to /var/lib/apptainer/mnt/session/libs/libGLdispatch.so DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLdispatch.so DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/libs to /var/lib/apptainer/mnt/session/final/.singularity.d/libs DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/.singularity.d/libs DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidiapersistenced to /var/lib/apptainer/mnt/session/final/usr/bin/nvidiapersistenced DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidiapersistenced DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidiacudampscontrol to /var/lib/apptainer/mnt/session/final/usr/bin/nvidiacudampscontrol DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidiacudampscontrol DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidiacudampsserver to /var/lib/apptainer/mnt/session/final/usr/bin/nvidiacudampsserver DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidiacudampsserver DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidiasmi to /var/lib/apptainer/mnt/session/final/usr/bin/nvidiasmi DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidiasmi DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidiadebugdump to /var/lib/apptainer/mnt/session/final/usr/bin/nvidiadebugdump DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidiadebugdump DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/etc/resolv.conf to /var/lib/apptainer/mnt/session/final/etc/resolv.conf DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/etc/passwd to /var/lib/apptainer/mnt/session/final/etc/passwd DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/etc/group to /var/lib/apptainer/mnt/session/final/etc/group VERBOSE [U=0,P=1355208]    addCwdMount()                 /root found within container DEBUG   [U=0,P=1355208]    create()                      Chroot into /var/lib/apptainer/mnt/session/final DEBUG   [U=0,P=1355231]    Chroot()                      Hold reference to host / directory DEBUG   [U=0,P=1355231]    Chroot()                      Called pivot_root on /var/lib/apptainer/mnt/session/final DEBUG   [U=0,P=1355231]    Chroot()                      Change current directory to host / directory DEBUG   [U=0,P=1355231]    Chroot()                      Apply slave mount propagation for host / directory DEBUG   [U=0,P=1355231]    Chroot()                      Called unmount(/, syscall.MNT_DETACH) DEBUG   [U=0,P=1355231]    Chroot()                      Changing directory to / to avoid getpwd issues DEBUG   [U=0,P=1355208]    create()                      Chdir into / to avoid errors VERBOSE [U=0,P=1355230]    wait_child()                  rpc server exited with status 0 DEBUG   [U=0,P=1355230]    init()                        Set container privileges DEBUG   [U=0,P=1355230]    apply_privileges()            Effective capabilities:   0x000001ffffffffff DEBUG   [U=0,P=1355230]    apply_privileges()            Permitted capabilities:   0x000001ffffffffff DEBUG   [U=0,P=1355230]    apply_privileges()            Bounding capabilities:    0x000001ffffffffff DEBUG   [U=0,P=1355230]    apply_privileges()            Inheritable capabilities: 0x000001ffffffffff DEBUG   [U=0,P=1355230]    apply_privileges()            Ambient capabilities:     0x000001ffffffffff DEBUG   [U=0,P=1355230]    apply_privileges()            Set user ID to 0 DEBUG   [U=0,P=1355230]    set_parent_death_signal()     Set parent death signal to 9 DEBUG   [U=0,P=1355230]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter DEBUG   [U=0,P=1355230]    func1()                       executablePath does not exist, assuming default prefix DEBUG   [U=0,P=1355230]    startup()                     apptainer runtime engine selected VERBOSE [U=0,P=1355230]    startup()                     Execute stage 2 DEBUG   [U=0,P=1355230]    StageTwo()                    Entering stage 2 DEBUG   [U=0,P=1355230]    StartProcess()                Setting umask in container to 0002 DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC__module_raw%%"" to container environment: invalid key DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_ml%%"" to container environment: invalid key DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_module%%"" to container environment: invalid key DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_scl%%"" to container environment: invalid key DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_which%%"" to container environment: invalid key DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/01base.sh DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/10docker2singularity.sh DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/90environment.sh DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/94appsbase.sh DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/95apps.sh DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/99base.sh DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/99runtimevars.sh DEBUG   [U=0,P=1355230]    sylogBuiltin()                Running action command exec DEBUG   [U=0,P=1355208]    PostStartProcess()            Post start process Python 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. ```",2025-01-24T20:16:31Z,stat:awaiting tensorflower type:build/install comp:gpu TF 2.18,open,0,3,https://github.com/tensorflow/tensorflow/issues/85689,The docker  TF container tries to load libcudnn.so.9 However the container has only been built with libcudnn.so.8 More detail here: tensorflow 2.18 requires libcudnn.so.9,"> The docker TF container tries to load libcudnn.so.9 However the container has only been built with libcudnn.so.8 >  > More detail here: tensorflow 2.18 requires libcudnn.so.9 Thanks, how do we get the maintained to flx if?",", I request you to take a look at this issue where a similar feature has been proposed and it is still open. Also I request to follow the similar feature which has been proposed to have the updates on the similar issue. Thank you!"
copybara-service[bot],Update Bazel/CMake to use a version of XNNPACK that defines `xnn_define_static_slice_v3()`,Update Bazel/CMake to use a version of XNNPACK that defines `xnn_define_static_slice_v3()`,2025-01-24T20:04:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85688
copybara-service[bot],[pjrt] Use the `PjRtMemorySpace` version of `BufferFromHostBuffer`,[pjrt] Use the `PjRtMemorySpace` version of `BufferFromHostBuffer` Buffers live in memory spaces and not on devices. The `PjRtDevice` version of `BufferFromHostBuffer` is deprecated and will be removed once the migration is complete.,2025-01-24T20:02:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85687
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@c27ba678,Integrate StableHLO at openxla/stablehlo Reverts a47a28e840cf97148669ba3483cd72e87f0efa5b,2025-01-24T20:02:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85686
copybara-service[bot],"Make `DynamicShape`, `Sharding`, and `ArraySpec` hashable","Make `DynamicShape`, `Sharding`, and `ArraySpec` hashable",2025-01-24T20:00:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85685
copybara-service[bot],Add test case for async execution of Pixel Dispatch API,Add test case for async execution of Pixel Dispatch API,2025-01-24T19:20:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85684
copybara-service[bot],[xla:cpu] Add a simple heuristic to decide if XNNPACK fusion should use a thread pool,[xla:cpu] Add a simple heuristic to decide if XNNPACK fusion should use a thread pool,2025-01-24T19:16:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85683
copybara-service[bot],Add a workaround for gsutil not working properly on MSYS2.,Add a workaround for gsutil not working properly on MSYS2.,2025-01-24T19:04:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85682
copybara-service[bot],[MultiHostHloRunner] Add GPU profiler support to multihost_hlo_runner,[MultiHostHloRunner] Add GPU profiler support to multihost_hlo_runner,2025-01-24T18:59:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85681
copybara-service[bot],[xla:cpu] Add XnnFusionThunk options to be able to run without a thread pool,[xla:cpu] Add XnnFusionThunk options to be able to run without a thread pool,2025-01-24T18:40:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85680
copybara-service[bot],Implement on_device_shape() and logical_on_device_shape().,Implement on_device_shape() and logical_on_device_shape(). FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21375 from shraiysh:while_loop_analysis a435fbd2eadc17269d7bccbe141dcf7a21cc20e8,2025-01-24T18:25:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85679
copybara-service[bot],Add GitHub Action for XLA CPU CI tests to test new self-hosted runners,Add GitHub Action for XLA CPU CI tests to test new selfhosted runners,2025-01-24T18:06:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85678
copybara-service[bot],[XLA:CPU] Remove special case code in ElementalKernelEmitter,[XLA:CPU] Remove special case code in ElementalKernelEmitter,2025-01-24T17:41:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85677
copybara-service[bot],Modify the TFLite delegate for XNNPACK to use the new `xnn_define_static_slice_v3()` API for slice and strided-slice ops.,Modify the TFLite delegate for XNNPACK to use the new `xnn_define_static_slice_v3()` API for slice and stridedslice ops.,2025-01-24T17:41:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85676
copybara-service[bot],[XLA:CPU] Move dot emitter to new kernel API,[XLA:CPU] Move dot emitter to new kernel API,2025-01-24T17:37:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85675
copybara-service[bot],[xla:cpu] Fix build errors from ACL,[xla:cpu] Fix build errors from ACL Reported in: https://github.com/jaxml/jax/issues/26062,2025-01-24T17:21:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85674
copybara-service[bot],[StableHLO] Bugfix to disable reorder around reshape/broadcast with dynamic shape,[StableHLO] Bugfix to disable reorder around reshape/broadcast with dynamic shape,2025-01-24T17:20:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85673
copybara-service[bot],Serialize calls to NeuronCompilation_finish with a mutex,"Serialize calls to NeuronCompilation_finish with a mutex According to MTK, NeuronCompilation_finish is not thread safe.",2025-01-24T16:59:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85672
copybara-service[bot],[XLA:LAYOUT_ASSIGNMENT] assign custom call layouts with high priority.,[XLA:LAYOUT_ASSIGNMENT] assign custom call layouts with high priority. Reverts changelist 675348603,2025-01-24T14:35:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85671
copybara-service[bot],Fix `LITERT_ASSIGN_OR_RETURN` for move only types.,Fix `LITERT_ASSIGN_OR_RETURN` for move only types.,2025-01-24T14:31:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85670
copybara-service[bot],#litert Add `LiteRtCompareApiVersion` function.,litert Add `LiteRtCompareApiVersion` function.,2025-01-24T14:29:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85669
Chiebuka-Chibuike2024,difficulty installing tensorflow," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution Windows 11  Mobile device _No response_  Python version 3.12.2  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I expected to import TensorFlow, but I keep getting the error message below: Traceback (most recent call last):   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\Users\user\Desktop\Ban6440Milestone2assignment.py"", line 77, in      import tensorflow as tf   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 85, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.  Standalone code to reproduce the issue ```shell import tensorflow as tf ```  Relevant log output ```shell Traceback (most recent call last):   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""C:\Users\user\Desktop\Ban6440Milestone2assignment.py"", line 77, in      import tensorflow as tf   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\__init__.py"", line 40, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 85, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```",2025-01-24T14:00:16Z,type:build/install,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85668,Are you satisfied with the resolution of your issue? Yes No,Please perform a search for similar issues.
Chuan1937,tensorflow takes a long time to prepare before the first iteration," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version TF 2.10.0  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 11.4/8.9.1  GPU model and memory Nvidia Tesla K20m  Current behavior? tensorflow takes a long time to prepare before the first iteration.I used my custom model for training, but it took 4060 minutes from the time the data was ready to the first iteration. This was true even for a very small dataset. And my model only had 835,620 parameters. This model is used to pick up the phase of seismic data. If an experiment is conducted, the data can be fabricated by itself.  Standalone code to reproduce the issue ```shell import numpy as np import matplotlib matplotlib.use('agg') import os os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'   Suppress TensorFlow logs from tensorflow.python import keras from tensorflow.keras import backend as K from tensorflow.keras.layers import (     Add, Activation, LSTM, Conv1D, MaxPooling1D, UpSampling1D,     Cropping1D, SpatialDropout1D, Bidirectional, BatchNormalization,add,InputSpec, LayerNormalization,Layer, Dense, Dropout,Layer ) from tensorflow.keras.optimizers import Adam from tensorflow import keras import tensorflow as tf from tensorflow.keras import initializers, regularizers, constraints, activations def f1(y_true, y_pred):     def recall(y_true, y_pred):         '''Recall metric. Computes the recall, a metric for multilabel classification.'''         true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))         possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))         recall = true_positives / (possible_positives + tf.keras.backend.epsilon())         return recall     def precision(y_true, y_pred):         '''Precision metric. Computes the precision, a metric for multilabel classification.'''         true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))         predicted_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_pred, 0, 1)))         precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())         return precision     precision_val = precision(y_true, y_pred)     recall_val = recall(y_true, y_pred)      F1 score calculation     return 2 * (precision_val * recall_val) / (precision_val + recall_val + tf.keras.backend.epsilon()) class LayerNormalization(keras.layers.Layer):     def __init__(self,                  center=True,                  scale=True,                  epsilon=None,                  gamma_initializer='ones',                  beta_initializer='zeros',                  **kwargs):         super(LayerNormalization, self).__init__(**kwargs)         self.supports_masking = True         self.center = center         self.scale = scale         if epsilon is None:             epsilon = K.epsilon() * K.epsilon()         self.epsilon = epsilon         self.gamma_initializer = keras.initializers.get(gamma_initializer)         self.beta_initializer = keras.initializers.get(beta_initializer)     def get_config(self):         config = {             'center': self.center,             'scale': self.scale,             'epsilon': self.epsilon,             'gamma_initializer': keras.initializers.serialize(self.gamma_initializer),             'beta_initializer': keras.initializers.serialize(self.beta_initializer),         }         base_config = super(LayerNormalization, self).get_config()         return dict(list(base_config.items()) + list(config.items()))     def compute_output_shape(self, input_shape):         return input_shape     def compute_mask(self, inputs, input_mask=None):         return input_mask     def build(self, input_shape):         self.input_spec = InputSpec(shape=input_shape)         shape = input_shape[1:]         if self.scale:             self.gamma = self.add_weight(                 shape=shape,                 initializer=self.gamma_initializer,                 name='gamma',             )         if self.center:             self.beta = self.add_weight(                 shape=shape,                 initializer=self.beta_initializer,                 name='beta',             )         super(LayerNormalization, self).build(input_shape)     def call(self, inputs, training=None):         mean = K.mean(inputs, axis=1, keepdims=True)         variance = K.mean(K.square(inputs  mean), axis=1, keepdims=True)         std = K.sqrt(variance + self.epsilon)         outputs = (inputs  mean) / std         if self.scale:             outputs *= self.gamma         if self.center:             outputs += self.beta         return outputs class FeedForward(keras.layers.Layer):     def __init__(self,                  units,                  activation='relu',                  use_bias=True,                  kernel_initializer='glorot_normal',                  bias_initializer='zeros',                  dropout_rate=0.0,                  **kwargs):         self.supports_masking = True         self.units = units         self.activation = keras.activations.get(activation)         self.use_bias = use_bias         self.kernel_initializer = keras.initializers.get(kernel_initializer)         self.bias_initializer = keras.initializers.get(bias_initializer)         self.dropout_rate = dropout_rate         self.W1, self.b1 = None, None         self.W2, self.b2 = None, None         super(FeedForward, self).__init__(**kwargs)     def get_config(self):         config = {             'units': self.units,             'activation': keras.activations.serialize(self.activation),             'use_bias': self.use_bias,             'kernel_initializer': keras.initializers.serialize(self.kernel_initializer),             'bias_initializer': keras.initializers.serialize(self.bias_initializer),             'dropout_rate': self.dropout_rate,         }         base_config = super(FeedForward, self).get_config()         return dict(list(base_config.items()) + list(config.items()))     def compute_output_shape(self, input_shape):         return input_shape     def compute_mask(self, inputs, input_mask=None):         return input_mask     def build(self, input_shape):         feature_dim = int(input_shape[1])         self.W1 = self.add_weight(             shape=(feature_dim, self.units),             initializer=self.kernel_initializer,             name='{}_W1'.format(self.name),         )         if self.use_bias:             self.b1 = self.add_weight(                 shape=(self.units,),                 initializer=self.bias_initializer,                 name='{}_b1'.format(self.name),             )         self.W2 = self.add_weight(             shape=(self.units, feature_dim),             initializer=self.kernel_initializer,             name='{}_W2'.format(self.name),         )         if self.use_bias:             self.b2 = self.add_weight(                 shape=(feature_dim,),                 initializer=self.bias_initializer,                 name='{}_b2'.format(self.name),             )         super(FeedForward, self).build(input_shape)     def call(self, x, mask=None, training=None):         h = K.dot(x, self.W1)         if self.use_bias:             h = K.bias_add(h, self.b1)         if self.activation is not None:             h = self.activation(h)         if 0.0  0.0:             self.add_loss(self._attention_regularizer(a))         if self.return_attention:             return [v, a]         return v     def _call_additive_emission(self, inputs):         input_shape = K.shape(inputs)         batch_size = input_shape[0]         input_len = inputs.get_shape().as_list()[1]          h_{t, t'} = \tanh(x_t^T W_t + x_{t'}^T W_x + b_h)         q = K.expand_dims(K.dot(inputs, self.Wt), 2)         k = K.expand_dims(K.dot(inputs, self.Wx), 1)         if self.use_additive_bias:             h = K.tanh(q + k + self.bh)         else:             h = K.tanh(q + k)          e_{t, t'} = W_a h_{t, t'} + b_a         if self.use_attention_bias:             e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))         else:             e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))         return e     def _call_multiplicative_emission(self, inputs):          e_{t, t'} = x_t^T W_a x_{t'} + b_a         e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))         if self.use_attention_bias:             e += self.ba[0]         return e     def compute_output_shape(self, input_shape):         output_shape = input_shape         if self.return_attention:             attention_shape = (input_shape[0], output_shape[1], input_shape[1])             return [output_shape, attention_shape]         return output_shape     def compute_mask(self, inputs, mask=None):         if self.return_attention:             return [mask, None]         return mask     def _attention_regularizer(self, attention):         batch_size = K.cast(K.shape(attention)[0], K.floatx())         input_len = K.shape(attention)[1]         indices = K.expand_dims(K.arange(0, input_len), axis=0)         diagonal = K.expand_dims(K.arange(0, input_len), axis=1)         eye = K.cast(K.equal(indices, diagonal), K.floatx())         return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(             attention,             K.permute_dimensions(attention, (0, 2, 1)))  eye)) / batch_size          def get_custom_objects():         return {'SeqSelfAttention': SeqSelfAttention} def _block_BiLSTM(filters, drop_rate, padding, inpR):     'Returns LSTM residual block'         prev = inpR      x_rnn = Bidirectional(LSTM(filters, return_sequences=True, dropout=drop_rate, recurrent_dropout=drop_rate))(prev)     符合使用cudnn核心的LSTM     x_rnn = Bidirectional(LSTM(filters, return_sequences=True, dropout=drop_rate, recurrent_dropout=0, activation='tanh', recurrent_activation='sigmoid', use_bias=True, unroll=False))(prev)     NiN = Conv1D(filters, 1, padding = padding)(x_rnn)          res_out = BatchNormalization()(NiN)     return res_out def _block_CNN_1(filters, ker, drop_rate, activation, padding, inpC):      ' Returns CNN residual blocks '     prev = inpC     layer_1 = BatchNormalization()(prev)      act_1 = Activation(activation)(layer_1)      act_1 = SpatialDropout1D(drop_rate)(act_1, training=True)     conv_1 = Conv1D(filters, ker, padding = padding)(act_1)      layer_2 = BatchNormalization()(conv_1)      act_2 = Activation(activation)(layer_2)      act_2 = SpatialDropout1D(drop_rate)(act_2, training=True)     conv_2 = Conv1D(filters, ker, padding = padding)(act_2)     res_out = add([prev, conv_2])     return res_out  def _transformer(drop_rate, width, name, inpC):      ' Returns a transformer block containing one addetive attention and one feed  forward layer with residual connections '     x = inpC     att_layer, weight = SeqSelfAttention(return_attention =True,                                                                                 attention_width = width,                                          name=name)(x)   att_layer = Dropout(drop_rate)(att_layer, training=True)         att_layer2 = add([x, att_layer])         norm_layer = LayerNormalization()(att_layer2)     FF = FeedForward(units=128, dropout_rate=drop_rate)(norm_layer)     FF_add = add([norm_layer, FF])         norm_out = LayerNormalization()(FF_add)     return norm_out, weight  def _encoder(filter_number, filter_size, depth, drop_rate, ker_regul, bias_regul, activation, padding, inpC):     ' Returns the encoder that is a combination of residual blocks and maxpooling.'             e = inpC     for dp in range(depth):         e = Conv1D(filter_number[dp],                     filter_size[dp],                     padding = padding,                     activation = activation,                    kernel_regularizer = ker_regul,                    bias_regularizer = bias_regul,                    )(e)                      e = MaxPooling1D(2, padding = padding)(e)                 return(e)  def _decoder(filter_number, filter_size, depth, drop_rate, ker_regul, bias_regul, activation, padding, inpC):     ' Returns the dencoder that is a combination of residual blocks and upsampling. '                d = inpC     for dp in range(depth):                 d = UpSampling1D(2)(d)          if dp == 2:             d = Cropping1D(cropping=(1, 1))(d)                    d = Conv1D(filter_number[dp],                     filter_size[dp],                     padding = padding,                     activation = activation,                    kernel_regularizer = ker_regul,                    bias_regularizer = bias_regul,                    )(d)             return(d)   def _lr_schedule(epoch):     ' Learning rate is scheduled to be reduced after 40, 60, 80, 90 epochs.'     lr = 1e3     if epoch > 90:         lr *= 0.5e3     elif epoch > 60:         lr *= 1e3     elif epoch > 40:         lr *= 1e2     elif epoch > 20:         lr *= 1e1     print('Learning rate: ', lr)     return lr class cred2():     def __init__(self,                  nb_filters=[8, 16, 16, 32, 32, 96, 96, 128],                  kernel_size=[11, 9, 7, 7, 5, 5, 3, 3],                  padding='same',                  activationf='relu',                  endcoder_depth=7,                  decoder_depth=7,                  cnn_blocks=5,                  BiLSTM_blocks=3,                  drop_rate=0.1,                  loss_weights=[0.2, 0.3, 0.5],                  loss_types=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy'],                                                   kernel_regularizer=keras.regularizers.l1(1e4),                  bias_regularizer=keras.regularizers.l1(1e4),                  ):         self.kernel_size = kernel_size         self.nb_filters = nb_filters         self.padding = padding         self.activationf = activationf         self.endcoder_depth= endcoder_depth         self.decoder_depth= decoder_depth         self.cnn_blocks= cnn_blocks         self.BiLSTM_blocks= BiLSTM_blocks              self.drop_rate= drop_rate         self.loss_weights= loss_weights           self.loss_types = loss_types                self.kernel_regularizer = kernel_regularizer              self.bias_regularizer = bias_regularizer      def __call__(self, inp):         x = inp         x = _encoder(self.nb_filters,                      self.kernel_size,                      self.endcoder_depth,                      self.drop_rate,                      self.kernel_regularizer,                      self.bias_regularizer,                     self.activationf,                      self.padding,                     x)             for cb in range(self.cnn_blocks):             x = _block_CNN_1(self.nb_filters[6], 3, self.drop_rate, self.activationf, self.padding, x)             if cb > 2:                 x = _block_CNN_1(self.nb_filters[6], 2, self.drop_rate, self.activationf, self.padding, x)         for bb in range(self.BiLSTM_blocks):             x = _block_BiLSTM(self.nb_filters[1], self.drop_rate, self.padding, x)         x, weightdD0 = _transformer(self.drop_rate, None, 'attentionD0', x)                      encoded, weightdD = _transformer(self.drop_rate, None, 'attentionD', x)                      decoder_D = _decoder([i for i in reversed(self.nb_filters)],                               [i for i in reversed(self.kernel_size)],                               self.decoder_depth,                               self.drop_rate,                               self.kernel_regularizer,                               self.bias_regularizer,                              self.activationf,                               self.padding,                                                           encoded)         d = Conv1D(1, 11, padding = self.padding, activation='sigmoid', name='detector')(decoder_D)         '''         The requirements to use the cuDNN implementation are:         activation == tanh         recurrent_activation == sigmoid         recurrent_dropout == 0         unroll is False         use_bias is True         Inputs, if use masking, are strictly rightpadded.         Eager execution is enabled in the outermost context.         '''          PLSTM = LSTM(self.nb_filters[1], return_sequences=True, dropout=self.drop_rate, recurrent_dropout=self.drop_rate)(encoded)          假设 self.nb_filters 和 self.drop_rate 已经定义         PLSTM = LSTM(self.nb_filters[1],                      return_sequences=True,                      dropout=self.drop_rate,                      recurrent_dropout=0,                      activation='tanh',                      recurrent_activation='sigmoid',                      use_bias=True,                      unroll=False)(encoded)         norm_layerP, weightdP = SeqSelfAttention(return_attention=True,                                                  attention_width= 3,                                                  name='attentionP')(PLSTM)         decoder_P = _decoder([i for i in reversed(self.nb_filters)],                              [i for i in reversed(self.kernel_size)],                              self.decoder_depth,                              self.drop_rate,                              self.kernel_regularizer,                              self.bias_regularizer,                             self.activationf,                              self.padding,                                                         norm_layerP)         P = Conv1D(1, 11, padding = self.padding, activation='sigmoid', name='picker_P')(decoder_P)          SLSTM = LSTM(self.nb_filters[1], return_sequences=True, dropout=self.drop_rate, recurrent_dropout=self.drop_rate)(encoded)          SLSTM = LSTM(self.nb_filters[1],               return_sequences=True,               dropout=self.drop_rate,               recurrent_dropout=0,               activation='tanh',               recurrent_activation='sigmoid',               use_bias=True,               unroll=False)(encoded)         norm_layerS, weightdS = SeqSelfAttention(return_attention=True,                                                  attention_width= 3,                                                  name='attentionS')(SLSTM)         decoder_S = _decoder([i for i in reversed(self.nb_filters)],                              [i for i in reversed(self.kernel_size)],                             self.decoder_depth,                              self.drop_rate,                              self.kernel_regularizer,                              self.bias_regularizer,                             self.activationf,                              self.padding,                                                         norm_layerS)          S = Conv1D(1, 11, padding = self.padding, activation='sigmoid', name='picker_S')(decoder_S)         model = keras.models.Model(inputs=inp, outputs=[d, P, S])         model.compile(loss=self.loss_types, loss_weights=self.loss_weights,                 optimizer=Adam(lr=_lr_schedule(0)), metrics=[f1])         return model  input=keras.layers.Input(shape=(12000,3))  model=cred2()(input)  model.summary() ```  Relevant log output ```shell ```",2025-01-24T13:47:24Z,stat:awaiting response stale type:performance TF 2.10,closed,4,7,https://github.com/tensorflow/tensorflow/issues/85667,This is likely the time spent JITing the Python imperative code to the graph representation that TF uses.,"How can I speed it up? Otherwise, I have to wait for nearly 60 minutes every time.","I think that that's too much. You could try writing TF code in graph mode directly (or switching to JAX for even more performance gains  since you use Keras, use Keras 3 and switch to Jax backend).","It is normal for the first epoch to take longer to train than subsequent epochs, as TensorFlow needs to compile the model and allocate resources. However, an hour does seem like a long time.  **Here are a few things that could be causing the slow training time:** * Larger models with more parameters will take longer to train. * Training on a larger dataset will take longer. * The speed of your CPU, GPU, and available RAM will all affect training speed. * If you are using custom code in your training loop, it could be slowing things down. **Here are some tips to improve training speed:** * If you are just starting out, try using a smaller model with fewer parameters. * If you have a large dataset, try using a smaller subset for training. * If you have a GPU, you can use it to accelerate training. * Use a profiler to identify bottlenecks in your training code. * A smaller batch size can sometimes improve training speed.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[XLA:GPU] Add factor step support to matmul perf table.,[XLA:GPU] Add factor step support to matmul perf table.,2025-01-24T12:31:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85666
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-24T12:06:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85665
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-24T11:38:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85664
copybara-service[bot],Integrate LLVM at llvm/llvm-project@401831740700,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 401831740700,2025-01-24T11:34:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85663
copybara-service[bot],[XLA:GPU] additional logging in row reduction emitter,[XLA:GPU] additional logging in row reduction emitter,2025-01-24T11:12:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85662
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-24T10:48:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85661
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-24T10:48:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85660
copybara-service[bot],[XLA:GPU] Add matmul perf table generator.,"[XLA:GPU] Add matmul perf table generator. ``` This tool runs specified matrix shapes and datatypes (HLO dots) on given hardware and saves clock cycles for each. Matrix shapes can be specified by defining a search space. Assume matrix multiplication dims: [n,k] @ [k,m] > [n,m]. The specification has a format {m,n,k}_spec='start=,stop=,step=' Which means for a particular spec we will generate a set   { + n *  | n *     for every n w/ {0}} Program expects a spec for every dim. The generated matrix multplication shapes are a cartesian product of these three specs (+ specified data types). Usage: 1. Run cartesian product for   shape:{256x256x256} x dtype:{bf16,bf16>bf16 and bf16,bf16>f32} and dump to proto. bazel run matmul_perf_table_gen_main config=cuda  \   alsologtostderr \   m_spec='start=256,stop=256,step=1' \   n_spec='start=256,stop=256,step=1' \   k_spec='start=256,stop=256,step=1' \   dtypes_spec='lhs=bf16,rhs=bf16,out=bf16;lhs=bf16,rhs=bf16,out=f32' \   output=/tmp/proto.pbtxt cat /tmp/proto.pbtxt entries {   key: ""sm_86""   value {     entries {       instruction {         name: ""_""         opcode: ""dot""         shape {           element_type: BF16           dimensions: 256           dimensions: 256         }         ...       }       clock_cycles: 10022     }     entries {       instruction {         name: ""_""         opcode: ""dot""         shape {           element_type: F32           dimensions: 256           dimensions: 256        }        ...       clock_cycles: 10137     }   } } 2. Run cartesian product for   shape:{8x16x16 and 16x16x16 and 24x16x16} x dtype:{bf16,bf16>bf16} and print to stdout. bazel run matmul_perf_table_gen_main config=cuda  \   alsologtostderr \   m_spec='start=8,stop=24,step=8' \   n_spec='start=16,stop=16,step=1' \   k_spec='start=16,stop=16,step=1' \   dtypes_spec='lhs=bf16,rhs=bf16,out=bf16 \ entries {   key: ""sm_90""   value {     entries {       instruction {         name: ""_""         opcode: ""dot""         shape {           element_type: BF16           dimensions: 24           dimensions: 16         }       }       ...       clock_cycles: 10961     }     entries {       instruction {         name: ""_""         opcode: ""dot""         shape {           element_type: BF16           dimensions: 16           dimensions: 16         }       }       ...       clock_cycles: 9440     }     entries {       instruction {         name: ""_""         opcode: ""dot""         shape {           element_type: BF16           dimensions: 8           dimensions: 16         }       }       ...       clock_cycles: 9440     }   } } ```",2025-01-24T10:38:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85659
copybara-service[bot],Update CUDA SDK to 12.8,"Update CUDA SDK to 12.8 It also adds cuDNN 9.7.0 to the distribution list, but doesn't enable it by default yet.",2025-01-24T10:24:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85658
copybara-service[bot],[XLA:GPU] Reuse some of the helper functions in Triton emitter.,[XLA:GPU] Reuse some of the helper functions in Triton emitter.,2025-01-24T10:07:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85657
copybara-service[bot],Add synchronization barrier for CUDA host callbacks to allow TSAN to run,"Add synchronization barrier for CUDA host callbacks to allow TSAN to run CUDA stream host callbacks are getting executed from a CUDAmanaged thread pool. Calling `cuStreamSynchronize` blocks until the stream has finished executing all its operations including the host callbacks. Unfortunately thread sanitizer can't identify `cuStreamSynchronize` as a synchronization barrier between a memory write in a host callback and a memory read on the main thread. This is because the CUDA libraries are not instrumented for thread sanitizer and don't use a synchronization mechanism that could be intercepted (they probably use some variation of atomics, maybe some handrolled futex implementation; pthread mutexes would have been fine). So this change is adding an explicit synchronization barrier in the form of a mutex to `CUDAStream`. This of course has some unnecessary overheard but being able to run thread sanitizer is a huge advantage so that in my opinion it's worth it. The implementation uses an atomic integer paired with a mutex protected predicate. I've also tried a simpler implementation that just uses a mutex but that's roughly 3% slower. It's also possible to hide the additional lock behind a `ifdef THREAD_SANITIZER` but I rather keep the more complex implementation than making the sanitizer code path execute different code. Reverts changelist 721179542 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/86164 from tensorflow:mihaimaruseacpatch1 d5e7459e51c112b117e52a5d5ec0629ebf384715",2025-01-24T10:01:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85656
copybara-service[bot],[XLA:GPU] Fix TMA global_strides variable,[XLA:GPU] Fix TMA global_strides variable Global_strides rank should actually only be equal to rank  1 of the tensor according to https://docs.nvidia.com/cuda/cudadriverapi/group__CUDA__TENSOR__MEMORY.html.,2025-01-24T09:22:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85655
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-24T09:21:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85654
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-24T09:20:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85653
copybara-service[bot],compat: Update forward compatibility horizon to 2025-01-24,compat: Update forward compatibility horizon to 20250124 Reverts ef2fce2eef747e32861a89556187eef255ea9306,2025-01-24T09:20:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85652
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-24T09:18:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85651
copybara-service[bot],compat: Update forward compatibility horizon to 2025-01-24,compat: Update forward compatibility horizon to 20250124 Reverts ef2fce2eef747e32861a89556187eef255ea9306,2025-01-24T09:17:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85650
copybara-service[bot],Automated Code Change,Automated Code Change Reverts ef2fce2eef747e32861a89556187eef255ea9306,2025-01-24T09:16:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85649
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-24T09:16:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85648
copybara-service[bot],compat: Update forward compatibility horizon to 2025-01-24,compat: Update forward compatibility horizon to 20250124 Reverts ef2fce2eef747e32861a89556187eef255ea9306,2025-01-24T09:15:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85647
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-24T09:15:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85646
copybara-service[bot],compat: Update forward compatibility horizon to 2025-01-24,compat: Update forward compatibility horizon to 20250124 Reverts ef2fce2eef747e32861a89556187eef255ea9306,2025-01-24T09:15:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85645
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-24T09:15:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85644
copybara-service[bot],Automated Code Change,Automated Code Change Reverts ef2fce2eef747e32861a89556187eef255ea9306,2025-01-24T09:13:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85643
copybara-service[bot],Automated Code Change,Automated Code Change Reverts ef2fce2eef747e32861a89556187eef255ea9306,2025-01-24T09:11:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85642
Venkat6871,Fix typos in documentation strings,"Hi, Team I observed few typos in the documentation strings and I have fixed those typos so please do the needful. Thank you.",2025-01-24T08:56:37Z,size:S,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85639,Hi  Can you please resolve the conflicts? Thank you! ,"This is an empty PR after rebase, meaning that the same typos got fixed by a different one."
copybara-service[bot],Reverts ef2fce2eef747e32861a89556187eef255ea9306,Reverts ef2fce2eef747e32861a89556187eef255ea9306,2025-01-24T08:43:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85638
copybara-service[bot],[core/kernels] matmul_op_test: Enable for aarch64,[core/kernels] matmul_op_test: Enable for aarch64,2025-01-24T08:40:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85637
copybara-service[bot],Protect CompileModuleToLlvmIr calls by LLVMCommandLineOptionsLock,Protect CompileModuleToLlvmIr calls by LLVMCommandLineOptionsLock XLA needs to set LLVM's global command line option flags in a some places to influence settings in the compilation pipeline. Since writing or reading these command line flags is not threadsafe and global we have a processwide lock `LLVMCommandLineOptionsLock` which manages multiple LLVM users with different flags in the same process. Turns out the lock was not correctly in place for our LLVM IR emitter code path. This led to a race condition which potentially affected users that compile multiple modules in parallel from different threads. So this change fixes that and also adds a test that should light up when running under TSAN if the compilation pipeline becomes nonthreadsafe in the future.,2025-01-24T07:07:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85636
chunhsue,Branch for Pack and DUS op,,2025-01-24T06:51:33Z,size:XL,closed,0,1,https://github.com/tensorflow/tensorflow/issues/85635,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Remove uid from window prefetch,Remove uid from window prefetch,2025-01-24T06:24:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85634
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-24T05:14:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85633
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-24T05:13:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85632
intelav,LiteRT build for Android failing," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version v2.17.0  Custom code No  OS platform and distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version bazel 6.5.0  GCC/compiler version NDK26,NDK28  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following this link https://ai.google.dev/edge/litert/build/android for building libtensorflowlite.so for my android JNI project, but its failing all  the time with below error snapshot **Initial Steps** git clone https://github.com/tensorflow/tensorflow.git cd tensorflow git checkout v2.17.0 ./configure ( For Android Environment) **Below is the content of .tf_configure.bazelrc in my build environment** build action_env PYTHON_BIN_PATH=""/usr/bin/python3"" build action_env PYTHON_LIB_PATH=""/usr/lib/python3.10/distpackages"" build python_path=""/usr/bin/python3"" build action_env CLANG_COMPILER_PATH=""/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linuxx86_64/bin/clang17"" build repo_env=CC=/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linuxx86_64/bin/clang17 build repo_env=BAZEL_COMPILER=/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linuxx86_64/bin/clang17 build copt=Wnognuoffsetofextensions build:opt copt=Wnosigncompare build:opt host_copt=Wnosigncompare build action_env ANDROID_NDK_HOME=""/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/"" build action_env ANDROID_NDK_VERSION=""26"" build action_env ANDROID_NDK_API_LEVEL=""21"" build action_env ANDROID_BUILD_TOOLS_VERSION=""35.0.0"" build action_env ANDROID_SDK_API_LEVEL=""35"" build action_env ANDROID_SDK_HOME=""/media/avaish/aiwork/Androidsdk/"" test test_size_filters=small,medium test:v1 test_tag_filters=benchmarktest,no_oss,oss_excluded,gpu,oss_serial test:v1 build_tag_filters=benchmarktest,no_oss,oss_excluded,gpu test:v2 test_tag_filters=benchmarktest,no_oss,oss_excluded,gpu,oss_serial,v1only test:v2 build_tag_filters=benchmarktest,no_oss,oss_excluded,gpu,v1only **Expected Output**  libtensorflowlite.so should get built successfully.   Standalone code to reproduce the issue ```shell **Build Command** avaishdekstop:/media/avaish/linuxgames/litebuild/tensorflow$ bazel build c opt cxxopt=std=c++17 config=android_arm64   fat_apk_cpu=x86,x86_64,arm64v8a,armeabiv7a   define=android_dexmerger_tool=d8_dexmerger   define=android_incremental_dexing_tool=d8_dexbuilder   //tensorflow/lite/java:tensorflowlite ```  Relevant log output ```shell INFO: Reading 'startup' options from /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=153 INFO: Reading rc options for 'build' from /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc:   'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Reading rc options for 'build' from /media/avaish/linuxgames/litebuild/tensorflow/.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=/usr/bin/python3 action_env PYTHON_LIB_PATH=/usr/lib/python3.10/distpackages python_path=/usr/bin/python3 action_env CLANG_COMPILER_PATH=/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linuxx86_64/bin/clang17 repo_env=CC=/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linuxx86_64/bin/clang17 repo_env=BAZEL_COMPILER=/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/toolchains/llvm/prebuilt/linuxx86_64/bin/clang17 copt=Wnognuoffsetofextensions action_env ANDROID_NDK_HOME=/media/avaish/aiwork/Androidsdk/ndk/26.1.10909125/ action_env ANDROID_NDK_VERSION=26 action_env ANDROID_NDK_API_LEVEL=21 action_env ANDROID_BUILD_TOOLS_VERSION=35.0.0 action_env ANDROID_SDK_API_LEVEL=35 action_env ANDROID_SDK_HOME=/media/avaish/aiwork/Androidsdk/ INFO: Found applicable config definition build:short_logs in file /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:android_arm64 in file /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc: config=android cpu=arm64v8a fat_apk_cpu=arm64v8a INFO: Found applicable config definition build:android in file /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc: crosstool_top=//external:android/crosstool host_crosstool_top=//tools/cpp:toolchain dynamic_mode=off define=xnn_enable_avxvnniint8=false noenable_platform_specific_config copt=w cxxopt=std=c++17 host_cxxopt=std=c++17 define=with_xla_support=false config=no_tfrt INFO: Found applicable config definition build:no_tfrt in file /media/avaish/linuxgames/litebuild/tensorflow/.bazelrc: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils INFO: Analyzed target //tensorflow/lite/java:tensorflowlite (2 packages loaded, 8416 targets configured). INFO: Found 1 target... ERROR: /media/avaish/linuxgames/litebuild/tensorflow/tensorflow/lite/c/jni/BUILD:12:43: Compiling tensorflow/lite/c/jni/jni_utils.: undeclared inclusion(s) in rule '//tensorflow/lite/c/jni:jni_utils': this rule is missing dependency declarations for the following files included by 'tensorflow/lite/c/jni/jni_utils.cc':   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stdarg.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stdint.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stddef.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/__stddef_max_align_t.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stdbool.h' Target //tensorflow/lite/java:tensorflowlite failed to build Use verbose_failures to see the command lines of failed build steps. ```",2025-01-24T05:07:38Z,stat:awaiting response type:build/install stale comp:lite 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85631,"If I run the same build command again , similiar errors apperas from compiling other files ERROR: /home/avaish/.cache/bazel/_bazel_avaish/d98cf14fd195122b7f9fe191efe765ef/external/ruy/ruy/BUILD:423:11: Compiling ruy/denormal.: undeclared inclusion(s) in rule '//ruy:denormal': this rule is missing dependency declarations for the following files included by 'ruy/denormal.cc':   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stdint.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stddef.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/__stddef_max_align_t.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/xmmintrin.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/mmintrin.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/mm_malloc.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/stdarg.h'   'external/androidndk/toolchains/llvm/prebuilt/linuxx86_64/lib/clang/17/include/emmintrin.h' Target //tensorflow/lite/java:tensorflowlite failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 0.903s, Critical Path: 0.12s INFO: 12 processes: 12 internal. FAILED: Build did NOT complete successfully","Hi,  I apologize for the delay in my response, I was trying to replicate the same behavior from my end but I'm getting different error and Build did NOT complete successfully so I have added error log below for reference please let me know if Am I missing something here to replicate same behavior which you reported here ? ``` (base) gaikwadrahuln1standard1gput4x1tfliteubuntu22:~/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow$ bazel build c opt cxxopt=std=c++17 config=android_arm64   fat_apk_cpu=x86,x86_64,arm64v8a,armeabiv7a   define=android_dexmerger_tool=d8_dexmerger   define=android_incremental_dexing_tool=d8_dexbuilder   //tensorflow/lite/java:tensorflowlite Extracting Bazel installation... Starting local Bazel server and connecting to it... INFO: Reading 'startup' options from /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=190 INFO: Reading rc options for 'build' from /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc:   'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Reading rc options for 'build' from /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=/home/gaikwadrahul/miniconda3/bin/python3 action_env PYTHON_LIB_PATH=/home/gaikwadrahul/miniconda3/lib/python3.12/sitepackages python_path=/home/gaikwadrahul/miniconda3/bin/python3 action_env CLANG_COMPILER_PATH=/usr/bin/clang17 repo_env=CC=/usr/bin/clang17 repo_env=BAZEL_COMPILER=/usr/bin/clang17 copt=Wnognuoffsetofextensions action_env ANDROID_NDK_HOME=/home/gaikwadrahul/androidndkr25b action_env ANDROID_NDK_VERSION=25 action_env ANDROID_NDK_API_LEVEL=21 action_env ANDROID_BUILD_TOOLS_VERSION=34.0.0 action_env ANDROID_SDK_API_LEVEL=33 action_env ANDROID_SDK_HOME=/home/gaikwadrahul/Android/Sdk INFO: Found applicable config definition build:short_logs in file /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:android_arm64 in file /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc: config=android cpu=arm64v8a fat_apk_cpu=arm64v8a INFO: Found applicable config definition build:android in file /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc: crosstool_top=//external:android/crosstool host_crosstool_top=//tools/cpp:toolchain dynamic_mode=off noenable_platform_specific_config copt=w cxxopt=std=c++17 host_cxxopt=std=c++17 define=with_xla_support=false config=no_tfrt INFO: Found applicable config definition build:no_tfrt in file /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/.bazelrc: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils DEBUG: /home/gaikwadrahul/TFLiteIssue CC(LiteRT build for Android failing)/tensorflow/tensorflow/tools/toolchains/python/python_repo.bzl:32:14:  TF_PYTHON_VERSION environment variable was not set correctly; using Python 3.11. To set Python version, run: export TF_PYTHON_VERSION=3.11 INFO: Analyzed target //tensorflow/lite/java:tensorflowlite (146 packages loaded, 15447 targets configured). INFO: Found 1 target... ERROR: /home/gaikwadrahul/.cache/bazel/_bazel_gaikwadrahul/17e9d465a5125118a485d68d85fbf68a/external/flatbuffers/src/BUILD.bazel:19:11: Compiling src/code_generators.cpp [for tool] failed: (Exit 1): clang17 failed: error executing command (from target //src:code_generators) /usr/bin/clang17 U_FORTIFY_SOURCE fstackprotector Wall Wthreadsafety Wselfassign Wunusedbutsetparameter Wnofreenonheapobject fcolordiagnostics fnoomitframepointer g0 O2 ... (remaining 27 arguments skipped) In file included from external/flatbuffers/src/code_generators.cpp:17: bazelout/k8optexec50AE0418/bin/external/flatbuffers/src/_virtual_includes/code_generators/flatbuffers/code_generators.h:20:10: fatal error: 'map' file not found    20           ^~~~~ 1 error generated. Target //tensorflow/lite/java:tensorflowlite failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 126.844s, Critical Path: 12.54s INFO: 84 processes: 61 internal, 23 local. FAILED: Build did NOT complete successfully ``` Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],"Make the TF shape inference update the XlaCallModule's StableHLO module when the shapes are refined, if enabled by `enable_stablehlo_propagation`. It is disabled by default for now.","Make the TF shape inference update the XlaCallModule's StableHLO module when the shapes are refined, if enabled by `enable_stablehlo_propagation`. It is disabled by default for now.",2025-01-24T02:42:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85630
ghinaaraf,Can not import tensorflow as tf," Issue type Performance  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.11.5  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense  Standalone code to reproduce the issue ```shell i always use python with VS Code, and yesterday my code finally success. but today when I run these code ""import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense"" . it always become error, even I had uninstall and install the tensorflow again, it not helped ```  Relevant log output ```shell ImportError                               Traceback (most recent call last) File c:\Users\Ghinaa\AppData\Local\Programs\Python\Python311\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[52], line 1 > 1 import tensorflow as tf       2 from tensorflow.keras.models import Sequential       3 from tensorflow.keras.layers import Dense File c:\Users\Ghinaa\AppData\Local\Programs\Python\Python311\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 ... Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```",2025-01-24T02:39:30Z,type:performance,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85629,Please search for duplicates before opening a new issue,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Add new send/recv channel Megascale stat type.,Add new send/recv channel Megascale stat type.,2025-01-24T01:40:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85628
copybara-service[bot],"When locking a tensor buffer, always use the associated event (if available)","When locking a tensor buffer, always use the associated event (if available)",2025-01-24T01:31:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85627
copybara-service[bot],Enable p2p pipelining when `xla_gpu_experimental_enable_pipeline_parallelism_opt` is enabled,Enable p2p pipelining when `xla_gpu_experimental_enable_pipeline_parallelism_opt` is enabled,2025-01-24T00:30:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85626
copybara-service[bot],Add reserved_hbm_usage_bytes to xla.Compilationresult.,Add reserved_hbm_usage_bytes to xla.Compilationresult.,2025-01-24T00:10:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85625
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-24T00:04:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85624
copybara-service[bot],#sdy add sharding rules for ragged_all_to_all custom call,sdy add sharding rules for ragged_all_to_all custom call,2025-01-23T23:56:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85623
copybara-service[bot],Internal change only.,Internal change only.,2025-01-23T23:46:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85622
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:45:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85621
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:42:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85620
copybara-service[bot],[XLA:GPU] fix collective permute cycle decomposer and make it work for multiple cycles,[XLA:GPU] fix collective permute cycle decomposer and make it work for multiple cycles,2025-01-23T23:30:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85619
copybara-service[bot],Check for nullptr return from malloc called from AllocateBuffers in Literal.,Check for nullptr return from malloc called from AllocateBuffers in Literal.,2025-01-23T23:26:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85618
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:25:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85617
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:17:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85616
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:13:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85615
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:10:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85614
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:10:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85613
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:09:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85612
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T23:07:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85611
copybara-service[bot],Provide Target machine architecture for cross-compile scenarios.,"Provide Target machine architecture for crosscompile scenarios. CUDA/CUDNN/NCCL repositories are created on a host machine, so if we need to download redistributions for other architectures in crosscompile scenario, we need to pass the target architecture name to repo rules, e.g. `repo_env=CUDA_REDIST_TARGET_PLATFORM=aarch64`",2025-01-23T23:01:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85610
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T22:52:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85609
Jerry-Ge,[mlir][tosa] Update Tensorflow to match TOSA v1.0 specification,We've been pushing TOSA v1.0 LLVM patches to upstream. This PR includes commits to keep the following operators to be aligned with LLVM:  **Acc_type for Conv Operators:**  LLVM Patch: https://github.com/llvm/llvmproject/pull/121466 **Tile Op Update** LLVM Patch: https://github.com/llvm/llvmproject/pull/122547 **Pad Op Update** LLVM Patch: https://github.com/llvm/llvmproject/pull/123133 **Equalize Ranks Update** LLVM Patch: https://github.com/llvm/llvmproject/pull/104501,2025-01-23T22:49:54Z,size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85608
copybara-service[bot],Fix model_builder_test in cl/718992549,Fix model_builder_test in cl/718992549,2025-01-23T22:48:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85607
copybara-service[bot],[ODML] Pass expand-tuple : Migrate from MHLO to StableHLO,[ODML] Pass expandtuple : Migrate from MHLO to StableHLO FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21825 from zhenyingliu:scheduler 1c5720711c6a7d8173e132922b67eee6e2e8b9dd,2025-01-23T22:38:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85606
copybara-service[bot],Update compiler plugin interface to handle multi-byte code results.,Update compiler plugin interface to handle multibyte code results.,2025-01-23T22:22:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85605
MahmoudBahar,TensorFlow warning shows whenever importing it," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version v2.18.0rc24g6550e4bd802  Custom code No  OS platform and distribution Linux Ubuntu 24.10 x86_64  Mobile device _No response_  Python version 3.12.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version CUDA:12.6  GPU model and memory _No response_  Current behavior? `cuFFT`, `cuDNN` and `cuBLAS` warnings showing after importing `TensorFlow`  OS: Ubuntu 24.10 x86_64  Host: G5 5590  Kernel: 6.11.013generic  CPU: Intel i79750H (12) @ 4.500GHz  GPU: NVIDIA GeForce GTX 1650 Mobile / MaxQ  GPU: Intel CoffeeLakeH GT2 [UHD Graphics 630] > whenever running the following code it gives that warning also it outputs the predicted output but after the warning: ```python import tensorflow as tf print(tf.config.list_physical_devices('GPU')) ``` > output: ``` 20250123 21:08:06.468437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1737659286.484845  763412 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1737659286.489647  763412 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250123 21:08:06.505984: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] ``` > also i know that for that warning (`cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`) i must rebuild the tensorflow from binaries enabling the AVX2 and FMA instructions but what about the others?  Standalone code to reproduce the issue ```shell import tensorflow as tf print(tf.config.list_physical_devices('GPU')) ```  Relevant log output ```shell ```",2025-01-23T21:56:39Z,type:bug 2.18.rc,open,1,3,https://github.com/tensorflow/tensorflow/issues/85604,"Hi **** , Apologies for the delay, and thank you for raising your concern here. It seems there might be a version mismatch. Could you please verify the compatibility of all the versions you are using? For your reference, I have provided relevant documentation below. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Hello , sorry for replying lately. I checked the versions as in the documentation you sent me as following: I am working in conda enviroment with the following configs: python=3.12.8 running the following code to check the version of `TensorFlow`: ```python import tensorflow as tf print(tf.__version__) ``` would get the following output: ```bash 20250205 05:42:43.524743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1738726963.539270   17574 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1738726963.543483   17574 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250205 05:42:43.558701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 2.18.0 ``` showing that `TensorFlow` installed with version `2.18.0` running the following code in terminal to check compiler and build tools would get that output: ```bash (tf) username:~$ python c ""import tensorflow as tf; print(tf.sysconfig.get_compile_flags())"" 20250205 05:47:22.629352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1738727242.644411   17974 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1738727242.648762   17974 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250205 05:47:22.664596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. ['I/home/mahmoudbahar/anaconda3/envs/tf/lib/python3.12/sitepackages/tensorflow/include', 'D_GLIBCXX_USE_CXX11_ABI=1', 'std=c++17', 'DEIGEN_MAX_ALIGN_BYTES=64'] (tf) username:~$ python c ""import tensorflow as tf; print(tf.sysconfig.get_build_info())"" 20250205 05:47:39.771357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1738727259.787632   18027 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1738727259.792173   18027 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250205 05:47:39.808482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. OrderedDict({'cpu_compiler': '/usr/lib/llvm18/bin/clang', 'cuda_compute_capabilities': ['sm_60', 'sm_70', 'sm_80', 'sm_89', 'compute_90'], 'cuda_version': '12.5.1', 'cudnn_version': '9', 'is_cuda_build': True, 'is_rocm_build': False, 'is_tensorrt_build': False}) ``` running the following code to check if `TensorFlow` can recognize the cudnn version: ```python import tensorflow as tf print(tf.sysconfig.get_build_info()['cudnn_version']) ``` outputs the following: ```bash 20250205 05:31:31.865306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1738726291.880059   16607 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1738726291.884535   16607 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250205 05:31:31.900147: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 9 ``` showing that `cuDNN` installed with the package is version is `9` running the following code to check if `TensorFlow` can recognize the cuda version: ```python import tensorflow as tf print(tf.sysconfig.get_build_info()['cuda_version']) ``` outputs the following: ```bash 20250205 05:33:28.844110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1738726408.859348   16754 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1738726408.863713   16754 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250205 05:33:28.880097: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 12.5.1 ``` showing that that `CUDA` installed with the package is `12.5.1` `nvidiacudatoolkit` is not installed globally in my machine. so the following would be expected: ```bash (tf) username:~$ nvcc version Command 'nvcc' not found, but can be installed with: sudo apt install nvidiacudatoolkit ``` There is a CUDA version installed globally which is 12.6 so the following output is as expected: ```bash (tf) username:~$ nvidiasmi Wed Feb  5 05:39:56 2025        ++  ++ ``` > [!IMPORTANT] > running the following commands in terminal would show that they are not installed globally > cmake version > bazel version"
copybara-service[bot],Fix zero size tensor does not recognized as constant.,Fix zero size tensor does not recognized as constant.,2025-01-23T21:46:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85603
Jerry-Ge,[Tosa] Add acc_type to Tosa Conv Ops,Also adjust to TileOp multiples as input,2025-01-23T21:11:35Z,size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85602
copybara-service[bot],"Remove all the redundant byte code serialization code. All paths now use the ""external buffer"" approach with flexbuffers.","Remove all the redundant byte code serialization code. All paths now use the ""external buffer"" approach with flexbuffers.",2025-01-23T21:08:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85601
copybara-service[bot],Move transfer python bindings into jax.,Move transfer python bindings into jax.,2025-01-23T20:49:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85600
copybara-service[bot],Reverts 053e69960c930ab150a1312e25c4d79a0b54d9db,Reverts 053e69960c930ab150a1312e25c4d79a0b54d9db,2025-01-23T20:18:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85599
copybara-service[bot],Fix output shape of slice op test.,Fix output shape of slice op test.,2025-01-23T20:06:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85598
copybara-service[bot],Integrate multi-bytecode handling with apply plugin. This should also cleanup the multi plugin application flow.,Integrate multibytecode handling with apply plugin. This should also cleanup the multi plugin application flow. Integrate the more streamlined approach with multibytecode support with flexbuffers in the runtime as well.,2025-01-23T19:54:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85597
copybara-service[bot],"Adjust the wheel.sh, libtensorflow.sh scripts to be Win-compatible for upload.","Adjust the wheel.sh, libtensorflow.sh scripts to be Wincompatible for upload. Temporary workaround.",2025-01-23T19:33:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85596
xXExilXx,Pip cant find TensorFlow package,"i tried installing the package by running ""pip install tensorflow"" but it always returns a error that it couldn't resolve the package name. i cant tell if there is a problem with my project or im just to dumb to install it",2025-01-23T19:08:57Z,,closed,0,3,https://github.com/tensorflow/tensorflow/issues/85595,"Please post information about your system, operating system, version of Python you are using, what command you are running, what is the error. Use \`\`\` to wrap around code/error blocks. Don't post images. Right now, there is really no information to go on from your post. You can also search for similar issues (e.g. CC(Unable to install TensorFlow: No matching distribution found for TensorFlow!) could be a candidate?)",my sys info: ``` Operating System: Microsoft Windows [Version 10.0.26100.2894] Python Version: Python 3.13.1 System Architecture: 64bit Machine Name: EXILS Current User: deion System Memory: Manufacturer    Capacity Speed       Unknown      17179869184 2667 CPU Information: Name                                            NumberOfCores MaxClockSpeed                                               AMD Ryzen 5 3600 6Core Processor                           6          3600 GPU Information: Name  NVIDIA GeForce RTX 2060 ``` The console log:  ``` Windows PowerShell Copyright (C) Microsoft Corporation. All rights reserved. Install the latest PowerShell for new features and improvements! https://aka.ms/PSWindows (.venv) PS C:\Users\deion\PycharmProjects\MusicMLM> pip install tensorflow ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) ERROR: No matching distribution found for tensorflow (.venv) PS C:\Users\deion\PycharmProjects\MusicMLM>  ```,"It's a duplicate, please search for similar issues here before making new ones"
copybara-service[bot],PR #21391: [NVIDIA GPU] Add an option to xla_gpu_disable_async_collectives to disable all async collectives using a single op,"PR CC(tf.linspace doesn't except integers for start/stop and floats for num): [NVIDIA GPU] Add an option to xla_gpu_disable_async_collectives to disable all async collectives using a single op Imported from GitHub PR https://github.com/openxla/xla/pull/21391 xla_gpu_disable_async_collectives accepts a list of collective names, to be more efficient, we add a ""ALLCOLLECTIVE"" option to the enum so we can disable all ops with a single element. Copybara import of the project:  d561e610c170fac3a6a92436fc0983e383ed93eb by TJ Xu : add an option to disable all async collectives using a single op  e30cee65f20468eb5332a747382eaf78c64a99d5 by TJ Xu : add option in the test locally Merging this change closes CC(tf.linspace doesn't except integers for start/stop and floats for num) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21391 from Tixxx:tixxx/add_disable_all e30cee65f20468eb5332a747382eaf78c64a99d5",2025-01-23T19:06:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85594
copybara-service[bot],Use `tsl/platform:test_main` for all test targets with benchmarks,Use `tsl/platform:test_main` for all test targets with benchmarks,2025-01-23T18:57:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85593
copybara-service[bot],"[pjrt] Removed `PjRtRawDeviceBuffer`, `CreateRawDeviceBuffer` and `GetOnDeviceSizeInBytes` from `PjRtClient`","[pjrt] Removed `PjRtRawDeviceBuffer`, `CreateRawDeviceBuffer` and `GetOnDeviceSizeInBytes` from `PjRtClient` The class and both methods were only used in TPU client tests.",2025-01-23T18:45:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85592
copybara-service[bot],Allow backend-specific `test_migrated_to_hlo_runner_pjrt` tags to be used.,"Allow backendspecific `test_migrated_to_hlo_runner_pjrt` tags to be used. Certain tests don't work on all backends, so it should be possible to suppress the inclusion of the `test_migrated_to_hlo_runner_pjrt` tag and only apply it to those backends that work.",2025-01-23T18:42:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85591
copybara-service[bot],[xla:emitters] xla.pure_call: support noinline attribute,[xla:emitters] xla.pure_call: support noinline attribute Paves the way for upcoming work.,2025-01-23T18:31:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85590
copybara-service[bot],Remove StreamExecutor::UnifiedMemoryAllocate and ::UnifiedMemoryDeallocate.,Remove StreamExecutor::UnifiedMemoryAllocate and ::UnifiedMemoryDeallocate. All callers are migrated to StreamExecutor::CreateMemoryAllocator(MemoryType::kUnified).,2025-01-23T18:14:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85589
copybara-service[bot],Use ANGLE to provide GL on Linux in LiteRt.,Use ANGLE to provide GL on Linux in LiteRt.,2025-01-23T18:04:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85588
copybara-service[bot],Use a switch to check opcode for better readability,Use a switch to check opcode for better readability,2025-01-23T17:56:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85587
copybara-service[bot],[cpp23] Remove std::aligned_storage<> in tensorflow,[cpp23] Remove std::aligned_storage in tensorflow std::aligned_storage is deprecated in cpp23.,2025-01-23T17:21:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85586
copybara-service[bot],[xla:cpu] Fix mac os compilation error part 2,[xla:cpu] Fix mac os compilation error part 2,2025-01-23T17:19:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85585
copybara-service[bot],[XLA:GPU] tool to print data from ncu-rep,[XLA:GPU] tool to print data from ncurep useful when we to script metric collection. Or when you are connected to via ssh and want a short summary without invoking ncu UI. It does it by exporting ncurep to CSV. Example usage: >ncu_rep i /tmp/profile.ncurep Metric                        register/thread >ncu_rep i /tmp/triton/profile.ncurep f raw m gpu__time_duration.sum 61.440000 us,2025-01-23T16:07:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85584
tilakrayal,Fixed the broken links in config.proto,,2025-01-23T15:46:40Z,ready to pull size:XS comp:core,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85583
copybara-service[bot],Automated g4 rollback of changelist 718427277.,Automated g4 rollback of changelist 718427277. Reverts 44c1714581a1a8b44dff0a66107afab5105a7ab8,2025-01-23T15:41:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85582
tilakrayal,Fixed the typos in multiple files,,2025-01-23T15:35:14Z,ready to pull size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85581
copybara-service[bot],Integrate LLVM at llvm/llvm-project@4f26edd5e9eb,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 4f26edd5e9eb,2025-01-23T15:00:48Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/85580, please don't spam with LLM generated content. It goes against both TF's code of conduct and GitHub's code of conduct
copybara-service[bot],Remove gpu_only_cc_library,"Remove gpu_only_cc_library Since we removed all direct dependencies on CUDA and ROCm headers, this macro shouldn't be necessary anymore.",2025-01-23T14:04:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85579
copybara-service[bot],[xla:emitters] define xla.backend_kind,"[xla:emitters] define xla.backend_kind This paves the way for distinguishing between target backends. Note that we're dropping the explicit include of xla_attrs.td from gpu_attrs.td, which avoids redefining the enums. Note that we include xla_ops.h from xla_gpu_ops.h.",2025-01-23T14:03:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85578
copybara-service[bot],[core/kernels] Remove deprecated bitcast_op,[core/kernels] Remove deprecated bitcast_op tensorflow/c/kernels:bitcast_op should be used instead.,2025-01-23T13:39:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85577
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T13:01:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85576
copybara-service[bot],Add GTest matchers for `Expected` and `LiteRtStatus`.,"Add GTest matchers for `Expected` and `LiteRtStatus`.  `IsOk()` matches `kLiteRtStatusOk` and `litert::Expected` objects holding a value.  `IsError()` matches any `kLiteRtStatusError*` value and `litert::Expected` objects holding an error.  `IsError(status_code)` matches a specific status code error.  `IsError(status_code, msg)` matches a specific status code error and message. This also adds two convenience macros.  `LITERT_ASSERT_OK`, equivalent to `ASSERT_THAT(expr, IsOk())`.  `LITERT_EXPECT_OK`, equivalent to `EXPECT_THAT(expr, IsOk())`.",2025-01-23T12:53:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85575
copybara-service[bot],PR #21639: [GPU] Fix sharded autotuning test.,PR CC(Unnecessary character in logging.h): [GPU] Fix sharded autotuning test. Imported from GitHub PR https://github.com/openxla/xla/pull/21639 Autotuning relies on device assignment (replicas / partitions) to calculate sharding. Copybara import of the project:  bca2fc3edbb456fa420a1bbee356e8dc3f6ddeb9 by Ilia Sergachev : [GPU] Fix sharded autotuning test. Autotuning relies on device assignment (replicas / partitions) to calculate sharding. Merging this change closes CC(Unnecessary character in logging.h) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21639 from openxla:fix_sharded_autotuning_test bca2fc3edbb456fa420a1bbee356e8dc3f6ddeb9,2025-01-23T12:40:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85574
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T12:12:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85573
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T12:04:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85572
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T11:47:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85571
copybara-service[bot],[pjrt] Added a default memory space to `PjRtStreamExecutorDevice`,[pjrt] Added a default memory space to `PjRtStreamExecutorDevice`,2025-01-23T11:18:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85570
copybara-service[bot],[XLA:GPU] Minor code cleanup,[XLA:GPU] Minor code cleanup,2025-01-23T10:47:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85569
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T10:47:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85568
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T10:45:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85567
copybara-service[bot],[XLA:GPU] Expose ExecutionProfile through PjRt interface.,[XLA:GPU] Expose ExecutionProfile through PjRt interface.,2025-01-23T10:37:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85566
copybara-service[bot],[XLA:GPU] Allow passing execution profile in functional HLO runner,[XLA:GPU] Allow passing execution profile in functional HLO runner,2025-01-23T10:34:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85565
copybara-service[bot],[XLA:GPU] Always run under profiles in the multihost runner,[XLA:GPU] Always run under profiles in the multihost runner And output nsprecision execution duration to log.,2025-01-23T10:32:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85564
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T10:22:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85563
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T10:16:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85562
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T10:12:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85561
Redoxon33,Compatibility table for TensorFlow 2.18 with CUDA and cuDNN is missing on the official website,"In previous versions of TensorFlow, the official documentation included a clear compatibility table specifying which versions of TensorFlow worked with specific versions of CUDA and cuDNN. However, upon reviewing the documentation for TensorFlow 2.18, I noticed this information is no longer available. These tables were  helpful for users, as they prevented installation and compatibility issues when setting up the development environment. It would be great if this compatibility table could be brought back to the official documentation. If it was removed due to any updates or errors that have been identified, it would also be helpful to notify users of such changes. If you're reading this issue, I kindly ask that you notify users if any updates or changes regarding this topic are implemented.",2025-01-23T10:09:06Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/85560,"After further investigation, I discovered that the issue is specific to the Spanish versions of the documentation (both Latin American and European Spanish), which appear to be outdated compared to the English version. The compatibility table is available in the English documentation, but it is not present in the Spanish translations. It would be helpful if an indicator could be added to show that the Spanish documentation is outdated or missing specific information, to prevent confusion for users relying on these versions."
cj401-amd,[RFC] rocprof insights for rocprof data," RFC for rocprof insights  Introduction `rocprof`, `rocprofv2`, and `rocprofv3` (rocprofilersdk) are the profiling tools that can be used to collect AMD hardware performance data when running applications with ROCm/HIP. The collected timeline trace data, which are JSON format for `rocprof` and `rocprofv2` and `pftrace` format for `rocprofv3`, can be visualized via `https://ui.perfetto.dev/` to guide the loop of profiling, analysis and optimization. To gain deep insights into specific running kernels, API launch and memory copy, it is necessary to obtain more statistics about them. rocprof insights, which is developed as a Python package, aims to provide the following functionalities: 1. Data loading, including loading CSV and JSON files saved from `rocprof v1/v2/v3` 2. Data analysis, providing:     Total running time     Number of calls (instances)     Average time     Median time     Min/max time     StdDev time    For each:     Kernel     HIP/HSA API calls     H2D, D2H, D2D memcopy     Checking private/group segment size (scratch/local memory, register spillage, shared local memory) 3. Data visualization, providing:     Pie chart plot of latency (running time)     Histogram of latency     Bar plot for kernels, API calls, etc. Other features we would like to explore are: 1. Can we overlay the latency on top of the original operators in the computational graph (latency per op/node)? 2. Can we trace the input/output values of every node in the computational graph for checking accuracy (mismatch) per node?",2025-01-23T10:00:14Z,size:XL,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85559
copybara-service[bot],Integrate LLVM at llvm/llvm-project@c6e7b4a61ab8,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match c6e7b4a61ab8,2025-01-23T09:57:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85558
copybara-service[bot],Move CudaComputeCapability version parser into factory function,"Move CudaComputeCapability version parser into factory function The constructor that was used to do string parsing can't have any error handling, therefore it aborts in case of a parsing error. A better design it to have a static factory function that returns a `absl::StatusOr`. That forces users to handle a parsing error. Also unit tests.",2025-01-23T09:50:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85557
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T09:49:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85556
copybara-service[bot],PR #21722: [CUDA] Fix LoadedNvJitLinkHasKnownIssues check,"PR CC(TensorFlow Samples Do Not Run  Can't get past documented 16358, 17393 regardless version used): [CUDA] Fix LoadedNvJitLinkHasKnownIssues check Imported from GitHub PR https://github.com/openxla/xla/pull/21722 Flip the logic of `LoadedNvJitLinkHasKnownIssues`.  If `GetNvJitLinkVersion()` return value is version `12.6` ; `LoadedNvJitLinkHasKnownIssues` now return `false` If `GetNvJitLinkVersion()` return value is version `12.5` ; `LoadedNvJitLinkHasKnownIssues` now return `false` If `GetNvJitLinkVersion()` return value is version `12.4` ; `LoadedNvJitLinkHasKnownIssues` now return `true` If `GetNvJitLinkVersion()` return value is version `0.0` ; `LoadedNvJitLinkHasKnownIssues` now return `true` Copybara import of the project:  e0d67890aff6abe1087920efe49d6f6c4744f80f by Hugo Mano : Fix LoadedNvJitLinkHasKnownIssues check Merging this change closes CC(TensorFlow Samples Do Not Run  Can't get past documented 16358, 17393 regardless version used) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21722 from hugomano:hugomano/cuda/nvjitlink_known_issues e0d67890aff6abe1087920efe49d6f6c4744f80f",2025-01-23T09:48:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85555
copybara-service[bot],[xla] Fix warnings in hlo_memory_scheduler,[xla] Fix warnings in hlo_memory_scheduler,2025-01-23T09:26:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85554
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T09:25:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85553
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T09:19:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85552
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T09:18:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85551
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T09:18:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85550
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T09:18:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85549
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T09:16:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85548
copybara-service[bot],compat: Update forward compatibility horizon to 2025-01-23,compat: Update forward compatibility horizon to 20250123,2025-01-23T09:16:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85547
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T09:16:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85546
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T09:16:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85545
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T09:14:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85544
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T09:12:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85543
copybara-service[bot],compat: Update forward compatibility horizon to 2025-01-23,compat: Update forward compatibility horizon to 20250123,2025-01-23T09:12:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85542
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T09:11:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85541
copybara-service[bot],Update GraphDef version to 2116.,Update GraphDef version to 2116.,2025-01-23T09:08:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85540
copybara-service[bot],PR #20794: [gpu][ds-fusion] Add handling for offset module in ds-fusion thunk,PR CC(Add all keep nodes to output lists): [gpu][dsfusion] Add handling for offset module in dsfusion thunk Imported from GitHub PR https://github.com/openxla/xla/pull/20794 This patch adds support for offset modules in ds fusion thunk. It also moves the `ResourceRequests` structure from `gpu_executable.cc` to `gpu_executable.h` because a valid implementation of the abstract class `Thunk::ResourceRequests` is required for calling `Thunk::Prepare()`. This is split from CC(An attempt at Tensorflow and Bazel CPU/GPU build (v1.8.0)) as per request. Copybara import of the project:  53226e9428dff816819c192735b9569ef3d309ea by Shraiysh Vaishay : [gpu][dsfusion] Add handling for offset module in dsfusion thunk This patch adds support for offset modules in ds fusion thunk. It also moves the `ResourceRequests` structure from `gpu_executable.cc` to `gpu_executable.h` because a valid implementation of the abstract class `Thunk::ResourceRequests` is required for calling `Thunk::Prepare()`.  e554ac6b09b5023082210c5100a1f1a86c1c6605 by Shraiysh Vaishay : Address comments and rebase  4ac8efa0c9885b0f21f526072f821ace40be4043 by Shraiysh Vaishay : Address comments  8c23e2a4e2ff66fb7361c48c9d988ebb2261bc41 by Shraiysh Vaishay : Addressed comments.  efbe6bfa6f10f1ee998683161e398ae3adc0f183 by Shraiysh Vaishay : Addressed comments Merging this change closes CC(Add all keep nodes to output lists) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20794 from shraiysh:ds_fusion_thunk_changes efbe6bfa6f10f1ee998683161e398ae3adc0f183,2025-01-23T09:08:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85539
copybara-service[bot],Reverts 2c4ffc1b0fa3d01c6627888db46d7367bf5b98be,Reverts 2c4ffc1b0fa3d01c6627888db46d7367bf5b98be,2025-01-23T07:57:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85538
copybara-service[bot],PR #21722: [CUDA] Fix LoadedNvJitLinkHasKnownIssues check,"PR CC(TensorFlow Samples Do Not Run  Can't get past documented 16358, 17393 regardless version used): [CUDA] Fix LoadedNvJitLinkHasKnownIssues check Imported from GitHub PR https://github.com/openxla/xla/pull/21722 Flip the logic of `LoadedNvJitLinkHasKnownIssues`.  If `GetNvJitLinkVersion()` return value is version `12.6` ; `LoadedNvJitLinkHasKnownIssues` now return `false` If `GetNvJitLinkVersion()` return value is version `12.5` ; `LoadedNvJitLinkHasKnownIssues` now return `false` If `GetNvJitLinkVersion()` return value is version `12.4` ; `LoadedNvJitLinkHasKnownIssues` now return `true` If `GetNvJitLinkVersion()` return value is version `0.0` ; `LoadedNvJitLinkHasKnownIssues` now return `true` Copybara import of the project:  e0d67890aff6abe1087920efe49d6f6c4744f80f by Hugo Mano : Fix LoadedNvJitLinkHasKnownIssues check Merging this change closes CC(TensorFlow Samples Do Not Run  Can't get past documented 16358, 17393 regardless version used) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21722 from hugomano:hugomano/cuda/nvjitlink_known_issues e0d67890aff6abe1087920efe49d6f6c4744f80f",2025-01-23T07:37:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85537
copybara-service[bot],fix GPU pywrap implementation,fix GPU pywrap implementation,2025-01-23T07:34:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85536
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T07:17:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85535
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T07:14:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85534
copybara-service[bot],Add Pack Op legalization.,Add Pack Op legalization.,2025-01-23T06:27:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85532
copybara-service[bot],Allow customization of HloParserOptions when using ParseAndReturnVerifiedModule.,Allow customization of HloParserOptions when using ParseAndReturnVerifiedModule.,2025-01-23T05:54:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85531
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-23T04:59:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85530
copybara-service[bot],Always link PjRt test client registries.,Always link PjRt test client registries. This way tests can be run with `dynamic_mode=off`.,2025-01-23T04:48:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85529
copybara-service[bot],[XLA:MemoryScheduler] Enable constant deferring by default.,"[XLA:MemoryScheduler] Enable constant deferring by default. Defer constant as close to the first user as possible in postprocessing for all scheduler algorithm, this greedy mechanism should always lower memory consumption.",2025-01-23T02:40:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85528
copybara-service[bot],Use the correct version macros for the Qualcomm's dispatch API,Use the correct version macros for the Qualcomm's dispatch API,2025-01-23T01:50:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85527
copybara-service[bot],Refactor Pixel Dispatch API code,Refactor Pixel Dispatch API code,2025-01-23T01:41:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85526
copybara-service[bot],Add DUS legalization with decomposition.,Add DUS legalization with decomposition.,2025-01-23T01:29:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85525
copybara-service[bot],Change noisy (and useless) XLA log line to VLOG,Change noisy (and useless) XLA log line to VLOG,2025-01-23T01:08:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85524
copybara-service[bot],Support IVP4 in socket_bulk_transport.,Support IVP4 in socket_bulk_transport.,2025-01-23T01:07:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85523
copybara-service[bot],[xla:cpu] Micro-optimizations for XLA:CPU kernels that use only x dimension,[xla:cpu] Microoptimizations for XLA:CPU kernels that use only x dimension ``` name                                   old cpu/op   new cpu/op   delta BM_KernelAsyncLaunch/1/process_time    10.0ns ± 2%  10.5ns ± 2%  +4.92%  (p=0.000 n=37+37) BM_KernelAsyncLaunch/4/process_time    32.5µs ± 8%  32.7µs ±16%    ~     (p=0.483 n=38+40) BM_KernelAsyncLaunch/8/process_time    51.7µs ± 7%  51.0µs ± 7%  1.46%  (p=0.011 n=39+34) BM_KernelAsyncLaunch/16/process_time   73.0µs ± 7%  71.5µs ± 6%  2.03%  (p=0.007 n=39+39) BM_KernelAsyncLaunch/32/process_time   74.8µs ± 9%  73.0µs ± 6%  2.45%  (p=0.001 n=39+35) BM_KernelAsyncLaunch/64/process_time   76.9µs ± 8%  74.4µs ± 6%  3.29%  (p=0.000 n=38+36) BM_KernelAsyncLaunch/128/process_time  85.8µs ±12%  80.9µs ±13%  5.68%  (p=0.000 n=40+40) BM_KernelAsyncLaunch/256/process_time   104µs ± 8%    95µs ± 7%  8.80%  (p=0.000 n=38+37) name                                   old time/op          new time/op          delta BM_KernelAsyncLaunch/1/process_time    10.0ns ± 1%          10.5ns ± 1%  +5.01%  (p=0.000 n=39+38) BM_KernelAsyncLaunch/4/process_time    13.4µs ±10%          13.3µs ±19%    ~     (p=0.747 n=38+40) BM_KernelAsyncLaunch/8/process_time    17.0µs ± 6%          16.7µs ± 6%  1.25%  (p=0.025 n=39+34) BM_KernelAsyncLaunch/16/process_time   20.4µs ± 9%          20.1µs ± 6%  1.68%  (p=0.043 n=39+37) BM_KernelAsyncLaunch/32/process_time   20.9µs ± 7%          20.6µs ± 6%    ~     (p=0.102 n=39+35) BM_KernelAsyncLaunch/64/process_time   21.4µs ± 8%          20.7µs ±10%  3.19%  (p=0.001 n=39+39) BM_KernelAsyncLaunch/128/process_time  23.6µs ± 9%          22.3µs ±12%  5.54%  (p=0.000 n=40+40) BM_KernelAsyncLaunch/256/process_time  27.7µs ± 7%          25.9µs ±10%  6.42%  (p=0.000 n=39+39) ```,2025-01-23T01:06:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85522
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-23T00:49:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85521
copybara-service[bot],[XLA:MSA] Add an option in WindowPrefetch to switch between different modes for window prefetch,"[XLA:MSA] Add an option in WindowPrefetch to switch between different modes for window prefetch In our current implementation of window prefetch, we don't actually perform prefetching. We only expose the window buffers from the reserved scoped memory. Because of that, we should distinguish these two different implementations. The one with prefetch is window prefetch, and the one without can be called window exposure. For window exposure, we don't need to call Prefetch, which allocates the buffer and prefetches the window. So in this change, we added an option to allow us to switch between window exposure and window prefetch and set window exposure as the default mode. Once we have the prefetch logic checked in, we can switch to use the window prefetch mode.",2025-01-23T00:43:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85520
copybara-service[bot],"Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `all-gather` with an `all-to-all`.","Move the sharding axes from dimensions that need replication to batch dimensions, such that we replace an `allgather` with an `alltoall`. Given the following input ``` ENTRY entry {   %param0 = f32[14,257] parameter(0), sharding={devices=[1,2]0,1}   %param1 = f32[14,116] parameter(1), sharding={devices=[1,2]0,1}   ROOT %concatenate = f32[14,373] concatenate(%param0, %param1),     dimensions={1}, sharding={devices=[1,2]0,1} } ``` The partitioner generates allgather before this change ``` ENTRY %entry_spmd (param: f32[14,129], param.1: f32[14,58]) > f32[14,187] {   %param = f32[14,129]{1,0} parameter(0), sharding={devices=[1,2] f32[14,187] {   %param = f32[14,129]{1,0} parameter(0), sharding={devices=[1,2]<=[2]}   %reshape.1 = f32[2,7,129]{2,1,0} reshape(f32[14,129]{1,0} %param)   %alltoall = f32[2,7,129]{2,1,0} alltoall(f32[2,7,129]{2,1,0} %reshape.1), channel_id=1, replica_groups={{0,1}}, dimensions={0}   %transpose = f32[7,2,129]{2,0,1} transpose(f32[2,7,129]{2,1,0} %alltoall), dimensions={1,0,2}   %reshape.2 = f32[7,258]{1,0} reshape(f32[7,2,129]{2,0,1} %transpose)   %slice = f32[7,257]{1,0} slice(f32[7,258]{1,0} %reshape.2), slice={[0:7], [0:257]}   %param.1 = f32[14,58]{1,0} parameter(1), sharding={devices=[1,2]<=[2]}   %reshape.5 = f32[2,7,58]{2,1,0} reshape(f32[14,58]{1,0} %param.1)   %alltoall.1 = f32[2,7,58]{2,1,0} alltoall(f32[2,7,58]{2,1,0} %reshape.5), channel_id=2, replica_groups={{0,1}}, dimensions={0}   %transpose.1 = f32[7,2,58]{2,0,1} transpose(f32[2,7,58]{2,1,0} %alltoall.1), dimensions={1,0,2}   %reshape.6 = f32[7,116]{1,0} reshape(f32[7,2,58]{2,0,1} %transpose.1)   %concatenate.1 = f32[7,373]{1,0} concatenate(f32[7,257]{1,0} %slice, f32[7,116]{1,0} %reshape.6), dimensions={1}   %constant.20 = f32[] constant(0)   %pad = f32[7,374]{1,0} pad(f32[7,373]{1,0} %concatenate.1, f32[] %constant.20), padding=0_0x0_1   %reshape.9 = f32[7,2,187]{2,1,0} reshape(f32[7,374]{1,0} %pad)   %alltoall.2 = f32[7,2,187]{2,1,0} alltoall(f32[7,2,187]{2,1,0} %reshape.9), channel_id=3, replica_groups={{0,1}}, dimensions={1}   %transpose.2 = f32[2,7,187]{2,0,1} transpose(f32[7,2,187]{2,1,0} %alltoall.2), dimensions={1,0,2}   ROOT %reshape.10 = f32[14,187]{1,0} reshape(f32[2,7,187]{2,0,1} %transpose.2) } ```",2025-01-22T23:48:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85519
copybara-service[bot],Integrate LLVM at llvm/llvm-project@c6e7b4a61ab8,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match c6e7b4a61ab8,2025-01-22T23:40:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85518
copybara-service[bot],Remove check for ROCm 5.7.0. Test should fail instead.,Remove check for ROCm 5.7.0. Test should fail instead.,2025-01-22T23:38:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85517
copybara-service[bot],[xla:cpu] Fix max os compilation error,[xla:cpu] Fix max os compilation error,2025-01-22T23:31:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85516
copybara-service[bot],Use StreamExecutor::CreateMemoryAllocator and StreamExecutorAllocator in common_runtime instead of DeviceMemAllocator.,Use StreamExecutor::CreateMemoryAllocator and StreamExecutorAllocator in common_runtime instead of DeviceMemAllocator.,2025-01-22T23:13:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85515
copybara-service[bot],[PJRT:C] Add PJRT_Client_DmaMap and PJRT_Client_DmaUnmap to PJRT C Api.,[PJRT:C] Add PJRT_Client_DmaMap and PJRT_Client_DmaUnmap to PJRT C Api.,2025-01-22T23:03:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85514
copybara-service[bot],litert: Add a rule to generate LiteRt C runtime shared library,litert: Add a rule to generate LiteRt C runtime shared library,2025-01-22T22:49:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85513
copybara-service[bot],[StableHLO] Port TF bounded dynamism MHLO fixes to StableHLO.,[StableHLO] Port TF bounded dynamism MHLO fixes to StableHLO.,2025-01-22T22:46:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85512
copybara-service[bot],Fixes double free crash,Fixes double free crash,2025-01-22T22:14:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85511
copybara-service[bot],Rollback of breaking change.,Rollback of breaking change. Reverts abccb28fff8048a0601e33a6ef7bd9ac0041da39,2025-01-22T21:53:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85510
copybara-service[bot],[JAX] Optimize array shard reordering,[JAX] Optimize array shard reordering This change adds a C++ implementation that uses `xla::ifrt::RemapArrays` to reorder shards of an array. This avoids creating intermediate singledevice arrays and accelerates reordering shards within `jax.device_put()` implementation.,2025-01-22T20:55:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85509
copybara-service[bot],[xla:cpu] Enable concurrency-optimized schedule by default,[xla:cpu] Enable concurrencyoptimized schedule by default Improve wall time at the cost of CPU time. ``` name                                                                    old cpu/op   new cpu/op   delta BM_HloModule/shorts_ranking_v2.b333429386.20240606/process_time         25.5ms ± 6%  32.3ms ± 9%  +26.71%  (p=0.000 n=140+140) BM_HloModule/torax_sparc_prd_theta_method_block_residual/process_time    973µs ± 5%  1093µs ± 3%  +12.28%  (p=0.000 n=136+138) BM_HloModule/torax_sparc_prd_theta_method_block_jacobian/process_time    972µs ± 4%  1091µs ± 3%  +12.19%  (p=0.000 n=135+140) BM_HloModule/diffrax.b380012920/process_time                            17.2ms ± 2%  17.3ms ± 2%   +0.29%  (p=0.000 n=127+134) name                                                                    old time/op          new time/op          delta BM_HloModule/shorts_ranking_v2.b333429386.20240606/process_time         8.39ms ± 6%          6.67ms ± 8%  20.52%  (p=0.000 n=140+140) BM_HloModule/torax_sparc_prd_theta_method_block_residual/process_time    867µs ± 4%           856µs ± 3%   1.31%  (p=0.000 n=136+138) BM_HloModule/torax_sparc_prd_theta_method_block_jacobian/process_time    868µs ± 3%           855µs ± 2%   1.48%  (p=0.000 n=132+140) BM_HloModule/diffrax.b380012920/process_time                            17.2ms ± 2%          17.3ms ± 2%   +0.28%  (p=0.001 n=127+134) ```,2025-01-22T19:46:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85508
copybara-service[bot],Use StreamExecutor::CreateMemoryAllocator in gpu_helpers.cc when MemoryType::kUnified is used.,Use StreamExecutor::CreateMemoryAllocator in gpu_helpers.::kUnified is used.,2025-01-22T19:38:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85507
copybara-service[bot],#sdy support ffi Python callbacks during round tripping.,sdy support ffi Python callbacks during round tripping.,2025-01-22T19:18:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85506
copybara-service[bot],Remove check for ROCm 5.7.0. Test should fail instead.,Remove check for ROCm 5.7.0. Test should fail instead.,2025-01-22T19:03:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85505
copybara-service[bot],[XLA:TPU] Document the computational cost of sorting the computations within a module.,[XLA:TPU] Document the computational cost of sorting the computations within a module.,2025-01-22T18:50:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85504
copybara-service[bot],[IFRT] Dump HLO before MHLO conversion in TFRT/IFRT,[IFRT] Dump HLO before MHLO conversion in TFRT/IFRT,2025-01-22T18:46:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85503
copybara-service[bot],Simplify HloRunnerPjRt use of `PjRtClient::BufferFromHostLiteral`.,Simplify HloRunnerPjRt use of `PjRtClient::BufferFromHostLiteral`.,2025-01-22T18:46:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85502
copybara-service[bot],#sdy Add mesh mesh deduplication pass.,"sdy Add mesh mesh deduplication pass. For a module with top level mesh symbols, this will merge any meshes with the same axis sizes (and in the same order) but different axis names to a single common mesh.  Since Shardy can't propagate across different meshes, but JAX export may create different meshes, we should merge these. Generally this pass should be a noop under JAX+XLA",2025-01-22T18:38:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85501
copybara-service[bot],Rollback of breaking change.,Rollback of breaking change. Reverts abccb28fff8048a0601e33a6ef7bd9ac0041da39,2025-01-22T18:16:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85500
copybara-service[bot],Internal only change.,Internal only change.,2025-01-22T18:14:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85499
copybara-service[bot],Update API compatibility test to Python3.12 values.,Update API compatibility test to Python3.12 values.,2025-01-22T17:24:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85498
copybara-service[bot],[XLA] Copy metadata to combined collectives.,[XLA] Copy metadata to combined collectives. This is a naive approach that copies the metadata from the first combined collective operation. We can explore a more complex solution if we encounter use cases where this is not correct.,2025-01-22T17:16:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85497
copybara-service[bot],#litert Change `LiteRtCompilationOptions` to an opaque type.,"litert Change `LiteRtCompilationOptions` to an opaque type.  Add a C++ wrapper type for easier management.  Make the compile options mandatory in `CompiledModel::Create`. This aligns   the default value for hardware accelerator selection (depending on how the   options were specified, you would either get `None` or `Cpu`).",2025-01-22T16:32:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85496
copybara-service[bot],Integrate LLVM at llvm/llvm-project@b7abc510c515,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match b7abc510c515,2025-01-22T15:35:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85495
copybara-service[bot],Integrate LLVM at llvm/llvm-project@d33e33fde770,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match d33e33fde770,2025-01-22T14:47:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85494
copybara-service[bot],Integrate Triton up to [515467a9](https://github.com/openai/triton/commits/515467a9b42b92f24af988b83b7de6c8910a0a69),Integrate Triton up to 515467a9,2025-01-22T14:23:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85493
copybara-service[bot],[XLA:GPU] Add --merge mode to matmul gen perf table tool.,[XLA:GPU] Add merge mode to matmul gen perf table tool.,2025-01-22T14:00:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85492
copybara-service[bot],Move fusion test tools to backends/gpu/codegen/tools directory.,Move fusion test tools to backends/gpu/codegen/tools directory.,2025-01-22T13:47:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85491
copybara-service[bot],"Add `LITERT_ASSIGN_OR_RETURN(decl, expr [, return_value])` macro for `litert::Expected`.","Add `LITERT_ASSIGN_OR_RETURN(decl, expr [, return_value])` macro for `litert::Expected`.  Returns the error if `expr` holds an error status.  If `return_value` is specified, it is used to build the returned error.  Assigns the value stored in `expr`'s return to `decl` otherwise.",2025-01-22T12:59:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85490
copybara-service[bot],Improve `LITERT_RETURN_IF_ERROR`.,Improve `LITERT_RETURN_IF_ERROR`.  Add tests.  Handle custom return values.  Return a `LiteRtStatus` or a `litert::Expected` depending on what is needed. Replace calls to `LITERT_RETURN_VAL_IF_NOT_OK` and `LITERT_RETURN_STATUS_IF_NOT_OK` with `LITERT_RETURN_IF_ERROR`.,2025-01-22T11:56:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85489
copybara-service[bot],Rename `LITERT_EXPECT_OK` to `LITERT_RETURN_IF_ERROR` to better align with other internal code semantics.,Rename `LITERT_EXPECT_OK` to `LITERT_RETURN_IF_ERROR` to better align with other internal code semantics.,2025-01-22T11:52:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85488
copybara-service[bot],PR #21680: [ds-fusion] Fix dependencies in call inliner,"PR CC(FixedLengthRecordDataset does not support compression_type): [dsfusion] Fix dependencies in call inliner Imported from GitHub PR https://github.com/openxla/xla/pull/21680 The call inliner pass does not honor control dependencies of the call instruction. This patch handles that. All the newly inlined instructions now depend on the predecessors of the call instruction to be inlined, and all the successors of the old call instruction will depend on the newly inlined root instruction. Added test for the same. Copybara import of the project:  3393d528f0933d9dbf7dace9f45a5b49191437e6 by Shraiysh Vaishay : [dsfusion] Fix dependencies in call inliner The call inliner pass does not honor control dependencies of the call instruction. This patch handles that. All the newly inlined instructions now depend on the predecessors of the call instruction to be inlined, and all the successors of the old call instruction will depend on the newly inlined root instruction. Added test for the same. Merging this change closes CC(FixedLengthRecordDataset does not support compression_type) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21680 from shraiysh:fix_call_inliner_relay_deps 3393d528f0933d9dbf7dace9f45a5b49191437e6",2025-01-22T11:51:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85487
copybara-service[bot],"PR #21430: When a profile is not found for the current device, default to the latest available instead of sm_86.","PR CC(Fix markdown indentation in install_raspbian.md): When a profile is not found for the current device, default to the latest available instead of sm_86. Imported from GitHub PR https://github.com/openxla/xla/pull/21430 Often, when a profile is not available, it is for a new device. So, it makes sense to grab the latest available profile, instead of the somewhat arbitrary sm_86. Copybara import of the project:  640cceebf2d43afbc43976a48e384a6444ca71bd by Dimitris Vardoulakis : When a profile is not found for the current device, default to the latest available instead of sm_86.  b25dc948527ebf0a26da014b17e9e84cdc1f00e3 by Dimitris Vardoulakis : Address review comments.  fe5a3933d7464f1008cbf4eeb9fca0e43f1652f9 by Dimitris Vardoulakis : Code review comments Merging this change closes CC(Fix markdown indentation in install_raspbian.md) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21430 from dimvar:defaultprofilelatest fe5a3933d7464f1008cbf4eeb9fca0e43f1652f9",2025-01-22T11:45:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85486
copybara-service[bot],[xla:cpu] drop Legacy from CompileLegacyCpuExecutable,"[xla:cpu] drop Legacy from CompileLegacyCpuExecutable This was considered legacy under the XLA Runtime transition, but XLA Runtime is no more, so this isn't legacy any more. While at it, remove a stale declaration of an XlaRuntime method whose definition was removed long ago.",2025-01-22T11:42:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85485
copybara-service[bot],[xla:cpu] xla.proto: remove dead xla_cpu fields,[xla:cpu] xla.proto: remove dead xla_cpu fields,2025-01-22T11:42:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85484
copybara-service[bot],[tf:xla] tfcompile: remove remnants of HloLowering compilation path,"[tf:xla] tfcompile: remove remnants of HloLowering compilation path This has long been dead in XLA:CPU; see [1] from Nov 2023. Remove it. While at it, also remove the ENABLE_MLIR_BRIDGE_TEST ifdefs in tfcompile_test.cc; that compiletime flag was removed in Sept 2024 [2]. [1] https://github.com/tensorflow/tensorflow/commit/3f8022ead6ec [2] https://github.com/tensorflow/tensorflow/commit/587df896857e",2025-01-22T11:40:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85483
copybara-service[bot],[XLA:CPU] Don't fuse concatenate if it has more than 8 arguments,[XLA:CPU] Don't fuse concatenate if it has more than 8 arguments,2025-01-22T10:53:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85482
copybara-service[bot],PR #21680: [ds-fusion] Fix dependencies in call inliner,"PR CC(FixedLengthRecordDataset does not support compression_type): [dsfusion] Fix dependencies in call inliner Imported from GitHub PR https://github.com/openxla/xla/pull/21680 The call inliner pass does not honor control dependencies of the call instruction. This patch handles that. All the newly inlined instructions now depend on the predecessors of the call instruction to be inlined, and all the successors of the old call instruction will depend on the newly inlined root instruction. Added test for the same. Copybara import of the project:  3393d528f0933d9dbf7dace9f45a5b49191437e6 by Shraiysh Vaishay : [dsfusion] Fix dependencies in call inliner The call inliner pass does not honor control dependencies of the call instruction. This patch handles that. All the newly inlined instructions now depend on the predecessors of the call instruction to be inlined, and all the successors of the old call instruction will depend on the newly inlined root instruction. Added test for the same. Merging this change closes CC(FixedLengthRecordDataset does not support compression_type) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21680 from shraiysh:fix_call_inliner_relay_deps 3393d528f0933d9dbf7dace9f45a5b49191437e6",2025-01-22T10:39:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85481
copybara-service[bot],[XLA:CPU][roll forward] Read thunks from proto when loading executable.,[XLA:CPU][roll forward] Read thunks from proto when loading executable. Add ToProto support remaining Thunk types Reverts 444d561f1b15a7bd6d4d7c8ac8044de0977cae60,2025-01-22T10:38:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85480
copybara-service[bot],[XLA:CPU] Allow reassoc on accumulation value in EmitMulAdd,[XLA:CPU] Allow reassoc on accumulation value in EmitMulAdd,2025-01-22T10:31:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85479
copybara-service[bot],Fix potential dangling pointers.,Fix potential dangling pointers.,2025-01-22T10:29:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85478
weilhuan-quic,Integrate Op Builder with LiteRT Compile Part," WHAT We replace the compiler part with Qualcomm implementations. This PR include commits in 3 PRs below. Follow these PRs or commit should help you review. You can get more details in the PR descriptions. 1. https://github.com/jiunkaiy/tensorflow/pull/1 2. https://github.com/jiunkaiy/tensorflow/pull/3 3. https://github.com/jiunkaiy/tensorflow/pull/5  TEST You can checkout this branch and run this test: ``` bazel build  c opt cxxopt=std=c++20 //tensorflow/lite/experimental/litert/vendors/qualcomm/compiler:qnn_compiler_plugin_test ./bazelbin/tensorflow/lite/experimental/litert/vendors/qualcomm/compiler/qnn_compiler_plugin_test ``` I disable these models because I don't have them.  kFeedForwardModel,  kKeyEinsumModel,  kQueryEinsumModel,  kValueEinsumModel,  kAttnVecEinsumModel,  kROPEModel,  kLookUpROPEModel,  kRMSNormModel,  kSDPAModel,  kAttentionModel,  kTransformerBlockModel,  kQSimpleMul16x16Model,  kQMulAdd16x16Model,  kQQueryEinsum16x8Model,  kQKeyEinsum16x8Model,  kQVauleEinsum16x8Model,  kQAttnVecEinsum16x8Model And you will see ``` [] Global test environment teardown [==========] 55 tests from 3 test suites ran. (338 ms total) [  PASSED  ] 54 tests. [  FAILED  ] 1 test, listed below: [  FAILED  ] SupportedOpsTest/QnnPluginOpValidationTest.SupportedOpsTest/4, where GetParam() = ""simple_slice_op.tflite"" ``` There are some bugs in simple_slice_op.mlir so the op validation will fail.",2025-01-22T10:19:36Z,comp:lite size:XL,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85477
weilhuan-quic,Support Qnn Wrappers for LiteRt," WHAT  Basic wrapper for QNN types, handle dynamic resources along with wrapper instances.  Make these wrappers independent to LiteRT/tflite  Only depend on QNN and STL   `ScalarParamWrapper`  Wrap `Qnn_Param_t` with `QNN_PARAMTYPE_SCALAR` for `paramType`   Choose correct `QNN_DATATYPE` based on the data type  `TensorParamWrapper`  Wrap `Qnn_Param_t` with `QNN_PARAMTYPE_TENSOR` for `paramType`  `UndefinedQuantizeParamsWrapper`  Wrap `Qnn_QuantizeParams_t`  Default for quantization parameter  `ScaleOffsetQuantizeParamsWrapper`  Wrap `Qnn_QuantizeParams_t` for pertensor quantization   `AxisScaleOffsetQuantizeParamsWrapper`  Wrap `Qnn_QuantizeParams_t`  for peraxis quantization  `TensorWrapper`  Wrap `Qnn_TensorType_t`  Handle dynamic resource, e.g. name, dimensions, weight data.  `OpWrapper`  Wrap `Qnn_OpConfig_t`  Handle dynamic resource, e.g. name, input output tensors, params",2025-01-22T09:57:06Z,awaiting review comp:lite size:L,closed,0,1,https://github.com/tensorflow/tensorflow/issues/85476,> Can this be developed outside of TF? As a plugin or a repo that uses TF as a dependency? >  > We're trying to reduce the amount of code that lands into the repo to not get (for at least the third time) into a situation where code becomes unmaintained because the original submitters abandoned the project Hi  let's chat offline
copybara-service[bot],PR #21412: [GPU] Use single cuDNN handle for graph deserialization.,"PR CC(Error in Distribution Strategy with train_and_evaluate): [GPU] Use single cuDNN handle for graph deserialization. Imported from GitHub PR https://github.com/openxla/xla/pull/21412 There are two ways to get a cuDNN handle in cuda_dnn.cc. Execution uses a single mutexlocked handle (GetHandle()); compilation uses disposable temporary handles for efficient parallelism (GetLocalHandle()). Deserialization, which happens during initialization of the GPU executable and is serial, can use either way, but is slightly more efficient when uses the single mutexlocked handle. Copybara import of the project:  4986c6bc200eb12de7b6ffef859800d2a9da3ff5 by Ilia Sergachev : [GPU] Use single cuDNN handle for graph deserialization. There are two ways to get a cuDNN handle in cuda_dnn.cc. Execution uses a single mutexlocked handle (GetHandle()); compilation uses disposable temporary handles for efficient parallelism (GetLocalHandle()). Deserialization, which happens during initialization of the GPU executable and is serial, can use either way, but is slightly more efficient when uses the single mutexlocked handle. Merging this change closes CC(Error in Distribution Strategy with train_and_evaluate) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21412 from openxla:cudnn_handle_deserialization 4986c6bc200eb12de7b6ffef859800d2a9da3ff5",2025-01-22T09:49:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85475
copybara-service[bot],Move CudaComputeCapability into its own file,Move CudaComputeCapability into its own file This moves the type into its own file (one class per file policy). This change only moves the code. Cleanups will follow separately.,2025-01-22T09:48:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85474
copybara-service[bot],[tpu/kernels] _pywrap_sparse_core_layout_header_only: Fix deps,"[tpu/kernels] _pywrap_sparse_core_layout_header_only: Fix deps Add missing :btree dep, remove unneeded :log dep.",2025-01-22T09:40:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85473
copybara-service[bot],[fuzzing] Tag tf_ops_fuzz_target_lib() targets manual,[fuzzing] Tag tf_ops_fuzz_target_lib() targets manual,2025-01-22T09:40:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85472
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:29:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85471
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:26:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85470
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:22:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85469
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:20:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85468
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:20:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85467
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:19:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85466
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:17:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85465
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:17:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85464
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:17:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85463
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:17:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85462
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:16:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85461
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:15:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85460
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:14:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85459
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:14:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85458
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:13:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85457
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:13:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85456
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-22T09:12:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85455
copybara-service[bot],[XLA:GPU] Cleanup includes and enforce op sorting in `triton/support_test.cc`,[XLA:GPU] Cleanup includes and enforce op sorting in `triton/support_test.cc`,2025-01-22T09:11:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85454
copybara-service[bot],Integrate LLVM at llvm/llvm-project@7084110518f9,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 7084110518f9,2025-01-22T09:05:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85453
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-22T08:43:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85452
copybara-service[bot],[XLA:GPU] Deprecate `--xla_gpu_triton_fusion_level`.,[XLA:GPU] Deprecate `xla_gpu_triton_fusion_level`.,2025-01-22T08:12:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85451
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-22T08:00:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85450
rizkyy702,issue, Issue type Others  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? import tensorflow as tf from google.colab import files uploaded_zip_file = list(files.upload().keys())[0] !unzip $uploaded_zip_file tf_model = tf.keras.models.load_model(uploaded_zip_file[:4])  Standalone code to reproduce the issue ```shell import tensorflow as tf from google.colab import files uploaded_zip_file = list(files.upload().keys())[0] !unzip $uploaded_zip_file tf_model = tf.keras.models.load_model(uploaded_zip_file[:4]) ```  Relevant log output ```shell ```,2025-01-22T07:52:23Z,type:others,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85449,"Closing as there's no error message, no informative title and the template doesn't seem to be filled correctly (TF 2.0 and nightly surely have diverged)",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Update the external buffer storage to use an ID based approach,Update the external buffer storage to use an ID based approach,2025-01-22T07:40:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85448
copybara-service[bot],Add serialization support for multi-byte code compilation results.,Add serialization support for multibyte code compilation results.,2025-01-22T07:39:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85447
copybara-service[bot],[XLA:GPU] Add triton support test for collective-broadcast op,[XLA:GPU] Add triton support test for collectivebroadcast op,2025-01-22T07:28:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85446
copybara-service[bot],Open up python targets to the public for LiteRT tflite/python/...,Open up python targets to the public for LiteRT tflite/python/...,2025-01-22T07:12:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85445
copybara-service[bot],PR #21616: [ROCM] Avoiding lazy initialization of blas handles on ROCM,PR CC([Java] Render secondary factory for default output types): [ROCM] Avoiding lazy initialization of blas handles on ROCM Imported from GitHub PR https://github.com/openxla/xla/pull/21616 Calling AsBlas() under stream capture causes runtime errors on ROCM side: ``` Hip error: 'operation would make the legacy stream depend on a capturing blocking stream'(906) at /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/hipBLASLt/library/src/amd_detail/hipblaslt.cpp:135 E0000 00:00:1737366478.217495  154310 rocm_blas.cc:130] failed to create rocBLAS handle: rocblas_status_internal_error ``` We avoid this by initializing Blas handles earlier in GpuExecutor::Init() function. Also I added a shardedcomputation test which verifies this issue.  rotation: would you please have a look? Copybara import of the project:  e10e5cd62f5c8ec1b3429391dc2e05e4e123cf07 by Pavel Emeliyanenko : Avoiding lazy initialization of blas handles on ROCM  645b83c1b07be74fba7fa994a2b764b3549335ac by Pavel Emeliyanenko : added missing hlo file  4d4ef9129bfe7afd8946e355b74d0600aaa3a130 by Pavel Emeliyanenko : added mutexes to InitBlas and AsBlas functions Merging this change closes CC([Java] Render secondary factory for default output types) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21616 from ROCm:ci_blas_stream_capture_fix 4d4ef9129bfe7afd8946e355b74d0600aaa3a130,2025-01-22T07:03:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85444
copybara-service[bot],Reimplement llvm::MapVector semantics and remove llvm dep.,Reimplement llvm::MapVector semantics and remove llvm dep.,2025-01-22T06:33:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85443
copybara-service[bot],Minor readability cleanup in latency hiding scheduler.,Minor readability cleanup in latency hiding scheduler.,2025-01-22T03:55:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85442
copybara-service[bot],[MHLO] Handle dynamic dimensions in HLO<->MHLO,"[MHLO] Handle dynamic dimensions in HLOMHLO  Fix creating constant zero for ConvertOp HLO>MHLO translation  Fix broadcast in dim bounded lowering from MHLO>HLO  Don't use StableHLO verification methods on MHLO ReshapeOp with bounded dynamic outputs ``` $ cat /tmp/t.hlo  HloModule main, entry_computation_layout={(pred[pred[ pred[>) > tensor> {   %0 = mhlo.constant dense : tensor   %1 = ""mhlo.get_dimension_size""(%arg0)  : (tensor>) > tensor   %2 = ""mhlo.set_dimension_size""(%0, %1)  : (tensor, tensor) > tensor>   %3 = mhlo.compare  NE, %arg0, %2 : (tensor>, tensor>) > tensor>   return %3 : tensor> } ``` Currently this fails when trying to create the `mhlo.constant dense` that gets fed into compare since constants cannot have a bounded size.",2025-01-22T03:54:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85441
copybara-service[bot],Integrate LLVM at llvm/llvm-project@7084110518f9,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 7084110518f9,2025-01-22T03:11:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85440
RyanRankeval,Spam,Spam removed,2025-01-22T02:32:47Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/85439,Please don't spam
copybara-service[bot],"Remove checks for MUL op, MLDrift supports the matrix-vector multiplication.","Remove checks for MUL op, MLDrift supports the matrixvector multiplication.",2025-01-22T01:32:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85438
copybara-service[bot],Move `parallel_gpu_execute.sh` to `build_tools/ci`,Move `parallel_gpu_execute.sh` to `build_tools/ci`,2025-01-22T00:40:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85437
copybara-service[bot],Minor code cleanup for latency hiding scheduler.,"Minor code cleanup for latency hiding scheduler. Refactor latency hiding profile read code, and add a helper function to determine whether an accuracy checker should be added or not. This is a noop.",2025-01-22T00:30:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85436
copybara-service[bot],Update visibility for Jax serving runtime experimental.,Update visibility for Jax serving runtime experimental.,2025-01-21T23:39:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85435
copybara-service[bot],[XLA] Add ragged-all-to-all support to latency hiding scheduler.,[XLA] Add raggedalltoall support to latency hiding scheduler.,2025-01-21T23:08:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85434
copybara-service[bot],[xla:cpu] Use Worker to parallelize host kernel execution,[xla:cpu] Use Worker to parallelize host kernel execution Move WorkerQueue and Worker to top level backends/cpu/runtime folder and use it for host kernels and XNNPACK parallel for runner.,2025-01-21T23:06:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85433
copybara-service[bot],Add support for creating MemoryType::kUnified MemoryAllocators to the StreamExecutor derived classes that support UnifiedMemoryAllocate interfaces.,Add support for creating MemoryType::kUnified MemoryAllocators to the StreamExecutor derived classes that support UnifiedMemoryAllocate interfaces.,2025-01-21T22:43:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85432
copybara-service[bot],"Give XLA it's own .bazelrc, remove the TensorFlow bazelrc from openxla/xla","Give XLA it's own .bazelrc, remove the TensorFlow bazelrc from openxla/xla",2025-01-21T22:35:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85431
copybara-service[bot],Remove unreferenced PreprocessGraphdef.,Remove unreferenced PreprocessGraphdef.,2025-01-21T22:33:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85430
copybara-service[bot],Fix a segmentation fault issue when the -c opt is turned on. Keep the function object around for absl::FunctionRef,Fix a segmentation fault issue when the c opt is turned on. Keep the function object around for absl::FunctionRef,2025-01-21T22:08:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85429
copybara-service[bot],[MSA] Microoptimizations in `AsynchronousCopyResource`.,"[MSA] Microoptimizations in `AsynchronousCopyResource`. Based on a profiling the memoryspace assignment algorithm, this change makes two small optimizations to `AsynchronousCopyResource`: * Pass a prereserved `std::vector>` instead of an `absl::flat_hash_map` to capture the changes to `delays`, because we do not need random access to the map, and a vector is faster to resize than a hash map. * Cache the raw data pointers from `std::vector` to avoid the overhead of bounds and null checking in the hardened `std::vector` implementation. * Replace the simple functions in `time_utils.cc` with inline implementations in `time_utils.h`: since these boil down to adding or subtracting `1`, the resulting code will be smaller and more efficient (and less likely to spill FP registers to the stack). * Refactor the innerloop that writes `delay_changes` so that the floatingpoint operations are not separated by a datadependent call, and we can keep more `float`s in registers.",2025-01-21T22:06:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85428
copybara-service[bot],Stablehlo integrations for result accuracy on Exp op.,Stablehlo integrations for result accuracy on Exp op.,2025-01-21T22:00:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85427
copybara-service[bot],`jax.tree.map`: preserve dict key order,"`jax.tree.map`: preserve dict key order This does not affect the default flattening order in general, but rather only affects the flattening order when used within `tree_map`.",2025-01-21T21:52:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85426
copybara-service[bot],Remove `use_parameter_layout_on_device`.,"Remove `use_parameter_layout_on_device`. With the removal of calls to `UpdateEntryComputationLayout`, it turns out this functionality is not necessary (and potentially harmful). Instead we opt to always respect the ECLprovided layout if the client supports it, or fall back to using the implementationdefined buffer ondevice layout otherwise. This patch also removes the remaining HloRunnerPjRt calls to `UpdateEntryComputationLayout`.",2025-01-21T21:35:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85425
copybara-service[bot],xla_compile_main: Make --output_file optional.,"xla_compile_main: Make output_file optional. Some compileonly use cases want the optimized binary or even just to verify that compilation succeeded, and the final binary can be large.",2025-01-21T21:34:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85424
copybara-service[bot],Enable dense resource import by default,Enable dense resource import by default This commit enables import of TF variables as dense resources by default during conversion in LiteRT Compiler.,2025-01-21T21:09:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85423
copybara-service[bot],Improve MSA readabilty by changing some auto types to their type name.,Improve MSA readabilty by changing some auto types to their type name. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21375 from shraiysh:while_loop_analysis a435fbd2eadc17269d7bccbe141dcf7a21cc20e8,2025-01-21T20:56:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85422
copybara-service[bot],Add separators to actually use `--split-input-file` flag.,Add separators to actually use `splitinputfile` flag.,2025-01-21T20:53:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85421
copybara-service[bot],Add HLO `RaggedAllToAll` --> `mhlo.custom_call @ragged_all_to_all` translation,"Add HLO `RaggedAllToAll` > `mhlo.custom_call ` translation Since `channel_handle` is used by select few ops, it's recommended to not import this attribute generally. Instead, just extract `channel_id` (`channel_type` is not used in this op) and set it as an `IntegerAttr` for roundtripping.",2025-01-21T20:18:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85420
copybara-service[bot],Execute host-to-host copies on the host.,Execute hosttohost copies on the host.,2025-01-21T20:13:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85419
primski,How do you install this using poetry on macos ?,"This still doesn't work  ``` [project] name = ""tf001"" version = ""0.1.0"" description = """" authors = [     {name = ""Me""} ] readme = ""README.md"" requirespython = "">=3.11"" [tool.poetry.dependencies] python = ""~3.11"" tensorflow = ""*"" [buildsystem] requires = [""poetrycore>=2.0.0,<3.0.0""] buildbackend = ""poetry.core.masonry.api"" ``` ```    Installing tensorflow (2.18.0): Failed   RuntimeError   Unable to find installation candidates for tensorflow (2.18.0)   at ~/Library/Application Support/pypoetry/venv/lib/python3.11/sitepackages/poetry/installation/chooser.py:86 in choose_for        82│         83│             links.append(link)        84│         85│         if not links:     →  86│             raise RuntimeError(f""Unable to find installation candidates for {package}"")        87│         88│          Get the best link        89│         chosen = max(links, key=lambda link: self._sort_key(package, link))        90│  Cannot install tensorflow. ``` ``` $ poetry version  Poetry (version 2.0.0) ``` So how do I install this ?",2025-01-21T20:05:28Z,stat:awaiting response type:support stale,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85418,"I removed it from pyproject.toml and used pip, it works with that. ","TF is developed with pip in mind, no one has tested if it can be installed with other systems (although likely it should, to the extent that they are compatible)",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Fix build rule of tensor_buffer,Fix build rule of tensor_buffer Exclude event..,2025-01-21T19:53:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85417
copybara-service[bot],Remove tf/compiler/mlir/lite/quantization/ir:QuantizationOpsTdFiles from tf/compiler/mlir/tfr:tfr_ops_td_files,Remove tf/compiler/mlir/lite/quantization/ir:QuantizationOpsTdFiles from tf/compiler/mlir/tfr:tfr_ops_td_files,2025-01-21T19:50:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85416
copybara-service[bot],Add a test case to capture QNN graph finalization failed on large 16-bits integer quantized Op.,Add a test case to capture QNN graph finalization failed on large 16bits integer quantized Op.,2025-01-21T19:24:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85415
copybara-service[bot],Fix definition with different parameter names,Fix definition with different parameter names The generated VhloAttrs.h.inc file has uses `odsPrinter` instead of `p`. I generally prefer descriptive variable names over one letter even though they are somewhat unambiguous in context.,2025-01-21T19:18:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85414
copybara-service[bot],Support multiple KV Cache layouts based on dimension of K and V tensors,Support multiple KV Cache layouts based on dimension of K and V tensors,2025-01-21T19:06:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85413
copybara-service[bot],Add mask as optional input for converter,Add mask as optional input for converter,2025-01-21T19:01:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85412
copybara-service[bot],Add transformation to allow transposed input to BatchMatMul,Add transformation to allow transposed input to BatchMatMul,2025-01-21T19:01:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85411
copybara-service[bot],Fix up the 2022 Win RBE config.,Fix up the 2022 Win RBE config.,2025-01-21T18:26:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85410
copybara-service[bot],test increasing the memory limit,test increasing the memory limit,2025-01-21T18:06:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85409
nickcama,numpy copy fix,"`astype()` function has `copy=True` as default, so this line was always resulting in a copy. By propagating the `copy=None`, which is the function default and which is also passed from Tensorflow, this forced extra copy is prevented. It will still perform the copy if `copy=True` is passed to the function.",2025-01-21T18:00:34Z,ready to pull size:XS python,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85408,"I added a unit test that shows copy was always happening before the change, even when copy=None is passed. And another test that shows that the copy does not happen after the change.",Added to the BUILD file.,"This is weird, on the CI we have I see this failure: ``` FAIL: test_no_copy_new_vs_old (__main__.NumpyCompatCopyBehaviorTest.test_no_copy_new_vs_old) NumpyCompatCopyBehaviorTest.test_no_copy_new_vs_old  Traceback (most recent call last):   File "".../tensorflow/python/util/numpy_compat_test.runfiles/org_tensorflow/tensorflow/python/util/numpy_compat_test.py"", line 44, in test_no_copy_new_vs_old     self.assertIsNot( AssertionError: unexpectedly identical: array([1., 2., 3.], dtype=float32) : Old code unexpectedly did NOT copy, but we expect it to always copy. ```",The CI is likely failing because it ran with numpy 1.x. And the issue is only with numpy 2.x. So I will only run the failing assert if numpy version >= 2.0.
copybara-service[bot],Migrate custom_call_test to always use PjRt for its test backend.,Migrate custom_call_test to always use PjRt for its test backend.,2025-01-21T17:37:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85407
copybara-service[bot],Fix a segmentation fault issue when the `-c opt` is turned on. Keep the function object around for absl::FunctionRef,Fix a segmentation fault issue when the `c opt` is turned on. Keep the function object around for absl::FunctionRef,2025-01-21T17:24:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85406
copybara-service[bot],Integrate LLVM at llvm/llvm-project@7084110518f9,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 7084110518f9,2025-01-21T16:36:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85405
copybara-service[bot],Internal utilities for LiteRT runtime.,Internal utilities for LiteRT runtime.,2025-01-21T16:34:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85404
Charlesnorris509,Fix TensorFlow 2.14.0 Installation/Run on C++ in Visual Studio Code," **This pull request addresses issue CC(Tensorflow 2.14.0 installation/run on C++ in visual studio code), which involves fixing the installation and running of TensorFlow 2.14.0 on C++ in Visual Studio Code.**  **Changes Made:** _BUILD File Updates: Added the pip_package target to tensorflow/tools/pip_package/BUILD. Ensured dependencies and sources for the pip_package target are correctly declared. Setup Script Updates: Updated tensorflow/tools/pip_package/setup.py to correctly configure for building the pip package. Manifest Updates: Ensured all necessary files are included in the package by updating tensorflow/tools/pip_package/MANIFEST.in._",2025-01-21T16:24:59Z,size:S invalid,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85403,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , Can you please sign CLA , thank you !",This is spam,> This is spam Sorry I'll make sure it doesn't happen again 😔,Please don't send LLM generated PRs and reviews.
copybara-service[bot],Prefer DMA buffers over other buffer types when internally creating tensor buffers.,Prefer DMA buffers over other buffer types when internally creating tensor buffers.,2025-01-21T16:22:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85402
copybara-service[bot],Add LiteRT accelerator API implementation.,Add LiteRT accelerator API implementation.,2025-01-21T16:21:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85401
copybara-service[bot],[XLA:GPU] Add matmul perf table gen sharding support via GNU parallel.,"[XLA:GPU] Add matmul perf table gen sharding support via GNU parallel. We support the partitioning granularity at HLO level, but in theory we can squeeze the most parallelism if we have granularity at HLO op level. This would probably require serialization of `ExplicitSpec` abstraction and introducing a twostep process in table generation. For now the implemented approach is good enough, we can revisit potential improvements later.",2025-01-21T15:26:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85400
copybara-service[bot],[XLA:GPU] Deduplicate dot specs in matmul perf table gen.,[XLA:GPU] Deduplicate dot specs in matmul perf table gen.,2025-01-21T15:14:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85399
copybara-service[bot],"[XLA:GPU] Clean up flags (and uses of) `--xla_gpu_enable_bf16_{3,6}way_gemm`.","[XLA:GPU] Clean up flags (and uses of) `xla_gpu_enable_bf16_{3,6}way_gemm`. Those are no longer necessary now that algorithms can be requested explicitly.",2025-01-21T14:23:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85398
copybara-service[bot],[XLA:GPU] Remove no-op flags `xla_gpu_enable_dot_strength_reduction`.,[XLA:GPU] Remove noop flags `xla_gpu_enable_dot_strength_reduction`.,2025-01-21T14:11:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85397
copybara-service[bot],[XLA:GPU] Rename `xla_gpu_enable_experimental_pipeline_parallelism_opt` to `xla_gpu_experimental_enable_pipeline_parallelism_opt`.,[XLA:GPU] Rename `xla_gpu_enable_experimental_pipeline_parallelism_opt` to `xla_gpu_experimental_enable_pipeline_parallelism_opt`. This is to follow the agreed upon flag nomenclature.,2025-01-21T14:10:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85396
copybara-service[bot],[XLA:GPU] Remove the no-op `xla_gpu_triton_fusion_level` from the debug options.,[XLA:GPU] Remove the noop `xla_gpu_triton_fusion_level` from the debug options.,2025-01-21T13:58:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85395
copybara-service[bot],[XLA:GPU] Deprecate `--xla_gpu_enable_dot_strength_reduction`.,[XLA:GPU] Deprecate `xla_gpu_enable_dot_strength_reduction`.,2025-01-21T13:41:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85394
copybara-service[bot],[pjrt] Migrated remaining uses of `CreateUninitializedBuffer` and `CreateErrorBuffer` to the `PjRtMemorySpace` variant,[pjrt] Migrated remaining uses of `CreateUninitializedBuffer` and `CreateErrorBuffer` to the `PjRtMemorySpace` variant This change also removes the deprecated `PjRtDevice` variants of these methods.,2025-01-21T13:39:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85393
copybara-service[bot],Captures std::string in Error class.,Captures std::string in Error class.,2025-01-21T13:34:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85392
copybara-service[bot],[XLA][NFC] Move all XLA:GPU flags into their own ordered section in `xla.proto`.,[XLA][NFC] Move all XLA:GPU flags into their own ordered section in `xla.proto`.,2025-01-21T13:33:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85391
copybara-service[bot],[XLA:GPU] update gemm_rewriter_fp8_test/SupportsF8NonMajorBatchDim,[XLA:GPU] update gemm_rewriter_fp8_test/SupportsF8NonMajorBatchDim A recent change to pass order changed the HLO and check started to fail. Remove the check completely as we don't really care which dimensions are used but whether nonmajor batch works at all.,2025-01-21T12:59:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85390
copybara-service[bot],[XLA:GPU] Add sweeping through HLOs support to matmul perf gen tool.,[XLA:GPU] Add sweeping through HLOs support to matmul perf gen tool.,2025-01-21T10:37:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85389
copybara-service[bot],[pjrt] CreateViewOfDeviceBuffer now accepts a memory instead of a device,"[pjrt] CreateViewOfDeviceBuffer now accepts a memory instead of a device I took a shortcut in some PjRt implementations and recovered the device from a memory space, even though in general a memory space can be attached to multiple devices.",2025-01-21T09:35:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85388
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:35:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85387
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:29:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85386
alvinwong64,Tensorflow 2.14.0 installation/run on C++ in visual studio code," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.14  Custom code Yes  OS platform and distribution macos 14.4  Mobile device _No response_  Python version _No response_  Bazel version 6.1  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Hi I would like to try to run a custom model on C++, how should I setup the tensorflow so that i could call and use function like i did in python?  Standalone code to reproduce the issue ```shell tried bazel build //tensorflow/tools/pip_package, yet it pops up error  Skipping '//tensorflow/tools/pip_package': no such target '//tensorflow/tools/pip_package:pip_package': target 'pip_package' not declared in package 'tensorflow/tools/pip_package' defined by /Users/XXXX/Documents/testing/tensorflow/tensorflow/tools/pip_package/BUILD (Tip: use `query ""//tensorflow/tools/pip_package:*""` to see all the targets in that package) WARNING: Target pattern parsing failed. ERROR: no such target '//tensorflow/tools/pip_package:pip_package': target 'pip_package' not declared in package 'tensorflow/tools/pip_package' defined by /Users/XXXX/Documents/testing/tensorflow/tensorflow/tools/pip_package/BUILD (Tip: use `query ""//tensorflow/tools/pip_package:*""` to see all the targets in that package) ```  Relevant log output ```shell ```",2025-01-21T09:26:43Z,stat:awaiting response type:build/install stale awaiting PR merge TF2.14,closed,0,8,https://github.com/tensorflow/tensorflow/issues/85385,"you need to set up TensorFlow for C++ development, the target //tensorflow/tools/pip_package:pip_package is specifically for creating Python pip packages, not for compiling TensorFlow for C++ use.  Install TensorFlow C++ Library,Build TensorFlow C++ API,Configure Build Settings,Build TensorFlow Shared Libraries then load and run your model",Could you guide me on how to install tensorflow c++ library and build it?,"yes ofc !  System Requirements: Linuxbased system,C++ compiler ,Python installed,Bazel  Install C++ Build Dependencies:  sudo aptget install buildessential Install Bazelisk  curl LO https://github.com/bazelbuild/bazelisk/releases/download/v1.10.0/bazelisklinuxamd64 chmod +x bazelisklinuxamd64 mv bazelisklinuxamd64 /usr/local/bin/bazel StepbyStep Installation: Clone the TensorFlow GitHub Repository git clone https://github.com/tensorflow/tensorflow.git cd tensorflow Configure the Build ./configure You’ll be prompted with several configuration options, including whether to enable CUDA (GPU support) and whether to build TensorFlow for specific systems or features. For building with GPU support, you’ll need to have the CUDA toolkit and cuDNN installed. Build the C++ Library bazel build //tensorflow:libtensorflow_cc.so This will build the TensorFlow C++ library as a shared object (.so) file. If you want to build a static library, you can modify the build command accordingly. For debugging and testing, you want to build the tensorflow_cc library with the c dbg flag: bazel build c dbg //tensorflow:libtensorflow_cc.so This will take some time, depending on your system's hardware. Include TensorFlow C++ Headers and Libraries located in the bazelbin/ directory.  bazelbin/ tensorflow","Hi while building, the build failed, with the use of verbose_failures ERROR: /Users/001522/Documents/testing/tensorflow/tensorflow/lite/acceleration/configuration/BUILD:41:8: Executing genrule //tensorflow/lite/acceleration/configuration:configuration_schema failed: (Exit 127): bash failed: error executing command (from target //tensorflow/lite/acceleration/configuration:configuration_schema)    (cd /private/var/tmp/_bazel_001522/497fb1666e8c0e255ebeec937d6e9115/execroot/org_tensorflow && \   exec env  \     PATH=/Users/001522/Library/Caches/bazelisk/downloads/sha256/c6b6dc17efcdf13fba484c6fe0b6c3361b888ae7b9573bc25a2dbe8c502448eb/bin:/Users/001522/anaconda3/bin:/Users/001522/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/usr/local/share/dotnet:~/.dotnet/tools \     PYTHON_BIN_PATH=/Users/001522/anaconda3/bin/python3 \     PYTHON_LIB_PATH=/Users/001522/anaconda3/lib/python3.10/sitepackages \     TF2_BEHAVIOR=1 \   /bin/bash c 'source external/bazel_tools/tools/genrule/genrulesetup.sh;          bazelout/darwin_arm64optexec50AE0418/bin/external/flatbuffers/flatc proto o bazelout/darwin_arm64opt/bin/tensorflow/lite/acceleration/configuration tensorflow/lite/acceleration/configuration/configuration.proto         perl p i e '\''s/tflite.proto/tflite/'\'' bazelout/darwin_arm64opt/bin/tensorflow/lite/acceleration/configuration/configuration.fbs     ')   Configuration: 156e18005636972e4bcf95905ead517f4bd9575d761ba24552404ce2ea964aec    Execution platform: //:platform : command not found Target //tensorflow:libtensorflow_cc.so failed to build",",  The build targets have changed. Please try the below command. `bazel build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow_cpu` And also it might fail if you don't have  dependencies (the instructions assume Ubuntu OSes, for other operating systems you have to find your own dependencies and install them). Also tensorflow 2.14 is pretty old, please try to install the tensorflow v2.18 and also try to follow the steps mentioned in the below document. https://www.tensorflow.org/install/sourcemacos Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:24:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85384
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:22:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85383
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:20:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85382
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:17:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85381
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:17:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85380
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:17:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85379
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:16:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85378
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:15:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85377
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:14:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85376
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:14:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85375
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:14:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85374
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:14:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85373
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-21T09:13:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85372
malakhovks,No matching distribution found for tensorflow==2.18.0 / Python 3.12.8 Alpine docker images," Try to install inside official Python 3.12.8 Alpine docker images > 3.12alpine⁠ Here is what I get: ``` ERROR: Ignored the following versions that require a different python version: 1.10.0 RequiresPython =3.8; 1.10.0rc1 RequiresPython =3.8; 1.10.0rc2 RequiresPython =3.8; 1.10.1 RequiresPython =3.8; 1.21.2 RequiresPython >=3.7,=3.7,=3.7,=3.7,=3.7,=3.7,=3.7,=3.7,=3.7,=3.7,=3.7,=3.8,=3.8,=3.8,=3.8,=3.8,=3.8,=3.8,=3.8,=3.8,=3.8,=3.8,<3.12 ERROR: Could not find a version that satisfies the requirement tensorflow==2.18.0 (from versions: none) ERROR: No matching distribution found for tensorflow==2.18.0 ``` On 3.12.0 and 3.11 the same issue",2025-01-21T08:36:18Z,,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85371," TensorFlow currently does not support Python 3.12 or Alpine Linux directly,Instead of Alpine, use a Debianbased image like python:3.11slim, which is compatible with TensorFlow","> TensorFlow currently does not support Python 3.12 or Alpine Linux directly,Instead of Alpine, use a Debianbased image like python:3.11slim, which is compatible with TensorFlow Thanks! Will do!"
copybara-service[bot],"PR #21430: When a profile is not found for the current device, default to the latest available instead of sm_86.","PR CC(Fix markdown indentation in install_raspbian.md): When a profile is not found for the current device, default to the latest available instead of sm_86. Imported from GitHub PR https://github.com/openxla/xla/pull/21430 Often, when a profile is not available, it is for a new device. So, it makes sense to grab the latest available profile, instead of the somewhat arbitrary sm_86. Copybara import of the project:  640cceebf2d43afbc43976a48e384a6444ca71bd by Dimitris Vardoulakis : When a profile is not found for the current device, default to the latest available instead of sm_86.  b25dc948527ebf0a26da014b17e9e84cdc1f00e3 by Dimitris Vardoulakis : Address review comments.  fe5a3933d7464f1008cbf4eeb9fca0e43f1652f9 by Dimitris Vardoulakis : Code review comments Merging this change closes CC(Fix markdown indentation in install_raspbian.md) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21430 from dimvar:defaultprofilelatest fe5a3933d7464f1008cbf4eeb9fca0e43f1652f9",2025-01-21T08:05:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85370
copybara-service[bot],Do not try to move copy over copy.,"Do not try to move copy over copy. Copies are considered unary elementwise ops. We need to make sure we don't try to move a copy over a copy, otherwise the MoveCopyToUsers pass will not converge to a fixed point.",2025-01-21T06:40:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85369
copybara-service[bot],[xla:cpu] Move work parallelization implementation to work_queue.h,[xla:cpu] Move work parallelization implementation to work_queue.h,2025-01-21T04:45:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85368
refraction-ray,Fix incorrect implementation of parallel matmul,"The current implementation for vectorized `matmul` misuse adjoint for transpose, which could leads to incorrect results for ``matmul`` within `vectorized_map` and complex dtypes (commonly used in quantum computing simulations such as in TensorCircuitNG).  Please see https://github.com/tensorflow/tensorflow/issues/52148 for details. Examples demonstrating the incorrect results with current implementation ```python import tensorflow as tf import numpy as np def matmul_adjoint(args):     t1, t2 = args     return tf.matmul(t1, t2, adjoint_a=False, adjoint_b=True) def matmul_transpose(args):     t1, t2 = args     return tf.matmul(t1, t2, transpose_a=False, transpose_b=True) def test_vectorized_matmul(dtype):     rdtype= np.ones(0, dtype).real.dtype     if dtype in (np.complex64, np.complex128):         a = np.random.rand(2,4,4).astype(rdtype) + 1j * np.random.rand(2,4,4).astype(rdtype)         b = np.random.rand(2,4,4).astype(rdtype) + 1j * np.random.rand(2,4,4).astype(rdtype)     else:         a = np.random.rand(2,4,4).astype(rdtype)          b = np.random.rand(2,4,4).astype(rdtype)     A = tf.convert_to_tensor(a, dtype=dtype)     B = tf.convert_to_tensor(b, dtype=dtype)     result_adjoint = tf.vectorized_map(matmul_adjoint,(A,B))     result_transpose = tf.vectorized_map(matmul_transpose,(A,B))     expected_transpose = []     for n in range(2):         expected_transpose.append(a[n] @ b[n].T)     expected_transpose = np.stack(expected_transpose)     expected_adjoint = []     for n in range(2):         expected_adjoint.append(a[n] @ (b[n].T.conj()))     expected_adjoint = np.stack(expected_adjoint)     eps = np.finfo(rdtype).eps     try:         np.testing.assert_allclose(result_adjoint, expected_adjoint,                                     atol=10*eps, rtol=10*eps)     except AssertionError as err:         print(''.join(['']*60))         print(f""matmul adjoint test failed failed for dtype {dtype} with error {err}"")     try:         np.testing.assert_allclose(result_transpose, expected_transpose, atol=10*eps, rtol=10*eps)     except AssertionError as err:         print(''.join(['']*60))         print(f""matmul_transpose failed for dtype {dtype} with error {err}"")      the following passes, so it seems that the transpose and adjoint arguments      to tf.matmul need to be swapped     np.testing.assert_allclose(result_adjoint, expected_transpose,                                 atol=10*eps, rtol=10*eps)     np.testing.assert_allclose(result_transpose, expected_adjoint,                                 atol=10*eps, rtol=10*eps) test_vectorized_matmul(np.float32) test_vectorized_matmul(np.complex64) ```",2025-01-21T01:07:48Z,awaiting review ready to pull size:XS,closed,0,1,https://github.com/tensorflow/tensorflow/issues/85367,"Please title the PR and the commit message so that they can be inspected by their own, without having to click on separate links to understand what the issue that gets solved is. Please make things easy to reason about when looking at the history of the file/repository. Please write explanatory git commit messages. The commit message is also the title of the PR if the PR has only one commit. It is thus twice important to have commit messages that are relevant, as PRs would be easier to understand and easier to analyze in search results. For how to write good quality git commit messages, please consult https://cbea.ms/gitcommit/ "
copybara-service[bot],Disable Embedding Pipelining When Recording Summaries,"Disable Embedding Pipelining When Recording Summaries Embedding pipelining requires a least two steps. However, because the inclusion of summary ops is expensive and only needed periodically, most users run a single training step when enabling summaries. This change detects when summaries are active and automatically disables pipelining (under the assumption that the user will only be running a single step).",2025-01-20T23:25:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85366
Reyadeyat,How to enable compiler flags for avx2 avx512f fma,"I've compiled Tensorflow on Debian 12 successfully but when use jupyter I get this message **To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild Tensorflow with the appropriate compiler flags.** ``` the compilation is like this: $ python ./configure.py Please specify optimization flags to use during compilation when bazel option ""config=opt"" is specified [Default is Wnosigncompare]: Wnosigncompare march=native mavx2 mavx512f mfma msse4.1 msse4.2 $ bazelisk build //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow_cpu local_ram_resources=2048 config=monolithic verbose_failures c opt config=opt copt=Wnognuoffsetofextensions copt=march=native copt=mavx copt=mavx2 copt=mfma copt=msse4.1 copt=msse4.2 ``` How to correctly compile tensorflow with AVX2 AVX512F FMA flags",2025-01-20T21:35:22Z,stat:awaiting response stale,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85365,"Verify Hardware Support,Configure TensorFlow,Compile with Bazel,Verify Build Success,Test the Build with print(tf.config.list_physical_devices('CPU'))",", In order to expedite the troubleshooting process, could you please provide the CUDA/cuDNN version, GPU model and also provide the tensorflow version which you are trying. Along with the information try to provide the error log which helps to analyse the issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
copybara-service[bot],[xla:emitters] drop first operand of functions marked with xla.entry and xla.backend_kind=cpu,"[xla:emitters] drop first operand of functions marked with xla.entry and xla.backend_kind=cpu This paves the way for CPU emitters. For consistency, this also adds the xla.backend_kind=gpu attribute to the GPU entry function.",2025-01-20T21:17:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85364
copybara-service[bot],[IFRT] Apply pass that merges multiple reshards into a single one when they have the same source and destination.,[IFRT] Apply pass that merges multiple reshards into a single one when they have the same source and destination.,2025-01-20T20:12:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85363
copybara-service[bot],[XLA:GPU] Fix for the case when Dot that was rewritten as multiply has BF16 arguments and algorithm=BF16_BF16_F32,[XLA:GPU] Fix for the case when Dot that was rewritten as multiply has BF16 arguments and algorithm=BF16_BF16_F32 We have a wide range of algorithms for dot and the majority of them require F32 arguments. But the BF16_BF16_F32 one actually could accept BF16 arguments because it does not affect the final precision of the dot. Lets relax the check.,2025-01-20T20:09:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85362
copybara-service[bot],[xla:emitters] lower_tensors.mlir: fix CHECK-HOPPER-LABEL,[xla:emitters] lower_tensors.mlir: fix CHECKHOPPERLABEL,2025-01-20T19:26:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85361
copybara-service[bot],Integrate LLVM at llvm/llvm-project@e2402615a5a7,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match e2402615a5a7,2025-01-20T19:11:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85360
copybara-service[bot],[xla:cpu] Use Eigen::ThreadPoolDevice in XLA CPU Kernel,"[xla:cpu] Use Eigen::ThreadPoolDevice in XLA CPU Kernel We always use intraop device to run XLA:CPU kernels, stop pretending that we might have some other option (until we have a real alternative).",2025-01-20T18:11:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85359
copybara-service[bot],Integrate LLVM at llvm/llvm-project@e2402615a5a7,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match e2402615a5a7,2025-01-20T16:42:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85358
copybara-service[bot],[xla:gpu] Do not use more that 1 NCCL id,[xla:gpu] Do not use more that 1 NCCL id We don't yet support scalable NCCL communicator initialization and should not be generating more than 1 unique id,2025-01-20T16:30:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85357
copybara-service[bot],[XLA:GPU] Remove obsolete `xla_gpu_experimental_enable_triton_i4_rewrites` flag.,[XLA:GPU] Remove obsolete `xla_gpu_experimental_enable_triton_i4_rewrites` flag.,2025-01-20T15:32:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85356
copybara-service[bot],[XLA][NFC] Add a note about where to add new flags in `xla.proto`.,[XLA][NFC] Add a note about where to add new flags in `xla.proto`.,2025-01-20T15:28:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85355
copybara-service[bot],[XLA:GPU] Remove simplify_int4_dots pass from gpu_compiler passes.,[XLA:GPU] Remove simplify_int4_dots pass from gpu_compiler passes. It is not in use anymore.,2025-01-20T14:40:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85354
copybara-service[bot],[XLA:GPU] Remove int4 rewrite special handling from legacy matmul emitter.,"[XLA:GPU] Remove int4 rewrite special handling from legacy matmul emitter. After cl/716137284 int4 is transparent to matmul emitter, so we no longer need these special handlings.",2025-01-20T14:40:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85353
copybara-service[bot],[XLA] add a flag to fail if HloPassFix cannot converge,[XLA] add a flag to fail if HloPassFix cannot converge flag is helpful to detect cases when a pass does not converge,2025-01-20T14:27:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85352
Chadster766,"Tutorial ""Multi-worker training with Keras"" fails to complete"," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version v1.12.1120353gc5bd67bc56f 2.19.0dev20250107  Custom code No  OS platform and distribution Debian 6.1.1231 (20250102) x86_64 GNU/Linux  Mobile device _No response_  Python version Python 3.12.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Following the tutorial everything goes well until you start the second worker. Then the below failure occures. 20250120 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250120 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250120 07:19:35.307721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250120 07:19:36.510476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDAcapable device is detected 20250120 07:19:36.510494: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""1"" 20250120 07:19:36.510499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to 1  this hides all GPUs from CUDA 20250120 07:19:36.510501: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually v=1 or vmodule=cuda_diagnostics=1) to get more diagnostic output from this module 20250120 07:19:36.510505: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael 20250120 07:19:36.510507: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: michael 20250120 07:19:36.510562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0 20250120 07:19:36.510572: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0 20250120 07:19:36.510574: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0 20250120 07:19:36.519175: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:637] Initializing CoordinationService WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc://localhost:12345 20250120 07:19:36.524874: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4677280066871850635 20250120 07:19:36.524894: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 1/2 tasks to connect. 20250120 07:19:36.524898: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:819] Example stragglers: /job:worker/replica:0/task:1 I0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected. 20250120 07:22:27.996664: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 13530699364709055870 20250120 07:22:27.996686: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 0/2 tasks to connect. /home/chad/anaconda3/lib/python3.12/sitepackages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.   warnings.warn( 20250120 07:22:28.461733: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations. Traceback (most recent call last):   File ""/home/chad/Documents/McCueFiles/NeuralNetworks/TensorFlowProject/TensorFlowDocExample/main.py"", line 21, in      multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)   File ""/home/chad/anaconda3/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/home/chad/anaconda3/lib/python3.12/sitepackages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor     return ops.EagerTensor(value, ctx.device_name, dtype)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ValueError: Attempt to convert a value (PerReplica:{   0:  }) with an unsupported type () to a Tensor.  Standalone code to reproduce the issue ```shell python main.py &> job_1.log ```  Relevant log output ```shell 20250120 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250120 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250120 07:19:35.307721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250120 07:19:36.510476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDAcapable device is detected 20250120 07:19:36.510494: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""1"" 20250120 07:19:36.510499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to 1  this hides all GPUs from CUDA 20250120 07:19:36.510501: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually v=1 or vmodule=cuda_diagnostics=1) to get more diagnostic output from this module 20250120 07:19:36.510505: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael 20250120 07:19:36.510507: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: michael 20250120 07:19:36.510562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0 20250120 07:19:36.510572: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0 20250120 07:19:36.510574: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0 20250120 07:19:36.519175: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:637] Initializing CoordinationService WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc://localhost:12345 20250120 07:19:36.524874: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4677280066871850635 20250120 07:19:36.524894: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 1/2 tasks to connect. 20250120 07:19:36.524898: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:819] Example stragglers: /job:worker/replica:0/task:1 I0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected. 20250120 07:22:27.996664: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 13530699364709055870 20250120 07:22:27.996686: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 0/2 tasks to connect. /home/chad/anaconda3/lib/python3.12/sitepackages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.   warnings.warn( 20250120 07:22:28.461733: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations. Traceback (most recent call last):   File ""/home/chad/Documents/McCueFiles/NeuralNetworks/TensorFlowProject/TensorFlowDocExample/main.py"", line 21, in      multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)   File ""/home/chad/anaconda3/lib/python3.12/sitepackages/keras/src/utils/traceback_utils.py"", line 122, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/home/chad/anaconda3/lib/python3.12/sitepackages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor     return ops.EagerTensor(value, ctx.device_name, dtype)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ValueError: Attempt to convert a value (PerReplica:{   0:  }) with an unsupported type () to a Tensor. ```",2025-01-20T14:03:18Z,type:bug TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/85351,"Hi **** , Apologies for the delay, and thank you for raising your concern here. Could you please provide more details, such as the versions of TensorFlow and any other relevant libraries you are using? Additionally, sharing your code would make it easier for us to troubleshoot the issue effectively. In the meantime, please ensure that all compatibility requirements are met. For your reference, here is the relevant documentation. Thank you!","Hi  , I think I've provided the info regarding versions of TensorFlow and any other relevant libraries in the issue creation. I'm not running any of my code I'm just using the jupyter notebook of the tutorial."
copybara-service[bot],PR #21620: [ROCM] Bugfixing typo in buffer_sharing,PR CC(transform_graph2.params): [ROCM] Bugfixing typo in buffer_sharing Imported from GitHub PR https://github.com/openxla/xla/pull/21620 rotation: could you have a look please? Copybara import of the project:  309a6dfbbf4bdd48785d628c0c74efdd029ee4eb by Pavel Emeliyanenko : Fixing typo in buffer_sharing Merging this change closes CC(transform_graph2.params) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21620 from ROCm:ci_buffer_sharing_fix 309a6dfbbf4bdd48785d628c0c74efdd029ee4eb,2025-01-20T13:28:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85350
copybara-service[bot],PR #21618: [ROCM] Fixing non-canonical dots and enabled conv_add_multiply_reorder on ROCM,"PR CC(AttributeError: module 'tensorflow.contrib.lite.python.convert_saved_model' has no attribute 'convert'): [ROCM] Fixing noncanonical dots and enabled conv_add_multiply_reorder on ROCM Imported from GitHub PR https://github.com/openxla/xla/pull/21618 This commit: https://github.com/openxla/xla/commit/749b8645ffe9f456690a9b9b7713f60320611779 fixing noncanonical dots problem on CUDA platform but not on ROCM. This PR fixes the same issue and also enabled conv_add_multiply_reorder (https://github.com/openxla/xla/commit/27a3a1ad55af87e4037602db7b423e8917a3bd70) for ROCM. Besides, I added the HLO used in the original fix to matmul_test.. rotation : could you please have a look ?  Copybara import of the project:  d58ef4aa497d77355fde7a05ba7abfbe167ccfed by Pavel Emeliyanenko : Fixing noncanonical dots on ROCM Merging this change closes CC(AttributeError: module 'tensorflow.contrib.lite.python.convert_saved_model' has no attribute 'convert') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21618 from ROCm:ci_dot_strength_reduce_fix d58ef4aa497d77355fde7a05ba7abfbe167ccfed",2025-01-20T12:48:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85349
copybara-service[bot],[XLA:CPU] Move kernel name from source to spec,[XLA:CPU] Move kernel name from source to spec,2025-01-20T12:13:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85348
copybara-service[bot],[XLA:GPU] Add tests for nested fusions in GpuIndexingPerformanceModel and SymbolicTileAnalysis.,[XLA:GPU] Add tests for nested fusions in GpuIndexingPerformanceModel and SymbolicTileAnalysis.,2025-01-20T11:25:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85346
copybara-service[bot],[XLA:CPU] Create KernelDefinition,[XLA:CPU] Create KernelDefinition,2025-01-20T11:03:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85344
copybara-service[bot],[XLA:GPU] NFC: Remove unused variable in SymbolicTileAnalysis.,[XLA:GPU] NFC: Remove unused variable in SymbolicTileAnalysis.,2025-01-20T10:27:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85342
copybara-service[bot],[XLA] drop unused EmitParallelFusedDynamicUpdateSliceInPlace NFC,[XLA] drop unused EmitParallelFusedDynamicUpdateSliceInPlace NFC,2025-01-20T09:53:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85341
wokron,Fix Bug in XSpace to StepStats Conversion for GPU Tracing to Keep Compatibility with RunMetadata,"In TensorFlow v2, we use `ConvertGpuXSpaceToStepStats` to convert `XSpace` data to `StepStats` used in v1. However, the timestamps in `XSpace` are relative and were not converted to the absolute time used in `RunMetadata` during the conversion. This discrepancy affected the correctness of analysis tools like `timeline.Timeline` in v1. See CC(TF timeline timestamp was shifted. The resultant timeline cannot be shown correctly in chrome tracing viewer) and tensorflow/profiler issue CC(Empty input to conv2d causes floating point exception). The following code can reproduce this issue: ```py import tensorflow as tf from tensorflow.python.client import timeline tf.compat.v1.disable_eager_execution() matrix1 = tf.constant([[3.0, 3.0]]) matrix2 = tf.constant([[2.0], [2.0]]) matrix3 = tf.constant([[1.0, 2.0], [3.0, 4.0]]) matrix4 = tf.constant([[2.0, 0.0], [1.0, 2.0]]) product1 = tf.matmul(matrix1, matrix2) product2 = tf.matmul(matrix3, matrix4) sum_product = tf.add(product1, product2) run_options = tf.compat.v1.RunOptions(     trace_level=tf.compat.v1.RunOptions.HARDWARE_TRACE ) run_metadata = tf.compat.v1.RunMetadata() with tf.compat.v1.Session() as sess:     result = sess.run(sum_product, options=run_options, run_metadata=run_metadata)     print(result) tl = timeline.Timeline(run_metadata.step_stats) ctf = tl.generate_chrome_trace_format() with open(""timeline.json"", ""w"") as f:     f.write(ctf) ``` Here is the result timeline_error.json, where some timestamps are in relative time, such as `13427`, while others are in absolute time, such as `1737358838975242`. This pull request addresses the issue by converting relative timestamps to absolute timestamps in the `SetNodeTimes` function.",2025-01-20T08:44:22Z,awaiting review ready to pull size:S comp:core,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85339
copybara-service[bot],PR #20288: cuda_root_path: Find cuda libraries when installed with conda packages,"PR CC(Revert ""[StreamExecutor] Merge StreamExecutor's and XLA's StatusOr classes.): cuda_root_path: Find cuda libraries when installed with conda packages Imported from GitHub PR https://github.com/openxla/xla/pull/20288 This fix emerged when looking in solving https://github.com/jaxml/jax/issues/24604 . In a nutshell, the official cuda package for conda (both in the `condaforge` and `nvidia` conda channels) install the CUDA libraries in a different location with respect to PyPI packages, so the logic to find them needs to be augmented to be able to find the CUDA libraries when installed from conda packages. I did not tested this with a tensorflow build, but probably this will also help in solving https://github.com/tensorflow/tensorflow/issues/56927 . xref: https://github.com/condaforge/tensorflowfeedstock/pull/408 xref: https://github.com/condaforge/jaxlibfeedstock/pull/288 Copybara import of the project:  a2ce85cf9df1ede3f3c1843ede55d4c76673910e by Silvio Traversaro : cuda_root_path: Find cuda libraries when installed with conda packages Merging this change closes CC(Revert ""[StreamExecutor] Merge StreamExecutor's and XLA's StatusOr classes.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20288 from traversaro:fixloadcudaconda a2ce85cf9df1ede3f3c1843ede55d4c76673910e",2025-01-20T07:58:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85338
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T05:52:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85336
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T04:01:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85335
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T03:33:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85334
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T03:31:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85333
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T03:27:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85332
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T03:23:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85331
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T03:22:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85330
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T03:22:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85329
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T03:22:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85328
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T03:22:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85327
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T03:22:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85326
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T03:18:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85325
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T03:17:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85324
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T03:15:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85323
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T02:46:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85322
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T02:37:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85321
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T02:36:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85320
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T02:34:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85319
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T02:33:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85318
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T02:33:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85317
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-20T02:28:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85316
copybara-service[bot],Set element size in bits in ShapeUtil::FillNewShape for SubByteNonPredTypes.,Set element size in bits in ShapeUtil::FillNewShape for SubByteNonPredTypes.,2025-01-20T00:38:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85315
copybara-service[bot],Integrate LLVM at llvm/llvm-project@13c761789753,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 13c761789753,2025-01-19T22:01:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85314
NLLAPPS,Cannot use GpuDelegate - java.lang.IllegalArgumentException: Internal error: Cannot create interpreter,"Hi, I get ""Cannot use GpuDelegate  java.lang.IllegalArgumentException: Internal error: Cannot create interpreter"" when attempting to use GpuDelegate I have seen a couple of issue related to this but all seems to be abandoned. I have created a repo replicating the issue.  You can see the config at https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.javaL114 **System information**  Android Device information: Samsung S23  TensorFlow Lite in Play Services SDK version : 16.4.0  Google Play Services version: 24.50.34 **Standalone code to reproduce the issue** Clone and run project from https://github.com/NLLAPPS/WhisperOffline/ **Any other info / logs** `Created TensorFlow Lite delegate for GPU. Created interpreter. Created interpreter. java.lang.IllegalArgumentException: Internal error: Cannot create interpreter:  at com.google.android.gms.tflite.NativeInterpreterWrapper.createInterpreter(Native Method) at com.google.android.gms.tflite.NativeInterpreterWrapper.zzs(com.google.android.gms:playservicestflitejava@.4.0:34) at com.google.android.gms.tflite.NativeInterpreterWrapper.(com.google.android.gms:playservicestflitejava@.4.0:14) at com.google.android.gms.tflite.zzd.(com.google.android.gms:playservicestflitejava@.4.0:3) at com.google.android.gms.tflite.InterpreterFactoryImpl.create(com.google.android.gms:playservicestflitejava@.4.0:4) at org.tensorflow.lite.InterpreterApi.create(InterpreterApi.java:373) at com.whispertflite.engine.WhisperEngineJava.loadModel(WhisperEngineJava.java:131) at com.whispertflite.engine.WhisperEngineJava.initialize(WhisperEngineJava.java:44) at com.whispertflite.asr.WhisperJava.loadModel(WhisperJava.java:72) at com.whispertflite.asr.WhisperJava.loadModel(WhisperJava.java:67) at com.whispertflite.MainActivity.initModel(MainActivity.java:247) at com.whispertflite.MainActivity.lambda$onCreate$3$comwhispertfliteMainActivity(MainActivity.java:155) at com.whispertflite.MainActivity$$ExternalSyntheticLambda0.run(D8$$SyntheticClass:0) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:644) at java.lang.Thread.run(Thread.java:1012)`",2025-01-19T22:01:13Z,comp:lite TFLiteGpuDelegate,open,0,12,https://github.com/tensorflow/tensorflow/issues/85313,"there is an issue with the configuration of the GpuDelegate or its compatibility with your environment, Test with CPUonly execution to confirm whether the issue is specific to the GPU Delegate","Hi and thank you. I have tested CPU only it works fine. What is the ""configuration of the GpuDelegate""? My configuration can be seen at https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.javaL114","'GpuDelegateFactory.Options' in your code uses generic settings. For better control, you can configure options like precision or inference preference. try this or Run a compatibility check  GpuDelegateFactory.Options gpuOptions = new GpuDelegateFactory.Options(); gpuOptions.setInferencePreference(GpuDelegateFactory.Options.INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER); gpuOptions.setPrecisionLossAllowed(true); ","Thanks, regarding ""compatibility check"". Is it possible to provide link to documentation for compatibility checking?",check out these links : https://stackoverflow.com/questions/50622525/whichtensorflowandcudaversioncombinationsarecompatible https://docs.nvidia.com/cuda/cudatoolkitreleasenotes/index.html https://docs.nvidia.com/deeplearning/cudnn/latest/reference/supportmatrix.html,"Your links seem to be related to PCs. Have I misunderstood what GpuDelegateFactory does. I thought it would be for using GPU on the phone since the artifact is ""playservicestflitegpu""",I also just noticed you are not related to this project. Do you have experience on implementing tflite on Android?,"you are correct in assuming that GpuDelegateFactory is intended for mobile devices to leverage the GPU for TensorFlow Lite inference, especially in Android using the playservicestflitegpu artifact. If you're targeting mobile platforms, this is the correct path to enable GPU acceleration dependencies {     implementation 'org.tensorflow:tensorflowlite:2.x.x'     implementation 'org.tensorflow:tensorflowlitegpu:2.x.x' } GpuDelegate delegate = new GpuDelegate(); Interpreter.Options options = new Interpreter.Options().addDelegate(delegate); Interpreter interpreter = new Interpreter(modelFile, options);","yes i am not a part of this project yet , im trying to contribute as much possible to be recognized by the organization before gsoc 2025","> yes i am not a part of this project yet , i'm trying to contribute as much possible to be recognized by the organization before gsoc 2025 I don't think you will be able to help me in this case. Issue seems to be related to actual SDK/API. Hopefully  will have a look at it.","Hi,   I apologize for the delayed response, I was trying to reproduce the similar issue from my end after cloning your provided repo but I'm getting below error message, if possible could you please help me to replicate the same issue from my end which you reported in the issue template to investigate this issue further from our end ? **Here is error log for reference :** ``` FAILURE: Build failed with an exception. * What went wrong: A problem occurred configuring root project 'WhisperOffline'. > Could not resolve all files for configuration ':classpath'.    > Could not find com.android.tools.build:gradle:8.10.2.      Searched in the following locations:         https://dl.google.com/dl/android/maven2/com/android/tools/build/gradle/8.10.2/gradle8.10.2.pom         https://repo.maven.apache.org/maven2/com/android/tools/build/gradle/8.10.2/gradle8.10.2.pom      Required by:          project : * Try: > Run with stacktrace option to get the stack trace. > Run with info or debug option to get more log output. > Run with scan to get full insights. > Get more help at https://help.gradle.org. BUILD FAILED in 829ms ``` Thank you for your cooperation and patience.","Hi, project is using com.android.tools.build:gradle:8.8.0 there is no com.android.tools.build:gradle:8.10.2. 8.10.2 is a Gradle version, not Android build tools version. Have you changed anything? This stack overflow post seems to suggest it may be related to Android Studio Gradle Settings.  Here is mine attached for the project !Image"
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T21:12:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85312
metal3d,LD_LIBRARY_PATH to set when installing tensorflow[and-cuda] with pip or poetry," Issue type Documentation Feature Request  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code No  OS platform and distribution Linux Fedora 41  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory Nvidia RTX 3060  Current behavior? Installing `tensorflow[andcuda]` is very easy and avoid installing cuda packages on our computers. It's then very confortable to be able to drop virtual envs. But there is a bad behavior. From scratch: ```bash rm rf .venv mkdir .venv poetry add ""tensorflow[andcuda]""  check poetry run python mytest.py W0000 00:00:1737317404.258825   10111 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. ``` As you can see, no CUDA shared library is loaded. The solution is to generate the `LD_LIBRARY_PATH` finding directories name of any "".so"" files related to ""nvidia"": ```bash export LD_LIBRARY_PATH=$(find .venv name ""*.so*""  paste d: s ) echo $LD_LIBRARY_PATH .venv/lib/python3.12/sitepackages/nvidia/cublas/lib:.venv/lib/python3.12/sitepackages/nvidia/cuda_cupti/lib:.venv/lib/python3.12/sitepackages/nvidia/cuda_nvcc/nvvm/lib64:.venv/lib/python3.12/sitepackages/nvidia/cuda_nvrtc/lib:.venv/lib/python3.12/sitepackages/nvidia/cuda_runtime/lib:.venv/lib/python3.12/sitepackages/nvidia/cudnn/lib:.venv/lib/python3.12/sitepackages/nvidia/cufft/lib:.venv/lib/python3.12/sitepackages/nvidia/curand/lib:.venv/lib/python3.12/sitepackages/nvidia/cusolver/lib:.venv/lib/python3.12/sitepackages/nvidia/cusparse/lib:.venv/lib/python3.12/sitepackages/nvidia/nccl/lib:.venv/lib/python3.12/sitepackages/nvidia/nvjitlink/lib  and now: poetry run python mytest.py I0000 00:00:1737317342.317060    9825 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4169 MB memory:  > device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6 ``` I'm not sur if I have to create the issue here or to Nvidia (where?), but it could be interesting to propose this solution for who has the same problem.  Standalone code to reproduce the issue ```shell poetry run python c ""import tensorflow;tensorflow.keras.Sequential().compile()"" ```  Relevant log output ```shell ```",2025-01-19T20:13:12Z,stat:awaiting response type:feature type:build/install stale type:docs-feature,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85311,"TensorFlow installed with tensorflow[andcuda] doesn't automatically set up the LD_LIBRARY_PATH environment variable to include the locations of the NVIDIA shared libraries packaged with the installation. Your solution is an effective way to resolve the problem by dynamically setting LD_LIBRARY_PATH to include the necessary directories containing the .so files.  et LD_LIBRARY_PATH dynamically to include the directories containing the NVIDIA .so files within the virtual environment use this command  export LD_LIBRARY_PATH=$(find .venv name ""*.so*""  paste d: s ) then verify echo $LD_LIBRARY_PATH",", Thank you for reporting the issue. This is a known issue where other issues are still open and developers are working on the same. I request you to take a look at this https://github.com/tensorflow/tensorflow/issues/62075 and where a similar issue has been proposed and it is still open. Also I request to follow the similar issue which has been proposed to have the updates on the similar issue. https://github.com/tensorflow/tensorflow/issues/70947 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[xla:cpu:xnn] Extract WorkQueue into a separate target,[xla:cpu:xnn] Extract WorkQueue into a separate target Added a benchmark for popping a task from a queue  Benchmark           Time             CPU   Iterations  BM_PopTask       2.63 ns         2.63 ns    265317289,2025-01-19T19:58:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85310
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T19:30:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85309
copybara-service[bot],Fix XLA build on CUDA Driver < 12.3,Fix XLA build on CUDA Driver < 12.3,2025-01-19T18:12:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85308
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T17:44:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85307
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T15:50:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85306
copybara-service[bot],[XLA:Python] Remove JAX CUDA rpaths setting.,"[XLA:Python] Remove JAX CUDA rpaths setting. This is now no longer used by JAX, and never matter on xla_extension.so because we no longer build it in a CUDAspecific configuration, instead using CUDA plugins.",2025-01-19T15:05:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85305
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T14:13:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85304
maxima120,Force TF to log GPU memory allocation, Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.18.0  Custom code No  OS platform and distribution Debian 12  Mobile device _No response_  Python version 3.11.2  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.2  GPU model and memory RTX 3060 12Gb  Current behavior? If I ran out of GPU memory I get error saying `Allocator (GPU_0_bfc) ran out of memory trying to allocate 9.78GiB` But if allocation succeeded there is no real way to know what the allocator did/doing.. I know that you can use nvidiasmi etc but this is an indirect way to estimate very crudely whats going on. What I believe would be very beneficial (because I dont know about you guys but I run out of memory so very often and I cant affort 128Gb card)  is to be able to force TF to log every and single one GPU memory allocation.  So I can see whats going on my healthy models and whats in the failing.,2025-01-19T13:50:42Z,stat:awaiting tensorflower type:feature comp:gpu,open,0,0,https://github.com/tensorflow/tensorflow/issues/85303
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T12:26:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85302
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T10:44:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85301
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T09:02:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85300
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T07:26:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85299
Animemchik,Unable to install TensorFlow: No matching distribution found for TensorFlow!," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.8  Custom code Yes  OS platform and distribution Windows 10  Mobile device _No response_  Python version 3.13  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I expected to install TensorFlow  Standalone code to reproduce the issue Can't install TensorFlow with pip ```shell > pip install tensorflow ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) [notice] A new release of pip is available: 24.2 > 24.3.1       [notice] To update, run: python.exe m pip install upgrade pip ERROR: No matching distribution found for tensorflow ``` Pls help  Relevant log output ```shell ```",2025-01-19T07:18:59Z,stat:awaiting response type:build/install type:support stale TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/85298,"There was no python 2.13 at the time of tensorflow 2.8 release. If you want to use TF 2.8 you need to use a Python version that is supported there. If you want to use Python 3.13, please see CC(It doesn't support on python3.13) ",> There was no python 2.13 at the time of tensorflow 2.8 release. >  > If you want to use TF 2.8 you need to use a Python version that is supported there. I also tried 2.7 and 2.6 but still can't do it,"TF 2.7 and TF 2.6 are even older, so of course they won't work with Python 3.13 either",TensorFlow requires specific Python versions depending on the TensorFlow version. try Python 3.8 to 3.11,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T05:40:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85297
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T04:35:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85295
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T04:01:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85294
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T03:59:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85293
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T03:43:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85292
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T03:41:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85291
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T03:41:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85290
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T03:33:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85289
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T03:30:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85288
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T03:27:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85287
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:32:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85286
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:31:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85285
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:31:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85284
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:30:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85283
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:30:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85282
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:30:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85281
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:29:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85280
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:27:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85279
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:25:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85278
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:25:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85277
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:25:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85276
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:22:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85275
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:18:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85273
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:15:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85272
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:14:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85271
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:11:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85270
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:10:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85269
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T02:09:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85268
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21437 from shraiysh:algebraic_simplifier_fix 3a7ab58814f35a8c7f22cb46248cead3c5cdca50,2025-01-19T02:08:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85267
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-19T01:59:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85266
copybara-service[bot],Add support for cuda driver versions 520 and 530 in hermetic cuda,Add support for cuda driver versions 520 and 530 in hermetic cuda,2025-01-18T21:07:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85265
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T20:57:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85264
yeyinqcri,Extend tensorflow restore kernel op to support type casting upon reading.,,2025-01-18T20:05:25Z,size:L comp:core,closed,0,3,https://github.com/tensorflow/tensorflow/issues/85263,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , Can you please sign the CLA and resolve the conflicts. Thank you !",close it as I push the change to the upstream by mistake. This optimization is specific to my company and should be pushed to my own's repo instead.
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T19:37:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85262
copybara-service[bot],[xla:cpu:xnn] Do not use XNNPACK for dots that require tiling by K,[xla:cpu:xnn] Do not use XNNPACK for dots that require tiling by K,2025-01-18T17:30:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85261
copybara-service[bot],[xla:cpu:xnn] Do not return XNN runner back to the pool until execution is complete,[xla:cpu:xnn] Do not return XNN runner back to the pool until execution is complete Fix a tsan warning from data races inside XNNPACK,2025-01-18T17:17:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85260
LongZE666,Aborted in `tensorflow.nn.depthwise_conv2d_backprop_filter`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.nn.depthwise_conv2d_backprop_filterr` triggers crash. The crash occurs in the  operator DepthwiseConv2dNativeBackpropFilter  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow filter_sizes = np.ones((13, 2, 5, 4), dtype=np.uint32) input = 26262.2175547925 out_backprop = 14557.552005335412 strides = [] padding = 'VALID' tensorflow.nn.depthwise_conv2d_backprop_filter(input=input, filter_sizes=filter_sizes, out_backprop=out_backprop, strides=strides, padding=padding) ```  Relevant log output ```shell 20250118 12:47:13.553183: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 0, 0, N Aborted (core dumped) ```",2025-01-18T12:49:02Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85258,"Hi **** , Apologies for the delay, and thank you for raising your concern here. I tested your code on Colab using TensorFlow 2.17.0, 2.18.0, and the nightly versions. The code is throwing a proper error message instead of aborting. Please find the gist here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
LongZE666,Aborted in `tensorflow.compat.v1.nn.conv2d_backprop_filter`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.compat.v1.nn.conv2d_backprop_filter` triggers crash. The crash occurs in the  operator Conv2DBackpropFilter  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow input = np.array([1.2807603e+38], dtype=np.float32) filter_sizes = np.ones((20, 6, 5, 3), dtype=np.uint16) strides = [4538426903074820908] padding = 'VALID' out_backprop = np.ones((19, 5, 11, 4), dtype=np.float32) tensorflow.compat.v1.nn.conv2d_backprop_filter(input=input, filter_sizes=filter_sizes, out_backprop=out_backprop, strides=strides, padding=padding) ```  Relevant log output ```shell 20250118 12:34:39.686592: F tensorflow/core/common_runtime/mkl_layout_pass.cc:2754] NonOKstatus: GetNodeAttr(orig_node>def(), ""strides"", &strides) status: INVALID_ARGUMENT: Attr strides has value 4538426903074820908 out of range for an int32 Aborted (core dumped) ```",2025-01-18T12:35:54Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85257,", I tried to execute the mentioned code and observed that the attribute error which might be due to a value is too large to fit into a 32bit signed integer variable. Could you try to provide the input which fits the stride and test the code.  Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
LongZE666,Aborted in `tensorflow.raw_ops.ScatterNdNonAliasingAdd`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.raw_ops.ScatterNdNonAliasingAdd` triggers crash. The crash occurs in the  operator ScatterNdNonAliasingAdd  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow input = np.array([False,  True,  True, False,  True, False,  True, False, False,      False, False,  True,  True, False, False,  True,  True]) indices = 1547981256 updates = np.array([ True, False,  True, False,  True]) tensorflow.raw_ops.ScatterNdNonAliasingAdd(input=input, indices=indices, updates=updates) ```  Relevant log output ```shell 20250118 12:28:50.452498: F ./tensorflow/core/framework/tensor.h:847] Check failed: new_num_elements == NumElements() (1 vs. 5) Aborted (core dumped) ```",2025-01-18T12:30:20Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85256,"Hi **** , Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.17.0 and the nightly versions and initially faced the same issue. Upon investigating, the main cause seems to be a mismatch in the dimensions and values. After making the necessary adjustments, it worked fine for me. I have detailed the findings and corrections in a Colab gist. Please find the gist here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
LongZE666,Aborted in `tensorflow.compat.v1.nn.depthwise_conv2d_native`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.compat.v1.nn.depthwise_conv2d_native` triggers crash. The crash occurs in the  operator DepthwiseConv2DNative  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow input = 18261.482131447112 strides = [13762484222233278712, 17895496137899471100] padding = 'VALID' dilations = [11529923499597944069, 6104546329631715214, 16719022861480294065, 15641088296656225516, 5831796171324899039, 2019887057379389965, 6603155403475823832, 128243198633280011, 1962111378253501566, 12787126510677041415, 10754291262680183180] filter = np.array([[ 36832. ,  8776. ,    255.2, 31200. ,  6408. , 12656. , 31696. , 65024. , 50688. ]], dtype=np.float16) tensorflow.compat.v1.nn.depthwise_conv2d_native(input=input, filter=filter, strides=strides, padding=padding, dilations=dilations) ```  Relevant log output ```shell 20250118 12:25:34.220155: F tensorflow/core/common_runtime/mkl_layout_pass.cc:2755] NonOKstatus: GetNodeAttr(orig_node>def(), ""dilations"", &dilations) status: INVALID_ARGUMENT: Attr dilations has value 6104546329631715214 out of range for an int32 Aborted (core dumped) ```",2025-01-18T12:26:48Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85255," Explanation of the Error: `Attr dilations has value [...] out of range for an int32` The error **`Attr dilations has value [...] out of range for an int32`** occurs because the `dilations` attribute passed to the TensorFlow operation contains values that exceed the range of a 32bit integer.   Why This Happens  The `dilations` parameter defines how the convolution kernel is spaced for height and width dimensions.  TensorFlow expects `dilations` to be a **list of small positive integers**, typically `[1, dilation_height, dilation_width, 1]`.  In your code, `dilations` contains excessively large values, such as:   ```python   dilations = [11529923499597944069, 6104546329631715214, ...] Another error I ran into:  Explanation of the Error: `Sliding window strides field must specify 4 dimensions` The error **`Sliding window strides field must specify 4 dimensions`** means that TensorFlow expects the `strides` parameter to be a list of exactly **4 integers**, corresponding to the following dimensions: 1. **Batch size**: Stride for the batch dimension (usually set to `1`). 2. **Height**: Stride for the height dimension. 3. **Width**: Stride for the width dimension. 4. **Channels**: Stride for the channels dimension (usually set to `1`).",  I tried to execute the mentioned code and observed that the attribute error which might be due to a value is too large to fit into a 32bit signed integer variable. Could you try to provide the input which fits the stride and test the code. Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
LongZE666,Aborted in `tensorflow.raw_ops.RecordInput`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.raw_ops.RecordInput` triggers crash. The crash occurs in the  operator RecordInput  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow file_pattern = '@\x0chnFvGe CC(can't install on ubuntu 12.04)FxIIw.\';$`E?8YC2p>\x0bl\\B*;z;]a4lrH{gWK}rAY{;T]2k&jEB)pQu{JLy1K{Q zGnevJS7G=N_{c}\x0cS4p\'] CC(Add support for Python 3.x)`T?FAep' file_random_seed = 0 batch_size =  8494458287779632258 tensorflow.raw_ops.RecordInput(file_pattern=file_pattern, file_random_seed=file_random_seed, batch_size=batch_size) ```  Relevant log output ```shell 20250118 12:21:05.765578: F tensorflow/core/framework/tensor_shape.cc:201] NonOKstatus: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be nonnegative, got 8494458287779632258 Aborted (core dumped) ```",2025-01-18T12:21:59Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85254,"Hi **** , Apologies for the delay, and thank you for raising your concern here. I tested your code on Colab using TensorFlow 2.17.0, 2.18.0, and the nightly versions. The code is throwing a proper error message instead of aborting. Please find the gist here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
LongZE666,Aborted in `tensorflow.nn.max_pool3d`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.nn.max_pool3d` triggers crash. The crash occurs in the  operator MaxPool3D  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow input = np.ones((20, 19, 20, 5, 0), dtype=np.float32) ksize = [3505357736] strides = [2230364274603370470]  padding = 'SAME' tensorflow.nn.max_pool3d(input=input, ksize=ksize, strides=strides, padding=padding) ```  Relevant log output ```shell 20250118 12:15:49.067137: F tensorflow/core/common_runtime/mkl_layout_pass.cc:1609] NonOKstatus: GetNodeAttr(n>def(), ""ksize"", &ksize) status: INVALID_ARGUMENT: Attr ksize has value 3505357736 out of range for an int32 Aborted (core dumped) ```",2025-01-18T12:16:46Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85253,", According to the official document for the tensorflow.nn.max_pool3d, the input for the strides should be **An int or list of ints that has length 1, 3 or 5. The stride of the sliding window for each dimension of the input tensor.** https://www.tensorflow.org/api_docs/python/tf/nn/max_pool3dargs Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
LongZE666,Floating point exception in `tensorflow.nn.max_pool1d`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.nn.max_pool1d` triggers crash. The crash occurs in the  operator MaxPool  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow input = np.ones((16,19,18), dtype=np.float32) ksize = [58486] strides = [49657] padding = 'VALID' tensorflow.nn.max_pool1d(input=input, ksize=ksize, strides=strides, padding=padding) ```  Relevant log output ```shell Floating point exception (core dumped) ```",2025-01-18T12:14:05Z,stat:awaiting response type:bug stale comp:apis comp:ops 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85252,"The crash is caused by invalid inputs, not a bug in TensorFlow.  To prevent crashes: Validate ksize and strides to ensure they align with the input dimensions. Use reasonable values for these parameters, adhering to the intended size and stride of your pooling operation.","Hi **** , Apologies for the delay, and thank you for raising your concern here. I tested your code on colab using TensorFlow 2.17.0, 2.18.0, and the nightly versions, and I did not encounter any issues. Please find the results in gist1 and gist2 for reference. If you are still facing issues, the main cause might be related to the kernel size and strides. Ensure that the kernel size is less than or equal to the input's second dimension, and the same applies to strides. Adjust these values accordingly, and the code should run smoothly. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
LongZE666,Aborted (core dumped) in `tensorflow.nn.max_pool1d`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.nn.max_pool1d` triggers crash. The crash occurs in the  operator MaxPool  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow input = np.ones((17,7,14), dtype=np.float32) ksize = [3134755050008138495] strides = [48732] padding = 'SAME' tensorflow.nn.max_pool1d(input=input, ksize=ksize, strides=strides, padding=padding) ```  Relevant log output ```shell 20250118 12:09:37.053592: F tensorflow/core/common_runtime/mkl_layout_pass.cc:1609] NonOKstatus: GetNodeAttr(n>def(), ""ksize"", &ksize) status: INVALID_ARGUMENT: Attr ksize has value 3134755050008138495 out of range for an int32 Aborted (core dumped) ```",2025-01-18T12:10:43Z,stat:awaiting response type:bug stale comp:ops 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85251,", This indicates the problem is due to Memory issue where OS crashed in allocating required memory which is expected.Please refer to the developer https://github.com/tensorflow/tensorflow/issues/59168issuecomment1405633596 related to malloc with High input size which will eventually lead to OS crash. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
LongZE666,Abort in `tensorflow.keras.backend.conv2d`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.1 tf 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tensorflow.keras.backend.conv2d` triggers crash. The crash occurs in the  operator Conv2D  Standalone code to reproduce the issue ```shell import pickle import numpy as np import tensorflow x = np.array([1.5841993e+38, 4.7480855e+36, 1.8916006e+38], dtype=np.float32) kernel = np.ones((13, 8, 5, 14), dtype=np.float32) tensorflow.keras.backend.conv2d(x=x, kernel=kernel) ```  Relevant log output ```shell 20250118 12:02:15.375499: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C Aborted (core dumped) ```",2025-01-18T12:04:15Z,stat:awaiting response type:bug stale comp:apis 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85250,"Hi **** , Apologies for the delay, and thank you for raising your concern here. It seems the issue might be caused by using the deprecated API `tensorflow.keras.backend.conv2d`. I recommend switching to the latest APIs for better compatibility and performance. Please find the documentation here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[xla:emitters] flatten_tensors: CPU support,[xla:emitters] flatten_tensors: CPU support,2025-01-18T11:51:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85249
copybara-service[bot],[xla:emitters] lower_tensors: initial CPU support,"[xla:emitters] lower_tensors: initial CPU support Still missing direct atomics (vs. cmpxchg), but we can add those later. Note that the added test for nongep loads would be an error in nonCPU, hence the separate test file for CPU.",2025-01-18T11:51:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85248
LongZE666,Aborted in ` tf.nn.conv3d_transpose`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.16.1  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tf.nn.conv3d_transpose` triggers crash.   Standalone code to reproduce the issue ```shell https://colab.research.google.com/drive/1wA5pUfm4HpNpHxPawrZt4CKS2YCUR4?usp=sharing ```  Relevant log output ```shell 20250118 11:31:33.696896: F tensorflow/core/common_runtime/mkl_layout_pass.cc:2754] NonOKstatus: GetNodeAttr(orig_node>def(), ""strides"", &strides) status: INVALID_ARGUMENT: Attr strides has value 5717580618211388939 out of range for an int32 Aborted (core dumped) ```",2025-01-18T11:34:53Z,stat:awaiting response type:bug stale comp:apis comp:ops TF 2.16,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85246,"code ``` import pickle import numpy as np import tensorflow input = np.ones((9, 12, 18, 6), dtype=np.float32) filters = np.ones((14, 1, 9, 10, 0), dtype=np.float32) strides = [5717580618211388939] padding = 'SAME' output_shape = np.array([], dtype=np.complex128) tensorflow.nn.conv3d_transpose(input=input, filters=filters, output_shape=output_shape, strides=strides, padding=padding) ``` result ``` 20250118 13:18:01.541522: F tensorflow/core/common_runtime/mkl_layout_pass.cc:2754] NonOKstatus: GetNodeAttr(orig_node>def(), ""strides"", &strides) status: INVALID_ARGUMENT: Attr strides has value 5717580618211388939 out of range for an int32 Aborted (core dumped) ``` The colab link given is inconsistent with the results on my machine. _","Hi **** , Apologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.18.0 and the nightly versions. However, instead of aborting, the code is throwing an error. I have provided an example with all the necessary steps, which is working fine. Please find the gist here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T11:18:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85245
copybara-service[bot],[pjrt] Use the `PjRtMemorySpace*` version of `BufferFromHostLiteral`,[pjrt] Use the `PjRtMemorySpace*` version of `BufferFromHostLiteral` Buffers live in memory spaces and not on devices. The `PjRtDevice` version of `BufferFromHostLiteral` is deprecated and will be removed once the migration is complete.,2025-01-18T09:51:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85243
LongZE666,Aborted  in `tf.raw_ops.RaggedGather`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tf.raw_ops.RaggedGather` triggers crash.  Standalone code to reproduce the issue ```shell params_nested_splits = tf.constant(0, shape=[3], dtype=tf.int64) params_dense_values = tf.constant(1, shape=[0], dtype=tf.float32) indices = tf.constant(0, shape=[], dtype=tf.int64) OUTPUT_RAGGED_RANK = 1 PARAMS_RAGGED_RANK = 1 tf.raw_ops.RaggedGather(     params_nested_splits=[params_nested_splits],     params_dense_values=params_dense_values,     indices=indices,     OUTPUT_RAGGED_RANK=1,     name=None ) ```  Relevant log output ```shell 20250118 09:30:00.549762: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64 Aborted (core dumped) ```",2025-01-18T09:32:16Z,type:bug comp:ops 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/85242,I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TFnightly. Please find the gist here for your reference. Thank you!
LongZE666,Segmentation fault (core dumped) in `RaggedTensorToTensor`," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.17  Custom code Yes  OS platform and distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? On specific inputs, `tf.raw_ops.RaggedTensorToTensor` triggers crash.  Standalone code to reproduce the issue ```shell import tensorflow as tf shape = tf.constant(1, shape=[], dtype=tf.int64) values = tf.constant(0, shape=[0], dtype=tf.int32) default_value = tf.constant(0, shape=[], dtype=tf.int32) row_partition_tensors = tf.constant([0, 1, 6], shape=[3], dtype=tf.int64) row_partition_types = [""ROW_SPLITS""] tf.raw_ops.RaggedTensorToTensor(     shape=shape,     values=values,     default_value=default_value,     row_partition_tensors=[row_partition_tensors],     row_partition_types=row_partition_types) ```  Relevant log output ```shell Segmentation fault (core dumped) ```",2025-01-18T09:27:19Z,type:bug comp:ops 2.17,open,0,1,https://github.com/tensorflow/tensorflow/issues/85240,I was able to reproduce the issue on Colab using TensorFlow v2.18.0 and TFnightly. Please find the gist here for your reference. Thank you!
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:21:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85239
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:20:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85238
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:19:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85237
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:19:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85236
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:19:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85235
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:18:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85234
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:18:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85233
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:18:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85232
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:18:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85231
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:17:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85230
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:17:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85229
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:17:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85228
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:15:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85227
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:14:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85226
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:14:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85225
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T09:11:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85224
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T08:22:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85223
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T06:36:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85220
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T05:44:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85219
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T05:20:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85218
cmilanes93,TFnode on TensorflowonSpark 2.2.5,"I'm running some code on Microsoft Fabric, and I use the following line: def map_fun(tf_args, ctx): cluster, server = TFNode.start_cluster_server(ctx) print('ctx') if ctx.job_name == ""ps"": server.join() else: print(""Hello from worker"", ctx.task_index) I'm getting an error regarding TFnode, does anyone know why this could be? TensorFlow version:2.12 TensorflowonSpark:2.2.5 The idea is to print a hello world to see if they are running on differents clusters.",2025-01-18T05:18:16Z,stat:awaiting response type:support stale TF 2.12,closed,0,9,https://github.com/tensorflow/tensorflow/issues/85217,"Hi, can someone help me? This is going to blow my mind 😂",", Without the reproducible code, it would be difficult for us to debug the issue. In order to expedite the troubleshooting process, could you please provide a minimal code snippet you are using. Thank you!","The error is likely due to the fact that TensorFlowOnSpark (TFoS) does not support TensorFlow versions greater than 1.x. Your TensorFlow version is 2.12, and TFoS 2.2.5 is designed for TensorFlow 1.x, which leads to compatibility issues. install TensorFlow 1.15 to match the compatibility of TensorFlowOnSpark","> [](https://github.com/cmilanes93), Without the reproducible code, it would be difficult for us to debug the issue. In order to expedite the troubleshooting process, could you please provide a minimal code snippet you are using. Thank you! Hi this is the code: La función TFNode.start_cluster_server utilizada en la biblioteca tensorflowonspark está obsoleta y no debería usarse en las versiones actuales de TensorFlow. import os import datetime import numpy as np import pandas as pd  PySpark / Spark from pyspark.sql import SparkSession  TensorFlowOnSpark from tensorflowonspark import TFCluster, TFNode spark = SparkSession.builder \     .config(""spark.executor.instances"", ""2"") \     .config(""spark.executor.cores"", ""1"") \     .config(""spark.dynamicAllocation.enabled"", ""false"") \     .config(""spark.shuffle.service.enabled"", ""false"") \     .getOrCreate() def map_fun(tf_args, ctx):     cluster, server = TFNode.start_cluster_server(ctx)     print('ctx')     if ctx.job_name == ""ps"":         server.join()     else:         print(""Hello from worker"", ctx.task_index)  Configuración consistente cluster = TFCluster.run(     sc=spark.sparkContext,     map_fun=map_fun,     tf_args={},     num_executors=2,   Debe coincidir con spark.executor.instances     num_ps=0,          Número de servidores de parámetros     input_mode=TFCluster.InputMode.SPARK )  RDD vacío para el entrenamiento rdd = spark.sparkContext.parallelize([]) cluster.train(rdd, 1) cluster.shutdown()","> The error is likely due to the fact that TensorFlowOnSpark (TFoS) does not support TensorFlow versions greater than 1.x. Your TensorFlow version is 2.12, and TFoS 2.2.5 is designed for TensorFlow 1.x, which leads to compatibility issues. install TensorFlow 1.15 to match the compatibility of TensorFlowOnSpark Is there any other option, im using Microsoft Fabric, so there is no way to install a lower version than tensorflow 2.x.",", Looks like this issue is not related to the Tensorflow. Take a look at TensorFlow on Spark if you already have Spark cluster: https://github.com/yahoo/TensorFlowOnSpark Or if you're running Kubernetes, you can use a configuration from https://github.com/tensorflow/ecosystem Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T05:04:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85216
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T04:57:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85215
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T04:54:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85214
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T04:51:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85213
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T04:51:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85212
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T04:51:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85211
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T04:50:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85210
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T04:50:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85209
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T04:49:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85208
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T04:48:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85207
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T04:47:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85206
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T04:47:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85205
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-18T04:44:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85204
copybara-service[bot],[xla:cpu:xnn] Measure execution time of parallel task to decide the optimal number of workers,[xla:cpu:xnn] Measure execution time of parallel task to decide the optimal number of workers ``` ```,2025-01-18T04:09:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85203
copybara-service[bot],Add cache entries for reshape ops in SPMD.,"Add cache entries for reshape ops in SPMD. We may have two compatible sharding pairs when handling reshape. If we have two pairs, we use the first one. We can still use the second one to add as a sharding cache. Given the following reshape, ``` p0 = bf16[8,8] parameter(0), sharding={replicated} reshape = bf16[64] reshape(p0), sharding={devices=[4] (bf16[16], bf16[64]) {   %param = bf16[8,8]{1,0} parameter(0), sharding={replicated}   %constant = s32[4]{0} constant({0, 2, 4, 6})   %partitionid = u32[] partitionid()   %dynamicslice = s32[1]{0} dynamicslice(s32[4]{0} %constant, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.1 = s32[] reshape(s32[1]{0} %dynamicslice)   %constant.1 = s32[] constant(0)   %dynamicslice.1 = bf16[2,8]{1,0} dynamicslice(bf16[8,8]{1,0} %param, s32[] %reshape.1, s32[] %constant.1), dynamic_slice_sizes={2,8}   %reshape.2 = bf16[16]{0} reshape(bf16[2,8]{1,0} %dynamicslice.1)   %allgather = bf16[64]{0} allgather(bf16[16]{0} %reshape.2), channel_id=1, replica_groups={{0,1,2,3}}, dimensions={0}, use_global_device_ids=true   %abs.1 = bf16[64]{0} abs(bf16[64]{0} %allgather)   ROOT %tuple.1 = (bf16[16]{0}, bf16[64]{0}) tuple(bf16[16]{0} %reshape.2, bf16[64]{0} %abs.1) } ``` With this change, we replace reshard with reshape ``` ENTRY %reshape_spmd (param: bf16[8,8]) > (bf16[16], bf16[64]) {   %param = bf16[8,8]{1,0} parameter(0), sharding={replicated}   %constant = s32[4]{0} constant({0, 2, 4, 6})   %partitionid = u32[] partitionid()   %dynamicslice = s32[1]{0} dynamicslice(s32[4]{0} %constant, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.1 = s32[] reshape(s32[1]{0} %dynamicslice)   %constant.1 = s32[] constant(0)   %dynamicslice.1 = bf16[2,8]{1,0} dynamicslice(bf16[8,8]{1,0} %param, s32[] %reshape.1, s32[] %constant.1), dynamic_slice_sizes={2,8}   %reshape.2 = bf16[16]{0} reshape(bf16[2,8]{1,0} %dynamicslice.1)   %reshape.3 = bf16[64]{0} reshape(bf16[8,8]{1,0} %param)   %abs.1 = bf16[64]{0} abs(bf16[64]{0} %reshape.3)   ROOT %tuple.1 = (bf16[16]{0}, bf16[64]{0}) tuple(bf16[16]{0} %reshape.2, bf16[64]{0} %abs.1) } ```",2025-01-18T01:52:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85202
copybara-service[bot],`TfrtCpuClient` should respect `run_backend_only` option.,`TfrtCpuClient` should respect `run_backend_only` option.,2025-01-18T01:44:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85201
copybara-service[bot],[JAX] Include the memory kind when converting JAX/IFRT sharding types,[JAX] Include the memory kind when converting JAX/IFRT sharding types,2025-01-18T01:20:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85200
copybara-service[bot],[ODML] JAX to tfl_flatbuffer : convert HLO to StableHLO instead MHLO.,"[ODML] JAX to tfl_flatbuffer : convert HLO to StableHLO instead MHLO. Removed wrapper functions, changed signature of LoadProto..",2025-01-18T01:12:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85199
copybara-service[bot],[XLA:Python] Remove unused GOOGLE_CUDA/TENSORFLOW_USE_ROCM macro guarding a header inclusion.,"[XLA:Python] Remove unused GOOGLE_CUDA/TENSORFLOW_USE_ROCM macro guarding a header inclusion. As far as I can tell, this does nothing at all.",2025-01-18T01:12:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85198
copybara-service[bot],Move XLA specific bits out of TensorFlow's bazelrc,Move XLA specific bits out of TensorFlow's bazelrc There are much fewer of these than expected!,2025-01-18T01:07:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85197
copybara-service[bot],print is_fully_replicated in DebugString,print is_fully_replicated in DebugString,2025-01-18T00:49:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85196
copybara-service[bot],Add argument to fix a random seed when generating random arguments for HLO runner. Also add OutputFormat so that literal dumps can be saved as a pb file.,Add argument to fix a random seed when generating random arguments for HLO runner. Also add OutputFormat so that literal dumps can be saved as a pb file.,2025-01-18T00:43:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85195
copybara-service[bot],Fix issue with SparseCore device ids and trace viewer.,Fix issue with SparseCore device ids and trace viewer.,2025-01-18T00:22:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85194
copybara-service[bot],Fix typo satisifies->satisfies,Fix typo satisifies>satisfies,2025-01-18T00:17:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85193
copybara-service[bot],Integrate LLVM at llvm/llvm-project@e5a28a3b4d09,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match e5a28a3b4d09,2025-01-18T00:15:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85192
copybara-service[bot],Fix wheel_version processing for the cases when `WHEEL_VERSION` has a custom suffix (e.g. 2.19.0-rc1).,Fix wheel_version processing for the cases when `WHEEL_VERSION` has a custom suffix (e.g. 2.19.0rc1). The old implementation didn't delete `` symbol and expected the wheel filename to be `tensorflow2.19.0.rc1cp310cp310linux_x86_64.whl` instead of `tensorflow2.19.0rc1cp310cp310linux_x86_64.whl`.,2025-01-17T23:39:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85191
copybara-service[bot],Refactor `SpmdPartitioningVisitor::HandleReshape`. No behavior change.,Refactor `SpmdPartitioningVisitor::HandleReshape`. No behavior change.,2025-01-17T23:36:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85190
copybara-service[bot],The runtime should only infer an entry computation layout if passes run.,The runtime should only infer an entry computation layout if passes run.,2025-01-17T23:30:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85189
copybara-service[bot],[xla:emitters] support CPU in common lowering passes,[xla:emitters] support CPU in common lowering passes This paves the way for sharing these passes with CPU. Note that lower_tensors requires more work before CPU tests can pass.,2025-01-17T23:02:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85188
copybara-service[bot],Fix wrong name of the attribute for channel handle,"Fix wrong name of the attribute for channel handle The attribute should be named `channel_handle`, not `channel_id`.",2025-01-17T22:26:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85187
copybara-service[bot],Add argument to fix a random seed when generating random arguments for HLO runner. Also add OutputFormat so that literal dumps can be saved as a pb file.,Add argument to fix a random seed when generating random arguments for HLO runner. Also add OutputFormat so that literal dumps can be saved as a pb file.,2025-01-17T22:15:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85186
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T20:29:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85185
copybara-service[bot],[xla:cpu:xnn] Use persistent workers to execute pthreadpool parallel loops,[xla:cpu:xnn] Use persistent workers to execute pthreadpool parallel loops ``` name                    old cpu/op   new cpu/op   delta BM_SingleTask1DLoop     6.52ns ± 2%  6.19ns ± 1%   5.16%  (p=0.000 n=36+39) BM_Parallelize2DTile1D  11.8µs ±16%   9.0µs ±26%  23.30%  (p=0.000 n=39+39) BM_Parallelize3DTile2D  12.9µs ± 6%  11.1µs ±16%  14.41%  (p=0.000 n=38+40) name                    old time/op          new time/op          delta BM_SingleTask1DLoop     6.55ns ± 3%          6.20ns ± 2%   5.24%  (p=0.000 n=37+39) BM_Parallelize2DTile1D  18.6µs ±12%          13.4µs ±23%  27.84%  (p=0.000 n=38+40) BM_Parallelize3DTile2D  22.1µs ± 5%          15.9µs ±11%  28.07%  (p=0.000 n=35+40) name                    old INSTRUCTIONS/op  new INSTRUCTIONS/op  delta BM_SingleTask1DLoop       76.0 ± 0%            68.0 ± 0%  10.53%  (p=0.000 n=39+40) BM_Parallelize2DTile1D    124k ±15%             77k ±24%  37.28%  (p=0.000 n=39+38) BM_Parallelize3DTile2D    147k ± 8%             99k ±16%  32.69%  (p=0.000 n=38+40) ```,2025-01-17T20:17:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85184
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@c125b328,Integrate StableHLO at openxla/stablehlo,2025-01-17T19:50:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85183
copybara-service[bot],Integrate LLVM at llvm/llvm-project@bf17016a92bc,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match bf17016a92bc,2025-01-17T18:51:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85182
copybara-service[bot],Disable BatchMatMul unfolding by default.,"Disable BatchMatMul unfolding by default. BMM is more efficient than the unfolded path, this flag existed for historical reasons; TFLite did not implement BMM initially. Converters disable this by default anyway, this only affects custom tools that import these rules.",2025-01-17T17:59:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85181
copybara-service[bot],[MHLO] Add parity with HLO for bounded dynamism in broadcast_in_dim / reshape ops,"[MHLO] Add parity with HLO for bounded dynamism in broadcast_in_dim / reshape ops Allow a single bounded dynamic dimension. This is likely a short term fix as bounded dynamism as a while likely needs a lot of thought, but this solution with a single bounded dim is unambiguous so should be safe.",2025-01-17T17:46:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85180
copybara-service[bot],[XLA:CPU] Save compiled symbols into the cpu executable proto,[XLA:CPU] Save compiled symbols into the cpu executable proto,2025-01-17T17:40:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85179
copybara-service[bot],internal change only,internal change only,2025-01-17T17:32:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85178
copybara-service[bot],Move GraphImportConfig stringification to import file and delete mlir_roundtrip_flags.cc,Move GraphImportConfig stringification to import file and delete mlir_roundtrip_flags.cc,2025-01-17T17:07:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85176
copybara-service[bot],internal change only,internal change only,2025-01-17T15:18:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85175
henghamao,Could not get sample weight from customized loss," Issue type Feature Request  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.13.1  Custom code Yes  OS platform and distribution CentOS 7.9  Mobile device _No response_  Python version 3.8.3  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? We used customized loss for the model training, and would like to get sample weight to calculate the loss. However, sample weight does not pass to loss function as expected.  Standalone code to reproduce the issue ```shell Here are the code to reproduce the issue. import tensorflow as tf from tensorflow.keras.layers import Dense import numpy as np def weighted_zero_mean_r2_loss(y_true, y_pred, sample_weight=None):     y_true = tf.cast(y_true, tf.float32)     y_pred = tf.cast(y_pred, tf.float32)     sample_weight = tf.cast(sample_weight, tf.float32)     weighted_squared_error = sample_weight * (y_true  y_pred) ** 2     weighted_true_squared = sample_weight * y_true ** 2     numerator = tf.reduce_sum(weighted_squared_error)     denominator = tf.reduce_sum(weighted_true_squared)     r2_score = 1  numerator / denominator     return r2_score def build_model():     metrics = 'mae'     loss = weighted_zero_mean_r2_loss 'mse'     output_num = 1     inputs = tf.keras.layers.Input(shape=(40, 36))     lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)     output = Dense(output_num, activation='linear')(lstm_out)     model = tf.keras.Model(inputs, output)     model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)     model.summary()     return model def data_gen(class_weights):     while True:         x_batch = np.random.rand(128, 40, 36)         y_batch = np.random.randint(0, 3, (128, 1))          Apply class weights to the labels         sample_weights = np.vectorize(lambda x: class_weights[x])(y_batch)         yield x_batch, y_batch, sample_weights model = build_model() cw = {0: 0.3, 1: 2.5, 2: 3.2} model.fit(data_gen(cw), epochs=2, steps_per_epoch=10) ```  Relevant log output ```shell Error messages: File ""/root/.virtualenvs/infinity_stock/lib/python3.8/sitepackages/keras/src/engine/training.py"", line 1338, in train_function  *         return step_function(self, iterator)     File ""/data/release/kagglejanestreet/scripts/python/test.py"", line 8, in weighted_zero_mean_r2_loss  *         sample_weight = tf.cast(sample_weight, tf.float32)     ValueError: None values not supported. ```",2025-01-17T15:16:39Z,type:feature comp:keras TF 2.13,closed,0,9,https://github.com/tensorflow/tensorflow/issues/85174,"I was able to reproduce the issue on Colab using TensorFlow v2.13 and TFnightly. Please find the gist1, gist2 here for your reference. Thank you!","Hi   When you are building a custom loss function, the loss function should have a signature of **```custom_loss_fn(y_true, y_pred)```** (source). When doing this, you are essentially trying to override the `call()` function in `class Loss` in keras/src/losses/loss/py.  You can still pass in a `sample_weight` parameter to the `model.fit()` call with the custom loss function, but sample_weight will be multiplied elementwise with the loss terms in the reduce_weighted_values() call. So it won't work the way you want it to for the r2 loss. One observation about your code is that sample_weight only depends on y_true, so you could move the sample_weight computation inside your function.  The following code would work: ``` def weighted_zero_mean_r2_loss(y_true, y_pred):     y_pred = tf.cast(y_pred, tf.float32)     y_true = tf.cast(y_true, tf.float32)     table = tf.lookup.StaticHashTable(         tf.lookup.KeyValueTensorInitializer(             keys=tf.constant([0, 1, 2], dtype=tf.int32),              values=tf.constant([0.3, 2.5, 3.2], dtype=tf.float32)),         default_value=1)     sample_weight = table.lookup(tf.cast(y_true, tf.int32))     weighted_squared_error = sample_weight * (y_true  y_pred) ** 2     weighted_true_squared = sample_weight * y_true ** 2     numerator = tf.reduce_sum(weighted_squared_error)     denominator = tf.reduce_sum(weighted_true_squared)     r2_score = 1  numerator / denominator     return r2_score ``` If you want to use a custom logic for applying sample_weights, there's another way to do it by subclassing the `keras.losses.Loss` class. You would have to override the `__init__` and `call` functions. You can pass in a custom function during `__init__` and use it during `call()`. For example: ``` class WeightedZeroMeanR2Loss((keras.losses.Loss):   def __init__(self, custom_weight_multiplier_fn, name='weighted_zero_mean_r2_loss'):     super().__init__(name=name)     self._custom_weight_multiplier_fn = custom_weight_multiplier_fn   def call(self, y_true, y_pred):      sample_weight = self._custom_weight_multiplier_fn(y_true)     weighted_squared_error = sample_weight * (y_true  y_pred) ** 2     weighted_true_squared = sample_weight * y_true ** 2     numerator = tf.reduce_sum(weighted_squared_error)     denominator = tf.reduce_sum(weighted_true_squared)     r2_score = 1  numerator / denominator     return r2_score ``` Note that you would probably need to use tensorflow operations and not python operations, since the loss function gets converted to a tf.function during the graph execution. You can initialize the model with the loss class above as follows: ```     model.compile(loss=WeightedZeroMeanR2Loss(custom_fn), optimizer=tf.keras.optimizers.Adam(), metrics=metrics) ``` I hope this explanation helps. Please let me know if you have any questions. ","Hi  , Thanks for the reply. The code is to reproduce the issue. And in real scenario, the sample weight could not refer by y_ture value.  The problme is the regression for multiple categories of data. The sample weight is to apply for different categories. y_ture is the data point at time stamp t. If it is a classify problem, we could refer sample weight by category label of y_ture value. However, for regression problems, we could not do that. In torch, we could easily use customize loss to calcualte weighted_r2_loss. Example code as below ``` class WeightedR2Loss(nn.Module):     """"""PyTorch loss function for weighted R².""""""     def __init__(self, epsilon: float = 1e38) > None:         """"""         Initialize the WeightedR2Loss class.         Args:             epsilon (float, optional): Small constant added to the denominator                  for numerical stability. Defaults to 1e38.         """"""         super(WeightedR2Loss, self).__init__()         self.epsilon = epsilon     def forward(         self,         y_pred: torch.Tensor,         y_true: torch.Tensor,         weights: torch.Tensor) > torch.Tensor:         """"""Compute the weighted R² loss.         Args:             y_true (torch.Tensor): Ground truth tensor.             y_pred (torch.Tensor): Predicted tensor.             weights (torch.Tensor): Weights for each observation (same shape as y_true).         Returns:             torch.Tensor: Computed weighted R² loss.         """"""         numerator = torch.sum(weights * (y_pred  y_true) ** 2)         denominator = torch.sum(weights * (y_true) ** 2) + 1e38         loss = numerator / denominator         return loss ``` Hope tf could provide similar solution to solve the problem.","  Thank you for going over your use case in detail. You can do this in Tensorflow by subclassing `keras.Model` and overriding the `compute_loss()` function. ``` class CustomModel(keras.Model):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)     def compute_loss(self, x, y_true, y_pred, sample_weight):         y_true = tf.cast(y_true, tf.float32)         y_pred = tf.cast(y_pred, tf.float32)         sample_weight = tf.cast(sample_weight, tf.float32)         weighted_squared_error = sample_weight * (y_true  y_pred) ** 2         weighted_true_squared = sample_weight * y_true ** 2         numerator = tf.reduce_sum(weighted_squared_error)         denominator = tf.reduce_sum(weighted_true_squared)         r2_score = 1  numerator / denominator         return r2_score def build_model():     metrics = ['mae']     output_num = 1     inputs = tf.keras.layers.Input(shape=(40, 36))     lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)     output = Dense(output_num, activation='linear')(lstm_out)     model = CustomModel(inputs, output)     class_weights = {0: 0.3, 1: 2.5, 2: 3.2}     model.compile(optimizer=tf.keras.optimizers.Adam(), metrics=metrics)     model.summary()     return model def data_gen(class_weights):     while True:         x_batch = np.random.rand(128, 40, 36)         y_batch = np.random.randint(0, 3, (128, 1))          Apply class weights to the labels         sample_weights = np.vectorize(lambda x: class_weights[x])(y_batch)         yield x_batch, y_batch, sample_weights model = build_model() cw = {0: 0.3, 1: 2.5, 2: 3.2} model.fit(data_gen(cw), epochs=2, steps_per_epoch=10) ```","  Great thanks for providing the solution. It works for our problems. BTW, there is another issue about class weight and sample weight with the similar code to reproduce the issue. https://github.com/tensorflow/tensorflow/issues/77958 We submited the issue a few months ago, and it did not get any further updates.",Happy to help!  Let me take a look at https://github.com/tensorflow/tensorflow/issues/77958.,"  Hi SanjaySG, We found a problem with the code. By using comput_loss(), the model tend to maximize the loss, rather than minimize the loss. Here is the code to repoduce the issue: ``` import tensorflow as tf from tensorflow.keras.layers import Dense import numpy as np class CustomModel(tf.keras.Model):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)     def compute_loss(self, x, y_true, y_pred, sample_weight):         y_true = tf.cast(y_true, tf.float32)         y_pred = tf.cast(y_pred, tf.float32)         sample_weight = tf.cast(sample_weight, tf.float32)         weighted_squared_error = sample_weight * (y_true  y_pred) ** 2         weighted_true_squared = sample_weight * y_true ** 2         numerator = tf.reduce_sum(weighted_squared_error)         denominator = tf.reduce_sum(weighted_true_squared)         r2_score = 1  numerator / denominator         return r2_score def build_model():     metrics = ['mae']     output_num = 1     inputs = tf.keras.layers.Input(shape=(40, 36))     lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)     output = Dense(output_num, activation='linear')(lstm_out)     model = CustomModel(inputs, output)     class_weights = {0: 0.3, 1: 2.5, 2: 3.2}     model.compile(optimizer=tf.keras.optimizers.Adam(), metrics=metrics)     model.summary()     return model def data_gen(class_weights):     while True:         x_batch = np.random.rand(128, 40, 36)         y_batch = np.random.randint(0, 3, (128, 1))          Apply class weights to the labels         sample_weights = np.vectorize(lambda x: class_weights[x])(y_batch)         yield x_batch, y_batch, sample_weights model = build_model() cw = {0: 0.3, 1: 2.5, 2: 3.2} model.fit(data_gen(cw), epochs=50, steps_per_epoch=10, verbose=2) ``` We observed the metric 'mae' keep growing, and the loss grows as well.","  This is down to the objective chosen for optimization. The model.fit() call tries to **minimize** the loss. r2 on the other hand should increase when the model performs better. A naive way to do this is by providing the negative of r2.  More importantly, r2 is not differentiable. So ideally, it shouldn't be used as a loss function for gradient descent, but it can be used as a metric. Something like MSE or MAE would be a better loss function. ",Great thanks for the explanations.
copybara-service[bot],internal change only,internal change only,2025-01-17T15:11:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85173
copybara-service[bot],Add some clarifying comments for Dockerfiles.,Add some clarifying comments for Dockerfiles.,2025-01-17T14:59:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85172
copybara-service[bot],[XLA:GPU] Allow fusing multiply into triton fusion.,[XLA:GPU] Allow fusing multiply into triton fusion. The pr enables multiply fusion for the cases when its arguments have positive effect on the input sizes. I.e. when total size of multiply args is less than the output size of the multiply. It is the case when we multiply s4 by a small number of scales and pass the result to the dot.,2025-01-17T14:51:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85171
copybara-service[bot],Fix up Windows libtensorflow artifacts to have the same location/naming as the rest.,Fix up Windows libtensorflow artifacts to have the same location/naming as the rest.,2025-01-17T14:40:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85170
copybara-service[bot],[XLA:GPU] Add ConvertIndexType pass,[XLA:GPU] Add ConvertIndexType pass Follow up to '[XLA:GPU] Add RewritePatterns for binary elementwise ops in SimplifyAffinePass.'. Fixed pass not finding the parent module (as it was a module op to begin with) and defaulting to i64.,2025-01-17T14:12:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85169
copybara-service[bot],Move Preprocessing of graphdef to graph_constructor to decouple code and allow file translation to use new tf2xla api.,Move Preprocessing of graphdef to graph_constructor to decouple code and allow file translation to use new tf2xla api.,2025-01-17T14:04:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85168
copybara-service[bot],[pjrt] Removed the deprecated `BufferFromHostLiteral` overload taking a `PjRtDevice`,[pjrt] Removed the deprecated `BufferFromHostLiteral` overload taking a `PjRtDevice` All usages were migrated to the overload taking a `PjRtMemorySpace`,2025-01-17T13:59:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85167
copybara-service[bot],[xla:gpu] extract atomic_rmw_utils to a separate library,"[xla:gpu] extract atomic_rmw_utils to a separate library It's not a pass, just some utility, and therefore should not be in the ""passes"" library. This paves the way for upcoming work  we will depend on this library without having to depend on the much larger :passes library.",2025-01-17T13:53:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85166
copybara-service[bot],[XLA:GPU][NFC] Optimize `TritonEmitterLongDeviceTest.FusionWithOutputContainingMoreThanInt32MaxElementsExecutesCorrectly` to run in reasonable time.,"[XLA:GPU][NFC] Optimize `TritonEmitterLongDeviceTest.FusionWithOutputContainingMoreThanInt32MaxElementsExecutesCorrectly` to run in reasonable time. The test now requires much fewer resources than it used toand runs more than `50x` faster. As a result, a dedicated test target is no longer necessary.",2025-01-17T13:48:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85165
copybara-service[bot],internal changes only,internal changes only,2025-01-17T13:07:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85161
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T12:46:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85147
copybara-service[bot],[pjrt] Use the `PjRtMemorySpace*` version of `BufferFromHostLiteral`,[pjrt] Use the `PjRtMemorySpace*` version of `BufferFromHostLiteral` Buffers live in memory spaces and not on devices. The `PjRtDevice` version of `BufferFromHostLiteral` is deprecated and will be removed once the migration is complete.,2025-01-17T12:21:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85145
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-17T11:45:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85144
copybara-service[bot],PR #21549: Remove rocdl_path dependency from non rocm builds,PR CC(tf.contrib.ffmpeg.decode_video Error): Remove rocdl_path dependency from non rocm builds Imported from GitHub PR https://github.com/openxla/xla/pull/21549 This PR removes the unwanted dependency to rocm while building for other platforms. Copybara import of the project:  0aecf04829d831fc5cacb5fee5575600121ec45b by Alexandros Theodoridis : Remove rocdl_path dependency from non rocm builds Merging this change closes CC(tf.contrib.ffmpeg.decode_video Error) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21549 from ROCm:ci_remove_rocdl_path_depenency_from_non_rocm_builds 0aecf04829d831fc5cacb5fee5575600121ec45b,2025-01-17T11:22:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85142
copybara-service[bot],[XLA:CPU] Read thunks from proto when loading executable.,[XLA:CPU] Read thunks from proto when loading executable.,2025-01-17T11:19:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85141
copybara-service[bot],[XLA:GPU][NFC] Add debugging information in case a test break.,"[XLA:GPU][NFC] Add debugging information in case a test break. The pattern in which it breaks is irregular, so it's worth pointing out.",2025-01-17T11:12:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85140
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T11:05:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85138
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-17T10:52:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85137
copybara-service[bot],Internal cosmetic change,Internal cosmetic change,2025-01-17T10:50:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85136
copybara-service[bot],Adapt HloFusionAdaptor GetRoots() method,"Adapt HloFusionAdaptor GetRoots() method For ProducerConsumer fusions, we can have the case that a multioutput fusion is created even if the producer fusion was not a multioutput fusion. This happens if the fusion root of the producer is also used outside of the created ProducerConsumer fusion, and we don't want to duplicate the producer. This change adds the possibility to create a HloFusionAdaptor for a ProducerConsumer fusion with extra outputs created for producer roots that are used outside the ProducerConsumer fusion.",2025-01-17T10:14:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85133
inkzk,Unequal width and height of stride in tf.nn.depthwise_conv2d not supported?,"Is that right? IF YES, how can I convert the pretrained weights trained with unequal strides to tensorflow `dwconv` with some other ops? THX!",2025-01-17T10:00:13Z,stat:awaiting response type:support stale,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85131,", Without the reproducible code, it would be difficult for us to debug the issue. In order to expedite the troubleshooting process, could you please provide a minimal code snippet and the TensorFlow version you are using. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Make creation of CompilationProvider depend on DebugOptions,"Make creation of CompilationProvider depend on DebugOptions  Backstory With the introduction of the CompilationProvider framework I created an instance of the CompilationProvider in the constructor of NVPTXCompiler. That meant individual HLO compilations couldn't influence the PTX compilation pipeline anymore, they would all go through the same compilation provider. That was very intentional because the PTX compilation path can influence the resulting binary but the caching logic that was inplace didn't account for that and happily returned a cache hit that was generated through a via a different path. This has led to subtle bugs and weird behaviour (compilation that should fail don't because they get served from the cache) in the past. But as it turns out there are downstream users of XLA that depend on the ability to influence the PTX compilation pipeline through the DebugOptions that come with an HLO module. They set `xla_gpu_cuda_data_dir` programmatically there and don't specify it through XLA_FLAGS (which was still working fine).  What's changing So in this change I make NVPTXCompiler handle multiple compilation providers based on the flags in the debug options. In almost all cases there will ever be one compilation provider, but since NVPTXCompiler can now handle different pipelines we also avoid the caching bug from before.  Implementation details 1. There is a new type `CompilationProviderOptions`. It holds all the parameters from `DebugOptions` that can influence how the PTX compilation pipeline gets constructed. It's hashable, so it can act as a key in a hash map. There is also a conversion function from debug options. 2. `AssembleCompilationProvider` now takes a `CompilationProviderOptions` value instead of a `DebugOptions` value. This way we make sure it only depends on parameters defined in the former. 3. `NVPTXCompiler` holds a mutexguarded map from `CompilationProviderOptions` to `CompilerProvider`. When an `HloModule` gets compiled we construct the `CompilationProviderOptions` from the debug options and retrieve the correct compilation provider from the map (assemble a new one, if none exists already).",2025-01-17T09:58:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85130
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:48:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85129
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20274 from ROCm:c64_atomics 6553059e8d5bf039ef526b2b904808b55051cab9,2025-01-17T09:35:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85128
copybara-service[bot],[XLA:GPU] Fix comment for `xla_gpu_analytical_latency_estimator_options` usage.,[XLA:GPU] Fix comment for `xla_gpu_analytical_latency_estimator_options` usage. s/ms/us.,2025-01-17T09:33:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85127
copybara-service[bot],[XLA:GPU] Add LegalizeSchedulingAnnotation pass in the gpu compilation pipeline.,"[XLA:GPU] Add LegalizeSchedulingAnnotation pass in the gpu compilation pipeline. The legalizer pass mentioned above   allows erroring out with meaningful messages when there is something unexpected about the user's annotated scheduling groups, and  preprocesses the scheduling annotations so that they are ready for the annotatedgroup scheduling in latency_hiding_scheduler. This CL also filters the annotated compute instructions to be only cublass gemm calls.",2025-01-17T09:27:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85126
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:22:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85125
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:16:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85124
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:16:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85123
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:16:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85122
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:16:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85121
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:15:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85120
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:15:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85119
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:15:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85118
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:15:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85117
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:15:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85116
copybara-service[bot],compat: Update forward compatibility horizon to 2025-01-17,compat: Update forward compatibility horizon to 20250117,2025-01-17T09:15:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85115
copybara-service[bot],compat: Update forward compatibility horizon to 2025-01-17,compat: Update forward compatibility horizon to 20250117,2025-01-17T09:14:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85114
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:14:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85113
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:14:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85112
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:14:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85111
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:14:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85110
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-17T09:13:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85109
copybara-service[bot],[XLA] Cleanup global_data.h references,[XLA] Cleanup global_data.h references global_data.h is deprecated,2025-01-17T09:09:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85108
copybara-service[bot],Move fusions.* and fusion_emitter.* to backends/gpu/codegen directory.,Move fusions.* and fusion_emitter.* to backends/gpu/codegen directory.,2025-01-17T07:26:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85105
copybara-service[bot],Allow IO adapters with the _experimental_strict_qdq strict flag.,Allow IO adapters with the _experimental_strict_qdq strict flag. The code path to use QDQ annotations is explicitly enabled via a flag. Previously the existence of certain nodes in the graph indicated a QAT graph which is no longer the case.,2025-01-17T02:23:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85100
copybara-service[bot],Rollback: Move GraphImportConfig stringification to import file and delete mlir_roundtrip_flags.cc,Rollback: Move GraphImportConfig stringification to import file and delete mlir_roundtrip_flags.cc,2025-01-17T02:13:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85099
copybara-service[bot],[TSL] Use absl::Time to compute timestamps for logging,[TSL] Use absl::Time to compute timestamps for logging This allows us to avoid having to handle platform specific details like localtime_r() vs localtime_s() vs localtime().,2025-01-16T23:54:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85098
copybara-service[bot],Add max infeed time core name to InputPipelineAnalysis.,Add max infeed time core name to InputPipelineAnalysis. Add max infeed table conversion in python.,2025-01-16T23:47:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85097
copybara-service[bot],Dump CL number as part of the filename of a HLO dump,Dump CL number as part of the filename of a HLO dump,2025-01-16T23:35:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85096
copybara-service[bot],Integrate LLVM at llvm/llvm-project@bf17016a92bc,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match bf17016a92bc,2025-01-16T23:15:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85095
copybara-service[bot],Migrate convert_test to always use PjRt for its test backend.,Migrate convert_test to always use PjRt for its test backend.,2025-01-16T23:00:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85094
copybara-service[bot],Add kCpu property tag.,Add kCpu property tag.,2025-01-16T22:58:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85093
copybara-service[bot],Use `HloRunnerPropertyTag::kCpu` in convert_test.,Use `HloRunnerPropertyTag::kCpu` in convert_test.,2025-01-16T22:57:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85092
copybara-service[bot],Prepare code for breaking change in Protobuf C++ API.,Prepare code for breaking change in Protobuf C++ API. Protobuf 6.30.0 will change the return types of Descriptor::name() and other methods to absl::string_view. This makes the code work both before and after such a change.,2025-01-16T22:50:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85091
copybara-service[bot],[Emitters] Move ir/ and transforms/ under emitters/ directory.,[Emitters] Move ir/ and transforms/ under emitters/ directory.,2025-01-16T22:36:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85090
copybara-service[bot],Reverts a7109ae416b6448afa708ec0b7925c7b0daadd81,Reverts a7109ae416b6448afa708ec0b7925c7b0daadd81,2025-01-16T21:29:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85089
copybara-service[bot],Add LLM inference engine based on CompiledModel APIs,Add LLM inference engine based on CompiledModel APIs The new pipeline is only enabled with `use_compiled_model` flag to the script. It will define `USE_LITERT_COMPILED_MODEL` for the executor builds. The KV Cache management logic is implemented in LlmLiteRtCompiledModelExecutor with TensorBuffers.,2025-01-16T21:22:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85088
copybara-service[bot],Rename variables for clarity and add missing imports,Rename variables for clarity and add missing imports,2025-01-16T21:18:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85087
copybara-service[bot],Refactor mechanisms of building TF wheel and storing TF project version.,"Refactor mechanisms of building TF wheel and storing TF project version. This change introduces a uniform way of building the TF wheel and controlling the filename version suffixes. A new repository rule `python_wheel_version_suffix_repository` provides information about project and wheel version suffixes. The final value depends on environment variables passed to Bazel command: `_ML_WHEEL_WHEEL_TYPE, _ML_WHEEL_BUILD_DATE, _ML_WHEEL_GIT_HASH, _ML_WHEEL_VERSION_SUFFIX` `tf_version.bzl` defines the TF project version and loads the version suffix information calculated by `python_wheel_version_suffix_repository`. The targets `//tensorflow/core/public:release_version, //tensorflow:tensorflow_bzl //tensorflow/tools/pip_package:setup_py` use the version chunks defined above. The version of the wheel in the build rule output depends on the environment variables. Environment variables combinations for creating wheels with different versions:   * snapshot (default build rule behavior): `repo_env=ML_WHEEL_TYPE=snapshot`   * release: `repo_env=ML_WHEEL_TYPE=release`   * release candidate: `repo_env=ML_WHEEL_TYPE=release repo_env=ML_WHEEL_VERSION_SUFFIX=rc1`   * nightly build with date as version suffix: `repo_env=ML_WHEEL_TYPE=nightly repo_env=ML_WHEEL_BUILD_DATE=`   * build with git data as version suffix: `repo_env=ML_WHEEL_TYPE=custom repo_env=ML_WHEEL_BUILD_DATE=$(git show s format=%as HEAD) repo_env=ML_WHEEL_GIT_HASH=$(git revparse HEAD)`",2025-01-16T21:16:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85086
copybara-service[bot],[TSL] Don't truncate thread ids,"[TSL] Don't truncate thread ids XNU platforms (iOS, macOS, etc.) use a 64bit thread identifier which is never reused. Overflowing 32 bits is quite easy.",2025-01-16T20:40:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85085
copybara-service[bot],Move GraphImportConfig stringification to import file and delete mlir_roundtrip_flags.cc,Move GraphImportConfig stringification to import file and delete mlir_roundtrip_flags.cc,2025-01-16T20:00:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85084
copybara-service[bot],Add serialization support for vhlo sub and or,Add serialization support for vhlo sub and or,2025-01-16T19:39:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85083
copybara-service[bot],Fix typo in comment for stablehlo->mhlo converter,Fix typo in comment for stablehlo>mhlo converter,2025-01-16T19:30:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85082
copybara-service[bot],Add a test for the `cholesky_expander` pass.,"Add a test for the `cholesky_expander` pass. There was some test coverage for `cholesky_expander`, but it wasn't located within the `hlo/` component and wasn't exactly a unit test as such.",2025-01-16T19:26:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85081
copybara-service[bot],[xla:cpu] Add IfRt benchmark with many kernels and results,[xla:cpu] Add IfRt benchmark with many kernels and results  Benchmark                      Time             CPU   Iterations  BM_IfRtAddScalars            283 ns          283 ns      9813844 BM_IfRtAddmanyScalars        867 ns          866 ns      3172014,2025-01-16T19:11:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85080
copybara-service[bot],LiteRT: Update CompiledModel API,LiteRT: Update CompiledModel API  Added CreateInputBuffer / CreateOutputBuffer APIs with tensor names.  Use unmanaged litert::Model to keep the original Model instead of pointer   which is unstable.  Added `const` to const methods.,2025-01-16T19:08:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85079
copybara-service[bot],[xla:gpu] move some transforms to xla/codegen/emitters/transforms,[xla:gpu] move some transforms to xla/codegen/emitters/transforms These will soon be shared between CPU and GPU.,2025-01-16T18:50:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85078
copybara-service[bot],Create `xla.bazelrc` in preparation for XLA having a completely independent .bazelrc,Create `xla.bazelrc` in preparation for XLA having a completely independent .bazelrc Starting with disabling `mavxvnniint8` from XNNPACK. In the future we can refactor more XLA specific bits into this file. Eventually we can stop using the TF bazelrc entirely.,2025-01-16T18:35:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85077
copybara-service[bot],Add hbm read and write time to Grappler::Costs.,Add hbm read and write time to Grappler::Costs.,2025-01-16T18:29:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85076
copybara-service[bot],Reverts 3bc00d7bec9ea7051cebae593cab467feba4ddc9,Reverts 3bc00d7bec9ea7051cebae593cab467feba4ddc9,2025-01-16T17:52:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85075
cg-tester,Urgent help needed.,"Hello, hope anybody help me please! I am going to train a model for information extraction from photo. I am using Windows 10. And am using python 3.10.10. Actually I used python 3.12 but I couldn't install tensorflowio package with python 3.12. Not sure reason yet. So I use python 3.10 now. I have prepared dataset and downloaded predefined model from Model Zoo. Now I need to train. Steps that I did: 1. git clone https://github.com/tensorflow/models.git     cd models 2. pip install tensorflowtext (It will automatically install tensorflow==2.10.1)     pip install tensorflowio 3. cd official     pip install r requirements.txt 4. cd ..     cd research     in object_detection/packages/tf2/setup.py file     change version of tfmodelsofficial into 2.10.1 or 2.10.0     python object_detection/packages/tf2/setup.py install 5. $env:PYTHONPATH = ""$(GetLocation):$(GetLocation)\slim""  in Powershell or     set PYTHONPATH=%cd%;%cd%\slim  in Cmd 6. protoc object_detection/protos/*.proto python_out=.     protoc and protobuf version are same. 3.19.6 7. Run train command     python object_detection/model_main_tf2.py pipeline_config_path=../../model/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu8/pipeline.config      model_dir=../../model/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu8/checkpoint num_train_steps=5000       sample_1_of_n_eval_examples=1 alsologtostderr I have not succeded in here. For not below error I have. venv\lib\sitepackages\tensorflow\python\framework\dtypes.py"", line 34, in      _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type() TypeError: Unable to convert function return value to a Python type! The signature was         () > handle But I tried with many other python versions, tensorflow versions, but every time I met version conflict issues and bugs inside packages. Hope anybody help me run this process to train. I need help urgently. Thanks.",2025-01-16T17:50:21Z,stat:awaiting response type:build/install stale subtype:windows TF 2.10,closed,0,6,https://github.com/tensorflow/tensorflow/issues/85074,"> Actually I used python 3.12 but I couldn't install tensorflowio package with python 3.12. Not sure reason yet. There is no support for Python 3.12 on Windows on tensorflowio. See https://pypi.org/project/tensorflowio/files, there is no Windows file there. You can install on Python 3.10 (or 3.11 too) because an older version supports Windows: https://pypi.org/project/tensorflowio/0.31.0/files. But that version, 0.31.0, was released nearly 2 years ago, so it is not compatible with Tensorflow 2.10 which was released in 2022. You need to have everything at compatible versions. Ideally, you'd also use versions that are in the support window (last release and 1 or 2 releases behind that).","Thanks for your reply   Well, could you let me know versions that are compatible? I use python 3.10 now, because I am using Windows OS. I need to train model on Windows OS absolutely. So could you give me list of compatible versions of packages like tensorflow, tensorflowtext, tensorflowio, protobuf, tfmodelsofficial and so on. Highest available version of tensorflowtext is 2.10, so I am using tensorflow 2.10 but still have many issues inside tensorflow package files. Not sure reason. Hope you to provide me good set of packages and usage experience, if possible. Thanks very much. Hope to build friendship!!!","TensorFlow is the main repo in the ecosystem. We guarantee support for the last released version and 1 or 2 behind it. You can use https://pypi.org/ to locate all of the packages you need and make sure they were released at around the same time. For a package, if you go to the files tab you can see all combinations that are supported. Or you can try `pip install $package==$version` (`$package` and `$version` are placeholders here) and see if they get installed on your system. If you cannot install one, then most likely your system is not supported / too old (either the system or the software stack you are using).",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[Emitters] Add a link to the external vector atomic_rmw lowering bug.,[Emitters] Add a link to the external vector atomic_rmw lowering bug.,2025-01-16T17:24:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85073
copybara-service[bot],[pjrt] Use the `PjRtMemorySpace*` version of `BufferFromHostLiteral`,[pjrt] Use the `PjRtMemorySpace*` version of `BufferFromHostLiteral` Buffers live in memory spaces and not on devices. The `PjRtDevice` version of `BufferFromHostLiteral` is deprecated and will be removed once the migration is complete.,2025-01-16T17:12:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85072
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-16T17:05:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85071
cg-tester,Unable to convert function return value to a Python type! Error Fix Solution Needed," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.10.1  Custom code Yes  OS platform and distribution Windows 10  Mobile device _No response_  Python version 3.10.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am using Windows 10 I use python 3.10.10, because when I use python 3.12, I couldn't install tensorflowio I installed tensorflow 2.10.1, because only tensorflowtext 2.10.0 is available on Windows 10. Actually I am going to train model. So I cloned tensorflow models repository and made dataset, ... I want help to run models/research/object_detection/model_main_tf2.py file without error. Please help me.  Standalone code to reproduce the issue ```shell in tensorflow/python/framework/dtypes.py line 29 _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type() Here I have error TypeError: Unable to convert function return value to a Python type! The signature was         () > handle ```  Relevant log output ```shell ```",2025-01-16T16:40:25Z,type:bug,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85070,"This looks like a duplicate. Please don't open multiple issues for the same thing. Also, please don't ask for urgent help, this is an OSS community.",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[xla:cpu] Disable AVX_VNNI in XNNPACK by default.,"[xla:cpu] Disable AVX_VNNI in XNNPACK by default. The feature invokes compiler flag mavxvnniint8 which isn't available in many compilers, e.g., not in clang older than version 16.",2025-01-16T16:18:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85069
copybara-service[bot],[XLA:GPU] Add ConvertIndexType pass,[XLA:GPU] Add ConvertIndexType pass Follow up to '[XLA:GPU] Add RewritePatterns for binary elementwise ops in SimplifyAffinePass.'. Fixed pass not finding the parent module (as it was a module op to begin with) and defaulting to i64.,2025-01-16T16:08:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85068
copybara-service[bot],[xla:cpu:xnn] Add Dot op support to XNN fusion emitter.,[xla:cpu:xnn] Add Dot op support to XNN fusion emitter. + Refactor IsXnnDotSupported. + Move xnn_fusion_test..,2025-01-16T15:42:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85067
copybara-service[bot],Integrate LLVM at llvm/llvm-project@bf17016a92bc,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match bf17016a92bc,2025-01-16T15:37:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85066
gmjw,tf.math.bincount no longer broadcasts over weights," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.18.0 (behaviour started 2.15)  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04.5 LTS  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version Could not find cuda drivers on your machine  GPU model and memory _No response_  Current behavior? The `tf.math.bincount` implementation used to broadcast nicely when `values` was a flat tensor, allowing the same `values` to be used for all rows of the `weights` passed in.  However, this is no longer the case. The new behaviour seems to make it impossible to broadcast `values` at all, even if reshaping them to match the shape of `weights`. If you run the script below using tf 2.14, it runs fine and produces sensible output, two columns bincounted. However, if run in 2.15 or later, it throws `weights must be the same shape as arr or a length0 Tensor`. I have tried reshaping the values to match the shape of weights (as per the commented out line of code in the snippet below), but this still does not return the same output as in 2.14  the output is a single column, rather than sensibly bincounted data for two columns. This does not seem to be solvable by using the other arguments (e.g. axis=1).  Standalone code to reproduce the issue ```shell import numpy as np import tensorflow as tf values = tf.constant(     [1, 2, 3, 1, 2, 3], )  Alternative: Try repeating values to be same shape as weights  values = tf.repeat(tf.reshape(values, (1, 1)), 2, axis=1) weights = tf.constant([     [0.1,  0.5],   corresponds to 1, as per `values` above     [0.2,  0.1],   corresponds to 2     [0.3,  0.03],   etc     [0.01, 0.05],     [0.02, 0.01],     [0.03, 0.0], ]) print('values.shape', values.shape) print('weights.shape', weights.shape) out = tf.math.bincount(values, weights=weights) print() print(out) print()  The output describes the amount of weight associated  with each value, per column of `weights`. expected = [     [0.0, 0.0],   weight associated with 0     [0.11, 0.55],   with 1     [0.22, 0.11],   etc     [0.33, 0.03], ] np.testing.assert_allclose(out, expected) ```  Relevant log output ```shell ```",2025-01-16T15:32:01Z,type:support,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85065,"Apologies, I've realised that things work properly if the inputs to bincount are transposed. So, the example above works fine if the `Alternative:` approach is taken, repeating the values, then everything is transposed. Marking this issue as closed.",Are you satisfied with the resolution of your issue? Yes No
SankuriJeyaSanjana,Tensorflow related issue," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Collecting tensorflowNote: you may need to restart the kernel to use updated packages.   Using cached tensorflow2.18.0cp310cp310win_amd64.whl.metadata (3.3 kB) Requirement already satisfied: tensorflowintel==2.18.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflow) (2.18.0) Requirement already satisfied: abslpy>=1.0.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (2.1.0) Requirement already satisfied: astunparse>=1.6.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (1.6.3) Requirement already satisfied: flatbuffers>=24.3.25 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (24.12.23) Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (0.4.0) Requirement already satisfied: googlepasta>=0.1.1 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (0.2.0) Requirement already satisfied: libclang>=13.0.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (18.1.1) Requirement already satisfied: opteinsum>=2.3.2 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (3.4.0) Requirement already satisfied: packaging in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (24.2) Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,=3.20.3 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (5.29.3) Requirement already satisfied: requests=2.21.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (2.32.3) Requirement already satisfied: setuptools in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (57.4.0) Requirement already satisfied: six>=1.12.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (1.17.0) Requirement already satisfied: termcolor>=1.1.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (2.5.0) Requirement already satisfied: typingextensions>=3.6.6 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (4.12.2) Requirement already satisfied: wrapt>=1.11.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (1.17.2) Requirement already satisfied: grpcio=1.24.3 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (1.69.0) Requirement already satisfied: tensorboard=2.18 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (2.18.0) Requirement already satisfied: keras>=3.5.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (3.8.0) Requirement already satisfied: numpy=1.26.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (2.0.2) Requirement already satisfied: h5py>=3.11.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (3.12.1) Requirement already satisfied: mldtypes=0.4.0 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (0.4.1) ... Requirement already satisfied: mdurl~=0.1 in c:\users\avs mani\desktop\project\venv\lib\sitepackages (from markdownitpy>=2.2.0>rich>keras>=3.5.0>tensorflowintel==2.18.0>tensorflow) (0.1.2) Using cached tensorflow2.18.0cp310cp310win_amd64.whl (7.5 kB) Installing collected packages: tensorflow Successfully installed tensorflow2.18.0 Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... WARNING: Error parsing dependencies of tensorflowgpu: [Errno 2] No such file or directory: 'c:\\users\\avs mani\\desktop\\project\\venv\\lib\\sitepackages\\tensorflow_gpu2.10.1.distinfo\\METADATA'  ImportError                               Traceback (most recent call last) File c:\Users\AVS MANI\Desktop\Project\venv\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The operation completed successfully. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[1], line 5       1  Install tensorflow package       3 get_ipython().run_line_magic('pip', 'install tensorflow') > 5 import tensorflow as tf   type: ignore       7 from tensorflow.keras.models import Sequential  type: ignore       9 from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout  type: ignore File c:\Users\AVS MANI\Desktop\Project\venv\lib\sitepackages\tensorflow\__init__.py:40 ... Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...  Standalone code to reproduce the issue ```shell import tensorflow as tf   type: ignore from tensorflow.keras.models import Sequential  type: ignore from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout  type: ignore from tensorflow.keras.datasets import mnist  type: ignore ```  Relevant log output ```shell ```",2025-01-16T15:24:33Z,type:build/install,closed,0,2,https://github.com/tensorflow/tensorflow/issues/85064,"Please use \`\`\` to quote error messages to make them more readable. Also, please search previous issues, this issue has been discussed multiple times.",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],PR #20274: [ROCm] Emit allocas on function entry in lower_tensors.cc,PR CC(Error:Execution failed for task ':buildNativeBazel'. > A problem occurred starting process 'command '/usr/local/bin/bazel''): [ROCm] Emit allocas on function entry in lower_tensors.://github.com/openxla/xla/pull/20274 This fixes //tensorflow/compiler/tests:segment_reduction_ops_test_gpu Copybara import of the project:  6553059e8d5bf039ef526b2b904808b55051cab9 by Dragan Mladjenovic : [ROCm] Emit allocas on function entry in lower_tensors.(Error:Execution failed for task ':buildNativeBazel'. > A problem occurred starting process 'command '/usr/local/bin/bazel'') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20274 from ROCm:c64_atomics 6553059e8d5bf039ef526b2b904808b55051cab9,2025-01-16T15:23:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85063
copybara-service[bot],Add nsz fastmath flag to AddF ops in reducers.,Add nsz fastmath flag to AddF ops in reducers. This allows to fold the initial addition of 0.,2025-01-16T15:21:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85062
copybara-service[bot],[XLA:GPU] Simplify and refactor dot algorithm tests.,[XLA:GPU] Simplify and refactor dot algorithm tests. No changes in the business logic. Changes:  Rename tests to clarify their purpose.  Remove unneeded tuple type.  Use ValuesIn instead of Combile(Values(...)) for clarity.,2025-01-16T15:01:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85061
copybara-service[bot],[XLA:GPU] Unit test to ensure Cub Sort honors XLA's totalorder sort semantics in the presence of positive and negative NaNs and Zeros.,[XLA:GPU] Unit test to ensure Cub Sort honors XLA's totalorder sort semantics in the presence of positive and negative NaNs and Zeros.,2025-01-16T14:27:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85060
copybara-service[bot],[XLA:GPU] Deallocate TMA tensor maps after execution,[XLA:GPU] Deallocate TMA tensor maps after execution,2025-01-16T13:54:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85058
copybara-service[bot],[xla:cpu:benchmarks] Update microbenchmark path in CPU benchmark workflow.,[xla:cpu:benchmarks] Update microbenchmark path in CPU benchmark workflow. The benchmarks folder was moved from //xla/service/cpu to //xla/backends/cpu in https://github.com/openxla/xla/pull/21463,2025-01-16T13:52:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85057
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-16T12:37:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85047
copybara-service[bot],[XLA:GPU] Use inline assembly for vectorized AtomicRMWOp for Hopper.,[XLA:GPU] Use inline assembly for vectorized AtomicRMWOp for Hopper. Notable speedups:  b381831740_segment_sum                    1.02x         1.96  ms          1.92  ms  b381831740_segment_sum_fusion             1.06x         0.581 us          0.549 us  scatter_f32_1024_16_32_False_False_add    2.85x         13.1  us          4.6   us  scatter_f32_1024_16_32_False_True_add     2.86x         13.6  us          4.8   us,2025-01-16T12:28:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85046
copybara-service[bot],PR #21511: Fix bitcast transposes in layout normalization.,"PR CC(Hi i  run the flowing code): Fix bitcast transposes in layout normalization. Imported from GitHub PR https://github.com/openxla/xla/pull/21511 Currently, transposes that are bitcasts are converted to a bitcast that does not satisfy the invariants of the layout normalization pass. As far as I can tell, the special handling of bitcast transposes does nothing useful, so we can simply remove it. While we're here, we can stop emitting identity transposes. This fixes https://github.com/jaxml/jax/issues/25759. Copybara import of the project:  90aab3260c3f66384c89d552105755185a0635aa by Johannes Reifferscheid : Fix bitcast transposes in layout normalization. Currently, transposes that are bitcasts are converted to a bitcast that does not satisfy the invariants of the layout normalization pass. As far as I can tell, the special handling of bitcast transposes does nothing useful, so we can simply remove it. While we're here, we can stop emitting identity transposes. Merging this change closes CC(Hi i  run the flowing code) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21511 from jreiffers:main 90aab3260c3f66384c89d552105755185a0635aa",2025-01-16T11:52:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85045
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-16T11:14:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85038
copybara-service[bot],[XLA:GPU] Remove no-op flag `--xla_gpu_enable_triton_softmax_fusion`.,[XLA:GPU] Remove noop flag `xla_gpu_enable_triton_softmax_fusion`.,2025-01-16T11:08:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85037
copybara-service[bot],Avoids out of bounds access on 'begins_are_dynamic' and 'ends_are_dynamic'.,Avoids out of bounds access on 'begins_are_dynamic' and 'ends_are_dynamic'.,2025-01-16T10:50:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85036
user-redans,Issues on trying to compile TensorFlow C API for JETSON AGX Xavier using Bazel,"On my JETSON AGX Xavier, with: cuda: 11.4.315 cuDNN: 8.6.0 tensorrt: 8.5.2.2 jetpack: 5.1.3 python3 c “import tensorflow as tf; print(‘TensorFlow version:’, tf.version)” TensorFlow version: 2.11.0 I can’t compile tf with bazel ( bazel version: bazel 5.3.0 ) , error: ~/tensorflow$ bazel build config=opt config=cuda //tensorflow:libtensorflow.so Starting local Bazel server and connecting to it… WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Reading ‘startup’ options from /home/redans/tensorflow/.bazelrc: windows_enable_symlinks INFO: Options provided by the client: Inherited ‘common’ options: isatty=1 terminal_columns=237 INFO: Reading rc options for ‘build’ from /home/redans/tensorflow/.bazelrc: Inherited ‘common’ options: experimental_repo_remote_exec INFO: Reading rc options for ‘build’ from /home/redans/tensorflow/.bazelrc: ‘build’ options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Reading rc options for ‘build’ from /home/redans/tensorflow/.tf_configure.bazelrc: ‘build’ options: action_env PYTHON_BIN_PATH=/usr/bin/python3.9 action_env PYTHON_LIB_PATH=/usr/local/lib/python3.9/distpackages python_path=/usr/bin/python3.9 action_env PYTHONPATH=/usr/local/lib/python3.9/distpackages:/usr/local/lib/python3.9/distpackages:/home/redans/ros2_ws/install/yolov8_ros/lib/python3.9/sitepackages:/home/redans/ros2_ws/install/yolov8_msgs/lib/python3.9/sitepackages:/home/redans/ros2_ws/install/realsense2_camera_msgs/lib/python3.9/sitepackages:/opt/ros/humble/lib/python3.9/sitepackages action_env LD_LIBRARY_PATH=/usr/local/cuda11.4/lib64:/home/redans/local/lib/python3.8/distpackages/tensorflow:/home/redans/ros2_ws/install/yolov8_msgs/lib:/home/redans/ros2_ws/install/realsense2_camera/lib:/home/redans/ros2_ws/install/realsense2_camera_msgs/lib:/opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/aarch64linuxgnu:/opt/ros/humble/lib:/usr/local/cuda11.4/lib64: action_env GCC_HOST_COMPILER_PATH=/usr/bin/aarch64linuxgnugcc9 config=cuda INFO: Found applicable config definition build:short_logs in file /home/redans/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /home/redans/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda repo_env=HERMETIC_CUDA_VERSION=12.5.1 repo_env=HERMETIC_CUDNN_VERSION=9.3.0 //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.tf_configure.bazelrc: repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=7.2 INFO: Found applicable config definition build:opt in file /home/redans/tensorflow/.tf_configure.bazelrc: copt=Wnosigncompare host_copt=Wnosigncompare INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda repo_env=HERMETIC_CUDA_VERSION=12.5.1 repo_env=HERMETIC_CUDNN_VERSION=9.3.0 //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.tf_configure.bazelrc: repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=7.2 INFO: Found applicable config definition build:linux in file /home/redans/tensorflow/.bazelrc: host_copt=w copt=Wnoall copt=Wnoextra copt=Wnodeprecated copt=Wnodeprecateddeclarations copt=Wnoignoredattributes copt=Wnoarraybounds copt=Wunusedresult copt=Werror=unusedresult copt=Wswitch copt=Werror=switch define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=dynamic_kernels experimental_guard_against_concurrent_changes INFO: Found applicable config definition build:dynamic_kernels in file /home/redans/tensorflow/.bazelrc: define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS ERROR: Traceback (most recent call last): File “/home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/rules_python/python/versions.bzl”, line 734, column 32, in PLATFORMS = _generate_platforms() File “/home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/rules_python/python/versions.bzl”, line 723, column 15, in _generate_platforms }  dict INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda repo_env=HERMETIC_CUDA_VERSION=12.5.1 repo_env=HERMETIC_CUDNN_VERSION=9.3.0 //cuda:include_cuda_libs=true INFO: Found applicable config definition build:cuda in file /home/redans/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda repo_env=HERMETIC_CUDA_VERSION=12.5.1 repo_env=HERMETIC_CUDNN_VERSION=9.3.0 //cuda:include_cuda_libs=true WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. ERROR: //:enable_cuda :: Error loading option //:enable_cuda: error loading package ‘’: at /home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/local_tsl/third_party/py/python_init_repositories.bzl:3:6: at /home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/rules_python/python/repositories.bzl:24:6: at /home/redans/.cache/bazel/_bazel_redans/e3bb405f92452fe8b27464d0b3fdd1a7/external/rules_python/python/private/python_register_multi_toolchains.bzl:22:6: initialization of module ‘python/versions.bzl’ failed Do you have any suggestions?",2025-01-16T10:26:30Z,stat:awaiting response type:build/install stale subtype:bazel TF 2.11,closed,0,3,https://github.com/tensorflow/tensorflow/issues/85034,"redans, Tensorflow v2.11 is a pretty older version which is not actively supported. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.In this case, can you please try installing TensorFlow v2.11 which the respective configurations. https://www.tensorflow.org/install/source Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Integrate LLVM at llvm/llvm-project@c24ce324d563,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match c24ce324d563,2025-01-16T10:21:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85033
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-16T10:11:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85032
copybara-service[bot],[TF-TRT] Added an example to Xlogy documentation.,[TFTRT] Added an example to Xlogy documentation.,2025-01-16T09:46:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85030
copybara-service[bot],Added example to tf.math.floormod,Added example to tf.math.floormod,2025-01-16T08:55:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85028
copybara-service[bot],[XLA:GPU] Fix MacOS build,"[XLA:GPU] Fix MacOS build The standard format for the linker option is `rpath,`.   The `rpath=` is a GNU extension that MacOS linker doesn't recognize.",2025-01-16T08:08:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85027
rogerci91,tf.config.LogicalDeviceConfiguration() not able to set the memory limit but tf.config.experimental.VirtualDeviceConfiguration() is able to," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution Ubuntu 22.04.4 LTS  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 12.2  GPU model and memory RTX A5000 24Gb  Current behavior? I was trying to set the memory limit of 10Gb on the virtual device using tf.config.LogicalDeviceConfiguration(), but when I trained the model it was taking way more than 10Gb of memory. Eventually I was able to set the memory limit using tf.config.experimental.VirtualDeviceConfiguration() but I'm not sure why  Standalone code to reproduce the issue ```shell  this was not able to set the memory limit gpus = tf.config.list_physical_devices('GPU') if gpus:     try:         tf.config.set_visible_devices(gpus[0], 'GPU')         tf.config.set_logical_device_configuration(gpus[0],         [tf.config.LogicalDeviceConfiguration(memory_limit=10*1024)])         logical_gpus = tf.config.list_logical_devices('GPU')     except RuntimeError as e:         print(e)   this was able to set the memory limit gpus = tf.config.list_physical_devices('GPU') if gpus:     try:         tf.config.experimental.set_virtual_device_configuration(             gpus[0],             [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10*1024)]         )     except RuntimeError as e:         print(e) ```  Relevant log output ```shell ```",2025-01-16T07:37:49Z,stat:awaiting response type:bug stale 2.17,closed,0,5,https://github.com/tensorflow/tensorflow/issues/85026,"TensorFlow's tf.config.experimental.set_virtual_device_configuration is more focused on managing device resources at a lower level, such as limiting memory and managing the allocation between multiple virtual devices. This is why it worked for you while the tf.config.LogicalDeviceConfiguration did not.If you need to limit memory usage, you should use tf.config.experimental.set_virtual_device_configuration() to ensure the desired memory cap is respected.","Hi **** , Hi **** Thank you for your pointers. Apologies for the delay, and thank you for raising your concern here. As  mentioned, `tf.config.experimental.set_virtual_device_configuration()` works by managing resource allocation at a lower level, including limiting memory usage and handling allocation across multiple virtual devices. Therefore, `tf.config.LogicalDeviceConfiguration()` will not achieve the desired results in this scenario. It is recommended to use `tf.config.experimental.set_virtual_device_configuration()` for better outcomes. Here is the relevant TensorFlow documentation for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[xla:gpu] Cleanup AsNcclUniqueIds,[xla:gpu] Cleanup AsNcclUniqueIds,2025-01-16T07:04:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85025
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-16T06:40:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85023
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-16T06:31:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85022
copybara-service[bot],[IFRT] Add ifrt.bzl and pjrt_ifrt.bzl for package management.,[IFRT] Add ifrt.bzl and pjrt_ifrt.bzl for package management.,2025-01-16T04:00:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85020
copybara-service[bot],LiteRT: Update Model API,"LiteRT: Update Model API  Added access to Input / Output Tensor.  When a model is loaded with signature, update tensor names of I/O tensors   with signature I/O names.",2025-01-16T03:58:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85019
copybara-service[bot],[Shardy] Remove StableHLO -> MHLO -> StableHLO extra step as flatten-tuple and prepare-for-export passes are now migrated to StableHLO.,[Shardy] Remove StableHLO > MHLO > StableHLO extra step as flattentuple and prepareforexport passes are now migrated to StableHLO.,2025-01-16T03:44:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85018
copybara-service[bot],Adds a minimal but viable implementation of string arrays (with `numpy.dtypes.StringDType`) in JAX. Currently this only supports making of a string array by means of either `jax.numpy.asarray` or `jax.device_put` and reading it back with `jax.device_get`.,Adds a minimal but viable implementation of string arrays (with `numpy.dtypes.StringDType`) in JAX. Currently this only supports making of a string array by means of either `jax.numpy.asarray` or `jax.device_put` and reading it back with `jax.device_get`.,2025-01-16T03:41:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85017
copybara-service[bot],Add rudimentary support for device_layout version of BufferFromHostLiteral.,"Add rudimentary support for device_layout version of BufferFromHostLiteral. When `use_parameter_layout_on_device=true`, we call the `BufferFromHostLiteral` implementation that allows for a `device_layout` to be provided. This function was not implemented for `PjRtStreamExecutorClient` nor `TfrtCpuClient`.",2025-01-16T02:47:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85016
copybara-service[bot],cleanup of deprecated test methods,cleanup of deprecated test methods,2025-01-16T02:46:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85015
copybara-service[bot],Add kUsingGpuRocm property tag.,Add kUsingGpuRocm property tag. Tests can query this tag to determine whether they are running under ROCm.,2025-01-16T02:43:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85014
copybara-service[bot],Migrate convolution_test to always use PjRt for its test backend.,Migrate convolution_test to always use PjRt for its test backend.,2025-01-16T02:39:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85013
copybara-service[bot],cleanup of deprecated test methods,cleanup of deprecated test methods,2025-01-16T02:35:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85012
copybara-service[bot],[XLA:Python] Port pmap_lib.cc to use PyType_FromSpec() to construct a heap type object.,"[XLA:Python] Port pmap_lib.() to construct a heap type object. This updates pmap_lib to use a more modern API that exposes fewer CPython internals. The same change has already been made to our other directlyconstructed heap type objects (e.g., PjitFunction); this one was simply an outstanding cleanup. The proximate reason for the change is that under Python 3.14t (from CPython main) directly constructing a heap type breaks deferred reference counting because we cannot set the `unique_id` field on a heap type object, as PyType_FromSpec does here: https://github.com/python/cpython/blob/d05140f9f77d7dfc753dd1e5ac3a5962aaa03eff/Objects/typeobject.cL3946 Hence porting to the newer API is both a small cleanup and prevents a concurrency problem under a future Python version. While we are here, also use a managed weakrefs and managed dictionary under newer Python versions, as pjit already does.",2025-01-16T01:56:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85011
copybara-service[bot],Reverts 312fe365ee90ca039ba02c25e0de5b124aaad357,Reverts 312fe365ee90ca039ba02c25e0de5b124aaad357,2025-01-16T01:41:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85010
copybara-service[bot],Update the type for external buffer to store the name in the value rather than the key. Add getter and unit tests,Update the type for external buffer to store the name in the value rather than the key. Add getter and unit tests,2025-01-16T01:31:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85009
copybara-service[bot],Fix ASSERT_OK_AND_ASSIGN issue in ClientLibraryTestRunner.,Fix ASSERT_OK_AND_ASSIGN issue in ClientLibraryTestRunner.,2025-01-16T01:05:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85008
copybara-service[bot],Add basic DCN transfer library.,Add basic DCN transfer library.,2025-01-16T01:00:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85007
ThomasHughesIV,4080 RTX not detected on windows 11 24H2," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Windows 11 24h2  Mobile device _No response_  Python version 3.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version Cuda compilation tools, release 12.6, V12.6.85; cuDNN: Driver Version: 561.17   GPU model and memory 4080 RTX  Current behavior? physical_devices = tf.config.list_physical_devices() to return GPU information.  Instead, all I got was this: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]  Standalone code to reproduce the issue ```shell import tensorflow as tf  physical_devices = tf.config.list_physical_devices()  print(physical_devices) ```  Relevant log output ```shell ```",2025-01-16T00:58:02Z,type:build/install subtype:windows TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/85006,"I've tried different python versions, down to 3.10.  I cannot get tensor to detect my GPU so I can play around with building models and learn how to do this in vs2022",Are you satisfied with the resolution of your issue? Yes No,Resolving as duplicate and pointing to a ticket that has no resolution isn't a very good resolution.   At the very least I should get a ETA or some effort to explain this getting fixed.,That is discussed in the duplicated ticket.
copybara-service[bot],switch singleton LiteRT environment to passed in parameter.,switch singleton LiteRT environment to passed in parameter.,2025-01-16T00:51:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85005
copybara-service[bot],[XLA][hlo-opt] Refactor opt tooling to make it easier to add new options.,[XLA][hloopt] Refactor opt tooling to make it easier to add new options. Also adds a test for the listpasses feature.,2025-01-16T00:37:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85004
copybara-service[bot],[XLA:SchedulingAnnotations] Support having multiple computations containing the same scheduling annotation. Treat the same-id annotation groups from different computations independently.,[XLA:SchedulingAnnotations] Support having multiple computations containing the same scheduling annotation. Treat the sameid annotation groups from different computations independently.,2025-01-16T00:19:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/85003
copybara-service[bot],Integrate LLVM at llvm/llvm-project@34d50721dbc6,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 34d50721dbc6,2025-01-16T00:11:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85002
copybara-service[bot],Remove tf/compiler/mlir/lite/quantization/ir:QuantizationOpsTdFiles from tf/compiler/mlir/quantization/common/quantization_lib:quantization_td_files,Remove tf/compiler/mlir/lite/quantization/ir:QuantizationOpsTdFiles from tf/compiler/mlir/quantization/common/quantization_lib:quantization_td_files,2025-01-15T23:44:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85001
copybara-service[bot],Remove all lite deps from //third_party/tensorflow/compiler/mlir:passes,Remove all lite deps from //third_party/tensorflow/compiler/mlir:passes,2025-01-15T23:40:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/85000
copybara-service[bot],Remove //third_party/tensorflow/compiler/mlir/lite:tensorflow_lite from //third_party/tensorflow/compiler/mlir:passes,Remove //third_party/tensorflow/compiler/mlir/lite:tensorflow_lite from //third_party/tensorflow/compiler/mlir:passes,2025-01-15T23:39:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84999
copybara-service[bot],build: Add `release_version.h` to control the TF wheel filename and maintain reproducible wheel content and filename results. ,"build: Add `release_version.h` to control the TF wheel filename and maintain reproducible wheel content and filename results.  It enables ability to pass defines values to `//tensorflow/core/public:release_version`. Instead of having TF version in three places (`setup.py`, `version.h` and `tensorflow.bzl`), it should be located in one place only. Declaration of TF version in a new `tf_version.bzl` file will allow passing this value to build rules wrapping `setup.py` and `release_version.h`. Dependency on `//tensorflow/core/public:release_version` should be added if `TF_MAJOR_VERSION, TF_MINOR_VERSION, TF_PATCH_VERSION, TF_VERSION_SUFFIX` values are used in the code. Dependency on `//tensorflow/core/public:version` should be added if graphDef compatibility versions or checkpoint compatibility versions are used in the code. The next step would be to change cc_library release_version in the following way: ``` cc_library(     name = ""release_version"",     hdrs = [""release_version.h""],     defines = [         ""TF_MAJOR_VERSION={}"".format(MAJOR_VERSION),         ""TF_MINOR_VERSION={}"".format(MINOR_VERSION),         ""TF_PATCH_VERSION={}"".format(PATCH_VERSION),         ""TF_VERSION_SUFFIX={}"".format(TF_SEMANTIC_VERSION_SUFFIX),     ],     visibility = [""//visibility:public""], ) ``` The version chunks will be created from the value in `tf_version.bzl`. The version suffix will be created by a new repository rule, and it will be controlled by environment variables.",2025-01-15T23:38:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84998
copybara-service[bot],Remove //third_party/tensorflow/compiler/mlir/lite:lift_tflite_flex_ops from //third_party/tensorflow/compiler/mlir:passes,Remove //third_party/tensorflow/compiler/mlir/lite:lift_tflite_flex_ops from //third_party/tensorflow/compiler/mlir:passes,2025-01-15T23:38:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84997
copybara-service[bot],Remove //third_party/tensorflow/compiler/mlir/lite/quantization:quantization_passes from //third_party/tensorflow/compiler/mlir:passes,Remove //third_party/tensorflow/compiler/mlir/lite/quantization:quantization_passes from //third_party/tensorflow/compiler/mlir:passes,2025-01-15T23:32:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84996
copybara-service[bot],Remove //third_party/tensorflow/compiler/mlir/lite/quantization/tensorflow:tf_quantization_passes from //third_party/tensorflow/compiler/mlir:passes,Remove //third_party/tensorflow/compiler/mlir/lite/quantization/tensorflow:tf_quantization_passes from //third_party/tensorflow/compiler/mlir:passes,2025-01-15T23:30:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84995
copybara-service[bot],Remove //third_party/tensorflow/compiler/mlir/lite:tensorflow_lite_quantize from //third_party/tensorflow/compiler/mlir:passes,Remove //third_party/tensorflow/compiler/mlir/lite:tensorflow_lite_quantize from //third_party/tensorflow/compiler/mlir:passes,2025-01-15T23:25:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84994
copybara-service[bot],Remove //third_party/tensorflow/compiler/mlir/lite:tensorflow_lite_optimize from //third_party/tensorflow/compiler/mlir:passes,Remove //third_party/tensorflow/compiler/mlir/lite:tensorflow_lite_optimize from //third_party/tensorflow/compiler/mlir:passes,2025-01-15T23:23:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84993
copybara-service[bot],Remove //third_party/tensorflow/compiler/mlir/lite:tensorflow_lite_legalize_tf from //third_party/tensorflow/compiler/mlir:passes,Remove //third_party/tensorflow/compiler/mlir/lite:tensorflow_lite_legalize_tf from //third_party/tensorflow/compiler/mlir:passes,2025-01-15T23:18:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84992
copybara-service[bot],Remove //third_party/tensorflow/compiler/mlir/lite/core/c:tflite_common from //third_party/tensorflow/compiler/mlir/quantization/tensorflow/utils:tf_to_xla_attribute_utils,Remove //third_party/tensorflow/compiler/mlir/lite/core/c:tflite_common from //third_party/tensorflow/compiler/mlir/quantization/tensorflow/utils:tf_to_xla_attribute_utils,2025-01-15T23:11:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84991
copybara-service[bot],[xla:collectives] Always use ncclCommInitRankConfig for clique initialization,"[xla:collectives] Always use ncclCommInitRankConfig for clique initialization New NCCL initialization API breaks JAX OSS CI. Partially revert the original CL to keep API changes, but revert NCCL implementation.",2025-01-15T22:38:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84990
copybara-service[bot],"Google-internal change, should be no-op externally.","Googleinternal change, should be noop externally.",2025-01-15T22:32:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84989
copybara-service[bot],Remove from tf/compiler/mlir/lite:tensorflow_lite from tf/compiler/mlir/quantization/tensorflow:passes,Remove from tf/compiler/mlir/lite:tensorflow_lite from tf/compiler/mlir/quantization/tensorflow:passes,2025-01-15T22:29:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84988
copybara-service[bot],Disable warning about changed files in the tar operation.,Disable warning about changed files in the tar operation.,2025-01-15T22:11:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84987
copybara-service[bot],Use `use_parameter_layout_on_device` for HloPjRtTestBase.,Use `use_parameter_layout_on_device` for HloPjRtTestBase.,2025-01-15T22:06:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84986
copybara-service[bot],[xla:cpu] Use StringRef::contains in test as mangling rules might change the function name,"[xla:cpu] Use StringRef::contains in test as mangling rules might change the function name MacOS mangling changes the function name, use less strict contains check that must work on all platforms.",2025-01-15T20:47:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84985
copybara-service[bot],Don't statically dequantize input tensors,Don't statically dequantize input tensors,2025-01-15T20:10:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84984
copybara-service[bot],Allow only serializing external tensors,Allow only serializing external tensors MLDrift's program cache serialization can fail if MLDrift changes OR the GPU driver changes. It is not currently possible to detect GPU driver changes on all devices so serializing the program cache isn't always safe. Allow serializing external tensors without serializing the program cache.,2025-01-15T19:45:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84983
copybara-service[bot],Integrate LLVM at llvm/llvm-project@c24ce324d563,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match c24ce324d563,2025-01-15T19:38:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84982
copybara-service[bot],Reverts ad41d6fe7fda310fff0a94185e47edd525c7c21c,Reverts ad41d6fe7fda310fff0a94185e47edd525c7c21c,2025-01-15T19:32:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84981
copybara-service[bot],Proof-of-concept: Shard-local logging from JAX.,"Proofofconcept: Shardlocal logging from JAX. As discussed in https://github.com/jaxml/jax/issues/25842, since JAX's current logging mechanisms (e.g. `jax.debug.print`) are built on callbacks, logging a sharded array requires an expensive allgather operation. It would sometimes be useful to be able to separately print the local data shard on each worker. These parallel changes to XLA and JAX are meant as an experiment to demonstrate the custom GSPMD partitioning logic needed for this behavior. I'm currently using a new FFI handler that doesn't do anything, but this is sufficient to test the partitioning logic. It should be feasible to apply this same logic to a custom call encapsulating a callback.",2025-01-15T19:18:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84980
copybara-service[bot],[XLA:Python] Fix scoping of gil_release.,"[XLA:Python] Fix scoping of gil_release. We had the GIL released when constructing an nb::bytes object, which isn't allowed. In passing, also avoid an unnecessary string copy.",2025-01-15T19:06:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84979
copybara-service[bot],[XLA:CollectivePipeliner] Introduce all-gather as a formatting op.,[XLA:CollectivePipeliner] Introduce allgather as a formatting op.,2025-01-15T19:04:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84978
codinglover222,target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize fail to build," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.19  Custom code No  OS platform and distribution Linux Debian 6.1.1191 (20241122) x86_64 x86_64 x86_64 GNU/Linux  Mobile device _No response_  Python version 3.10  Bazel version bazel 6.5.0  GCC/compiler version gcc version 13.1.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior?  Background I follow the guidance here to run unit test locally. I use a google cloud compute engine. gcc version 13.1.0 I run the follow command to docker image tensorflow/build:2.19python3.10. docker run it v $PWD:/tmp w /tmp tensorflow/build:2.19python3.10 bash c ""bazel build experimental_action_cache_store_output_metadata disk_cache=~/.cache/bazel jobs=3 config=linux //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize""  The build failed and the error message is: /usr/include/c++/13/bits/unique_ptr.h:1070:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous 1070  ```  Relevant log output ```shell ```",2025-01-15T17:34:09Z,type:build/install comp:lite TF 2.18,closed,1,26,https://github.com/tensorflow/tensorflow/issues/84977, ,I also tested on new docker SIG image the the same error occurs. tensorflow/build:2.19python3.12 /usr/include/c++/11/bits/unique_ptr.h:962:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous   962 |     { return unique_ptr(new _Tp(std::forward(__args)...)); },"We are also seeing the same error with gcc 12, python 3.11 and tensorflow master branch. Could someone please provide any pointers on this issue?","Hi,   I apologize for the delayed response, thank you for bringing this issue to our attention I'll try to replicate the same behavior from my end and will update you.  Thank you for your cooperation and patience.","Hi,   I apologize for the delayed response, I've been attempting to reproduce the behavior you described using the provided command within the issue template.  Unfortunately, I've encountered a different problem the build process is freezing on my end and preventing completion.  This is occurring on a GCP VM instance.  I've included the output log below for your review.  To confirm, Did you face similar issue from your end ? If possible could you please help us with exact steps which you followed before encountering the reported error in the issue template which will help us to investigate this issue further from our end ? ``` (base) gaikwadrahuln1standard1gput4x1tfliteubuntu22:~/tensorflow$ sudo docker run it v $PWD:/tmp w /tmp tensorflow/build:2.19python3.10 bash c ""bazel build experimental_action_cache_store_output_metadata disk_cache=~/.cache/bazel jobs=3 config=linux //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize"" 2025/02/03 13:17:27 Downloading https://releases.bazel.build/6.5.0/release/bazel6.5.0linuxx86_64... Extracting Bazel installation... Starting local Bazel server and connecting to it... WARNING: The following configs were expanded more than once: [dynamic_kernels]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Invocation ID: c5c3346d1cbd4a23aa602aa5d8cd318a INFO: Reading 'startup' options from /tmp/.bazelrc: windows_enable_symlinks INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=190 INFO: Reading rc options for 'build' from /tmp/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:   'build' options: action_env=DOCKER_CACHEBUSTER=1738455273945121626 host_action_env=DOCKER_HOST_CACHEBUSTER=1738455274013979537 INFO: Reading rc options for 'build' from /tmp/.bazelrc:   'build' options: define framework_shared_object=true define tsl_protobuf_header_only=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive features=force_no_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 experimental_cc_shared_library experimental_link_static_libraries_once=false incompatible_enforce_config_setting_visibility INFO: Found applicable config definition build:short_logs in file /tmp/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /tmp/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:linux in file /tmp/.bazelrc: host_copt=w copt=Wnoall copt=Wnoextra copt=Wnodeprecated copt=Wnodeprecateddeclarations copt=Wnoignoredattributes copt=Wnoarraybounds copt=Wunusedresult copt=Werror=unusedresult copt=Wswitch copt=Werror=switch define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=dynamic_kernels experimental_guard_against_concurrent_changes INFO: Found applicable config definition build:dynamic_kernels in file /tmp/.bazelrc: define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS INFO: Found applicable config definition build:linux in file /tmp/.bazelrc: host_copt=w copt=Wnoall copt=Wnoextra copt=Wnodeprecated copt=Wnodeprecateddeclarations copt=Wnoignoredattributes copt=Wnoarraybounds copt=Wunusedresult copt=Werror=unusedresult copt=Wswitch copt=Werror=switch define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++17 host_cxxopt=std=c++17 config=dynamic_kernels experimental_guard_against_concurrent_changes INFO: Found applicable config definition build:dynamic_kernels in file /tmp/.bazelrc: define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS DEBUG: /root/.cache/bazel/_bazel_root/d42b9c57d24cf5db3bd8d332dc35437f/external/local_tsl/third_party/py/python_repo.bzl:154:14:  HERMETIC_PYTHON_VERSION variable was not set correctly, using default version. Python 3.10 will be used. To select Python version, either set HERMETIC_PYTHON_VERSION env variable in your shell:   export HERMETIC_PYTHON_VERSION=3.12 OR pass it as an argument to bazel command directly or inside your .bazelrc file:   repo_env=HERMETIC_PYTHON_VERSION=3.12 DEBUG: /root/.cache/bazel/_bazel_root/d42b9c57d24cf5db3bd8d332dc35437f/external/local_tsl/third_party/py/python_repo.bzl:87:10:  ============================= Hermetic Python configuration: Version: ""3.10"" Kind: """" Interpreter: ""default"" (provided by rules_python) Requirements_lock label: ""//:requirements_lock_3_10.txt"" ===================================== WARNING: The following configs were expanded more than once: [dynamic_kernels]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Analyzed target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize (185 packages loaded, 8165 targets configured). INFO: Found 1 target... [2,289 / 2,432] 6 actions, 4 running     Compiling mlir/lib/Analysis/FlatLinearValueConstraints.cpp; 64s remotecache     Compiling tensorflow/compiler/mlir/tensorflow/ir/tf_ops_a_m.cc; 40s local, remotecache     Compiling mlir/lib/Dialect/Affine/IR/AffineValueMap.cpp; 40s remotecache ``` Thank you for your cooperation and patience.",Hi    Did you post the entire error log? I don't see any error in above log you shared.,"Hi,   I have provided the complete build log above. However, the build process stalled at step [2,289 / 2,432] and did not complete.  After waiting for over 30 minutes, there was no further progress.  Could you please share the precise steps you took prior to encountering this issue?  Your assistance in replicating the same issue from our end would be greatly appreciated. Thank you.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Hi  , I will share the steps soon.","Hi , We are encountering the same issue while building TensorFlow on a ppc64le machine. I have attached the build script and the patch we applied. Please review them and let me know if you need any additional details. fixubi.patch Steps_To_Build_TF.txt",Same issue here on x86_64 and aarch64 with GCC 13.2.0: * x86_64 env * x86_64 log * aarch64 env * aarch64 log,We are also encountering the same issue trying to build TF at tipoftree for x86_64 (albeit using hermetic Clang).,"Hi,   Please take a look into this issue. Thank you","Hey All, , , I'm trying to understand the workflow we are building for here. I.e. what shared/static libraries and/or packages do you need to do what you want? I have tested TF 2.19 and it builds fine. We should be building against the LiteRT instructions moving forward for TFLite Workflows. Of course maybe there is a shared/static library and/or package that is not currently available that way  in which case please let me know so we can move forward on this.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"I don't think this issue has anything to do with LiteRT, I encounter this bug anytime I try to build TensorFlow with GCC.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Issue is not stale, it's just being ignored by the TF developers. ",", what's your workflow which requires this target? i.e. what are you trying to do that builds this target? You can also use https://github.com/googleaiedge/aiedgetorch to quantize pytorch models."," GCC is complaining that an inherited constructor is creating an ambiguity. I do not know whether this is a GCC bug, or if GCC is correct that the call is ambiguous. But you can fix the problem by getting rid of the inherited constructor. The following patch allows my GCC 13 build to finish: ``` diff git a/tensorflow/compiler/mlir/lite/transforms/default_quant_params./tensorflow/compiler/mlir/lite/transforms/default_quant_params...800a3fb0603 100644  a/tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc +++ b/tensorflow/compiler/mlir/lite/transforms/default_quant_params.,7 +54,9 @@ namespace {  class DefaultQuantParamsPass      : public impl::DefaultQuantParamsPassBase {   public:   using DefaultQuantParamsPassBase::DefaultQuantParamsPassBase; +  DefaultQuantParamsPass() +  { +  }    explicit DefaultQuantParamsPass(double default_min, double default_max,                                    bool is_signed) { ```"," my target is `//tensorflow/tools/pip_package:wheel`. I have no interest in quantized models.  thanks for the patch! Let me try that and get back to you. I'm guessing this isn't something we can merge into main, I wonder if there is a better way to unambiguously inherit the constructor."," I think this is fine to merge; it's a trivial change that breaks nothing. I will submit a PR. Please do let me know if this fixes the issue for you. The title of this issue should probably be ""TensorFlow build failure with GCC 13"". It has nothing to do with the build target. Also, GCC might be correct that this is an actual ambiguity; I am not sure. I will see if I can create a small reproducer.",Created a small example and asked on StackOverflow (https://stackoverflow.com/q/79553477/),"Sounds like this is actually a tensorflow issue then , can you please reroute appropriately? Apologies for the confusion.",I also opened https://github.com/llvm/llvmproject/issues/134287 OK time for me to get back to work.,Are you satisfied with the resolution of your issue? Yes No
chandu464,Failed to load native TensorFlow Lite methods,"Hi, I'm trying to use tensorflow lite version 2.15.0 to run my tflite model and I'm getting an error when initializing the interpreter. I'm adding the tensor flow libraries in the gradle file  `implementation('org.tensorflow:tensorflowlite') { version { strictly(""2.15.0"") } }` Code:  ``` isLibraryLoaded = false private fun initInterpreter(): Interpreter? {         val tfliteOptions = Interpreter.Options()         tfliteOptions.setNumThreads(2)         if (!isLibraryLoaded) {             System.loadLibrary(""tensorflowlite_jni"")             ARLog.d(""OmniSenseMLDepthImageProcessor"",""Tensor flow lite Library load successful"")             isLibraryLoaded = true         }         return try {             Log.d(""InterpreterDelegate"", ""Thread id getInterpreter ==  ${Thread.currentThread().name}"")             val interpreter = org.tensorflow.lite.Interpreter(loadModelFile(resolveModelFilePath()), tfliteOptions)             Log.d(""InterpreterDelegate"", ""Interpreter initialized successfully"")             return interpreter         } catch (e: Exception) {             Log.e(""InterpreterDelegate"", ""Error: Could not initialize $tfLiteModel interpreter!: ${e.message}"")             e.printStackTrace()             null         }     } ``` TFLite version: 2.15.0 Device: Samsung S20 Error:  ```  E  FATAL EXCEPTION: pool140thread1                                                                                                     Process: com.amazon.mShop.android.shopping, PID: 14755                                                                                                     java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():                                                                                                       java.lang.UnsatisfiedLinkError: dlopen failed: library ""libtensorflowlite_jni_gms_client.so"" not found                                                                                                     	at org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:137)                                                                                                     	at org.tensorflow.lite.NativeInterpreterWrapper.(NativeInterpreterWrapper.java:62)                                                                                                     	at org.tensorflow.lite.NativeInterpreterWrapperExperimental.(NativeInterpreterWrapperExperimental.java:36)                                                                                                     	at org.tensorflow.lite.Interpreter.(Interpreter.java:232)                                                                                                     	at com.a9.fez.tflite.TFLiteInterpreterDelegate.initInterpreter(TFLiteInterpreterDelegate.kt:50)                                                                                                     	at com.a9.fez.tflite.TFLiteInterpreterDelegate.getValue(TFLiteInterpreterDelegate.kt:29)  Caused by: java.lang.UnsatisfiedLinkError: No implementation found for void org.tensorflow.lite.TensorFlowLite.nativeDoNothing() (tried Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing and Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing__)  is the library loaded, e.g. System.loadLibrary?                                                                                                     	at org.tensorflow.lite.TensorFlowLite.nativeDoNothing(Native Method)                                                                                                     	at org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:132)                                                                                                     	... 14 more ```",2025-01-15T17:27:48Z,stat:awaiting response type:support stale comp:lite TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84976,"Hi,   I apologize for the delayed response, if possible could you please help us with your Github repo along with TFLite model and complete steps to replicate the same behavior from our end to investigate this issue further from our end ? Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
Sqvid,build(aarch64): Update to oneDNN-3.7 + ACL-24.12,"Bumps the aarch64compatible oneDNN version to 3.7 and the ACL version to 24.12. This brings better performance, improved memory management, and numerous bug fixes over the previous, long outofdate versions. cc:  ",2025-01-15T17:08:03Z,awaiting review ready to pull size:XL,closed,1,34,https://github.com/tensorflow/tensorflow/issues/84975,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.", Almost all the patches could be removed because they have been upstreamed. One exception amongst the removed patches is this change to enable blocked sbgemm formats in oneDNN and the corresponding changes in ACL. Could you let me know if these are still needed and whether they can be rebased? The old patchfiles are not compatible with the current source. Thanks., ,"Hi , Can you please sign CLA , thank you !!"," Thanks for the reminder. Yes I have got in touch with my company's CLA Point of Contact, and will hopefully be registered shortly.", The CLA is now signed. Thank you for your patience.,"Hi , Can you please resolve the conflicts. Thank you !","Hi , Can you please resolve the conflicts? Thank you !","> Hi , Can you please resolve the conflicts? Thank you ! Done. Thank you!", Sorry for the delay. I'm working on merging the PR., Thanks! That's okay I needed to patch `third_party/xla/xla/tsl/util/onednn_threadpool.h` anyway. (i did this in the latest push), Either way could we hold out on merging till the 3.7 release is finalised? I don't want to merge in the rc if we can avoid it," Sounds good. Let's wait until the 3.7 release is finalized then. In the meanwhile, I think you can split the changes to remove the OpenMP build to another PR, and we can merge it without waiting.", Apologies for the delay in my update. I have updated the PR with the the final oneDNN release., I'd prefer to keep these OMP changes here since they are buildsystem related. I will be making a followup PR after this one is merged to removed deadcode (paths guarded by `ifdef AARCH64 && OMP`) from source files.,  Please let me know if you'd like to see any further changes or if you're happy to merge, Thank you for the approval. I see the import/copybara job is failing. Is this something I need to address on my end? ,  During some endtoend testing of tensorflow against the latest versions it seems there is some unexpected behaviour on some BF16 models. I'd advise **against** merging this just now. I will update the PR once we know more. Thank you. I am also happy to convert this into a draft for now if that helps, Converting to draft sounds good. Thank you very much!, The aformentioned issue has been fixed in oneDNN main and included here as a patchfile. Please let me know if everything seems to be in order.,"I think since the time this PR was first opened, the `third_party/` paths have shifted around a bit. To match what seems to be the current state of `master`, I have put patchfiles in the following directories: `third_party/mkl_dnn/` (picked up by `tensorflow/workspace2.bzl`) `third_party/compute_library/` (picked up by `tensorflow/workspace2.bzl`) `third_party/xla/third_party/mkl_dnn/` (picked up by `third_party/xla/tsl_workspace2.bzl`) `third_party/xla/third_party/compute_library/` (picked up by`third_party/xla/tsl_workspace2.bzl`) **deleted:** `third_party/xla/third_party/tsl/third_party/` I would appreciate it if you could sanity check these paths since the duplicated directories can be rather confusing. Thanks.",>Thank you for the updates. Let's try. Looks like it's passed the internal test suite? Would you like any further changes or are you happy with the patch asis?,"It still needs the internal approval (which I gave now), so hopefully all should merge soon.",It seems there's some XLA failure right now: https://github.com/openxla/xla/actions/runs/14225255197/job/39863144414,"  Thanks for the approval. >It seems there's some XLA failure right now: https://github.com/openxla/xla/actions/runs/14225255197/job/39863144414 Looking at the failing log it seems like it is failing because it is trying to apply a patchfile that I have deleted in this patchset. ``` Error in patch: Error applying patch /home/runner/work/xla/xla/third_party/mkl_dnn/onednn_acl_threadcap.patch:  Cannot find patch file: /home/runner/work/xla/xla/third_party/mkl_dnn/onednn_acl_threadcap.patch ERROR: Evaluation of query ""deps(//xla/...)"" failed: preloading transitive closure failed: no such package '@//': Error applying patch /home/runner/work/xla/xla/third_party/mkl_dnn/ onednn_acl_threadcap.patch: Cannot find patch file: /home/runner/work/xla/xla/third_party/mkl_dnn/onednn_acl_threadcap.patch ``` But that has been deleted here. Could you advise me on how I can delete this file in TensorFlow without breaking the XLA repo? Thanks", can you advise on this? I thought Copybara would be able to propagate the deletion to the XLA repo too.,>  can you advise on this? I thought Copybara would be able to propagate the deletion to the XLA repo too. Unfortunately the xla workspace and the tsl modifications need to be handled by patching the internal CL to include them.   is doing more work to untangle these but as it stands it does require the manual intervention , Thanks for having a look. Is there any further required from my end? Or can this be resolved and merged by the TensorFlow/XLA dev teams?,Requesting a reapproval () after manual mergeconflict resolution,"  Could you advise on how we can proceed with the PR? Unfortunately the CI failure caused a delay during which a mergeconflict arose. I have fixed it but it has reset the approval. If the XLA CI failure happens again, I fear that this cycle might just repeat. Is there any way that the next approval could be followed by manual intervention on the failing XLA job? Apologies for the inconvenience, and thank you."
copybara-service[bot],[pallas:triton] Added a PjRt extension for compiling Triton IR to PTX,"[pallas:triton] Added a PjRt extension for compiling Triton IR to PTX This change is necessary to improve the stability and backward compatibility of Pallas Triton calls, because unlike PTX, the Triton dialect has no stability guarantees and does change in practice. The implementation only supports CUDA at the moment. More work is needed to support ROCm.",2025-01-15T15:58:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84974
copybara-service[bot],[xla:cpu] scatter_benchmark: remove unique_indices=true,"[xla:cpu] scatter_benchmark: remove unique_indices=true It's not clear to me that the update slices cannot overlap, regardless of parameter values. (If they were simple scatters I'd easily reason about this, but alas, they're not). Remove the flags then. Reverts 312fe365ee90ca039ba02c25e0de5b124aaad357",2025-01-15T15:57:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84973
copybara-service[bot],Pass in pointer instead of Shape object to InstructionValueSet constructor,Pass in pointer instead of Shape object to InstructionValueSet constructor Avoids the copying of the shape object.,2025-01-15T15:24:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84972
copybara-service[bot],[XLA:GPU] Make LayoutAssignment aware of Cub Radix Sort custom calls.,[XLA:GPU] Make LayoutAssignment aware of Cub Radix Sort custom calls. Ensure that all operands and outputs have the same layout.,2025-01-15T15:23:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84971
copybara-service[bot],[XLA:GPU] Enable Triton MLIR int4 -> int8 rewrite,[XLA:GPU] Enable Triton MLIR int4 > int8 rewrite Roll forward the flag flip once more,2025-01-15T15:22:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84970
arianehlima,"How can I compile TensorFlowLite for Swift without Bitcode?""","Hello! I would like to use TensorFlowLite Swift without Bitcode, as Apple has discontinued the use of Bitcode. I am using version 2.17.0 available on CocoaPods, but the binaries already come with Bitcode. How can I resolve this? And does TensorFlowLite Swift have a version available that does not require Rosetta to run on ARM architectures? Thank you",2025-01-15T15:09:18Z,stat:awaiting response type:support stale comp:lite iOS,closed,0,6,https://github.com/tensorflow/tensorflow/issues/84969,"Hi,   I apologize for the delay in response, I believe you're following this official documentation TensorFlow Lite for Swift and as far I know you can build the `TensorFlowLite` Swift library target using below command by disabling the bitcode  ``` bazel build tensorflow/lite/swift:TensorFlowLite \     define=ios_arm64=true \     define=enable_bitcode=false ``` You can also do manually enable/disable bitcode support go to **Project > Build Settings > search for 'bitcode' in the searchfield > set to YES/NO.** May I know which Xcode version you're using ? Please make sure you're using the latest version of TensorFlow Lite from CocoaPods or by building from source also ensure your Xcode project is correctly configured for ARM architecture in your project's build settings. If I have missed something here please let me know. Thank you for your cooperation and patience.",Hi  ! Thank for your response! I am using Xcode 15.4 and TensorflowLiteSwift 2.14.0 from CocoaPods.,"Hi,   Thank you for the details, if possible could you please give it try with `TensorflowLiteSwift 2.17.0` and let us know is it working as expected or not ? If issue still persists please let us know error log for investigate this issue further from our end. Thank you for your cooperation and understanding.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[XLA:GPU] remove GpuElementalIrEmitter,[XLA:GPU] remove GpuElementalIrEmitter By making ElementalIrEmmiter nonabstract we can completely drop GpuElementalIrEmitter,2025-01-15T14:52:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84968
copybara-service[bot],Cherry-pick https://github.com/triton-lang/triton/pull/5528 which reverts multiple PRs causing issues both in upstream triton-lang & internally.,Cherrypick https://github.com/tritonlang/triton/pull/5528 which reverts multiple PRs causing issues both in upstream tritonlang & internally.,2025-01-15T14:52:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84967
MichaelHudgins,Bump ml-dtypes upper bound,ml_dtypes upper bound is too restrictive and is causing conflicts when installed with other ML ecosystem components.,2025-01-15T14:51:18Z,TF 2.18,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84966
copybara-service[bot],PR #21474: [ROCm] Fix gpu_index_test,"PR CC(Fix incorrect check logic for state_forget_sig gate): [ROCm] Fix gpu_index_test Imported from GitHub PR https://github.com/openxla/xla/pull/21474 For AMDGPUs the expected IR in `CompatibleUseLinearIndexWithReshapeAndBroadcast` test is: ``` %urem = urem i32 %4, 14 %8 = zext nneg i32 %urem to i64 %9 = getelementptr inbounds nuw float, ptr addrspace(1) %.global1, i64 %8 ``` Changed the pattern to reflect that. Copybara import of the project:  2491b60802f5bd8951551eb6f66bc47d0d5a720c by Milica Makevic : [ROCm] Fix gpu_index_test Merging this change closes CC(Fix incorrect check logic for state_forget_sig gate) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21474 from ROCm:ci_fix_gpu_index_test 2491b60802f5bd8951551eb6f66bc47d0d5a720c",2025-01-15T14:42:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84965
copybara-service[bot],[xla:cpu] scatter_benchmark: add SimpleScatterReduceF32_R3,"[xla:cpu] scatter_benchmark: add SimpleScatterReduceF32_R3 Add a ""simple"" scatter benchmark with a reduce combiner.",2025-01-15T14:40:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84964
user-redans,TensorRT ( C++ ) inference strange behavior on Jetson AGX Xavier,"I developed 2 distinct models, for 2 use cases, to analyzed some vibration patterns: one of them when system is turn on and second when system is shut down (so there are no any vibration detected ) The entire training process uses TensorFlow 2.7.0 (an auto encoder in python) to create .h5 models, which are converted to .onnx models files and then to .engine files for the Jetson platform (Jetson AGX Xavier CUDA ). Jetson AGX Xavier specs: cuda: 11.4.315 cuDNN: 8.6.0 tensorRT: 8.5.2.2 jetpack: 5.1.3 python3 c ""import tensorflow as tf; print('TensorFlow version:', tf.__version__)"" TensorFlow version: 2.11.0 Auto encoder trainig script in python ( sample) : ``` input_img = tf.keras.layers.Input(shape=(2000, lines))    Encoder   x = tf.keras.layers.Conv1D(12, 128, padding='same')(input_img)   x = tf.keras.layers.MaxPooling1D(4)(x)   Downsample: 2000 > 500   x = tf.keras.layers.Conv1D(12, 64, padding='same')(x)   x = tf.keras.layers.MaxPooling1D(2)(x)   Downsample: 500 > 250   x = tf.keras.layers.Conv1D(12, 16, padding='same')(x)   x = tf.keras.layers.MaxPooling1D(2)(x)   Downsample: 250 > 125    Bottleneck   x = tf.keras.layers.Flatten()(x)   x = tf.keras.layers.Dense(self.__config['MODEL']['ENCODED_STATE_SIZE'])(x)    Decoder   x = tf.keras.layers.Dense(125 * 12)(x)   Expand to match last encoder feature size   x = tf.keras.layers.Reshape((125, 12))(x)   x = tf.keras.layers.UpSampling1D(2)(x)   Upsample: 125 > 250   x = tf.keras.layers.Conv1D(12, 16, padding='same')(x)   x = tf.keras.layers.UpSampling1D(2)(x)   Upsample: 250 > 500   x = tf.keras.layers.Conv1D(12, 64, padding='same')(x)   x = tf.keras.layers.UpSampling1D(4)(x)   Upsample: 500 > 2000   x = tf.keras.layers.Conv1D(lines, 128, padding='same')(x)   Correct Final Layer    Model definition   self.__model = tf.keras.models.Model(input_img, x) ``` It doesn't matter which model I use, inference result values are the SAME, exactly the same values, as if the neural network learned nothing...... You can see below 2 comparative charts with the inference values !Image Don't assume that the data might be corrupted, I have collected enough data to train for both cases and I've checked their validity The confusing part is that inference works in python, using TensorFlow 2.7.0 with GPU, an Ubuntu Focal x86_64...I mean, I saw different values between 2 charts In Jetson I've made a py script to convert .h5 model file into .onnx and then into .engine format: ``` import tf2onnx import tensorflow as tf import argparse import subprocess def convert_h5_to_onnx(h5_model_path, onnx_model_path):     print(""Converting .h5 model to ONNX..."")     model = tf.keras.models.load_model(h5_model_path)     model_proto, _ = tf2onnx.convert.from_keras(model, opset=13)     with open(onnx_model_path, ""wb"") as f:         f.write(model_proto.SerializeToString())     print(f""ONNX model saved at {onnx_model_path}"") def convert_onnx_to_trt(onnx_model_path, engine_model_path, trt_precision_mode):     print(""Converting ONNX model to TensorRT Engine..."")     fp_precision_flag = 'fp16' if trt_precision_mode.upper() == 'FP16' else ''     trtexec_path = ""/usr/src/tensorrt/bin/trtexec""     command = f""{trtexec_path} onnx={onnx_model_path} saveEngine={engine_model_path} {fp_precision_flag}""     process = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)     if process.returncode != 0:         print(f""Error in converting to TensorRT engine:\n{process.stderr.decode('utf8')}"")     else:         print(f""TensorRT engine saved at {engine_model_path}"")  Main if __name__ == ""__main__"":     parser = argparse.ArgumentParser(description=""Convert a .h5 model to ONNX and TensorRT engine format"")     parser.add_argument(""h5_model_path"", type=str, required=True, help=""Path to the .h5 model file"")     parser.add_argument(""onnx_model_path"", type=str, required=True, help=""Path to save the converted ONNX model"")     parser.add_argument(""engine_model_path"", type=str, required=True, help=""Path to save the converted TensorRT engine"")     parser.add_argument(""trt_precision_mode"", type=str, choices=['FP32', 'FP16'], default=""FP16"", help=""Precision mode for TensorRT engine (FP32 or FP16)"")     args = parser.parse_args()     convert_h5_to_onnx(args.h5_model_path, args.onnx_model_path)     convert_onnx_to_trt(args.onnx_model_path, args.engine_model_path, args.trt_precision_mode) ``` ""RunInference"" is my C/C++ inference function using TensorRT ( as input data , I used FFT s  of the raw values ) ``` void RunInference(ICudaEngine* engine, IExecutionContext* context, int input_index, int output_index, kiss_fft_cpx* x_fft, kiss_fft_cpx* y_fft, kiss_fft_cpx* z_fft, float* predicted_output, int g_code, const char* clientName) {     int batchSize = 1;     int input_size = batchSize * 2000 * 3 * sizeof(float);  // [1, 2000, 3]     int output_size = batchSize * 3 * sizeof(float);        // [1, 3]     // Prepare normalized input data and set DC component to zero     float input_data[2000 * 3];     const int MN = 4000;     for (int i = 0; i enqueueV2(buffers, stream, nullptr);     cudaStreamSynchronize(stream);     // Copy the output data from GPU to CPU     cudaMemcpy(predicted_output, buffers[output_index], output_size, cudaMemcpyDeviceToHost);     // Free GPU memory     cudaFree(buffers[input_index]);     cudaFree(buffers[output_index]);     cudaStreamDestroy(stream); } ``` This is how I load one model in app and how I call inference function: ``` 	IRuntime* runtime = createInferRuntime(gLogger); 	if (!runtime) { 		write_log(LOG_ERROR, ""client_handler: Failed to create runtime for client %s"", client.ClientName); 		return (void*)1; 	} 	std::vector engine_data = loadEngine(client.ModelPath, client.ClientName);	           ICudaEngine* engine = runtime>deserializeCudaEngine(engine_data.data(), engine_data.size(), nullptr);           if (!engine) {           	write_log(LOG_ERROR, ""client_handler: Failed to create engine for thread %s"", client.ClientName);           	return (void*)1;           }             IExecutionContext* context = engine>createExecutionContext();             if (!context) {             write_log(LOG_ERROR, ""client_handler: Failed to create execution context for thread %s"", client.ClientName);							             engine>destroy();             return (void*)1;             }             int input_index = engine>getBindingIndex(client.ModelInputBindingName) ;//get from config file             int output_index = engine>getBindingIndex(client.ModelOutputBindingName); //get from config file       	   RunInference(engine, context, input_index, output_index, x_fft, y_fft, z_fft, predicted_output, client.G_code, client.ClientName);       	// Synchronize the GPU to ensure all operations are completed       	cudaDeviceSynchronize();       	// Check for CUDA errors after synchronization       	cudaError_t err = cudaGetLastError();       	if (err != cudaSuccess) {       		write_log(LOG_ERROR, ""CUDA error after synchronization in thread '%s': %s"", client.ClientName, cudaGetErrorString(err));       	} else {       		write_log(LOG_INFO, ""GPU synchronized successfully for thread '%s'"", client.ClientName);       	}       	 context>destroy();       	 engine>destroy();          runtime>destroy(); ``` I want to point out that the vibrations are detected by the application, but I don’t understand why the range of values doesn’t change depending on the trained model from the two scenarios. I suspect the problem might be with the model conversion or the inference process / function in TensorRT using C/C++. Do you have any suggestions?",2025-01-15T14:39:02Z,stat:awaiting response TF 2.11,closed,0,2,https://github.com/tensorflow/tensorflow/issues/84963,"Hi **redans** , Apologies for the delay, and thank you for raising your concern here. I reproduced the code you shared but encountered a different error. Could you please share the Colab gist with all the dependencies so I can analyze it further? Also, it seems there might be some compatibility issues, so please check those for a smoother run. I have attached the documentation for your reference. Thank you!","I solved it! In py traning model code, I 've changed FFT calculation, I've added kissFFT lib, like in CPP inference code."
tboby,Windows libtensorflow size increased 4x with 2.17," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.17+  Custom code No  OS platform and distribution Windows x86_64  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Libtensorflow.dll for windows was 238MB with 2.16.2, is 909MB with 2.17.0, and is 931MB with 2.18.0. At the same time the linux versions haven't changed significantly. I would not expect the tensorflow binaries on windows to be over twice the size of linux, nor for the size to increase so much without any notice in the release notes. I've tried to look through the bazel configs but there's nothing obvious to me which would cause this!  2.16.2  versions_2.16.2_libtensorflowcpuwindowsx86_64 238MB versions_2.16.2_libtensorflowcpulinuxx86_64 422MB  2.17.0  versions_2.17.0_libtensorflowcpuwindowsx86_64 909MB versions_2.17.0_libtensorflowcpulinuxx86_64 412MB  2.18.0 versions_2.18.0_libtensorflowcpuwindowsx86_64 931MB  Standalone code to reproduce the issue ```shell Libtensorflow is provided by google via the GCS buckets documented here https://www.tensorflow.org/install/lang_c ```  Relevant log output ```shell ```",2025-01-15T14:25:26Z,type:build/install subtype:windows 2.17,open,0,0,https://github.com/tensorflow/tensorflow/issues/84962
copybara-service[bot],[xla] scatter_simplifier: document simple scatter semantics + add example tests,"[xla] scatter_simplifier: document simple scatter semantics + add example tests Make the documentation selfcontained so that one can understand ""simple scatter"" without having to first understand the semantics of ""general scatter"".",2025-01-15T13:55:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84961
copybara-service[bot],Integrate LLVM at llvm/llvm-project@c24ce324d563,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match c24ce324d563,2025-01-15T13:41:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84960
copybara-service[bot],Integrate Triton up to [632bfc3](https://github.com/openai/triton/commits/632bfc342d3a7d63ce8b21209355139ee070d392),Integrate Triton up to 632bfc3 Reverts 1893c104926a6559d8f482e4384f4a64e55501a0,2025-01-15T13:40:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84959
copybara-service[bot],Move emitter_loc_op_builder to xla/codegen directory.,"Move emitter_loc_op_builder to xla/codegen directory. It is currently used only for triton emitters, but it can be used by other emitters as well.",2025-01-15T13:36:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84958
copybara-service[bot],Update the link to the fusion emitter tests.,Update the link to the fusion emitter tests.,2025-01-15T13:13:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84957
copybara-service[bot],PR #21396: [ROCm] Fix build break due to XNNPACK update and add NCCL_MAX_NCHANNELS to multi gpu tests,"PR CC(tensorflowDemo): [ROCm] Fix build break due to XNNPACK update and add NCCL_MAX_NCHANNELS to multi gpu tests Imported from GitHub PR https://github.com/openxla/xla/pull/21396 `NCCL_MAX_NCHANNELS=1`  is necessary for collective ops tests to pass in CI. As for XNNPACK problem, similar fix has already been done for singlegpu tests > https://github.com/openxla/xla/pull/20975 Copybara import of the project:  631fa6b7fc859c083e0735d2ce47167cbf57c174 by Milica Makevic : Fix build break due to XNNPACK update  d226a07701ddd88d45e7b27406d2915d032832a4 by Milica Makevic : Add NCCL_MAX_NCHANNELS env variable to multi gpu tests  b826eee7ab0e2a1f4d062871465ab4dac48ead37 by Milica Makevic : Split bazel command arguments in multiple lines Merging this change closes CC(tensorflowDemo) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21396 from ROCm:ci_fix_xnnpack_build_err b826eee7ab0e2a1f4d062871465ab4dac48ead37",2025-01-15T13:11:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84956
copybara-service[bot],Revert Triton commit https://github.com/triton-lang/triton/pull/5389,Revert Triton commit https://github.com/tritonlang/triton/pull/5389 This has been reverted in triton at head & is causing many of our internal tests to fail as well.,2025-01-15T13:08:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84955
copybara-service[bot],PR #21396: [ROCm] Fix build break due to XNNPACK update and add NCCL_MAX_NCHANNELS to multi gpu tests,"PR CC(tensorflowDemo): [ROCm] Fix build break due to XNNPACK update and add NCCL_MAX_NCHANNELS to multi gpu tests Imported from GitHub PR https://github.com/openxla/xla/pull/21396 `NCCL_MAX_NCHANNELS=1`  is necessary for collective ops tests to pass in CI. As for XNNPACK problem, similar fix has already been done for singlegpu tests > https://github.com/openxla/xla/pull/20975 Copybara import of the project:  631fa6b7fc859c083e0735d2ce47167cbf57c174 by Milica Makevic : Fix build break due to XNNPACK update  d226a07701ddd88d45e7b27406d2915d032832a4 by Milica Makevic : Add NCCL_MAX_NCHANNELS env variable to multi gpu tests  b826eee7ab0e2a1f4d062871465ab4dac48ead37 by Milica Makevic : Split bazel command arguments in multiple lines Merging this change closes CC(tensorflowDemo) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21396 from ROCm:ci_fix_xnnpack_build_err b826eee7ab0e2a1f4d062871465ab4dac48ead37",2025-01-15T12:59:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84954
copybara-service[bot],Move remaining fusion codegen to xla/backends/gpu/codegen/,Move remaining fusion codegen to xla/backends/gpu/codegen/,2025-01-15T12:44:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84953
copybara-service[bot],check in elemental gpu emitter,check in elemental gpu emitter,2025-01-15T12:44:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84952
copybara-service[bot],[XLA:GPU] Calculate packing dim for s4 dots from stride and shape values.,"[XLA:GPU] Calculate packing dim for s4 dots from stride and shape values. The previous implementation of the Triton Int4 Rewrite relied on the packed_dim attr that emitter was submitting. This approach was incorrect when we had some non trivial hlos that had 2d tensor and a bitcast to 3d tensor and 3d dot. The old way counted the Side (lhs/rhs), layout of the s4 tensor, and contracting dim/noncontracting dim indexes of the dot. As a result 3d dot was not able to emit 2d tensor with the proper packed_dim attribute due to a crash (different rank of s4 input and the rank of the dot argument. XLA does the packing along the minormost physical dim when it transfers the s4 tensor from the host to GPU. We use this fact and calculate the packing dim by looking at the stride sizes and the shape. This logic does not depend on the side. It only checks the stride sizes and if both of them equal to 1 then we also check the shape of the MakeTensorPtrOp. As a result of that we do not pass the attribute anymore, do not involve the side of the s4 parameter of the dot, and the layout of the dot operand or fusion input. And finally can handle the fusions with the bitcasts. We expect that the strides and the shape have const values.",2025-01-15T12:11:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84951
copybara-service[bot],[xla:cpu:benchmarks] Move benchmarks to the new //xla/backends/cpu folder,[xla:cpu:benchmarks] Move benchmarks to the new //xla/backends/cpu folder,2025-01-15T12:06:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84950
copybara-service[bot],[XLA:GPU] Fix comment for `xla_gpu_analytical_latency_estimator_options` usage.,[XLA:GPU] Fix comment for `xla_gpu_analytical_latency_estimator_options` usage. s/ms/us.,2025-01-15T11:40:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84949
copybara-service[bot],[xla] scatter_simplifier: simplify IsSimplifiedScatter function,[xla] scatter_simplifier: simplify IsSimplifiedScatter function Use DeMorgan's law to simplify the boolean logic in the IsSimplifierScatter helper. It's more readable this way.,2025-01-15T11:19:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84948
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-15T11:17:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84947
copybara-service[bot],[XLA:GPU] Drop most of GPU elemental IR emitter,[XLA:GPU] Drop most of GPU elemental IR emitter,2025-01-15T11:07:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84946
copybara-service[bot],[XLA] Fix undefined behaviors for missing HloModule schedule.,[XLA] Fix undefined behaviors for missing HloModule schedule. The documentation states that `HloModule::schedule()` CHECK fails if no schedule is set but it does not. Also improve debugability of LHS tests and clarify a comment.,2025-01-15T11:06:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84945
copybara-service[bot],[XLA:GPU] Add missing pass in the Triton CUDA compilation pipeline.,[XLA:GPU] Add missing pass in the Triton CUDA compilation pipeline.,2025-01-15T10:52:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84943
copybara-service[bot],PR #21410: [ROCm] Register gfx12xx,PR CC(Memory leak with tf.py_func): [ROCm] Register gfx12xx Imported from GitHub PR https://github.com/openxla/xla/pull/21410 This PR registers new arch to xla repo. Copybara import of the project:  64abffc2aa18daddcc2678b32f75d7ca122b01a2 by scxfjiang : register gfx12xx Merging this change closes CC(Memory leak with tf.py_func) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21410 from ROCm:dev_register_gfx12xx 64abffc2aa18daddcc2678b32f75d7ca122b01a2,2025-01-15T10:51:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84942
copybara-service[bot],[XLA] Fix build failure in MacOS,"[XLA] Fix build failure in MacOS Broken since November 28, 2024.",2025-01-15T10:44:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84941
copybara-service[bot],Move fusion tests to xla/backends/gpu/codegen/emitters/tests/,Move fusion tests to xla/backends/gpu/codegen/emitters/tests/,2025-01-15T10:35:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84939
copybara-service[bot],Move fusions/triton.* to triton/fusion.*,Move fusions/triton.* to triton/fusion.* Move it to the xla/backends/gpu/codegen/triton directory and rename to fusion.*,2025-01-15T09:17:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84935
jiunkaiy,Qualcomm AI Engine Direct - Add dispatch options for QC,Summary:  Add htp runtime options  Add log level settings dispatch_delegate_qualcomm_test !image,2025-01-15T08:54:11Z,awaiting review comp:lite size:L,open,0,0,https://github.com/tensorflow/tensorflow/issues/84932
copybara-service[bot],Delete file that already got moved to new location.,Delete file that already got moved to new location. Apparently I made a mistake when merging and somehow this file did not get deleted. I verified that the file at the new location is identical (up to the expected differences in header includes).,2025-01-15T08:38:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84930
copybara-service[bot],Add field in internal model for storing (non-tensor) buffers that will be appended to the back and edges to them from ops.,Add field in internal model for storing (nontensor) buffers that will be appended to the back and edges to them from ops.,2025-01-15T07:45:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84928
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-15T07:25:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84927
copybara-service[bot],PR #21437: [ds-fusion] Fix algebraic simplifier error in debug mode.,"PR CC(Resnet50 applying last pooling layer regardless pooling parameter): [dsfusion] Fix algebraic simplifier error in debug mode. Imported from GitHub PR https://github.com/openxla/xla/pull/21437 This error was observed while trying to land CC(Raspberry Pi install command not properly formatted.) (which is needed for the dsfusion work). This error occurs when there is a constant operation that can be converted into a scalar broadcast, but some other operation is a successor for the constant operation (via control dependency). Such a dependency is not relayed and so the operation is not converted even after the `ReplaceWithNewInstruction` function call. This causes a runtime error in debug mode testing. Fixing this by relaying this control dependency. Copybara import of the project:  9601c96d468a0d56d9f3ed0a925186ab49a4341b by Shraiysh Vaishay : [dsfusion] Fix algebraic simplifier error in debug mode. This error was observed while trying to land CC(Raspberry Pi install command not properly formatted.) (which is needed for the dsfusion work). This error occurs when there is a constant operation that can be converted into a scalar broadcast, but some other operation is a successor for the constant operation (via control dependency). Such a dependency is not relayed and so the operation is not converted even after the `ReplaceWithNewInstruction` function call. This causes a runtime error in debug mode testing. Fixing this by relaying this control dependency.  3a7ab58814f35a8c7f22cb46248cead3c5cdca50 by Shraiysh Vaishay : Change the function in dfs_hlo_visitor to always relay control deps. Merging this change closes CC(Resnet50 applying last pooling layer regardless pooling parameter) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21437 from shraiysh:algebraic_simplifier_fix 3a7ab58814f35a8c7f22cb46248cead3c5cdca50",2025-01-15T07:03:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84926
copybara-service[bot],Add functions for working with dispatch op custom options using the flex buffer api.,Add functions for working with dispatch op custom options using the flex buffer api.,2025-01-15T06:44:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84925
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-15T06:21:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84923
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-15T06:19:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84922
Jaswanth28,"tensorflow cuda Unable to register cuDNN factory error in wsl2 with tf 2.17,18"," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version tf 2.18  Custom code No  OS platform and distribution wsl2 ubuntu 24.04lts  Mobile device windows 11 x86  Python version 3.12  Bazel version _No response_  GCC/compiler version gcc (Ubuntu 13.3.06ubuntu2~24.04) 13.3.0  CUDA/cuDNN version 12.5/9.3  GPU model and memory rtx 4060 laptop gpu/8gb vram/16gb ram  Current behavior? python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"" 20250115 05:03:52.570454: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250115 05:03:52.577748: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1736917432.586305     920 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1736917432.588797     920 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250115 05:03:52.597990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. v2.18.0rc24g6550e4bd802 2.18.0  Standalone code to reproduce the issue steps: https://docs.nvidia.com/cuda/cudainstallationguidelinux/index.htmlverifyyouhaveacudacapablegpu use wsl method ```shell python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"" ```  Relevant log output ```shell 20250115 05:03:52.570454: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250115 05:03:52.577748: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1736917432.586305     920 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1736917432.588797     920 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250115 05:03:52.597990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. v2.18.0rc24g6550e4bd802 2.18.0 ```",2025-01-15T05:05:58Z,stat:awaiting response type:build/install stale wsl2 TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84921,"Hi **** , Apologies for the delay, and thank you for reporting the issue. This is a known issue, and there are other related open issues that developers are actively working on. I would recommend taking a look at CC(cuDNN, cuFFT, and cuBLAS Errors), where a similar issue has been reported and is still under review. Additionally, please follow CC(Unable to register cuDNN factory, cuFFT factory, and cuBLAS factory), which tracks updates on a related issue for more information and progress. Thank you for your patience and understanding!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-15T04:21:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84920
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-15T04:17:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84919
weilhuan-quic,LiteRt Qualcomm wrappers,,2025-01-15T03:51:50Z,size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84918
copybara-service[bot],Title: NCCL cost model adjustment,Title: NCCL cost model adjustment Description: We precised how to set NIC speed in Gbytes/sec where Gbytes=10^9 bytes (and not 1024^3 bytes/sec),2025-01-15T02:25:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84917
copybara-service[bot],Canonicalize inputs of conditionals into tuples in `ConditionalCanonicalizer`,"Canonicalize inputs of conditionals into tuples in `ConditionalCanonicalizer` `DynamicDimensionInference` expects all conditional inputs/outputs to be tuplized so that it can easily add more inputs and `RET_CHECK`fails otherwise, but `ConditionalCanonicalizer` only canonicalizes the outputs. This CL changes the canonicalizer to tuplize the inputs of conditionals as well.",2025-01-15T02:23:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84916
copybara-service[bot],Implement CopyRawToHost for TfrtCpuClient.,Implement CopyRawToHost for TfrtCpuClient.,2025-01-15T02:15:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84915
copybara-service[bot],Remove stub dependency from cudart cc_library.,Remove stub dependency from cudart cc_library.,2025-01-15T02:09:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84914
copybara-service[bot],Add kUsingGpuRocm5_7_0 property tag.,Add kUsingGpuRocm5_7_0 property tag.,2025-01-15T01:34:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84913
copybara-service[bot],Migrate array_elementwise_ops_test to always use PjRt for its test backend.,Migrate array_elementwise_ops_test to always use PjRt for its test backend. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/83343 from cybersupersoap:MapUnstageabortfix fce0e590c252fb4c437ed8c19636790adcf12773,2025-01-15T01:34:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84911
copybara-service[bot],Remove stub dependency from cudart cc_library.,Remove stub dependency from cudart cc_library.,2025-01-15T01:34:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84910
copybara-service[bot],"Mark mhlo::add,sub,min as legal for misc types. These are supported by new shlo reference kernel but not by legacy tfl kernels.","Mark mhlo::add,sub,min as legal for misc types. These are supported by new shlo reference kernel but not by legacy tfl kernels.",2025-01-15T01:33:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84909
copybara-service[bot],"In SPMD partitioner, preprocess the sharding on singleton dimensions (dimensions whose size is 1).","In SPMD partitioner, preprocess the sharding on singleton dimensions (dimensions whose size is 1). It is meaningless to partition a dimension whose size is 1. Redundant padding and unpadding may be inserted. To avoid this, we replicate the sharding on these dimensions as a preprocessing. Take the following input as example ``` ENTRY entry {   %constant.785 = f32[1,8] constant({{0,1,2,3,4,5,6,7}}), sharding={devices=[1,8] f32[] {   %constant.8 = u32[8]{0} constant({0, 1, 2, 3, 4, 5, 6, 7})   %partitionid = u32[] partitionid()   %dynamicslice.3 = u32[1]{0} dynamicslice(u32[8]{0} %constant.8, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.2 = u32[] reshape(u32[1]{0} %dynamicslice.3)   %constant.9 = u32[] constant(0)   %compare = pred[] compare(u32[] %reshape.2, u32[] %constant.9), direction=EQ   %broadcast = pred[1,1]{1,0} broadcast(pred[] %compare), dimensions={}   %constant.0 = f32[1,8]{1,0} constant({ { 0, 1, 2, 3, 4, 5, 6, 7 } })   %constant.1 = s32[] constant(0)   %constant.2 = s32[8]{0} constant({0, 1, 2, 3, 4, 5, 6, 7})   %dynamicslice = s32[1]{0} dynamicslice(s32[8]{0} %constant.2, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape = s32[] reshape(s32[1]{0} %dynamicslice)   %dynamicslice.1 = f32[1,1]{1,0} dynamicslice(f32[1,8]{1,0} %constant.0, s32[] %constant.1, s32[] %reshape), dynamic_slice_sizes={1,1}   %copy = f32[1,1]{1,0} copy(f32[1,1]{1,0} %dynamicslice.1)   %constant.10 = f32[] constant(0)   %broadcast.1 = f32[1,1]{1,0} broadcast(f32[] %constant.10), dimensions={}   %select = f32[1,1]{1,0} select(pred[1,1]{1,0} %broadcast, f32[1,1]{1,0} %copy, f32[1,1]{1,0} %broadcast.1)   %allreduce = f32[1,1]{1,0} allreduce(f32[1,1]{1,0} %select), channel_id=1, replica_groups={{0,1,2,3,4,5,6,7}}, use_global_device_ids=true, to_apply=%add.clone   ROOT %reshape.3 = f32[] reshape(f32[1,1]{1,0} %allreduce) } ``` Result with this improvement ``` ENTRY %entry_spmd () > f32[] {   %constant.0 = f32[1,8]{1,0} constant({ { 0, 1, 2, 3, 4, 5, 6, 7 } })   %slice.0 = f32[1,1]{1,0} slice(f32[1,8]{1,0} %constant.0), slice={[0:1], [0:1]}   ROOT %reshape.1 = f32[] reshape(f32[1,1]{1,0} %slice.0) } ``` Reverts a7703e73d050fe62fd59ffd4435a8f9249dfc99c FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21273 from nvcastet:ncclCommInitRankScalable dd6362af36a1f4d22532ad15b2007527898b5fa1",2025-01-15T00:31:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84908
copybara-service[bot],Propagate op name in the tfrt OpKernelRunner.,Propagate op name in the tfrt OpKernelRunner.,2025-01-15T00:16:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84907
copybara-service[bot],Create a generic tsl::Allocator that works in terms of stream_executor::MemoryAllocators.,Create a generic tsl::Allocator that works in terms of stream_executor::MemoryAllocators.,2025-01-15T00:10:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84906
copybara-service[bot],Remove a check for the sharding in `FindPadWithWrapPattern`.,"Remove a check for the sharding in `FindPadWithWrapPattern`. It is unnecessary to have the same sharding since `FindPadWithWrapPattern` is only used to rewrite the graph with full shape. Even if the shardings are different, we can still rewrite the graph. The partitioner will handle the different sharding.",2025-01-15T00:04:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84905
copybara-service[bot],AllocationRequest.end_time is inclusive. The start and end times of MsaBufferInterval are inclusive. Update logging in MSA to indicate as much.,AllocationRequest.end_time is inclusive. The start and end times of MsaBufferInterval are inclusive. Update logging in MSA to indicate as much.,2025-01-14T23:25:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84904
copybara-service[bot],Vhlo changes for result accuracy attributes to stablehlo.,Vhlo changes for result accuracy attributes to stablehlo.,2025-01-14T23:13:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84903
copybara-service[bot],PR #21273: [XLA:GPU] Add support for NCCL ncclCommInitRankScalable API,PR CC(error occurring while running my motion detectioncode on jupyter notebook): [XLA:GPU] Add support for NCCL ncclCommInitRankScalable API Imported from GitHub PR https://github.com/openxla/xla/pull/21273 `ncclCommInitRankScalable` enables the initialization of communicators via multiple roots which improves the init performance at large scale. The maximum number of ranks associated with a root rank to initialize a NCCL communicator can be tuned via `xla_gpu_nccl_init_max_rank_per_root_ratio`. Default is 128 ranks per root. Copybara import of the project:  98ef02dabc0bcb2c8206753bec4873c5f48e269f by Nicolas Castet : [XLA:GPU] Add support for NCCL ncclCommInitRankScalable API  f146a48fef5f1a1098b5c01ae79c5a0d9a9af8d7 by Nicolas Castet : Address review comments  dd6362af36a1f4d22532ad15b2007527898b5fa1 by Nicolas Castet : Add GpuCliqueKey::GetSubKeys unit test Merging this change closes CC(error occurring while running my motion detectioncode on jupyter notebook) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21273 from nvcastet:ncclCommInitRankScalable dd6362af36a1f4d22532ad15b2007527898b5fa1,2025-01-14T22:56:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84902
copybara-service[bot],Delete remnants of PhaseOrderPipeline.,Delete remnants of PhaseOrderPipeline.,2025-01-14T22:54:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84901
copybara-service[bot],add changelist number to tfrt pjrt impl,"add changelist number to tfrt pjrt impl plumb through cl as ""cl_number"" in attributes. This attribute can be used by direct callers of the tpu library to determine the build version.",2025-01-14T22:39:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84900
copybara-service[bot],Support expanding ragged all-to-all dims similar to all-to-alls.,Support expanding ragged alltoall dims similar to alltoalls.,2025-01-14T22:27:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84899
copybara-service[bot],Reverts a7703e73d050fe62fd59ffd4435a8f9249dfc99c,Reverts a7703e73d050fe62fd59ffd4435a8f9249dfc99c,2025-01-14T22:26:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84898
Yadan-Wei,Seg Fault when iterate dataset created from data service," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Segfault when trying to iterate dataset get from data service.  Standalone code to reproduce the issue ```shell  start the data service file start_dataservice.py import tensorflow as tf dispatcher = tf.data.experimental.service.DispatchServer(     tf.data.experimental.service.DispatcherConfig(port=50050), start=True ) dispatcher_address = dispatcher.target.split(""://"")[1] worker = tf.data.experimental.service.WorkerServer(     tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True ) print(""Starting Worker"") worker.join()  test file test_dataset_service.py import tensorflow as tf import numpy as np flags = tf.compat.v1.app.flags flags.DEFINE_bool(""local"", False, ""Run data service in process"") flags.DEFINE_bool(""distribute"", False, ""Run data service in distributed_epoch mode"") FLAGS = flags.FLAGS def local_service():     print(""Starting Local Service"")     dispatcher = tf.data.experimental.service.DispatchServer(         tf.data.experimental.service.DispatcherConfig(port=50050), start=True     )     dispatcher_address = dispatcher.target.split(""://"")[1]     worker = tf.data.experimental.service.WorkerServer(         tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True     )     print(""Dispatcher target is "", dispatcher.target)     return dispatcher, worker, dispatcher.target def apply_transformations(ds_train):     ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)     ds_train = ds_train.cache()     ds_train = ds_train.shuffle(60000)     ds_train = ds_train.batch(128)     ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)     return ds_train (x_train, y_train), _ = tf.keras.datasets.mnist.load_data() x_train = x_train / np.float32(255) y_train = y_train.astype(np.int64) ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)) def normalize_img(image, label):     """"""Normalizes images: `uint8` > `float32`.""""""     return tf.cast(image, tf.float32) / 255.0, label ds_train = apply_transformations(ds_train)  Create dataset however you were before using the tf.data service. dataset = ds_train if FLAGS.local:     dispatcher, worker, service = local_service() else:     dispatcher_address = ""localhost""     dispatcher_port = ""50050""     service = ""grpc://{}:{}"".format(dispatcher_address, dispatcher_port) if FLAGS.distribute:     processing_mode = ""distributed_epoch"" else:     processing_mode = ""parallel_epochs""  This will register the dataset with the tf.data service cluster so that  tf.data workers can run the dataset to produce elements. The dataset returned  from applying `distribute` will fetch elements produced by tf.data workers. dataset = dataset.apply(     tf.data.experimental.service.distribute(processing_mode=processing_mode, service=service) ) for (x1, y1), (x2, y2) in zip(dataset, ds_train):     np.allclose(x1, x2)     np.allclose(y1, y2) print(""verified mnist dataset locally vs over service"")  script to run  python m pip install upgrade pip python m pip install tensorflow==2.18.0 python m pip install 'protobuf device: 0, name: NVIDIA A100SXM440GB, pci bus id: 0000:10:1c.0, compute capability: 8.0 I0000 00:00:1736891783.520395    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 37945 MB memory:  > device: 1, name: NVIDIA A100SXM440GB, pci bus id: 0000:10:1d.0, compute capability: 8.0 I0000 00:00:1736891783.522012    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 37945 MB memory:  > device: 2, name: NVIDIA A100SXM440GB, pci bus id: 0000:20:1c.0, compute capability: 8.0 I0000 00:00:1736891783.523626    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 37945 MB memory:  > device: 3, name: NVIDIA A100SXM440GB, pci bus id: 0000:20:1d.0, compute capability: 8.0 I0000 00:00:1736891783.525222    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 37945 MB memory:  > device: 4, name: NVIDIA A100SXM440GB, pci bus id: 0000:90:1c.0, compute capability: 8.0 I0000 00:00:1736891783.526807    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 37945 MB memory:  > device: 5, name: NVIDIA A100SXM440GB, pci bus id: 0000:90:1d.0, compute capability: 8.0 I0000 00:00:1736891783.528377    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 37945 MB memory:  > device: 6, name: NVIDIA A100SXM440GB, pci bus id: 0000:a0:1c.0, compute capability: 8.0 I0000 00:00:1736891783.529933    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 37945 MB memory:  > device: 7, name: NVIDIA A100SXM440GB, pci bus id: 0000:a0:1d.0, compute capability: 8.0 /test/bin/testDataservice: line 5:  9168 Segmentation fault      (core dumped) python ${BIN_DIR}/test_dataset_service.py local=False ```",2025-01-14T21:57:20Z,type:bug comp:ops TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/84897
copybara-service[bot],Pass flatten-tuple : Migrate from MHLO to StableHLO,Pass flattentuple : Migrate from MHLO to StableHLO,2025-01-14T21:46:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84896
copybara-service[bot],Always use int32 dtype for the inputs of dynamic partition. Only int32 operands,Always use int32 dtype for the inputs of dynamic partition. Only int32 operands are supported.,2025-01-14T20:58:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84895
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T20:56:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84894
copybara-service[bot],Integrate LLVM at llvm/llvm-project@19032bfe87fa,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 19032bfe87fa,2025-01-14T20:37:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84893
copybara-service[bot],PR #21273: [XLA:GPU] Add support for NCCL ncclCommInitRankScalable API,PR CC(error occurring while running my motion detectioncode on jupyter notebook): [XLA:GPU] Add support for NCCL ncclCommInitRankScalable API Imported from GitHub PR https://github.com/openxla/xla/pull/21273 `ncclCommInitRankScalable` enables the initialization of communicators via multiple roots which improves the init performance at large scale. The maximum number of ranks associated with a root rank to initialize a NCCL communicator can be tuned via `xla_gpu_nccl_init_max_rank_per_root_ratio`. Default is 128 ranks per root. Copybara import of the project:  98ef02dabc0bcb2c8206753bec4873c5f48e269f by Nicolas Castet : [XLA:GPU] Add support for NCCL ncclCommInitRankScalable API  f146a48fef5f1a1098b5c01ae79c5a0d9a9af8d7 by Nicolas Castet : Address review comments  dd6362af36a1f4d22532ad15b2007527898b5fa1 by Nicolas Castet : Add GpuCliqueKey::GetSubKeys unit test Merging this change closes CC(error occurring while running my motion detectioncode on jupyter notebook) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21273 from nvcastet:ncclCommInitRankScalable dd6362af36a1f4d22532ad15b2007527898b5fa1,2025-01-14T20:02:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84892
copybara-service[bot],[XLA:GPU] Add `RewritePattern`s for binary elementwise ops in `SimplifyAffinePass`.,"[XLA:GPU] Add `RewritePattern`s for binary elementwise ops in `SimplifyAffinePass`. The rewrites allow us to concretize `index` computations into computations on integers with fixed widths. Triton forces `index`es to concretize to 32bitwide integers, forcing us to concretize early in order to work around an integer overflow when we use `ApplyIndexingOp`s to compute a linear offset into an array with more than `2^32` elements. Eventually, the concretization should be made into a proper passbut we start with a set of `RewritePattern`s to fix the existing integer overflow.",2025-01-14T19:52:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84891
copybara-service[bot],"When reduce_scoped_memory is enabled, the size of scoped allocations can change. In that case, the peak_memory_usage_ should also be updated.","When reduce_scoped_memory is enabled, the size of scoped allocations can change. In that case, the peak_memory_usage_ should also be updated.",2025-01-14T19:52:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84890
copybara-service[bot],Improvements to Loop Analysis's pattern matching to handle copies.,Improvements to Loop Analysis's pattern matching to handle copies. Supports loop trip count calculation in cases where 1. the induction variable in the while body is copied and subsequently incremented. 2. the induction variable increment in the while body is followed by copies or tuple>GTE. 3. the induction variable initialization before the while instruction across copies and GTETuple pairs.,2025-01-14T19:51:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84889
copybara-service[bot],[xla:cpu] Make sure that NanoRt managed temp is properly aligned,[xla:cpu] Make sure that NanoRt managed temp is properly aligned,2025-01-14T19:39:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84888
copybara-service[bot],Update ml_dtypes version in ml_dtypes.cmake.,Update ml_dtypes version in ml_dtypes.cmake. I forgot to update ml_dtypes.cmake when updating the ml_dtypes version in https://github.com/tensorflow/tensorflow/pull/84530.,2025-01-14T19:35:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84887
copybara-service[bot],[xla:cpu:benchmarks] Fix benchmarks not running.,[xla:cpu:benchmarks] Fix benchmarks not running. Temporarily change `//:gtest_main` back to `//xla/tsl/platform:test_main`. There was an issue passing `benchmark_filter=all` flag to `//:gtest_main`.,2025-01-14T19:28:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84886
copybara-service[bot],Enable support for DenseResourceElementsAttr in TFL Op folders. Folders should handle both DenseResourceElementsAttr and DenseElementsAttr.,"Enable support for DenseResourceElementsAttr in TFL Op folders. Folders should handle both DenseResourceElementsAttr and DenseElementsAttr. 1. Initial version of utilities to `GetValues` from both DenseResourceElementsAttr and DenseElementsAttr as `ArrayRef`. 2. Following Op folders have been updated    a. TransposeOp::fold    b. Op::fold    c. reshapeOp::fold 3. Folder logic, in the case of DenseResourceElementsAttr, will reuse the existing memory instead of allocating new memory.",2025-01-14T19:05:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84885
copybara-service[bot],Add a couple of unit tests for Timespan::ExpandToInclude().,Add a couple of unit tests for Timespan::ExpandToInclude().,2025-01-14T19:04:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84884
copybara-service[bot],Add DmaMap and DmaUnmap.,Add DmaMap and DmaUnmap.,2025-01-14T19:03:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84883
copybara-service[bot],Add xnn_define_static_slice_v3() ,"Add xnn_define_static_slice_v3()  Workinprogress, tests needed",2025-01-14T18:56:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84882
copybara-service[bot],Actually allocated a temporary buffer before calling NanoRt Executable. Add a test that fails if this isn't done correctly.,Actually allocated a temporary buffer before calling NanoRt Executable. Add a test that fails if this isn't done correctly.,2025-01-14T18:48:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84881
Morehman27,First project,https://github.com/nodejs/node/commit/05d6227a8898a7638fbffeae435ce73ecad525b2,2025-01-14T18:46:17Z,stat:awaiting response,closed,0,2,https://github.com/tensorflow/tensorflow/issues/84880,"Hi **** , Can you please fill the issue template.Also can you please elaborate about your Feature and please specify the Use Cases for this feature. Thank you!",This is spam.
copybara-service[bot],[PJRT:CPU] Improve handling of input buffers with errors and last execution error,"[PJRT:CPU] Improve handling of input buffers with errors and last execution error * When the CPU client has an input buffer's definition event available, do not ignore it if the event has an error. Instead, take such as definition event as an input dependency, which will poison output buffers. This behavior is consistent with how input error buffers poison output buffers. * If an execution is enqueued before the previous execution has not finished, and the first execution fails with an error, the second execution does not fail accidentally. This is done by clearing any error set in `GetLastEnqueueEvent()` because this last enqueue event is used for force sequential execution, not to propagate errors across independently enqueued computations. * An error buffer is created a dummy internal buffer in `TfrtCpuClient::CreateErrorBuffer`, which meets internal invariants that even an error `TfrtCpuBuffer` has a buffer and can handle metadata operations including `TfrtCpuBuffer::BufferSizes()`.",2025-01-14T18:40:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84879
copybara-service[bot],[HLO Componentization] Add deprecation timeline to aliased build targets.,[HLO Componentization] Add deprecation timeline to aliased build targets. This step towards encouraging extrenal projects to migrate to the already migrated hlo subcomponents.,2025-01-14T18:40:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84878
copybara-service[bot],[HLO Componentization] Add deprecation timeline to aliased build targets.,[HLO Componentization] Add deprecation timeline to aliased build targets. This step towards encouraging extrenal projects to migrate to the already migrated hlo subcomponents.,2025-01-14T18:40:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84878
copybara-service[bot],Change `tsl/platform:test_main` back to alias of `test_main` defined in XLA,"Change `tsl/platform:test_main` back to alias of `test_main` defined in XLA This may have had unintended consequences for benchmark targets, so revert until we can have a better fix.",2025-01-14T18:25:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84877
copybara-service[bot],Fix Thread-safety warning.,Fix Threadsafety warning.,2025-01-14T18:20:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84876
copybara-service[bot],Move various headers to `xla/tsl`,Move various headers to `xla/tsl` Moves  byte_order.h  crash_analysis.h  dynamic_annotations.h  grpc_credentials.h  intrusive_ptr.h  prefetch.h  ram_file_system.h  resource.h  resource_loader.h  rocm_rocdl_path.h  stack_frame.h,2025-01-14T18:17:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84875
gaurides,[oneDNN] Add Infer after last allow to be added to Allow list,"In the current implementation of automixed precision pass for FP16_CPU, the infer node is added to allow set only if both its upstream and downstream nodes are in the allow list. This could cause cast node being inserted in between fuseable nodes. To address this problem, a subpass is added to place an infer node to allow set if its direct upstream is in allow set. This feature was enabled for BF16, and now is being enabled for FP16_CPU to keep the bf16/fp16 optimizations same on CPU. eg: if we have this sequence MatMul > BiasAdd > Relu > Identity Then for FP16_CPU with current implementation, Cast op will be inserted between MM & BiasAdd and the fusion won't happen MatMul (FP16_CPU) > Cast(Fp16 >FP32) > BiasAdd > Relu > Identity With implementation in this PR, Cast op will be inserted after the Infer nodes so that the last fusion is possible MatMul > BiasAdd > Relu > Cast (FP16_CPU > FP32) > Identity FusedMatMul > Cast > Identity",2025-01-14T17:59:57Z,awaiting review size:M comp:core,open,0,1,https://github.com/tensorflow/tensorflow/issues/84874,"Hi , Can you please review this PR? Thank you ! "
copybara-service[bot],"Updates the Evaluator to calculate a maximum memory lower bound (i.e., the amount of memory that even the ""smallest"" possible solution is guaranteed to consume).","Updates the Evaluator to calculate a maximum memory lower bound (i.e., the amount of memory that even the ""smallest"" possible solution is guaranteed to consume).",2025-01-14T17:42:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84873
copybara-service[bot],Reverts fd41705e0ad7a123a9d01b8be2a3b34b3266493e,Reverts fd41705e0ad7a123a9d01b8be2a3b34b3266493e,2025-01-14T17:27:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84872
copybara-service[bot],[XLA:GPU] drop unused gpu_emitter in ir_emitter_unnested,[XLA:GPU] drop unused gpu_emitter in ir_emitter_unnested,2025-01-14T16:56:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84871
copybara-service[bot],[XLA:GPU] Delete GPU IrEmitter::HandleFusion,"[XLA:GPU] Delete GPU IrEmitter::HandleFusion it is not used anywhere, and we can remove a use of GpuElementalIrEmitter",2025-01-14T16:54:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84870
copybara-service[bot],Integrate LLVM at llvm/llvm-project@19032bfe87fa,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 19032bfe87fa,2025-01-14T16:21:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84869
copybara-service[bot],[WIP] Remove the need to run the default scheduler before LHS.,[WIP] Remove the need to run the default scheduler before LHS.,2025-01-14T16:15:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84868
copybara-service[bot],Give meaningful names to HLO modules in `triton_fusion_emitter_int4_device_test.cc`.,Give meaningful names to HLO modules in `triton_fusion_emitter_int4_device_test.cc`. This will make it easier to identify failing tests among the dumps.,2025-01-14T14:12:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84867
copybara-service[bot],[XLA:GPU] Set --set_xla_gpu_experimental_pack_dot_operands_along_k_dimension to true by default.,"[XLA:GPU] Set set_xla_gpu_experimental_pack_dot_operands_along_k_dimension to true by default. If your model started to OOM as a result of this, do this in order of preference. 1. (most preferred)  Layout (subbyte typed) weights of your model in such a way that the contracting (K) dimension of dots they are used in is minor. 2. Use AUTO layout (from JAX side) 3. Set set_xla_gpu_experimental_pack_dot_operands_along_k_dimension=false",2025-01-14T13:58:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84866
copybara-service[bot],[XLA:TPU] Use `MakeComputationPostOrder()` instead of `MakeComputationSorted()` in `HloDataflowAnalysis::InitializeInstructionValueSets()`.,[XLA:TPU] Use `MakeComputationPostOrder()` instead of `MakeComputationSorted()` in `HloDataflowAnalysis::InitializeInstructionValueSets()`.,2025-01-14T13:45:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84865
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T12:59:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84861
copybara-service[bot],Update code links in documentation.,Update code links in documentation.,2025-01-14T12:33:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84859
copybara-service[bot],[XLA] do not log warnings from hlo pass,[XLA] do not log warnings from hlo pass,2025-01-14T11:57:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84855
copybara-service[bot],Move triton codegen to xla/backends/gpu/codegen/triton,Move triton codegen to xla/backends/gpu/codegen/triton,2025-01-14T11:50:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84854
copybara-service[bot],[XLA:GPU] Require packed dot operands to be packed along contracting dimension.,"[XLA:GPU] Require packed dot operands to be packed along contracting dimension. For now, only do that if `xla_gpu_experimental_pack_dot_operands_along_k_dimension` is set.",2025-01-14T11:31:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84851
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T10:03:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84845
copybara-service[bot],[XLA:GPU] IWYU in hlo_pass_fix.h NFC,[XLA:GPU] IWYU in hlo_pass_fix.h NFC,2025-01-14T10:02:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84844
jiunkaiy,Qualcomm AI Engine Direct - Add dispatch options for QC,Summary:  Add htp runtime options  Add log level settings  Fix event BUILD,2025-01-14T10:00:09Z,size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84843
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T09:56:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84842
copybara-service[bot],[XLA] typo and comment formatting NFC,[XLA] typo and comment formatting NFC,2025-01-14T09:54:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84841
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T09:16:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84837
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T09:16:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84836
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T09:16:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84835
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T09:16:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84834
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T09:14:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84833
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T09:12:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84832
fsamekl,Yolov8-seg.pt segmentation model is deployed on Android after training,"**System information**  Android Device information (use `adb shell getprop ro.build.fingerprint`   if possible): ”“” vivo/PD2020/PD2020:10/QP1A.190711.020/compiler10141555:user/releasekeys “”“  TensorFlow Lite in Play Services SDK version (found in `build.gradle`): ”“”     implementation 'org.tensorflow:tensorflowlitetaskvision:0.4.0'     implementation 'org.tensorflow:tensorflowlitegpudelegateplugin:0.4.0'     implementation 'org.tensorflow:tensorflowlitegpu:2.9.0' “”“  Google Play Services version   (`Settings` > `Apps` > `Google Play Services` > `App details`): **Standalone code to reproduce the issue** ”“Here is the full code of the program”“ """""" public class MainActivity extends AppCompatActivity {     private String MODEL = ""best_float32_metadata.tflite"";          protected void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);         setContentView(R.layout.activity_main);         Bitmap bitmap = BitmapFactory.decodeResource(getResources(), R.drawable.a3);         bitmap = imageScale(bitmap, 640, 640);         TensorImage tensorImage = TensorImage.fromBitmap(bitmap);         Log.e(""HENG"", String.valueOf(tensorImage.getBuffer()));         Log.e(""HENG"", String.valueOf(tensorImage.getTensorBuffer()));         Log.e(""HENG"", String.valueOf(tensorImage.getDataType()));         Log.e(""HENG"", String.valueOf(tensorImage.getColorSpaceType()));         ImageSegmenter.ImageSegmenterOptions options = ImageSegmenter.ImageSegmenterOptions.builder()                 .setBaseOptions(BaseOptions.builder().build())                 .setOutputType(OutputType.CONFIDENCE_MASK)                 .build();         ImageSegmenter imageSegmenter = null;         try {             imageSegmenter = ImageSegmenter.createFromFileAndOptions(this, MODEL, options);         } catch (IOException e) {             throw new RuntimeException(e);         }         List results = imageSegmenter.segment(tensorImage);         Log.e(""HENG"", ""HELLO: ""+results.toString());     }     // 图像缩放方法     public static Bitmap imageScale(Bitmap bitmap, int new_w, int new_h) {         Bitmap scaledBitmap = Bitmap.createScaledBitmap(bitmap, new_w, new_h, true);         return scaledBitmap;     } } """""" **Any other info / logs** Please allow me to repeat my question. Thank you, First, (I may have solved the first problem, but I am not sure) I trained my data set with yolov8seg.pt to get a model. I converted it to tflite format, copied the 32bit model generated by best_float32.tflite into asssets in Android, and then modified the path of model to run the following original code. I got two error messages: ""1 Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.2、java.lang.IllegalStateException: Error getting native address of native library: task_vision_jni”, After I searched, I found“ https://stackoverflow.com/questions/66727627/failedtoinitializedetectorinputtensorhastypektflitefloat32mlkit ”I got a copy of the code in this link. After I tried to run it, I put the model I got into assets again. After running, it was not the above error message (the error message is the second problem). Second, (from the followup to the first question), I also got two errors after running the modified model ""best_float32_metadata.tflite"". The first one is ""java.lang.illegalargumentexception: error occurred when initializing imagesegment: image segmentation models are expected to have only 1 output, found 2"". It says that the model actually returns two outputs, which is a very important question. The second one seems to be the same as the first one, ""java.lang.runtimeexception: unable to start action""activity componentinfo{com.example.yoloseg_android/com.example.yoloseg_android.mainactivity}: java.lang.illegalstateexception: error getting native address of native library: task_vision_jni "", I don't understand this. These are my two problems. I think the second problem should be solved.  Note: I can use the deeplabv3.tflite model officially provided by tensorflow to get the output smoothly",2025-01-14T08:50:42Z,type:support comp:lite Android,open,0,3,https://github.com/tensorflow/tensorflow/issues/84829,I uploaded the project to GitHub. Thank you for your help“https://github.com/fsamekl/Yolov8segAndroidtflite/tree/master”,"Hi,   I apologize for the delayed response, The first issue, `Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images` it requires specifying `NormalizationOptions` metadata to preprocess input images."", was due to lack of metadata. To be more specific, floating models require metadata information of NormalizationOptions. LiteRT Metadata Writer API provides an easytouse API to create Model Metadata for popular ML tasks supported by the TFLite Task Library. You can add metadata using Image segmenters, Please refer TensorFlow Lite Image Segmentation Demo example which may help you to solve your issue. Thank you for your understanding and patience.","> Hi, [](https://github.com/fsamekl) I apologize for the delayed response, The first issue, `Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images` it requires specifying `NormalizationOptions` metadata to preprocess input images."", was due to lack of metadata. To be more specific, floating models require metadata information of NormalizationOptions. >  > LiteRT Metadata Writer API provides an easytouse API to create Model Metadata for popular ML tasks supported by the TFLite Task Library. You can add metadata using Image segmenters, Please refer TensorFlow Lite Image Segmentation Demo example which may help you to solve your issue. >  > Thank you for your understanding and patience. The second question you didn't answer me. I used the segmentation model trained by yolov8. It has two outputs, but imagesegment=imagesegment.createfromfileandoptions (this, model, options); This can only accept one output. What should I do? I uploaded the code to GitHub. I hope you can help solve it"
copybara-service[bot],PR #21223: [nfc] Cleanup build files for expander transforms,"PR CC(Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.): [nfc] Cleanup build files for expander transforms Imported from GitHub PR https://github.com/openxla/xla/pull/21223 This is part3 of CC(pywrap_tensorflow_internal cannot load/not found  Windows tensorflow CPU) and CC(Make protocol used in estimator customizable.). Motivation: Smaller build files, fewer merge conflicts, and convinience in development (more intuitive targets). For discussion surrounding this, check thread in CC(pywrap_tensorflow_internal cannot load/not found  Windows tensorflow CPU). Copybara import of the project:  7832f8483091b70214d17789c05c38959d67a515 by Shraiysh Vaishay : [nfc] Cleanup build files for expander transforms This is part3 of CC(pywrap_tensorflow_internal cannot load/not found  Windows tensorflow CPU) and CC(Make protocol used in estimator customizable.). Motivation: Smaller build files, fewer merge conflicts, and convinience in development (more intuitive targets). For discussion surrounding this, check thread in CC(pywrap_tensorflow_internal cannot load/not found  Windows tensorflow CPU). Merging this change closes CC(Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21223 from shraiysh:cleanup_expander_transforms 7832f8483091b70214d17789c05c38959d67a515",2025-01-14T08:36:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84828
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21223 from shraiysh:cleanup_expander_transforms 7832f8483091b70214d17789c05c38959d67a515,2025-01-14T08:36:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84827
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T08:00:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84824
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T07:55:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84822
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T07:54:08Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84821
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T07:53:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84820
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T07:53:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84819
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T07:30:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84817
copybara-service[bot],PR #21375: [ds-fusion] Get While loop analysis with copy fusion,"PR CC(Raspberry Pi install command not properly formatted.): [dsfusion] Get While loop analysis with copy fusion Imported from GitHub PR https://github.com/openxla/xla/pull/21375 In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index. Copybara import of the project:  3147ec926aa1c6fdfa2f4376668434c9a2fbeb87 by Shraiysh Vaishay : [dsfusion] Get While loop analysis with copy fusion In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index.  a435fbd2eadc17269d7bccbe141dcf7a21cc20e8 by Shraiysh Vaishay : Relay control dependencies while converting fusion to call (extractor) Merging this change closes CC(Raspberry Pi install command not properly formatted.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21375 from shraiysh:while_loop_analysis a435fbd2eadc17269d7bccbe141dcf7a21cc20e8",2025-01-14T06:30:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84815
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-14T05:07:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84814
copybara-service[bot],Enable support for DenseResourceElementsAttr in TFL Op folders.,Enable support for DenseResourceElementsAttr in TFL Op folders.,2025-01-14T04:08:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84813
copybara-service[bot],Internal only change for instrumentation,Internal only change for instrumentation,2025-01-14T04:06:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84812
copybara-service[bot],PR #21104: [NVIDIA GPU] Preserve backend config when folding transpose,PR CC(Feature Request: 5D rot90 (for voxel grid rotations)): [NVIDIA GPU] Preserve backend config when folding transpose Imported from GitHub PR https://github.com/openxla/xla/pull/21104 Transpose folding pass doesn't preserve backend config when creating the new dot with transpose folded. Changing the behavior to copy the old dot's config to the new dot. Copybara import of the project:  d2d6b628af1cab777a210e4ac62184e52fe9f4a9 by TJ Xu : Preserve backend config when folding transpose  6b5fa3a1cb70a790803e3ac57ff8329690e88e5e by TJ Xu : use SetupDerivedInstruction instead of just copying the backend config Merging this change closes CC(Feature Request: 5D rot90 (for voxel grid rotations)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21104 from Tixxx:tixxx/transpose_folding 6b5fa3a1cb70a790803e3ac57ff8329690e88e5e,2025-01-14T02:30:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84811
copybara-service[bot],Moving the logic of making a copy for rhs from `PartitionDot` to `HandleDotHelper`.,"Moving the logic of making a copy for rhs from `PartitionDot` to `HandleDotHelper`. `HandleDotHelper` is called once for a single dot operation, while `PartitionDot` can be called many times. We need to consider adding a copy only once.",2025-01-14T02:21:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84810
copybara-service[bot],PR #21380: Add F4E2M1FN and F8E8M0FNU types,"PR CC(tf.GradientTape.gradient raise error with tf.nn.relu6): Add F4E2M1FN and F8E8M0FNU types Imported from GitHub PR https://github.com/openxla/xla/pull/21380 Previous PR https://github.com/openxla/xla/pull/19096 was rolled back, retrying. This PR adds F4E2M1FN primitive type (4bit float with 2 bits exponent and 1 bit mantissa), F8E8M0FNU primitive type (8bit float with 8 bits exponent, no mantissa and no sign) and enables loads/stores in the same way S4/U4 type is implemented. This will enable using microscaling (MX) formats (RFC), such as MXFP4. ```c F4E2M1FN  Exponent bias: 1  Maximum stored exponent value: 3 (binary 11)  Maximum unbiased exponent value: 3  1 = 2  Minimum stored exponent value: 1 (binary 01)  Minimum unbiased exponent value: 1 − 1 = 0  Has Positive and Negative zero  Doesn't have infinity  Doesn't have NaNs Additional details:  Zeros (+/): S.00.0  Max normal number: S.11.1 = ±2^(2) x (1 + 0.5) = ±6.0  Min normal number: S.01.0 = ±2^(0) = ±1.0  Min subnormal number: S.00.1 = ±2^(0) x 0.5 = ±0.5 F8E8M0FNU  Exponent bias: 127  Maximum stored exponent value: 254 (binary 1111'1110)  Maximum unbiased exponent value: 254  127 = 127  Minimum stored exponent value: 0 (binary 0000'0000)  Minimum unbiased exponent value: 0 − 127 = 127  Doesn't have zero  Doesn't have infinity  NaN is encoded as binary 1111'1111 Additional details:  Zeros cannot be represented  Negative values cannot be represented  Mantissa is always 1 ``` Related PRs:  https://github.com/openxla/stablehlo/pull/2582  https://github.com/jaxml/ml_dtypes/pull/181  https://github.com/llvm/llvmproject/pull/95392  https://github.com/llvm/llvmproject/pull/108877  https://github.com/jaxml/ml_dtypes/pull/166  https://github.com/llvm/llvmproject/pull/107127  https://github.com/llvm/llvmproject/pull/111028 Copybara import of the project:  d7e00c49a4b4f26c06266d6bb941275e67464c01 by Sergey Kozub : Add F4E2M1FN and F8E8M0FNU types Merging this change closes CC(tf.GradientTape.gradient raise error with tf.nn.relu6) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21380 from openxla:skozub/e2m1_e8m0 d7e00c49a4b4f26c06266d6bb941275e67464c01",2025-01-14T01:51:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84809
copybara-service[bot],Fix the converter input reshaping failure when the XLA call chain has a PartitionedCall instead of StatefulPartitionedCall,Fix the converter input reshaping failure when the XLA call chain has a PartitionedCall instead of StatefulPartitionedCall,2025-01-14T01:48:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84808
copybara-service[bot],Remove TFL Interpreter deprecation notice from LiteRT Interpreter.,Remove TFL Interpreter deprecation notice from LiteRT Interpreter.,2025-01-14T01:15:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84807
copybara-service[bot],Fix a typo in the TFG dialect's GraphFuncOp::getCalledFunction,Fix a typo in the TFG dialect's GraphFuncOp::getCalledFunction,2025-01-14T00:31:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84806
copybara-service[bot],Create copy if the operands of gather/scatter instructions overlap.,"Create copy if the operands of gather/scatter instructions overlap. A gather has two operands, input and indices. If they point to the same instruction, create a copy for indices. A scatter has n inputs, 1 indices, and n updates (2n+1 operands in total). We allow overlap between n inputs. We also allow overlap between n updates. We need to create a copy if * indices overlap with any input or update * update overlap with any input The added copy will be removed if it is redundant in the following memory related passes (e.g., CopyInsertion).",2025-01-13T23:26:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84805
copybara-service[bot],Breaking internal tests,Breaking internal tests Reverts 5e78ccd69eca07a84d315e9384f2d69c8e23e26c,2025-01-13T23:19:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84804
copybara-service[bot],In progress experimention. Add StringDType to JAX's supported types.,In progress experimention. Add StringDType to JAX's supported types.,2025-01-13T23:04:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84803
copybara-service[bot],Make dot thunk capable of running without a thread pool.,Make dot thunk capable of running without a thread pool.,2025-01-13T22:57:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84802
copybara-service[bot],Extracts a util function `MakeACopyAndReturnItsPartitionedHlo` from dot_handler.cc.,Extracts a util function `MakeACopyAndReturnItsPartitionedHlo` from dot_handler.cc. This function creates a copy for the HloInstruction in the given PartitionedHlo and returns a new PartitionedHlo for the copy. This can be reused by other operators (like gather/scatter).,2025-01-13T22:45:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84801
copybara-service[bot],Disable failed test.,Disable failed test.,2025-01-13T22:25:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84800
copybara-service[bot],Format one constraint for stablehlo.scatter operation.,Format one constraint for stablehlo.scatter operation.,2025-01-13T21:57:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84799
copybara-service[bot],add PJRT runtime profiling library,add PJRT runtime profiling library,2025-01-13T21:49:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84798
copybara-service[bot],Use `@com_google_googletest//:gtest_main` instead of `tsl/platform:test_main`,Use `//:gtest_main` instead of `tsl/platform:test_main`,2025-01-13T21:46:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84797
copybara-service[bot],[xla:gpu] Move XLA:GPU runtime to xla/backends/gpu,[xla:gpu] Move XLA:GPU runtime to xla/backends/gpu,2025-01-13T21:32:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84796
copybara-service[bot],Pass stablehlo-ext-prepare-for-hlo-export : Migrate from MHLO to StableHLO,Pass stablehloextprepareforhloexport : Migrate from MHLO to StableHLO,2025-01-13T21:12:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84795
copybara-service[bot],"In preparation for the upcoming JAX support for the `StringDType`, this CL makes two minor tweaks to the `BasicStringArray` class (the string array implementation in the PjRt-IFRT backend):  (1) `CopyToHostBuffer` now supports the host buffer semantics of `kImmutableUntilTransferCompletes`. (2) `FullyReplicated` now works with `ConcreteSharding`.","In preparation for the upcoming JAX support for the `StringDType`, this CL makes two minor tweaks to the `BasicStringArray` class (the string array implementation in the PjRtIFRT backend):  (1) `CopyToHostBuffer` now supports the host buffer semantics of `kImmutableUntilTransferCompletes`. (2) `FullyReplicated` now works with `ConcreteSharding`.",2025-01-13T21:04:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84794
copybara-service[bot],Use buffer offset when the mlir module size is greater than 2GB,"Use buffer offset when the mlir module size is greater than 2GB flatbuffer_export., this is expensive on HWM usage. We can do better by analyzing the module size beforehand.",2025-01-13T20:25:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84793
copybara-service[bot],Decrease Linux CPU wheel limit size to 260M.,Decrease Linux CPU wheel limit size to 260M.,2025-01-13T20:19:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84792
copybara-service[bot],PR #21223: [nfc] Cleanup build files for expander transforms,"PR CC(Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.): [nfc] Cleanup build files for expander transforms Imported from GitHub PR https://github.com/openxla/xla/pull/21223 This is part3 of CC(pywrap_tensorflow_internal cannot load/not found  Windows tensorflow CPU) and CC(Make protocol used in estimator customizable.). Motivation: Smaller build files, fewer merge conflicts, and convinience in development (more intuitive targets). For discussion surrounding this, check thread in CC(pywrap_tensorflow_internal cannot load/not found  Windows tensorflow CPU). Copybara import of the project:  7832f8483091b70214d17789c05c38959d67a515 by Shraiysh Vaishay : [nfc] Cleanup build files for expander transforms This is part3 of CC(pywrap_tensorflow_internal cannot load/not found  Windows tensorflow CPU) and CC(Make protocol used in estimator customizable.). Motivation: Smaller build files, fewer merge conflicts, and convinience in development (more intuitive targets). For discussion surrounding this, check thread in CC(pywrap_tensorflow_internal cannot load/not found  Windows tensorflow CPU). Merging this change closes CC(Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21223 from shraiysh:cleanup_expander_transforms 7832f8483091b70214d17789c05c38959d67a515",2025-01-13T20:14:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84791
copybara-service[bot],Increase wheel limit size up to 270M for a temporary nightlies fix.,Increase wheel limit size up to 270M for a temporary nightlies fix.,2025-01-13T20:13:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84790
copybara-service[bot],Generate a build identifier for external tensor rearragement files,Generate a build identifier for external tensor rearragement files,2025-01-13T20:05:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84789
copybara-service[bot],[xla:cpu:nanort] Suppress msan warnings from uninitialized output buffers,[xla:cpu:nanort] Suppress msan warnings from uninitialized output buffers,2025-01-13T20:00:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84788
copybara-service[bot],Integrate LLVM at llvm/llvm-project@7aebacbee965,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 7aebacbee965,2025-01-13T19:56:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84787
copybara-service[bot],PR #21375: [ds-fusion] Get While loop analysis with copy fusion,"PR CC(Raspberry Pi install command not properly formatted.): [dsfusion] Get While loop analysis with copy fusion Imported from GitHub PR https://github.com/openxla/xla/pull/21375 In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index. Copybara import of the project:  ae85690876a106c4d74715fed299779e29e8e641 by Shraiysh Vaishay : [dsfusion] Get While loop analysis with copy fusion In later stages of optimization, there are instances of copy fusion on the parameter of the while body. With this, we need to allow inlining of fusions while getting the induction variable index, otherwise we cannot deduce the tuple index. Merging this change closes CC(Raspberry Pi install command not properly formatted.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21375 from shraiysh:while_loop_analysis ae85690876a106c4d74715fed299779e29e8e641",2025-01-13T19:47:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84786
copybara-service[bot],Reverts 49b2a877907e02bc592edd6abb9abcda6fc36247,Reverts 49b2a877907e02bc592edd6abb9abcda6fc36247,2025-01-13T19:35:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84785
copybara-service[bot],CompiledModel: Refactor lock usage,"CompiledModel: Refactor lock usage Instead of using misleading ScopedLock, maintain vector of LiteRtTensorBuffer and absl::Cleanup to unlock explicitly.",2025-01-13T19:19:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84784
kossivi4,Tensorflow importing error please any help,"Traceback (most recent call last):   File ""C:\anaconda\envs\Mymachenv\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 62, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File """", line 1, in    File ""C:\anaconda\envs\Mymachenv\lib\sitepackages\tensorflow\__init__.py"", line 37, in      from tensorflow.python.tools import module_util as _module_util   File ""C:\anaconda\envs\Mymachenv\lib\sitepackages\tensorflow\python\__init__.py"", line 36, in      from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   File ""C:\anaconda\envs\Mymachenv\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 77, in      raise ImportError( ImportError: Traceback (most recent call last):   File ""C:\anaconda\envs\Mymachenv\lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 62, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",2025-01-13T19:14:50Z,type:build/install,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84783,"Hi **** , Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios: ```  You need to install the MSVC 2019 redistributable  Your CPU does not support AVX2 instructions  Your CPU/Python is on 32 bits  There is a library that is in a different location/not installed on your system that cannot be loaded. ``` Also kindly provide the environment details and the steps followed to install the tensorflow. CC(Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.) Thank you!","Hi  , This type of error in TensorFlow typically arises from compatibility issues or missing dependencies. You can try these common solutions: 1. Ensure your TensorFlow version is compatible with your Python version and operating system. You can refer to official documentation for compatibility matrix. 2. Try Reinstalling TensorFlow using pip or conda. 3. Isolate TensorFlow and its dependencies (virtual environment), this will prevent conflicts with other packages. 4. TensorFlow might require specific Visual C++ Redistributable packages. Install Microsoft Visual C++ Redistributable. 5. Check you GPU drivers are compatible and uptodate. Install correct versions of CUDA and cuDNN compatible with your TensorFlow version and GPU. 6. Verify that the PATH environment variable includes the directories where TensorFlow's DLLs are located. 7. After making any changes, restart your terminal or IDE to ensure the changes take effect.",This is a duplicate of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) Closing unless you have information that is different from that.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Regenerate pyi stubs with absl::Span imports included,Regenerate pyi stubs with absl::Span imports included,2025-01-13T18:24:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84782
copybara-service[bot],Reverts 49b2a877907e02bc592edd6abb9abcda6fc36247,Reverts 49b2a877907e02bc592edd6abb9abcda6fc36247,2025-01-13T18:16:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84781
copybara-service[bot],Fix an overflow issue in TransposePlan,Fix an overflow issue in TransposePlan,2025-01-13T18:10:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84780
copybara-service[bot],[XLA:GPU] Enable vectorization of the indices operand for scatter.,[XLA:GPU] Enable vectorization of the indices operand for scatter.,2025-01-13T17:52:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84779
copybara-service[bot],[XLA:CPU] Implement deserialization from proto to thunks,[XLA:CPU] Implement deserialization from proto to thunks,2025-01-13T17:48:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84778
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-13T17:37:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84777
copybara-service[bot],Implement CHLO->StableHLO ragged_dot mode 1 decomposition.,Implement CHLO>StableHLO ragged_dot mode 1 decomposition.,2025-01-13T17:34:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84776
copybara-service[bot],[XLA] HLO Pass Unit Testing: Add `RunAndFilecheckHloRewrite` API version that takes HLO module with interleaved // CEHCK lines as an input.,[XLA] HLO Pass Unit Testing: Add `RunAndFilecheckHloRewrite` API version that takes HLO module with interleaved // CEHCK lines as an input.,2025-01-13T17:30:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84775
copybara-service[bot],[XLA:Python] Fix bug introduced in https://github.com/openxla/xla/pull/21265,[XLA:Python] Fix bug introduced in https://github.com/openxla/xla/pull/21265 which meant that we never actually populated C++ pmap cache entries.,2025-01-13T17:10:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84774
copybara-service[bot],Add the Windows 2022 Dockerfile.,Add the Windows 2022 Dockerfile.,2025-01-13T17:06:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84773
copybara-service[bot],Integrate StableHLO at openxla/stablehlo@b2d36c56,Integrate StableHLO at openxla/stablehlo,2025-01-13T17:02:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84772
copybara-service[bot],[XLA:GPU] Fix the unpack dim calculation for I4 rewrite with non major_2_minor layouts,[XLA:GPU] Fix the unpack dim calculation for I4 rewrite with non major_2_minor layouts,2025-01-13T16:59:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84771
copybara-service[bot],Fix a bug where creating a TFRT buffer may access an unset byte_strides object when the layout is packed.,Fix a bug where creating a TFRT buffer may access an unset byte_strides object when the layout is packed.,2025-01-13T16:37:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84770
copybara-service[bot],Perform Set key operation first in the Exchange Topology to keep existing behavior unchanged.,"Perform Set key operation first in the Exchange Topology to keep existing behavior unchanged. Only when coordination_agent_recoverable is set, it tries to reconnect to the cluster and would lead to AlreadyExists error. In this case the already_existing error can be handled by checking the existing topology is same as the new one.",2025-01-13T16:22:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84769
copybara-service[bot],Reverts 7869999086e70e24c0d1bda491d80cd28b127866,Reverts 7869999086e70e24c0d1bda491d80cd28b127866,2025-01-13T16:19:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84768
copybara-service[bot],[XLA:GPU] Enable int4 matmul rewriting with Triton MLIR rewriter by default.,[XLA:GPU] Enable int4 matmul rewriting with Triton MLIR rewriter by default.,2025-01-13T15:41:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84767
copybara-service[bot],[xla:cpu:nanort] Add microbenchmark for NanoRt IFRT Client,[xla:cpu:nanort] Add microbenchmark for NanoRt IFRT Client  Benchmark                  Time             CPU   Iterations  BM_IfRtAddScalars        307 ns          307 ns      2334921 ~120 ns out of 307 spent in constructing ifrt::Arrays from host buffers.,2025-01-13T14:19:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84766
copybara-service[bot],PR #19699: Explicit stream annotation: Set ExecutionStreamId based on frontend attribute,PR CC([Intel MKL] Finished support for bad usernames in the CI build scripts.): Explicit stream annotation: Set ExecutionStreamId based on frontend attribute Imported from GitHub PR https://github.com/openxla/xla/pull/19699 This PR picks up the stream annotation frontend attribute on async methods and assigns the matching ExecutionStreamId. This PR is another part of breaking up PR CC(Add parallel implementation of CTC greedy decoder). Copybara import of the project:  db0c310c9ab2df25ce6537618a15fbb2ec3122e8 by chaserileyroberts : Explicit streams are picked up in stream assignment. Merging this change closes CC([Intel MKL] Finished support for bad usernames in the CI build scripts.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19699 from chaserileyroberts:chase/runtime_explicit_streams db0c310c9ab2df25ce6537618a15fbb2ec3122e8,2025-01-13T13:59:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84765
copybara-service[bot],PR #20633: Improve the error message of the host out-of-memory,"PR CC(Delete 3_datasets.ipynb): Improve the error message of the host outofmemory Imported from GitHub PR https://github.com/openxla/xla/pull/20633 When working on weight offloading and activation offloading for MaxText Llama27B on a GH200, a host memory Out of Memory (OOM) error occurred as a large amount of memory was offloaded from the device to host memory. This CL clarifies that it was a host OOM, not a device OOM, and suggests using the environment variable XLA_PJRT_GPU_HOST_MEMORY_LIMIT_GB to increase the host memory limit. Copybara import of the project:  e38fac1d00c13185cbb96972814ddc45b0508cd8 by Jane Liu : Improve the error message of the host outofmemory. Merging this change closes CC(Delete 3_datasets.ipynb) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20633 from zhenyingliu:hostOOM e38fac1d00c13185cbb96972814ddc45b0508cd8",2025-01-13T13:56:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84764
copybara-service[bot],Move emitters to directory xla/backends/gpu/codegen/emitters.,Move emitters to directory xla/backends/gpu/codegen/emitters.,2025-01-13T13:08:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84763
copybara-service[bot],[XLA:GPU] Dumping unoptimized HLO snapshots should not trigger dumping of all available information for the HLO module.,[XLA:GPU] Dumping unoptimized HLO snapshots should not trigger dumping of all available information for the HLO module.,2025-01-13T12:43:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84762
copybara-service[bot],Support loading unoptimized HLO snapshot with arguments.,"Support loading unoptimized HLO snapshot with arguments. This should allow us to reproduce hlo runs with the same arguments and, specifically, run benchamkrs that are arguments dependent.",2025-01-13T12:41:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84761
copybara-service[bot],PR #21271: [ds-fusion] Move the resource requests class into a dedicated header,PR CC(tf.nn.softmax_cross_entropy_with_logits_v2 returns wrong value with soft labels): [dsfusion] Move the resource requests class into a dedicated header Imported from GitHub PR https://github.com/openxla/xla/pull/21271 This patch moves the resource requests class outside the gpu_executable. This is required because it will be used for thunk initializations in testing thunks. Also renamed the virtual class to `ResourceRequestsInterface` to avoid confusion with the concrete class. Copybara import of the project:  fd0c23223388dd455d7302f5a556c8838f6f9e21 by Shraiysh Vaishay : [dsfusion] Move the resource requests class into a dedicated header This patch moves the resource requests class outside the gpu_executable. This is required because it will be used for thunk initializations in testing thunks. Also renamed the virtual class to `ResourceRequestsInterface` to avoid confusion with the concrete class. Merging this change closes CC(tf.nn.softmax_cross_entropy_with_logits_v2 returns wrong value with soft labels) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21271 from shraiysh:resource_requests_refactor fd0c23223388dd455d7302f5a556c8838f6f9e21,2025-01-13T12:07:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84759
copybara-service[bot],Integrate LLVM at llvm/llvm-project@b270525f730b,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match b270525f730b,2025-01-13T12:03:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84758
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-13T11:58:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84756
copybara-service[bot],[XLA:GPU] Delete no-op logic for constructing collective combiner keys.,[XLA:GPU] Delete noop logic for constructing collective combiner keys. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19699 from chaserileyroberts:chase/runtime_explicit_streams db0c310c9ab2df25ce6537618a15fbb2ec3122e8,2025-01-13T11:48:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84754
copybara-service[bot],PR #21343: Re-apply PR #21213: [GPU] Fix mutex locking of a cuDNN handle.,PR CC(Tensorboard cuts off tops of graphs): Reapply PR CC(Passing/Retrieving String data to/from TFLite model): [GPU] Fix mutex locking of a cuDNN handle. Imported from GitHub PR https://github.com/openxla/xla/pull/21343 PR CC(Passing/Retrieving String data to/from TFLite model) got accidentally overwritten due to a merge conflict with PR CC(Typo in tf.Session fixed). Copybara import of the project:  f04617f665a6a3c61e76fd4c9e9ebbfb720741bb by Ilia Sergachev : Reapply PR CC(Passing/Retrieving String data to/from TFLite model): [GPU] Fix mutex locking of a cuDNN handle. PR CC(Passing/Retrieving String data to/from TFLite model) got accidentally overwritten due to a merge conflict with PR CC(Typo in tf.Session fixed). Merging this change closes CC(Tensorboard cuts off tops of graphs) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21343 from openxla:fix_cudnn_mutex f04617f665a6a3c61e76fd4c9e9ebbfb720741bb,2025-01-13T11:10:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84753
copybara-service[bot],PR #20948: [GPU] Fix NCCL with CUDA 12.6.3.,PR CC(How can I do type cast:  Tensor  to  Tensor ? ): [GPU] Fix NCCL with CUDA 12.6.3. Imported from GitHub PR https://github.com/openxla/xla/pull/20948 Copybara import of the project:  bc566f263b10c2fffe14e0f8007f60983509e02e by Ilia Sergachev : [GPU] Fix NCCL with CUDA 12.6.3. Merging this change closes CC(How can I do type cast:  Tensor  to  Tensor ? ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20948 from openxla:fix_nccl_cuda_1263 bc566f263b10c2fffe14e0f8007f60983509e02e,2025-01-13T11:08:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84752
copybara-service[bot],PR #20633: Improve the error message of the host out-of-memory,"PR CC(Delete 3_datasets.ipynb): Improve the error message of the host outofmemory Imported from GitHub PR https://github.com/openxla/xla/pull/20633 When working on weight offloading and activation offloading for MaxText Llama27B on a GH200, a host memory Out of Memory (OOM) error occurred as a large amount of memory was offloaded from the device to host memory. This CL clarifies that it was a host OOM, not a device OOM, and suggests using the environment variable XLA_PJRT_GPU_HOST_MEMORY_LIMIT_GB to increase the host memory limit. Copybara import of the project:  e38fac1d00c13185cbb96972814ddc45b0508cd8 by Jane Liu : Improve the error message of the host outofmemory. Merging this change closes CC(Delete 3_datasets.ipynb) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20633 from zhenyingliu:hostOOM e38fac1d00c13185cbb96972814ddc45b0508cd8",2025-01-13T10:53:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84751
copybara-service[bot],[XLA:GPU] Fix broken build,[XLA:GPU] Fix broken build,2025-01-13T10:49:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84749
copybara-service[bot],PR #19699: Explicit stream annotation: Set ExecutionStreamId based on frontend attribute,PR CC([Intel MKL] Finished support for bad usernames in the CI build scripts.): Explicit stream annotation: Set ExecutionStreamId based on frontend attribute Imported from GitHub PR https://github.com/openxla/xla/pull/19699 This PR picks up the stream annotation frontend attribute on async methods and assigns the matching ExecutionStreamId. This PR is another part of breaking up PR CC(Add parallel implementation of CTC greedy decoder). Copybara import of the project:  db0c310c9ab2df25ce6537618a15fbb2ec3122e8 by chaserileyroberts : Explicit streams are picked up in stream assignment. Merging this change closes CC([Intel MKL] Finished support for bad usernames in the CI build scripts.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19699 from chaserileyroberts:chase/runtime_explicit_streams db0c310c9ab2df25ce6537618a15fbb2ec3122e8,2025-01-13T10:31:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84746
copybara-service[bot],PR #19462: Add ExplicitStreamAnnotationAsyncWrapper pass,"PR CC(Fix discrepancies between doc and implementation for math_ops): Add ExplicitStreamAnnotationAsyncWrapper pass Imported from GitHub PR https://github.com/openxla/xla/pull/19462 This PR introduces a new pass `ExplicitStreamAnnotationAsyncWrapper`. This pass takes `kCall` instructions that are annotated with the frontend attribute `xla_gpu_experimental_stream_annotation`, and wraps the call with an async startdone pair. Initially, I tried to integrate this with the existing `StreamAttributeAnnotator` and `StreamAttributeAsyncWrapper` passes, but much of that code is specifically just for the `windowedeinsum` logic, and users wont necessarily want both enabled at the same time. Thus, I found it cleaner to instead just make a new pass. Copybara import of the project:  4873de9e28551ace7be0ab7e15f5d6e31e17a6cb by chaserileyroberts : Added new async wrap pass Merging this change closes CC(Fix discrepancies between doc and implementation for math_ops) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19462 from chaserileyroberts:chase/stream_async_wrap 4873de9e28551ace7be0ab7e15f5d6e31e17a6cb",2025-01-13T10:18:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84745
copybara-service[bot],Rename MlirFusionEmitterBase to EmitterBase and move the code (NFC),Rename MlirFusionEmitterBase to EmitterBase and move the code (NFC) The code is moved to xla/backends/gpu/codegen/emitters directory.,2025-01-13T10:00:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84744
copybara-service[bot],PR #21301: Document the xla_gpu_sharded_autotuning flag,"PR CC(After TensorFlow 1.1.0, libtensorflow_inference.so error: undefined reference to 'TF_NewGraph'): Document the xla_gpu_sharded_autotuning flag Imported from GitHub PR https://github.com/openxla/xla/pull/21301 Copybara import of the project:  54d932d3204174c087be84ed636ede6c9020aae3 by Dimitris Vardoulakis : Document the xla_gpu_sharded_autotuning flag Merging this change closes CC(After TensorFlow 1.1.0, libtensorflow_inference.so error: undefined reference to 'TF_NewGraph') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21301 from dimvar:documentshardautotuning 54d932d3204174c087be84ed636ede6c9020aae3",2025-01-13T09:48:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84742
alekstheod,Develop upstream sync 20250113,"Weekly sync 20250113 Unmerged paths:   (use ""git add ..."" to mark resolution)         both modified:   .bazelrc         both modified:   tensorflow/core/common_runtime/gpu/gpu_device_test.:   tensorflow/core/kernels/matmul_op_fused.:   tensorflow/core/kernels/matmul_op_impl.h         both modified:   tensorflow/core/kernels/matmul_util.:   tensorflow/core/kernels/matmul_util.h         both modified:   third_party/gpus/rocm_configure.bzl         both modified:   third_party/xla/third_party/tsl/third_party/gpus/rocm_configure.bzl         both modified:   third_party/xla/xla/service/gpu/fusions/triton/dot_algorithms_test.:   third_party/xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.:   third_party/xla/xla/tests/BUILD",2025-01-13T09:36:26Z,size:XL,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84741
copybara-service[bot],PR #21301: Document the xla_gpu_sharded_autotuning flag,"PR CC(After TensorFlow 1.1.0, libtensorflow_inference.so error: undefined reference to 'TF_NewGraph'): Document the xla_gpu_sharded_autotuning flag Imported from GitHub PR https://github.com/openxla/xla/pull/21301 Copybara import of the project:  54d932d3204174c087be84ed636ede6c9020aae3 by Dimitris Vardoulakis : Document the xla_gpu_sharded_autotuning flag Merging this change closes CC(After TensorFlow 1.1.0, libtensorflow_inference.so error: undefined reference to 'TF_NewGraph') FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21301 from dimvar:documentshardautotuning 54d932d3204174c087be84ed636ede6c9020aae3",2025-01-13T09:31:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84740
codinglover222,target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize fail to build ,"docker run it v $PWD:/tmp w /tmp tensorflow/build:2.19python3.10  bash c ""bazel build experimental_action_cache_store_output_metadata disk_cache=~/.cache/bazel  jobs=3 config=linux   //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize""  Error message is  /usr/include/c++/13/bits/unique_ptr.h:1070:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous  1070        ^~~~~~~~~~~~~~~~~~~~~~ tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:54:7: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(mlir::TFL::{anonymous}::DefaultQuantParamsPass&&)' (deleted) Target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 407.917s, Critical Path: 206.23s INFO: 43 processes: 7 internal, 36 local. FAILED: Build did NOT complete successfully",2025-01-13T09:25:23Z,stat:awaiting response comp:lite,closed,0,3,https://github.com/tensorflow/tensorflow/issues/84739,"Hi **** , Can you please fill the issue template.Also can you please elaborate about your Feature and please specify the Use Cases for this feature. Thank you!",Thanks  I created Issue CC(target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize fail to build) should I close this one?,"Hi **** , Thank you for providing all the information. Yes, please feel free to close this issue as it is already being tracked in another issue opened by you. Thank you!"
copybara-service[bot],PR #19451: Setting xla_gpu_multi_streamed_windowed_einsum to true by default,PR CC(Feature Request: evaluate both train_data and test_data using tf.contrib.learn.Experiment?): Setting xla_gpu_multi_streamed_windowed_einsum to true by default Imported from GitHub PR https://github.com/openxla/xla/pull/19451 We are trying to deprecate xla_gpu_multi_streamed_windowed_einsum  since we always have better perf with it enabled. This is the first pr to enable it by default to test for stability. Copybara import of the project:  808a9cc0af8901d36a3c219bdf19f38323d01bf3 by Tj Xu : Turn xla_gpu_multi_streamed_windowed_einsum on by default  8221fc4481773f457f5e0235625be22f255fe75b by TJ Xu : Add an option to StreamAttributeAnnotator to skip annotating copystart and async DUS Don't annotate copystart and async DUS when the pass is run before remat  352c1c593b9dcd895f123dea4f7c38e44a787ae6 by TJ Xu : Remove the option to skip annotating copy start and inpect if the module has schedule  257ff6768b59fc7c47c04fa5faa524399f74c80e by TJ Xu : Address rollback by disabling a2a rewrite by default  d3bafebdc0961d61384a49616c29cb9bb6c59db9 by TJ Xu : reverted new flag changes Merging this change closes CC(Feature Request: evaluate both train_data and test_data using tf.contrib.learn.Experiment?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19451 from Tixxx:tixxx/remove_multi_stream_flag d3bafebdc0961d61384a49616c29cb9bb6c59db9,2025-01-13T09:13:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84738
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-13T08:11:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84730
copybara-service[bot],PR #73149: Example usage is created for tf.sets.size in sets_impl.py,PR CC(Example usage is created for tf.sets.size in sets_impl.py): Example usage is created for tf.sets.size in sets_impl.py Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/73149 Example is provided for tf.sets.size in sets_impl.py Thank You Copybara import of the project:  bf84bda7a94609110b6752ef5d6d84801a167575 by LakshmiKalaKadali : Example usage is created for tf.sets.size in sets_impl.py Example is provided for tf.sets.size in sets_impl.py Merging this change closes CC(Example usage is created for tf.sets.size in sets_impl.py) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/73149 from tensorflow:LakshmiKalaKadalipatch4 bf84bda7a94609110b6752ef5d6d84801a167575,2025-01-13T07:28:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84722
copybara-service[bot],PR #63959: Typos are fixed in quantization_debugger.ipynb,PR CC(Typos are fixed in quantization_debugger.ipynb): Typos are fixed in quantization_debugger.ipynb Imported from GitHub PR https://github.com/tensorflow/tensorflow/pull/63959 Typos and Grammatical errors are fixed Copybara import of the project:  bc8a6549529194af0ed5d5e86d93fe8d0652c10e by LakshmiKalaKadali : Typos are fixed in quantization_debugger.ipynb Typos and Grammatical errors are fixed Merging this change closes CC(Typos are fixed in quantization_debugger.ipynb) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/63959 from tensorflow:LakshmiKalaKadalipatch3 bc8a6549529194af0ed5d5e86d93fe8d0652c10e,2025-01-13T07:23:41Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/84721,Check out this pull request on&nbsp;    See visual diffs & provide feedback on Jupyter Notebooks.    Powered by ReviewNB
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-13T07:22:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84719
copybara-service[bot],PR #20739: Arch specific FP8 SDPA test,"PR CC(Tensorflow lite, invalid quantization ranges): Arch specific FP8 SDPA test Imported from GitHub PR https://github.com/openxla/xla/pull/20739 The workspace size required by CuDNN are different on Hopper and Blackwell. To make the HLO string arch agnostic, pin the workspace size to 0. Copybara import of the project:  f9eafadfdb69d3e68ba6c186979f65dd40e45000 by shuw : Fix fp8 SDPA test workspace to 0 Merging this change closes CC(Tensorflow lite, invalid quantization ranges) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20739 from wenscarl:arch_spec_fp8_sdpa_test f9eafadfdb69d3e68ba6c186979f65dd40e45000",2025-01-13T06:56:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84714
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-13T05:37:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84704
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-13T05:25:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84703
copybara-service[bot],Fix --cpu=k8 mapping,Fix cpu=k8 mapping,2025-01-13T04:17:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84702
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-13T03:45:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84700
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-13T03:39:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84699
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-13T03:00:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84698
copybara-service[bot],Reverts d42f44f9c6912bd26a0810f004a4cd8527b9d9fd,Reverts d42f44f9c6912bd26a0810f004a4cd8527b9d9fd,2025-01-12T23:50:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84697
axlrommel,enhancement: add sonargit pr metrics,"This PR introduces a GitHub workflow that leverages the SonarGit Action to collect pull request metrics such as open times, merge rates, and change failure rates. These metrics provide actionable insights to help improve the repository's development workflow. Currently, the workflow logs PR information to the console. Optionally, SonarGit offers a dashboard to visualize the data for deeper analysis—and it’s completely free for life! I’m doing this to help the developer community and to get my name out there. If you’d like more information or assistance in setting this up, feel free to reach out.",2025-01-12T22:41:52Z,size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84696
copybara-service[bot],[XLA:Python] Add locking around the pytree registry in free threading mode.,[XLA:Python] Add locking around the pytree registry in free threading mode. Fixes tsan races from JAX test suite under free threading.,2025-01-12T18:11:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84695
copybara-service[bot],PR #21265: Attempt to add pmap free-threading support,PR CC(//tensorflow/python/kernel_tests:conv_ops_test fails on AVX512 builds): Attempt to add pmap freethreading support Imported from GitHub PR https://github.com/openxla/xla/pull/21265 Description:  A tentative to add freethreading to pmap_lib Copybara import of the project:  d2f5df9c0decdb7e55a2013f5506dee6fc358298 by vfdev5 : WIP Merging this change closes CC(//tensorflow/python/kernel_tests:conv_ops_test fails on AVX512 builds) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21265 from vfdev5:pmapfreethreadingsupport d2f5df9c0decdb7e55a2013f5506dee6fc358298,2025-01-12T17:15:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84694
copybara-service[bot],Update nanobind to a commit that includes,Update nanobind to a commit that includes * https://github.com/wjakob/nanobind/commit/20a367a056d39970ef3cd3bcbd86ccc839828f0a * https://github.com/wjakob/nanobind/commit/27ba245d82babdd7f504977a1dff25adff00eabf which are two commits that fix race conditions under Python 3.13 freethreading.,2025-01-12T16:06:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84693
Cyprian-igban,tensorflow," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.8  Custom code Yes  OS platform and distribution windows  Mobile device windows  Python version 3.9  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? unable to load tensorflow as tf  Standalone code to reproduce the issue ```shell  Import necessary libraries import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense from tensorflow.keras.datasets import fashion_mnist from tensorflow.keras.utils import to_categorical  Load and preprocess the dataset (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255 x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255 y_train = to_categorical(y_train, 10) y_test = to_categorical(y_test, 10)  Define the CNN model model = Sequential([     Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),     MaxPooling2D((2, 2)),     Conv2D(64, (3, 3), activation='relu'),     MaxPooling2D((2, 2)),     Flatten(),     Dense(128, activation='relu'),     Dense(10, activation='softmax') ])  Compile the model model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  Train the model model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=32)  Evaluate the model test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2) print(f""Test Accuracy: {test_acc:.2f}"") ```  Relevant log output ```shell ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[1], line 1 > 1 import tensorflow as tf       3 a = tf.constant(2)       4 b = tf.constant(3) File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:85      83     sys.setdlopenflags(_default_dlopen_flags)      84 except ImportError: > 85   raise ImportError(      86       f'{traceback.format_exc()}'      87       f'\n\nFailed to load the native TensorFlow runtime.\n'      88       f'See https://www.tensorflow.org/install/errors '      89       f'for some common causes and solutions.\n'      90       f'If you need help, create an issue '      91       f'at https://github.com/tensorflow/tensorflow/issues '      92       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\lenovo\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. ```",2025-01-12T10:37:11Z,type:build/install,closed,0,3,https://github.com/tensorflow/tensorflow/issues/84692,hey igban check out CC(ImportError: DLL load failed while importing _pywrap_tensorflow_internal:),Duplicate of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.),Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-12T06:15:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84691
copybara-service[bot],Add a utility pass for writing atom programs and main IFRT func to files.,Add a utility pass for writing atom programs and main IFRT func to files.,2025-01-12T04:57:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84690
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-12T03:34:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84689
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-12T03:32:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84688
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-12T03:25:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84687
advaitpatel,"optimizing layers utilities by improving runtime, memory and readability","This PR introduces several optimizations and enhancements to the layer utilities module, improving runtime performance, memory efficiency, and overall code readability. These changes aim to streamline operations, reduce redundancy, and ensure better maintainability. The code has not been updated in the past 2 years. List of the changes:  introduced VALID_DATA_FORMATS and VALID_PADDING constants to replace repetitive string comparisons. Used set membership checks for faster validation.  combined tuple validation and conversion into a single step.  improved error handling and error messages for clarity.  added lazy evaluation for `true_fn` and `false_fn` using lambdas, ensuring functions are only executed when needed.  simplified checks for boolean and integer inputs to avoid unnecessary conditions.",2025-01-12T00:10:46Z,size:M,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84686,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",I have signed the CLA,refactored code based on the above PR review feedback  refactored convert_data_format to use matchcase for better readability and maintainability.  avoided unnecessary dictionary creation for a single get operation.  enhanced error messages to provide more clarity to users.,closing this for now. and will raise a new one
copybara-service[bot],"Makes keyword arguments of functions loaded from TF1 SavedModels be treated as `POSITIONAL_OR_KEYWROD` (instead of `KEYWORD_ONLY`) arguments by `FunctionType`, so that `FunctionType` won't mistakenly change their order (which can lead to an order mismatch with the underlying TF Graph when calling or re-saving the function).","Makes keyword arguments of functions loaded from TF1 SavedModels be treated as `POSITIONAL_OR_KEYWROD` (instead of `KEYWORD_ONLY`) arguments by `FunctionType`, so that `FunctionType` won't mistakenly change their order (which can lead to an order mismatch with the underlying TF Graph when calling or resaving the function).",2025-01-11T20:44:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84685
copybara-service[bot],[xla:cpu:nanort] Use XLA/TSL utils when possible and add a few micro optimizations,[xla:cpu:nanort] Use XLA/TSL utils when possible and add a few micro optimizations Sprinkle std::move when appropriate and add ABSL_PREDICT_FALSE on unlikely branches. ``` name                old cpu/op   new cpu/op   delta BM_IfRtAddScalars   372ns ± 2%   355ns ± 3%  4.75%  (p=0.000 n=38+35) ```,2025-01-11T19:09:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84684
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T17:55:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84683
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T17:37:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84682
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T17:18:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84681
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T16:42:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84680
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T16:34:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84679
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T16:33:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84678
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T16:30:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84677
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T16:00:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84676
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-11T09:39:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84675
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:20:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84674
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:20:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84673
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:19:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84672
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:19:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84671
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:19:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84670
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:19:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84669
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:19:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84668
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:18:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84667
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:18:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84666
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:18:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84665
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:18:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84664
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:17:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84663
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:16:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84662
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:16:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84661
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:16:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84660
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:15:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84659
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:15:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84658
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:14:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84657
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:14:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84656
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:14:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84655
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T09:13:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84654
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-11T07:53:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84653
copybara-service[bot],Update users of TSL headers and targets to new location in XLA,Update users of TSL headers and targets to new location in XLA Updating:   `env.h`   `env_time.h`   `errors.h`   `file_statistics.h`   `file_system.h`   `file_system_helper.h`   `logging.h`   `macros.h`   `status.h`   `status_matchers.h`   `status_to_from_proto.h`   `statusor.h`   `test.h`   `test_benchmark.h`   `threadpool.h`   `threadpool_async_executor.h`   `threadpool_interface.h`   `threadpool_options.h`   `types.h` and associated targets.,2025-01-11T06:13:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84652
copybara-service[bot],fix a bug when partitioning scatter instruction with same operands,fix a bug when partitioning scatter instruction with same operands,2025-01-11T05:46:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84651
copybara-service[bot],[xla:pjrt] Add support for forwarding FFI context to C API client,"[xla:pjrt] Add support for forwarding FFI context to C API client + Correctly (zero/value)initialize PJRT_ExecuteOptions in tests and pjrt_c_api_client ``` If the number of initializer clauses is less than the number of members or initializer list is completely empty, the remaining members are valueinitialized ``` Context: https://github.com/openxla/xla/pull/20429",2025-01-11T05:13:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84650
copybara-service[bot],Internal relative changes only,Internal relative changes only,2025-01-11T04:54:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84649
codinglover222,Adjust the build config to an existing value defined in .bazelrc,"In .bazelrc, there is no build configuration called opt. Update to avoid build failure. ",2025-01-11T04:51:38Z,ready to pull size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84648
copybara-service[bot],PR #18838: [NVIDIA GPU] Support multi-operand collective-permute,"PR CC(Add DeviceSet to Cluster): [NVIDIA GPU] Support multioperand collectivepermute Imported from GitHub PR https://github.com/openxla/xla/pull/18838 For collectivepermutes with small message sizes, it is beneficial to combine them into a single collective because 1. it gets rid of some kernel launch overhead, and allows NCCL to do some message fusion; 2. fewer collectives make it easier for LHS to make better decision. In order to support combining collectivepermutes, we need to support multioperand collectivepermute first, a.k.a. the combined collectivepermute. This PR extends the existing CP interface by overloading it, so that a CP can have multiple operands. Copybara import of the project:  5e10aba5b8f6ae66d1071a1894a87987b6a5bceb by Terry Sun : support multioperand cp  170fead3de942f5e14f4936df1d76bf7e5e319d4 by Terry Sun : minor refactoring  0d85070baee3f26075f0b3660c4674d7b414c861 by Terry Sun : update python interface  9812a104822ea479d29fef0531b9e10d5c2a831d by Terry Sun : polish python interface  3a1552cbcd2e26f814373e0e01adbe8eceb3be9f by Terry Sun : formatting  d3657f81ac57dc1de86561b3449d051d178e0f75 by Terry Sun : formatting  9caacb4e84ac3bb580443afc76e048a6e264094a by Terry Sun : refactor overloading  0aff5e0a372af9e4a859b54681acf0501adca096 by Terry Sun : minor refactor  20a0e3d7dd57a7d70cffe20a1b35fb4b4c1e5c8a by Terry Sun : add parser test  e5ad9d9b601ada1c3984f34967c98574061bd043 by Terry Sun : fix merge issue Merging this change closes CC(Add DeviceSet to Cluster) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18838 from terryysun:terryysun/grouped_cp e5ad9d9b601ada1c3984f34967c98574061bd043",2025-01-11T04:15:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84646
copybara-service[bot],[xla:pjrt] Add PJRT_ExecuteContext pointer to PJRT_ExecuteOptions,[xla:pjrt] Add PJRT_ExecuteContext pointer to PJRT_ExecuteOptions For consistency with C++ API pass execute context in ExecuteOptions.,2025-01-11T04:15:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84645
copybara-service[bot],"OpenCL wrappers for device, command queue and buffer management","OpenCL wrappers for device, command queue and buffer management OpenCL loaders for various platforms.",2025-01-11T04:15:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84644
copybara-service[bot],[xla:pjrt] Implement default PJRT_ExecuteContext_Create in pjrt_c_api_wrapper_impl,[xla:pjrt] Implement default PJRT_ExecuteContext_Create in pjrt_c_api_wrapper_impl,2025-01-11T04:02:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84643
copybara-service[bot],PR #20924: Fix typo in the definition of XLA_PredicatedExtractOp,PR CC(Not able to build tensorflow lite iOS library): Fix typo in the definition of XLA_PredicatedExtractOp Imported from GitHub PR https://github.com/openxla/xla/pull/20924 Copybara import of the project:  b5fc4cb865855be6c653b269592931fe7a2c8fd1 by Dimitris Vardoulakis : Fix typo in the definition of XLA_PredicatedExtractOp Merging this change closes CC(Not able to build tensorflow lite iOS library) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20924 from dimvar:dvmisc b5fc4cb865855be6c653b269592931fe7a2c8fd1,2025-01-11T03:59:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84642
copybara-service[bot],PR #21022: [XLA:CPU][oneDNN] Modify addend shape for small Convolution + Bias contraction,"PR CC(Feature request : MoorePenrose pseudoinverse): [XLA:CPU][oneDNN] Modify addend shape for small Convolution + Bias contraction Imported from GitHub PR https://github.com/openxla/xla/pull/21022 For small constant biases, XLA may perform constant folding, which can alter the shape of the bias passed to oneDNN. This PR removes any extraneous trivial dimensions from the bias that is passed to the oneDNN library. It also adds a test in all applicable dtypes to test the functionality. Copybara import of the project:  5ce5ba690d5acd1ebedca1484483639b2f3b0be1 by Akhil Goel : Modify conv bias shape Merging this change closes CC(Feature request : MoorePenrose pseudoinverse) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21022 from Inteltensorflow:akhil/conv_bias_fix 5ce5ba690d5acd1ebedca1484483639b2f3b0be1",2025-01-11T02:27:04Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/84641,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],PR #20965: [GPU][NFC] Add new cuDNN status handling macro.,PR CC(Typo in tf.Session fixed): [GPU][NFC] Add new cuDNN status handling macro. Imported from GitHub PR https://github.com/openxla/xla/pull/20965 Copybara import of the project:  951a7f4c1cd0191f5e7260370fbef942d0ab76ca by Ilia Sergachev : [GPU][NFC] Add new cuDNN status handling macro. Merging this change closes CC(Typo in tf.Session fixed) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20965 from openxla:cudnn_status_macro 951a7f4c1cd0191f5e7260370fbef942d0ab76ca,2025-01-11T02:20:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84640
copybara-service[bot],Integrate LLVM at llvm/llvm-project@b302633bc5b9,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match b302633bc5b9,2025-01-11T02:07:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84639
copybara-service[bot],PR #21134: [XLA:GPU] Add profiler annotation for sequential thunk.,"PR CC(apply_gradients does not work with var_list): [XLA:GPU] Add profiler annotation for sequential thunk. Imported from GitHub PR https://github.com/openxla/xla/pull/21134 This PR wraps sequential thunk with profiler annotations, which will make loop iterations, and conditional branch more easy to read in the profiler.  The nsys profile looks like this:  !image Copybara import of the project:  eea74b86f5e2b71c915553ec302e16645927e191 by Shawn Wang : add nvtx marker for sequential thunk Merging this change closes CC(apply_gradients does not work with var_list) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21134 from shawnwang18:shawnw/add_profiler_for_while_body_cond eea74b86f5e2b71c915553ec302e16645927e191",2025-01-11T02:03:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84638
copybara-service[bot],PR #20808: [GSPMD] Partitions collective permute instructions in manual sharding group.,"PR CC(Feature requested by issue 18354: No gradient defined for operation DepthwiseConv2dNativeBackpropFilter ): [GSPMD] Partitions collective permute instructions in manual sharding group. Imported from GitHub PR https://github.com/openxla/xla/pull/20808 This is a small fix in GSPMD partitioning for partitioning collective permutes instructions added in manual sharding group. In JAX, we can add `ppermute` instruction in shard_map. In cases where we have shard_map with auto axes specified, collective permuting an operand even with the same sharding will end up with an `allgather` and then collective permute, which leads to inefficient collectives. The correct and efficient way is to partition the collective permute as an elementwise op. The unit test added provides a repro. Also, the JAX unit test in https://github.com/jaxml/jax/blob/fa9c7edf736516052df6eab22947bc627d0deca3/tests/shard_map_test.pyL2167 gives a realworld JAX example. Copybara import of the project:  8ee6ecd51f6e4aae8e3d92a6a439a60f53ab02ae by Yunlong Liu : A hacky fix on partitioning collective permute.  e50e87696defb290f7561a7808ee42ebbc11e144 by Yunlong Liu : Local change.  84eb38597c783a4488774823c2c464296a8c54c7 by Yunlong Liu : Simplifies sharding in tests. Merging this change closes CC(Feature requested by issue 18354: No gradient defined for operation DepthwiseConv2dNativeBackpropFilter ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20808 from yliu120:cp_sharding_2 84eb38597c783a4488774823c2c464296a8c54c7",2025-01-11T02:02:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84637
copybara-service[bot],PR #20340: Fix missing template value,"PR CC(reg. tensorflowgpu in amd windows): Fix missing template value Imported from GitHub PR https://github.com/openxla/xla/pull/20340 Fixes a bug introduced in this change: https://github.com/google/tsl/pull/2944 The change makes use of a template variable `%{compiler}`, that is not defined for this file. This causes the `fnocanonicalsystemheaders` option to be set for Clang builds, and Clang will fail with an error about that command line flag not being defined. Copybara import of the project:  75a3d3fbcf2ead55df3872aa80ff21ac3dd9336c by Charles Hofer : Fix missing template value  e08537b09200b0037db7a05780dea0d525399376 by Charles Hofer : Change flag to compiler_is_clang  373f359cbd8d02ee850d98fed92a7bbca4a09c1b by Charles Hofer : Fix typo  2be3c309d05f93a48dd9fdd06af8159108920516 by Harsha HS : [ROCm] Add cudaonly tags for nvidia profiler test Merging this change closes CC(reg. tensorflowgpu in amd windows) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20340 from ROCm:fixmissingtemplatevalue 2be3c309d05f93a48dd9fdd06af8159108920516",2025-01-11T01:55:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84636
copybara-service[bot],PR #20494: Update slop_factor flag desc in debug_options_flags.cc,PR CC(Why dense layer cannot be speed up in tf.contrib.trt ?): Update slop_factor flag desc in debug_options_flags.://github.com/openxla/xla/pull/20494 Copybara import of the project:  04a8e94d73c04e7ffcf3674698a5ad3063918703 by Sevin Varoglu : Update slop_factor flag desc in debug_options_flags. : Fix error  0347b54cdc337e6239f89baf69ba3f6d6c8f160c by Sevin Varoglu : Add default value Merging this change closes CC(Why dense layer cannot be speed up in tf.contrib.trt ?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20494 from sfvaroglu:sevin/update_comment 0347b54cdc337e6239f89baf69ba3f6d6c8f160c,2025-01-11T01:29:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84635
copybara-service[bot],Create a SourceTargetPairs class.,Create a SourceTargetPairs class.,2025-01-11T00:33:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84634
copybara-service[bot],PR #21213: [GPU] Fix mutex locking of a cuDNN handle.,"PR CC(Passing/Retrieving String data to/from TFLite model): [GPU] Fix mutex locking of a cuDNN handle. Imported from GitHub PR https://github.com/openxla/xla/pull/21213 The CudnnHandle object containing a mutex has to stay alive while cudnnHandle_t it guards is in use. This brings the use in sync with the other uses in this file. There is no evidence that this caused failures so far, rather prefetching potential problems, therefore no test added. Copybara import of the project:  04729723c06b5dd8e819d45290268bcde2c2ee00 by Ilia Sergachev : [GPU] Fix mutex locking of a cuDNN handle. The CudnnHandle object containing a mutex has to stay alive while cudnnHandle_t it guards is in use. This brings the use in sync with the other uses in this file. There is no evidence that this caused failures so far, rather prefetching potential problems, therefore no test added. Merging this change closes CC(Passing/Retrieving String data to/from TFLite model) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21213 from openxla:fix_cudnn_locking 04729723c06b5dd8e819d45290268bcde2c2ee00",2025-01-11T00:33:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84633
copybara-service[bot],PR #21192: [xla:cpu] Add XLA_VLOG_LINES to oneDNN rewriter passes,PR CC(Unimplemented cast int64 to string is not supported): [xla:cpu] Add XLA_VLOG_LINES to oneDNN rewriter passes Imported from GitHub PR https://github.com/openxla/xla/pull/21192 Enables logging when we set TF_CPP_MAX_VLOG_LEVEL and TF_CPP_MIN_LOG_LEVEL Copybara import of the project:  5f0a1883fc9638be4a47d3a3578fdbdb3f2352e1 by Crefeda Rodrigues : [xla:cpu] Add XLA_VLOG_LINES to oneDNN rewriter passes Signedoffby: Crefeda Rodrigues  Merging this change closes CC(Unimplemented cast int64 to string is not supported) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21192 from cfRod:onednnpasslogging 5f0a1883fc9638be4a47d3a3578fdbdb3f2352e1,2025-01-11T00:33:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84632
copybara-service[bot],Allows suboptimal solutions for partial mesh shapes when given a *hard* memory budget constraint.,Allows suboptimal solutions for partial mesh shapes when given a *hard* memory budget constraint.,2025-01-11T00:16:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84631
copybara-service[bot],Add idle and busy time for TPUs to OpStats.,Add idle and busy time for TPUs to OpStats. Add DutyCycleCombiner for handling intra and inter chip duty cycle aggregation. Fix DutyCycleTracker bugs with idleness and duplicate active times.,2025-01-11T00:08:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84630
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-10T23:09:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84629
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-10T23:08:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84628
copybara-service[bot],Apply proper version scripts to pywrap_library artifacts,Apply proper version scripts to pywrap_library artifacts,2025-01-10T23:07:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84627
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-10T23:05:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84626
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-10T23:05:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84625
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-10T23:05:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84624
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-10T23:05:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84623
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*` FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19451 from Tixxx:tixxx/remove_multi_stream_flag d3bafebdc0961d61384a49616c29cb9bb6c59db9,2025-01-10T23:05:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84622
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-10T23:05:02Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84621
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-10T23:04:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84620
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-10T23:04:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84619
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-10T23:04:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84618
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-10T23:04:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84617
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-10T23:04:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84616
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-10T23:03:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84615
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-10T23:01:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84614
copybara-service[bot],internal change only to update dependency visibility,internal change only to update dependency visibility FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19066 from Inteltensorflow:mabuzain/handleonednnscalar 576e244530ce0698de0b7137d8e93965fef9d528,2025-01-10T22:45:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84613
copybara-service[bot],Clarify error messaging for struct size check,"Clarify error messaging for struct size check This function is the struct size checker that's used only on the plugin side (and is only valid there since it's checking to see if the struct is greater than or equal). Make the error text it generates clear that the plugin version is later than the framework, and hence, is an unsupported combo.",2025-01-10T22:24:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84612
copybara-service[bot],[XLA:Python] Make sure we hold the lock on cache_ when destroying executables_ in PjitFunction.,"[XLA:Python] Make sure we hold the lock on cache_ when destroying executables_ in PjitFunction. cache_'s object lock protects executables_ under freethreading mode, so we have to hold the lock.",2025-01-10T22:14:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84611
copybara-service[bot],Reverts b164ad6269c98426b8ae8b1ce9b7926bf2691b11,Reverts b164ad6269c98426b8ae8b1ce9b7926bf2691b11,2025-01-10T21:50:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84610
copybara-service[bot],Add default condition to `selects` inside `uv.BUILD`,Add default condition to `selects` inside `uv.BUILD`,2025-01-10T21:35:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84609
copybara-service[bot],[XLA:Python] Use PyEval_SetProfileAllThreads to install the python profiler in all threads under Python 3.12+.,"[XLA:Python] Use PyEval_SetProfileAllThreads to install the python profiler in all threads under Python 3.12+. This API is threadsafe under Python 3.13 freethreading, not to mention simpler.",2025-01-10T21:25:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84608
copybara-service[bot],Create an IFRT wrapper around NanoRT.,"Create an IFRT wrapper around NanoRT. This will allow NanoRT to be easily used from a caller that depends on IFRT, but we can add faster ""passthrough"" APIs as needed when we encounter performance defects.",2025-01-10T21:08:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84607
copybara-service[bot],[XLA:Python] Migrate python profiler hooks to nanobind instead of pybind11.,"[XLA:Python] Migrate python profiler hooks to nanobind instead of pybind11. This is one of the few remaining users of pybind11 left in JAX. Change in preparation for supporting Python 3.13 freethreading mode in JAX. In passing, also remove a stale comment referencing pybind11 in xla/python.",2025-01-10T21:00:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84606
copybara-service[bot],[Coordination Service] Fix pjrt_c_api_gpu_test after introducing TryGet,[Coordination Service] Fix pjrt_c_api_gpu_test after introducing TryGet KV try_get functions should be linked in pjrt_client creation,2025-01-10T20:40:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84605
copybara-service[bot],Integrate LLVM at llvm/llvm-project@35e76b6a4fc7,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 35e76b6a4fc7,2025-01-10T20:00:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84604
copybara-service[bot],[xla:cpu] Micro-optimizations for ThunkExecutor,[xla:cpu] Microoptimizations for ThunkExecutor Keep inedges and outedges in a dense container to optimize data locality on a hot path. For smallish thunk sequences all out edges should fit into L1 cache. name                                   old cpu/op   new cpu/op   delta BM_SyncThunkExecutor/1/process_time    29.4ns ± 2%  29.6ns ± 2%  +0.81%   BM_SyncThunkExecutor/2/process_time     103ns ± 2%   101ns ± 3%  1.63%   BM_SyncThunkExecutor/4/process_time     173ns ± 3%   171ns ± 2%  1.10%   BM_SyncThunkExecutor/8/process_time     320ns ± 2%   317ns ± 2%  0.95%   BM_SyncThunkExecutor/16/process_time    652ns ± 2%   638ns ± 2%  2.21%   BM_SyncThunkExecutor/32/process_time   1.28µs ± 3%  1.25µs ± 5%  2.03%   BM_SyncThunkExecutor/64/process_time   2.71µs ± 6%  2.61µs ± 6%  3.73%   BM_SyncThunkExecutor/128/process_time  5.73µs ± 4%  5.41µs ± 3%  5.46%   BM_SyncThunkExecutor/256/process_time  12.0µs ± 3%  11.1µs ± 2%  6.81%   BM_SyncThunkExecutor/512/process_time  25.1µs ± 4%  23.1µs ± 3%  7.93%,2025-01-10T19:59:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84603
copybara-service[bot],[XLA:GPU][Emitters] Allow to vectorize 128 bits for scatter.,[XLA:GPU][Emitters] Allow to vectorize 128 bits for scatter.,2025-01-10T18:29:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84602
copybara-service[bot],[TF:TPU] Enable cast tests for recently added FP8 types.,"[TF:TPU] Enable cast tests for recently added FP8 types. This registers `float8_e4m3fnuz`, `float8_e4m3b11fnuz` and `float8_e5m2fnuz` as supported types for TF TPU devices.",2025-01-10T18:13:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84601
copybara-service[bot],Delete tfcompile documentation,Delete tfcompile documentation tfcompile is deprecated and will be eventually removed. Don't mention it in public documentation.,2025-01-10T18:08:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84600
copybara-service[bot],[XLA:GPU][Emitters] Allow unrolling loops that yield values defined above.,[XLA:GPU][Emitters] Allow unrolling loops that yield values defined above. The change upstream has been integrated.,2025-01-10T17:32:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84599
copybara-service[bot],Cleanup: Remove PjRtMemoryDescription in favor of MemoryKind.,Cleanup: Remove PjRtMemoryDescription in favor of MemoryKind.,2025-01-10T16:58:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84598
copybara-service[bot],Reverts a72d9bf92d333bff536f0b9d8eb05d7cff468023,Reverts a72d9bf92d333bff536f0b9d8eb05d7cff468023,2025-01-10T16:44:14Z,ready to pull,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84597
copybara-service[bot],[XLA:GPU] add fusion wrapper tool,[XLA:GPU] add fusion wrapper tool converts a file with a single pass to a module,2025-01-10T16:17:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84596
copybara-service[bot],Migrate tf lite owned parsers from mlir_roundtrip_flags to their directory.,Migrate tf lite owned parsers from mlir_roundtrip_flags to their directory.,2025-01-10T16:07:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84595
copybara-service[bot],Add JAX unit test for Shardy which causes the compiler to introduce the `mlir::tensor::TensorDialect`. This was causing the compiler to crash.,Add JAX unit test for Shardy which causes the compiler to introduce the `mlir::tensor::TensorDialect`. This was causing the compiler to crash.,2025-01-10T15:53:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84594
copybara-service[bot],#sdy fix bug due to tensor dialect being introduced,"sdy fix bug due to tensor dialect being introduced When investigating a bug, I discovered this fails in JAX: ```py NS = jax.sharding.NamedSharding P = jax.sharding.PartitionSpec mesh = jax.sharding.Mesh(         np.reshape(np.array(jax.devices()), (4,2)), ('data', 'model')) in_avals = (jax.ShapeDtypeStruct((4, 8), jnp.float32),) shardings = (NS(mesh, P('data',)),) (jax.jit, out_shardings=shardings) def gen_dummy_inputs():   return tuple(       jax.random.normal(           jax.random.key(42), shape=in_aval.shape       ).astype(in_aval.dtype)       for in_aval in in_avals   ) gen_dummy_inputs() ``` with the error ``` LLVM ERROR: Building op `tensor.cast` but it isn't known in this MLIRContext: the dialect may not be loaded or this operation hasn't been added by the dialect. See also https://mlir.llvm.org/getting_started/Faq/registeredloadeddependentwhatsupwithdialectsmanagement ``` This was because the sdyroundtripimport introduces the tensor dialect. I'm unsure which pass adds it, but overall what I see is it is actually undone. The details shouldn't matter as long as the pass doesn't crash and the dialect doesn't show up during propagation.",2025-01-10T15:51:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84593
copybara-service[bot],Integrate Triton up to [632bfc34](https://github.com/openai/triton/commits/632bfc342d3a7d63ce8b21209355139ee070d392),Integrate Triton up to 632bfc34,2025-01-10T15:21:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84592
copybara-service[bot],Integrate LLVM at llvm/llvm-project@a531800344dc,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match a531800344dc,2025-01-10T14:41:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84591
copybara-service[bot],tfcompile: mark tf_library as deprecated,tfcompile: mark tf_library as deprecated,2025-01-10T14:40:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84590
copybara-service[bot],"When calling AppendFeatureValues, reserve capacity for the new total size, not","When calling AppendFeatureValues, reserve capacity for the new total size, not just the size of the newly added values.",2025-01-10T14:35:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84589
copybara-service[bot],tfcompile: internal visibility change,tfcompile: internal visibility change,2025-01-10T14:22:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84588
copybara-service[bot],[XLA:CPU] Add initial thunk serialization.,[XLA:CPU] Add initial thunk serialization.,2025-01-10T14:15:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84587
copybara-service[bot],Roll back https://github.com/openxla/xla/pull/19067 because it broke tests.,Roll back https://github.com/openxla/xla/pull/19067 because it broke tests. Reverts 83fb63b0afac8c0efa34c9003bd46b4e916a7146,2025-01-10T13:43:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84586
yangjingyuan000804,Tensorflow.math.floormod()," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The operation tf.math.floormod supports float types, but when performing the operation on two floattype tensors with GPU, an internal error occurs. `import tensorflow as tf x = tf.constant([10, 15, 7.5], dtype=tf.float32) y = tf.constant([3, 4, 2.5], dtype=tf.float32) name = ""random_floormod_operation"" result_code = tf.math.floormod(x,y,name) print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"") print(result_code)` **20250110 09:34:03.279577: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable TF_ENABLE_ONEDNN_OPTS=0. 20250110 09:34:03.294385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20250110 09:34:03.312397: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20250110 09:34:03.317811: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250110 09:34:03.330916: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250110 09:34:04.370985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20250110 09:34:05.819237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 513 MB memory: > device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6 20250110 09:34:05.819757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory: > device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6 20250110 09:34:05.820176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory: > device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1736501646.122604 70797 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.124987 70795 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.127360 70789 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.129708 70796 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.132058 70788 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.135845 70801 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.137479 70799 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.139073 70793 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.140713 70787 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.142340 70802 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.143637 70786 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.144931 70783 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.159628 70796 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. 20250110 09:34:06.630804: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_UNSUPPORTED_PTX_VERSION' 20250110 09:34:06.630843: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250110 09:34:06.630870: W tensorflow/core/framework/op_kernel.cc:1828] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250110 09:34:06.630894: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' Traceback (most recent call last): File ""/root/myFuzzer/outputs/test5/code/tensorflow.math.floormod/tensorflow.math.floormod38.py"", line 5, in result_code = tf.math.floormod(x,y,name) File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/sitepackages/tensorflow/python/ops/weak_tensor_ops.py"", line 142, in wrapper return op(*args, kwargs) File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/sitepackages/tensorflow/python/ops/gen_math_ops.py"", line 4177, in floor_mod _ops.raise_from_not_ok_status(e, name) File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/sitepackages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status raise core._status_to_exception(e) from None  pylint: disable=protectedaccess tensorflow.python.framework.errors_impl.InternalError: {{function_node _wrapped__FloorMod_device/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:FloorMod] name: random_floormod_operation The operation runs normally under integer types with GPU. `import tensorflow as tf x = tf.constant([10, 15, 7], dtype=tf.int32) y = tf.constant([3, 4, 2], dtype=tf.int32) name = ""random_floormod_operation"" result_code = tf.math.floormod(x,y,name) print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"") print(result_code)` 20250110 09:38:07.541149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20250110 09:38:07.559247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20250110 09:38:07.564692: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250110 09:38:07.577837: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250110 09:38:08.635137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20250110 09:38:10.256193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 513 MB memory: > device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6 20250110 09:38:10.256732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory: > device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6 20250110 09:38:10.257179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory: > device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6 !!!!!!!!!!!!!!!!!!!!!!!!!!! tf.Tensor([ 1 3 1], shape=(3,), dtype=int32) The float type is correct for the CPU as well. `import tensorflow as tf x = tf.constant([10, 15, 7.8], dtype=tf.float32) y = tf.constant([3, 4, 2.5], dtype=tf.float32) name = ""random_floormod_operation"" with tf.device('/CPU:0'):     result_code = tf.math.floormod(x,y,name) print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"") print(result_code)`20250110 12:57:31.435249: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250110 12:57:31.449893: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20250110 12:57:31.467647: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20250110 12:57:31.472965: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250110 12:57:31.486020: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250110 12:57:32.528966: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20250110 12:57:34.007683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 447 MB memory:  > device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6 20250110 12:57:34.008219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory:  > device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6 20250110 12:57:34.008642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory:  > device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6 !!!!!!!!!!!!!!!!!!!!!!!!!!! tf.Tensor([ 1.        3.         0.3000002], shape=(3,), dtype=float32)  Standalone code to reproduce the issue ```shell import tensorflow as tf x = tf.constant([10, 15, 7.5], dtype=tf.float32) y = tf.constant([3, 4, 2.5], dtype=tf.float32) name = ""random_floormod_operation"" result_code = tf.math.floormod(x,y,name) ```  Relevant log output _No response_",2025-01-10T12:58:11Z,stat:awaiting response type:bug stale comp:apis 2.17,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84585,"Hi **** , Apologies for the delay, and welcome to TensorFlow! I tried running your code on Colab using TensorFlow 2.17.0 and 2.18.0 versions with GPU, and I did not encounter any issues. Please find the gist attached here for your reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Don't set the promotion state explicetly.,Don't set the promotion state explicetly. The method `_set_promotion_state` was removed in numpy 2.2 and the promotion state is set to weak by default: https://numpy.org/devdocs/release/2.2.0notes.htmlnep50promotionstateoptionremoved,2025-01-10T12:30:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84584
copybara-service[bot],[XLA:CPU] Emit nested computation name rather than caller's,[XLA:CPU] Emit nested computation name rather than caller's,2025-01-10T12:15:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84583
copybara-service[bot],[XLA] Support nested fusions in HloFusionAdaptor,"[XLA] Support nested fusions in HloFusionAdaptor So far, we assumed that `HloComputationFusion` itself contains no fusion instructions. Adding support for that is one step towards a generic Triton emitter that uses nested fusions for the operands of some specific ops (`dot`, `reduce` and potentially `concat`).",2025-01-10T12:13:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84582
copybara-service[bot],[xla:cpu] Add operator[] to SortIterator,[xla:cpu] Add operator[] to SortIterator So it satisfies the requirements for random access iterators. Upcoming libc++ change requires this https://github.com/llvm/llvmproject/commit/69b54c1a05c0c63ee28de1279b3a689b7f026e94,2025-01-10T12:04:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84581
copybara-service[bot],Fix typo regarding `ImportConstantsPass` comment.,Fix typo regarding `ImportConstantsPass` comment. The pass is still operating on `mhlo` constants not `stablehlo` constants. This is because we need to call `mhlo::createFlattenTuplePass` after which is a greedy pattern with folding.,2025-01-10T11:34:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84580
copybara-service[bot],[XLA:CPU] Integrating ObjectLoader into JITCompiler,[XLA:CPU] Integrating ObjectLoader into JITCompiler,2025-01-10T10:25:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84579
yangjingyuan000804,Tensorflow.math.floormod()," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17.0  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The operation tf.math.floormod supports float types, but when performing the operation on two floattype tensors, an internal error occurs. `import tensorflow as tf x = tf.constant([10, 15, 7.5], dtype=tf.float32) y = tf.constant([3, 4, 2.5], dtype=tf.float32) name = ""random_floormod_operation"" result_code = tf.math.floormod(x,y,name) print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"") print(result_code)` **20250110 09:34:03.279577: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250110 09:34:03.294385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20250110 09:34:03.312397: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20250110 09:34:03.317811: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250110 09:34:03.330916: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250110 09:34:04.370985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20250110 09:34:05.819237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 513 MB memory:  > device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6 20250110 09:34:05.819757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory:  > device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6 20250110 09:34:05.820176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory:  > device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1736501646.122604   70797 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.124987   70795 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.127360   70789 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.129708   70796 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.132058   70788 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.135845   70801 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.137479   70799 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.139073   70793 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.140713   70787 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.142340   70802 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.143637   70786 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.144931   70783 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. W0000 00:00:1736501646.159628   70796 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver. 20250110 09:34:06.630804: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_UNSUPPORTED_PTX_VERSION' 20250110 09:34:06.630843: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250110 09:34:06.630870: W tensorflow/core/framework/op_kernel.cc:1828] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' 20250110 09:34:06.630894: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' Traceback (most recent call last):   File ""/root/myFuzzer/outputs/test5/code/tensorflow.math.floormod/tensorflow.math.floormod38.py"", line 5, in      result_code = tf.math.floormod(x,y,name)   File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/sitepackages/tensorflow/python/ops/weak_tensor_ops.py"", line 142, in wrapper     return op(*args, **kwargs)   File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/sitepackages/tensorflow/python/ops/gen_math_ops.py"", line 4177, in floor_mod     _ops.raise_from_not_ok_status(e, name)   File ""/root/miniconda3/envs/fuzz4all/lib/python3.10/sitepackages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status     raise core._status_to_exception(e) from None   pylint: disable=protectedaccess tensorflow.python.framework.errors_impl.InternalError: {{function_node __wrapped__FloorMod_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:FloorMod] name: random_floormod_operation** The operation runs normally under integer types. `import tensorflow as tf x = tf.constant([10, 15, 7], dtype=tf.int32) y = tf.constant([3, 4, 2], dtype=tf.int32) name = ""random_floormod_operation"" result_code = tf.math.floormod(x,y,name) print(""!!!!!!!!!!!!!!!!!!!!!!!!!!!"") print(result_code)` **20250110 09:38:07.541149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20250110 09:38:07.559247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20250110 09:38:07.564692: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250110 09:38:07.577837: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 20250110 09:38:08.635137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20250110 09:38:10.256193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 513 MB memory:  > device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:38:00.0, compute capability: 8.6 20250110 09:38:10.256732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory:  > device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6 20250110 09:38:10.257179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22463 MB memory:  > device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6 !!!!!!!!!!!!!!!!!!!!!!!!!!! tf.Tensor([ 1 3  1], shape=(3,), dtype=int32)**  Standalone code to reproduce the issue ```shell import tensorflow as tf x = tf.constant([10, 15, 7.5], dtype=tf.float32) y = tf.constant([3, 4, 2.5], dtype=tf.float32) name = ""random_floormod_operation"" result_code = tf.math.floormod(x,y,name) ```  Relevant log output _No response_",2025-01-10T09:40:55Z,type:bug,closed,0,1,https://github.com/tensorflow/tensorflow/issues/84577,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T09:21:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84576
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T09:20:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84575
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T09:20:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84574
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T09:19:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84573
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T09:18:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84572
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T09:17:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84571
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T09:17:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84570
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T09:17:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84569
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T09:16:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84568
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T09:16:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84567
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T09:15:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84566
copybara-service[bot],[NFC] Minor cleanup.,[NFC] Minor cleanup.,2025-01-10T09:03:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84565
copybara-service[bot],PR #21191: [xla:cpu] Fix missing header in oneDNN ACL build,"PR CC(Use correct hash_bucket_size parameter): [xla:cpu] Fix missing header in oneDNN ACL build Imported from GitHub PR https://github.com/openxla/xla/pull/21191 Fixes build error  ``` Compiling src/cpu/jit_utils/jit_utils.cpp failed: (Exit 1): clang failed: error executing command (from target //:mkl_dnn_acl) /usr/lib/llvm14/bin/clang U_FORTIFY_SOURCE fstackprotector Wall Wthreadsafety Wselfassign Wunusedbutsetparameter Wnofreenonheapobject fcolordiagnostics fnoomitframepointer g0 ... (remaining 126 arguments skipped) Use sandbox_debug to see verbose messages from the sandbox and retain the sandbox build root for debugging external/mkl_dnn_acl_compatible/src/cpu/jit_utils/jit_utils.cpp:34:10: fatal error: 'common/ittnotify/jitprofiling.h' file not found include ""common/ittnotify/jitprofiling.h""          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 1 error generated. INFO: Elapsed time: 524.121s, Critical Path: 452.78s INFO: 543 processes: 45 internal, 498 linuxsandbox. FAILED: Build did NOT complete successfully ``` Build step: bazel build config=mkl_aarch64_threadpool test_output=all spawn_strategy=sandboxed //xla/... Copybara import of the project:  23e8fadc3e88208219e685115435c40674efec43 by Crefeda Rodrigues : [xla:cpu] Fix missing headers in oneDNN ACL build Signedoffby: Crefeda Rodrigues  Merging this change closes CC(Use correct hash_bucket_size parameter) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21191 from cfRod:aarch64xlabuild 23e8fadc3e88208219e685115435c40674efec43",2025-01-10T08:47:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84564
copybara-service[bot],PR #21234: [ROCm] Fix failing dot tests,"PR CC(Default model signatures of Estimator can not be used by TensorFlow Serving RESTful APIs): [ROCm] Fix failing dot tests Imported from GitHub PR https://github.com/openxla/xla/pull/21234 Issue introduced here https://github.com/openxla/xla/commit/d1f63e2f60ee4ccb73a5e06484f4783eae79420a The following tests failed to build: ``` //xla/tests:dot_operation_single_threaded_runtime_test_gpu_amd_any FAILED TO BUILD //xla/tests:dot_operation_test_autotune_disabled_gpu_amd_any    FAILED TO BUILD //xla/tests:dot_operation_test_gpu_amd_any                      FAILED TO BUILD ``` ...with: ``` [20250109T01:19:37.573Z] xla/tests/dot_operation_test.cc:1014:24: error: there are no arguments to ‘CreateScalarMaxComputation’ that depend on a template parameter, so a declaration of ‘CreateScalarMaxComputation’ must be available [fpermissive] [20250109T01:19:37.573Z]  1014    XlaComputation max = CreateScalarMaxComputation(F32, &builder); ``` Returning `""xla/hlo/builder/lib/arithmetic.h""` include fixed the problem. Copybara import of the project:  79efd63f12e9b41da73a91c2ed1813559734712c by Milica Makevic : Add ""xla/hlo/builder/lib/arithmetic.h"" include to dot_operation_test Merging this change closes CC(Default model signatures of Estimator can not be used by TensorFlow Serving RESTful APIs) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21234 from ROCm:ci_fix_dot_operation_test 79efd63f12e9b41da73a91c2ed1813559734712c",2025-01-10T08:41:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84563
copybara-service[bot],Fix after cl/713720788: Add back the necessary build target.,Fix after cl/713720788: Add back the necessary build target.,2025-01-10T08:15:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84562
copybara-service[bot],[XLA:GPU] Fix broken build.,[XLA:GPU] Fix broken build.,2025-01-10T08:12:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84561
copybara-service[bot],[lite/kernels] cpu_backend_gemm: Update TFLITE_WITH_RUY comments,[lite/kernels] cpu_backend_gemm: Update TFLITE_WITH_RUY comments Document default on ARM and x86. Remove mention on nonexistent TFLITE_X86_RUY_ENABLED,2025-01-10T07:50:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84560
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T07:23:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84559
johnnkp,gen_quantized_function_library: clang-cl compilation file path error," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.19 nightly  Custom code Yes  OS platform and distribution Windows 11 24H2  Mobile device _No response_  Python version Anaconda 2024.101  Bazel version 6.5.0  GCC/compiler version Visual Studio 2022 (build tools 14.42) + LLVM 19.1.6 + msys2x86_6420241208  CUDA/cuDNN version CUDA 12.6.3 + CUDNN 9.6.0  GPU model and memory GTX 1050 Ti 4GB  Current behavior? `gen_quantized_function_library` is trying to read `'C:\\msys64\\home\\*\\_bazel_*\\*\\execroot\\org_tensorflow\\bazelout\\x64_windowsoptexec*\\bin\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.exe.runfiles\\org_tensorflow\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.py'` on Windows. However, `os.path.exists()` cannot resolve `\\` symbol. Correct path is just like: `'C:/msys64/home/AMD/_bazel_amd/2oea4ayg/execroot/org_tensorflow/bazelout/x64_windowsoptexecBB41B15F/bin/tensorflow/compiler/mlir/quantization/tensorflow/gen_quantized_function_library'`  Standalone code to reproduce the issue ```shell 1. download https://github.com/johnnkp/tensorflowwgputest/archive/refs/heads/default_memory_space_description.zip and extract 2. run `python configure.py` to configure Windows CUDA build 3. run `bazel build config=win_clang config=cuda_wheel config=opt define=no_tensorflow_py_deps=true repo_env=TF_PYTHON_VERSION=3.12 //tensorflow/tools/pip_package:wheel repo_env=WHEEL_NAME=tensorflow_gpu` ```  Relevant log output ```shell ERROR: C:/users/amd/downloads/tensorflowwgputest/tensorflow/compiler/mlir/quantization/tensorflow/BUILD:38:8: Executing genrule //tensorflow/compiler/mlir/quantization/tensorflow:quantized_function_library failed: (Exit 1): bash.exe failed: error executing command (from target //tensorflow/compiler/mlir/quantization/tensorflow:quantized_function_library)   cd /d C:/msys64/home/amd/_bazel_amd/2oea4ayg/execroot/org_tensorflow   SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Windows;C:\Windows\System32;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6\extras\CUPTI\lib64;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6\bin;C:\Program Files\LLVM\bin;C:\Users\AMD\anaconda3\Scripts;C:\Users\AMD\anaconda3;C:\msys64\usr\local\bin;C:\msys64\usr\bin;C:\msys64\usr\bin;C:\msys64\opt\bin;C:\Windows\System32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\msys64\usr\bin\site_perl;C:\msys64\usr\bin\vendor_perl;C:\msys64\usr\bin\core_perl     SET PYTHON_BIN_PATH=C:/Users/AMD/anaconda3/python.exe     SET PYTHON_LIB_PATH=C:/Users/AMD/anaconda3/Lib/sitepackages     SET TF2_BEHAVIOR=1   C:\msys64\usr\bin\bash.exe c source external/bazel_tools/tools/genrule/genrulesetup.sh; bazelout/x64_windowsoptexecBB41B15F/bin/tensorflow/compiler/mlir/quantization/tensorflow/gen_quantized_function_library.exe output_file bazelout/x64_windowsopt/bin/tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library.h src 'tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library_uniform_quantized.mlir tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library.mlir tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library_uniform_quantized_drq.mlir tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library_tf_drq.mlir tensorflow/compiler/mlir/quantization/tensorflow/passes/quantized_function_library_xla_weight_only.mlir'  Configuration: bb43855b4d3a8365141a732926c882a21d79a321eb57928ec09cdaf313f3403c  Execution platform: //tensorflow/tools/toolchains/win:x64_windowsclangcl Traceback (most recent call last):   File ""C:\msys64\home\AMD\_bazel_amd\2oea4ayg\execroot\org_tensorflow\bazelout\x64_windowsoptexecBB41B15F\bin\tensorflow\compiler\mlir\quantization\tensorflow\gen_quantized_function_library"", line 559, in      Main()   File ""C:\msys64\home\AMD\_bazel_amd\2oea4ayg\execroot\org_tensorflow\bazelout\x64_windowsoptexecBB41B15F\bin\tensorflow\compiler\mlir\quantization\tensorflow\gen_quantized_function_library"", line 490, in Main     assert os.path.exists(main_filename), \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ AssertionError: Cannot exec() 'C:\\msys64\\home\\AMD\\_bazel_amd\\2oea4ayg\\execroot\\org_tensorflow\\bazelout\\x64_windowsoptexecBB41B15F\\bin\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.exe.runfiles\\org_tensorflow\\tensorflow\\compiler\\mlir\\quantization\\tensorflow\\gen_quantized_function_library.py': file not found. Target //tensorflow/tools/pip_package:wheel failed to build INFO: Elapsed time: 65.631s, Critical Path: 5.13s INFO: 30 processes: 13 disk cache hit, 8 internal, 9 local. FAILED: Build did NOT complete successfully ```",2025-01-10T07:01:31Z,stat:awaiting response type:build/install subtype:windows TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84558,"I found out `rules_python` provide the python file template, but modify `python/private/python_bootstrap_template.txt` will not affect the generated `.py`.","Hi **** , Apologies for the delay, and thank you for raising your issue here. The main cause appears to be related to your file path. On Windows, path handling is different, and the error suggests that os.path.exists() cannot correctly resolve the path due to the use of backslashes (`\`). These are standard in Windows paths but need to be properly managed in Python. To resolve this, configure your setup to use forward slashes (`/`) instead of backslashes (`\`) for Windows paths. Python, especially when running in environments like MSYS2 or Git Bash, often handles forward slashes more consistently. If you have already tried this, please rebuild the Bazel target that generates the `.py` file. If the issue persists, let us know so we can further assist you. Thank you!","Although I already found out a fix for this issue, I am not going to create a pull request because my compilation failed with nvcc error in windows. And my fix seems unnecessary for other builds.",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T06:54:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84557
copybara-service[bot],PR #21245: Fix failing test //xla/pjrt/gpu:pjrt_client_test_se_gpu,PR CC(TFLite Android: Model file will not load. startOffset and declaredLength problems): Fix failing test //xla/pjrt/gpu:pjrt_client_test_se_gpu Imported from GitHub PR https://github.com/openxla/xla/pull/21245 This test fails because the hold is not checked before use. Added the check. Copybara import of the project:  c4c71fecbdd28080fd9b50c2adc7d05c65dc6921 by Shraiysh Vaishay : Fix failing test //xla/pjrt/gpu:pjrt_client_test_se_gpu This test fails because the hold is not checked before use. Added the check. Merging this change closes CC(TFLite Android: Model file will not load. startOffset and declaredLength problems) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21245 from shraiysh:pjrt_client_test_se_gpu c4c71fecbdd28080fd9b50c2adc7d05c65dc6921,2025-01-10T06:42:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84556
copybara-service[bot],[xla:cpu][oneDNN] Add missing deps for onednn.,[xla:cpu][oneDNN] Add missing deps for onednn.,2025-01-10T06:19:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84555
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T06:14:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84554
copybara-service[bot],Internal test changes for int1/2 types.,Internal test changes for int1/2 types.,2025-01-10T06:04:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84553
copybara-service[bot],Handle missing dtype cases in `xla::ifrt::DType::DebugString()`,Handle missing dtype cases in `xla::ifrt::DType::DebugString()`,2025-01-10T06:00:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84552
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T05:37:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84551
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T05:29:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84550
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T05:23:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84549
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T05:13:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84548
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T04:44:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84547
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T04:42:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84546
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T04:35:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84545
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T04:33:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84544
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T04:32:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84543
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T04:31:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84542
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T04:31:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84541
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T04:30:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84540
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T04:30:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84539
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T04:26:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84538
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T04:22:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84537
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-10T04:20:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84536
copybara-service[bot],"[lib/jpeg] Remove unused ""tensorflow/core/platform/types.h"" include from header files.","[lib/jpeg] Remove unused ""tensorflow/core/platform/types.h"" include from header files. This reduces the number of transitive includes, which complicate the build graph, and lead to layering check failures.",2025-01-10T04:19:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84535
copybara-service[bot],Add support for int1 types in literal.cc,Add support for int1 types in literal.cc,2025-01-10T03:20:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84534
copybara-service[bot],In progress experimention. Add StringDType to JAX's supported types.,In progress experimention. Add StringDType to JAX's supported types.,2025-01-10T03:16:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84533
copybara-service[bot],In progress experimention. Add StringDType to JAX's supported types.,In progress experimention. Add StringDType to JAX's supported types.,2025-01-10T03:05:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84532
copybara-service[bot],[xla:cpu] Micro-optimizations for ThunkExecutor,[xla:cpu] Microoptimizations for ThunkExecutor Keep inedges and outedges in a dense container to optimize data locality on a hot path. For smallish thunk sequences all out edges should fit into L1 cache. name                                   old cpu/op   new cpu/op   delta BM_SyncThunkExecutor/1/process_time    29.4ns ± 2%  29.6ns ± 2%  +0.81%   BM_SyncThunkExecutor/2/process_time     103ns ± 2%   101ns ± 3%  1.63%   BM_SyncThunkExecutor/4/process_time     173ns ± 3%   171ns ± 2%  1.10%   BM_SyncThunkExecutor/8/process_time     320ns ± 2%   317ns ± 2%  0.95%   BM_SyncThunkExecutor/16/process_time    652ns ± 2%   638ns ± 2%  2.21%   BM_SyncThunkExecutor/32/process_time   1.28µs ± 3%  1.25µs ± 5%  2.03%   BM_SyncThunkExecutor/64/process_time   2.71µs ± 6%  2.61µs ± 6%  3.73%   BM_SyncThunkExecutor/128/process_time  5.73µs ± 4%  5.41µs ± 3%  5.46%   BM_SyncThunkExecutor/256/process_time  12.0µs ± 3%  11.1µs ± 2%  6.81%   BM_SyncThunkExecutor/512/process_time  25.1µs ± 4%  23.1µs ± 3%  7.93%,2025-01-10T02:10:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84531
copybara-service[bot],Update ml_dtypes version to 0fa5313b65efe848c5968a15dd37dd220cc29567.,"Update ml_dtypes version to 0fa5313b65efe848c5968a15dd37dd220cc29567. Also add mxfloat as a dependency to TensorFlow and TSL. This is needed to merge https://github.com/openxla/xla/pull/19096. Previously this was done in the merge commit for that PR, but the PR was rolled back since the new types caused an internal TF Android build to fail. Now it's being done in this separate, smaller change so its easier to rollback if issues occur.",2025-01-10T02:08:58Z,,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84530, I believe you should also update `GIT_TAG` in `ml_dtypes.cmake`. CMake build is currently failing due to missing mxfloat.,Thank you for letting me know! I will update this immediately.,Thank you!,Updated the GIT_TAG in https://github.com/tensorflow/tensorflow/commit/30e98fcdd9deccb894eb85cc384d06fa32b8d6f2.
andresserranodev,FAILURE: Build failed with an exception during step 3 of Digit Classifier Codelab,"I encountered an issue during step 3 of the Digit Classifier Codelab). Specifically, while syncing the Gradle project, I received the following error: ``` * What went wrong: Failed to notify dependency resolution listener. 'void org.gradle.api.artifacts.DependencySubstitutions$Substitution.with(org.gradle.api.artifacts.component.ComponentSelector)' ``` Steps to Reproduce Download the project as a .zip file from the Digit Classifier Codelab. Open the project in Android Studio. Run the Gradle sync. Expected Behavior Gradle sync should complete successfully, allowing me to continue with the codelab. Actual Behavior The sync fails with the above error message. Is there a missing dependency or configuration step that I overlooked? Could this be related to a mismatch in Gradle or JDK versions? Environment: Android Studio Version: Android Studio Ladybug | 2024.2.1 Patch 3 Gradle Version: gradle8.5bin.zip Operating System: macOS 15.2 (24C101) JDK Version: 17.0.13 and 21.0.3",2025-01-10T02:00:19Z,stat:awaiting response stale comp:lite subtype:macOS,closed,0,5,https://github.com/tensorflow/tensorflow/issues/84529,"Hi,  I apologize for the delayed response, I would suggest you to please clone this repo and follow below steps for building the project :  Open Android Studio. From the Welcome screen, select Open an existing Android Studio project.  From the Open File or Project window that appears, navigate to and select the `tensorflowlite/examples/digit_classifier/android` directory. Click OK.  If it asks you to do a Gradle Sync, click OK.  With your Android device connected to your computer and developer mode enabled, click on the green Run arrow in Android Studio. There might be `gradle version 8.5` compatibility issues at the moment so I tried with gradle version` 7.6.2 `instead of `8.5` so I changed this line `distributionUrl=https\://services.gradle.org/distributions/gradle8.5bin.zip` to this `distributionUrl=https\://services.gradle.org/distributions/gradle7.6.2bin.zip` in `gradlewrapper.properties` and things are working as expected for reference I've added output screenshot below so at the moment please use gradle version `7.6.2` and our relevant team will fix this issue with gradle version `8.5` soon **Here is output screenshot for reference :** !image Please give it try from your end and let us know is it working as expected or not with gradle version `7.6.2` ? Thank you for your cooperation and patience.",Hi  ! The gradle version 7.6.2 works. Many thanks! ,"Hi,   Thank you for your kind words, Good to hear that things are working as expected with `gradle version 7.6.2` If your issue has been resolved please feel free to close this issue. We understand this is a known issue and our relavant team is actively working on a permanent resolution. Thank you for your patience and understanding",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,The gradle version 7.6.2 works. Many thanks!
copybara-service[bot],"[PJRT:C] Fix PJRT_Api_STRUCT_SIZE, with the change to Create API to support TryGet API.","[PJRT:C] Fix PJRT_Api_STRUCT_SIZE, with the change to Create API to support TryGet API.",2025-01-10T01:44:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84528
copybara-service[bot],Fix bad merge that skipped exporting tags.,Fix bad merge that skipped exporting tags.,2025-01-10T01:32:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84527
copybara-service[bot],Plug the `allow_id_dropping` from the user configuration.,Plug the `allow_id_dropping` from the user configuration.,2025-01-10T00:59:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84526
copybara-service[bot],Internal change only,Internal change only,2025-01-10T00:59:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84525
copybara-service[bot],Extract a common helper function `HandleElementwiseWithDimsToReplicate` in `SpmdPartitioningVisitor`.,"Extract a common helper function `HandleElementwiseWithDimsToReplicate` in `SpmdPartitioningVisitor`. Based on that, add `HandleCholesky` and `HandleTriangularSolve`. Before this change, we replicate all dimensions in these ops. With this cl, we only replicate the last two dimensions for these two operations.",2025-01-10T00:52:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84524
Catakang,Memory Allocation Issues," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18.0  Custom code Yes  OS platform and distribution Ubuntu 24.04.1 LTS on WSL2  Mobile device _No response_  Python version 3.12.8  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version 90300  GPU model and memory RTX 4070 12GB  Current behavior? I create a virtual gpu with a hard limit of 10GB. I start training the network and it works for bit but then says out of memory and tries to allocate more than the set limit. What I expect to happen is that is stays within the 10GB limit and can train the network successfully.  Standalone code to reproduce the issue ```shell import numpy as np import keras from keras import layers import tensorflow as tf import tensorflow_datasets as tfds import matplotlib.pyplot as plt %matplotlib inline tfds.disable_progress_bar() gpus = tf.config.list_physical_devices('GPU') if gpus:    Restrict TensorFlow to only allocate 1GB of memory on the first GPU   try:     tf.config.set_logical_device_configuration(         gpus[0],         [tf.config.LogicalDeviceConfiguration(memory_limit=10240)])     logical_gpus = tf.config.list_logical_devices('GPU')     print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")   except RuntimeError as e:      Virtual devices must be set before GPUs have been initialized     print(e) train_ds, validation_ds, test_ds = tfds.load(     ""cats_vs_dogs"",      Reserve 10% for validation and 10% for test     split=[""train[:40%]"", ""train[40%:50%]"", ""train[50%:60%]""],     as_supervised=True,   Include labels ) resize_fn = keras.layers.Resizing(150, 150) train_ds = train_ds.map(lambda x, y: (resize_fn(x), y)) validation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y)) test_ds = test_ds.map(lambda x, y: (resize_fn(x), y)) augmentation_layers = [     layers.RandomFlip(""horizontal""),     layers.RandomRotation(0.1), ] def data_augmentation(x):     for layer in augmentation_layers:         x = layer(x)     return x train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y)) from tensorflow import data as tf_data batch_size = 16 train_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache() validation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache() test_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache() base_model = keras.applications.Xception(     weights=""imagenet"",   Load weights pretrained on ImageNet.     input_shape=(150, 150, 3),     include_top=False, )   Do not include the ImageNet classifier at the top.  Freeze the base_model base_model.trainable = False  Create new model on top inputs = keras.Input(shape=(150, 150, 3))  Pretrained Xception weights requires that input be scaled  from (0, 255) to a range of (1., +1.), the rescaling layer  outputs: `(inputs * scale) + offset` scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=1) x = scale_layer(inputs)  The base model contains batchnorm layers. We want to keep them in inference mode  when we unfreeze the base model for finetuning, so we make sure that the  base_model is running in inference mode here. x = base_model(x, training=False) x = keras.layers.GlobalAveragePooling2D()(x) x = keras.layers.Dropout(0.2)(x)   Regularize with dropout outputs = keras.layers.Dense(1)(x) model = keras.Model(inputs, outputs) model.summary(show_trainable=True) model.compile(     optimizer=keras.optimizers.Adam(),     loss=keras.losses.BinaryCrossentropy(from_logits=True),     metrics=[keras.metrics.BinaryAccuracy()], ) epochs = 2 print(""Fitting the top layer of the model"") model.fit(train_ds, epochs=epochs, validation_data=validation_ds) ```  Relevant log output ```shell 20250109 19:39:57.953074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1736469597.967544   14431 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1736469597.971752   14431 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20250109 19:39:57.986195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 1 Physical GPUs, 1 Logical GPUs I0000 00:00:1736469600.052169   14431 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10240 MB memory:  > device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:0a:00.0, compute capability: 8.9 Fitting the top layer of the model Epoch 1/2 20250109 19:40:04.479339: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1736469604.583055   14486 service.cc:148] XLA service 0x7f8df8002230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: I0000 00:00:1736469604.583109   14486 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4070, Compute Capability 8.9 20250109 19:40:04.722034: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. I0000 00:00:1736469605.234339   14486 cuda_dnn.cc:529] Loaded cuDNN version 90300   7/582 ━━━━━━━━━━━━━━━━━━━━ 12s 22ms/step  binary_accuracy: 0.5658  loss: 0.6950  I0000 00:00:1736469608.599510   14486 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process. 243/582 ━━━━━━━━━━━━━━━━━━━━ 16s 48ms/step  binary_accuracy: 0.8715  loss: 0.2755 20250109 19:40:20.475756: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 1073741824 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469620.475821   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 1073741824 20250109 19:40:20.615636: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 966367744 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469620.615700   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 966367744 20250109 19:40:20.769033: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 869731072 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469620.769095   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 869731072 20250109 19:40:20.906909: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 782758144 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469620.906973   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 782758144 20250109 19:40:21.048863: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 704482304 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469621.048940   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 704482304 20250109 19:40:21.229614: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 634034176 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469621.229682   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 634034176 20250109 19:40:21.371940: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 570630912 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469621.372000   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 570630912 20250109 19:40:21.510751: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 513568000 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469621.510817   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 513568000 20250109 19:40:21.650945: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 462211328 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469621.651034   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 462211328 20250109 19:40:21.814945: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 415990272 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469621.815035   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 415990272 20250109 19:40:21.954790: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 374391296 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469621.954851   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 374391296 20250109 19:40:22.094150: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 336952320 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469622.094219   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 336952320 20250109 19:40:22.267664: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 303257088 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory ... 20250109 19:40:23.128022: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 161164032 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469623.128090   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 161164032 20250109 19:40:23.296856: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 145047808 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory W0000 00:00:1736469623.296940   14537 device_host_allocator.h:61] could not allocate pinned host memory of size: 145047808 Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ```",2025-01-10T00:49:17Z,stat:awaiting response type:bug stale TF 2.18,closed,1,7,https://github.com/tensorflow/tensorflow/issues/84523,"I am facing exactly the same issue for a while now on two slightly different systems:  It even shows the warnings when setting `os.environ['TF_CPP_MIN_LOG_LEVEL'] = ""3""`. In my case, my code runs through despite these error messages, but on the one hand they are annoying and also a bit worrying and on the other hand I have the feeling that they affect the execution time, because I often observe strange behaviour regarding the execution times.","It does affect execution time and my code does not run to completion with the errors as I let it run for an hour to see what happened and it finished with a message saying '0 successful operations'. I don't know why I am running into this error as I am just trying to follow a tutorial on the keras website. I may not have a multi GPU setup to train a bunch of networks in an optimized fashion but surely 10gb on my 4070 should be plenty to run 2 epochs with a batch size of 10. This is ridiculous and from what I am reading from other GitHub issues, this has been an issue for years on certain systems that they have simply not fixed(if I understand everything right) and I really just want to figure out tensorflow for my science fair project.","I will mention I have tried growing the memory, I tried the malloc cuda async flag, I tried slowly reducing the batch size smaller and smaller, this really shouldn't be that complicated.","Hi **** , Apologies for the delay, and thank you for raising your concern here. I attempted to run your code on Colab using the TensorFlow nightly version but encountered a different issue. I have attached a gist for your review—could you please check and let me know if I made any mistakes while executing your code? Additionally, in your setup, consider disabling XLA to potentially reduce memory usage. Let us know if you are still encountering the same issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Add original cp name prefix to the names of the decomposed instructions when breaking circular cp for better traceability.,Add original cp name prefix to the names of the decomposed instructions when breaking circular cp for better traceability.,2025-01-10T00:30:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84522
copybara-service[bot],[xla] Rename RendezvousSingle to Rendezvous,[xla] Rename RendezvousSingle to Rendezvous,2025-01-10T00:26:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84521
copybara-service[bot],Remove host memory space as input to HostOffloader constructor.,Remove host memory space as input to HostOffloader constructor.,2025-01-10T00:26:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84520
copybara-service[bot],[xla:cpu] Migrate ReduceScatter to RendezvousSingle API,[xla:cpu] Migrate ReduceScatter to RendezvousSingle API,2025-01-10T00:25:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84519
copybara-service[bot],[xla] Delete unused refcounting hashmap,[xla] Delete unused refcounting hashmap,2025-01-10T00:24:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84518
copybara-service[bot],DO NOT SUBMIT: test for presubmit.,DO NOT SUBMIT: test for presubmit.,2025-01-10T00:22:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84517
copybara-service[bot],Add `SpmdPartitioningVisitor::HandleBitcastConvert`.,"Add `SpmdPartitioningVisitor::HandleBitcastConvert`. Before this change, we use the default action for BitcastConvert operations. If the input and output has the same rank, it is recognized as an elementwise operations and is handled by `HandleElementwise`. However, if the input and output has different rank, we will always replicate the input, which is inefficient. With this cl, we can handle cases with different rank smartly. We keep the sharding in batch dims and only replicate the extra dims. Given the following input ``` ENTRY entry {   p0 = s64[4] parameter(0), sharding={devices=[2,2] f32[2,1] {   %param = s64[2]{0} parameter(0), sharding={devices=[2,2] f32[2,1] {   %param = s64[2]{0} parameter(0), sharding={devices=[2,2]0,2,1,3 last_tile_dim_replicate}   %collectivepermute = s64[2]{0} collectivepermute(s64[2]{0} %param), channel_id=1, source_target_pairs={{0,0},{2,1},{1,2},{3,3}}   %result.1 = f32[2,2]{1,0} bitcastconvert(s64[2]{0} %collectivepermute)   %constant.3 = s32[4]{0} constant({0, 0, 2, 2})   %partitionid = u32[] partitionid()   %dynamicslice.1 = s32[1]{0} dynamicslice(s32[4]{0} %constant.3, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.1 = s32[] reshape(s32[1]{0} %dynamicslice.1)   %subtract = s32[] subtract(s32[] %reshape.1, s32[] %reshape.1)   %constant.4 = s32[4]{0} constant({0, 1, 0, 1})   %dynamicslice.2 = s32[1]{0} dynamicslice(s32[4]{0} %constant.4, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.2 = s32[] reshape(s32[1]{0} %dynamicslice.2)   %constant.6 = s32[] constant(0)   %subtract.1 = s32[] subtract(s32[] %reshape.2, s32[] %constant.6)   ROOT %dynamicslice.4 = f32[2,1]{1,0} dynamicslice(f32[2,2]{1,0} %result.1, s32[] %subtract, s32[] %subtract.1), dynamic_slice_sizes={2,1} } ```",2025-01-09T23:24:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84516
copybara-service[bot],[XLA] Simplify the scheduler test HLO.,[XLA] Simplify the scheduler test HLO.,2025-01-09T23:24:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84515
copybara-service[bot],Make ML Drift's fingerprinting logic into a helper function,Make ML Drift's fingerprinting logic into a helper function,2025-01-09T23:16:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84514
copybara-service[bot],Refactor GetIfrtHloSharding and GetIfrtConcreteEvenSharding,Refactor GetIfrtHloSharding and GetIfrtConcreteEvenSharding to be available in jaxlib. These will be useful for implementing c++ device_put.,2025-01-09T23:11:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84513
copybara-service[bot],[XLA:SchedulingAnnotations] Handle instructions with control dependencies.,[XLA:SchedulingAnnotations] Handle instructions with control dependencies.,2025-01-09T23:10:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84512
copybara-service[bot],Add remaining FP8 (B11)FNUZ types to Tensorflow. This exposes,Add remaining FP8 (B11)FNUZ types to Tensorflow. This exposes   * `tf.experimental.float8_e4m3fnuz`   * `tf.experimental.float8_e4m3b11fnuz`   * `tf.experimental.float8_e5m2fnuz` as public tensorflow dtypes. With this change we can create and save tensors with these types.,2025-01-09T22:58:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84511
copybara-service[bot],Use `@com_google_googletest//:gtest_main` instead of `tsl/platform:test_main`,"Use `//:gtest_main` instead of `tsl/platform:test_main` Also sets `test test_env=""GTEST_INSTALL_FAILURE_SIGNAL_HANDLER=1""` in the bazelrc to retain stacktrace behavior when tests are killed",2025-01-09T22:47:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84510
copybara-service[bot],Fix oss buld error of dispatch_api,Fix oss buld error of dispatch_api,2025-01-09T22:46:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84509
copybara-service[bot],This is the first step in unifying the various StreamExecutor::Allocate and ::Deallocate methods.  (e.g. HostMemoryAllocate & HostMemoryDeallocate),This is the first step in unifying the various StreamExecutor::Allocate and ::Deallocate methods.  (e.g. HostMemoryAllocate & HostMemoryDeallocate) Future CLs will make use of these new classes to eliminate the need for adding bespoke Allocate/Deallocate pairs as new MemoryTypes are added.,2025-01-09T22:42:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84508
copybara-service[bot],In progress experimention. Add StringDType to JAX's supported types.,In progress experimention. Add StringDType to JAX's supported types.,2025-01-09T22:35:25Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84507
copybara-service[bot],Reverts f25e674a719c0f96a71111ad69bdb1e820cd4118,Reverts f25e674a719c0f96a71111ad69bdb1e820cd4118,2025-01-09T22:21:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84506
copybara-service[bot],[XLA:TPU] Avoid unnecessary and potentially expensive computation sorting during instruction fusion.,[XLA:TPU] Avoid unnecessary and potentially expensive computation sorting during instruction fusion. The computations are not being sorted in a semantically meaningful order; they are sorted by instruction count with ties being broken consistently but arbitrarily (based on a hash of the string representation of the computation). There is therefore no reason why these passes need to traverse the computations in this specific order.,2025-01-09T22:09:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84505
copybara-service[bot],Fix missing BUILD dependency in compiler/xla/tests:collective_pipeline_parallelism_test.,Fix missing BUILD dependency in compiler/xla/tests:collective_pipeline_parallelism_test.,2025-01-09T22:00:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84504
copybara-service[bot],Remove unused data members from PluggableDeviceProcessState.,Remove unused data members from PluggableDeviceProcessState.,2025-01-09T22:00:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84503
copybara-service[bot],Cleanup. Sort the declarations in spmd_partitioner.,Cleanup. Sort the declarations in spmd_partitioner.,2025-01-09T21:55:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84502
copybara-service[bot],[xla:python] Add method to get python callback capsule without requiring operand or result shapes / returning capsule descriptor.,[xla:python] Add method to get python callback capsule without requiring operand or result shapes / returning capsule descriptor.,2025-01-09T21:38:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84501
copybara-service[bot],Add tests to make sure DenseResourceElementsAttr are handled/supported by flatbuffer_export.,"Add tests to make sure DenseResourceElementsAttr are handled/supported by flatbuffer_export. Note that, flatbuffer_import will remain unchanged and it will not create DenseResourceElementsAttr, now.",2025-01-09T21:33:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84500
copybara-service[bot],Speed-up DepthwiseInputCopyOp,"Speedup DepthwiseInputCopyOp Optimize this by replacing multiplication with advancing the pointer every iteration. Also avoid reloading depth/etc. from args every time. Fixing benchmark for depthwise conv and running them I get a lot of noise, but it seems positive overall. name                                                                                   old cpu/op   new cpu/op   delta BM_ConvFloatDepthwiseFwdCPU1_conv0_float/real_time  [32_112_112_3_8_24_3_3_1_2_cpu1 ]  33.4µs ±16%  34.7µs ±28%     ~     (p=0.284 n=38+39) BM_ConvFloatDepthwiseFwdCPU4_conv0_float/real_time  [32_112_112_3_8_24_3_3_1_2_cpu4 ]  27.3µs ±57%  26.6µs ±52%     ~     (p=0.556 n=40+40) BM_ConvFloatDepthwiseFwdCPU1_conv1_float/real_time  [32_112_112_64_1_64_3_3_1_2_cpu1]  35.6µs ±24%  36.3µs ±27%     ~     (p=0.283 n=35+40) BM_ConvFloatDepthwiseFwdCPU4_conv1_float/real_time  [32_112_112_64_1_64_3_3_1_2_cpu4]  30.0µs ±27%  31.1µs ±33%     ~     (p=0.377 n=36+34) BM_ConvFloatDepthwiseFwdCPU1_conv2_float/real_time  [32_56_56_128_1_128_3_3_1_2_cpu1]  32.8µs ±14%  33.1µs ±18%     ~     (p=0.761 n=33+38) BM_ConvFloatDepthwiseFwdCPU4_conv2_float/real_time  [32_56_56_128_1_128_3_3_1_2_cpu4]  25.7µs ±57%  26.4µs ±55%     ~     (p=0.609 n=40+40) BM_ConvFloatDepthwiseFwdCPU1_conv3_float/real_time  [32_56_56_128_1_128_3_3_2_2_cpu1]  32.2µs ±17%  31.7µs ±12%     ~     (p=0.204 n=37+35) BM_ConvFloatDepthwiseFwdCPU4_conv3_float/real_time  [32_56_56_128_1_128_3_3_2_2_cpu4]  27.8µs ±32%  27.0µs ±24%     ~     (p=0.341 n=34+39) BM_ConvFloatDepthwiseFwdCPU1_conv4_float/real_time  [32_28_28_128_1_128_3_3_1_2_cpu1]  32.1µs ±13%  31.9µs ±12%     ~     (p=0.470 n=39+36) BM_ConvFloatDepthwiseFwdCPU4_conv4_float/real_time  [32_28_28_128_1_128_3_3_1_2_cpu4]  26.2µs ±30%  25.5µs ±44%     ~     (p=0.677 n=38+37) BM_ConvFloatDepthwiseFwdCPU1_conv5_float/real_time  [32_14_14_512_1_512_3_3_1_2_cpu1]  31.5µs ±18%  31.7µs ±17%     ~     (p=0.742 n=38+39) BM_ConvFloatDepthwiseFwdCPU4_conv5_float/real_time  [32_14_14_512_1_512_3_3_1_2_cpu4]  28.5µs ±28%  27.3µs ±29%     ~     (p=0.208 n=35+37) BM_ConvFloatDepthwiseFwdCPU1_conv6_float/real_time  [32_7_7_1024_1_1024_3_3_1_2_cpu1]  29.3µs ±16%  28.9µs ±21%     ~     (p=0.334 n=39+31) BM_ConvFloatDepthwiseFwdCPU4_conv6_float/real_time  [32_7_7_1024_1_1024_3_3_1_2_cpu4]  8.35µs ±62%  7.08µs ±46%  15.24%  (p=0.026 n=40+37) BM_ConvFloatDepthwiseFwdCPU1_conv7_float/real_time  [32_112_112_3_8_24_3_3_2_2_cpu1 ]  31.2µs ±17%  31.4µs ±22%     ~     (p=0.987 n=35+38) BM_ConvFloatDepthwiseFwdCPU4_conv7_float/real_time  [32_112_112_3_8_24_3_3_2_2_cpu4 ]  25.9µs ±45%  26.5µs ±32%     ~     (p=0.859 n=39+38) BM_ConvFloatDepthwiseFwdCPU1_conv8_float/real_time  [32_112_112_3_8_24_3_3_2_1_cpu1 ]  30.0µs ±16%  30.5µs ±18%     ~     (p=0.228 n=34+33) BM_ConvFloatDepthwiseFwdCPU4_conv8_float/real_time  [32_112_112_3_8_24_3_3_2_1_cpu4 ]  26.2µs ±41%  24.4µs ±53%     ~     (p=0.288 n=36+40) BM_ConvFloatDepthwiseFwdCPU1_conv9_float/real_time  [1_100_100_72_1_72_3_3_1_2_cpu1 ]  26.5µs ±16%  25.6µs ±15%     ~     (p=0.051 n=34+37) BM_ConvFloatDepthwiseFwdCPU4_conv9_float/real_time  [1_100_100_72_1_72_3_3_1_2_cpu4 ]  6.33µs ±37%  5.60µs ±36%  11.46%  (p=0.011 n=40+35) BM_ConvFloatDepthwiseFwdCPU1_conv10_float/real_time [1_100_100_72_1_72_5_5_1_2_cpu1 ]  26.4µs ±13%  27.8µs ±20%     ~     (p=0.140 n=33+40) BM_ConvFloatDepthwiseFwdCPU4_conv10_float/real_time [1_100_100_72_1_72_5_5_1_2_cpu4 ]  14.6µs ±79%   9.2µs ±90%  36.75%  (p=0.000 n=40+40)",2025-01-09T21:30:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84499
copybara-service[bot],AHWB must outlive tensor buffer,AHWB must outlive tensor buffer,2025-01-09T20:57:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84498
mahmoud-abuzaina,Replacement PR for #76210 Add support for quint8 type for uniform_quantize and uniform_ dequantize ops,This PR is a replacement for https://github.com/tensorflow/tensorflow/pull/76210,2025-01-09T20:55:07Z,awaiting review ready to pull size:L,closed,0,3,https://github.com/tensorflow/tensorflow/issues/84497,abuzaina Can you please rebase and try again.  The internal error fixed on sync. , I just rebased it. Thanks!,>> Presubmit  Linux x86 CPU  Py+CPP Test Suite `//tensorflow/python/kernel_tests:collective_ops_multi_worker_test` timeout. Looking into it.  
copybara-service[bot],Add support for default memory space descriptions;,Add support for default memory space descriptions;,2025-01-09T20:36:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84496
copybara-service[bot],Move convert_async_collectives_to_sync to collectives directory,Move convert_async_collectives_to_sync to collectives directory,2025-01-09T20:32:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84495
copybara-service[bot],[XLA] Make HloParser idempotent wrt independent parses,[XLA] Make HloParser idempotent wrt independent parses Using a static local variable means that parsing the same text twice would result in different modules.,2025-01-09T20:04:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84494
copybara-service[bot],Add source line and stack_frame functionality in hlo_module_map and utils,Add source line and stack_frame functionality in hlo_module_map and utils,2025-01-09T19:19:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84493
copybara-service[bot],Fix ios compilation issue,Fix ios compilation issue,2025-01-09T18:57:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84492
codinglover222,Add div/truediv support for bool-val tensor.,"To achieve compliance of tensorflow divide/truediv operator on boolean tensors with numpy, the solution is to cast tensors to int32. Numpy divide behavior on boolean values  Tensorflow divide/truediv behavior on boolean tensors   ",2025-01-09T18:41:25Z,comp:ops size:M,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84491,It depends. If we want to keep the same type then a table would be better ``` T T T T F ? F F ? F T F ``` Where `?` should probably be nans? If we change type (convert to float) then conversion is useful.,I question the value of this.  We don't necessarily want to blindly follow what numpy does.  What would a user actually need a truediv with booleans for?,"Since tensorflow tensors supports operations with numpy arrays, it is good to have numpylike features in tensorflow. This may make tensorflow math more consistent with users that are familiar with numpy. Just my thoughts.","Hi , , This PR has been drafted for a few days. Could you please look into this one and see if it's completed?  Thank you !"
copybara-service[bot],[xla:gpu] Rename gpu_clique_locking to gpu_cliques for consistency with XLA:CPU,[xla:gpu] Rename gpu_clique_locking to gpu_cliques for consistency with XLA:CPU,2025-01-09T18:33:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84490
copybara-service[bot],[xla:cpu] Migrate AllReduce to RendezvousSingle API,[xla:cpu] Migrate AllReduce to RendezvousSingle API,2025-01-09T18:16:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84489
copybara-service[bot],[xla:cpu] Migrate AllGather to RendezvousSingle API,[xla:cpu] Migrate AllGather to RendezvousSingle API,2025-01-09T18:15:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84488
copybara-service[bot],[XLA:GPU] Replace genrule by LLVM archive parser to load fatbin in tests,[XLA:GPU] Replace genrule by LLVM archive parser to load fatbin in tests generated .a library has a different name depending on cuda/rocm build.,2025-01-09T18:09:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84487
copybara-service[bot],[XLA:TPU] Disable memory space assignment pass via `exec_time_optimization_effort`.,[XLA:TPU] Disable memory space assignment pass via `exec_time_optimization_effort`.,2025-01-09T17:51:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84486
copybara-service[bot],Add more traces for custom calls.,Add more traces for custom calls.,2025-01-09T17:33:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84485
copybara-service[bot],Plumb memory space descriptions through IFRT.,Plumb memory space descriptions through IFRT.,2025-01-09T17:31:50Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84484
copybara-service[bot],Add LiteRT accelerator API implementation.,Add LiteRT accelerator API implementation.,2025-01-09T17:28:03Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84483
copybara-service[bot],Integrate LLVM at llvm/llvm-project@644de6ad1c75,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 644de6ad1c75,2025-01-09T17:14:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84482
vfdev-5,Updated rules_python patch to get 3.13.2 python, ,2025-01-09T17:07:50Z,comp:xla size:M,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84481,"Hi 5, Can you please resolve the conflicts. Many thanks !"," sorry for delay, I resolved the conflict between this PR and the master branch and also updated 3.13.1 > 3.13.2","Hi, 5 unfortunately with how we have the dependencies setup between TF, XLA, and TSL this is not importing right now and i will need to patch it google side.  Could you please leave only the direct TF third_party change (third_party/py) and i will patch back in the XLA and TSL ones.",For reference xla commit corresponding to this is: https://github.com/openxla/xla/commit/2264e8c19d321520c8ade747e699ce7b8b9dcc38
copybara-service[bot],Update users of moved TSL headers to use new location in XLA for `activity_watcher`,Update users of moved TSL headers to use new location in XLA for `activity_watcher`,2025-01-09T17:05:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84480
copybara-service[bot],PR #20911: [XLA:GPU] Update cudnn frontend version to 1.9,"PR CC(Edit  a link to notebooks directory): [XLA:GPU] Update cudnn frontend version to 1.9 Imported from GitHub PR https://github.com/openxla/xla/pull/20911 cudnn frontend 1.9 is released, there are some new features that cudnn flash attention will incorporate, hence this PR. * flex attention with arbitrary pointwise operations after softmax in cudnn flash attention graph. * sequence packing enhancement with reduced workspace size. Release note: https://github.com/NVIDIA/cudnnfrontend/releases/tag/v1.9.0 Copybara import of the project:  07a0d7a6cdff107b3c69dd2a66fc2b247de056e7 by cjkkkk : update Merging this change closes CC(Edit  a link to notebooks directory) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20911 from Cjkkkk:update_cudnn_fe_1.9 07a0d7a6cdff107b3c69dd2a66fc2b247de056e7",2025-01-09T16:34:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84479
copybara-service[bot],[XLA:CPU] Remove unused stream_executor host code.,[XLA:CPU] Remove unused stream_executor host code.,2025-01-09T15:34:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84478
copybara-service[bot],[XLA:GPU] Migrate collective bytes transfered to `BytesTransferred`.,[XLA:GPU] Migrate collective bytes transfered to `BytesTransferred`. `output_bytes_accessed` is not necessarily transferred by the network (for example reduce scatter breaks the rule),2025-01-09T15:30:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84477
copybara-service[bot],[XLA:GPU] Fix reduce scatter transfered bytes.,[XLA:GPU] Fix reduce scatter transfered bytes.,2025-01-09T15:25:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84476
dewa-ai,Update README.md,"This code provides the implementation of a custom version of the Adam optimizer called NonFusedAdam for TensorFlow. It inherits from its base optimizer class and implements the Adam algorithm with or without AMSGrad and other hyperparameters including learning rate, betas, and epsilon. To use this optimizer: 1. Make sure TensorFlow is installed. 2. Save the optimizer code in a Python file (like custom_adam.py). 3. Import the optimizer in your script. 4. Build a model, compile it using NonFusedAdam, and train it with your dataset (such as MNIST). 5. Run the script to train the model and check its performance.",2025-01-09T14:54:44Z,size:M,closed,0,1,https://github.com/tensorflow/tensorflow/issues/84475,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],[XLA:GPU] move TransposeFolding after simplifier pipeline,"[XLA:GPU] move TransposeFolding after simplifier pipeline Combination of DotDecompose, AlgebraicSimplifier, and TransposeFolding might never reach a fixed point and stuck in a rewrite loop. Moving TransposeFolding in simplification2 pipeline should result in a similar output as we keep the relative order. Also:  Added a few tests to detect / record cases when we run into rewrite loop  Log warning when HloPassFix reaches iteraction limit as this is likely a similar bug",2025-01-09T14:47:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84474
copybara-service[bot],Update to match upstream API change (NFC).,"Update to match upstream API change (NFC). This method was renamed but staging function kept, switch to renamed variant.",2025-01-09T13:21:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84473
copybara-service[bot],Update to match upstream API change (NFC).,"Update to match upstream API change (NFC). This method was renamed but staging function kept, switch to renamed variant.",2025-01-09T13:17:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84472
copybara-service[bot],Moving test from Triton patch file internally. The associated fix was obsolete when reorderValues function was removed. We still want to keep the test and remove the patch.,Moving test from Triton patch file internally. The associated fix was obsolete when reorderValues function was removed. We still want to keep the test and remove the patch.,2025-01-09T13:10:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84471
copybara-service[bot],Update to match upstream API change (NFC).,"Update to match upstream API change (NFC). This method was renamed but staging function kept, switch to renamed variant.",2025-01-09T13:07:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84470
copybara-service[bot],Adding vectorization support for atomic_rmw.,"Adding vectorization support for atomic_rmw. Currently only supports f32 vectors of size 2 or 4. There is a bug in LLVM when lowering to PTX that lowers the vectorized atomic RMW incorrectly. For now, we scalarize, so effectively this is disabled. This should be followedup with a direct lowering to PTX as a workaround.",2025-01-09T12:39:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84469
copybara-service[bot],Move xla::gpu::mlir_converter namespace to xla::emitters namespace.,Move xla::gpu::mlir_converter namespace to xla::emitters namespace. The code is not gpu specific. Also move the code to a corresponding directory.,2025-01-09T12:34:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84468
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T12:34:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84467
fighting300,Build iOS tensorflowLite error with iOS library," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version 2.8  Custom code Yes  OS platform and distribution iOS   Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? When i build tensorflow lite with library, i encounter error below，the error show：Undefined symbols for architecture arm64:   ""std::__1::basic_string, std::__1::allocator>::find(char, unsigned long) const"",   Standalone code to reproduce the issue ```shell Undefined symbols for architecture arm64:   ""std::__1::basic_string, std::__1::allocator>::find(char, unsigned long) const"", referenced from:       l001 in libNeuralnet_a.a32       l001 in libNeuralnet_a.a32       l001 in libNeuralnet_a.a32       l001 in libNeuralnet_a.a32       l001 in libNeuralnet_a.a32       l001 in libNeuralnet_a.a32   ""std::__1::basic_string, std::__1::allocator>::rfind(char, unsigned long) const"", referenced from:       l001 in libNeuralnet_a.a32       l001 in libNeuralnet_a.a32       l001 in libNeuralnet_a.a32   ""std::__1::basic_string, std::__1::allocator>::compare(unsigned long, unsigned long, char const*) const"", referenced from:       l001 in libNeuralnet_a.a32   ""std::__1::basic_string, std::__1::allocator>::compare(unsigned long, unsigned long, char const*, unsigned long) const"", referenced from: ```  Relevant log output _No response_",2025-01-09T10:59:27Z,stat:awaiting response type:bug stale comp:lite TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84466,"Hi,   I apologize for the delayed response, if possible could you please help us with exact steps which you followed before encountering mentioned error in the issue template or if you followed any official documentation please help us with it to replicate the same behavior from our end ? **EDIT :** I believe you're following this official documentation and I see you mentioned TensorFlow version 2.8 so could you please give it try with latest version of TensorFlow and see is it working as expected or not ? If issue still persists please let us know with error log for further investigation Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[xla:cpu:benchmarks] Add scripts to run Gemma2 Keras model.,[xla:cpu:benchmarks] Add scripts to run Gemma2 Keras model.,2025-01-09T10:29:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84465
copybara-service[bot],[XLA:GPU] NFC: a few small polishing touches.,[XLA:GPU] NFC: a few small polishing touches.,2025-01-09T10:10:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84464
Venkat6871,Fix typos in multiple documentation strings,"Hi, Team I observed few typos in the documentation strings and I have fixed those typos so please do the needful. Thank you.",2025-01-09T10:07:53Z,ready to pull comp:dist-strat size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84463
copybara-service[bot],Update GraphDef version to 2102.,Update GraphDef version to 2102.,2025-01-09T09:30:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84462
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T09:27:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84461
MoFHeka,GPU Profiling: MemoryProfile do not contain memory events when profile remote worker.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version nightly  Custom code No  OS platform and distribution Ubuntu 22.04  Python version Python 3.12  CUDA/cuDNN version CUDA 12.4  GPU model and memory A100 80GB  Current behavior? Start a simple any collective training with Tensorflow cluster config. And then use RPC client capture_profile in Tensorboard or tf.profiler.experimental.client.trace.  No any memory profile events or OP profiler, but only trace view.  Standalone code to reproduce the issue **tf_allreduce.py** ```python import tensorflow as tf from tensorflow.python.ops.collective_ops import all_reduce, all_reduce_v2 from tensorflow.python.eager import context from tensorflow.core.protobuf import config_pb2 from tensorflow.python.distribute import cluster_resolver as cluster_resolver_lib cluster_resolver = cluster_resolver_lib.TFConfigClusterResolver() cluster = cluster_resolver.cluster_spec() task_type = cluster_resolver.task_type task_id = cluster_resolver.task_id experimental_config = config_pb2.ConfigProto.Experimental(     share_cluster_devices_in_session=False,     share_session_state_in_clusterspec_propagation=False ) config = config_pb2.ConfigProto(experimental=experimental_config) config.experimental.collective_group_leader = '/job:worker/replica:0/task:0' server = tf.distribute.Server(cluster,                               job_name=task_type,                               task_index=task_id,                               protocol=""grpc"",  ""grpc+verbs""                               config=config) run_options = config_pb2.RunOptions() with tf.compat.v1.Session(target=server.target, config=config) as sess:     tensor = tf.Variable(tf.ones([2, 2]), dtype=tf.float32)     init = tf.compat.v1.global_variables_initializer()     sess.run(init)     sess.run(tf.print([""tensor:"",tensor]))     reduced_tensor = all_reduce(tensor, group_size=2, group_key=4321, instance_key=1234, merge_op='Add', final_op='Id', communication_hint='auto')     run_options.experimental.collective_graph_key = 6     while True:         sess.run(tf.print([""reduced_tensor:"",reduced_tensor]), options=run_options) ``` Run script to start server. ```bash CUDA_VISIBLE_DEVICES=0 TF_CONFIG='{""cluster"":{""worker"":[""localhost:2223"",""localhost:2224""]},""task"":{""type"":""worker"",""index"":0}}' python tf_allreduce.py& CUDA_VISIBLE_DEVICES=1 TF_CONFIG='{""cluster"":{""worker"":[""localhost:2223"",""localhost:2224""]},""task"":{""type"":""worker"",""index"":1}}' python tf_allreduce.py& ```  use capture_profile in Tensorboard or tf.profiler.experimental.client.trace. ```python tf.profiler.experimental.client.trace(   'grpc://localhost:2223,grpc://localhost:2224',    '/tmp/my_tb_dir',    2000, ) ``` Try to convert xplane.pb to memory_profile, nothing show. ```python from tensorflow.python.profiler.internal import _pywrap_profiler as profiler_wrapper json = profiler_wrapper.xspace_to_tools_data([""xxx.xplane""], ""memory_profile"") ``` **Relevant log output** ``` {""memoryProfilePerAllocator"":{},""numHosts"":1,""memoryIds"":[]} ``` Relative issue: CC(GPU Profiling: MemoryProfile do not contain memory events.) ",2025-01-09T09:26:20Z,stat:awaiting tensorflower type:bug comp:core TF 2.18,open,0,0,https://github.com/tensorflow/tensorflow/issues/84460
Nana16794625829,-,,2025-01-09T09:23:28Z,size:M,closed,0,1,https://github.com/tensorflow/tensorflow/issues/84459,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],PR #21163: [GPU] Redefine the flag xla_gpu_cudnn_gemm_fusion_level.,PR CC(FAILED: Build did NOT complete successfully (101 packages loaded)): [GPU] Redefine the flag xla_gpu_cudnn_gemm_fusion_level. Imported from GitHub PR https://github.com/openxla/xla/pull/21163 The levels defined so far were used for testing/benchmarking. The new definitions will help the architecturetargeted deployment of the feature. This change also lets the relevant tests run manually on Ampere+ GPUs  previously they were skipped before Hopper. Copybara import of the project:  6bcca3cead59f584a6f7d69e2b56aeda94e97414 by Ilia Sergachev : [GPU] Redefine the flag xla_gpu_cudnn_gemm_fusion_level. The levels defined so far were used for testing/benchmarking. The new definitions will help the architecturetargeted deployment of the feature. This change also lets the relevant tests run manually on Ampere+ GPUs  previously they were skipped before Hopper.  91ea9520de83a75b2c9d32f513d0d3d6044ca8dd by Ilia Sergachev : add missing build dependency Merging this change closes CC(FAILED: Build did NOT complete successfully (101 packages loaded)) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21163 from openxla:cudnn_gemm_redefine_levels 91ea9520de83a75b2c9d32f513d0d3d6044ca8dd,2025-01-09T09:18:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84458
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T09:18:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84457
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T09:16:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84456
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T09:14:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84455
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T09:14:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84454
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T09:13:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84453
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T09:12:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84452
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T09:11:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84451
copybara-service[bot],PR #21166: [DOC] Fix a link in the documentation.,PR CC(Numpy with Tensorflow Eager as backend?): [DOC] Fix a link in the documentation. Imported from GitHub PR https://github.com/openxla/xla/pull/21166 Copybara import of the project:  b939d5aea471e4b267a806b19102b6d56a7abe0a by Ilia Sergachev : [DOC] Fix a link in the documentation. Merging this change closes CC(Numpy with Tensorflow Eager as backend?) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21166 from openxla:fix_doc b939d5aea471e4b267a806b19102b6d56a7abe0a,2025-01-09T08:56:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84450
copybara-service[bot],PR #21175: [DOC] Fix a mistype.,PR CC(Unable to convert keras_model.h5 to .tflite): [DOC] Fix a mistype. Imported from GitHub PR https://github.com/openxla/xla/pull/21175 Copybara import of the project:  caaf17448ae8dade929d728852093ec82384337b by Ilia Sergachev : [DOC] Fix a mistype. Merging this change closes CC(Unable to convert keras_model.h5 to .tflite) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21175 from openxla:fix_mistype caaf17448ae8dade929d728852093ec82384337b,2025-01-09T08:46:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84449
copybara-service[bot],PR #21123: Disable cuDNN fusions explicitly in tests that are testing the Triton path,"PR CC(Unresolved External Symbol: public: double __cdecl double_conversion::StringToDoubleConverter::StringToDouble(char const *,int,int *)const): Disable cuDNN fusions explicitly in tests that are testing the Triton path Imported from GitHub PR https://github.com/openxla/xla/pull/21123 cuDNN fusions are OFF by default, and some tests that are testing the Triton codegen path implicitly rely on this. It is best to turn off cuDNN fusions explicitly in these tests, e.g., NVIDIA has internal builds that turn on cuDNN fusions and these tests suddenly start to fail in CI. Copybara import of the project:  ab9827658a7bfbc68620b920525360e5df9dfaf2 by Dimitris Vardoulakis : Disable cuDNN fusions explicitly in tests that are testing the Triton path. Merging this change closes CC(Unresolved External Symbol: public: double __cdecl double_conversion::StringToDoubleConverter::StringToDouble(char const *,int,int *)const) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21123 from dimvar:disablecudnnintritontests ab9827658a7bfbc68620b920525360e5df9dfaf2",2025-01-09T08:44:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84448
copybara-service[bot],Generalize `GetFirstMergeableDimForSortOperand` and rename it as `GetFirstTargetDimToMoveShardingTiles`.,Generalize `GetFirstMergeableDimForSortOperand` and rename it as `GetFirstTargetDimToMoveShardingTiles`. `GetFirstTargetDimToMoveShardingTiles` can be used for moving the sharding tiles from a source dimension to a target dimension when the source dimension and target dimension are different and the size of target dimension is divisible by the merged tile size. This util function will be used in the dimensions that need replication in the partitioner. This cl has no behavior change. We will use this util function to support 1. Concat dimension in concat operations 2. Slice dimensions in dynamicslice operations,2025-01-09T07:39:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84447
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T07:15:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84446
copybara-service[bot],PR #20340: Fix missing template value,"PR CC(reg. tensorflowgpu in amd windows): Fix missing template value Imported from GitHub PR https://github.com/openxla/xla/pull/20340 Fixes a bug introduced in this change: https://github.com/google/tsl/pull/2944 The change makes use of a template variable `%{compiler}`, that is not defined for this file. This causes the `fnocanonicalsystemheaders` option to be set for Clang builds, and Clang will fail with an error about that command line flag not being defined. Copybara import of the project:  75a3d3fbcf2ead55df3872aa80ff21ac3dd9336c by Charles Hofer : Fix missing template value  e08537b09200b0037db7a05780dea0d525399376 by Charles Hofer : Change flag to compiler_is_clang  373f359cbd8d02ee850d98fed92a7bbca4a09c1b by Charles Hofer : Fix typo  2be3c309d05f93a48dd9fdd06af8159108920516 by Harsha HS : [ROCm] Add cudaonly tags for nvidia profiler test Merging this change closes CC(reg. tensorflowgpu in amd windows) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20340 from ROCm:fixmissingtemplatevalue 2be3c309d05f93a48dd9fdd06af8159108920516",2025-01-09T07:13:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84445
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T07:08:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84444
15001366252, expect the compilation to pass but there are errors， how to fix it," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.15.1  Custom code Yes  OS platform and distribution LINUX Ubuntu 18.04  Mobile device _No response_  Python version 3.8  Bazel version 6.1.0  GCC/compiler version 9.5.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior?  expect the compilation to pass but there are errors ERROR: /home/aist/.cache/bazel/_bazel_aist/572037a80443c6142f9def570fe3ea55/external/XNNPACK/BUILD.bazel:3050:19: Compiling src/tensor.c failed: (Exit 1): gcc failed: error executing command (from target //:subgraph) /usr/local/bin/gcc U_FORTIFY_SOURCE fstackprotector Wall Wunusedbutsetparameter Wnofreenonheapobject fnoomitframepointer g0 O2 'D_FORTIFY_SOURCE=1' DNDEBUG ffunctionsections ... (remaining 98 arguments skipped) In file included from external/XNNPACK/src/tensor.c:19: external/XNNPACK/src/xnnpack/subgraph.h:431:17: error: array type has incomplete element type 'xnn_timestamp' {aka 'struct timespec'}   431                  ^~~~~~~~ Target //tensorflow:libtensorflow_cc.so failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 969.727s, Critical Path: 11.17s INFO: 2534 processes: 701 internal, 1833 local. FAILED: Build did NOT complete successfully  Standalone code to reproduce the issue ```shell bazel build config=opt //tensorflow:libtensorflow_cc.so ```  Relevant log output _No response_",2025-01-09T06:47:58Z,type:build/install,closed,0,1,https://github.com/tensorflow/tensorflow/issues/84443,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T06:42:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84442
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T06:33:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84441
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T06:32:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84440
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T06:21:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84439
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T06:21:22Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84438
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T06:21:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84437
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T06:20:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84436
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T06:19:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84435
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T06:19:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84434
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T06:15:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84433
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T06:14:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84432
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T06:06:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84431
copybara-service[bot],[xla] Delete unused Rendezvous implementation,[xla] Delete unused Rendezvous implementation,2025-01-09T05:43:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84430
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-09T05:36:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84429
copybara-service[bot],Internal change only,Internal change only,2025-01-09T03:02:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84428
copybara-service[bot],Add original cp name prefix to the send/receives instructions for better readability.,Add original cp name prefix to the send/receives instructions for better readability.,2025-01-09T02:41:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84427
copybara-service[bot],Minor code simplification.,"Minor code simplification. There is only one call to `TfLiteDelegateCopyFromBufferHandleInternal`, which passes in `t` for the `tensor` parameter and `t>delegate` for the `delegate` parameter, so inside this function, `tensor>delegate` and `delegate` are equivalent expressions that evaluate to the same value.  But referencing `delegate` rather than `tensor>delegate` is simpler and more readable here, and makes the nullness check match the dereference on the following line, and is more consistent with the other functions in this file.  So this change modifies the code to use `delegate` rather than `tensor>delegate`.",2025-01-09T02:11:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84426
copybara-service[bot],Make PJRTArray::Create validate the create-request for addressable devices only.,Make PJRTArray::Create validate the createrequest for addressable devices only.,2025-01-09T02:09:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84425
copybara-service[bot],Increase wheel limit size up to 270M for a temporary nightlies fix.,Increase wheel limit size up to 270M for a temporary nightlies fix.,2025-01-09T01:16:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84424
copybara-service[bot],Internal change. Need to write more words to keep the presubmit happy.,Internal change. Need to write more words to keep the presubmit happy.,2025-01-09T01:11:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84423
copybara-service[bot],Remove unused free_visitors from DeviceMemAllocator.,Remove unused free_visitors from DeviceMemAllocator.,2025-01-09T01:10:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84422
copybara-service[bot],Remove unused MemoryTypeString function.,Remove unused MemoryTypeString function.,2025-01-09T01:09:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84421
copybara-service[bot],Extend MTK dispatch API to support DMA-BUF buffers,Extend MTK dispatch API to support DMABUF buffers,2025-01-09T00:48:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84420
copybara-service[bot],Port array_elementwise_ops_test to derive from `HloTestBase`.,Port array_elementwise_ops_test to derive from `HloTestBase`.,2025-01-09T00:38:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84419
copybara-service[bot],Port convert_test to derive from `HloTestBase`.,Port convert_test to derive from `HloTestBase`.,2025-01-09T00:36:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84418
copybara-service[bot],Port pad_test to derive from `HloTestBase`.,Port pad_test to derive from `HloTestBase`.,2025-01-09T00:35:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84416
copybara-service[bot],Port custom_call_test to derive from `HloTestBase`.,Port custom_call_test to derive from `HloTestBase`.,2025-01-09T00:33:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84415
copybara-service[bot],Add `ClientLibraryTestRunnerMixin`.,"Add `ClientLibraryTestRunnerMixin`. `ClientLibraryTestRunnerMixin` is a sortof replacement for `ClientLibraryTestBase` to run tests on top of `HloTestBase` and friends (e.g. `HloRunnerAgnosticTestBase`).  This is to enable a future migration to PjRt and TFRT. Due to `ClientLibraryTestBase` containing many clientspecific calls, moving tests is not as trivial as simply dropping in a new base class. The idea with this class is just to make that migration simpler and to reduce (but not eliminate) the amount of code changes required in tests. Migration timeline for `ClientLibraryTestBase` tests: 1. `class XYZ: ClientLibraryTestBase` (starting point) 2. `class XYZ: ClientLibraryTestRunnerMixin` (intermediate state) 3. `class XYZ: ClientLibraryTestRunnerMixin>` (end state)",2025-01-09T00:32:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84414
nathom,Unable to connect to TPU through Cloud VM (metadata issue?)," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source binary  TensorFlow version v2.18.0rc24g6550e4bd802 2.18.0  Custom code Yes  OS platform and distribution tpuubuntu2204base  Mobile device _No response_  Python version 3.11.2  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am on a VM instance trying to connect to a tpu v432 using a test script. I installed tensorflowtpu on both the VM (in venv) and TPU (globally) as per the instructions from the google website. It seems like there is an issue with getting TPU metadata. It is able to connect to the metadata server when I request manually from the VM: ``` $ curl http://169.254.169.254/computeMetadata/v1/ H ""MetadataFlavor: Google"" instance/ oslogin/ project/ ``` Any help would be appreciated!  Standalone code to reproduce the issue ```shell resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_name) tf.config.experimental_connect_to_cluster(resolver) try:     tf.tpu.experimental.initialize_tpu_system(resolver)     print(""TPU initialized:"", resolver.master()) except Exception as e:     print(""Failed to initialize TPU:"", e) ```  Relevant log output ```shell $ python hello.py 20250108 23:49:33.189260: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250108 23:49:33.221197: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:95] Opening library: /home/ucsdwanglab/test_tpu/.venv/lib/python3.11/sitepackages/tensorflow/python/platform/../../libtensorflow_cc.so.2 20250108 23:49:33.221290: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:121] Libtpu path is: /home/ucsdwanglab/test_tpu/.venv/lib/python3.11/sitepackages/libtpu/libtpu.so Failed to get TPU metadata (tpuenv) from instance metadata for variable CHIPS_PER_HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error === Source Location Trace: ===  learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93 learning/45eac/tfrc/runtime/env_var_utils.cc:50 Failed to get TPU metadata (tpuenv) from instance metadata for variable HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error === Source Location Trace: ===  learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93 learning/45eac/tfrc/runtime/env_var_utils.cc:50 Failed to get TPU metadata (tpuenv) from instance metadata for variable ALT: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error === Source Location Trace: ===  learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93 learning/45eac/tfrc/runtime/env_var_utils.cc:50 Failed to get TPU metadata (tpuenv) from instance metadata for variable WRAP: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error === Source Location Trace: ===  learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93 learning/45eac/tfrc/runtime/env_var_utils.cc:50 Failed to get TPU metadata (acceleratortype) from instance metadata for variable TPU_ACCELERATOR_TYPE: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error === Source Location Trace: ===  learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93 Failed to find host bounds for accelerator type: WARNING: could not determine TPU accelerator type, please set env var `TPU_ACCELERATOR_TYPE` manually, otherwise libtpu.so may not properly initialize. Failed to get TPU metadata (agentworkernumber) from instance metadata for variable TPU_WORKER_ID: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error === Source Location Trace: ===  learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93 Failed to get TPU metadata (workernetworkendpoints) from instance metadata for variable TPU_WORKER_HOSTNAMES: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error === Source Location Trace: ===  learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93 WARNING: Logging before InitGoogle() is written to STDERR E0000 00:00:1736380405.363400    3192 common_lib.cc:511] INVALID_ARGUMENT: Error: unexpected worker hostname 'WARNING: could not determine TPU worker hostnames or IP addresses' from env var TPU_WORKER_HOSTNAMES. Expecting a valid hostname or IP address without port number. (Full TPU workers' addr string: WARNING: could not determine TPU worker hostnames or IP addresses, please set env var `TPU_WORKER_HOSTNAMES` manually, otherwise libtpu.so may not properly initialize.) === Source Location Trace: ===  learning/45eac/tfrc/runtime/libtpu_init_utils.cc:173 20250108 23:56:48.526584: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1736380609.730442    3192 context_distributed_manager.cc:762] unknown service tensorflow.WorkerService Additional GRPC error information from remote target /job:worker/replica:0/task:0 while calling /tensorflow.WorkerService/GetStatus: :{""created"":"".730372913"",""description"":""Error received from peer ipv4:10.130.0.3:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""unknown service tensorflow.WorkerService"",""grpc_status"":12} E0108 23:56:49.730822322    3192 completion_queue.cc:244]    assertion failed: queue.num_items() == 0 https://symbolize.stripped_domain/r/?trace=7f1ccaf5cebc,7f1ccaf0e04f&map=  *** SIGABRT received by PID 3192 (TID 3192) on cpu 4 from PID 3192; stack trace: *** PC: @     0x7f1ccaf5cebc  (unknown)  (unknown)     @     0x7f1caa302841       1888  (unknown)     @     0x7f1ccaf0e050   18460496  (unknown)     @     0x7f1ccaed1c60  (unknown)  (unknown) https://symbolize.stripped_domain/r/?trace=7f1ccaf5cebc,7f1caa302840,7f1ccaf0e04f,7f1ccaed1c5f&map=  E0108 23:56:49.732558    3192 coredump_hook.cc:316] RAW: Remote crash data gathering hook invoked. E0108 23:56:49.732569    3192 coredump_hook.cc:355] RAW: Skipping coredump since rlimit was 0 at process start. E0108 23:56:49.732575    3192 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec. E0108 23:56:49.732580    3192 coredump_hook.cc:411] RAW: Sending fingerprint to remote end. E0108 23:56:49.732595    3192 coredump_hook.cc:420] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory E0108 23:56:49.732601    3192 coredump_hook.cc:472] RAW: Dumping core locally. E0108 23:56:49.745981    3192 process_state.cc:805] RAW: Raising signal 6 with default behavior Aborted ```",2025-01-09T00:04:51Z,stat:awaiting response type:bug comp:tpus TF 2.18,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84413,", Could you please provide more information and also steps you have followed to use the TPU's which helps to debug the issue in an effective way. Thank you!","I think this was my mistake. I was using a Google Cloud VM and trying to connect to the TPU pods from there. I was able to resolve the issue by connecting directly to one of the TPU hosts, and running commands on all workers using `gcloud`. Maybe the error messages could be made more helpful, though.",", Glad the issue was resolved by connecting the TPU hosts. Could you please feel free to move this issue to closed status. Thank you!",Are you satisfied with the resolution of your issue? Yes No
rizkyy702,bug," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible). ``` (You can paste links or attach files by dragging & dropping them below)  Provide links to your updated versions of the above two colab notebooks.  Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model. ```  Option B: Paste your code here or provide a link to a custom endtoend colab ``` (You can paste links or attach files by dragging & dropping them below)  Include code to invoke the TFLite Converter Python API and the errors.  Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model. ```  3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2025-01-09T00:02:56Z,TFLiteConverter,closed,0,1,https://github.com/tensorflow/tensorflow/issues/84412,Nothing provided in the template.
copybara-service[bot],Improve comment in ShapeUtil,Improve comment in ShapeUtil,2025-01-08T23:58:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84411
copybara-service[bot],Use const reference to context instead of universal reference.,Use const reference to context instead of universal reference.,2025-01-08T23:45:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84410
copybara-service[bot],Integrate LLVM at llvm/llvm-project@ac08f0dfef27,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match ac08f0dfef27,2025-01-08T23:24:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84409
copybara-service[bot],[xla:cpu] Migrate AllToAll to RendezvousSingle API,[xla:cpu] Migrate AllToAll to RendezvousSingle API,2025-01-08T22:47:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84408
copybara-service[bot],[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,[HLO Componentization] Populate hlo/testlib subcomponent (Phase II). This CL takes care of 1. Migrating external projects dependencies from ``` tensorflow/compiler/xla:test tensorflow/compiler/xla:test_helpers tensorflow/compiler/xla/service:pattern_matcher_gmock ``` to `tensorflow/compiler/xla/hlo/testlib:*`,2025-01-08T22:21:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84407
copybara-service[bot],"#tf-data For an empty `from_list`, update the error message to suggest a workaround.","tfdata For an empty `from_list`, update the error message to suggest a workaround.",2025-01-08T22:06:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84406
copybara-service[bot],[xla:cpu] Migrate CollectivePermute to RendezvousSingle API,[xla:cpu] Migrate CollectivePermute to RendezvousSingle API Migrate from deprecated rendezvous APIs to the new one.,2025-01-08T22:05:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84405
copybara-service[bot],Reverts changelist 546034127,Reverts changelist 546034127,2025-01-08T21:39:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84404
copybara-service[bot],[xla:collectives] Remove redundant nranks argument from collectives API,[xla:collectives] Remove redundant nranks argument from collectives API,2025-01-08T21:13:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84403
copybara-service[bot],[xla:cpu] Consolidate all XLA:CPU collectives under backends/cpu/collectives,[xla:cpu] Consolidate all XLA:CPU collectives under backends/cpu/collectives,2025-01-08T21:04:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84402
copybara-service[bot],PR #20808: [GSPMD] Partitions collective permute instructions in manual sharding group.,"PR CC(Feature requested by issue 18354: No gradient defined for operation DepthwiseConv2dNativeBackpropFilter ): [GSPMD] Partitions collective permute instructions in manual sharding group. Imported from GitHub PR https://github.com/openxla/xla/pull/20808 This is a small fix in GSPMD partitioning for partitioning collective permutes instructions added in manual sharding group. In JAX, we can add `ppermute` instruction in shard_map. In cases where we have shard_map with auto axes specified, collective permuting an operand even with the same sharding will end up with an `allgather` and then collective permute, which leads to inefficient collectives. The correct and efficient way is to partition the collective permute as an elementwise op. The unit test added provides a repro. Also, the JAX unit test in https://github.com/jaxml/jax/blob/fa9c7edf736516052df6eab22947bc627d0deca3/tests/shard_map_test.pyL2167 gives a realworld JAX example. Copybara import of the project:  8ee6ecd51f6e4aae8e3d92a6a439a60f53ab02ae by Yunlong Liu : A hacky fix on partitioning collective permute.  e50e87696defb290f7561a7808ee42ebbc11e144 by Yunlong Liu : Local change.  84eb38597c783a4488774823c2c464296a8c54c7 by Yunlong Liu : Simplifies sharding in tests. Merging this change closes CC(Feature requested by issue 18354: No gradient defined for operation DepthwiseConv2dNativeBackpropFilter ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20808 from yliu120:cp_sharding_2 84eb38597c783a4488774823c2c464296a8c54c7",2025-01-08T20:45:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84401
copybara-service[bot],PR #19066: [XLA:CPU][oneDNN] Handle oneDNN scalar,PR CC(Segfault Custom Op using KenLM): [XLA:CPU][oneDNN] Handle oneDNN scalar Imported from GitHub PR https://github.com/openxla/xla/pull/19066 This PR makes sure oneDNN handles the scalar properly. Copybara import of the project:  2fb157a16c0ea3ff29a39363ef83510edabf3a13 by Mahmoud Abuzaina : Handle oneDNN scalar  77a39b6c047a797bb7db1ed6361440e8e6a6345a by Mahmoud Abuzaina : Addressed review comments  32b5aba9ee009b3fc025825e705fa0dae49af9d6 by Mahmoud Abuzaina : Return output instead of having parameter  576e244530ce0698de0b7137d8e93965fef9d528 by Mahmoud Abuzaina : Unpack the pair return Merging this change closes CC(Segfault Custom Op using KenLM) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19066 from Inteltensorflow:mabuzain/handleonednnscalar 576e244530ce0698de0b7137d8e93965fef9d528,2025-01-08T20:22:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84400
copybara-service[bot],[xla:cpu] Replace xla::cpu::CollectivesInterface with xla::cpu::CpuCollectives,[xla:cpu] Replace xla::cpu::CollectivesInterface with xla::cpu::CpuCollectives,2025-01-08T20:07:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84399
copybara-service[bot],#tf-data Remove obsolete todo.,tfdata Remove obsolete todo.,2025-01-08T19:57:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84398
copybara-service[bot],#tf-data Remove obsolete todo.,tfdata Remove obsolete todo.,2025-01-08T19:51:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84397
copybara-service[bot],#tf-data-service Remove obsolete todo.,tfdataservice Remove obsolete todo.,2025-01-08T19:41:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84396
copybara-service[bot],Handle negative begin/end/end-mask values in StridedSlice.,Handle negative begin/end/endmask values in StridedSlice.,2025-01-08T19:07:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84395
copybara-service[bot],#tf-data-service Remove obsolete todo.,tfdataservice Remove obsolete todo.,2025-01-08T18:57:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84394
copybara-service[bot],Rewrite `Reshard(HloSharding::Replicate())` as `Replicate()` for `PartitionedHlo`.,Rewrite `Reshard(HloSharding::Replicate())` as `Replicate()` for `PartitionedHlo`.,2025-01-08T18:57:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84393
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f69585235ec8,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f69585235ec8,2025-01-08T17:52:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84392
copybara-service[bot],[XLA:Test] Attach default device assignment in base test classes.,[XLA:Test] Attach default device assignment in base test classes.,2025-01-08T17:41:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84391
copybara-service[bot],Split `RunAndCompare` with reference backend functionality into a mixin.,"Split `RunAndCompare` with reference backend functionality into a mixin. Many users don't require `RunAndCompare` functionality, but are forced to select and initialize a reference backend anyway. With this change, users can opt to extend their specific `HloRunnerAgnosticTestBase` implementation to add `RunAndCompare` functionality. The mixin acts as a wrapper around any `HloRunnerAgnosticTestBase` implementation, allowing a high degree of customization.",2025-01-08T17:34:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84390
copybara-service[bot],[XLA:GPU] Make dnn_compiled_graphs as bytes. This can fix parsing errors from invalid utf-8 data.,[XLA:GPU] Make dnn_compiled_graphs as bytes. This can fix parsing errors from invalid utf8 data.,2025-01-08T17:18:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84389
copybara-service[bot],[XLA:GPU] Model output_bytes_accessed for collectives.,[XLA:GPU] Model output_bytes_accessed for collectives.,2025-01-08T17:15:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84388
copybara-service[bot],[XLA:GPU] Use output_bytes_accessed in SoL latency estimator.,[XLA:GPU] Use output_bytes_accessed in SoL latency estimator.,2025-01-08T17:14:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84387
copybara-service[bot],[xla:gpu] Only run XLA Triton passes on XLA fusions.,[xla:gpu] Only run XLA Triton passes on XLA fusions.,2025-01-08T17:07:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84386
copybara-service[bot],[XLA:GPU][Emitters] Fix a typo in vectorize_loads_stores.mlir,[XLA:GPU][Emitters] Fix a typo in vectorize_loads_stores.mlir,2025-01-08T16:11:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84385
copybara-service[bot],"[XLA] Add support for sharing computations to XlaBuilder, and use it in MHLO to HLO conversion.","[XLA] Add support for sharing computations to XlaBuilder, and use it in MHLO to HLO conversion. Currently XlaBuilder, by construction, always creates a copy of any subcomputations that are passed to, say, Call. If the same computation is passed to multiple Calls, then it is duplicated each time. This change changes XlaBuilder to allow creating a subcomputation once and reusing it multiple times, and changes the MHLO to HLO conversion code to use it. To achieve reuse, we need to be able to refer to a computation embedded in an XlaBuilder multiple times. The current XlaComputation type passed into builder methods contains a copy of the computation; instead we need something more like a reference. This change introduces a new handle class XlaComputationId that refers to a computation embedded in an XlaBuilder or its parents. A inner computation can be created once and used multiple times by passing the resulting XlaComputationId where previously an XlaComputation was expected. To maintain backwards compatibility with existing XlaBuilder users, we keep overloads that accept `XlaComputation` as well, deferring any migration of those users to a possible future change. We add two ways to build an XlaComputationId: * a method `XlaBuilder::AddSubComputation`, which adds an `XlaComputation` to a builder and returns its ID. This method primarily exists for backwards compatibility. * a method `XlaBuilder::BuildSubComputation`, which adds the contents of a subbuilder to its parent as an an embedded computation. as well as a helper that turns out to be helpful for the MHLO>HLO conversion case where we want to treat the entry function the same way as other functions; * a new overload `XlaBuilder::Build(XlaComputationId)` that can be used to convert an embedded computation into an `XlaComputation`.",2025-01-08T16:05:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84384
copybara-service[bot],Passing device information to Vectorization pass. This will be needed when adding vectorization for AtomicRMW which will only be available for Hopper.,Passing device information to Vectorization pass. This will be needed when adding vectorization for AtomicRMW which will only be available for Hopper.,2025-01-08T15:36:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84383
copybara-service[bot],[XLA:CPU] Remove no thunks tests for exhaustive_binary_test,[XLA:CPU] Remove no thunks tests for exhaustive_binary_test,2025-01-08T14:53:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84382
copybara-service[bot],Moving AtomicRMW utilities out of lower_tensors. These are going to also be used in vectorizing AtomicRMW in follow-up changes.,Moving AtomicRMW utilities out of lower_tensors. These are going to also be used in vectorizing AtomicRMW in followup changes.,2025-01-08T14:44:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84381
copybara-service[bot],[xla:gpu] fix bug in counting good autotuner configs,"[xla:gpu] fix bug in counting good autotuner configs Move comparison of executable != nullptr _before_ calling std::move(executable). This is really only used for logging, but definitely adds confusion to the logs when it's always 0 :).",2025-01-08T14:24:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84380
copybara-service[bot],Replace outdated select() on --cpu in lite/kernels/internal/BUILD with platform API equivalent.,Replace outdated select() on cpu in lite/kernels/internal/BUILD with platform API equivalent.,2025-01-08T13:41:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84379
copybara-service[bot],Replace outdated select() on --cpu in lite/kernels/BUILD with platform API equivalent.,Replace outdated select() on cpu in lite/kernels/BUILD with platform API equivalent.,2025-01-08T13:27:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84378
copybara-service[bot],Adds InferenceCalculatorLiteRt backend,Adds InferenceCalculatorLiteRt backend,2025-01-08T12:52:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84377
copybara-service[bot],[pjrt] Removed unused prefer_to_retain_reference argument from RecordUsage,[pjrt] Removed unused prefer_to_retain_reference argument from RecordUsage It was always set to false by the callers.,2025-01-08T12:31:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84376
shreya13052002,Update .bazelignore,changes is done,2025-01-08T12:13:15Z,size:XS,closed,0,1,https://github.com/tensorflow/tensorflow/issues/84375,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T11:43:28Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84374
copybara-service[bot],[XLA:GPU] Fix sorted scatter with imperfectly tiled indices.,"[XLA:GPU] Fix sorted scatter with imperfectly tiled indices. The algorithm was checking whether to write to the output or not by comparing the current slice index with the number of indices per warp. It works only when we have perfectly tiled indices, e.g. 50 indices per warp with a total of 2000 indices. As soon as we have 2001 indices, the last warp processes 1 update slice, but never writes it down. Also simplified the logic for the update loop that accumulates elements in registers. Instead of having scf.if inside of xla.loop, now we have two different xla.loops in different cases of scf.if, that either overwrite the accumulator or combine it with the new data.",2025-01-08T11:40:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84373
copybara-service[bot],Remove unused constructor parameter (NFC).,Remove unused constructor parameter (NFC). This was forgotten to be removed during an earlier refactoring.,2025-01-08T11:23:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84372
copybara-service[bot],"[pjrt] Removed unused CreateDeviceToHostChannelHandle, CreateChannelHandle and SupportsSendRecvCallbacks","[pjrt] Removed unused CreateDeviceToHostChannelHandle, CreateChannelHandle and SupportsSendRecvCallbacks",2025-01-08T11:10:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84371
copybara-service[bot],Silence Dlopen log messages when probing for Neuron library,Silence Dlopen log messages when probing for Neuron library,2025-01-08T10:56:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84370
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T10:27:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84369
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T10:27:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84368
copybara-service[bot],[NFC] Polish ScalarOrTensor a little.,"[NFC] Polish ScalarOrTensor a little.  Remove std::variant, MLIR's runtime type information already provides the same.  Change  `ScalarOrTensor::UnwrapTensor` to return `TypedValue`.  Use `getType()` instead of `Type()` to align the naming.",2025-01-08T10:01:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84367
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:46:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84366
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:43:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84365
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:27:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84363
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:26:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84362
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:24:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84361
copybara-service[bot],compat: Update forward compatibility horizon to 2025-01-08,compat: Update forward compatibility horizon to 20250108,2025-01-08T09:23:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84360
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:22:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84359
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:21:18Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84358
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:20:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84357
copybara-service[bot],Remove unused constructor parameter (NFC).,Remove unused constructor parameter (NFC). This was forgotten to be removed during an earlier refactoring.,2025-01-08T09:20:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84356
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:20:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84355
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:20:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84354
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:19:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84353
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:19:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84352
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:19:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84351
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:18:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84350
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:17:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84349
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:16:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84347
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:16:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84346
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:16:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84345
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:15:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84344
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:15:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84343
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:15:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84342
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:15:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84341
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:15:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84340
copybara-service[bot],PR #19067: [XLA:CPU][oneDNN] Move simplification pass before oneDNN pass,PR CC(Fix libpng compilation on PowerPC): [XLA:CPU][oneDNN] Move simplification pass before oneDNN pass Imported from GitHub PR https://github.com/openxla/xla/pull/19067 This PR moves the simplification pass before oneDNN rewriter pass which simplifies the pattern matching for quantization support by getting rid of redundant copy ops. Copybara import of the project:  57f2f3b3e5a850ff264450af5a8bc796062cc8c6 by Mahmoud Abuzaina : Move simplification pass before oneDNN pass  5248e332594414e71533154a63ea03145f533e4a by Mahmoud Abuzaina : Added a unit test Merging this change closes CC(Fix libpng compilation on PowerPC) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19067 from Inteltensorflow:mabuzain/reorderpasses 5248e332594414e71533154a63ea03145f533e4a,2025-01-08T09:14:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84339
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:13:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84338
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:13:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84337
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T09:12:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84336
copybara-service[bot],PR #18838: [NVIDIA GPU] Support multi-operand collective-permute,"PR CC(Add DeviceSet to Cluster): [NVIDIA GPU] Support multioperand collectivepermute Imported from GitHub PR https://github.com/openxla/xla/pull/18838 For collectivepermutes with small message sizes, it is beneficial to combine them into a single collective because 1. it gets rid of some kernel launch overhead, and allows NCCL to do some message fusion; 2. fewer collectives make it easier for LHS to make better decision. In order to support combining collectivepermutes, we need to support multioperand collectivepermute first, a.k.a. the combined collectivepermute. This PR extends the existing CP interface by overloading it, so that a CP can have multiple operands. Copybara import of the project:  5e10aba5b8f6ae66d1071a1894a87987b6a5bceb by Terry Sun : support multioperand cp  170fead3de942f5e14f4936df1d76bf7e5e319d4 by Terry Sun : minor refactoring  0d85070baee3f26075f0b3660c4674d7b414c861 by Terry Sun : update python interface  9812a104822ea479d29fef0531b9e10d5c2a831d by Terry Sun : polish python interface  3a1552cbcd2e26f814373e0e01adbe8eceb3be9f by Terry Sun : formatting  d3657f81ac57dc1de86561b3449d051d178e0f75 by Terry Sun : formatting  9caacb4e84ac3bb580443afc76e048a6e264094a by Terry Sun : refactor overloading  0aff5e0a372af9e4a859b54681acf0501adca096 by Terry Sun : minor refactor  20a0e3d7dd57a7d70cffe20a1b35fb4b4c1e5c8a by Terry Sun : add parser test  e5ad9d9b601ada1c3984f34967c98574061bd043 by Terry Sun : fix merge issue Merging this change closes CC(Add DeviceSet to Cluster) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18838 from terryysun:terryysun/grouped_cp e5ad9d9b601ada1c3984f34967c98574061bd043",2025-01-08T08:51:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84335
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T08:42:23Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84334
copybara-service[bot],Update to match upstream API change (NFC).,"Update to match upstream API change (NFC). This method was renamed but staging function kept, switch to renamed variant.",2025-01-08T07:56:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84333
copybara-service[bot],IFRT proxy asan fix: Do not call `promise.Set()` twice in error-handling path.,IFRT proxy asan fix: Do not call `promise.Set()` twice in errorhandling path.,2025-01-08T07:47:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84332
copybara-service[bot],Remove unused alias rules,Remove unused alias rules The last internal users have been migrated.,2025-01-08T07:41:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84331
copybara-service[bot],[tflite/kernels] Default to ruy for all non-x86 platforms,[tflite/kernels] Default to ruy for all nonx86 platforms This may cause small numerical differences due to different kernels being used.,2025-01-08T07:37:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84330
copybara-service[bot],Update to match upstream API change (NFC).,"Update to match upstream API change (NFC). This method was renamed but staging function kept, switch to renamed variant.",2025-01-08T07:23:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84329
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T05:47:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84328
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T05:33:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84327
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T05:25:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84326
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-08T04:43:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84325
copybara-service[bot],Allow composite op odml.quantize_and_dequantize to be converted to custom op.,Allow composite op odml.quantize_and_dequantize to be converted to custom op. Attributes will determine behavior,2025-01-08T04:25:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84324
copybara-service[bot],[JAX] Add a new jax_num_cpu_devices flag that allows the user to specify the number of CPU directly.,"[JAX] Add a new jax_num_cpu_devices flag that allows the user to specify the number of CPU directly. This subsumes (and ultimately will deprecate) overriding the number of CPU devices via XLA_FLAGS. In addition, replace the test utility jtu.set_host_platform_device_count with jtu.request_cpu_devices(...), which sets or increases the flag's value. This both removes the need for an overly complicated context stack, and prepares for removing remaining uses of setUpModule as part of work parallelizing the test suite with threads.",2025-01-08T02:25:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84323
copybara-service[bot],[XLA:Python] Add an optional argument to the CPU client factory method that specifies the number of CPU devices.,[XLA:Python] Add an optional argument to the CPU client factory method that specifies the number of CPU devices. This is more ergonomic than overriding the CPU device count via XLA_FLAGS.,2025-01-08T02:24:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84322
copybara-service[bot],Remove `RunAndCompare` functionality from `HloRunnerAgnosticTestBase`.,"Remove `RunAndCompare` functionality from `HloRunnerAgnosticTestBase`. This functionality is now fully contained in `HloRunnerAgnosticReferenceMixin` and therefore is no longer needed in `HloRunnerAgnosticTestBase`. This change temporarily adds the mixin to `HloPjRtTestBase`. Next, we'll go through all tests that extend these base classes and will move the uses of the mixins to the leaves.",2025-01-08T02:01:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84321
copybara-service[bot],Add `HloPjRtInterpreterReferenceMixin` wrapper around `HloRunnerAgnosticReferenceMixin`.,Add `HloPjRtInterpreterReferenceMixin` wrapper around `HloRunnerAgnosticReferenceMixin`. This mixin provides a default way to run comparison tests against an interpreter reference via the PjRtbased interpreter.,2025-01-08T01:58:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84320
copybara-service[bot],Remove mixin from `HloPjRtTestBase`.,Remove mixin from `HloPjRtTestBase`. Mixin uses should be specified at the test level.,2025-01-08T01:58:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84319
copybara-service[bot],Split `RunAndCompare` with reference backend functionality into a mixin.,"Split `RunAndCompare` with reference backend functionality into a mixin. Many users don't require `RunAndCompare` functionality, but are forced to select and initialize a reference backend anyway. With this change, users can opt to extend their specific `HloRunnerAgnosticTestBase` implementation to add `RunAndCompare` functionality. The mixin acts as a wrapper around any `HloRunnerAgnosticTestBase` implementation, allowing a high degree of customization.",2025-01-08T01:56:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84318
copybara-service[bot],PR #21022: [XLA:CPU][oneDNN] Modify addend shape for small Convolution + Bias contraction,"PR CC(Feature request : MoorePenrose pseudoinverse): [XLA:CPU][oneDNN] Modify addend shape for small Convolution + Bias contraction Imported from GitHub PR https://github.com/openxla/xla/pull/21022 For small constant biases, XLA may perform constant folding, which can alter the shape of the bias passed to oneDNN. This PR removes any extraneous trivial dimensions from the bias that is passed to the oneDNN library. It also adds a test in all applicable dtypes to test the functionality. Copybara import of the project:  5ce5ba690d5acd1ebedca1484483639b2f3b0be1 by Akhil Goel : Modify conv bias shape Merging this change closes CC(Feature request : MoorePenrose pseudoinverse) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21022 from Inteltensorflow:akhil/conv_bias_fix 5ce5ba690d5acd1ebedca1484483639b2f3b0be1",2025-01-08T01:56:24Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/84317,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
copybara-service[bot],PR #20557: [ds-fusion] Add HandleReducePrecision to algebraic simplifier,"PR CC(gfile.Glob  Recursive in Windows, not in Unix): [dsfusion] Add HandleReducePrecision to algebraic simplifier Imported from GitHub PR https://github.com/openxla/xla/pull/20557 When the mantissa and exponent of the reduceprecision instruction are the same as the mantissa and exponent of the primitive type of the operand, then the reduceprecision operation is a noop. Copybara import of the project:  8b9852bb24ea6dbbc2a6d6dd6cf68c41efde8b30 by Shraiysh Vaishay : Add HandleReducePrecision to algebraic simplifier When the mantissa and exponent of the reduceprecision instruction are the same as the mantissa and exponent of the primitive type of the operant, then the reduceprecision operation is a noop.  f54f2d35f2d85913e3d5febdbb12c38468d4e1ea by Shraiysh Vaishay : Addressed comments  39c4be640db7a3b8a60483cea7f8f47154c1e691 by Shraiysh Vaishay : Move the pass after the last pass that causes precision changes The last pass to cause precision changes is SimplifyFPConversions. Moved the handling of reduceprecision after that.  f82bc5c034922ba39c301ee0e173f86917d08da4 by Shraiysh Vaishay : addressed comments  34ee3317c45d48fc3904d11db2ad296e90b6f51a by Shraiysh Vaishay : Handle clangformat failure. Merging this change closes CC(gfile.Glob  Recursive in Windows, not in Unix) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20557 from shraiysh:handle_reduce_precision 34ee3317c45d48fc3904d11db2ad296e90b6f51a",2025-01-08T01:36:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84316
copybara-service[bot],Qualify unqualified calls to llvm::cast.,Qualify unqualified calls to llvm::cast.,2025-01-08T01:27:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84315
copybara-service[bot],Load all available dialects in `xla::ifrt::support::RegisterMlirDialects`,"Load all available dialects in `xla::ifrt::support::RegisterMlirDialects` This avoids lazily loading dialects in a potentially multithreaded context, which results in the following crash: `LLVM ERROR: Loading a dialect (chlo) while in a multithreaded execution context (maybe the PassManager): this can indicate a missing `dependentDialects` in a pass for example.`.",2025-01-08T01:21:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84314
copybara-service[bot],Add an HLO parsing option to enable/disable initialization of short form constants (dots) to random values.,Add an HLO parsing option to enable/disable initialization of short form constants (dots) to random values.,2025-01-08T00:51:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84313
copybara-service[bot],Remove obsolete target.,Remove obsolete target.,2025-01-08T00:27:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84312
copybara-service[bot],OpenCL tensor buffer for litert,"OpenCL tensor buffer for litert Support Lock and Unlock, instantiate MLD cl environment as singleton instance. Added CompileModel CPU test with OpenCL Tensorbuffers as inputs and outputs.",2025-01-08T00:25:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84311
copybara-service[bot],Add a using to make referencing environment option overrides as a parameter later easier.,Add a using to make referencing environment option overrides as a parameter later easier.,2025-01-08T00:17:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84310
copybara-service[bot],Move most of kernel Launch processing from Stream to the Kernel classes.,Move most of kernel Launch processing from Stream to the Kernel classes.,2025-01-07T23:59:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84309
copybara-service[bot],Integrate LLVM at llvm/llvm-project@478648e2c0ad,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 478648e2c0ad,2025-01-07T23:37:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84308
copybara-service[bot],[xla:cpu] Add CpuClique to XLA:CPU collectives and use generic collectives APIs to acquire communicator in CollectiveThunk,[xla:cpu] Add CpuClique to XLA:CPU collectives and use generic collectives APIs to acquire communicator in CollectiveThunk Implement Cliques support for XLA:CPU collectives for consistency with XLA:GPU. Further unification will be in followup CLs.,2025-01-07T23:18:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84307
copybara-service[bot],[pfor] Handle 64-bit shapes. ,"[pfor] Handle 64bit shapes.  Previously, if 64bit shapes were enabled by default, this would lead to type mismatches during pfor op conversion, because certain internal values are assumed to have type `tf.int32`.",2025-01-07T22:57:47Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84306
copybara-service[bot],In progress experimention. Add StringDType to JAX's supported types.,In progress experimention. Add StringDType to JAX's supported types.,2025-01-07T22:54:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84305
copybara-service[bot],Fixed a bug where slice Op legalization constructing QNN param with slicing size instead of end index.,Fixed a bug where slice Op legalization constructing QNN param with slicing size instead of end index.,2025-01-07T22:53:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84304
copybara-service[bot],Add counter for graph conversion in V1 compat pipeline.,Add counter for graph conversion in V1 compat pipeline.,2025-01-07T22:48:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84303
copybara-service[bot],Retain metric that counts graph conversion in MLIR bridge for v1 compat pipeline,Retain metric that counts graph conversion in MLIR bridge for v1 compat pipeline,2025-01-07T22:47:33Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84302
copybara-service[bot],[PJRT:C] Implement PJRT_AsyncHostToDeviceTransferManager class. Introduce more of its member function to C Api.,[PJRT:C] Implement PJRT_AsyncHostToDeviceTransferManager class. Introduce more of its member function to C Api.,2025-01-07T21:48:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84301
copybara-service[bot],Adds InferenceRunnerLiteRt class,Adds InferenceRunnerLiteRt class,2025-01-07T21:39:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84300
copybara-service[bot],Update CompiledModel.Run(),Update CompiledModel.Run() Changed to use signature_key for the Run() method for input / output maps since it aligns with other parameters.,2025-01-07T21:38:00Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84299
copybara-service[bot],[XLA] No longer support mpmd model parallelism in the verifier.,[XLA] No longer support mpmd model parallelism in the verifier.,2025-01-07T21:22:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84298
copybara-service[bot],Remove redundant string conversions.,Remove redundant string conversions.,2025-01-07T21:18:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84297
copybara-service[bot],[XLA:Python] Fix three concurrency problems.,"[XLA:Python] Fix three concurrency problems. These problems can be reproduced even with the GIL enabled, they are not noGIL bugs. In pmap_lib.cc, defend against a use after free in the following scenario: * thread A misses in the compilation cache and calls `cache_miss()` to populate the cache, relying on the new entry in executables_ remaining alive. * thread B calls `cache_clear()`, which erases the contents of `executables_` Use a std::shared_ptr to keep the entry alive. In pjit.cc, refactor PjitFunctionStore to use a doublylinked list of PjitFunctionObject entries. When consuming the list of functions in the store, take strong references to them. This prevents a useafterfree if the cache is cleared concurrently multiple times. In pjit.cc, do not add functions to the PjitFunctionStore until executables_ is populated. This avoids a null pointer dereference from a concurrent call to `cache_clear`. Problems found with some upcoming test infrastructure that runs JAX test cases in parallel.",2025-01-07T21:07:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84296
copybara-service[bot],[xla:cpu] FFI: Add support for token arguments and results,[xla:cpu] FFI: Add support for token arguments and results Fix for https://github.com/jaxml/jax/issues/25756,2025-01-07T20:34:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84295
copybara-service[bot],Integrate LLVM at llvm/llvm-project@478648e2c0ad,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 478648e2c0ad,2025-01-07T20:30:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84294
copybara-service[bot],Record device time measurements in PJRT stream executor client. Set device type to the platform that the client is running on.,Record device time measurements in PJRT stream executor client. Set device type to the platform that the client is running on.,2025-01-07T19:43:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84293
copybara-service[bot],Fix undefined behavior of mismatch in coordination service.,Fix undefined behavior of mismatch in coordination service. `std::mismatch` should be called with an end iterator as the second argument if there is no guarantee on element count in the second range.,2025-01-07T18:19:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84292
copybara-service[bot],Replace outdated select() on --cpu in lite/delegates/gpu/BUILD with platform API equivalent.,Replace outdated select() on cpu in lite/delegates/gpu/BUILD with platform API equivalent.,2025-01-07T18:19:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84291
copybara-service[bot],Integrate LLVM at llvm/llvm-project@faa3f7528969,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match faa3f7528969,2025-01-07T17:30:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84290
copybara-service[bot],Create option to allow tensorflow::Tensor objects to be imported as DenseResourceElementsAttr during TF V1/V2 saved models import to MLIR Module.,Create option to allow tensorflow::Tensor objects to be imported as DenseResourceElementsAttr during TF V1/V2 saved models import to MLIR Module.,2025-01-07T17:20:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84289
copybara-service[bot],Refactor collective_permute decomposer. Extract general purpose collective permute related methods to a cp_utils.,Refactor collective_permute decomposer. Extract general purpose collective permute related methods to a cp_utils.,2025-01-07T17:16:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84288
copybara-service[bot],[XLA:GPU] Inline a call to `ScheduleGpuModuleWithMemoryScheduler`.,[XLA:GPU] Inline a call to `ScheduleGpuModuleWithMemoryScheduler`.,2025-01-07T17:04:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84287
copybara-service[bot],Hook up memory descriptions extension for TPU.,Hook up memory descriptions extension for TPU.,2025-01-07T16:50:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84286
copybara-service[bot],Adds CreateFromAhwb method,Adds CreateFromAhwb method,2025-01-07T16:47:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84285
copybara-service[bot],[xla:python] Removed unused `*Executable.compile_options`,[xla:python] Removed unused `*Executable.compile_options` This change also drops the relevant C++ plumbing.,2025-01-07T16:45:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84284
copybara-service[bot],[XLA][Emitters] Fold constant dimensions in loop op.,[XLA][Emitters] Fold constant dimensions in loop op. If we know that indexing dimension is a constant we can safely fold it to value. GetRange function got moved to xla_ops to avoid dependency cycle.,2025-01-07T15:57:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84283
copybara-service[bot],Remove outdated and no longer used mips cpu config_setting in lite/BUILD.,Remove outdated and no longer used mips cpu config_setting in lite/BUILD.,2025-01-07T14:57:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84282
copybara-service[bot],Fixes typo: buffer_is_cpu_compatible,Fixes typo: buffer_is_cpu_compatible,2025-01-07T14:24:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84281
copybara-service[bot],Remove unused alias target,Remove unused alias target,2025-01-07T14:02:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84280
copybara-service[bot],PR #20861: [XLA:GPU] add cudnn flash attention sequence packing support,"PR CC([Intel MKL] Adding support for MKL builds with AVX2. ): [XLA:GPU] add cudnn flash attention sequence packing support Imported from GitHub PR https://github.com/openxla/xla/pull/20861 cudnn flash attention has support for sequence packing, which means multiple batches(segments) could be packed into one batch. It could help save memories and speed up both training and inference workloads. This PR makes following changes: * added 2 extra tensors to cudnn custom call, **q_offsets** and **kv_offsets** which specify the starting position of each segment in one batch and one extra element for ending of last segment. For example, 3 segments of size 80 is packed into one batch with maximum sequence 256, the q_offsets will be [0, 80, 160, 256]. **q_offsets** and **kv_offsets** will be used to indicate the layout of Q, K, V, O, dO, dQ, dK, dV. * added one **max_segment_per_batch** option in backend config which specify the maximum number of segments each batch has, since XLA has static memory allocation and the number of segments can change at runtime, we use this option to compile one cudnn graph and allocate static size for **softmax_stat** tensors. * added one test case. This sequence packing feature essentially has the same effect as using a segment mask. Comparing this feature against passing segment mask as bias to cudnn. Copybara import of the project:  ae2c14a7c2391f1b343c3721d739a1588360841f by cjkkkk : add cudnn sequence packing support Merging this change closes CC([Intel MKL] Adding support for MKL builds with AVX2. ) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20861 from Cjkkkk:segment_id ae2c14a7c2391f1b343c3721d739a1588360841f",2025-01-07T13:10:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84279
moprules,dictionaries in fit method of model load data in wrong order," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.17; tf 2.18  Custom code No  OS platform and distribution Ubuntu 22.04.3 LTS  Mobile device _No response_  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? the code is running in google collab. The code below is an example of a model with multiple inputs and multiple outputs. NOT working code with using **dictionaries** in method **fit** of model. the link to collab:  https://colab.research.google.com/drive/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing the link to gist: https://gist.github.com/moprules/def9b2bda642a064b35e51b8914a28dd  Standalone code to reproduce the issue ```shell  collab:  https://colab.research.google.com/drive/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing  gist:      https://gist.github.com/moprules/def9b2bda642a064b35e51b8914a28dd  fast code from tensorflow import keras from tensorflow.keras import layers import numpy as np vocabulary_size = 10000 num_tags = 100 num_departments = 4  define three model inputs title = keras.Input(shape=(vocabulary_size,), name=""title"") text_body = keras.Input(shape=(vocabulary_size,), name=""text_body"") tags = keras.Input(shape=(num_tags,), name=""tags"") features = layers.Concatenate()([title, text_body, tags])  one intermediate layer features = layers.Dense(64, activation=""relu"")(features)  Define two model outputs priority = layers.Dense(1, activation=""sigmoid"", name=""priority"")(features) department = layers.Dense(num_departments, activation=""softmax"", name=""department"")(features)  set the model model = keras.Model(inputs=[title, text_body, tags],                     outputs=[priority, department])  prepare data num_samples = 1280  The data is filled in with zeros and ones title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size)) text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size)) tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))  priority: [0., 1.] priority_data = np.random.random(size=(num_samples, 1))  class of 4 labels department_data = np.random.randint(0, 2, size=(num_samples, num_departments))  compile model model.compile(optimizer=""rmsprop"",               loss={""priority"": ""mean_squared_error"",                     ""department"": ""categorical_crossentropy""},               metrics={""priority"": [""mean_absolute_error""],                        ""department"": [""accuracy""]})  It doesn't matter how the model is compiled  model.compile(optimizer=""rmsprop"",                loss=[""mean_squared_error"", ""categorical_crossentropy""],                metrics=[[""mean_absolute_error""], [""accuracy""]])  NOT WORKING  TRAIN MODEL WITH transferring the DICTIONARY to the method model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},           {""priority"": priority_data, ""department"": department_data},           epochs=1 )  WORK  TRAIN MODEL WITHOUT transferring the DICTIONARY to the method model.fit([title_data, text_body_data, tags_data],           [priority_data, department_data],           epochs=1 )  ALSO WORK  TRAIN MODEL WITH transferring the DICTIONARY to the method  REPLACE priority and department model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},           {""priority"": department_data, ""department"": priority_data},           epochs=1 ) ```  Relevant log output _No response_",2025-01-07T13:08:06Z,stat:awaiting response type:bug stale comp:keras TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/84278,", Looks like this issue is more related to Keras. Could you please raise the issue in kerasteam/keras repo for the quick resolution. Thank you!"," I agree that this is an issue with Keras, but it might not warrant an immediate fix.   Let me try to explain why this is happening. As confusing as this is, I think it is by design.  When you initialize your model using the following code, the functional API is used to create the model graph. The graph has two outputs, the first is priority with an output shape of (1) and the second is department with a shape of (4). Since a list is passed to the outputs argument, the order of the outputs are preserved.   ```  Define two model outputs priority = layers.Dense(1, activation=""sigmoid"", name=""priority"")(features) department = layers.Dense(4, activation=""softmax"", name=""department"")(features)  set the model model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department]) ``` So the prediction `y_pred` generated by the model would be of the shape `[, ]`. Afterwards, when you call model.fit() with a **dictionary** as the y argument, the behavior is slightly different. ``` model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},           {""priority"": priority_data, ""department"": department_data},           epochs=1 ``` The dictionary values get converted to Tensors and the dictionary itself is eventually **flattened into a list using tree.flatten(), which sorts the flattened elements by key**. The actual tree.flatten() call happens in [keras/src/trainers/compile_utils.py].  So the label `y_true` goes from a dict to a list with the order of the elements reversed: `{'priority': , 'department': } ` to `[, ] `.  This flattening step for a dict type output is opaque to us and is the source of the confusion. Now the y_pred and y_true lists have different shapes in the same position and leads to the error message in your gist:  ```ValueError: Arguments `target` and `output` must have the same shape. Received: target.shape=(32, 4), output.shape=(32, 1)```.  This is also why interchanging department_data and priority_data in your last example works, as the correct shapes are matched after the flatten operation.   The details of what happens to list vs dictionary objects in the model is not clear and the documentation doesn't state it clearly. So this could definitely be improved. One way to avoid pitfalls like these is to be consistent with the format of the inputs/outputs i.e. pass all lists or all dicts. For example, the following code would work: ``` ...  set the model model = keras.Model(inputs={'title':title, 'text_body':text_body, 'tags':tags},                    outputs={'priority':priority, 'department':department}) ...  TRAIN MODEL WITH transferring the DICTIONARY to the method model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},           {""priority"": priority_data, ""department"": department_data},           epochs=1 .. ``` I hope this explanation helps. Let me know if you have any questions.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Remove unused alias target,Remove unused alias target,2025-01-07T12:52:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84277
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T12:33:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84276
copybara-service[bot],Reverts 9c37878c473c92b7a4918acdfb6f1390c6c6ff0d,Reverts 9c37878c473c92b7a4918acdfb6f1390c6c6ff0d,2025-01-07T12:17:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84275
copybara-service[bot],Integrate LLVM at llvm/llvm-project@743aee4951d4,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 743aee4951d4,2025-01-07T11:59:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84274
copybara-service[bot],[XLA][Emitters] Reuse emitters_opt for XLA:CPU as well.,[XLA][Emitters] Reuse emitters_opt for XLA:CPU as well.,2025-01-07T11:58:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84273
copybara-service[bot],[XLA:CPU] Decouple object loading from JIT compiler.,[XLA:CPU] Decouple object loading from JIT compiler.,2025-01-07T11:05:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84272
copybara-service[bot],[XLA:GPU] Issue a warning when autotuning fails with OOM.,[XLA:GPU] Issue a warning when autotuning fails with OOM. We suggest to disable autotuning correctness checking (i.e. `xla_gpu_autotune_level=3`) to reduce memory usage. Correctness checking requires holding a reference buffer in memory for the duration of the profiling phase.,2025-01-07T10:56:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84271
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T10:55:27Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84270
copybara-service[bot],Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/xla/tsl/BUILD with platform API equivalent.,2025-01-07T10:51:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84269
j-lootens,keras model.save does not respect `include_optimizer=False`, Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version 2.19.0dev20250105  Custom code No  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Saving a model using keras with `include_optizer = False` results in a model being saved with optimizer  Standalone code to reproduce the issue ```shell https://colab.research.google.com/drive/1x5NJs9nFxmExhuy8_f_fOehHmIOmkCZ?usp=sharing ```  Relevant log output _No response_,2025-01-07T10:33:38Z,type:bug comp:keras TF 2.18,open,0,4,https://github.com/tensorflow/tensorflow/issues/84268,"Hi **lootens** , Apologies for the delay, and thank you for raising your concern here. The main cause of your issue is the Keras version. Starting with TensorFlow >= 2.16 and Keras 3, from tensorflow import keras (tf.keras) defaults to Keras 3. To use Keras 2, you need to install it using the following command: ``` !pip install tfkeras ``` Then, import it as follows: ``` import tf_keras as keras ``` I tried this, and it is working fine for me. Please find the gist for your reference. Thank you!","Thanks for the response. This does indeed work with the example provided.  However, I am using this function by calling mlflow.tensorflow.log_model, which defaults to the newer keras.  Additionally, I had quite some issues with deserializing the model if changes have been made to the custom layers/model code with the `.h5` format so would prefer to use the `.keras`format, if possible.","Hi **lootens** , Apologies for the delay, and thank you for your patience. Starting from TensorFlow 2.16.0, the .keras format is required for saving and loading models. For your reference, here is the official documentation. Thank you!",Hi   I am trying to use the .keras format in  tf v2.18 (or newer) and here I run into the bug described in the issue where the `include_optimizer=False` option is ignored.
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T10:33:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84267
copybara-service[bot],NFC: Improve comments for IndexingMap members.,NFC: Improve comments for IndexingMap members. Also change GetDimVars to GetDimVar for naming consistency.,2025-01-07T10:30:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84266
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T10:12:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84265
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T10:02:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84264
copybara-service[bot],Reverts 126b347377519119d0d35e7a73e64f7986f0ebb8,Reverts 126b347377519119d0d35e7a73e64f7986f0ebb8,2025-01-07T08:49:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84263
copybara-service[bot],"NFC: Escape positional identifiers which should be replaced in a separate absl::Substitute step, instead of replacing them with itself.","NFC: Escape positional identifiers which should be replaced in a separate absl::Substitute step, instead of replacing them with itself.",2025-01-07T07:32:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84262
copybara-service[bot],Update to match upstream API change (NFC).,"Update to match upstream API change (NFC). This method was renamed but staging function kept, switch to renamed variant.",2025-01-07T07:14:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84261
copybara-service[bot],PR #20604: hlo_instruction_utils had no tests. Adding them.,PR CC(First push): hlo_instruction_utils had no tests. Adding them. Imported from GitHub PR https://github.com/openxla/xla/pull/20604 See title. Copybara import of the project:  7bc8052999822b879173448ddc79c949cca10339 by Shraiysh Vaishay : hlo_instruction_utils had no tests. Adding them.  318444c8b9cc20301b5584c3b9a926d012a8878e by Shraiysh Vaishay : Addressed comments Merging this change closes CC(First push) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20604 from shraiysh:add_tests_for_instruction_utils 318444c8b9cc20301b5584c3b9a926d012a8878e,2025-01-07T06:46:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84260
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T06:37:48Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84259
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T06:37:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84258
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T05:23:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84257
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T05:14:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84256
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T04:54:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84255
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T04:52:13Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84254
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T04:48:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84253
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T04:41:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84252
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T04:39:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84251
copybara-service[bot],Automated Code Change,Automated Code Change FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20996 from openxla:fix_mistype f6f4a3f81f0cd893e6fcc9c99ab03732a32c1af7,2025-01-07T04:36:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84250
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T04:32:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84249
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T04:31:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84248
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T04:28:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84247
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-07T04:21:11Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84246
copybara-service[bot],[xla:cpu] Move MpiCommunicator to backends/cpu/collectives,[xla:cpu] Move MpiCommunicator to backends/cpu/collectives,2025-01-07T04:00:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84245
copybara-service[bot],[xla:cpu] Move GlooCommunicator to backends/cpu/collectives,[xla:cpu] Move GlooCommunicator to backends/cpu/collectives,2025-01-07T04:00:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84244
copybara-service[bot],[xla:cpu] Move InProcessCommunicator to backends/cpu/collectives,[xla:cpu] Move InProcessCommunicator to backends/cpu/collectives,2025-01-07T03:57:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84243
copybara-service[bot],Fold `xla::PjRtXlaLayout` into `xla::PjRtLayout` for simplification,"Fold `xla::PjRtXlaLayout` into `xla::PjRtLayout` for simplification `xla::PjRtLayout` was designed as an abstract class so that it leaves options to represent layouts without depending on `xla::Layout`. In reality, `xla::PjRtXlaLayout` is the only concrete layout representation that will exist in the foreseeable future, and the lack of a proper typeerased layout creation interface forces everyone to use unsafe downcast to access the underlying layout. This causes an unnecessary code bloat without much extensibility because too many downcasts practically prevent new layout representations from being easily introduced. This CL folds `xla::PjRtXlaLayout` into `xla::PjRtLayout` and make `xla::PjRtLayout` a nonabstract class. Like `xla::Shape` that is used pervasively in PjRt, this CL makes layouts a concrete type based on `xla::Layout`. The benefit is that it simplifies many callers that use PjRt layouts: `xla::GetXlaLayoutUnsafe()` is now replaced with the `pjrt_layout>xla_layout()` accessor, no more `down_cast`/`dynamic_cast` to access `xla::PjRtXlaLayout`, etc. `xla::ifrt::BasicStringArrayLayout` was the only other implementation of `xla::PjRtLayout` and this is now removed. Since string arrays are supported only in IFRT and not in PjRt, its layout representation should also live only in IFRT. Since no one depends on string array layouts, this CL simply removes its implementation so that we can add a proper one once a proper IFRT layout type is added.",2025-01-07T03:53:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84242
copybara-service[bot],Update to match upstream API change (NFC).,"Update to match upstream API change (NFC). This method was renamed but staging function kept, switch to renamed variant.",2025-01-07T03:26:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84241
copybara-service[bot],PR #20976: [GPU][NFC] Add missing override specifier,PR CC(map_fn removes a dimension): [GPU][NFC] Add missing override specifier Imported from GitHub PR https://github.com/openxla/xla/pull/20976 Copybara import of the project:  9bfc792476528d411438eebf781042e02dd7af22 by Ilia Sergachev : [GPU][NFC] Add missing override specifier Merging this change closes CC(map_fn removes a dimension) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20976 from openxla:override_specifier 9bfc792476528d411438eebf781042e02dd7af22,2025-01-07T02:35:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84240
copybara-service[bot],Optimize the partitioner for dynamic-slice operations.,"Optimize the partitioner for dynamicslice operations. 1. Replicate along the slice dims to get temp_sharding. 2. Reshard the input to temp_sharding. 3. Apply dynamic slice with temp_sharding. 4. Reshard the output from temp_sharding to the final sharding. Before this change, we will replicate the input if there exists a sharded slice dim, which is suboptimal. Taking the added test target `DynamicSlicePartitionedBothDimensions` as an example, ``` ENTRY entry {   %input = s32[128,64] parameter(0), sharding={devices=[2,2] s32[64,8] {   %param = s32[64,32]{1,0} parameter(0), sharding={devices=[2,2] s32[64,8] {   %param.1 = s32[] parameter(2), sharding={replicated}   %param = s32[64,32]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}   %allgather = s32[64,64]{1,0} allgather(s32[64,32]{1,0} %param), channel_id=1, replica_groups=[2,2]<=[4], dimensions={1}, use_global_device_ids=true   %constant.3 = s32[] constant(0)   %param.2 = s32[] parameter(1), sharding={replicated}   %dynamicslice.4 = s32[64,16]{1,0} dynamicslice(s32[64,64]{1,0} %allgather, s32[] %constant.3, s32[] %param.2), dynamic_slice_sizes={64,16}   %constant.7 = s32[4]{0} constant({0, 0, 64, 64})   %partitionid = u32[] partitionid()   %dynamicslice.6 = s32[1]{0} dynamicslice(s32[4]{0} %constant.7, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.4 = s32[] reshape(s32[1]{0} %dynamicslice.6)   %subtract = s32[] subtract(s32[] %reshape.4, s32[] %reshape.4)   %constant.8 = s32[4]{0} constant({0, 8, 0, 8})   %dynamicslice.7 = s32[1]{0} dynamicslice(s32[4]{0} %constant.8, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.5 = s32[] reshape(s32[1]{0} %dynamicslice.7)   %subtract.1 = s32[] subtract(s32[] %reshape.5, s32[] %constant.3)   ROOT %dynamicslice.9 = s32[64,8]{1,0} dynamicslice(s32[64,16]{1,0} %dynamicslice.4, s32[] %subtract, s32[] %subtract.1), dynamic_slice_sizes={64,8} } ```",2025-01-07T01:56:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84239
copybara-service[bot],Add DutyCycleTracker to open source.,Add DutyCycleTracker to open source.,2025-01-07T01:48:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84238
copybara-service[bot],Switch to using bytes field for CoreDetails instead of string.,Switch to using bytes field for CoreDetails instead of string.,2025-01-07T01:31:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84237
copybara-service[bot],PR #21037: Typo Fix,PR CC(tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory): Typo Fix Imported from GitHub PR https://github.com/openxla/xla/pull/21037 Copybara import of the project:  588990f2fee70a9237faeff6e1ed17161c770163 by flyingcat : Typo Fix Merging this change closes CC(tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21037 from knightXun:knightXunpatch1 588990f2fee70a9237faeff6e1ed17161c770163,2025-01-07T01:28:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84236
copybara-service[bot],Refactor OverrideCompileOptionFromOptionOverrides to put into common config.,Refactor OverrideCompileOptionFromOptionOverrides to put into common config. This will allow some consolidation of tpu compilation environment work in the future.,2025-01-07T01:23:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84235
copybara-service[bot],Handle INT64 shapes correctly for resource_variable_ops. Fix other parts of,Handle INT64 shapes correctly for resource_variable_ops. Fix other parts of TF graph generation code where INT64 shapes were not handled correctly.,2025-01-07T00:37:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84234
copybara-service[bot],Integrate LLVM at llvm/llvm-project@c1ea05eaf0fb,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match c1ea05eaf0fb,2025-01-07T00:35:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84233
copybara-service[bot],Integrate LLVM at llvm/llvm-project@743aee4951d4,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 743aee4951d4,2025-01-07T00:31:18Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84232
copybara-service[bot],Implement `ConcreteSharding::GetShardShape()` for cases where all per-shard shapes are the same,"Implement `ConcreteSharding::GetShardShape()` for cases where all pershard shapes are the same Ideally, this should've used `ConcreteEvenSharding`, but there are many existing places that unconditionally instantiate `ConcreteSharding` from a list of pershard shapes without checking for identical pershard shapes. This CL avoids callers from having to special case `ConcreteSharding` when the callsites require identical pershard shapes.",2025-01-06T23:59:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84231
copybara-service[bot],Remove special handling for Reduce in HostOffloader.,Remove special handling for Reduce in HostOffloader.,2025-01-06T23:44:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84230
copybara-service[bot],Reverts 9055c056336ab90f4c54e24dc9a77ce7afd85166,Reverts 9055c056336ab90f4c54e24dc9a77ce7afd85166,2025-01-06T23:22:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84229
copybara-service[bot],[MultiHostHloRunner] Add `XSpaceProfilerInterface` that supports returning XSpace proto in program after profiling,[MultiHostHloRunner] Add `XSpaceProfilerInterface` that supports returning XSpace proto in program after profiling,2025-01-06T22:57:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84228
copybara-service[bot],- Fix op_profile deduplicated grouping by including the root dedup node whose deduplicated op name is empty string, Fix op_profile deduplicated grouping by including the root dedup node whose deduplicated op name is empty string  Fixed op limit control on op_profile UI,2025-01-06T22:48:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84227
copybara-service[bot],Enabling folding of transpose ops with per-axis quant inputs.,Enabling folding of transpose ops with peraxis quant inputs.,2025-01-06T22:37:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84226
copybara-service[bot],Fix `HloRunnerAgnosticTestBase` includes.,Fix `HloRunnerAgnosticTestBase` includes. Many of the tests that extend `HloTestBase` rely on symbols included transitively.  The main ones are:  `PlatformUtil`  `LiteralUtil`  `LiteralTestUtil` This patch adds includes for these explicitly.,2025-01-06T22:34:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84225
copybara-service[bot],Add new class xla::ifrt::PjRtMemoryDescription.,"Add new class xla::ifrt::PjRtMemoryDescription. (This only adds the class, in preparation of plumbing memory descriptions through IFRT. No functional changes yet.)",2025-01-06T22:03:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84224
copybara-service[bot],[XLA:LatencyHidingScheduler] Do not assume the operand of a recv-done (or send-done) is always a recv (or send). This code change fixes the use_of_uninitialized_value runtime error that was caused by calling is_host_transfer on a failed casting operation in the `OutOfOrderStartAndDone` test (due to the operand of recv-done not being a recv op).,[XLA:LatencyHidingScheduler] Do not assume the operand of a recvdone (or senddone) is always a recv (or send). This code change fixes the use_of_uninitialized_value runtime error that was caused by calling is_host_transfer on a failed casting operation in the `OutOfOrderStartAndDone` test (due to the operand of recvdone not being a recv op).,2025-01-06T21:52:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84223
copybara-service[bot],Add missing split markers to test,Add missing split markers to test,2025-01-06T21:29:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84222
copybara-service[bot],Save TF wheel version and suffix in repository rule.,Save TF wheel version and suffix in repository rule.,2025-01-06T20:56:35Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84221
copybara-service[bot],"[Coordination Service]Allow restartable tasks to connect back to cluster, as long as they have the same local topology as before.","[Coordination Service]Allow restartable tasks to connect back to cluster, as long as they have the same local topology as before.",2025-01-06T20:52:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84220
copybara-service[bot],Reverts 04dd53811eb0b694a41cdd91767f6f452605387a,Reverts 04dd53811eb0b694a41cdd91767f6f452605387a,2025-01-06T20:37:47Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84219
copybara-service[bot],Add `device_count` accessor to `HloRunnerInterface`.,Add `device_count` accessor to `HloRunnerInterface`. Also fixes hlo_runner_interface includes.,2025-01-06T20:11:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84218
copybara-service[bot],Remove `SKIP_TEST_IF_NUM_DEVICES_LESS_THAN` macro.,"Remove `SKIP_TEST_IF_NUM_DEVICES_LESS_THAN` macro. Macros should be avoided whenever possible. The `SKIP_TEST_IF_NUM_DEVICES_LESS_THAN` macro does two things. It inserts a new field `num_devices`, polluting the scope of the rest of the test. It also adds an implicit/nonobvious dependency on the runner. This patch removes the macro and switches any remaining uses to use `HloRunnerInterface::device_count` with an explicit message instead.",2025-01-06T20:09:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84217
copybara-service[bot],Migrate replicated_io_feed_test to always use PjRt for its test backend.,Migrate replicated_io_feed_test to always use PjRt for its test backend.,2025-01-06T20:06:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84216
copybara-service[bot],Remove unnecessary 4D operand checks for dynamic update slice. We have 4D operand support in the shader.,Remove unnecessary 4D operand checks for dynamic update slice. We have 4D operand support in the shader.,2025-01-06T19:55:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84215
copybara-service[bot],Fix invalid memory access.,Fix invalid memory access.,2025-01-06T19:41:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84214
copybara-service[bot],Add a flag protected pass to lower fake_quant annotation.,Add a flag protected pass to lower fake_quant annotation. LowerQuantAnnotationsPass is added which lowers quant.fake_quant composites to a pair of tfl.Quantizetfl.Dequantize ops which are later consumed by the converter quantization passes.,2025-01-06T19:26:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84213
copybara-service[bot],Remove #ifdef TENSORFLOW_USE_ROCM from array_elementwise_ops_test.cc.,Remove ifdef TENSORFLOW_USE_ROCM from array_elementwise_ops_test.cc.,2025-01-06T19:06:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84212
copybara-service[bot],Simplify the test targets in SPMD partitioner for reverse operations.,Simplify the test targets in SPMD partitioner for reverse operations.,2025-01-06T18:44:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84211
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T18:13:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84210
copybara-service[bot],Forward `use_spmd_partitioning` in HloRunnerPjRt.,Forward `use_spmd_partitioning` in HloRunnerPjRt. This patch also removes an unused and redundant invocation of `GenerateDefaultCompileOptions`.,2025-01-06T17:50:23Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84209
copybara-service[bot],[xla:cpu] Extend Dot benchmark test to support BF16 data type.,[xla:cpu] Extend Dot benchmark test to support BF16 data type. + Add a variant of `LiteralUtil::CreateRandomLiteral` that takes `mean` and `stddev` of type double.,2025-01-06T16:56:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84208
copybara-service[bot],[XLA:TPU] Disable optimization passes based on effort flag,[XLA:TPU] Disable optimization passes based on effort flag,2025-01-06T15:02:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84207
copybara-service[bot],PR #21022: [XLA:CPU][oneDNN] Modify addend shape for small Convolution + Bias contraction,"PR CC(Feature request : MoorePenrose pseudoinverse): [XLA:CPU][oneDNN] Modify addend shape for small Convolution + Bias contraction Imported from GitHub PR https://github.com/openxla/xla/pull/21022 For small constant biases, XLA may perform constant folding, which can alter the shape of the bias passed to oneDNN. This PR removes any extraneous trivial dimensions from the bias that is passed to the oneDNN library. It also adds a test in all applicable dtypes to test the functionality. Copybara import of the project:  5ce5ba690d5acd1ebedca1484483639b2f3b0be1 by Akhil Goel : Modify conv bias shape Merging this change closes CC(Feature request : MoorePenrose pseudoinverse) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21022 from Inteltensorflow:akhil/conv_bias_fix 5ce5ba690d5acd1ebedca1484483639b2f3b0be1",2025-01-06T13:08:44Z,,open,0,1,https://github.com/tensorflow/tensorflow/issues/84206,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
shaoyuyoung,[XLA] can't compile the tf.keras.layers.Conv2D when padding='valid'," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version nightly  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? XLA can't compile the `tf.keras.layers.Conv2D` when `padding='valid'`. However, eager can pass the check. There exists a misalignment  Standalone code to reproduce the issue ```shell import os import tensorflow as tf tf.keras.utils.set_random_seed(42) tf.random.set_seed(42) os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3"" os.environ[""CUDA_VISIBLE_DEVICES""] = ""1"" x = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], dtype=tf.float32) inputs = [x] class Model(tf.keras.Model):     def __init__(self):         super(Model, self).__init__()         self.conv = tf.keras.layers.Conv2D(filters=1, kernel_size=4, padding='valid', activation='relu')     def call(self, x):         x = tf.reshape(x, [1, 3, 3, 1])         x = self.conv(x)         return x model = Model() model(*inputs) print(""succeed on eager"") class Model(tf.keras.Model):     def __init__(self):         super(Model, self).__init__()         self.conv = tf.keras.layers.Conv2D(filters=1, kernel_size=4, padding='valid', activation='relu')     .function(jit_compile=True)     def call(self, x):         x = tf.reshape(x, [1, 3, 3, 1])         x = self.conv(x)         return x model = Model() model(*inputs) print(""succeed on XLA"") ```  Relevant log output ```shell succeed on eager Negative dimension size caused by subtracting 4 from 3 for '{{node conv2d_1_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Reshape, conv2d_1_1/convolution/ReadVariableOp)' with input shapes: [1,3,3,1], [4,4,1,1]. ```",2025-01-06T12:07:56Z,stat:awaiting response type:bug stale comp:xla TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/84205,", I was able to reproduce the issue on TensorFlow v2.17, v2.18 and tfnightly. Kindly find the gist of it here. Also Could you please confirm whether it was working with the versions which are older than the Tensorflow 2.16?  From v2.16, the TensorFlow contains Keras3.0 and the previous version contains Keras2.0. Thank you!",Thank you for your confirmation on this issue. :),This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Drop shard barrier custom calls in sharding-remover HLO pass.,Drop shard barrier custom calls in shardingremover HLO pass. This enables them to be noops for SingleDeviceSharding.,2025-01-06T11:14:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84204
phandat128,Encountered unresolved custom op: XlaDynamicSlice,"Hi, i am doing a task in converting T5 model to TFLite for Android. Currently, i am using T5ModelForConditionalGeneration from Huggingface to convert. The conversion is done with some below logging but when load the `generator` from Interpretor, and run an inference example, i have faced this error. You can reproduce with the colab provided below. AFAIK, this XlaDynamicSlice is in TF ops but why this op cannot be resolved in this cases.  **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04  TensorFlow installed from (source or binary): PyPI 24.0 on Python 3.11.5  TensorFlow version (or github SHA if from source): 2.18.0 **Provide the text output from tflite_convert** In colab version, tflite_convert doesn't log anything, below log is in my local version ``` INFO:tensorflow:Assets written to: /tmp/tmpaxxybw9x/assets INFO:tensorflow:Assets written to: /tmp/tmpaxxybw9x/assets W0000 00:00:1736157114.568747 1061359 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format. W0000 00:00:1736157114.568765 1061359 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency. 20250106 16:51:54.568997: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpaxxybw9x 20250106 16:51:54.645325: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve } 20250106 16:51:54.645352: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpaxxybw9x 20250106 16:51:55.085153: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle. 20250106 16:51:56.061632: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpaxxybw9x 20250106 16:51:56.517300: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 1948307 microseconds. 20250106 16:52:30.233639: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3825] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s): Flex ops: FlexStridedSlice Details: 	tf.StridedSlice(tensor, tensor, tensor, tensor) > (tensor) : {begin_mask = 13 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 13 : i64, new_axis_mask = 2 : i64, shrink_axis_mask = 0 : i64} See instructions: https://www.tensorflow.org/lite/guide/ops_select 20250106 16:52:30.233666: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3836] The following operation(s) need TFLite custom op implementation(s): Custom ops: XlaDynamicSlice Details: 	tf.XlaDynamicSlice(tensor, tensor, tensor) > (tensor) : {device = """"} See instructions: https://www.tensorflow.org/lite/guide/ops_custom ``` **Standalone code to reproduce the issue**  Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. My reproduce code in Colab: https://colab.research.google.com/drive/1Rmhc_vpJSa7M1Vt4ugV5uORaEfRJMOw?usp=sharing Also, please include a link to a GraphDef or the model if possible. **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2025-01-06T10:46:54Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/84203,"Hi,   I apologize for the delayed response, I tried to replicate the same behavior from my end with your Google colab notebook and I'm also getting the same error message `RuntimeError: Encountered unresolved custom op: XlaDynamicSlice.` for reference here is gistfile so we'll have to dig more into this issue and will update you, thank you for bringing this issue to our attention Thank you for your cooperation and patience.","Hi,   I apologize for the delayed response, I see in provided output log it says : `The following operation(s) need TFLite custom op implementation(s):Custom ops: XlaDynamicSlice` so it's unsupported ops since the LiteRT( Formerly knowns as TFLite) builtin operator library only supports a limited number of TensorFlow operators, not every model is convertible. For details, refer to operator compatibility. To allow conversion, you'll have to provide their own custom implementation of an unsupported TensorFlow operator in LiteRT, known as a custom operator in your case `XlaDynamicSlice` Op for more details please refer this official documentation If it's not mandatory to use T5 model in your use case or project then you can give it try with other models which can fulfill your usecase/project need the alternatives to the T5 model, some prominent options include: GPT3 (and its variants like GPT3.5 and GPT4), BERT, RoBERTa, XLNet, FlanT5 (an enhanced version of T5) Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Change the name of the ir dumps to have the suffix before the -ir-(with/no)-opt,Change the name of the ir dumps to have the suffix before the ir(with/no)opt,2025-01-06T10:30:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84202
copybara-service[bot],PR #20744: [NVIDIA GPU] Add a flag to control a2a collective matmul rewrite,PR CC(Fix for issue 20361  changed the test cases' inputs and expected su…): [NVIDIA GPU] Add a flag to control a2a collective matmul rewrite Imported from GitHub PR https://github.com/openxla/xla/pull/20744 This is address the revert in https://github.com/openxla/xla/pull/19451 where customers see MFU when enabling collective matmul by default. The a2a collective matmul kicks in by default on some small gemms and lead to inefficient transformation. Adding a flag to disable it by default since it's experimental. Copybara import of the project:  f3d320881ba0de6cd07429dc00176231fd2a1d9a by TJ Xu : Add a flag to control a2a collective matmul rewrite  0068abc2dba6865debcd71b80b235b268f048e6c by TJ Xu : added more comment for the new flag  9f88fe9a7feba2945aafe087dcdd639348581422 by TJ Xu : add flag to debug options Merging this change closes CC(Fix for issue 20361  changed the test cases' inputs and expected su…) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20744 from Tixxx:tixxx/add_flag_a2a_gemm 9f88fe9a7feba2945aafe087dcdd639348581422,2025-01-06T09:34:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84201
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T09:28:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84200
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T09:14:52Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84199
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T09:02:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84198
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T08:55:20Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84197
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T08:51:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84196
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T08:49:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84195
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T08:34:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84194
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T07:51:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84193
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T07:44:07Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84192
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T07:40:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84191
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T07:36:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84190
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T07:35:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84189
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T07:30:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84188
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T07:29:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84187
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T07:28:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84186
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T07:20:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84185
saiabhinav001,Add documentation for TensorFlow Eager Monitoring Counter bindings,This PR adds documentation for the TensorFlow Eager Monitoring Counter API bindings. It provides detailed information on functions that allow creating a new counter reader and reading counter values (both with and without labels) in Python.  Key updates include:  Clear documentation for the `TFE_MonitoringCounterReader` class and associated functions.  Enhanced understanding of the module’s functionality to support TensorFlow performance analysis and issue diagnostics in Python.,2025-01-06T06:37:49Z,size:S,closed,0,1,https://github.com/tensorflow/tensorflow/issues/84184,Please don't open multiple PRs for the same code. This is the third we're seeing the same PR and this could be considered as spamming. Closing since we also have CC(Add documentation for TensorFlow Eager Monitoring Counter bindings) and CC(Add documentation for TensorFlow Eager Monitoring Counter bindings)
copybara-service[bot],Add a layout field to `xla::ifrt::ArraySpec`,"Add a layout field to `xla::ifrt::ArraySpec` This is the most natural way to incrementally add layout support. `layout` can be nullptr, in which case it indicates a default layout, or nonexistent layout info if the runtime and/or DType do not support layouts. For now, the layout field uses `std::shared_ptr` as the type, but this can be switched to an IFRTnative layout type once it's introduced. The serialization assumes that the layout is always `xla::PjRtXlaLayout`, which holds true because that is the only available `xla::PjRtLayout` implementation today. This isn't ideal, but it is the only feasible way due to the lack of typeerased deserialization for `xla::PjRtLayout`. We don't have to worry about the backward compatibility of the serialization format right now because IFRT Proxy does not support layouts yet.",2025-01-06T05:36:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84183
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T05:18:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84182
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T02:32:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84179
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T02:31:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84178
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T02:31:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84177
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T02:30:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84176
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T02:30:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84175
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T02:11:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84174
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-06T02:08:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84173
copybara-service[bot],Improve signature runner test coverage by adding some tests of out-of-range cases.,Improve signature runner test coverage by adding some tests of outofrange cases.,2025-01-05T23:18:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84172
maxx-st,MFCC-Example-Model converted from TF to TFlite fails with IsPowerOfTwo-RuntimeError inside rfft2d," 1. System information  OS Platform and Distribution: Linux Mint 6.2.9  TensorFlow installation: pip  TensorFlow library: 2.18.0 (latest)  2. Code Below is a minimum example which triggers the rfft2d IsPowerOfTwo RuntimeError. The MFCCCalculation was directly taken from the tutorial from tensorflow.org ``` import tensorflow as tf class MFCCLayer(tf.keras.layers.Layer):     def __init__(self, **kwargs):         super(MFCCLayer, self).__init__(**kwargs)     def call(self, pcm):          A 1024point STFT with frames of 64 ms and 75% overlap.         stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256, fft_length=1024)         spectrograms = tf.abs(stfts)          Warp the linear scale spectrograms into the melscale.         num_spectrogram_bins = stfts.shape[1]         lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80         linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(             num_mel_bins,             num_spectrogram_bins,             sample_rate,             lower_edge_hertz,             upper_edge_hertz,         )         mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)         mel_spectrograms.set_shape(             spectrograms.shape[:1].concatenate(linear_to_mel_weight_matrix.shape[1:])         )          Compute a stabilized log to get logmagnitude melscale spectrograms.         log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e6)          Compute MFCCs from log_mel_spectrograms and take the first 13.         mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[             ..., :13         ]         print(""mfccs.shape: "", mfccs.shape)         return mfccs def build_model(input_shape):     input_layer = tf.keras.layers.Input(shape=input_shape)     output_layer = MFCCLayer()(input_layer)     return tf.keras.models.Model(inputs=input_layer, outputs=output_layer) if __name__ == ""__main__"":     batch_size, num_samples, sample_rate = 32, 32000, 16000.0      A Tensor of [batch_size, num_samples] mono PCM samples in the range [1, 1].     pcm = tf.random.normal([batch_size, num_samples], dtype=tf.float32)     print(""pcm.shape: "", pcm.shape)     model = build_model(pcm.shape)     model.summary()      Convert to TensorFlow Lite and Save     converter = tf.lite.TFLiteConverter.from_keras_model(model)     converter.target_spec.supported_ops = [         tf.lite.OpsSet.TFLITE_BUILTINS,         tf.lite.OpsSet.SELECT_TF_OPS,     ]     converter.optimizations = [tf.lite.Optimize.DEFAULT]     tflite_model = converter.convert()     with open(""mfcc.tflite"", ""wb"") as f:         f.write(tflite_model)      Load the model and run inference     with open(""mfcc.tflite"", ""rb"") as f:         tflite_model = f.read()     interpreter = tf.lite.Interpreter(model_content=tflite_model)     interpreter.allocate_tensors()     input_details = interpreter.get_input_details()     output_details = interpreter.get_output_details()     pcm = tf.expand_dims(pcm, axis=0)   Add batch dimension     interpreter.set_tensor(input_details[0][""index""], pcm)     interpreter.invoke()   < RuntimeError: tensorflow/lite/kernels/rfft2d.cc:117 IsPowerOfTwo(fft_length_data[1]) was not true.Node number 42 (RFFT2D) failed to prepare.     mfccs = interpreter.get_tensor(output_details[0][""index""])     print(""mfccs.shape: "", mfccs.shape) ```  3. Failure after conversion As far as I know, the RuntimeError should't happen, as all supplied stftfunction arguments are power of two's? I am unsure if this is just a user error from myself or this is a bug. I couldn't find any info online, hence i ask here. Is a MFCCcalculation model possible with TFlite? Thanks for all help",2025-01-05T20:45:45Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.18,closed,0,5,https://github.com/tensorflow/tensorflow/issues/84171,"Hi, st  I apologize for the delayed response, I tried your code snippet from my end and I'm also getting same error message ` < RuntimeError: tensorflow/lite/kernels/rfft2d.cc:117 IsPowerOfTwo(fft_length_data[1]) was not true.Node number 42 (RFFT2D) failed to prepare.` so for reference here is gistfile,  we need to dig more into this issue and will update you, thank you for bringing this issue to our attention. Thank you for your cooperation and patience. ","Hi, st  I apologize for the delayed response, I see Supported Select TensorFlow operators does not support **STFT** so it's causing the error during TFLite conversion process as far I know you can precompute MFCCs outside TFLite using libraries like Librosa or Torchaudio then feed the MFCCs into a TFLite model TFLite builtin operator library only supports a limited number of TensorFlow operators, not every model is convertible. For details, refer to operator compatibility. To allow conversion, users can enable the usage of certain TensorFlow ops in their TFLite model. Please refer this comment in TFLM issue https://github.com/tensorflow/tflitemicro/issues/2676issuecomment2337246053 Thank you for your cooperation and patience.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],"[xla:cpu] Update tsl/platform header include (logging, test_benchmark)","[xla:cpu] Update tsl/platform header include (logging, test_benchmark)",2025-01-05T19:28:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84170
copybara-service[bot],[xla:collectives] Replace xla::cpu::CollectivesCommunicator with xla::Communicator,[xla:collectives] Replace xla::cpu::CollectivesCommunicator with xla::Communicator,2025-01-05T19:25:32Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84169
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-05T17:56:43Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84168
MSPanchenko,Broken compatibility with tensorflow-metal in 2.18," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code Yes  OS platform and distribution MacOS 15.2  Mobile device _No response_  Python version 3.11.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory Apple M2 Max GPU 38cores  Current behavior? Apple silicone GPU with tensorflowmetal==1.1.0  and python 3.11 works fine with tensorboard==2.17.0 This is normal output: ``` /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/bin/python /Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] Process finished with exit code 0 ``` But if I upgrade tensorflow to 2.18 I'll have error, attached in ""Relevant log output"" issue section  Standalone code to reproduce the issue ```shell import tensorflow as tf if __name__ == '__main__':     gpus = tf.config.experimental.list_physical_devices('GPU')     print(gpus) ```  Relevant log output ```shell /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/bin/python /Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py  Traceback (most recent call last):   File ""/Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py"", line 1, in      import tensorflow as tf   File ""/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/sitepackages/tensorflow/__init__.py"", line 437, in      _ll.load_library(_plugin_dir)   File ""/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/sitepackages/tensorflow/python/framework/load_library.py"", line 151, in load_library     py_tf.TF_LoadLibrary(lib) tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/sitepackages/tensorflowplugins/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN3tsl8internal10LogMessageC1EPKcii   Referenced from:  /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/sitepackages/tensorflowplugins/libmetal_plugin.dylib   Expected in:      /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/sitepackages/tensorflow/python/_pywrap_tensorflow_internal.so Process finished with exit code 1 ```",2025-01-05T17:26:17Z,stat:awaiting response type:bug stale comp:gpu TF 2.18,closed,0,8,https://github.com/tensorflow/tensorflow/issues/84167,Ditto. Was experimenting with `uv` and wasted an hour or so before reverting to `pyenv` setup. This failed to work with tensorflow 2.18 and tensorflowmetal 1.1.0 but worked with tensorflow 2.17,"Hi **** , Apologies for the delay, and thank you for raising your concern here. Unfortunately, we do not support the tensorflowmetal plugin at the moment. However, as a workaround, I tried it with TensorFlow 2.16.2, and it worked fine for me. I’m attaching a screenshot for your reference.  I would request you to please post your issue here a faster resolution. Thank you!","I've created a bug, let's wait  https://developer.apple.com/forums/thread/772147","Hi **** , Thanks for raising your concern in that particular repo. Please check there for further updates. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,tensorflowmetal 1.2.0 released and it works with tf 2.18! https://developer.apple.com/forums/thread/772147 
PeterRK,How can I use local CUDA instead of hermetic CUDA to build? , Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution Ubuntu 24.04  Mobile device _No response_  Python version 3.12  Bazel version 6.5.0  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? Configures about hermetic CUDA are confusing. It seems to bring extra dependencies into the output binary. I want to build a tflib depends on local CUDA libs.  Standalone code to reproduce the issue ```shell  ```  Relevant log output _No response_,2025-01-05T17:26:06Z,type:build/install subtype: ubuntu/linux TF 2.18,open,0,2,https://github.com/tensorflow/tensorflow/issues/84166,", Could you please provide the specific usecase for the above ask. And also Every TensorFlow release is compatible with a certain CUDA version, for more information please take a look at the tested build configurations. https://www.tensorflow.org/install/sourcegpu Thank you!","> , Could you please provide the specific usecase for the above ask. And also Every TensorFlow release is compatible with a certain CUDA version, for more information please take a look at the tested build configurations. >  > https://www.tensorflow.org/install/sourcegpu >  > Thank you! My local CUDA version is newer than the tested one for TF2.18, but TF2.16 can be built and work well with it. I think the real problem is output binary links to hermetic CUDA libs and fails to work with local CUDA libs. Is there any way to build TF2.18 without hermetic CUDA just like building old version TF?  "
saiabhinav001,Add documentation for TensorFlow Eager Monitoring Counter bindings,This PR adds documentation for the TensorFlow Eager Monitoring Counter API bindings. It provides detailed information on functions that allow creating a new counter reader and reading counter values (both with and without labels) in Python.  Key updates include:  Clear documentation for the `TFE_MonitoringCounterReader` class and associated functions.  Enhanced understanding of the module’s functionality to support TensorFlow performance analysis and issue diagnostics in Python.,2025-01-05T16:04:00Z,size:S,closed,0,1,https://github.com/tensorflow/tensorflow/issues/84165,"This looks like an invalid PR, so I am going to close it."
saiabhinav001,Add documentation for TensorFlow Eager Monitoring Counter bindings,This PR adds documentation for the TensorFlow Eager Monitoring Counter API bindings. It provides detailed information on functions that allow creating a new counter reader and reading counter values (both with and without labels) in Python.  Key updates include:  Clear documentation for the `TFE_MonitoringCounterReader` class and associated functions.  Enhanced understanding of the module’s functionality to support TensorFlow performance analysis and issue diagnostics in Python.,2025-01-05T15:57:21Z,size:S,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84164,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",  您好，邮件已经收到，我会尽快处理的。谢谢,"Hi  , Can you please sign CLA , thank you !",Closing since there's also CC(Add documentation for TensorFlow Eager Monitoring Counter bindings) (you don't need to create a new one just to sign CLA)
copybara-service[bot],[xla:collectives] Add CollectivePermute API to XLA communicator,[xla:collectives] Add CollectivePermute API to XLA communicator,2025-01-05T03:01:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84163
copybara-service[bot],[xla:collectives] Add AllToAll API to XLA communicator,[xla:collectives] Add AllToAll API to XLA communicator,2025-01-05T02:58:17Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84162
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-05T02:56:00Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84161
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-05T02:40:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84160
copybara-service[bot],Take memory kind into account when calculating the default layout,"Take memory kind into account when calculating the default layout Some device types have different default layout choices for different memories, e.g., `unpinned_host` uses descending layout whereas `device` uses compact layout. This CL makes sure that `xla::ifrt::Client::GetDefaultLayout()` takes this into account and returns the correct default layout for a given memory kind.",2025-01-05T01:57:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84159
gaikwadrahul8,Fix 08 broken links in best_practices.md,"Hi, Team I found 08 broken documentation links in this best_practices.md file so I have updated those links to functional links. Please review and merge this change as appropriate. Thank you for your consideration.",2025-01-04T18:03:50Z,comp:lite ready to pull size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84158
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T12:34:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84157
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T11:59:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84156
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T11:52:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84155
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T11:49:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84154
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T11:47:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84153
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T11:45:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84152
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T11:44:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84151
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T11:39:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84150
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T11:37:13Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84149
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T11:33:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84148
copybara-service[bot],PR #20996: [GPU][NFC] Fix a mistype.,PR CC(Terminology changes in bazel scripts): [GPU][NFC] Fix a mistype. Imported from GitHub PR https://github.com/openxla/xla/pull/20996 Copybara import of the project:  f6f4a3f81f0cd893e6fcc9c99ab03732a32c1af7 by Ilia Sergachev : [GPU][NFC] Fix a mistype. Merging this change closes CC(Terminology changes in bazel scripts) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20996 from openxla:fix_mistype f6f4a3f81f0cd893e6fcc9c99ab03732a32c1af7,2025-01-04T11:24:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84147
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T09:34:41Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84146
copybara-service[bot],compat: Update forward compatibility horizon to 2025-01-04,compat: Update forward compatibility horizon to 20250104,2025-01-04T09:24:40Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84145
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T09:23:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84144
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T09:19:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84143
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T09:17:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84142
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T09:15:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84141
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T09:15:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84140
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T09:14:29Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84139
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T09:13:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84138
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T09:12:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84137
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T09:12:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84136
copybara-service[bot],compat: Update forward compatibility horizon to 2025-01-04,compat: Update forward compatibility horizon to 20250104,2025-01-04T08:23:39Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84135
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-04T07:31:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84134
copybara-service[bot],[xla] Fix nvtx_with_cuda_kernels_test tags,[xla] Fix nvtx_with_cuda_kernels_test tags,2025-01-04T04:45:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84133
copybara-service[bot],[stream_executor] Always return non-const pointer to device memory from DeviceMemory/DeviceMemoryBase,[stream_executor] Always return nonconst pointer to device memory from DeviceMemory/DeviceMemoryBase Constness of DeviceMemoryBase does not imply constness of underlying device memory (similar to how constness of absl::Span is not related to constness of underlying data),2025-01-04T04:22:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84132
copybara-service[bot],Simplify the SPMD partitioner on concatenate operations.,"Simplify the SPMD partitioner on concatenate operations. There is NO change if the concatenate result is replicated along the concatenate dimension. If this dimension is partitioned, this cl simplifies the partitioner. **Before.** Allocate the full output shape (i.e., make the concat dimension replicated), each partition updates its owned region, allreduce across partitions and then slice its output region. **After.** 1. Replicate the final sharding along the concatenate dimension to get `temp_sharding`. 2. Reshard the operands to `temp_sharding`. 3. Concatenate the operands to get result in `temp_sharding`. 4. Reshard the result from `temp_sharding` to the final sharding. An advantage of this method is that we use the standard `Reshard` API to save the cache for concatenate. The partitioner remembers that concatenate already has a copy with replicated sharding along the concat dimension. It can avoid unnecessary reshards when handling the following pattern generated by `jax.numpy.roll`. ``` ENTRY entry {   %param0 = f32[256] parameter(0), sharding={devices=[4] f32[64] {   %constant = f32[] constant(0)   %broadcast.1 = f32[512]{0} broadcast(f32[] %constant), dimensions={}   %param = f32[64]{0} parameter(0), sharding={devices=[4] f32[64] {   %constant = f32[] constant(0)   %broadcast = f32[256]{0} broadcast(f32[] %constant), dimensions={}   %param = f32[64]{0} parameter(0), sharding={devices=[4]<=[4]}   %constant.1 = s32[4]{0} constant({0, 64, 128, 192})   %partitionid = u32[] partitionid()   %dynamicslice.1 = s32[1]{0} dynamicslice(s32[4]{0} %constant.1, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape = s32[] reshape(s32[1]{0} %dynamicslice.1)   %dynamicupdateslice = f32[256]{0} dynamicupdateslice(f32[256]{0} %broadcast, f32[64]{0} %param, s32[] %reshape)   %allreduce = f32[256]{0} allreduce(f32[256]{0} %dynamicupdateslice), channel_id=1, replica_groups=[1,4]<=[4], use_global_device_ids=true, to_apply=%add.clone   %concatenate.1 = f32[512]{0} concatenate(f32[256]{0} %allreduce, f32[256]{0} %allreduce), dimensions={0}   %param.1 = s32[] parameter(1), sharding={replicated}   %dynamicslice.4 = f32[256]{0} dynamicslice(f32[512]{0} %concatenate.1, s32[] %param.1), dynamic_slice_sizes={256}   ROOT %dynamicslice.6 = f32[64]{0} dynamicslice(f32[256]{0} %dynamicslice.4, s32[] %reshape), dynamic_slice_sizes={64} } ```",2025-01-04T04:19:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84131
copybara-service[bot],[xla:cpu] Migrate AllToAll to unified collectives API,[xla:cpu] Migrate AllToAll to unified collectives API,2025-01-04T04:16:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84130
copybara-service[bot],[xla:cpu] Migrate CollectivePermute to unified collectives API,[xla:cpu] Migrate CollectivePermute to unified collectives API,2025-01-04T03:35:52Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84129
copybara-service[bot],Simplify and standardize the SPMD partitioner on concatenate.,"Simplify and standardize the SPMD partitioner on concatenate. If the concatenate dimension is sharded, we 1. allocate the full size along the concatenate dimension 2. for each operand, each partition updates its owned region 3. allreduce to get the result with full size along the concatenate dimension 4. reshard to the final sharding, i.e., shard the concatenate dimension An alternative is to directly reshard each operand such that they are replicated along the concatenate dimension. However, this may introduce a lot of resharding compared to the method above. An advantage of this method is that we use the standard `Reshard` API to save the cache for concatenate. The partitioner can remember that the allreduce corresponds the concatenate result with replicated sharding along the concat dimension. It can avoid unnecessary reshards when handling the following pattern generated by `jax.numpy.roll`. ``` ENTRY entry {   %param0 = f32[256] parameter(0), sharding={devices=[4] f32[64] {   %constant = f32[] constant(0)   %broadcast.1 = f32[512]{0} broadcast(f32[] %constant), dimensions={}   %param = f32[64]{0} parameter(0), sharding={devices=[4] f32[64] {   %constant = f32[] constant(0)   %broadcast = f32[512]{0} broadcast(f32[] %constant), dimensions={}   %param = f32[64]{0} parameter(0), sharding={devices=[4]<=[4]}   %constant.3 = s32[4]{0} constant({0, 1, 2, 3})   %partitionid = u32[] partitionid()   %dynamicslice.1 = s32[1]{0} dynamicslice(s32[4]{0} %constant.3, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape = s32[] reshape(s32[1]{0} %dynamicslice.1)   %constant.4 = s32[] constant(64)   %multiply = s32[] multiply(s32[] %reshape, s32[] %constant.4)   %dynamicupdateslice = f32[512]{0} dynamicupdateslice(f32[512]{0} %broadcast, f32[64]{0} %param, s32[] %multiply)   %constant.9 = s32[] constant(256)   %add.2 = s32[] add(s32[] %multiply, s32[] %constant.9)   %dynamicupdateslice.1 = f32[512]{0} dynamicupdateslice(f32[512]{0} %dynamicupdateslice, f32[64]{0} %param, s32[] %add.2)   %allreduce = f32[512]{0} allreduce(f32[512]{0} %dynamicupdateslice.1), channel_id=1, replica_groups={{0,1,2,3}}, use_global_device_ids=true, to_apply=%add.clone   %param.1 = s32[] parameter(1), sharding={replicated}   %dynamicslice.6 = f32[256]{0} dynamicslice(f32[512]{0} %allreduce, s32[] %param.1), dynamic_slice_sizes={256}   %constant.12 = s32[4]{0} constant({0, 64, 128, 192})   %dynamicslice.7 = s32[1]{0} dynamicslice(s32[4]{0} %constant.12, u32[] %partitionid), dynamic_slice_sizes={1}   %reshape.4 = s32[] reshape(s32[1]{0} %dynamicslice.7)   ROOT %dynamicslice.8 = f32[64]{0} dynamicslice(f32[256]{0} %dynamicslice.6, s32[] %reshape.4), dynamic_slice_sizes={64} } ```",2025-01-04T02:40:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84128
copybara-service[bot],[Mosaic] Migrate hlo_legalize_to_arithmetic pass from MHLO to StableHLO.,[Mosaic] Migrate hlo_legalize_to_arithmetic pass from MHLO to StableHLO. HLO>MHLO>Arith to HLO>MHLO> StableHLO + MHLO >Arith,2025-01-04T02:10:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84127
copybara-service[bot],[HLO Componentization] Populate xla/hlo/ with hlo specific protos (Phase I).,[HLO Componentization] Populate xla/hlo/ with hlo specific protos (Phase I). This CL takes care of 1. Migrating the targets ``` xla/service:hlo_proto to xla/hlo/ir:hlo_ir_proto xla:xla_data_proto to xla/hlo/core:hlo_proto ``` 2. Setting up build aliases in xla or xla/service/ ensuring external dependencies are still satisfied. Phase II will take care of migration of external projects dependencies,2025-01-04T01:51:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84126
copybara-service[bot],[HLO Componentization] Avoid using deprecated headers/build-targets,[HLO Componentization] Avoid using deprecated headers/buildtargets This is done mainly to remove loads of build warnings on using deprecated targets.,2025-01-04T01:37:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84125
copybara-service[bot],Change IFRT and PjRt layout API to return `std::shared_ptr<const xla::PjRtLayout>` instead of `std::unique_ptr<xla::PjRtLayout>`,"Change IFRT and PjRt layout API to return `std::shared_ptr` instead of `std::unique_ptr` The current API design that uses `std::unique_ptr` has several issues: * The API requires `xla::PjRtLayout` to be copied in some scenarios, e.g., `xla::ifrt::Array` internally stores a layout and returns its copy every time `layout()` is called. This forces implementations to break the abstraction boundary because `xla::PjRtLayout` is an abstract class and `std::unique_ptr` is not copyable. The current implementation either stores `xla::Layout` and creates `xla::PjRtLayout` every time, or downcasts `xla::PjRtLayout` to `xla::PjRtXlaLayout` to perform the copy. * `xla::Layout` is expensive to copy (`sizeof(xla::Layout)` is 248 bytes as of 20250103) and copying `xla::PjRtXlaLayout` requires copying or moving `xla::Layout`. To address these two problems, this CL changes PjRt and IFRT APIs that return `xla::PjRtLayout` to instead use `std::shared_ptr`, so that PjRt layouts can be cheaply copied. Similar patterns have been used in other places such as `xla::ifrt::Sharding` and `xla::PjRtExecutable::GetHloModules()`. Some implementations have been updated to take advantage of this change. For example, `PjRtCApiBuffer::layout()` no longer performs a layout copy and instead reuses an internally cached instance of `std::shared_ptr`.",2025-01-04T01:34:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84124
copybara-service[bot],Add support for pinning tensors to device memory in XLA. When a tensor is pinned to device memory it will not be prefetched to alternate memory (or assigned in alternate memory altogether which is possible when it is not pinned).,Add support for pinning tensors to device memory in XLA. When a tensor is pinned to device memory it will not be prefetched to alternate memory (or assigned in alternate memory altogether which is possible when it is not pinned).,2025-01-04T00:57:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84123
copybara-service[bot],Reverts 04dd53811eb0b694a41cdd91767f6f452605387a,Reverts 04dd53811eb0b694a41cdd91767f6f452605387a,2025-01-04T00:38:58Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84122
copybara-service[bot],Forward `use_spmd_partitioning` in HloRunnerPjRt.,Forward `use_spmd_partitioning` in HloRunnerPjRt. This patch also removes an unused and redundant invocation of `GenerateDefaultCompileOptions`.,2025-01-04T00:34:34Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84121
copybara-service[bot],[xla] Disable nvtx_with_cuda_kernels_test on h100 backend,[xla] Disable nvtx_with_cuda_kernels_test on h100 backend,2025-01-03T22:42:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84120
dnmaster1,Tensortflow import issue after installation," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I resintalled Python and my Anaconda environment and reinstalled using pip from notebook. Please see attached installation log and then import logs  Standalone code to reproduce the issue ```shell pip install tensorflow Collecting tensorflow   Using cached tensorflow2.18.0cp312cp312win_amd64.whl.metadata (3.3 kB) Collecting tensorflowintel==2.18.0 (from tensorflow)   Using cached tensorflow_intel2.18.0cp312cp312win_amd64.whl.metadata (4.9 kB) Collecting abslpy>=1.0.0 (from tensorflowintel==2.18.0>tensorflow)   Using cached absl_py2.1.0py3noneany.whl.metadata (2.3 kB) Collecting astunparse>=1.6.0 (from tensorflowintel==2.18.0>tensorflow)   Using cached astunparse1.6.3py2.py3noneany.whl.metadata (4.4 kB) Collecting flatbuffers>=24.3.25 (from tensorflowintel==2.18.0>tensorflow)   Using cached flatbuffers24.12.23py2.py3noneany.whl.metadata (876 bytes) Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflowintel==2.18.0>tensorflow)   Using cached gast0.6.0py3noneany.whl.metadata (1.3 kB) Collecting googlepasta>=0.1.1 (from tensorflowintel==2.18.0>tensorflow)   Using cached google_pasta0.2.0py3noneany.whl.metadata (814 bytes) Collecting libclang>=13.0.0 (from tensorflowintel==2.18.0>tensorflow)   Using cached libclang18.1.1py2.py3nonewin_amd64.whl.metadata (5.3 kB) Collecting opteinsum>=2.3.2 (from tensorflowintel==2.18.0>tensorflow)   Using cached opt_einsum3.4.0py3noneany.whl.metadata (6.3 kB) Requirement already satisfied: packaging in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (24.1) Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,=3.20.3 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (4.25.3) Requirement already satisfied: requests=2.21.0 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (2.32.3) Requirement already satisfied: setuptools in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (75.1.0) Requirement already satisfied: six>=1.12.0 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (1.16.0) Collecting termcolor>=1.1.0 (from tensorflowintel==2.18.0>tensorflow)   Using cached termcolor2.5.0py3noneany.whl.metadata (6.1 kB) Requirement already satisfied: typingextensions>=3.6.6 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (4.11.0) Requirement already satisfied: wrapt>=1.11.0 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (1.14.1) Collecting grpcio=1.24.3 (from tensorflowintel==2.18.0>tensorflow)   Using cached grpcio1.68.1cp312cp312win_amd64.whl.metadata (4.0 kB) Collecting tensorboard=2.18 (from tensorflowintel==2.18.0>tensorflow)   Using cached tensorboard2.18.0py3noneany.whl.metadata (1.6 kB) Collecting keras>=3.5.0 (from tensorflowintel==2.18.0>tensorflow)   Using cached keras3.7.0py3noneany.whl.metadata (5.8 kB) Requirement already satisfied: numpy=1.26.0 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (1.26.4) Requirement already satisfied: h5py>=3.11.0 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorflowintel==2.18.0>tensorflow) (3.11.0) Collecting mldtypes=0.4.0 (from tensorflowintel==2.18.0>tensorflow)   Using cached ml_dtypes0.4.1cp312cp312win_amd64.whl.metadata (20 kB) Requirement already satisfied: wheel=0.23.0 in c:\users\dhima\anaconda3\lib\sitepackages (from astunparse>=1.6.0>tensorflowintel==2.18.0>tensorflow) (0.44.0) Requirement already satisfied: rich in c:\users\dhima\anaconda3\lib\sitepackages (from keras>=3.5.0>tensorflowintel==2.18.0>tensorflow) (13.7.1) Collecting namex (from keras>=3.5.0>tensorflowintel==2.18.0>tensorflow)   Using cached namex0.0.8py3noneany.whl.metadata (246 bytes) Collecting optree (from keras>=3.5.0>tensorflowintel==2.18.0>tensorflow)   Using cached optree0.13.1cp312cp312win_amd64.whl.metadata (48 kB) Requirement already satisfied: charsetnormalizer=2 in c:\users\dhima\anaconda3\lib\sitepackages (from requests=2.21.0>tensorflowintel==2.18.0>tensorflow) (3.3.2) Requirement already satisfied: idna=2.5 in c:\users\dhima\anaconda3\lib\sitepackages (from requests=2.21.0>tensorflowintel==2.18.0>tensorflow) (3.7) Requirement already satisfied: urllib3=1.21.1 in c:\users\dhima\anaconda3\lib\sitepackages (from requests=2.21.0>tensorflowintel==2.18.0>tensorflow) (2.2.3) Requirement already satisfied: certifi>=2017.4.17 in c:\users\dhima\anaconda3\lib\sitepackages (from requests=2.21.0>tensorflowintel==2.18.0>tensorflow) (2024.12.14) Requirement already satisfied: markdown>=2.6.8 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorboard=2.18>tensorflowintel==2.18.0>tensorflow) (3.4.1) Collecting tensorboarddataserver=0.7.0 (from tensorboard=2.18>tensorflowintel==2.18.0>tensorflow)   Using cached tensorboard_data_server0.7.2py3noneany.whl.metadata (1.1 kB) Requirement already satisfied: werkzeug>=1.0.1 in c:\users\dhima\anaconda3\lib\sitepackages (from tensorboard=2.18>tensorflowintel==2.18.0>tensorflow) (3.0.3) Requirement already satisfied: MarkupSafe>=2.1.1 in c:\users\dhima\anaconda3\lib\sitepackages (from werkzeug>=1.0.1>tensorboard=2.18>tensorflowintel==2.18.0>tensorflow) (2.1.3) Requirement already satisfied: markdownitpy>=2.2.0 in c:\users\dhima\anaconda3\lib\sitepackages (from rich>keras>=3.5.0>tensorflowintel==2.18.0>tensorflow) (2.2.0) Requirement already satisfied: pygments=2.13.0 in c:\users\dhima\anaconda3\lib\sitepackages (from rich>keras>=3.5.0>tensorflowintel==2.18.0>tensorflow) (2.15.1) Requirement already satisfied: mdurl~=0.1 in c:\users\dhima\anaconda3\lib\sitepackages (from markdownitpy>=2.2.0>rich>keras>=3.5.0>tensorflowintel==2.18.0>tensorflow) (0.1.0) Using cached tensorflow2.18.0cp312cp312win_amd64.whl (7.5 kB) Using cached tensorflow_intel2.18.0cp312cp312win_amd64.whl (390.3 MB) Using cached absl_py2.1.0py3noneany.whl (133 kB) Using cached astunparse1.6.3py2.py3noneany.whl (12 kB) Using cached flatbuffers24.12.23py2.py3noneany.whl (30 kB) Using cached gast0.6.0py3noneany.whl (21 kB) Using cached google_pasta0.2.0py3noneany.whl (57 kB) Using cached grpcio1.68.1cp312cp312win_amd64.whl (4.4 MB) Using cached keras3.7.0py3noneany.whl (1.2 MB) Using cached libclang18.1.1py2.py3nonewin_amd64.whl (26.4 MB) Using cached ml_dtypes0.4.1cp312cp312win_amd64.whl (127 kB) Using cached opt_einsum3.4.0py3noneany.whl (71 kB) Using cached tensorboard2.18.0py3noneany.whl (5.5 MB) Using cached termcolor2.5.0py3noneany.whl (7.8 kB) Using cached tensorboard_data_server0.7.2py3noneany.whl (2.4 kB) Using cached namex0.0.8py3noneany.whl (5.8 kB) Using cached optree0.13.1cp312cp312win_amd64.whl (292 kB) Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboarddataserver, optree, opteinsum, mldtypes, grpcio, googlepasta, gast, astunparse, abslpy, tensorboard, keras, tensorflowintel, tensorflow Successfully installed abslpy2.1.0 astunparse1.6.3 flatbuffers24.12.23 gast0.6.0 googlepasta0.2.0 grpcio1.68.1 keras3.7.0 libclang18.1.1 mldtypes0.4.1 namex0.0.8 opteinsum3.4.0 optree0.13.1 tensorboard2.18.0 tensorboarddataserver0.7.2 tensorflow2.18.0 tensorflowintel2.18.0 termcolor2.5.0 ```  Relevant log output ```shell import tensorflow as tf O/p  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[2], line 1 > 1 import tensorflow as tf File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:85      83     sys.setdlopenflags(_default_dlopen_flags)      84 except ImportError: > 85   raise ImportError(      86       f'{traceback.format_exc()}'      87       f'\n\nFailed to load the native TensorFlow runtime.\n'      88       f'See https://www.tensorflow.org/install/errors '      89       f'for some common causes and solutions.\n'      90       f'If you need help, create an issue '      91       f'at https://github.com/tensorflow/tensorflow/issues '      92       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\dhima\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```",2025-01-03T19:35:37Z,type:bug,closed,0,7,https://github.com/tensorflow/tensorflow/issues/84119,"check python version ,I think it need 3.10 version because it was giving the similar type of error in 3.12 if still error persists try with gpu","Hi Manoj  I tried 3.10 ad 3.12. I don't believe they are now available for download. I also tried tensorflowcpu, but it didn't help. Where do you find tensorflowgpu? I don't have a GPU, but my CPU is ARM arch. Thanks Dhimant On Sat, Jan 4, 2025 at 6:22 AM Manoj Nayak ***@***.***> wrote: > check python version ,I think it need 3.10 version because it was giving > the similar type of error in 3.12 > if still error persists try with gpu > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. Please only open new issues if there is information (like your CPU specs) that make your problem different than the existing one.,Are you satisfied with the resolution of your issue? Yes No,"I did search. You closed my previous issue. On Sun, Jan 5, 2025 at 9:37 AM Mihai Maruseac ***@***.***> wrote: > Duplicate of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) > . Please do a > search before opening new issues. Please only open new issues if there is > information (like your CPU specs) that make your problem different than the > existing one. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","Please reopen. This is not closed. On Sun, Jan 5, 2025 at 9:37 AM Mihai Maruseac ***@***.***> wrote: > Closed CC(Tensortflow import issue after installation)  as > completed. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",Opening multiple issues can be considered as spam. Let's move discussion to just one issue.
copybara-service[bot],Integrate LLVM at llvm/llvm-project@3ef78188d0d3,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 3ef78188d0d3,2025-01-03T17:21:03Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84116
copybara-service[bot],Update to match upstream API change (NFC).,"Update to match upstream API change (NFC). This method was renamed but staging function kept, switch to renamed variant.",2025-01-03T17:14:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84115
copybara-service[bot],[XLA:CPU] Move IrEmitter2 test cases to new Kernel API versions,[XLA:CPU] Move IrEmitter2 test cases to new Kernel API versions,2025-01-03T17:10:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84114
copybara-service[bot],[XLA:CPU] Remove old IrEmitter::EmitElementalHostKernel,[XLA:CPU] Remove old IrEmitter::EmitElementalHostKernel,2025-01-03T17:09:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84113
copybara-service[bot],[xla:cpu] Migrate ReduceScatter to unified collectives API,[xla:cpu] Migrate ReduceScatter to unified collectives API,2025-01-03T16:59:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84112
copybara-service[bot],[xla:cpu] Migrate AllGather to unified collectives API,[xla:cpu] Migrate AllGather to unified collectives API,2025-01-03T16:58:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84111
copybara-service[bot],Support evaluation in the absence of layouts when possible,"Support evaluation in the absence of layouts when possible Only bitcast requires the layout to be known when evaluating HLO. In all other cases, we can evaluate without knowing the layout. This is needed for collective pipelining where we have to analyse while loops before layouts were assigned.",2025-01-03T16:51:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84110
copybara-service[bot],PR #20954: [XLA:GPU] migrate command buffer to use buffer_use.h,"PR CC(Entry point not found error reported when calling tf.contrib.rnn.BasicRNNCell): [XLA:GPU] migrate command buffer to use buffer_use.h Imported from GitHub PR https://github.com/openxla/xla/pull/20954 This PR does not introduce new functionality, it's a refactoring, and is covered by existing command buffer tests. Copybara import of the project:  b1a0efcc8bdd74c2785dcd924c3da461b2b90ef4 by Shawn Wang : migrate command buffer to use buffer_use.h Merging this change closes CC(Entry point not found error reported when calling tf.contrib.rnn.BasicRNNCell) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20954 from shawnwang18:shawnw/migrate_buffer_use b1a0efcc8bdd74c2785dcd924c3da461b2b90ef4",2025-01-03T16:50:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84109
copybara-service[bot],Update to match upstream API change (NFC).,"Update to match upstream API change (NFC). This method was renamed but staging function kept, switch to renamed variant.",2025-01-03T16:44:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84108
copybara-service[bot],Replace outdated select() on --cpu in compiler/mlir/lite/kernels/internal/BUILD with platform API equivalent.,Replace outdated select() on cpu in compiler/mlir/lite/kernels/internal/BUILD with platform API equivalent.,2025-01-03T16:16:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84107
copybara-service[bot],[XLA:CPU] Remove CompiledFunctionLibrary from JitCompiler.,[XLA:CPU] Remove CompiledFunctionLibrary from JitCompiler.,2025-01-03T16:16:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84106
copybara-service[bot],[XLA:CPU] Decouple compiled function library from JIT compiler.,[XLA:CPU] Decouple compiled function library from JIT compiler.,2025-01-03T15:33:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84105
zzzHou01,"KeyError: ""There is no item named 'PetImages\\Cat\\0.jpg' in the archive"" When Running TensorFlow Locally(CPU) on Anaconda in VS Code."," Issue type Documentation Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version 2.17.1  Custom code Yes  OS platform and distribution window11  Mobile device _No response_  Python version 3.10.11  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I am a beginner and encountering an issue while trying to run TensorFlow locally using Anaconda in VS Code. The same code runs smoothly on Google Colab, but when executed locally, it fails during the dataset download process with the following error. What I've Tried Redownloading TensorFlow and TensorFlow Datasets to ensure they are up to date. Manually Unzipping the Dataset and verifying if 'PetImages/Cat/0.jpg' exists in the archive. Readjusting Python, TensorFlow, and TensorFlow Datasets Versions to match those in Colab by using Python 3.10.11 instead of Python 3.10.12. Recreating the Virtual Environment in Anaconda to ensure a clean setup. Downloading the Cats vs Dogs Dataset from Different Sources, but the issue persists. Asking ChatGPT for assistance, but the issue remains unresolved. Additional Information On Google Colab, the same code runs without any issues, and the dataset downloads successfully. In VS Code, the error consistently occurs during the dataset download process, indicating that 'PetImages/Cat/0.jpg' is missing from the archive. Network Stability: I have a stable internet connection, and downloads complete without interruption, but the error persists. Questions Why does the KeyError occur in VS Code but not in Google Colab? Could this be related to the way the dataset is being downloaded or unzipped locally? Are there any compatibility issues between the Python/TensorFlow versions and the dataset? Request for Help I would greatly appreciate any guidance or suggestions on how to resolve this issue. Thank you in advance for your assistance!  Standalone code to reproduce the issue ```shell import tensorflow_datasets as tfds import tensorflow as tf import numpy as np CatsVsDogs_OrgData, info=tfds.load(name='cats_vs_dogs', with_info=True,                   split=tfds.Split.TRAIN) ```  Relevant log output ```shell PS C:\Users\jbb86\桌面\圖樣辨識> & C:/Users/jbb86/桌面/圖樣辨識/.venv/Scripts/python.exe c:/Users/jbb86/桌面/圖樣辨識/.venv/CatsVsDogs.py 20250103 22:38:53.599253: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 20250103 22:38:54.247816: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floatingpoint roundoff errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\Users\jbb86\tensorflow_datasets\cats_vs_dogs\4.0.1... Dl Size...: 100% 824887076/824887076 [00:00     CatsVsDogs_OrgData, info=tfds.load(name='cats_vs_dogs', with_info=True,   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__     return function(*args, **kwargs)   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\load.py"", line 661, in load     _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\load.py"", line 517, in _download_and_prepare_builder     dbuilder.download_and_prepare(**download_and_prepare_kwargs)   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__     return function(*args, **kwargs)   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\dataset_builder.py"", line 756, in download_and_prepare     self._download_and_prepare(   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\dataset_builder.py"", line 1752, in _download_and_prepare     split_infos = self._generate_splits(dl_manager, download_config)   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\dataset_builder.py"", line 1727, in _generate_splits     future = split_builder.submit_split_generation(   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\split_builder.py"", line 436, in submit_split_generation     return self._build_from_generator(**build_kwargs)   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\core\split_builder.py"", line 496, in _build_from_generator     for key, example in utils.tqdm(   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tqdm\std.py"", line 1181, in __iter__     for obj in iterable:   File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\sitepackages\tensorflow_datasets\image_classification\cats_vs_dogs.py"", line 117, in _generate_examples     new_fobj = zipfile.ZipFile(buffer).open(fname)   File ""C:\Users\jbb86\AppData\Local\Programs\Python\Python310\lib\zipfile.py"", line 1516, in open     zinfo = self.getinfo(name)   File ""C:\Users\jbb86\AppData\Local\Programs\Python\Python310\lib\zipfile.py"", line 1443, in getinfo     raise KeyError( KeyError: ""There is no item named 'PetImages\\\\Cat\\\\0.jpg' in the archive"" ```",2025-01-03T14:48:36Z,type:others awaiting PR merge 2.17,open,0,7,https://github.com/tensorflow/tensorflow/issues/84104,", Could you please provide the document which you are following to execute the code so that it helps to debug the issue. Thank you!","> , Could you please provide the document which you are following to execute the code so that it helps to debug the issue. Thank you! Thank you for your response! I am following the TensorFlow Cats vs Dogs tutorial provided on the official TensorFlow website. Source:https://www.tensorflow.org/tutorials/images/transfer_learning","I can replicate the issue on Windows 10 with TF 2.18, python 3.12 and tensorflowdatasets 4.9.7. It doesn't happen on Colab or MacOS.  I believe the issue has to do with how the file path is parsed in Windows vs Unix/Linux. I'll send out a CL to fix this shortly. This fix is going to be in tensorflow/datasets by the way.  Also, this is probably a duplicate of issues/3918 in tensorflow/datasets.  There is a hacky fix available in the issuecomment1892835410 in the meantime.","This fix for this was merged to tensorflow/datasets through https://github.com/tensorflow/datasets/commit/9969ce542f4b0e1cbf0a085e8e0df11bccea5c17. Once there is a new release, the problem should be fixed.","> I can replicate the issue on Windows 10 with TF 2.18, python 3.12 and tensorflowdatasets 4.9.7. It doesn't happen on Colab or MacOS. >  > I believe the issue has to do with how the file path is parsed in Windows vs Unix/Linux. I'll send out a CL to fix this shortly. This fix is going to be in tensorflow/datasets by the way. >  > Also, this is probably a duplicate of issues/3918 in tensorflow/datasets. [](https://github.com/zzzHou01) There is a hacky fix available in the issuecomment1892835410 in the meantime. > 此修復已透過 tensorflow/datasets @ 9969ce5合併到 tensorflow/datasets 。一旦有新版本發布，該問題就會解決。 Thanks a lot for your detailed explanation! It helped me solve my problem. I really appreciate your support.",> 我可以在裝有 TF 2.18、python 3.12 和 tensorflowdatasets 4.9.7 的 Windows 10 上複製該問題。這在 Colab 或 MacOS 上不會發生。 >  > 我相信這個問題與 Windows 和 Unix/Linux 中檔案路徑的解析方式有關。我將很快發送 CL 來修復此問題。順便說一下，這個修復將在tensorflow/datasets中進行。 >  > 此外，這可能是tensorflow/datasets 中issues/3918的重複。[](https://github.com/zzzHou01)同時，issuecomment1892835410中有一個可用的駭客修復程序。 Thanks a lot for your detailed explanation! It helped me solve my problem. I really appreciate your support.,"> This fix for this was merged to tensorflow/datasets through tensorflow/datasets. Once there is a new release, the problem should be fixed. Thanks a lot for your detailed explanation! It helped me solve my problem. I really appreciate your support."
dnmaster1,Tensorflow not supported on Windows + ARM CPUs," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.18  Custom code No  OS platform and distribution Windows 11  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? I can't import tensorflow  Standalone code to reproduce the issue ```shell I can't import tensorflow. Installation is successful. I uninstalled and reinstalled ```  Relevant log output ```shell  ImportError                               Traceback (most recent call last) File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:70      69 try: > 70   from tensorflow.python._pywrap_tensorflow_internal import *      71  This try catch logic is because there is no bazel equivalent for py_extension.      72  Externally in opensource we must enable exceptions to load the shared object      73  by exposing the PyInit symbols with pybind. This error will only be      74  caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.      75       76  This logic is used in other internal projects using py_extension. ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. During handling of the above exception, another exception occurred: ImportError                               Traceback (most recent call last) Cell In[9], line 1 > 1 import tensorflow as tf File ~\anaconda3\Lib\sitepackages\tensorflow\__init__.py:40      37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")      39  Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596 > 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow   pylint: disable=unusedimport      41 from tensorflow.python.tools import module_util as _module_util      42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader File ~\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py:85      83     sys.setdlopenflags(_default_dlopen_flags)      84 except ImportError: > 85   raise ImportError(      86       f'{traceback.format_exc()}'      87       f'\n\nFailed to load the native TensorFlow runtime.\n'      88       f'See https://www.tensorflow.org/install/errors '      89       f'for some common causes and solutions.\n'      90       f'If you need help, create an issue '      91       f'at https://github.com/tensorflow/tensorflow/issues '      92       f'and include the entire stack trace above this error message.') ImportError: Traceback (most recent call last):   File ""C:\Users\dhima\anaconda3\Lib\sitepackages\tensorflow\python\pywrap_tensorflow.py"", line 70, in      from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common causes and solutions. If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message. ```",2025-01-03T14:33:01Z,stat:awaiting tensorflower type:feature type:build/install subtype:windows TF 2.18,open,0,21,https://github.com/tensorflow/tensorflow/issues/84102,Duplicate of https://github.com/tensorflow/tensorflow/issues/19584. Please do a search before opening new issues. This is a very old issue and manifests because old PCs with Windows cannot load libraries needed by TF because they have very old architecture sets.,Are you satisfied with the resolution of your issue? Yes No,"I have brand new PC with snapdragon X plus. It is not working. It is in fact working on my old pc On Fri, Jan 3, 2025, 11:59 AM Mihai Maruseac ***@***.***> wrote: > Duplicate of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) > . Please do a > search before opening new issues. This is a very old issue and manifests > because old PCs with Windows cannot load libraries needed by TF because > they have very old architecture sets. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","Please don't close the issue. Yourassumotion is incorrect On Fri, Jan 3, 2025, 11:59 AM Mihai Maruseac ***@***.***> wrote: > Closed CC(Tensorflow not supported on Windows + ARM CPUs)  as > completed. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",What are your CPU specs?,"[image: image.png] On Sat, Jan 4, 2025 at 12:09 PM Mihai Maruseac ***@***.***> wrote: > What are your CPU specs? > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",The image is not getting displayed. Can you paste them instead as text?,"Device name Master2024 Processor Snapdragon(R) X Plus  X1P42100  Qualcomm(R) Oryon(TM) CPU 3.24 GHz Installed RAM 16.0 GB (15.6 GB usable) Device ID BE82051361FA4460944DDA3541AEED2D Product ID 003422133297204AAOEM System type 64bit operating system, ARMbased processor Pen and touch Pen and touch support with 10 touch points Windows specs Edition Windows 11 Home Version 24H2 Installed on ‎12/‎30/‎2024 OS build 26100.2605 Serial number YX0ECPSK Experience Windows Feature Experience Pack 1000.26100.36.0 On Sun, Jan 5, 2025 at 9:31 AM Mihai Maruseac ***@***.***> wrote: > The image is not getting displayed. Can you paste them instead as text? > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","I think TensorFlow on Windows is only supported on Intel, not ARM. But this is indeed different than the other one, so reopening.","Confirmed that Windows support only exists for Intel CPUs. On https://pypi.org/project/tensorflow/files there is no Windows + ARM wheel. On https://pypi.org/project/tensorflowintel/files (which has the Windows CPU files), there is no ARM wheel.","This is also weird, because the pip installer should not have proceeded due to missing wheels. But, can you try installing the linux wheel, via WSL? Not guaranteed to work, but it might.","So my CPU configuration is not supported? On Mon, Jan 6, 2025, 8:36 AM Mihai Maruseac ***@***.***> wrote: > This is also weird, because the pip installer should not have proceeded > due to missing wheels. > > But, can you try installing the linux wheel, via WSL? Not guaranteed to > work, but it might. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","What is ""missing wheels""? I will try WSL now and get back to you. On Mon, Jan 6, 2025, 8:36 AM Mihai Maruseac ***@***.***> wrote: > This is also weird, because the pip installer should not have proceeded > due to missing wheels. > > But, can you try installing the linux wheel, via WSL? Not guaranteed to > work, but it might. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","> So my CPU configuration is not supported? It looks like that. It is different than CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) in that that issue refers to Intel CPUs that are too old and don't have AVX/AVX2 extensions, whereas here your CPU is ARM, which is a completely different instruction set. It needs files compiled specifically for this architecture and we currently only do that for Linux and Apple (due to Mac M* family). > What is ""missing wheels""? The unit of shipping a Python package is called a wheel. For most projects, there is just one single file for any combination of Python, operating system, architecture. But, since TF needs to compile C++ extensions and link to C Python code, TensorFlow needs to ship different files for different supported configurations. In this case, there is no file that can be served for Windows + ARM CPU. When installing a Python package (via `pip install`, but other installers should follow a relatively similar process), `pip` is looking at the list of files for the specified version and tries to find the one that matches all the architecture tags it knows (Python version, operating system, CPU architecture, GLIB version, manylinux standard, etc.). This ensures that what gets installed works on the system. There in an exception to the above process with CPU extensions: there is no tag for them (since they are quite a lot and can result in exponential explosion of configurations) so `pip` cannot determine them before downloading. This is why CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) exists: people have been trying to use TF on very old CPUs and the Windows error message is very confusing (compared to Linux/Mac where it states clearly that the instruction set is not supported). In your case, it is very weird that an ARM CPU and a Windows OS did result in a successful download, even though there is no wheel that matches these tags. > I will try WSL now and get back to you. I hope that works, but note that WSL support is best effort, added just so that people on Windows can run TF with some GPU support. Alternatively, and something I would recommend, is to use Colab. I'm actually using that for a lot of experiments and it's really nice.","I'm marking this as a subissue of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) since it has the same behavior: CPUs that don't support AVX instructions sets (includes ARM ones) are not able to run TF. The only difference, and why this is not marked as duplicate, is that this is a totally different family of CPUs, and likely compiling from source on your own system will produce a wheel that works.","Thank you Mihai! When can I expect resolution? Also, is this something I can compile on my computer? If so, do you have instructions to compile? Thank you again for your help. Dhimant On Thu, Jan 16, 2025 at 4:35 PM Mihai Maruseac ***@***.***> wrote: > I'm marking this as a subissue of CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) >  since it has the > same behavior: CPUs that don't support AVX instructions sets (includes ARM > ones) are not able to run TF. The only difference, and why this is not > marked as duplicate, is that this is a totally different family of CPUs, > and likely compiling from source on your own system will produce a wheel > that works. > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","It is unclear when support will come. There is/was some work to support ARM CPUs on Mac, I think it might come to Linux and maybe windows later but unclear.  might have more details on the plan. Regarding compiling on own computer, that should definitely be possible. There are some instructions at https://www.tensorflow.org/install/source_windows but I haven't checked how up to date they are.","> There is/was some work to support ARM CPUs on Mac, I think it might come to Linux and maybe windows later but unclear To my knowledge there are no current plans to support arm on windows natively.  We do currently publish wheels for Linux and macOS arm64. ","> > So my CPU configuration is not supported? >  > It looks like that. It is different than  CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) in that that issue refers to Intel CPUs that are too old and don't have AVX/AVX2 extensions, whereas here your CPU is ARM, which is a completely different instruction set. It needs files compiled specifically for this architecture and we currently only do that for Linux and Apple (due to Mac M* family). >  > > What is ""missing wheels""? >  > The unit of shipping a Python package is called a wheel. For most projects, there is just one single file for any combination of Python, operating system, architecture. But, since TF needs to compile C++ extensions and link to C Python code, TensorFlow needs to ship different files for different supported configurations. In this case, there is no file that can be served for Windows + ARM CPU. >  > When installing a Python package (via `pip install`, but other installers should follow a relatively similar process), `pip` is looking at the list of files for the specified version and tries to find the one that matches all the architecture tags it knows (Python version, operating system, CPU architecture, GLIB version, manylinux standard, etc.). This ensures that what gets installed works on the system. >  > There in an exception to the above process with CPU extensions: there is no tag for them (since they are quite a lot and can result in exponential explosion of configurations) so `pip` cannot determine them before downloading. This is why  CC(Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.) exists: people have been trying to use TF on very old CPUs and the Windows error message is very confusing (compared to Linux/Mac where it states clearly that the instruction set is not supported). >  > In your case, it is very weird that an ARM CPU and a Windows OS did result in a successful download, even though there is no wheel that matches these tags. >  > > I will try WSL now and get back to you. >  > I hope that works, but note that WSL support is best effort, added just so that people on Windows can run TF with some GPU support. >  > Alternatively, and something I would recommend, is to use Colab. I'm actually using that for a lot of experiments and it's really nice. I have the same issue. I am trying to use TensorFlow in a Windows VM running on my M2 Mac. Would you recommend collab for inference in production?", Were you able to compile Tensorflow yourself in your arm+windows setup as  suggested?,"No, I couldnt On Sun, Feb 9, 2025, 5:09 PM Sakhile Mamba ***@***.***> wrote: >   Were you able to compile > Tensorflow yourself in your arm+windows setup as  >  suggested? > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >"
copybara-service[bot],PR #20994: [GPU] Allow horizontal fusion with shared operands via concatenation.,PR CC(can not build tensoflow inference library): [GPU] Allow horizontal fusion with shared operands via concatenation. Imported from GitHub PR https://github.com/openxla/xla/pull/20994 Copybara import of the project:  2d3f14a878bad09e25f5bbeb5e758cc76d19462b by Ilia Sergachev : [GPU] Allow horizontal fusion with shared operands via concatenation. Merging this change closes CC(can not build tensoflow inference library) Reverts 9055c056336ab90f4c54e24dc9a77ce7afd85166 FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20994 from openxla:horizontal_fusion_shared_operands 2d3f14a878bad09e25f5bbeb5e758cc76d19462b,2025-01-03T12:53:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84101
copybara-service[bot],[XLA:CPU] Use ElementalKernelEmitter in ThunkEmitter,[XLA:CPU] Use ElementalKernelEmitter in ThunkEmitter,2025-01-03T12:26:42Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84100
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T10:54:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84099
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T10:51:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84098
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T10:50:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84097
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T10:48:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84096
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T10:47:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84095
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T10:46:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84094
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T10:43:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84093
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T10:38:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84092
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T10:36:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84091
copybara-service[bot],PR #20595: [nfc] Cleanup build files for simplifier transforms,"PR CC(Make protocol used in estimator customizable.): [nfc] Cleanup build files for simplifier transforms Imported from GitHub PR https://github.com/openxla/xla/pull/20595 This is part2 of CC(pywrap_tensorflow_internal cannot load/not found  Windows tensorflow CPU). Motivation: Smaller build files, fewer merge conflicts, and convinience in development (more intuitive targets). For discussion surrounding this, check thread in CC(pywrap_tensorflow_internal cannot load/not found  Windows tensorflow CPU). Copybara import of the project:  8e547580014fc3686e4bbaeadd4561de36f426df by Shraiysh Vaishay : [nfc] Cleanup build files for simplifier transforms This is part2 of https://github.com/openxla/xla/pull/18785. Motivation: Smaller build files, fewer merge conflicts, and convinience in development (more intuitive targets). Merging this change closes CC(Make protocol used in estimator customizable.) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20595 from shraiysh:cleanup_simplifier_build_files 8e547580014fc3686e4bbaeadd4561de36f426df",2025-01-03T10:36:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84090
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T10:33:57Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84089
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T10:33:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84088
copybara-service[bot],Integrate LLVM at llvm/llvm-project@3ef78188d0d3,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 3ef78188d0d3,2025-01-03T10:28:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84087
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T10:26:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84086
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T10:26:19Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84085
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T10:23:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84084
copybara-service[bot],Reverts 4b360b07d5473d9846b7af27a2f6c326b9ac1bde,Reverts 4b360b07d5473d9846b7af27a2f6c326b9ac1bde,2025-01-03T09:25:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84083
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:25:44Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84082
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:22:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84081
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:22:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84080
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:22:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84079
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:21:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84078
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:21:11Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84077
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:20:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84076
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:20:30Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84075
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:20:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84074
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:19:55Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84073
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:19:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84072
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:19:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84071
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:19:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84070
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:19:05Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84069
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:18:36Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84068
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:18:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84067
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:18:31Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84066
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:18:21Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84065
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:18:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84064
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:18:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84063
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:17:35Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84062
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:17:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84061
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:16:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84060
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:16:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84059
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:16:02Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84058
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:15:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84057
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:15:45Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84056
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:15:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84055
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:15:06Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84054
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:15:01Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84053
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:14:49Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84052
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:14:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84051
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:14:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84050
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:14:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84049
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:13:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84048
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:12:56Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84047
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:12:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84046
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T09:12:17Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84045
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T08:53:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84044
dayalatamai,GPU Delegate for Object detection and Image classification from Tensorflow lite for Android ,"I'm using Android Kotlin code for object detection and image classification  https://github.com/tensorflow/examples/tree/master/lite/examples var currentDelegate: Int = 1,   val optionsBuilder =             ObjectDetector.ObjectDetectorOptions.builder()                 .setScoreThreshold(threshold)                 .setMaxResults(maxResults)         // Set general detection options, including number of used threads         val baseOptionsBuilder = BaseOptions.builder() val compatList = CompatibilityList()                 if (compatList.isDelegateSupportedOnThisDevice) {                     val delegateOptions = compatList.bestOptionsForThisDevice                     baseOptionsBuilder.useGpu()                     Utils.readLog(TAG, ""Using GPU delegate with options: $delegateOptions"")                 } else {                     baseOptionsBuilder.setNumThreads(4)                 } optionsBuilder.setBaseOptions(baseOptionsBuilder.build()) try {             objectDetector =                 ObjectDetector.createFromFileAndOptions(                     context,                     modelName,                     optionsBuilder.build()                 )         } catch (e: IllegalStateException) {             objectDetectorListener?.onError(                 ""Object detector failed to initialize. See error logs for details""             )             Utils.readLog(TAG, "" "" + e.message)         } //Image Classification  try {             imageClassifier =                 ImageClassifier.createFromFileAndOptions(                     context,                     modelName,                     optionsBuilder.build()                 )         } catch (e: IllegalStateException) {             imageClassifierListener?.onError(                 ""Image classifier failed to initialize. See error logs for details""             )             Utils.readLog(TAG, ""TFLite failed to load model with error: "" + e.message)         } Here I'm Using above code baseOptionsBuilder using same for both object detection and image classification but if i'm using currentDelegate = 1 for both it is lagging a lot and object detection and image classification is not working with GPU.  Is there anything else that i can use to make it work?",2025-01-03T08:26:52Z,type:support comp:lite TFLiteGpuDelegate,closed,0,9,https://github.com/tensorflow/tensorflow/issues/84043,"Hi,   I apologize for the delayed response to confirm, have you followed this official documentation  ? If not please follow that and let us know is it resolving your issue  or not ? Please make sure that models you are using are compatible with the GPU delegate. If models has unsupported ops it will fall back to CPU execution please check GPU ML operations support If issue still persists please help us with your Github repo along with complete steps to replicate same behavior from our end to investigate this issue from our end. Thank you for your cooperation and patience."," , Thanks for your response.  As I'm using tensorflow lite (already mentioned the url of library) for object detection and image classification.   I want to perform these operation on GPU delegate and in tensorflow already have GPU delegate support so with that how can i merge the google GPU delegate. Adding Object detection and image classification classes for your reference, Please let me know where should i make changes for the same. Thank you once again. class ObjectDetectorHelper(     private var threshold: Float = 0.5f,     private var numThreads: Int = 1,     private var maxResults: Int = 5,     private var currentDelegate: Int = 1,     private var currentModel: Int = 0,     private val context: Context,     private val objectDetectorListener: DetectorListener? ) {     private var objectDetector: ObjectDetector? = null     init {         setupObjectDetector()     }     fun clearObjectDetector() {         objectDetector = null     }     private fun setupObjectDetector() {         detectorScope.launch {             val optionsBuilder =                 ObjectDetector.ObjectDetectorOptions.builder()                     .setScoreThreshold(threshold)                     .setMaxResults(maxResults)             val baseOptionsBuilder = BaseOptions.builder()             when (currentDelegate) {                 DELEGATE_CPU > {                     baseOptionsBuilder.setNumThreads(numThreads)                 }                 DELEGATE_GPU > {                     val compatList = CompatibilityList()                     if (compatList.isDelegateSupportedOnThisDevice) {                         val delegateOptions = compatList.bestOptionsForThisDevice                         baseOptionsBuilder.useGpu()                     } else {                         baseOptionsBuilder.setNumThreads(4)                     }                 }                 DELEGATE_NNAPI > {                     baseOptionsBuilder.setNumThreads(numThreads).useNnapi()                 }             }             optionsBuilder.setBaseOptions(baseOptionsBuilder.build())             val modelName =                 when (currentModel) {                     MODEL_MOBILENETV1 > ""efficientdet_lite2_OD7.tflite""                     else > ""efficientdet_lite2_OD7.tflite""                 }             try {                 objectDetector =                     ObjectDetector.createFromFileAndOptions(                         context,                         modelName,                         optionsBuilder.build()                     )             } catch (e: IllegalStateException) {                 objectDetectorListener?.onError(                     ""Object detector failed to initialize. See error logs for details""                 )             }         }     }     fun detect(image: Bitmap, imageRotation: Int, localAspectRation: Int) {         if (objectDetector == null) {             setupObjectDetector()         }         detectorScope.launch {             try {                 var inferenceTime = SystemClock.uptimeMillis()                 val imageProcessor =                     ImageProcessor.Builder()                         .add(Rot90Op(imageRotation / 90))                         .build()                 val tensorImage = imageProcessor.process(TensorImage.fromBitmap(image))                 val results1 = objectDetector?.detect(tensorImage)                 inferenceTime = SystemClock.uptimeMillis()  inferenceTime                 var results: MutableList? = null                 if (results1!!.size > 1) {                     for (i in 0 until results1.size) {                         Utils.readLog(""objectDetector boundingBox1 Width  ${results1[i].boundingBox.width()}"")                         Utils.readLog(""objectDetector boundingBox1 Height  ${results1[i].boundingBox.height()}"")                         if (localAspectRation == AspectRatio.RATIO_4_3) {                             if (results1[i].boundingBox.height() > 300  results1[i].boundingBox.width() > 400) {                                 results = listOf(results1[i]).toMutableList()                                 break                             }                         } else {                             if (results1[i].boundingBox.height() > 350) {                                 results = listOf(results1[i]).toMutableList()                                 break                             }                         }                     }                 } else {                     results = results1                 }                 objectDetectorListener?.onResults(                     results,                     inferenceTime,                     tensorImage.height,                     tensorImage.width                 )             } catch (e: Exception) {                 objectDetectorListener?.onError(""Error detecting an image: ${e.message}"")             }         }     }     interface DetectorListener {         fun onError(error: String){}         fun onResults(             results: MutableList?,             inferenceTime: Long,             imageHeight: Int,             imageWidth: Int         )     }     companion object {         const val DELEGATE_CPU = 0         const val DELEGATE_GPU = 1         const val DELEGATE_NNAPI = 2         const val MODEL_MOBILENETV1 = 0         private val detectorScope = CoroutineScope(newSingleThreadContext(""DetectorGpuThread""))         private const val TAG = ""ObjectDetectorHelper""     } } class ImageClassifierHelper(     private var threshold: Float = 0.5f,     private var numThreads: Int = 1,     private var maxResults: Int = 1,     private var currentDelegate: Int = 0,     private val context: Context,     private val imageClassifierListener: ClassifierListener? ) {     private var imageClassifier: ImageClassifier? = null     init {         setupImageClassifier()     }     fun clearImageClassifier() {         imageClassifier = null     }     private fun setupImageClassifier() {         val optionsBuilder = ImageClassifier.ImageClassifierOptions.builder()             .setScoreThreshold(threshold)             .setMaxResults(maxResults)         val baseOptionsBuilder = BaseOptions.builder()         when (currentDelegate) {             DELEGATE_CPU > {                 baseOptionsBuilder.setNumThreads(numThreads)             }             DELEGATE_GPU > {                 val compatList = CompatibilityList()                 if (compatList.isDelegateSupportedOnThisDevice) {                     val delegateOptions = compatList.bestOptionsForThisDevice                     baseOptionsBuilder.useGpu()                 } else {                     baseOptionsBuilder.setNumThreads(4)                 }             }             DELEGATE_NNAPI > {                 baseOptionsBuilder.setNumThreads(numThreads).useNnapi()             }         }         optionsBuilder.setBaseOptions(baseOptionsBuilder.build())         val modelName = ""reshaped_model_optimized_graph.tflite""         try {             imageClassifier =                 ImageClassifier.createFromFileAndOptions(context, modelName, optionsBuilder.build())         } catch (e: IllegalStateException) {             imageClassifierListener?.onError(                 ""Image classifier failed to initialize. See error logs for details""             )         }     }     fun classify(image: Bitmap, rotation: Int) {         if (imageClassifier == null) {             setupImageClassifier()         }         var inferenceTime = SystemClock.uptimeMillis()         val imageProcessor =             ImageProcessor.Builder()                 .build()         val tensorImage = imageProcessor.process(TensorImage.fromBitmap(image))         val imageProcessingOptions = ImageProcessingOptions.builder()             .setOrientation(getOrientationFromRotation(rotation))             .build()         val results = imageClassifier?.classify(tensorImage, imageProcessingOptions)         inferenceTime = SystemClock.uptimeMillis()  inferenceTime         imageClassifierListener?.onResults(             results,             inferenceTime         )     }     private fun getOrientationFromRotation(rotation: Int) : ImageProcessingOptions.Orientation {         return when (rotation) {             Surface.ROTATION_270 >                 ImageProcessingOptions.Orientation.BOTTOM_RIGHT             Surface.ROTATION_180 >                 ImageProcessingOptions.Orientation.RIGHT_BOTTOM             Surface.ROTATION_90 >                 ImageProcessingOptions.Orientation.TOP_LEFT             else >                 ImageProcessingOptions.Orientation.RIGHT_TOP         }     }     interface ClassifierListener {         fun onError(error: String)         fun onResults(             results: List?,             inferenceTime: Long         )     }     companion object {         const val DELEGATE_CPU = 0         const val DELEGATE_GPU = 1         const val DELEGATE_NNAPI = 2         private const val TAG = ""ImageClassifierHelper""     } } *********** Major Issue i'm facing here ************* If I'm trying to use Gpu in Image classification instead of object detection the entire camera screen is lagging(app is getting very slow) and Image is not classifying while classification we're getting the result :  [Classifications{categories=[], headIndex=0}] Category is 'unknown'  Here, I want to use Gpu delegate for image classification as this is consuming most of the memory and at the last it is crashing in Most of Pixels device. As, you suggested here https://ai.google.dev/edge/litert/android/gpukotlin How to use the same in my code snippet.","Hi,  To confirm, To enable access to the GPU delegate, add `com.google.ai.edge.litert:litertgpudelegateplugin` to your app's `build.gradle` file and also try to Add this line to your `build.gradle` `implementation 'org.tensorflow:tensorflowlitegpuapi:2.10.0'` and see does it solve your issue or not ?  Please refer this somewhat similar issues about GPU delegate https://github.com/tensorflow/tensorflow/issues/57934 and please refer this official blog If possible could you please help us with your Github repo ( which includes all the dependancies) along with complete steps to replicate the same behavior from our end that will be more convenient to resolve the issue ? Thank you for your cooperation and patience. ","Thanks,  for the update but com.google.ai.edge.litert:litertgpudelegateplugin where do i need to add into build.gradle file and the another is already there implementation 'org.tensorflow:tensorflowlitegpuapi:2.10.0' in build.gradle  Still i'm getting the same if i'm trying to enable gpu for image classification App is lagging and image is not classifying for the same and it is crashing most of the time so sharing a crash report for the same.   As you have requested for git repo, i do not have the access to share and it is private repo so. Meanwhile i can share my build.gradle file with you for the reference. ******** build.gradle ********* apply plugin: 'com.android.application' apply plugin: 'com.google.gms.googleservices' apply plugin: 'com.google.firebase.crashlytics' apply plugin: 'kotlinandroid' apply plugin: 'kotlinxserialization' apply plugin: 'androidx.navigation.safeargs' android {     ndkVersion ""25.1.8937393""     signingConfigs {         config {             keyAlias 'video inventory mobile manager'             keyPassword 'vimmapp'             storeFile file('/Users/nuncitpc108/Documents/Android/VIMM_STILL_PHOTOS/vimmvidcomandroid/VIMM360/vimm.keystore')             storePassword 'vimmapp'         }     }     namespace ""com.tab.and2""     compileSdk 34     buildToolsVersion '34.0.0'     defaultConfig {         applicationId ""com.tab.and2""         minSdkVersion 28         targetSdkVersion 34         versionCode 65         versionName ""4.5.5.01.08.2025""         multiDexEnabled true         resConfig ""en""         ndk {             abiFilters 'armeabiv7a', 'arm64v8a'         }     }     splits {         abi {             include ""armeabiv7a"", ""arm64v8a""         }     }     applicationVariants.configureEach { variant >         variant.outputs.each { output >             def versionCodes = [""armeabiv7a"": 1, ""arm64v8a"": 2]             def abi = output.getFilter(com.android.build.OutputFile.ABI)             if (abi != null) {  // null for the universaldebug, universalrelease variants                 output.versionCodeOverride =                         versionCodes.get(abi) * 1048576 + defaultConfig.versionCode             }         }     }     def buildType // Your variable     android.applicationVariants.configureEach { variant >         variant.outputs.configureEach {             buildType = variant.buildType.name // Sets the current build type             outputFileName = ""VIMM360_$buildType(${variant.versionCode}v_${variant.versionName})_"" + new Date().format('hh_mmaa') + "".apk""         }     }     dexOptions {         preDexLibraries = true         javaMaxHeapSize ""4g"" // 2g should be also OK     }     lintOptions {         checkReleaseBuilds false         abortOnError false     }     packagingOptions { //        exclude 'METAINF/DEPENDENCIES'         exclude(""METAINF/DEPENDENCIES"")         exclude(""METAINF/LICENSE"")         exclude(""METAINF/LICENSE.txt"")         exclude(""METAINF/license.txt"")         exclude(""METAINF/NOTICE"")         exclude(""METAINF/NOTICE.txt"")         exclude(""METAINF/notice.txt"")         exclude(""METAINF/ASL2.0"")         exclude(""METAINF/*.kotlin_module"")     }     buildTypes {         release {             debuggable false             minifyEnabled false             proguardFiles getDefaultProguardFile('proguardandroid.txt'), 'proguardrules.pro'             signingConfig signingConfigs.config             firebaseCrashlytics {                 mappingFileUploadEnabled false             }         }         debug {             applicationIdSuffix ''             minifyEnabled false             proguardFiles getDefaultProguardFile('proguardandroid.txt'), 'proguardrules.pro'             firebaseCrashlytics {                 mappingFileUploadEnabled false             }         }     }     buildFeatures {         compose true         buildConfig true         viewBinding true     }     composeOptions {         kotlinCompilerExtensionVersion compose_version     }     compileOptions {         sourceCompatibility JavaVersion.VERSION_17         targetCompatibility JavaVersion.VERSION_17     }     kotlinOptions {         jvmTarget = '17'     }     sourceSets {         main {             jniLibs.srcDirs = ['src/main/jniLibs']         }     } } dependencies {     implementation 'androidx.multidex:multidex:2.0.1'     implementation 'com.google.android.gms:playservicesvision:20.1.3'     implementation 'me.dm7.barcodescanner:zbar:1.9.8'     implementation files('libs/environment3.jar')     implementation 'androidx.legacy:legacysupportv13:1.0.0'     implementation 'androidx.recyclerview:recyclerview:1.3.2'     implementation 'androidx.appcompat:appcompat:1.6.1'     implementation 'androidx.cardview:cardview:1.0.0'     implementation 'com.google.apis:googleapiservicesstorage:v1rev991.22.0'     implementation 'com.github.bumptech.glide:glide:4.16.0'     implementation 'androidx.constraintlayout:constraintlayout:2.1.4'     annotationProcessor 'com.github.bumptech.glide:compiler:4.14.2'     implementation 'com.arthenica:mobileffmpegfullgpl:4.4'     implementation ""org.jetbrains.kotlin:kotlinstdlib:1.8.10""     implementation(platform(""org.jetbrains.kotlin:kotlinbom:1.8.10""))     implementation ""org.jetbrains.kotlinx:kotlinxserializationjson:1.5.0""     implementation ""org.jetbrains.kotlin:kotlinstdlib:1.8.20""     implementation ""androidx.fragment:fragmentktx:1.5.7""     implementation 'com.google.ar:core:1.36.0'     implementation ""androidx.window:window:1.3.0""     implementation 'com.squareup.okhttp3:okhttp:4.9.1'     implementation 'com.google.code.gson:gson:2.8.9'     androidTestImplementation 'androidx.test.espresso:espressocore:3.4.0'     androidTestImplementation 'androidx.test.ext:junit:1.1.3'     testImplementation 'junit:junit:4.13.2'     implementation 'com.google.android.material:material:1.9.0'     implementation 'com.amazonaws:awsandroidsdkcore:2.7.7'     implementation 'com.amazonaws:awsandroidsdks3:2.7.7'     def lifecycle_version = ""2.7.0""     implementation ""com.ricoh360.thetaclient:thetaclient:1.10.2""     implementation ""androidx.core:corektx:1.9.0""     //noinspection GradleDependency     implementation""org.jetbrains.kotlin:kotlinstdlibjdk8:1.8.0""     implementation ""androidx.compose.ui:ui:$compose_version""     implementation ""androidx.compose.material:material:$compose_version""     implementation ""androidx.compose.ui:uitoolingpreview:$compose_version""     implementation ""androidx.lifecycle:lifecycleruntimektx:$lifecycle_version""     implementation ""androidx.lifecycle:lifecycleviewmodelktx:$lifecycle_version""     implementation ""androidx.lifecycle:lifecycleviewmodelcompose:$lifecycle_version""     implementation 'androidx.activity:activitycompose:1.9.3'     implementation ""androidx.ui:uiframework:0.1.0dev10""     implementation ""androidx.navigation:navigationcompose:2.7.7""     implementation 'androidx.webkit:webkit:1.10.0'     implementation ""org.jetbrains.kotlinx:kotlinxcoroutinesandroid:$coroutines_version""     implementation 'org.jetbrains.kotlinx:kotlinxserializationjson:1.6.0'     implementation 'com.jakewharton.timber:timber:5.0.1'     implementation 'io.coilkt:coilcompose:2.2.2'     implementation ""io.ktor:ktorclientcio:2.3.9""     implementation ""androidx.activity:activityktx:$activity_version""     testImplementation 'org.junit.jupiter:junitjupiter:5.9.0'     testImplementation ""org.jetbrains.kotlinx:kotlinxcoroutinestest:$coroutines_version""     testImplementation 'org.junit.jupiter:junitjupiter'     androidTestImplementation 'androidx.test.ext:junit:1.1.5'     androidTestImplementation 'androidx.test.espresso:espressocore:3.5.1'     androidTestImplementation ""androidx.compose.ui:uitestjunit4:$compose_version""     debugImplementation ""androidx.compose.ui:uitooling:$compose_version""     debugImplementation ""androidx.compose.ui:uitestmanifest:$compose_version""     implementation platform('com.google.firebase:firebasebom:33.4.0')     implementation(""com.google.firebase:firebasecrashlytics"")     implementation(""com.google.firebase:firebaseanalytics"")     implementation 'androidx.localbroadcastmanager:localbroadcastmanager:1.1.0'     implementation ""androidx.navigation:navigationfragmentktx:$nav_version""     implementation ""androidx.navigation:navigationuiktx:$nav_version""     def camerax_version = '1.3.4' //'1.1.0beta03'     implementation ""androidx.camera:cameracore:$camerax_version""     implementation ""androidx.camera:cameracamera2:$camerax_version""     implementation ""androidx.camera:cameralifecycle:$camerax_version""     implementation ""androidx.camera:cameraview:$camerax_version""     implementation ""androidx.camera:cameraextensions:$camerax_version""     implementation 'com.google.android.gms:playservicestflitejava:16.0.1'     implementation 'com.google.android.gms:playservicestflitegpu:16.1.0'     implementation 'org.tensorflow:tensorflowlitetaskvision:0.4.0'     implementation 'org.tensorflow:tensorflowlitegpudelegateplugin:0.4.0'     implementation 'org.tensorflow:tensorflowlitegpu:2.12.0'     implementation 'org.tensorflow:tensorflowlitegpuapi:2.10.0'     implementation 'org.tensorflow:tensorflowlite:2.12.0'     implementation 'org.tensorflow:tensorflowlitesupport:0.4.1'     implementation 'com.google.guava:listenablefuture:9999.0emptytoavoidconflictwithguava'     configurations {         all*.exclude group: 'com.google.guava', module: 'listenablefuture'         all*.exclude module:'guavajdk5'     } } ","Hi,   As far I know not all TensorFlow operations (ops) are supported by the GPU delegate. If a model includes unsupported ops TensorFlow Lite will execute those ops on the CPU instead which can lead to suboptimal performance and increased inference time due to the overhead of CPUGPU synchronization, please refer this official documentation for GPU ML operations support Not all devices support the necessary GPU features for TensorFlow Lite's GPU delegate so if you don't mind could you please give it try on different devices and see, Is it working as expected or not ? certain models may inherently be less compatible with GPU acceleration due to their architecture or specific operations used within them so please make sure that used models support GPU acceleration If possible please try it by Installing the required libraries in `build.gradle` like below : ``` implementation(""org.tensorflow:tensorflowlite:+"") implementation(""org.tensorflow:tensorflowlitegpu:+"") implementation(""org.tensorflow:tensorflowlitegpuapi:+"") implementation(""org.tensorflow:tensorflowlitegpudelegateplugin:+"") implementation(""org.tensorflow:tensorflowlitesupport:+"") ``` For GPU Delegate settings import below module : ``` import org.tensorflow.lite.Interpreter import org.tensorflow.lite.gpu.CompatibilityList import org.tensorflow.lite.gpu.GpuDelegate ``` Set `GPUDelegate` in the model’s Interpreter options like below : ``` val compatList = CompatibilityList() val options = Interpreter.Options().apply{     if(compatList.isDelegateSupportedOnThisDevice){         // if the device has a supported GPU, add the GPU delegate         val delegateOptions = compatList.bestOptionsForThisDevice         this.addDelegate(GpuDelegate(delegateOptions))     } else {         // if the GPU is not supported, run on 4 threads         this.setNumThreads(4)     } } interpreter = Interpreter(model, options) ``` Thank you for your cooperation and patience.","Hey,   Thank you for the response. As you said *Not all devices support the necessary GPU features for TensorFlow Lite's GPU delegate* Right! I've checked most of the device like samsung(s10,s20,s23 ultra,s22,m31) and pixels(2xl,3a,5,7). If i'm enabling  **private var currentDelegate: Int = 1** for both image classification and object detection on given code snippet and checking which supports GPU delegate in above devices, App is getting very slow (getting lag) and in some pixel devices it is not detecting any object and not classifying any image or getting crash sometimes.  If any device does not support GPU then, i'm using 4 threads to run those TensorFlow operations (ops).  Some observations :  If i'm enabling GPU only for image classification  App is getting very slow and lagging a lot. if i'm enabling GPU only for object detection  It is working some what fine.  Expected Solution :  When GPU is enable for image classification, It should work very smoothly as image classification TensorFlow operation is consuming a lot of memory than object detection operation. Adding a reference video for the same from Pixel 5 (image classification and object detection for both GPU is enabled)","Hi,   Please take a look into this issue. Thank you.","Hi.. let's transfer this to the right repo for now, for future reference, please start using the LiteRT repo for future issues. New Issue: https://github.com/googleaiedge/LiteRT/issues/1160",Are you satisfied with the resolution of your issue? Yes No
Myre29,Error occured when compling TensorFlow C++ interface with Bazel," Issue type Build/Install  Have you reproduced the bug with TensorFlow Nightly? No  Source source  TensorFlow version tf 2.15  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version 6.1  GCC/compiler version 8.9  CUDA/cuDNN version 12.2 / 8.9.6.50  GPU model and memory _No response_  Current behavior? I want to install Tensorflow C++ interface, and have followed the version matching and procedure using Bazel. Previously I have encountered the error for rules_python file. The corresponding file has been downloaded and the corresponding url link has been revised in the WORKSPACE for this file. But when fetching repository  and unknownlinuxgnu, the following error occured. But I cannot find the url or the command for these two file that I can revise the link with the location of the corresponding file stated in the error information. The configuration information is as follows: ./configure You have bazel 6.1.0 installed. Please specify the location of python. [Default is /home/workspace/anaconda3/bin/python3]:  Found possible Python library paths:   /home/workspace/anaconda3/lib/python3.10/sitepackages Please input the desired Python library path to use.  Default is [/home/workspace/anaconda3/lib/python3.10/sitepackages] Do you wish to build TensorFlow with ROCm support? [y/N]: N No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: y CUDA support will be enabled for TensorFlow. Do you wish to build TensorFlow with TensorRT support? [y/N]: N No TensorRT support will be enabled for TensorFlow. Found CUDA 12.2 in:     /usr/local/cuda12.2/targets/x86_64linux/lib     /usr/local/cuda12.2/targets/x86_64linux/include Found cuDNN 8 in:     /usr/local/cuda12.2/targets/x86_64linux/lib     /usr/local/cuda12.2/targets/x86_64linux/include Please specify a list of commaseparated CUDA compute capabilities you want to build with. You can find the compute capability of your device at: https://developer.nvidia.com/cudagpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code. Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 8.9]:  Do you want to use clang as CUDA compiler? [Y/n]: n nvcc will be used as CUDA compiler. Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:  Please specify optimization flags to use during compilation when bazel option ""config=opt"" is specified [Default is Wnosigncompare]:  Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""config="" to your build command. See .bazelrc for more details. 	config=mkl         	 Build with MKL support. 	config=mkl_aarch64 	 Build with oneDNN and Compute Library for the Arm Architecture (ACL). 	config=monolithic  	 Config for mostly static monolithic build. 	config=numa        	 Build with NUMA support. 	config=dynamic_kernels	 (Experimental) Build kernels into separate shared objects. 	config=v1          	 Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features: 	config=nogcp       	 Disable GCP support. 	config=nonccl      	 Disable NVIDIA NCCL support. Configuration finished  Standalone code to reproduce the issue ```shell bazel build config=opt config=cuda verbose_failures //tensorflow:libtensorflow_cc.so ```  Relevant log output ```shell WARNING: Download from https://github.com/indygreg/pythonbuildstandalone/releases/download/20231002/cpython3.10.13+20231002x86_64unknownlinuxgnuinstall_only.tar.gz failed: class java.io.IOException connect timed out ERROR: An error occurred during the fetch of repository 'python_x86_64unknownlinuxgnu':    Traceback (most recent call last): 	File ""/home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/rules_python/python/repositories.bzl"", line 175, column 34, in _python_repository_impl 		rctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error downloading [https://github.com/indygreg/pythonbuildstandalone/releases/download/20231002/cpython3.10.13+20231002x86_64unknownlinuxgnuinstall_only.tar.gz] to /home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/python_x86_64unknownlinuxgnu/temp11172848094686838309/cpython3.10.13+20231002x86_64unknownlinuxgnuinstall_only.tar.gz: connect timed out ERROR: /home/workspace/Desktop/software/tensorflow2.15.0/WORKSPACE:36:27: fetching python_repository rule //external:python_x86_64unknownlinuxgnu: Traceback (most recent call last): 	File ""/home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/rules_python/python/repositories.bzl"", line 175, column 34, in _python_repository_impl 		rctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error downloading [https://github.com/indygreg/pythonbuildstandalone/releases/download/20231002/cpython3.10.13+20231002x86_64unknownlinuxgnuinstall_only.tar.gz] to /home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/python_x86_64unknownlinuxgnu/temp11172848094686838309/cpython3.10.13+20231002x86_64unknownlinuxgnuinstall_only.tar.gz: connect timed out ERROR: Error computing the main repository mapping: Encountered error while reading extension file 'requirements.bzl': no such package '//': no such package 'unknownlinuxgnu//': java.io.IOException: Error downloading [https://github.com/indygreg/pythonbuildstandalone/releases/download/20231002/cpython3.10.13+20231002x86_64unknownlinuxgnuinstall_only.tar.gz] to /home/workspace/.cache/bazel/_bazel_think/2d2d375446b702059350bd230a24520a/external/python_x86_64unknownlinuxgnu/temp11172848094686838309/cpython3.10.13+20231002x86_64unknownlinuxgnuinstall_only.tar.gz: connect timed out Loading:      Fetching repository ; Restarting. 55s ```",2025-01-03T07:21:59Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/84042,", According to the official document, for tensorflow v2.15, the compiler is Clang 16.0.0. Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations. https://www.tensorflow.org/install/sourcegpu Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-03T05:30:24Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84041
copybara-service[bot],Add an API for retrieving topology description from `xla::Topology`,"Add an API for retrieving topology description from `xla::Topology` `xla::ifrt::Topology::description()` returns `const std::shared_ptr` of the given IFRT topology. All IFRT implementations already have this internally, so this is simply upstreaming this interface. We may consider defining an IFRTspecific type if needed.",2025-01-03T03:25:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84040
ZerryNi,The test case label_image .py of tensorflow2.4.1 source code fails to be execued.," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.4.1  Custom code Yes  OS platform and distribution _No response_  Mobile device _No response_  Python version 3.7.12  Bazel version 3.7  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The test case label_image.py fails to be executed,and the message ""module 'tensorfle' has no attribute 'GrapDef'"" is displayed. !image  Standalone code to reproduce the issue ```shell import tensorflow as tf graph_def = tf.GraphDef() ```  Relevant log output _No response_",2025-01-03T03:03:30Z,stat:awaiting response type:bug stale TF 2.4,closed,0,8,https://github.com/tensorflow/tensorflow/issues/84039,"Hi **** , Welcome to TensorFlow, and thank you for raising your concern here. The error you are facing occurs because 'GraphDef' is not directly accessible under the TensorFlow module in TensorFlow 2.x. To use 'GraphDef,' you need to import it from compat.v1. In TensorFlow 2.x, 'GraphDef' is located within the compat.v1 module to support backward compatibility with TensorFlow 1.x code. Please find the gist here for your reference.  Additionally, we recommend using the latest versions of TensorFlow for better performance and compatibility. Thank you!","Hi,l've used import tensorflow.compat.v1 as tf ,tf.disable_v2_behavior(),but the system still rreports that the incepion_v3_2016_08_28_frozem.pb file is missing,and the same error is reported after I download the file. !errortensorflow","Hi **** , Apologies for the delay, and thank you for your patience. It appears that you are still using an older version. Could you please upgrade to the latest TensorFlow version? Let us know if the issue still persists. Thank you!","Hi,thank you vert much for your reply,but we require tensorflow 3.4.1.","Hi **** , Apologies for the delay, and thanks for your patience. We do not support older or deprecated versions. Please migrate to the latest version for better results. Here, I am providing documentation for migration. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],[XLA:GPU] add inline comments about the intended use cases and limitations of collective send/recv combiner pass,[XLA:GPU] add inline comments about the intended use cases and limitations of collective send/recv combiner pass,2025-01-03T00:22:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84038
copybara-service[bot],[xla:collectives] Migrate Broadcast to type-safe RankId to identify broadcast root,[xla:collectives] Migrate Broadcast to typesafe RankId to identify broadcast root,2025-01-02T23:47:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84037
copybara-service[bot],[xla:cpu] Use uint16_t for indexing host kernel tasks,[xla:cpu] Use uint16_t for indexing host kernel tasks,2025-01-02T23:02:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84036
copybara-service[bot],PR #20975: [ROCm] Fix build break due to XNNPACK update and cuda profiler test,PR CC(Toco not working on Windows): [ROCm] Fix build break due to XNNPACK update and cuda profiler test Imported from GitHub PR https://github.com/openxla/xla/pull/20975 ROCm build and test breaks due to https://github.com/openxla/xla/pull/20542 and https://github.com/openxla/xla/pull/20488 Copybara import of the project:  5003753dbe6a8b92ca64eb5c74af6653e9c95ce0 by Harsha HS : [ROCm] Fix build break due to XNNPACK update and cuda profiler test ROCm build and test breaks due to https://github.com/openxla/xla/pull/20542 and https://github.com/openxla/xla/pull/20488 Merging this change closes CC(Toco not working on Windows) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20975 from ROCm:ci_fix_xnnpack_cvti_20250102 5003753dbe6a8b92ca64eb5c74af6653e9c95ce0,2025-01-02T22:43:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84035
copybara-service[bot],Add an experimental flag for strict Q-DQ annotation,Add an experimental flag for strict QDQ annotation This would provide a route for experimentation with JAX QAT lowering.,2025-01-02T21:55:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84034
sdp009,TFLITE NMS kernel Inconsistent Outputs and Out of Memory issues," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? Yes  Source binary  TensorFlow version tf 2.16.2, tf 2.18, tf 2.19.0dev2024122  Custom code Yes  OS platform and distribution Linux Ubuntu 22  Mobile device Android  Python version 3.10.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? The TFLITE NMS kernel output is not same as Tensorflow NMS output. Although the TFLITE NMS is a dynamic output shape layer, it is _**appending 0's**_ in the ""selected_indices"" output till ""max_output_size"", defeating the purpose of dynamic output. **TFLITE NMS output must identically match with TF NMS output.** For large ""max_output_size"", the TFLITE NMS results in super slow computation and many times it goes Outofmemory on Android devices. The subsequent **Gather** ops, after NMS suffers heavily due to appended 0's in the TFLITE NMS output. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/non_max_suppression.ccL190C44L190C59  **Requesting to fix this behavior and ensure both TF and TFLITE NMS output are exactly same.** !image  Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np  Test inputs from : https://github.com/onnx/onnx/blob/main/docs/Operators.mdNonMaxSuppression : nonmaxsuppression_limit_output_size boxes = np.array(     [         [0.0, 0.0, 1.0, 1.0],         [0.0, 0.1, 1.0, 1.1],         [0.0, 0.1, 1.0, 0.9],         [0.0, 10.0, 1.0, 11.0],         [0.0, 10.1, 1.0, 11.1],         [0.0, 100.0, 1.0, 101.0],     ] ).astype(np.float32) scores = np.array([0.9, 0.75, 0.6, 0.95, 0.5, 0.3]).astype(np.float32) import tensorflow as tf max_output_size = tf.constant(tf.int32.max, dtype=tf.int32) iou_threshold = 0.5 selected_indices = tf.image.non_max_suppression(     boxes, scores, max_output_size, iou_threshold ) print(selected_indices)     returns expected output : tf.Tensor([3 0 5], shape=(3,), dtype=int32) .function(input_signature=[     tf.TensorSpec(shape=[6, 4], dtype=tf.float32),     tf.TensorSpec(shape=[6], dtype=tf.float32), ]) def nms_function(boxes, scores):     return tf.image.non_max_suppression(boxes, scores, max_output_size=tf.constant(tf.int32.max, dtype=tf.int32), iou_threshold=0.5) concrete_function = nms_function.get_concrete_function() print(concrete_function(boxes, scores))     returns expected output : tf.Tensor([3 0 5], shape=(3,), dtype=int32) converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_function]) tflite_model = converter.convert() with open('test_nms.tflite', 'wb') as f:     f.write(tflite_model) interpreter = tf.lite.Interpreter(model_path='test_nms.tflite') interpreter.allocate_tensors() input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() interpreter.set_tensor(input_details[0]['index'], boxes) interpreter.set_tensor(input_details[1]['index'], scores) interpreter.invoke() selected_indices = interpreter.get_tensor(output_details[0]['index']) print(selected_indices)     returns incorrect output appended with 0's : [3 0 5 ... 0 0 0] print(selected_indices.shape)     incorrect output of shape : (2147483647,)  above causes OOM error ```  Relevant log output ```shell TF output = tf.Tensor([3 0 5], shape=(3,), dtype=int32) TF Lite output = [3 0 5 ... 0 0 0]  ; shape = (2147483647,) ```",2025-01-02T21:14:57Z,stat:awaiting tensorflower type:bug comp:lite TF 2.18,closed,0,7,https://github.com/tensorflow/tensorflow/issues/84033,"Please add label ""comp:lite""","Hi,   I apologize for the delayed response, I tried to run provided code in Google colab but I'm getting `Your session crashed after using all available RAM.` so that is due to OOM issue for reference I've added gistfile and output log screenshot below so we'll have to dig more into this issue and update you !image Thank you for your cooperation and patience.","For easier debugging, please reduce the 'max_output_size' to a smaller value to avoid OOM issues. But in actual practice, there are some scenarios where higher 'max_output_size' did caused OOM issues on resource constraint Android devices.","Hi  , did you got chance to inspect it further ? Thanks.","Hi,  Please take a look into this issue. Thank you.","Hi , we will be moving this to LiteRT. Please follow progress there.",Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Integrate LLVM at llvm/llvm-project@f739aa400416,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match f739aa400416,2025-01-02T18:45:22Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84032
copybara-service[bot],[xla:collectives] Remove Send/Recv Ptr To/From peer from Communicator API,[xla:collectives] Remove Send/Recv Ptr To/From peer from Communicator API Sending and receiving pointers is not a part of generic communicator API.,2025-01-02T18:11:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84031
copybara-service[bot],[xla:collectives] Migrate Send/Recv to type-safe RankId to identify peers,[xla:collectives] Migrate Send/Recv to typesafe RankId to identify peers,2025-01-02T17:11:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84030
copybara-service[bot],Import of PR https://github.com/openxla/xla/pull/20858.,Import of PR https://github.com/openxla/xla/pull/20858. Add alternative CUDA root that is used in some systems.,2025-01-02T16:26:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84029
copybara-service[bot],PR #20964: [GPU][NFC] Add missing override specifier.,PR CC(Using tf.contrib.training.batch_sequences_with_states with tf.data.Dataset): [GPU][NFC] Add missing override specifier. Imported from GitHub PR https://github.com/openxla/xla/pull/20964 Copybara import of the project:  349a170bb112671863d81c62cd2db8e71b8f9296 by Ilia Sergachev : [GPU][NFC] Add missing override specifier. Merging this change closes CC(Using tf.contrib.training.batch_sequences_with_states with tf.data.Dataset) FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20964 from openxla:nfc_override 349a170bb112671863d81c62cd2db8e71b8f9296,2025-01-02T15:22:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84028
antipisa,Tensorflow BackupAndRestore method does not work," Issue type Bug  Have you reproduced the bug with TensorFlow Nightly? No  Source condaforge   TensorFlow version 2.17  Custom code No  OS platform and distribution Linux RHEL8  Mobile device _No response_  Python version 3.11.8  Bazel version _No response_  GCC/compiler version GCC 11.2.8  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? BackupAndRestore example code does not work  Standalone code to reproduce the issue ```shell import keras import numpy as np class InterruptingCallback(keras.callbacks.Callback):    def on_epoch_begin(self, epoch, logs=None):      if epoch == 4:        raise RuntimeError('Interrupting!') callback = keras.callbacks.BackupAndRestore(backup_dir=""/tmp/backup"") model = keras.models.Sequential([keras.layers.Dense(10)]) model.compile(keras.optimizers.SGD(), loss='mse') try:    model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,              batch_size=1, callbacks=[callback, InterruptingCallback()],              verbose=0) except Exception as e:    print(e) history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),                      epochs=10, batch_size=1, callbacks=[callback],                      verbose=0) len(history.history['loss']) ```  Relevant log output ```shell ValueError: To use the BackupAndRestore method, your model must be built before you call `fit()`. Model is unbuilt. You can build it beforehand by calling it on a batch of data. ```",2025-01-02T15:20:08Z,stat:awaiting response type:bug comp:keras 2.17,closed,0,9,https://github.com/tensorflow/tensorflow/issues/84027,"The error message you're encountering: ``` ValueError: To use the BackupAndRestore method, your model must be built before you call `fit()`. Model is unbuilt. You can build it beforehand by calling it on a batch of data. ``` indicates that the `BackupAndRestore` callback expects the model to be built before calling `fit()`, but in your case, the model is not explicitly built before the training loop starts.  Understanding the Problem:  **`BackupAndRestore` callback** requires the model to be ""built"" before starting training. The model needs to know the input shapes and architecture in order to correctly manage the backup and restoration processes.  **Model Building**: When you define a `Sequential` model without specifying input shapes, TensorFlow won't know the input shape until data is passed to the model. Therefore, you need to either specify the input shape when defining the model or pass a batch of data to the model before calling `fit()`.  Solution 1: Define the Input Shape in the Model You can explicitly define the input shape when creating the model, which ensures the model is ""built"" before training starts: ```python import keras import numpy as np class InterruptingCallback(keras.callbacks.Callback):    def on_epoch_begin(self, epoch, logs=None):      if epoch == 4:        raise RuntimeError('Interrupting!') callback = keras.callbacks.BackupAndRestore(backup_dir=""/tmp/backup"")  Define the model with an explicit input shape model = keras.models.Sequential([     keras.layers.InputLayer(input_shape=(20,)),   Specify the input shape     keras.layers.Dense(10) ]) model.compile(keras.optimizers.SGD(), loss='mse')  Now the model is built before training try:     model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,               batch_size=1, callbacks=[callback, InterruptingCallback()],               verbose=0) except Exception as e:     print(e) history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),                     epochs=10, batch_size=1, callbacks=[callback],                     verbose=0) print(len(history.history['loss'])) ```  Explanation:  **`InputLayer`**: By adding the `InputLayer` with an explicit `input_shape=(20,)`, you're telling Keras the expected shape of the input data, which ensures that the model is built before calling `fit()`.  Solution 2: Build the Model Before Calling `fit()` Alternatively, you can use a batch of data to build the model explicitly before training. This can be done using the `model.build()` method: ```python import keras import numpy as np class InterruptingCallback(keras.callbacks.Callback):    def on_epoch_begin(self, epoch, logs=None):      if epoch == 4:        raise RuntimeError('Interrupting!') callback = keras.callbacks.BackupAndRestore(backup_dir=""/tmp/backup"")  Define the model without specifying input shape model = keras.models.Sequential([     keras.layers.Dense(10) ]) model.compile(keras.optimizers.SGD(), loss='mse')  Build the model by passing a batch of data model.build(input_shape=(None, 20))   Here, 20 is the number of features in your input data  Now the model is built before calling fit try:     model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,               batch_size=1, callbacks=[callback, InterruptingCallback()],               verbose=0) except Exception as e:     print(e) history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),                     epochs=10, batch_size=1, callbacks=[callback],                     verbose=0) print(len(history.history['loss'])) ```  Explanation:  **`model.build()`**: This explicitly builds the model by providing the `input_shape`. After this call, the model is ready for training, and the `BackupAndRestore` callback will work correctly. To fix the issue, you need to ensure that the model is built (either by specifying the input shape or by explicitly calling `model.build()`) before invoking `fit()`. Both solutions will address the error and allow the `BackupAndRestore` callback to function as expected.","Hi **** , Thanks for raising your concern here. The raised PR has been merged. Could you please check and let us know if the issue still persists?  Thank you!","   Does Solution 1 work when the input layer is a normalization layer? E.g for this example ``` import tensorflow as tf x = tf.random.uniform((100, 1)) y = tf.random.uniform((100, 1)) z = tf.random.uniform((100, 1)) xyz = tf.concat([x, y, z], 1) horsepower_normalizer = tf.keras.layers.Normalization(input_shape=(3,), axis=1) horsepower_normalizer.adapt(xyz) horsepower_model = tf.keras.models.Sequential([     horsepower_normalizer,     tf.keras.layers.Dense(units=1) ]) horsepower_model(xyz) ``` EDIT: This raises a UserWarning about using input_shape in a layer. Is there a better way? ","  Solution 1 raises `ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received: (), (of type ) ' ` ","Hi **** , Apologies for the delay, and thank you for your patience. I tried running your code on Colab using TensorFlow 2.17.0 and 2.18.0 versions, but I am not facing any issues. Could you please check which warning you are encountering? Please find the gist here for your reference. Thank you!","Hi  , the problem is fixed if one imports the following way: ``` from tensorflow import keras from keras import layers from keras.layers import InputLayer, Dense ```","Hi **** , Glad to see your issue is resolved! Please feel free to close this issue if everything is working as expected. Thank you!",Are you satisfied with the resolution of your issue? Yes No,"Hi  ,  May I ask why the discrepancy when specifying input_shape in an Input layer vs when calling model.build? ``` model.build(input_shape=(None, 20))   Here, 20 is the number of features in your input data  input_shape is (None,  20) versus (20, None) in the below keras.layers.InputLayer(input_shape=(20,)),   Specify the input shape ```"
copybara-service[bot],[XLA:CPU] Pass explicit HLO instruction to ElementalKernelEmitter,[XLA:CPU] Pass explicit HLO instruction to ElementalKernelEmitter,2025-01-02T15:04:51Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84026
copybara-service[bot],[XLA:CPU] Move LlvmIrKernelSpec out of testlib,[XLA:CPU] Move LlvmIrKernelSpec out of testlib,2025-01-02T14:58:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84025
copybara-service[bot],[XLA:CPU] Move ElementalKernelEmitter out of testlib,[XLA:CPU] Move ElementalKernelEmitter out of testlib,2025-01-02T14:56:54Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84024
copybara-service[bot],Integrate LLVM at llvm/llvm-project@1623c435948a,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 1623c435948a,2025-01-02T14:43:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84023
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T12:28:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84022
copybara-service[bot],"[XLA:GPU] Populate tma info at runtime when available. Currently, we are never passing this in, but we need it to be in place before we implement it at compile time.","[XLA:GPU] Populate tma info at runtime when available. Currently, we are never passing this in, but we need it to be in place before we implement it at compile time.",2025-01-02T12:27:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84021
copybara-service[bot],[XLA:GPU] Create a pathway between XLA compile-time and runtime to pass a TMAMetadata struct through. This communication pathway is necessary for implementing Nvidia's TMA feature on Hopper+ architectures.,[XLA:GPU] Create a pathway between XLA compiletime and runtime to pass a TMAMetadata struct through. This communication pathway is necessary for implementing Nvidia's TMA feature on Hopper+ architectures.,2025-01-02T11:14:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84020
jonasrsv42,Mixing Keras Layers and TF modules.," Issue type Support  Have you reproduced the bug with TensorFlow Nightly? Yes  Source source  TensorFlow version 2.17  Custom code Yes  OS platform and distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.12  Bazel version _No response_  GCC/compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current behavior? tf.Module can trace tf.Variable but it cannot trace variables from tf.keras or tf.keras.Variable.   Standalone code to reproduce the issue ```shell class MockLayer(tf.Module):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)         self.m = tf.keras.Variable(tf.random.normal([5, 5]), name=""m"")         self.w = tf.keras.Variable(tf.random.normal([5, 5]), name=""w"")     def __call__(self, inputs):         return self.m * inputs layer1 = MockLayer() print([v.name for v in layer1.trainable_variables]) ``` is empty. ``` class MockLayer(tf.Module):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)         self.m = tf.Variable(tf.random.normal([5, 5]), name=""m"")         self.w = tf.Variable(tf.random.normal([5, 5]), name=""w"")     def __call__(self, inputs):         return self.m * inputs layer1 = MockLayer() print([v.name for v in layer1.trainable_variables]) ``` Works. Specifically I am more interested in keras layers like  ``` class MockLayer(tf.Module):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)         self.norm = tf.keras.layers.LayerNormalization(*args, **kwargs)     def __call__(self, inputs):         return self.norm(inputs) ``` For which tracing does not appear to work.  I am interested in trying https://github.com/google/sequencelayers but they seem mostly broken on this TF version and python version due to the tracing issues.  I am curious to try fixing it, but not sure what's a supported path. Using keras layers in tf.Module does not work due to tracing issues.. and using Keras layers only does not work because Keras is missing many features like composite tensors.  Is there a way around this? :) ```  Relevant log output _No response_",2025-01-02T10:46:55Z,stat:awaiting response type:support stale comp:keras 2.17,closed,1,7,https://github.com/tensorflow/tensorflow/issues/84019,"I just hit the same issue trying to upgrade some code that was previously working on tensorflow 2.14; My setup is the same as described:    Top level `tf.Module`    Submodules are `keras.Model`s The kerasbased submodules are now not being detected at the `tf.Module` level because the reflection based implementation is explicitly looking for submodules that extend `tf.Module` (here). It appears that since the 2.16 release that switched to keras 3.X, `keras.Model` / `keras.layers.Layer` no longer extends `tf.Module` but instead only extends the underlying `AutoTrackable` class via its own `TFLayer` class (here). This contradicts / invalidates the tensorflow documentation here: > tf.keras.layers.Layer is the base class of all Keras layers, and it inherits from tf.Module. Looking through some issues in the keras repo, it appears this is intentional, unfortunately. Specifically this comment. There is a section in the keras documentation  about this and gives some direction for downgrading keras to v2 in order to support this structure.",", By default Tensorflow v2.17, v2.18 contains the Keras3.0.  As mentioned in this comment, Keras 3 design supports multiple backends (like JAX and PyTorch) in addition to TensorFlow. To achieve this, core classes like Model and Layer no longer rely on TensorFlow `tf.Module`. As a result, variable tracking within models and layers is no longer automatic. As this issue is more related to Keras, Kindly raise the request in the Kerasteam/keras repo for further discussion. Thank you!","I can raise it in Keras. But out of curiousity, I am trying to use tensorflow without Keras because Keras is missing features I want. (E.g Composite objects for Tensors)  But it seems with this split Tensorflow loses implementations for many common layers? Is there some intended replacement implementation of these layers for tensorflow if Keras will no longer work?  Or is the expectation that we should roll our own using tensorflow primitives for all the layers that used to be in tf.keras? ",It is unlikely that TF would get these layers.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
copybara-service[bot],Integrate LLVM at llvm/llvm-project@a3744f065a3c,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match a3744f065a3c,2025-01-02T10:28:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84018
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:26:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84017
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:26:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84016
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:26:15Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84015
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:25:09Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84014
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:23:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84013
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:22:12Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84012
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:21:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84011
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:19:53Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84010
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:19:51Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84009
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:18:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84008
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:17:59Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84007
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:16:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84006
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:16:36Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84005
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:16:14Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84004
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:15:16Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/84003
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:15:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84002
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:14:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84001
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T09:13:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/84000
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T08:47:56Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83999
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T08:46:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83998
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T08:40:57Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83997
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T08:29:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83996
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T08:21:19Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83995
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T08:06:10Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83994
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T07:49:40Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83993
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T07:37:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83992
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T07:19:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83991
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T07:17:27Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83990
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T07:13:42Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83989
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T07:07:20Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83988
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-02T04:20:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83987
omarakl,I'm having a problem in converting my (.h5) model to (.tflite),"import tensorflow as tf store .h5 file in your .py folder load h5 module model=tf.keras.models.load_model('enhanced_model.h5') tflite_converter = tf.lite.TFLiteConverter.from_keras_model(model) convert tflite_model = tflite_converter.convert() open(""final.tflite"", ""wb"").write(tflite_model)  Output:  Exception encountered: int() argument must be a string, a byteslike object or a real number, not 'list' PS C:\Users\USER\Desktop\New folder (2)> ",2025-01-01T23:13:07Z,TFLiteConverter,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83986
copybara-service[bot],[xla:cpu] Keep pointers and primitive sizes next to each other for cache locality,[xla:cpu] Keep pointers and primitive sizes next to each other for cache locality ``` name                                                                                old cpu/op   new cpu/op   delta BM_Sort1D/input_size:1000/num_inputs:1/is_stable:0/sort_ascending:1/process_time    11.5µs ± 2%  11.4µs ± 2%  0.33%  (p=0.015 n=78+78) BM_Sort1D/input_size:1000/num_inputs:2/is_stable:0/sort_ascending:1/process_time    98.4µs ± 2%  98.2µs ± 2%  0.25%  (p=0.024 n=76+75) BM_Sort1D/input_size:1000/num_inputs:4/is_stable:0/sort_ascending:1/process_time     127µs ± 2%   127µs ± 2%    ~     (p=0.706 n=77+78) BM_Sort1D/input_size:1000/num_inputs:8/is_stable:0/sort_ascending:1/process_time     197µs ± 2%   199µs ± 2%  +1.07%  (p=0.000 n=74+74) BM_Sort1D/input_size:1000/num_inputs:16/is_stable:0/sort_ascending:1/process_time    341µs ± 2%   340µs ± 2%  0.26%  (p=0.034 n=76+74) BM_Sort1D/input_size:1000/num_inputs:32/is_stable:0/sort_ascending:1/process_time    920µs ± 2%   833µs ± 2%  9.45%  (p=0.000 n=76+71) BM_Sort1D/input_size:1000/num_inputs:1/is_stable:0/sort_ascending:0/process_time    87.5µs ± 2%  85.7µs ± 2%  1.97%  (p=0.000 n=71+77) BM_Sort1D/input_size:1000/num_inputs:2/is_stable:0/sort_ascending:0/process_time    98.3µs ± 2%  98.0µs ± 2%  0.31%  (p=0.005 n=74+77) BM_Sort1D/input_size:1000/num_inputs:4/is_stable:0/sort_ascending:0/process_time     127µs ± 1%   127µs ± 2%    ~     (p=0.518 n=77+77) BM_Sort1D/input_size:1000/num_inputs:8/is_stable:0/sort_ascending:0/process_time     197µs ± 2%   200µs ± 2%  +1.20%  (p=0.000 n=73+73) BM_Sort1D/input_size:1000/num_inputs:16/is_stable:0/sort_ascending:0/process_time    340µs ± 2%   340µs ± 2%    ~     (p=0.979 n=77+73) BM_Sort1D/input_size:1000/num_inputs:32/is_stable:0/sort_ascending:0/process_time    921µs ± 2%   833µs ± 2%  9.59%  (p=0.000 n=75+72) ```,2025-01-01T21:43:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83985
copybara-service[bot],[xla:cpu] Optimize filling compared values,[xla:cpu] Optimize filling compared values ``` name                                                                               old cpu/op   new cpu/op   delta BM_Sort1D/input_size:1000/num_inputs:1/is_stable:0/sort_ascending:1/process_time   11.4µs ± 2%  11.5µs ± 2%   +0.48%  (p=0.000 n=76+76) BM_Sort1D/input_size:1000/num_inputs:2/is_stable:0/sort_ascending:1/process_time   98.1µs ± 1%  98.2µs ± 2%     ~     (p=0.522 n=78+74) BM_Sort1D/input_size:1000/num_inputs:4/is_stable:0/sort_ascending:1/process_time    125µs ± 2%   127µs ± 1%   +1.28%  (p=0.000 n=78+78) BM_Sort1D/input_size:1000/num_inputs:8/is_stable:0/sort_ascending:1/process_time    195µs ± 1%   197µs ± 2%   +0.84%  (p=0.000 n=74+75) BM_Sort1D/input_size:1000/num_inputs:16/is_stable:0/sort_ascending:1/process_time   336µs ± 2%   340µs ± 2%   +1.34%  (p=0.000 n=74+75) BM_Sort1D/input_size:1000/num_inputs:32/is_stable:0/sort_ascending:1/process_time  1.15ms ± 1%  0.92ms ± 2%  20.25%  (p=0.000 n=74+75) BM_Sort1D/input_size:1000/num_inputs:1/is_stable:0/sort_ascending:0/process_time   87.2µs ± 1%  87.5µs ± 2%   +0.28%  (p=0.009 n=80+79) BM_Sort1D/input_size:1000/num_inputs:2/is_stable:0/sort_ascending:0/process_time   98.0µs ± 2%  98.1µs ± 2%     ~     (p=0.378 n=77+78) BM_Sort1D/input_size:1000/num_inputs:4/is_stable:0/sort_ascending:0/process_time    125µs ± 1%   127µs ± 2%   +1.29%  (p=0.000 n=77+76) BM_Sort1D/input_size:1000/num_inputs:8/is_stable:0/sort_ascending:0/process_time    195µs ± 2%   197µs ± 2%   +0.66%  (p=0.000 n=77+76) BM_Sort1D/input_size:1000/num_inputs:16/is_stable:0/sort_ascending:0/process_time   335µs ± 2%   339µs ± 2%   +1.43%  (p=0.000 n=75+74) BM_Sort1D/input_size:1000/num_inputs:32/is_stable:0/sort_ascending:0/process_time  1.15ms ± 1%  0.92ms ± 1%  20.18%  (p=0.000 n=74+76) ```,2025-01-01T21:40:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83984
copybara-service[bot],[xla:cpu] NFC: Add const qualification to Input/DInput pointers,[xla:cpu] NFC: Add const qualification to Input/DInput pointers,2025-01-01T21:39:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83983
copybara-service[bot],Integrate LLVM at llvm/llvm-project@91c5de7fb8f9,Integrate LLVM at llvm/llvmproject Updates LLVM usage to match 91c5de7fb8f9,2025-01-01T21:27:55Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83982
copybara-service[bot],[xla] Update warnings.bazelrc,[xla] Update warnings.bazelrc,2025-01-01T17:42:24Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83981
copybara-service[bot],[xla:cpu] Remove todo,[xla:cpu] Remove todo,2025-01-01T17:10:28Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83980
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T09:20:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83979
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T09:18:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83978
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T09:16:26Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83977
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T09:15:37Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83976
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T09:14:54Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83975
dependabot[bot],Bump ubuntu from `278628f` to `80dd3c3` in /tensorflow/tools/gcs_test,"Bumps ubuntu from `278628f` to `80dd3c3`. ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2025-01-01T08:21:29Z,ready to pull size:XS dependencies docker,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83974
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T05:44:32Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83973
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T05:35:26Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83972
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T05:34:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83971
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T05:33:49Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83970
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T05:33:05Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83969
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T05:30:16Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83968
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T05:29:41Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83967
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T05:26:04Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83966
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T05:21:46Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83965
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T05:19:38Z,,open,0,0,https://github.com/tensorflow/tensorflow/issues/83964
copybara-service[bot],Automated Code Change,Automated Code Change,2025-01-01T05:14:39Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/83963
